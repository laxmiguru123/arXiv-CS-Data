summary
"We discuss how distributed designs that draw from biological network
metaphors can largely improve the current state of information retrieval and
knowledge management of distributed information systems. In particular, two
adaptive recommendation systems named TalkMine and @ApWeb are discussed in more
detail. TalkMine operates at the semantic level of keywords. It leads different
databases to learn new and adapt existing keywords to the categories recognized
by its communities of users using distributed algorithms. @ApWeb operates at
the structural level of information resources, namely citation or hyperlink
structure. It relies on collective behavior to adapt such structure to the
expectations of users. TalkMine and @ApWeb are currently being implemented for
the research library of the Los Alamos National Laboratory under the Active
Recommendation Project. Together they define a biologically motivated
information retrieval system, recommending simultaneously at the level of user
knowledge categories expressed in keywords, and at the level of individual
documents and their associations to other documents. Rather than passive
information retrieval, with this system, users obtain an active, evolving
interaction with information resources."
"Computers and devices are largely unaware of events taking place in the
world. This could be changed if news were made available in a
computer-understandable form. In this paper we present XML documents called
NewsForms that represent the key points of 17 types of news events. We discuss
the benefits of computer-understandable news and present the NewsExtract
program for converting text news stories into NewsForms."
"Data modeling is one of the most difficult tasks in application engineering.
The engineer must be aware of the use cases and the required application
services and at a certain point of time he has to fix the data model which
forms the base for the application services. However, once the data model has
been fixed it is difficult to consider changing needs. This might be a problem
in specific domains, which are as dynamic as the healthcare domain. With fuzzy
data we address all those data that are difficult to organize in a single
database. In this paper we discuss a gradual and pragmatic approach that uses
the XML technology to conquer more model flexibility. XML may provide the clue
between unstructured text data and structured database solutions and shift the
paradigm from ""organizing the data along a given model"" towards ""organizing the
data along user requirements""."
"In this paper we discuss several issues related to automated text
classification of web sites. We analyze the nature of web content and metadata
in relation to requirements for text features. We find that HTML metatags are a
good source of text features, but are not in wide use despite their role in
search engine rankings. We present an approach for targeted spidering including
metadata extraction and opportunistic crawling of specific semantic hyperlinks.
We describe a system for automatically classifying web sites into industry
categories and present performance results based on different combinations of
text features and training data. This system can serve as the basis for a
generalized framework for automated metadata creation."
"Context and context-awareness provides computing environments with the
ability to usefully adapt the services or information they provide. It is the
ability to implicitly sense and automatically derive the user needs that
separates context-aware applications from traditionally designed applications,
and this makes them more attentive, responsive, and aware of their user's
identity, and their user's environment. This paper argues that context-aware
applications capable of supporting complex, cognitive activities can be built
from a model of context called Activity-Centric Context. A conceptual model of
Activity-Centric context is presented. The model is illustrated via a detailed
example."
"We present a new visualization method to summarize and present periodic
population movement between distinct locations, such as floors, buildings,
cities, or the like. In the specific case of this paper, we have chosen to
focus on student movement between college dormitories on the Columbia
University campus. The visual information is presented to the information
analyst in the form of an interactive geographical map, in which specific
temporal periods as well as individual buildings can be singled out for
detailed data exploration. The navigational interface has been designed to
specifically meet a geographical setting."
"The adoption of Grid technology has the potential to greatly aid the BaBar
experiment. BdbServer was originally designed to extract copies of data from
the Objectivity/DB database at SLAC and IN2P3. With data now stored in multiple
locations in a variety of data formats, we are enhancing this tool. This will
enable users to extract selected deep copies of event collections and ship them
to the requested site using the facilities offered by the existing Grid
infrastructure. By building on the work done by various groups in BaBar, and
the European DataGrid, we have successfully expanded the capabilities of the
BdbServer software. This should provide a framework for future work in data
distribution."
"The BABAR Web site was established in 1993 at the Stanford Linear Accelerator
Center (SLAC) to support the BABAR experiment, to report its results, and to
facilitate communication among its scientific and engineering collaborators,
currently numbering about 600 individuals from 75 collaborating institutions in
10 countries. The BABAR Web site is, therefore, a community Web site. At the
same time it is hosted at SLAC and funded by agencies that demand adherence to
policies decided under different priorities. Additionally, the BABAR Web
administrators deal with the problems that arise during the course of managing
users, content, policies, standards, and changing technologies. Desired
solutions to some of these problems may be incompatible with the overall
administration of the SLAC Web sites and/or the SLAC policies and concerns.
There are thus different perspectives of the same Web site and differing
expectations in segments of the SLAC population which act as constraints and
challenges in any review or re-engineering activities. Web Engineering, which
post-dates the BABAR Web, has aimed to provide a comprehensive understanding of
all aspects of Web development. This paper reports on the first part of a
recent review of application of Web Engineering methods to the BABAR Web site,
which has led to explicit user and information models of the BABAR community
and how SLAC and the BABAR community relate and react to each other. The paper
identifies the issues of a community Web site in a hierarchical,
semi-governmental sector and formulates a strategy for periodic reviews of
BABAR and similar sites."
"WWW has a scale-free structure where novel information is often difficult to
locate. Moreover, Intelligent agents easily get trapped in this structure. Here
a novel method is put forth, which turns these traps into information
repositories, supplies: We populated an Internet environment with intelligent
news foragers. Foraging has its associated cost whereas foragers are rewarded
if they detect not yet discovered novel information. The intelligent news
foragers crawl by using the estimated long-term cumulated reward, and also have
a finite sized memory: the list of most promising supplies. Foragers form an
artificial life community: the most successful ones are allowed to multiply,
while unsuccessful ones die out. The specific property of this community is
that there is no direct communication amongst foragers but the centralized
rewarding system. Still, fast division of work is achieved."
"In this short technical report, we define on the sample space R^D a distance
between data points which depends on their correlation. We also derive an
expression for the center of mass of a set of points with respect to this
distance."
"In this lecture I discuss some aspects of MKM, Mathematical Knowledge
Management, with particuar emphasis on information storage and information
retrieval."
"The article presents an online relevancy tuning method using explicit user
feedback. The author developed and tested a method of words' weights
modification based on search result evaluation by user. User decides whether
the result is useful or not after inspecting the full result content. The
experiment proved that the constantly accumulated words weights base leads to
better search quality in a specified data domain. The author also suggested
future improvements of the method."
"In-degree, PageRank, number of visits and other measures of Web page
popularity significantly influence the ranking of search results by modern
search engines. The assumption is that popularity is closely correlated with
quality, a more elusive concept that is difficult to measure directly.
Unfortunately, the correlation between popularity and quality is very weak for
newly-created pages that have yet to receive many visits and/or in-links.
Worse, since discovery of new content is largely done by querying search
engines, and because users usually focus their attention on the top few
results, newly-created but high-quality pages are effectively ``shut out,'' and
it can take a very long time before they become popular.
  We propose a simple and elegant solution to this problem: the introduction of
a controlled amount of randomness into search result ranking methods. Doing so
offers new pages a chance to prove their worth, although clearly using too much
randomness will degrade result quality and annul any benefits achieved. Hence
there is a tradeoff between exploration to estimate the quality of new pages
and exploitation of pages already known to be of high quality. We study this
tradeoff both analytically and via simulation, in the context of an economic
objective function based on aggregate result quality amortized over time. We
show that a modest amount of randomness leads to improved search results."
"The use of citation counts to assess the impact of research articles is well
established. However, the citation impact of an article can only be measured
several years after it has been published. As research articles are
increasingly accessed through the Web, the number of times an article is
downloaded can be instantly recorded and counted. One would expect the number
of times an article is read to be related both to the number of times it is
cited and to how old the article is. This paper analyses how short-term Web
usage impact predicts medium-term citation impact. The physics e-print archive
(arXiv.org) is used to test this."
"This article is a critique of: ""The 'Green' and 'Gold' Roads to Open Access:
The Case for Mixing and Matching"" by Jean-Claude Guedon (in Serials Review
30(4) 2004).
  Open Access (OA) means: free online access to all peer-reviewed journal
articles. Jean-Claude Guedon argues against the efficacy of author
self-archiving of peer-reviewed journal articles (the ""Green"" road to OA). He
suggests instead that we should convert to Open Access Publishing (the ""Golden""
road to OA) by ""mixing and matching"" Green and Gold as follows: o First,
self-archive dissertations (not published, peer-reviewed journal articles). o
Second, identify and tag how those dissertations have been evaluated and
reviewed. o Third, self-archive unrefereed preprints (not published,
peer-reviewed journal articles). o Fourth, develop new mechanisms for
evaluating and reviewing those unrefereed preprints, at multiple levels. The
result will be OA Publishing (Gold). I argue that rather than yet another 10
years of speculation like this, what is actually needed (and imminent) is for
OA self-archiving to be mandated by research funders and institutions so that
the self-archiving of published, peer-reviewed journal articles (Green) can be
fast-forwarded to 100% OA."
"In this paper we present a number of measures that compare rankings of search
engine results. We apply these measures to five queries that were monitored
daily for two periods of about 21 days each. Rankings of the different search
engines (Google, Yahoo and Teoma for text searches and Google, Yahoo and
Picsearch for image searches) are compared on a daily basis, in addition to
longitudinal comparisons of the same engine for the same query over time. The
results and rankings of the two periods are compared as well."
"This paper presents an original methodology to consider question answering.
We noticed that query expansion is often incorrect because of a bad
understanding of the question. But the automatic good understanding of an
utterance is linked to the context length, and the question are often short.
This methodology proposes to analyse the documents and to construct an
informative structure from the results of the analysis and from a semantic text
expansion. The linguistic analysis identifies words (tokenization and
morphological analysis), links between words (syntactic analysis) and word
sense (semantic disambiguation). The text expansion adds to each word the
synonyms matching its sense and replaces the words in the utterances by
derivatives, modifying the syntactic schema if necessary. In this way, whatever
enrichment may be, the text keeps the same meaning, but each piece of
information matches many realisations. The questioning method consists in
constructing a local informative structure without enrichment, and matches it
with the documentary structure. If a sentence in the informative structure
matches the question structure, this sentence is the answer to the question."
"External linguistic resources have been used for a very long time in
information extraction. These methods enrich a document with data that are
semantically equivalent, in order to improve recall. For instance, some of
these methods use synonym dictionaries. These dictionaries enrich a sentence
with words that have a similar meaning. However, these methods present some
serious drawbacks, since words are usually synonyms only in restricted
contexts. The method we propose here consists of using word sense
disambiguation rules (WSD) to restrict the selection of synonyms to only these
that match a specific syntactico-semantic context. We show how WSD rules are
built and how information extraction techniques can benefit from the
application of these rules."
"This paper presents some experiments in clustering homogeneous XMLdocuments
to validate an existing classification or more generally anorganisational
structure. Our approach integrates techniques for extracting knowledge from
documents with unsupervised classification (clustering) of documents. We focus
on the feature selection used for representing documents and its impact on the
emerging classification. We mix the selection of structured features with fine
textual selection based on syntactic characteristics.We illustrate and evaluate
this approach with a collection of Inria activity reports for the year 2003.
The objective is to cluster projects into larger groups (Themes), based on the
keywords or different chapters of these activity reports. We then compare the
results of clustering using different feature selections, with the official
theme structure used by Inria."
"The main aspects of XML retrieval are identified by analysing and comparing
the following two behaviours: the behaviour of the assessor when judging the
relevance of returned document components; and the behaviour of users when
interacting with components of XML documents. We argue that the two INEX
relevance dimensions, Exhaustivity and Specificity, are not orthogonal
dimensions; indeed, an empirical analysis of each dimension reveals that the
grades of the two dimensions are correlated to each other. By analysing the
level of agreement between the assessor and the users, we aim at identifying
the best units of retrieval. The results of our analysis show that the highest
level of agreement is on highly relevant and on non-relevant document
components, suggesting that only the end points of the INEX 10-point relevance
scale are perceived in the same way by both the assessor and the users. We
propose a new definition of relevance for XML retrieval and argue that its
corresponding relevance scale would be a better choice for INEX."
"This paper investigates the impact of three approaches to XML retrieval:
using Zettair, a full-text information retrieval system; using eXist, a native
XML database; and using a hybrid system that takes full article answers from
Zettair and uses eXist to extract elements from those articles. For the
content-only topics, we undertake a preliminary analysis of the INEX 2003
relevance assessments in order to identify the types of highly relevant
document components. Further analysis identifies two complementary sub-cases of
relevance assessments (""General"" and ""Specific"") and two categories of topics
(""Broad"" and ""Narrow""). We develop a novel retrieval module that for a
content-only topic utilises the information from the resulting answer list of a
native XML database and dynamically determines the preferable units of
retrieval, which we call ""Coherent Retrieval Elements"". The results of our
experiments show that -- when each of the three systems is evaluated against
different retrieval scenarios (such as different cases of relevance
assessments, different topic categories and different choices of evaluation
metrics) -- the XML retrieval systems exhibit varying behaviour and the best
performance can be reached for different values of the retrieval parameters. In
the case of INEX 2003 relevance assessments for the content-only topics, our
newly developed hybrid XML retrieval system is substantially more effective
than either Zettair or eXist, and yields a robust and a very effective XML
retrieval."
"Three approaches to content-and-structure XML retrieval are analysed in this
paper: first by using Zettair, a full-text information retrieval system; second
by using eXist, a native XML database, and third by using a hybrid XML
retrieval system that uses eXist to produce the final answers from likely
relevant articles retrieved by Zettair. INEX 2003 content-and-structure topics
can be classified in two categories: the first retrieving full articles as
final answers, and the second retrieving more specific elements within articles
as final answers. We show that for both topic categories our initial hybrid
system improves the retrieval effectiveness of a native XML database. For
ranking the final answer elements, we propose and evaluate a novel retrieval
model that utilises the structural relationships between the answer elements of
a native XML database and retrieves Coherent Retrieval Elements. The final
results of our experiments show that when the XML retrieval task focusses on
highly relevant elements our hybrid XML retrieval system with the Coherent
Retrieval Elements module is 1.8 times more effective than Zettair and 3 times
more effective than eXist, and yields an effective content-and-structure XML
retrieval."
"This paper presents some experiments in clustering homogeneous XMLdocuments
to validate an existing classification or more generally anorganisational
structure. Our approach integrates techniques for extracting knowledge from
documents with unsupervised classification (clustering) of documents. We focus
on the feature selection used for representing documents and its impact on the
emerging classification. We mix the selection of structured features with fine
textual selection based on syntactic characteristics.We illustrate and evaluate
this approach with a collection of Inria activity reports for the year 2003.
The objective is to cluster projects into larger groups (Themes), based on the
keywords or different chapters of these activity reports. We then compare the
results of clustering using different feature selections, with the official
theme structure used by Inria."
"In this paper, we present an algorithm for automatically building expertise
evidence for finding experts within an organization by combining structured
corporate information with different content. We also describe our test data
collection and our evaluation method. Evaluation of the algorithm shows that
using organizational structure leads to a significant improvement in the
precision of finding an expert. Furthermore we evaluate the impact of using
different data sources on the quality of the results and conclude that Expert
Finding is not a ""one engine fits all"" solution. It requires an analysis of the
information space into which a solution will be placed and the appropriate
selection and weighting scheme of the data sources."
"As Web sites are now ordinary products, it is necessary to explicit the
notion of quality of a Web site. The quality of a site may be linked to the
easiness of accessibility and also to other criteria such as the fact that the
site is up to date and coherent. This last quality is difficult to insure
because sites may be updated very frequently, may have many authors, may be
partially generated and in this context proof-reading is very difficult. The
same piece of information may be found in different occurrences, but also in
data or meta-data, leading to the need for consistency checking. In this paper
we make a parallel between programs and Web sites. We present some examples of
semantic constraints that one would like to specify (constraints between the
meaning of categories and sub-categories in a thematic directory, consistency
between the organization chart and the rest of the site in an academic site).
We present quickly the Natural Semantics, a way to specify the semantics of
programming languages that inspires our works. Then we propose a specification
language for semantic constraints in Web sites that, in conjunction with the
well known ``make'' program, permits to generate some site verification tools
by compiling the specification into Prolog code. We apply our method to a large
XML document which is the scientific part of our institute activity report,
tracking errors or inconsistencies and also constructing some indicators that
can be used by the management of the institute."
"A growing category of vehicle-infrastructure cooperative (VIC) applications
requires telematics software components distributed between an
infrastructure-based management center and a number of vehicles. This article
presents an approach based on a software framework, focusing on a Telematic
Management System (TMS), a component suite aimed to run inside an
infrastructure-based operations center, in some cases interacting with legacy
systems like Advanced Traffic Management Systems or Vehicle Relationship
Management. The TMS framework provides support for modular, flexible,
prototyping and implementation of VIC applications. This work has received the
support of the European Commission in the context of the projects REACT and
CyberCars."
"This paper presents a novel analysis and visualization of English Wikipedia
data. Our specific interest is the analysis of basic statistics, the
identification of the semantic structure and age of the categories in this free
online encyclopedia, and the content coverage of its highly productive authors.
The paper starts with an introduction of Wikipedia and a review of related
work. We then introduce a suite of measures and approaches to analyze and map
the semantic structure of Wikipedia. The results show that co-occurrences of
categories within individual articles have a power-law distribution, and when
mapped reveal the nicely clustered semantic structure of Wikipedia. The results
also reveal the content coverage of the article's authors, although the roles
these authors play are as varied as the authors themselves. We conclude with a
discussion of major results and planned future work."
"This paper introduces Google Web APIs (Google APIs) as an instrument and
playground for webometric studies. Several examples of Google APIs
implementations are given. Our examples show that this Google Web Service can
be used successfully for informetric Internet based studies albeit with some
restrictions."
"The continuous improvement in TQM is considered as the core value by which
organisation could maintain a competitive edge. Several techniques and tools
are known to support this core value but most of the time these techniques are
informal and without modelling the interdependence between the core value and
tools. Thus, technique formalisation is one of TQM challenges for increasing
efficiency of quality process implementation. In that way, the paper proposes
and experiments an advanced quality modelling approach based on meta-modelling
the ""process approach"" as advocated by the standard ISO9000:2000. This
meta-model allows formalising the interdependence between technique, tools and
core value"
"This paper reports on the INRIA group's approach to XML mining while
participating in the INEX XML Mining track 2005. We use a flexible
representation of XML documents that allows taking into account the structure
only or both the structure and content. Our approach consists of representing
XML documents by a set of their sub-paths, defined according to some criteria
(length, root beginning, leaf ending). By considering those sub-paths as words,
we can use standard methods for vocabulary reduction, and simple clustering
methods such as K-means that scale well. We actually use an implementation of
the clustering algorithm known as ""dynamic clouds"" that can work with distinct
groups of independent variables put in separate variables. This is useful in
our model since embedded sub-paths are not independent: we split potentially
dependant paths into separate variables, resulting in each of them containing
independant paths. Experiments with the INEX collections show good results for
the structure-only collections, but our approach could not scale well for large
structure-and-content collections."
"Latent Semantic Analysis (LSA) is a well known method for information
retrieval. It has also been applied as a model of cognitive processing and
word-meaning acquisition. This dual importance of LSA derives from its capacity
to modulate the meaning of words by contexts, dealing successfully with
polysemy and synonymy. The underlying reasons that make the method work are not
clear enough. We propose that the method works because it detects an underlying
block structure (the blocks corresponding to topics) in the term by document
matrix. In real cases this block structure is hidden because of perturbations.
We propose that the correct explanation for LSA must be searched in the
structure of singular vectors rather than in the profile of singular values.
Using Perron-Frobenius theory we show that the presence of disjoint blocks of
documents is marked by sign-homogeneous entries in the vectors corresponding to
the documents of one block and zeros elsewhere. In the case of nearly disjoint
blocks, perturbation theory shows that if the perturbations are small the zeros
in the leading vectors are replaced by small numbers (pseudo-zeros). Since the
singular values of each block might be very different in magnitude, their order
does not mirror the order of blocks. When the norms of the blocks are similar,
LSA works fine, but we propose that when the topics have different sizes, the
usual procedure of selecting the first k singular triplets (k being the number
of blocks) should be replaced by a method that selects the perturbed Perron
vectors for each block."
"This paper takes a look at the general characteristics of business or
economic intelligence system. The role of the user within this type of system
is emphasized. We propose two models which we consider important in order to
adapt this system to the user. The first model is based on the definition of
decisional problem and the second on the four cognitive phases of human
learning. We also describe the application domain we are using to test these
models in this type of system."
"We describe a new wavelet transform, for use on hierarchies or binary rooted
trees. The theoretical framework of this approach to data analysis is
described. Case studies are used to further exemplify this approach. A first
set of application studies deals with data array smoothing, or filtering. A
second set of application studies relates to hierarchical tree condensation.
Finally, a third study explores the wavelet decomposition, and the
reproducibility of data sets such as text, including a new perspective on the
generation or computability of such data objects."
"The methodology of context-sensitive access to e-documents considers context
as a problem model based on the knowledge extracted from the application
domain, and presented in the form of application ontology. Efficient access to
an information in the text form is needed. Wiki resources as a modern text
format provides huge number of text in a semi formalized structure. At the
first stage of the methodology, documents are indexed against the ontology
representing macro-situation. The indexing method uses a topic tree as a middle
layer between documents and the application ontology. At the second stage
documents relevant to the current situation (the abstract and operational
contexts) are identified and sorted by degree of relevance. Abstract context is
a problem-oriented ontology-based model. Operational context is an
instantiation of the abstract context with data provided by the information
sources. The following parts of the methodology are described: (i) metrics for
measuring similarity of e-documents to ontology, (ii) a document index storing
results of indexing of e-documents against the ontology; (iii) a method for
identification of relevant e-documents based on semantic similarity measures.
Wikipedia (wiki resource) is used as a corpus of e-documents for approach
evaluation in a case study. Text categorization, the presence of metadata, and
an existence of a lot of articles related to different topics characterize the
corpus."
"Information on any given topic is often scattered across the web. Previously
this scatter has been characterized through the distribution of a set of facts
(i.e. pieces of information) across web pages, showing that typically a few
pages contain many facts on the topic, while many pages contain just a few.
While such approaches have revealed important scatter phenomena, they are lossy
in that they conceal how specific facts (e.g. rare facts) occur in specific
types of pages (e.g. fact-rich pages). To reveal such regularities, we
construct bi-partite networks, consisting of two types of vertices: the facts
contained in webpages and the webpages themselves. Such a representation
enables the application of a series of network analysis techniques, revealing
structural features such as connectivity, robustness, and clustering. We
discuss the implications of each of these features to the users' ability to
find comprehensive information online. Finally, we compare the bipartite graph
structure of webpages and facts with the hyperlink structure between the
webpages."
"The present work falls in the line of activities promoted by the European
Languguage Resource Association (ELRA) Production Committee (PCom) and raises
issues in methods, procedures and tools for the reusability, creation, and
management of Language Resources. A two-fold purpose lies behind this
experiment. The first aim is to investigate the feasibility, define methods and
procedures for combining two Italian lexical resources that have incompatible
formats and complementary information into a Unified Lexicon (UL). The adopted
strategy and the procedures appointed are described together with the driving
criterion of the merging task, where a balance between human and computational
efforts is pursued. The coverage of the UL has been maximized, by making use of
simple and fast matching procedures. The second aim is to exploit this newly
obtained resource for implementing the phonological and morphological layers of
the CLIPS lexical database. Implementing these new layers and linking them with
the already exisitng syntactic and semantic layers is not a trivial task. The
constraints imposed by the model, the impact at the architectural level and the
solution adopted in order to make the whole database `speak' efficiently are
presented. Advantages vs. disadvantages are discussed."
"This paper gives an overview of current trends in manual indexing on the Web.
Along with a general rise of user generated content there are more and more
tagging systems that allow users to annotate digital resources with tags
(keywords) and share their annotations with other users. Tagging is frequently
seen in contrast to traditional knowledge organization systems or as something
completely new. This paper shows that tagging should better be seen as a
popular form of manual indexing on the Web. Difference between controlled and
free indexing blurs with sufficient feedback mechanisms. A revised typology of
tagging systems is presented that includes different user roles and knowledge
organization systems with hierarchical relationships and vocabulary control. A
detailed bibliography of current research in collaborative tagging is included."
"We study the notion of hierarchy in the context of visualizing textual data
and navigating text collections. A formal framework for ``hierarchy'' is given
by an ultrametric topology. This provides us with a theoretical foundation for
concept hierarchy creation. A major objective is {\em scalable} annotation or
labeling of concept maps. Serendipitously we pursue other objectives such as
deriving common word pair (and triplet) phrases, i.e., word 2- and 3-grams. We
evaluate our approach using (i) a collection of texts, (ii) a single text
subdivided into successive parts (for which we provide an interactive
demonstrator), and (iii) a text subdivided at the sentence or line level. While
detailing a generic framework, a distinguishing feature of our work is that we
focus on {\em locality} of hierarchic structure in order to extract semantic
information."
"We consider the wavelet transform of a finite, rooted, node-ranked, $p$-way
tree, focusing on the case of binary ($p = 2$) trees. We study a Haar wavelet
transform on this tree. Wavelet transforms allow for multiresolution analysis
through translation and dilation of a wavelet function. We explore how this
works in our tree context."
"Wikis can be considered as public domain knowledge sharing system. They
provide opportunity for those who may not have the privilege to publish their
thoughts through the traditional methods. They are one of the fastest growing
systems of online encyclopaedia. In this study, we consider the importance of
wikis as a way of creating, sharing and improving public knowledge. We identify
some of the problems associated with wikis to include, (a) identification of
the identities of information and its creator (b) accuracy of information (c)
justification of the credibility of authors (d) vandalism of quality of
information (e) weak control over the contents. A solution to some of these
problems is sought through the use of an annotation model. The model assumes
that contributions in wikis can be seen as annotation to the initial document.
It proposed a systematic control of contributors and contributions to the
initiative and the keeping of records of what existed and what was done to
initial documents. We believe that with this model, analysis can be done on the
progress of wiki initiatives. We assumed that using this model, wikis can be
better used for creation and sharing of knowledge for public use."
"The primary objective of document annotation in whatever form, manual or
electronic is to allow those who may not have control to original document to
provide personal view on information source. Beyond providing personal
assessment to original information sources, we are looking at a situation where
annotation made can be used as additional source of information for document
tracking and recommendation service. Most of the annotation tools existing
today were conceived for their independent use with no reference to the creator
of the annotation. We propose AMIEDoT (Annotation Model for Information
Exchange and Document Tracking) an annotation model that can assist in document
tracking and recommendation service. The model is based on three parameters in
the acts of annotation. We believe that introducing document parameters, time
and the parameters of the creator of annotation into an annotation process can
be a dependable source to know, who used a document, when a document was used
and for what a document was used for. Beyond document tracking, our model can
be used in not only for selective dissemination of information but for
recommendation services. AMIEDoT can also be used for information sharing and
information reuse."
"The objective of most users for consulting any information database,
information warehouse or the internet is to resolve one problem or the other.
Available online or offline annotation tools were not conceived with the
objective of assisting users in their bid to resolve a decisional problem.
Apart from the objective and usage of annotation tools, how these tools are
conceived and classified has implication on their usage. Several criteria have
been used to categorize annotation concepts. Typically annotation are conceived
based on how it affect the organization of document been considered for
annotation or the organization of the resulting annotation. Our approach is
annotation that will assist in information research for decision making.
Annotation model for information exchange (AMIE) was conceived with the
objective of information sharing and reuse."
"In a way similar to the string-to-string correction problem we address time
series similarity in the light of a time-series-to-time-series-correction
problem for which the similarity between two time series is measured as the
minimum cost sequence of ""edit operations"" needed to transform one time series
into another. To define the ""edit operations"" we use the paradigm of a
graphical editing process and end up with a dynamic programming algorithm that
we call Time Warp Edit Distance (TWED). TWED is slightly different in form from
Dynamic Time Warping, Longest Common Subsequence or Edit Distance with Real
Penalty algorithms. In particular, it highlights a parameter which drives a
kind of stiffness of the elastic measure along the time axis. We show that the
similarity provided by TWED is a metric potentially useful in time series
retrieval applications since it could benefit from the triangular inequality
property to speed up the retrieval process while tuning the parameters of the
elastic measure. In that context, a lower bound is derived to relate the
matching of time series into down sampled representation spaces to the matching
into the original space. Empiric quality of the TWED distance is evaluated on a
simple classification task. Compared to Edit Distance, Dynamic Time Warping,
Longest Common Subsequnce and Edit Distance with Real Penalty, TWED has proven
to be quite effective on the considered experimental task."
"This paper presents the principles of ontology-supported and ontology-driven
conceptual navigation. Conceptual navigation realizes the independence between
resources and links to facilitate interoperability and reusability. An engine
builds dynamic links, assembles resources under an argumentative scheme and
allows optimization with a possible constraint, such as the user's available
time. Among several strategies, two are discussed in detail with examples of
applications. On the one hand, conceptual specifications for linking and
assembling are embedded in the resource meta-description with the support of
the ontology of the domain to facilitate meta-communication. Resources are like
agents looking for conceptual acquaintances with intention. On the other hand,
the domain ontology and an argumentative ontology drive the linking and
assembling strategies."
"The goal of our work is to use a set of reports and extract named entities,
in our case the names of Industrial or Academic partners. Starting with an
initial list of entities, we use a first set of documents to identify syntactic
patterns that are then validated in a supervised learning phase on a set of
annotated documents. The complete collection is then explored. This approach is
similar to the ones used in data extraction from semi-structured documents
(wrappers) and do not need any linguistic resources neither a large set for
training. As our collection of documents would evolve over years, we hope that
the performance of the extraction would improve with the increased size of the
training set."
"The topological structures of the Internet and the Web have received
considerable attention. However, there has been little research on the
topological properties of individual web sites. In this paper, we consider
whether web sites (as opposed to the entire Web) exhibit structural
similarities. To do so, we exhaustively crawled 18 web sites as diverse as
governmental departments, commercial companies and university departments in
different countries. These web sites consisted of as little as a few thousand
pages to millions of pages. Statistical analysis of these 18 sites revealed
that the internal link structure of the web sites are significantly different
when measured with first and second-order topological properties, i.e.
properties based on the connectivity of an individual or a pairs of nodes.
However, examination of a third-order topological property that consider the
connectivity between three nodes that form a triangle, revealed a strong
correspondence across web sites, suggestive of an invariant. Comparison with
the Web, the AS Internet, and a citation network, showed that this third-order
property is not shared across other types of networks. Nor is the property
exhibited in generative network models such as that of Barabasi and Albert."
"The exponential growth of multimedia information and the development of
various communication media generated new problems at various levels including
the rate of flow of information, problems of storage and management. The
difficulty which arises is no longer the existence of information but rather
the access to this information. When designing multimedia information retrieval
system, it is appropriate to bear in mind the potential users and their
information needs. We assumed that multimedia information representation which
takes into account explicitly the users' needs and the cases of use could
contribute to the adaptation potentials of the system for the end-users. We
believe also that responses of multimedia information system would be more
relevant to the users' needs if the types of results to be used from the system
were identified before the design and development of the system. We propose the
integration of the users' information needs. More precisely integrating usage
contexts of resulting information in an information system (during creation and
feedback) should enhance more pertinent users' need. The first section of this
study is dedicated to traditional multimedia information systems and
specifically the approaches of representing multimedia information. Taking into
account the dynamism of users, these approaches do not permit the explicit
integration of the users' information needs. In this paper, we will present our
proposals based on economic intelligence approach. This approach emphasizes the
importance of starting any process of information retrieval witch the user
information need."
This note argues about the validity of web-graph data used in the literature.
"In the first design stage, image reference plays a double role of means of
formulation and resolution of problems. In our approach, we consider image
reference as a support of creation activity to generate ideas and we propose a
tool for navigation in references by image in order to assist daylight ambience
design. Within this paper, we present, in a first part, the semantic indexation
method to be used for the indexation of our image database. In a second part we
propose a synthetic analysis of various modes of referential navigation in
order to propose a tool implementing all or a part of these modes."
"Wikipedia is a useful source of knowledge that has many applications in
language processing and knowledge representation. The Wikipedia category graph
can be compared with the class hierarchy in an ontology; it has some
characteristics in common as well as some differences. In this paper, we
present our approach for answering entity ranking queries from the Wikipedia.
In particular, we explore how to make use of Wikipedia categories to improve
entity ranking effectiveness. Our experiments show that using categories of
example entities works significantly better than using loosely defined target
categories."
"The traditional entity extraction problem lies in the ability of extracting
named entities from plain text using natural language processing techniques and
intensive training from large document collections. Examples of named entities
include organisations, people, locations, or dates. There are many research
activities involving named entities; we are interested in entity ranking in the
field of information retrieval. In this paper, we describe our approach to
identifying and ranking entities from the INEX Wikipedia document collection.
Wikipedia offers a number of interesting features for entity identification and
ranking that we first introduce. We then describe the principles and the
architecture of our entity ranking system, and introduce our methodology for
evaluation. Our preliminary results show that the use of categories and the
link structure of Wikipedia, together with entity examples, can significantly
improve retrieval effectiveness."
"The paper introduces a novel iterative method that assigns a reputation to n
+ m items: n raters and m objects. Each rater evaluates a subset of objects
leading to a n x m rating matrix with a certain sparsity pattern. From this
rating matrix we give a nonlinear formula to define the reputation of raters
and objects. We also provide an iterative algorithm that superlinearly
converges to the unique vector of reputations and this for any rating matrix.
In contrast to classical outliers detection, no evaluation is discarded in this
method but each one is taken into account with different weights for the
reputation of the objects. The complexity of one iteration step is linear in
the number of evaluations, making our algorithm efficient for large data set.
Experiments show good robustness of the reputation of the objects against
cheaters and spammers and good detection properties of cheaters and spammers."
"This paper shows how authority files can be encoded for the Semantic Web with
the Simple Knowledge Organisation System (SKOS). In particular the application
of SKOS for encoding the structure, management, and utilization of country
codes as defined in ISO 3166 is demonstrated. The proposed encoding gives a use
case for SKOS that includes features that have only been discussed little so
far, such as multiple notations, nested concept schemes, changes by versioning."
"This technical report details a family of time warp distances on the set of
discrete time series. This family is constructed as an editing distance whose
elementary operations apply on linear segments. A specific parameter allows
controlling the stiffness of the elastic matching. It is well suited for the
processing of event data for which each data sample is associated with a
timestamp, not necessarily obtained according to a constant sampling rate. Some
properties verified by these distances are proposed and proved in this report."
"With the World Wide Web's ubiquity increase and the rapid development of
various online businesses, the complexity of web sites grow. The analysis of
web user's navigational pattern within a web site can provide useful
information for server performance enhancements, restructuring a website and
direct marketing in e-commerce etc. In this paper, an algorithm is proposed for
mining such navigation patterns. The key insight is that users access
information of interest and follow a certain path while navigating a web site.
If they don't find it, they would backtrack and choose among the alternate
paths till they reach the destination. The point they backtrack is the
Intermediate Reference Location. Identifying such Intermediate locations and
destinations out of the pattern will be the main endeavor in the rest of this
report."
"Engineering a Web search engine offering effective and efficient information
retrieval is a challenging task. This document presents our experiences from
designing and developing a Web search engine offering a wide spectrum of
functionalities and we report some interesting experimental results. A rather
peculiar design choice of the engine is that its index is based on a DBMS,
while some of the distinctive functionalities that are offered include advanced
Greek language stemming, real time result clustering, and advanced link
analysis techniques (also for spam page detection)."
"Multimedia uploaded content is tagged and recommended by users of
collaborative systems, resulting in informal classifications also known as
folksonomies. Faceted web ranking has been proved a reasonable alternative to a
single ranking which does not take into account a personalized context. In this
paper we analyze the online computation of rankings of users associated to
facets made up of multiple tags. Possible applications are user reputation
evaluation (ego-ranking) and improvement of content quality in case of
retrieval. We propose a solution based on PageRank as centrality measure: (i) a
ranking for each tag is computed offline on the basis of the corresponding
tag-dependent subgraph; (ii) a faceted order is generated by merging rankings
corresponding to all the tags in the facet. The fundamental assumption,
validated by empirical observations, is that step (i) is scalable. We also
present algorithms for part (ii) having time complexity O(k), where k is the
number of tags in the facet, well suited to online computation."
"The Visual Object Information Retrieval (VOIR) system described in this paper
implements an image retrieval approach that combines two layers, the conceptual
and the visual layer. It uses terms from a textual thesaurus to represent the
conceptual information and also works with image regions, the visual
information. The terms are related with the image regions through a weighted
association enabling the execution of concept-level queries. VOIR uses
region-based relevance feedback to improve the quality of the results in each
query session and to discover new associations between text and image. This
paper describes a user-centred and task-oriented comparative evaluation of VOIR
which was undertaken considering three distinct versions of VOIR: a full-fledge
version; one supporting relevance feedback only at image level; and a third
version not supporting relevance feedback at all. The evaluation performed
showed the usefulness of region based relevance feedback in the context of VOIR
prototype."
"The explosive rate of information growth and availability often makes it
increasingly difficult to locate information pertinent to your needs. These
problems are often compounded when keyword based search methodologies are not
adequate for describing the information you seek. In many instances,
information such as Web site URLs, phone numbers, etc. can often be better
identified through the use of a textual pattern than by keyword. For example,
many more phone numbers could be picked up by a search for the pattern (XXX)
XXX-XXXX, where X could be any digit, than would be by a search for any
specific phone number (i.e. the keyword approach). Programming languages
typically allow for the matching of textual patterns via the usage of regular
expressions. This tutorial will provide an introduction to the basics of
programming regular expressions as well as provide an introduction to how
regular expressions can be applied to data processing tasks such as information
extraction and search refinement."
"We developed a system, TermWatch
(https://stid-bdd.iut.univ-metz.fr/TermWatch/index.pl), which combines a
linguistic extraction of terms, their structuring into a terminological network
with a clustering algorithm. In this paper we explore its ability in
integrating the most promising aspects of the studies on query refinement:
choice of meaningful text units to cluster (domain terms), choice of tight
semantic relations with which to cluster terms, structuring of terms in a
network enabling abetter perception of domain concepts. We have run this
experiment on the 367 645 English abstracts of PASCAL 2005-2006 bibliographic
database (http://www.inist.fr) and compared the structured terminological
resource automatically build by TermWarch to the English segment of TermScience
resource (http://termsciences.inist.fr/) containing 88 211 terms."
"The aim of this paper is the supervised classification of semi-structured
data. A formal model based on bayesian classification is developed while
addressing the integration of the document structure into classification tasks.
We define what we call the structural context of occurrence for unstructured
data, and we derive a recursive formulation in which parameters are used to
weight the contribution of structural element relatively to the others. A
simplified version of this formal model is implemented to carry out textual
documents classification experiments. First results show, for a adhoc weighting
strategy, that the structural context of word occurrences has a significant
impact on classification results comparing to the performance of a simple
multinomial naive Bayes classifier. The proposed implementation competes on the
Reuters-21578 data with the SVM classifier associated or not with the splitting
of structural components. These results encourage exploring the learning of
acceptable weighting strategies for this model, in particular boosting
strategies."
"We present a new method to accelerate the HITS algorithm by exploiting
hyperlink structure of the web graph. The proposed algorithm extends the idea
of authority and hub scores from HITS by introducing two diagonal matrices
which contain constants that act as weights to make authority pages more
authoritative and hub pages more hubby. This method works because in the web
graph good authorities are pointed to by good hubs and good hubs point to good
authorities. Consequently, these pages will collect their scores faster under
the proposed algorithm than under the standard HITS. We show that the authority
and hub vectors of the proposed algorithm exist but are not necessarily be
unique, and then give a treatment to ensure the uniqueness property of the
vectors. The experimental results show that the proposed algorithm can improve
HITS computations, especially for back button datasets."
"This paper describes a clustering method to group the most similar and
important weblogs with their descriptive shared words by using a technique from
multilinear algebra known as PARAFAC tensor decomposition. The proposed method
first creates labeled-link network representation of the weblog datasets, where
the nodes are the blogs and the labels are the shared words. Then, 3-way
adjacency tensor is extracted from the network and the PARAFAC decomposition is
applied to the tensor to get pairs of node lists and label lists with scores
attached to each list as the indication of the degree of importance. The
clustering is done by sorting the lists in decreasing order and taking the
pairs of top ranked blogs and words. Thus, unlike standard co-clustering
methods, this method not only groups the similar blogs with their descriptive
words but also tends to produce clusters of important blogs and descriptive
words."
"Nowadays people realize that it is difficult to find information simply and
quickly on the bulletin boards. In order to solve this problem, people propose
the concept of bulletin board search engine. This paper describes the
priscrawler system, a subsystem of the bulletin board search engine, which can
automatically crawl and add the relevance to the classified attachments of the
bulletin board. Priscrawler utilizes Attachrank algorithm to generate the
relevance between webpages and attachments and then turns bulletin board into
clear classified and associated databases, making the search for attachments
greatly simplified. Moreover, it can effectively reduce the complexity of
pretreatment subsystem and retrieval subsystem and improve the search
precision. We provide experimental results to demonstrate the efficacy of the
priscrawler."
"In this paper, we study the problems of video processing in video search
engine. Video has now become a very important kind of data in Internet; while
searching for video is still a challenging task due to the inner properties of
video: requiring enormous storage space, being independent, expressing
information hiddenly. To handle the properties of video more effectively, in
this paper, we propose a new video processing method in video search engine. In
detail, the core of the new video processing method is creating pavideoge--a
new data type, which contains the video advantages and webpage advantages. The
pavideoge has four attributes: real link, videorank, text information and
playnum. Each of them combines video's properties with webpage's. Video search
engine based on the pavideoge can retrieve video more effectively. The
experiment results show the encouraging performance of our approach. Based on
the pavideoge, our video search engine can retrieve more precise videos in
comparsion with previous related work."
"We describe the Universal Recommender, a recommender system for semantic
datasets that generalizes domain-specific recommenders such as content-based,
collaborative, social, bibliographic, lexicographic, hybrid and other
recommenders. In contrast to existing recommender systems, the Universal
Recommender applies to any dataset that allows a semantic representation. We
describe the scalable three-stage architecture of the Universal Recommender and
its application to Internet Protocol Television (IPTV). To achieve good
recommendation accuracy, several novel machine learning and optimization
problems are identified. We finally give a brief argument supporting the need
for machine learning recommenders."
"A content-based network representation of web logs (blogs) using a basic
word-overlap similarity measure is presented. Due to a strong signal in blog
data the approach is sufficient for accurately classifying blogs. Using Swedish
blog data we demonstrate that blogs that treat similar subjects are organized
in clusters that, in turn, are hierarchically organized in higher-order
clusters. The simplicity of the representation renders it both computationally
tractable and transparent. We therefore argue that the approach is suitable as
a baseline when developing and analyzing more advanced content-based
representations of the blogosphere."
"The WWW is the most important source of information. But, there is no
guarantee for information correctness and lots of conflicting information is
retrieved by the search engines and the quality of provided information also
varies from low quality to high quality. We provide enhanced trustworthiness in
both specific (entity) and broad (content) queries in web searching. The
filtering of trustworthiness is based on 5 factors: Provenance, Authority, Age,
Popularity, and Related Links. The trustworthiness is calculated based on these
5 factors and it is stored thereby increasing the performance in retrieving
trustworthy websites. The calculated trustworthiness is stored only for static
websites. Quality is provided based on policies selected by the user. Quality
based ranking of retrieved trusted information is provided using WIQA (Web
Information Quality Assessment) Framework."
"This document describes the BM25 and BM25F implementation using the Lucene
Java Framework. Both models have stood out at TREC by their performance and are
considered as state-of-the-art in the IR community. BM25 is applied to
retrieval on plain text documents, that is for documents that do not contain
fields, while BM25F is applied to documents with structure."
"In this paper, we explain social information retrieval (SIR) and
collaborative information retrieval (CIR). We see SIR as a way of knowing who
to collaborate with in resolving an information problem while CIR entails the
process of mutual understanding and solving of an information problem among
collaborators. We are interested in the transition from SIR to CIR hence we
developed a communication model to facilitate knowledge sharing during CIR."
"This paper discusses a research on web document analysis for companies listed
on Bursa Malaysia which is the forerunner of financial and investment center in
Malaysia. Data set used in this research are from the company web documents
listed in the Main Board and Second Board on Bursa Malaysia. This research has
used the Web Resources Extraction System which was developed by the research
group mainly to extract information for the web documents involved. Our
research findings have shown that the level of website usage among the
companies on Bursa Malaysia is still minimal. Furthermore, research has also
found that 60.02 percent of the image files are utilized making it the most
used type of file in creating websites."
"Indexing learning documents using the Learning Object Metadata (LOM) is often
carried out manually by archivists. Filling out the LOM fields is a long and
difficult task, requiring a complete reading and a full knowledge on the topic
dealt within the document. In this paper, we present an innovative model and
method to assist the archivists in finding the important concepts and keywords
of a learning document. The application is performed using wikipedia's category
links."
"Document indexation is an essential task achieved by archivists or automatic
indexing tools. To retrieve relevant documents to a query, keywords describing
this document have to be carefully chosen. Archivists have to find out the
right topic of a document before starting to extract the keywords. For an
archivist indexing specialized documents, experience plays an important role.
But indexing documents on different topics is much harder. This article
proposes an innovative method for an indexing support system. This system takes
as input an ontology and a plain text document and provides as output
contextualized keywords of the document. The method has been evaluated by
exploiting Wikipedia's category links as a termino-ontological resources."
"Web blog is used as a collaborative platform to publish and share
information. The information accumulated in the blog intrinsically contains the
knowledge. The knowledge shared by the community of people has intangible value
proposition. The blog is viewed as a multimedia information resource available
on the Internet. In a blog, information in the form of text, image, audio and
video builds up exponentially. The multimedia information contained in an Atom
blog does not have the capability, which is required by the software processes
so that Atom blog content can be accessed, processed and reused over the
Internet. This shortcoming is addressed by exploring OWL knowledge modeling,
semantic annotation and semantic categorization techniques in an Atom blog
sphere. By adopting these techniques, futuristic Atom blogs can be created and
deployed over the Internet."
"Within the documentary system domain, the integration of thesauri for
indexing and retrieval information steps is usual. In libraries, documents own
rich descriptive information made by librarians, under descriptive notice based
on Rameau thesaurus. We exploit two kinds of information in order to create a
first semantic structure. A step of conceptualization allows us to define the
various modules used to automatically build the semantic structure of the
indexation work. Our current work focuses on an approach that aims to define an
ontology based on a thesaurus. We hope to integrate new knowledge
characterizing the territory of our structure (adding ""toponyms"" and links
between concepts) thanks to a geographic information system (GIS)."
"Automatic construction of ontologies from text is generally based on
retrieving text content. For a much more rich ontology we extend these
approaches by taking into account the document structure and some external
resources (like thesaurus of indexing terms of near domain). In this paper we
describe how these external resources are at first analyzed and then exploited.
This method has been applied on a geographical domain and the benefit has been
evaluated."
"This paper presents a work package realized for the G\'eOnto project. A new
method is proposed for an enrichment of a first geographical ontology developed
beforehand. This method relies on text analysis by lexico-syntactic patterns.
  From the retrieve of n-ary relations the method automatically detect those
involved in a spatial and/or temporal relation in a context of a description of
journeys."
"Titles are denoted by the TITLE element within a web page. We queried the
title against the the Yahoo search engine to determine the page's status
(found, not found). We conducted several tests based on elements of the title.
These tests were used to discern whether we could predict a pages status based
on the title. Our results increase our ability to determine bad titles but not
our ability to determine good titles."
"In Information Retrieval (IR), whether implicitly or explicitly, queries and
documents are often represented as vectors. However, it may be more beneficial
to consider documents and/or queries as multidimensional objects. Our belief is
this would allow building ""truly"" interactive IR systems, i.e., where
interaction is fully incorporated in the IR framework.
  The probabilistic formalism of quantum physics represents events and
densities as multidimensional objects. This paper presents our first step
towards building an interactive IR framework upon this formalism, by stating
how the first interaction of the retrieval process, when the user types a
query, can be formalised. Our framework depends on a number of parameters
affecting the final document ranking. In this paper we experimentally
investigate the effect of these parameters, showing that the proposed
representation of documents and queries as multidimensional objects can compete
with standard approaches, with the additional prospect to be applied to
interactive retrieval."
"We study numerically the spectrum and eigenstate properties of the Google
matrix of various examples of directed networks such as vocabulary networks of
dictionaries and university World Wide Web networks. The spectra have gapless
structure in the vicinity of the maximal eigenvalue for Google damping
parameter $\alpha$ equal to unity. The vocabulary networks have relatively
homogeneous spectral density, while university networks have pronounced
spectral structures which change from one university to another, reflecting
specific properties of the networks. We also determine specific properties of
eigenstates of the Google matrix, including the PageRank. The fidelity of the
PageRank is proposed as a new characterization of its stability."
"Term extraction is one of the layers in the ontology development process
which has the task to extract all the terms contained in the input document
automatically. The purpose of this process is to generate list of terms that
are relevant to the domain of the input document. In the literature there are
many approaches, techniques and algorithms used for term extraction. In this
paper we propose a new approach using particle swarm optimization techniques in
order to improve the accuracy of term extraction results. We choose five
features to represent the term score. The approach has been applied to the
domain of religious document. We compare our term extraction method precision
with TFIDF, Weirdness, GlossaryExtraction and TermExtractor. The experimental
results show that our propose approach achieve better precision than those four
algorithm."
"Text segmentation is an inherent part of an OCR system irrespective of the
domain of application of it. The OCR system contains a segmentation module
where the text lines, words and ultimately the characters must be segmented
properly for its successful recognition. The present work implements a Hough
transform based technique for line and word segmentation from digitized images.
The proposed technique is applied not only on the document image dataset but
also on dataset for business card reader system and license plate recognition
system. For standardization of the performance of the system the technique is
also applied on public domain dataset published in the website by CMATER,
Jadavpur University. The document images consist of multi-script printed and
hand written text lines with variety in script and line spacing in single
document image. The technique performs quite satisfactorily when applied on
mobile camera captured business card images with low resolution. The usefulness
of the technique is verified by applying it in a commercial project for
localization of license plate of vehicles from surveillance camera images by
the process of segmentation itself. The accuracy of the technique for word
segmentation, as verified experimentally, is 85.7% for document images, 94.6%
for business card images and 88% for surveillance camera images."
"The paper presents our design of a next generation information retrieval
system based on tag co-occurrences and subsequent clustering. We help users
getting access to digital data through information visualization in the form of
tag clusters. Current problems like the absence of interactivity and semantics
between tags or the difficulty of adding additional search arguments are
solved. In the evaluation, based upon SERVQUAL and IT systems quality
indicators, we found out that tag clusters are perceived as more useful than
tag clouds, are much more trustworthy, and are more enjoyable to use."
"The existing information retrieval techniques do not consider the context of
the keywords present in the user's queries. Therefore, the search engines
sometimes do not provide sufficient information to the users. New methods based
on the semantics of user keywords must be developed to search in the vast web
space without incurring loss of information. The semantic based information
retrieval techniques need to understand the meaning of the concepts in the user
queries. This will improve the precision-recall of the search results.
Therefore, this approach focuses on the concept based semantic information
retrieval. This work is based on Word sense disambiguation, thesaurus WordNet
and ontology of any domain for retrieving information in order to capture the
context of particular concept(s) and discover semantic relationships between
them."
"In this paper we describe a mechanism to improve Information Retrieval (IR)
on the web. The method is based on Formal Concepts Analysis (FCA) that it is
makes semantical relations during the queries, and allows a reorganizing, in
the shape of a lattice of concepts, the answers provided by a search engine. We
proposed for the IR an incremental algorithm based on Galois lattice. This
algorithm allows a formal clustering of the data sources, and the results which
it turns over are classified by order of relevance. The control of relevance is
exploited in clustering, we improved the result by using ontology in field of
image processing, and reformulating the user queries which make it possible to
give more relevant documents."
"Fast and high quality document clustering is an important task in organizing
information, search engine results obtaining from user query, enhancing web
crawling and information retrieval. With the large amount of data available and
with a goal of creating good quality clusters, a variety of algorithms have
been developed having quality-complexity trade-offs. Among these, some
algorithms seek to minimize the computational complexity using certain
criterion functions which are defined for the whole set of clustering solution.
In this paper, we are proposing a novel document clustering algorithm based on
an internal criterion function. Most commonly used partitioning clustering
algorithms (e.g. k-means) have some drawbacks as they suffer from local optimum
solutions and creation of empty clusters as a clustering solution. The proposed
algorithm usually does not suffer from these problems and converge to a global
optimum, its performance enhances with the increase in number of clusters. We
have checked our algorithm against three different datasets for four different
values of k (required number of clusters)."
"Click through rates (CTR) offer useful user feedback that can be used to
infer the relevance of search results for queries. However it is not very
meaningful to look at the raw click through rate of a search result because the
likelihood of a result being clicked depends not only on its relevance but also
the position in which it is displayed. One model of the browsing behavior, the
{\em Examination Hypothesis} \cite{RDR07,Craswell08,DP08}, states that each
position has a certain probability of being examined and is then clicked based
on the relevance of the search snippets. This is based on eye tracking studies
\cite{Claypool01, GJG04} which suggest that users are less likely to view
results in lower positions. Such a position dependent variation in the
probability of examining a document is referred to as {\em position bias}. Our
main observation in this study is that the position bias tends to differ with
the kind of information the user is looking for. This makes the position bias
{\em query specific}. In this study, we present a model for analyzing a query
specific position bias from the click data and use these biases to derive
position independent relevance values of search results. Our model is based on
the assumption that for a given query, the positional click through rate of a
document is proportional to the product of its relevance and a {\em query
specific} position bias. We compare our model with the vanilla examination
hypothesis model (EH) on a set of queries obtained from search logs of a
commercial search engine. We also compare it with the User Browsing Model (UBM)
\cite{DP08} which extends the cascade model of Craswell et al\cite{Craswell08}
by incorporating multiple clicks in a query session. We show that the our
model, although much simpler to implement, consistently outperforms both EH and
UBM on well-used measures such as relative error and cross entropy."
"The shift from an information society to a knowledge society require rapid
information harvesting, reliable search and instantaneous on demand delivery.
Information extraction agents are used to explore and collect data available
from Web, in order to effectively exploit such data for business purposes, such
as automatic news filtering, advertisement or product searching and price
comparing. In this paper, we develop a real-time automatic harvesting agent for
adverts posted on Servihoo web portal and an SMS-based notification system. It
uses the URL of the web portal and the object model, i.e., the fields of
interests and a set of rules written using the HTML parsing functions to
extract latest adverts information. The extraction engine executes the
extraction rules and stores the information in a database to be processed for
automatic notification. This intelligent system helps to tremendously save
time. It also enables users or potential product buyers to react more quickly
to changes and newly posted sales adverts, paving the way to real-time best buy
deals."
"Data generated in the fields of science, technology, business and in many
other fields of research are increasing in an exponential rate. The way to
extract knowledge from a huge set of data is a challenging task. This paper
aims to propose a hybrid and viable method to deal with an information system
in data mining, using topological techniques and the significance of the
attributes measured using rough set theory, to compute the reduct, This will
reduce the randomness in the process of elimination of redundant attributes,
which, in turn, will reduce the complexity of the computation of reducts of an
information system where a large amount of data have to be processed."
"In this paper we introduce the concept of dynamic link pages. A web site/page
contains a number of links to other pages. All the links are not equally
important. Few links are more frequently visited and few rarely visited. In
this scenario, identifying the frequently used links and placing them in the
top left corner of the page will increase the user's satisfaction. This process
will reduce the time spent by a visitor on the page, as most of the times, the
popular links are presented in the visible part of the screen itself. Also, a
site can be indexed based on the popular links in that page. This will increase
the efficiency of the retrieval system. We presented a model to display the
popular links, and also proposed a method to increase the quality of retrieval
system."
"We describe a clustering method for labeled link network (semantic graph)
that can be used to group important nodes (highly connected nodes) with their
relevant link's labels by using PARAFAC tensor decomposition. In this kind of
network, the adjacency matrix can not be used to fully describe all information
about the network structure. We have to expand the matrix into 3-way adjacency
tensor, so that not only the information about to which nodes a node connects
to but by which link's labels is also included. And by applying PARAFAC
decomposition on this tensor, we get two lists, nodes and link's labels with
scores attached to each node and labels, for each decomposition group. So
clustering process to get the important nodes along with their relevant labels
can be done simply by sorting the lists in decreasing order. To test the
method, we construct labeled link network by using blog's dataset, where the
blogs are the nodes and labeled links are the shared words among them. The
similarity measures between the results and standard measures look promising,
especially for two most important tasks, finding the most relevant words to
blogs query and finding the most similar blogs to blogs query, about 0.87."
"Geographic location search engines allow users to constrain and order search
results in an intuitive manner by focusing a query on a particular geographic
region. Geographic search technology, also called location search, has recently
received significant interest from major search engine companies. Academic
research in this area has focused primarily on techniques for extracting
geographic knowledge from the web. In this paper, we study the problem of
efficient query processing in scalable geographic search engines. Query
processing is a major bottleneck in standard web search engines, and the main
reason for the thousands of machines used by the major engines. Geographic
search engine query processing is different in that it requires a combination
of text and spatial data processing techniques. We propose several algorithms
for efficient query processing in geographic search engines, integrate them
into an existing web search query processor, and evaluate them on large sets of
real data and query traces."
"Mining Time Series data has a tremendous growth of interest in today's world.
To provide an indication various implementations are studied and summarized to
identify the different problems in existing applications. Clustering time
series is a trouble that has applications in an extensive assortment of fields
and has recently attracted a large amount of research. Time series data are
frequently large and may contain outliers. In addition, time series are a
special type of data set where elements have a temporal ordering. Therefore
clustering of such data stream is an important issue in the data mining
process. Numerous techniques and clustering algorithms have been proposed
earlier to assist clustering of time series data streams. The clustering
algorithms and its effectiveness on various applications are compared to
develop a new method to solve the existing problem. This paper presents a
survey on various clustering algorithms available for time series datasets.
Moreover, the distinctiveness and restriction of previous research are
discussed and several achievable topics for future study are recognized.
Furthermore the areas that utilize time series clustering are also summarized."
"Several steps are missing in the current high-speed race towards the holistic
support of citizen needs in the domain of eGovernment. This paper is focused on
how to provide support for the citizen profile. This profile, in a wide sense,
includes personal information as well documents in possession of the citizen.
This also involves the provision of those mechanisms required to publish,
access and submit the convenient information to a Public Administration in due
curse of a transactional services provided with the last one. Main features of
the system are related to interoperability and possibilities for its inclusion
in a cost effective manner in already developed platforms. To make that
possible, this approach will take full advantage of semantic technologies and
the RESTful paradigm to design the entire system. The paper presents the
overall system with some notes on the deployment of the solution for its
further reuse in similar contexts."
"One of the most important issues in Information Retrieval is inferring the
intents underlying users' queries. Thus, any tool to enrich or to better
contextualized queries can proof extremely valuable. Entity extraction,
provided it is done fast, can be one of such tools. Such techniques usually
rely on a prior training phase involving large datasets. That training is
costly, specially in environments which are increasingly moving towards real
time scenarios where latency to retrieve fresh informacion should be minimal.
In this paper an `on-the-fly' query decomposition method is proposed. It uses
snippets which are mined by means of a na\""ive statistical algorithm. An
initial evaluation of such a method is provided, in addition to a discussion on
its applicability to different scenarios."
"The high-level contribution of this paper is the development and
implementation of an algorithm to selfextract secondary keywords and their
combinations (combo words) based on abstracts collected using standard primary
keywords for research areas from reputed online digital libraries like IEEE
Explore, PubMed Central and etc. Given a collection of N abstracts, we
arbitrarily select M abstracts (M<< N; M/N as low as 0.15) and parse each of
the M abstracts, word by word. Upon the first-time appearance of a word, we
query the user for classifying the word into an Accept-List or non-Accept-List.
The effectiveness of the training approach is evaluated by measuring the
percentage of words for which the user is queried for classification when the
algorithm parses through the words of each of the M abstracts. We observed that
as M grows larger, the percentage of words for which the user is queried for
classification reduces drastically. After the list of acceptable words is built
by parsing the M abstracts, we now parse all the N abstracts, word by word, and
count the frequency of appearance of each of the words in Accept-List in these
N abstracts. We also construct a Combo-Accept-List comprising of all possible
combinations of the single keywords in Accept-List and parse all the N
abstracts, two successive words (combo word) at a time, and count the frequency
of appearance of each of the combo words in the Combo-Accept-List in these N
abstracts."
"We address the problem of cross-referencing text fragments with Wikipedia
pages, in a way that synonymy and polysemy issues are resolved accurately and
efficiently. We take inspiration from a recent flow of work [Cucerzan 2007,
Mihalcea and Csomai 2007, Milne and Witten 2008, Chakrabarti et al 2009], and
extend their scenario from the annotation of long documents to the annotation
of short texts, such as snippets of search-engine results, tweets, news, blogs,
etc.. These short and poorly composed texts pose new challenges in terms of
efficiency and effectiveness of the annotation process, that we address by
designing and engineering TAGME, the first system that performs an accurate and
on-the-fly annotation of these short textual fragments. A large set of
experiments shows that TAGME outperforms state-of-the-art algorithms when they
are adapted to work on short texts and it results fast and competitive on long
texts."
"Existing models for ranking documents(mostly in world wide web) are prestige
based. In this article, three algorithms to objectively judge the merit of a
document are proposed - 1) Citation graph maxflow 2) Recursive Gloss Overlap
based intrinsic merit scoring and 3) Interview algorithm. A short discussion on
generic judgement and its mathematical treatment is presented in introduction
to motivate these algorithms."
"This study considers the extent to which users with the same query agree as
to what is relevant, and how what is considered relevant may translate into a
retrieval algorithm and results display. To combine user perceptions of
relevance with algorithm rank and to present results, we created a prototype
digital library of scholarly literature. We confine studies to one population
of scientists (paleontologists), one domain of scholarly scientific articles
(paleo-related), and a prototype system (PaleoLit) that we built for the
purpose. Based on the principle that users do not pre-suppose answers to a
given query but that they will recognize what they want when they see it, our
system uses a rules-based algorithm to cluster results into fuzzy categories
with three relevance levels. Our system matches at least 1/3 of our
participants' relevancy ratings 87% of the time. Our subsequent usability study
found that participants trusted our uncertainty labels but did not value our
color-coded horizontal results layout above a standard retrieval list. We posit
that users make such judgments in limited time, and that time optimization per
task might help explain some of our findings."
"This paper attempts to discuss the evolution of the retrieval approaches
focusing on development, challenges and future direction of the image
retrieval. It highlights both the already addressed and outstanding issues. The
explosive growth of image data leads to the need of research and development of
Image Retrieval. However, Image retrieval researches are moving from keyword,
to low level features and to semantic features. Drive towards semantic features
is due to the problem of the keywords which can be very subjective and time
consuming while low level features cannot always describe high level concepts
in the users' mind. Hence, introducing an interpretation inconsistency between
image descriptors and high level semantics that known as the semantic gap. This
paper also discusses the semantic gap issues, user query mechanisms as well as
common ways used to bridge the gap in image retrieval."
"In this paper we demonstrate the applicability of latent Dirichlet allocation
(LDA) for classifying large Web document collections. One of our main results
is a novel influence model that gives a fully generative model of the document
content taking linkage into account. In our setup, topics propagate along links
in such a way that linked documents directly influence the words in the linking
document. As another main contribution we develop LDA specific boosting of
Gibbs samplers resulting in a significant speedup in our experiments. The
inferred LDA model can be applied for classification as dimensionality
reduction similarly to latent semantic indexing. In addition, the model yields
link weights that can be applied in algorithms to process the Web graph; as an
example we deploy LDA link weights in stacked graphical learning. By using
Weka's BayesNet classifier, in terms of the AUC of classification, we achieve
4% improvement over plain LDA with BayesNet and 18% over tf.idf with SVM. Our
Gibbs sampling strategies yield about 5-10 times speedup with less than 1%
decrease in accuracy in terms of likelihood and AUC of classification."
"Wiktionary is a unique, peculiar, valuable and original resource for natural
language processing (NLP). The paper describes an open-source Wiktionary
parser: its architecture and requirements followed by a description of
Wiktionary features to be taken into account, some open problems of Wiktionary
and the parser. The current implementation of the parser extracts the
definitions, semantic relations, and translations from English and Russian
Wiktionaries. The paper's goal is to interest researchers (1) in using the
constructed machine-readable dictionary for different NLP tasks, (2) in
extending the software to parse 170 still unused Wiktionaries. The comparison
of a number and types of semantic relations, a number of definitions, and a
number of translations in the English Wiktionary and the Russian Wiktionary has
been carried out. It was found that the number of semantic relations in the
English Wiktionary is larger by 1.57 times than in Russian (157 and 100
thousands). But the Russian Wiktionary has more ""rich"" entries (with a big
number of semantic relations), e.g. the number of entries with three or more
semantic relations is larger by 1.63 times than in the English Wiktionary. Upon
comparison, it was found out the methodological shortcomings of the Wiktionary."
"Vertical search engines focus on specific slices of content, such as the Web
of a single country or the document collection of a large corporation. Despite
this, like general open web search engines, they are expensive to maintain,
expensive to operate, and hard to design. Because of this, predicting the
response time of a vertical search engine is usually done empirically through
experimentation, requiring a costly setup. An alternative is to develop a model
of the search engine for predicting performance. However, this alternative is
of interest only if its predictions are accurate. In this paper we propose a
methodology for analyzing the performance of vertical search engines. Applying
the proposed methodology, we present a capacity planning model based on a
queueing network for search engines with a scale typically suitable for the
needs of large corporations. The model is simple and yet reasonably accurate
and, in contrast to previous work, considers the imbalance in query service
times among homogeneous index servers. We discuss how we tune up the model and
how we apply it to predict the impact on the query response time when
parameters such as CPU and disk capacities are changed. This allows a manager
of a vertical search engine to determine a priori whether a new configuration
of the system might keep the query response under specified performance
constraints."
"Text classification is the process of classifying documents into predefined
categories based on their content. It is the automated assignment of natural
language texts to predefined categories. Text classification is the primary
requirement of text retrieval systems, which retrieve texts in response to a
user query, and text understanding systems, which transform text in some way
such as producing summaries, answering questions or extracting data. Existing
supervised learning algorithms for classifying text need sufficient documents
to learn accurately. This paper presents a new algorithm for text
classification using artificial intelligence technique that requires fewer
documents for training. Instead of using words, word relation i.e. association
rules from these words is used to derive feature set from pre-classified text
documents. The concept of na\""ive Bayes classifier is then used on derived
features and finally only a single concept of genetic algorithm has been added
for final classification. A system based on the proposed algorithm has been
implemented and tested. The experimental results show that the proposed system
works as a successful text classifier."
"This paper is about an information retrieval evaluation on three different
retrieval-supporting services. All three services were designed to compensate
typical problems that arise in metadata-driven Digital Libraries, which are not
adequately handled by a simple tf-idf based retrieval. The services are: (1) a
co-word analysis based query expansion mechanism and re-ranking via (2)
Bradfordizing and (3) author centrality. The services are evaluated with
relevance assessments conducted by 73 information science students. Since the
students are neither information professionals nor domain experts the question
of inter-rater agreement is taken into consideration. Two important
implications emerge: (1) the inter-rater agreement rates were mainly fair to
moderate and (2) after a data-cleaning step which erased the assessments with
poor agreement rates the evaluation data shows that the three retrieval
services returned disjoint but still relevant result sets."
"The Open Archive Initiative Protocol for Metadata Handling (OAI-PMHiii) is a
standard that is seeing increased use as a means for exchanging structured
metadata. OAI-PMH implementations must support Dublin Core as a metadata
standard, with other metadata formats as optional. We have developed tools
which enable Mercury to consume metadata from OAI-PMH services in any of the
metadata formats we support (Dublin Core, Darwin Core, FCDC CSDGM, GCMD DIF,
EML, and ISO 19115/19137). We are also making ORNL DAAC metadata available
through OAI-PMH for other metadata tools to utilize. This paper describes
Mercury capabilities with multiple metadata formats, in general, and, more
specifically, the results of our OAI-PMH implementations and the lessons
learned."
"In Knowledge Management, variations in information expressions have proven a
real challenge. In particular, classical semantic relations (e.g. synonymy) do
not connect words with different parts-of-speech. The method proposed tries to
address this issue. It consists in building a derivational resource from a
morphological derivation tool together with derivational guidelines from a
dictionary in order to store only correct derivatives. This resource, combined
with a syntactic parser, a semantic disambiguator and some derivational
patterns, helps to reformulate an original sentence while keeping the initial
meaning in a convincing manner This approach has been evaluated in three
different ways: the precision of the derivatives produced from a lemma; its
ability to provide well-formed reformulations from an original sentence,
preserving the initial meaning; its impact on the results coping with a real
issue, ie a question answering task . The evaluation of this approach through a
question answering system shows the pros and cons of this system, while
foreshadowing some interesting future developments."
"While social network analysis often focuses on graph structure of social
actors, an increasing number of communication networks now provide textual
content within social activity (email, instant messaging, blogging,
collaboration networks). We present an open source visualization software,
GraphDuplex, which brings together social structure and textual content, adding
a semantic dimension to social analysis. GraphDuplex eventually connects any
number of social or semantic graphs together, and through dynamic queries
enables user interaction and exploration across multiple graphs of different
nature."
"It is known that humans can easily read words where the letters have been
jumbled in a certain way. This paper examines this problem by associating a
distance measure with the jumbling process. Modifications to text were
generated according to the Damerau-Levenshtein distance and it was checked if
the users are able to read it. Graphical representations of the results are
provided."
"We present a new algorithm for behavioral targeting of banner advertisements.
We record different user's actions such as clicks, search queries and page
views. We use the collected information on the user to estimate in real time
the probability of a click on a banner. A banner is displayed if it either has
the highest probability of being clicked or if it is the one that generates the
highest average profit."
"Semantic web is the next generation web, which concerns the meaning of web
documents It has the immense power to pull out the most relevant information
from the web pages, which is also meaningful to any user, using software
agents. In today's world, agent communication is not possible if concerned
ontology is changed a little. We have pointed out this very problem and
developed an Ontology Purification System to help agent communication. In our
system you can send queries and view the search results. If it can't meet the
criteria then it finds out the mismatched elements. Modification is done within
a second and you can see the difference. That's why we emphasis on the word
dynamic. When Administrator is updating the system, at the same time that
updation is visible to the user."
"Due to the emergence of the semantic Web and the increasing need to formalize
human knowledge, ontologie engineering is now an important activity. But is
this activity very different from other ones like software engineering, for
example ? In this paper, we investigate analogies between ontologies on one
hand, types, objects and data bases on the other one, taking into account the
notion of evolution of an ontology. We represent a unique ontology using
different paradigms, and observe that the distance between these different
concepts is small. We deduce from this constatation that ontologies and more
specifically ontology description languages can take advantage of beeing
fertilizated with some other computer science domains and inherit important
characteristics as modularity, for example."
"Query expansion is a functionality of search engines that suggests a set of
related queries for a user-issued keyword query. Typical corpus-driven keyword
query expansion approaches return popular words in the results as expanded
queries. Using these approaches, the expanded queries may correspond to a
subset of possible query semantics, and thus miss relevant results. To handle
ambiguous queries and exploratory queries, whose result relevance is difficult
to judge, we propose a new framework for keyword query expansion: we start with
clustering the results according to user specified granularity, and then
generate expanded queries, such that one expanded query is generated for each
cluster whose result set should ideally be the corresponding cluster. We
formalize this problem and show its APX-hardness. Then we propose two efficient
algorithms named iterative single-keyword refinement and partial elimination
based convergence, respectively, which effectively generate a set of expanded
queries from clustered results that provide a classification of the original
query results. We believe our study of generating an optimal query based on the
ground truth of the query results not only has applications in query expansion,
but has significance for studying keyword search quality in general."
"The growing need to manage and exploit the proliferation of online data
sources is opening up new opportunities for bringing people closer to the
resources they need. For instance, consider a recommendation service through
which researchers can receive daily pointers to journal papers in their fields
of interest. We survey some of the known approaches to the problem of technical
paper recommendation and ask how they can be extended to deal with multiple
information sources. More specifically, we focus on a variant of this problem -
recommending conference paper submissions to reviewing committee members -
which offers us a testbed to try different approaches. Using WHIRL - an
information integration system - we are able to implement different
recommendation algorithms derived from information retrieval principles. We
also use a novel autonomous procedure for gathering reviewer interest
information from the Web. We evaluate our approach and compare it to other
methods using preference data provided by members of the AAAI-98 conference
reviewing committee along with data about the actual submissions."
"Color descriptors are one of the important features used in content-based
image retrieval. The Dominant Color Descriptor (DCD) represents a few
perceptually dominant colors in an image through color quantization. For image
retrieval based on DCD, the earth mover's distance and the optimal color
composition distance are proposed to measure the dissimilarity between two
images. Although providing good retrieval results, both methods are too
time-consuming to be used in a large image database. To solve the problem, we
propose a new distance function that calculates an approximate earth mover's
distance in linear time. To calculate the dissimilarity in linear time, the
proposed approach employs the space-filling curve for multidimensional color
space. To improve the accuracy, the proposed approach uses multiple curves and
adjusts the color positions. As a result, our approach achieves
order-of-magnitude time improvement but incurs small errors. We have performed
extensive experiments to show the effectiveness and efficiency of the proposed
approach. The results reveal that our approach achieves almost the same results
with the EMD in linear time."
"Problem Statement: The huge number of information on the web as well as the
growth of new inexperienced users creates new challenges for information
retrieval. It has become increasingly difficult for these users to find
relevant documents that satisfy their individual needs. Certainly the current
search engines (such as Google, Bing and Yahoo) offer an efficient way to
browse the web content. However, the result quality is highly based on uses
queries which need to be more precise to find relevant documents. This task
still complicated for the majority of inept users who cannot express their
needs with significant words in the query. For that reason, we believe that a
reformulation of the initial user's query can be a good alternative to improve
the information selectivity. This study proposes a novel approach and presents
a prototype system called PRESY (Profile-based REformulation SYstem) for
information retrieval on the web. Approach: It uses an incremental approach to
categorize users by constructing a contextual base. The latter is composed of
two types of context (static and dynamic) obtained using the users' profiles.
The architecture proposed was implemented using .Net environment to perform
queries reformulating tests. Results: The experiments gives at the end of this
article show that the precision of the returned content is effectively
improved. The tests were performed with the most popular searching engine (i.e.
Google, Bind and Yahoo) selected in particular for their high selectivity.
Among the given results, we found that query reformulation improve the first
three results by 10.7% and 11.7% of the next seven returned elements. So as we
can see the reformulation of users' initial queries improves the pertinence of
returned content."
"In this paper, an Eliteness Hypothesis for information retrieval is proposed,
where we define two generative processes to create information items and
queries. By assuming the deterministic relationships between the eliteness of
terms and relevance, we obtain a new theoretical retrieval framework. The
resulting ranking function is a unified one as it is capable of using available
relevance information on both the document and the query, which is otherwise
unachievable by existing retrieval models. Our preliminary experiment on a
simple ranking function has demonstrated the potential of the approach."
"We propose a new task of recommending touristic locations based on a user's
visiting history in a geographically remote region. This can be used to plan a
touristic visit to a new city or country, or by travel agencies to provide
personalised travel deals.
  A set of geotags is used to compute a location similarity model between two
different regions. The similarity between two landmarks is derived from the
number of users that have visited both places, using a Gaussian density
estimation of the co-occurrence space of location visits to cluster related
geotags. The standard deviation of the kernel can be used as a scale parameter
that determines the size of the recommended landmarks.
  A personalised recommendation based on the location similarity model is
evaluated on city and country scale and is able to outperform a location
ranking based on popularity. Especially when a tourist filter based on visit
duration is enforced, the prediction can be accurately adapted to the
preference of the user. An extensive evaluation based on manual annotations
shows that more strict ranking methods like cosine similarity and a proposed
RankDiff algorithm provide more serendipitous recommendations and are able to
link similar locations on opposite sides of the world."
"Many large-scale Web applications that require ranked top-k retrieval such as
Web search and online advertising are implemented using inverted indices. An
inverted index represents a sparse term-document matrix, where non-zero
elements indicate the strength of term-document association. In this work, we
present an approach for lossless compression of inverted indices. Our approach
maps terms in a document corpus to a new term space in order to reduce the
number of non-zero elements in the term-document matrix, resulting in a more
compact inverted index. We formulate the problem of selecting a new term space
that minimizes the resulting index size as a matrix factorization problem, and
prove that finding the optimal factorization is an NP-hard problem. We develop
a greedy algorithm for finding an approximate solution. A side effect of our
approach is increasing the number of terms in the index, which may negatively
affect query evaluation performance. To eliminate such effect, we develop a
methodology for modifying query evaluation algorithms by exploiting specific
properties of our compression approach. Our experimental evaluation
demonstrates that our approach achieves an index size reduction of 20%, while
maintaining the same query response times. Higher compression ratios up to 35%
are achievable, however at the cost of slightly longer query response times.
Furthermore, combining our approach with other lossless compression techniques,
namely variable-byte encoding, leads to index size reduction of up to 50%."
"Web search engines and specialized online verticals are increasingly
incorporating results from structured data sources to answer semantically rich
user queries. For example, the query \WebQuery{Samsung 50 inch led tv} can be
answered using information from a table of television data. However, the users
are not domain experts and quite often enter values that do not match precisely
the underlying data. Samsung makes 46- or 55- inch led tvs, but not 50-inch
ones. So a literal execution of the above mentioned query will return zero
results. For optimal user experience, a search engine would prefer to return at
least a minimum number of results as close to the original query as possible.
Furthermore, due to typical fast retrieval speeds in web-search, a search
engine query execution is time-bound.
  In this paper, we address these challenges by proposing algorithms that
rewrite the user query in a principled manner, surfacing at least the required
number of results while satisfying the low-latency constraint. We formalize
these requirements and introduce a general formulation of the problem. We show
that under a natural formulation, the problem is NP-Hard to solve optimally,
and present approximation algorithms that produce good rewrites. We empirically
validate our algorithms on large-scale data obtained from a commercial search
engine's shopping vertical."
"For ambiguous queries, conventional retrieval systems are bound by two
conflicting goals. On the one hand, they should diversify and strive to present
results for as many query intents as possible. On the other hand, they should
provide depth for each intent by displaying more than a single result. Since
both diversity and depth cannot be achieved simultaneously in the conventional
static retrieval model, we propose a new dynamic ranking approach. Dynamic
ranking models allow users to adapt the ranking through interaction, thus
overcoming the constraints of presenting a one-size-fits-all static ranking. In
particular, we propose a new two-level dynamic ranking model for presenting
search results to the user. In this model, a user's interactions with the
first-level ranking are used to infer this user's intent, so that second-level
rankings can be inserted to provide more results relevant for this intent.
Unlike for previous dynamic ranking models, we provide an algorithm to
efficiently compute dynamic rankings with provable approximation guarantees for
a large family of performance measures. We also propose the first principled
algorithm for learning dynamic ranking functions from training data. In
addition to the theoretical results, we provide empirical evidence
demonstrating the gains in retrieval quality that our method achieves over
conventional approaches."
"The field of information extraction from the Web emerged with the growth of
the Web and the multiplication of online data sources. This paper is an
analysis of information extraction methods. It presents a service oriented
approach for web information extraction considering both web data management
and extraction services. Then we propose an SOA based architecture to enhance
flexibility and on-the-fly modification of web extraction services. An
implementation of the proposed architecture is proposed on the middleware level
of Java Enterprise Edition (JEE) servers."
"The rapid growth of web has resulted in vast volume of information.
Information availability at a rapid speed to the user is vital. English
language (or any for that matter) has lot of ambiguity in the usage of words.
So there is no guarantee that a keyword based search engine will provide the
required results. This paper introduces the use of dictionary (standardised) to
obtain the context with which a keyword is used and in turn cluster the results
based on this context. These ideas can be merged with a metasearch engine to
enhance the search efficiency."
"Recommender systems engage user profiles and appropriate filtering techniques
to assist users in finding more relevant information over the large volume of
information. User profiles play an important role in the success of
recommendation process since they model and represent the actual user needs.
However, a comprehensive literature review of recommender systems has
demonstrated no concrete study on the role and impact of knowledge in user
profiling and filtering approache. In this paper, we review the most prominent
recommender systems in the literature and examine the impression of knowledge
extracted from different sources. We then come up with this finding that
semantic information from the user context has substantial impact on the
performance of knowledge based recommender systems. Finally, some new clues for
improvement the knowledge-based profiles have been proposed."
"We present a new approach to evaluate chord recognition systems on songs
which do not have full annotations. The principle is to use online chord
databases to generate high accurate ""pseudo annotations"" for these songs and
compute ""pseudo accuracies"" of test systems. Statistical models that model the
relationship between ""pseudo accuracy"" and real performance are then applied to
estimate test systems' performance. The approach goes beyond the existing
evaluation metrics, allowing us to carry out extensive analysis on chord
recognition systems, such as their generalizations to different genres. In the
experiments we applied this method to evaluate three state-of-the-art chord
recognition systems, of which the results verified its reliability."
"Over the last fifteen years, web searching has seen tremendous improvements.
Starting from a nearly random collection of matching pages in 1995, today,
search engines tend to satisfy the user's informational need on well-formulated
queries. One of the main remaining challenges is to satisfy the users' needs
when they provide a poorly formulated query. When the pages matching the user's
original keywords are judged to be unsatisfactory, query expansion techniques
are used to alter the result set. These techniques find keywords that are
similar to the keywords given by the user, which are then appended to the
original query leading to a perturbation of the result set. However, when the
original query is sufficiently ill-posed, the user's informational need is best
met using entirely different keywords, and a small perturbation of the original
result set is bound to fail.
  We propose a novel approach that is not based on the keywords of the original
query. We intentionally seek out orthogonal queries, which are related queries
that have low similarity to the user's query. The result sets of orthogonal
queries intersect with the result set of the original query on a small number
of pages. An orthogonal query can access the user's informational need while
consisting of entirely different terms than the original query. We illustrate
the effectiveness of our approach by proposing a query expansion method derived
from these observations that improves upon results obtained using the Yahoo
BOSS infrastructure."
"Interoperability is a feature required by the Semantic Web. It is provided by
the ontology matching methods and algorithms. But now ontologies are presented
not only in English, but in other languages as well. It is important to use an
automatic translation for obtaining correct matching pairs in multilingual
ontology matching. The translation into many languages could be based on the
Google Translate API, the Wiktionary database, etc. From the point of view of
the balance of presence of many languages, of manually crafted translations, of
a huge size of a dictionary, the most promising resource is the Wiktionary. It
is a collaborative project working on the same principles as the Wikipedia. The
parser of the Wiktionary was developed and the machine-readable dictionary was
designed. The data of the machine-readable Wiktionary are stored in a
relational database, but with the help of D2R server the database is presented
as an RDF store. Thus, it is possible to get lexicographic information
(definitions, translations, synonyms) from web service using SPARQL requests.
In the case study, the problem entity is a task of multilingual ontology
matching based on Wiktionary data accessible via SPARQL endpoint. Ontology
matching results obtained using Wiktionary were compared with results based on
Google Translate API."
"This paper aims to review the fiercely discussed question of whether the
ranking of Wikipedia articles in search engines is justified by the quality of
the articles. After an overview of current research on information quality in
Wikipedia, a summary of the extended discussion on the quality of encyclopedic
entries in general is given. On this basis, a heuristic method for evaluating
Wikipedia entries is developed and applied to Wikipedia articles that scored
highly in a search engine retrieval effectiveness test and compared with the
relevance judgment of jurors. In all search engines tested, Wikipedia results
are unanimously judged better by the jurors than other results on the
corresponding results position. Relevance judgments often roughly correspond
with the results from the heuristic evaluation. Cases in which high relevance
judgments are not in accordance with the comparatively low score from the
heuristic evaluation are interpreted as an indicator of a high degree of trust
in Wikipedia. One of the systemic shortcomings of Wikipedia lies in its
necessarily incoherent user model. A further tuning of the suggested criteria
catalogue, for instance the different weighing of the supplied criteria, could
serve as a starting point for a user model differentiated evaluation of
Wikipedia articles. Approved methods of quality evaluation of reference works
are applied to Wikipedia articles and integrated with the question of search
engine evaluation."
"Every business needs knowledge about their competitors to survive better. One
of the information repositories is web. Retrieving Specific information from
the web is challenging. An Ontological model is developed to capture specific
information by using web semantics. From the Ontology model, the relations
between the data are mined using decision tree. From all these a new framework
is developed for Business Intelligence."
"Looking into the growth of information in the web it is a very tedious
process of getting the exact information the user is looking for. Many search
engines generate user profile related data listing. This paper involves one
such process where the rating is given to the link that the user is clicking
on. Rather than avoiding the uninterested links both interested links and the
uninterested links are listed. But sorted according to the weightings given to
each link by the number of visit made by the particular user and the amount of
time spent on the particular link."
"Looking into the growth of information in the web it is a very tedious
process of getting the exact information the user is looking for. Many search
engines generate user profile related data listing. This paper involves one
such process where the rating is given to the link that the user is clicking
on. Rather than avoiding the uninterested links both interested links and the
uninterested links are listed. But sorted according to the weightings given to
each link by the number of visit made by the particular user and the amount of
time spent on the particular link."
"Enormous explosion in the number of the World Wide Web pages occur every day
and since the efficiency of most of the information processing systems is found
to be less, the potential of the Internet applications is often underutilized.
Efficient utilization of the web can be exploited when similar web pages are
rigorously, exhaustively organized and clustered based on some domain knowledge
(semantic-based) .Ontology which is a formal representation of domain knowledge
aids in such efficient utilization. The performance of almost all the
semantic-based clustering techniques depends on the constructed ontology,
describing the domain knowledge . The proposed methodology provides an enhanced
pronominal anaphora resolution, one of the key aspects of semantic analysis in
Natural Language Processing for obtaining cross references within a web page
providing better ontology construction. The experimental data sets exhibits
better efficiency of the proposed method compared to earlier traditional
algorithms."
"Nowadays folksonomy is used as a system derived from user-generated
electronic tags or keywords that annotate and describe online content. But it
is not a classification system as an ontology. To consider it as a
classification system it would be necessary to share a representation of
contexts by all the users. This paper is proposing the use of folksonomies and
network theory to devise a new concept: a ""Folksodriven Structure Network"" to
represent folksonomies. This paper proposed and analyzed the network structure
of Folksodriven tags thought as folsksonomy tags suggestions for the user on a
dataset built on chosen websites. It is observed that the Folksodriven Network
has relative low path lengths checking it with classic networking measures
(clustering coefficient). Experiment result shows it can facilitate
serendipitous discovery of content among users. Neat examples and clear
formulas can show how a ""Folksodriven Structure Network"" can be used to tackle
ontology mapping challenges."
"This paper proposes a number of explicit and implicit ratings in product
recommendation system for Business-to-customer e-commerce purposes. The system
recommends the products to a new user. It depends on the purchase pattern of
previous users whose purchase pattern is close to that of a user who asks for a
recommendation. The system is based on weighted cosine similarity measure to
find out the closest user profile among the profiles of all users in database.
It also implements Association rule mining rule in recommending the products.
Also, this product recommendation system takes into consideration the time of
transaction of purchasing the items, thus eliminating sequence recognition
problem. Experimental result shows for implicit rating, the proposed method
gives acceptable performance in recommending the products. It also shows
introduction of association rule improves the performance measure of
recommendation system."
"Search Engine has become a major tool for searching any information from the
World Wide Web (WWW). While searching the huge digital library available in the
WWW, every effort is made to retrieve the most relevant results. But in WWW
majority of the Web pages are in HTML format and there are no such tags which
tells the crawler to find any specific domain. To find more relevant result we
use Ontology for that particular domain. If we are working with multiple
domains then we use multiple ontologies. Now in order to design a domain
specific search engine for multiple domains, crawler must crawl through the
domain specific Web pages in the WWW according to the predefined ontologies."
"We consider a decentralized detection problem in a power-constrained wireless
sensor networks (WSNs), in which a number of sensor nodes collaborate to detect
the presence of a deterministic vector signal. The signal to be detected is
assumed known \emph{a priori}. Given a constraint on the total amount of
transmit power, we investigate the optimal linear precoding design for each
sensor node. More specifically, in order to achieve the best detection
performance, shall sensor nodes transmit their raw data to the fusion center
(FC), or transmit compressed versions of their original data? The optimal power
allocation among sensors is studied as well. Also, assuming a fixed total
transmit power, we examine how the detection performance behaves with the
number of sensors in the network. A new concept ""detection outage"" is proposed
to quantify the reliability of the overall detection system. Finally,
decentralized detection with unknown signals is studied. Numerical results are
conducted to corroborate our theoretical analysis and to illustrate the
performance of the proposed algorithm."
"This paper presents a framework for increasing the relevancy of the web pages
retrieved by the search engine. The approach introduces a Predictive
Prefetching Engine (PPE) which makes use of various data mining algorithms on
the log maintained by the search engine. The underlying premise of the approach
is that in the case of cluster accesses, the next pages requested by users of
the Web server are typically based on the current and previous pages requested.
Based on same, rules are drawn which then lead the path for prefetching the
desired pages. To carry out the desired task of prefetching the more relevant
pages, agents have been introduced."
"Web Usage mining is a very important tool to extract the hidden business
intelligence data from large databases. The extracted information provides the
organizations with the ability to produce results more effectively to improve
their businesses and increasing of sales. Co-clustering is a powerful
bipartition technique which identifies group of users associated to group of
web pages. These associations are quantified to reveal the users' interest in
the different web pages' clusters. In this paper, Fuzzy Co-Clustering algorithm
is proposed for clickstream data to identify the subset of users of similar
navigational behavior /interest over a subset of web pages of a website.
Targeting the users group for various promotional activities is an important
aspect of marketing practices. Experiments are conducted on real dataset to
prove the efficiency of proposed algorithm. The results and findings of this
algorithm could be used to enhance the marketing strategy for directing
marketing, advertisements for web based businesses and so on."
"The vehicles for official knowledge, as well as university libraries, suffer
from an increasingly visible lack of interest. This is due to the advent of
fully digital practices. By studying the psychological and cognitive models in
information retrieval initiated in the 1980s, it is possible to use these
theories and apply them practically to the Information Retrieval System, taking
into account the requirements of virtual libraries. New metadata standards
along with modern tools that help managing references should help automating
the process of scientific research. We offer a practical implementation of the
given theories to test them when they are applied to the information retrieval
in computer sciences. This case under study will highlight good practices in
gleaning and harvesting scientific literature."
"The process of creating modern Web media experiences is challenged by the
need to adapt the content and presentation choices to dynamic real-time
fluctuations of user interest across multiple audiences. We introduce FAME - a
Framework for Agile Media Experiences - which addresses this scalability
problem. FAME allows media creators to define abstract page models that are
subsequently transformed into real experiences through algorithmic
experimentation. FAME's page models are hierarchically composed of simple
building blocks, mirroring the structure of most Web pages. They are resolved
into concrete page instances by pluggable algorithms which optimize the pages
for specific business goals. Our framework allows retrieving dynamic content
from multiple sources, defining the experimentation's degrees of freedom, and
constraining the algorithmic choices. It offers an effective separation of
concerns in the media creation process, enabling multiple stakeholders with
profoundly different skills to apply their crafts and perform their duties
independently, composing and reusing each other's work in modular ways."
"Click-through data has been used in various ways in Web search such as
estimating relevance between documents and queries. Since only search snippets
are perceived by users before issuing any clicks, the relevance induced by
clicks are usually called \emph{perceived relevance} which has proven to be
quite useful for Web search. While there is plenty of click data for popular
queries, very little information is available for unpopular tail ones. These
tail queries take a large portion of the search volume but search accuracy for
these queries is usually unsatisfactory due to data sparseness such as limited
click information. In this paper, we study the problem of modeling perceived
relevance for queries without click-through data. Instead of relying on users'
click data, we carefully design a set of snippet features and use them to
approximately capture the perceived relevance. We study the effectiveness of
this set of snippet features in two settings: (1) predicting perceived
relevance and (2) enhancing search engine ranking. Experimental results show
that our proposed model is effective to predict the relative perceived
relevance of Web search results. Furthermore, our proposed snippet features are
effective to improve search accuracy for longer tail queries without
click-through data."
"We present a methodology combining surface NLP and Machine Learning
techniques for ranking asbtracts and generating summaries based on annotated
corpora. The corpora were annotated with meta-semantic tags indicating the
category of information a sentence is bearing (objective, findings, newthing,
hypothesis, conclusion, future work, related work). The annotated corpus is fed
into an automatic summarizer for query-oriented abstract ranking and multi-
abstract summarization. To adapt the summarizer to these two tasks, two novel
weighting functions were devised in order to take into account the distribution
of the tags in the corpus. Results, although still preliminary, are encouraging
us to pursue this line of work and find better ways of building IR systems that
can take into account semantic annotations in a corpus."
"In information retrieval research; Genetic Algorithms (GA) can be used to
find global solutions in many difficult problems. This study used different
similarity measures (Dice, Inner Product) in the VSM, for each similarity
measure we compared ten different GA approaches based on different fitness
functions, different mutations and different crossover strategies to find the
best strategy and fitness function that can be used when the data collection is
the Arabic language. Our results shows that the GA approach which uses
one-point crossover operator, point mutation and Inner Product similarity as a
fitness function is the best IR system in VSM."
"The Use of genetic algorithms in the Information retrieval (IR) area,
especially in optimizing a user query in Arabic data collections is presented
in this paper. Very little research has been carried out on Arabic text
collections. Boolean model have been used in this research. To optimize the
query using GA we used different fitness functions, different mutation
strategies to find which is the best strategy and fitness function that can be
used with Boolean model when the data collection is the Arabic language. Our
results show that the best GA strategy for the Boolean model is the GA (M2,
Precision) method."
"Searching is an important tool of information gathering, if information is in
the form of picture than it play a major role to take quick action and easy to
memorize. This is a human tendency to retain more picture than text. The
complexity and the occurrence of variety of query can give variation in result
and provide the humans to learn something new or get confused. This paper
presents a development of a framework that will focus on recourse
identification for the user so that they can get faster access with accurate &
concise results on time and analysis of the change that is evident as the
scenario changes from text to picture retrieval. This paper also provides a
glimpse how to get accurate picture information in advance and extended
technologies searching framework. The new challenges and design techniques of
picture retrieval systems are also suggested in this paper."
"As the amount of online document increases, the demand for document
classification to aid the analysis and management of document is increasing.
Text is cheap, but information, in the form of knowing what classes a document
belongs to, is expensive. The main purpose of this paper is to explain the
expectation maximization technique of data mining to classify the document and
to learn how to improve the accuracy while using semi-supervised approach.
Expectation maximization algorithm is applied with both supervised and
semi-supervised approach. It is found that semi-supervised approach is more
accurate and effective. The main advantage of semi supervised approach is
""Dynamically Generation of New Class"". The algorithm first trains a classifier
using the labeled document and probabilistically classifies the unlabeled
documents. The car dataset for the evaluation purpose is collected from UCI
repository dataset in which some changes have been done from our side."
"This paper describes our work which is based on discovering context for text
document categorization. The document categorization approach is derived from a
combination of a learning paradigm known as relation extraction and an
technique known as context discovery. We demonstrate the effectiveness of our
categorization approach using reuters 21578 dataset and synthetic real world
data from sports domain. Our experimental results indicate that the learned
context greatly improves the categorization performance as compared to
traditional categorization approaches."
"The semantic analysis of documents is a domain of intense research at
present. The works in this domain can take several directions and touch several
levels of granularity. In the present work we are exactly interested in the
thematic analysis of the textual documents. In our approach, we suggest
studying the variation of the theme relevance within a text to identify the
major theme and all the minor themes evoked in the text. This allows us at the
second level of analysis to identify the relations of thematic associations in
a textual corpus. Through the identification and the analysis of these
association relations we suggest generating thematic paths allowing users,
within the frame work of information search system, to explore the corpus
according to their themes of interest and to discover new knowledge by
navigating in the thematic association relations."
"We present a simple web search engine for indexing and searching html
documents using python programming language. Because python is well known for
its simple syntax and strong support for main operating systems, we hope it
will be beneficial for learning information retrieval techniques, especially
web search engine technology."
"Nowadays, with the increasing number of integrated data sources, there is a
real trend to personalize mediation systems to improve user satisfaction. To
make these systems user sensitive, we propose a personalization platform called
2P-Med. 2P-Med allows personalizing any mediation system used in any domain
following a cyclic process. The process includes building and managing adequate
user profiles and sources profiles, content and quality matching, source
selection, adapting the mediator responses to user preferences and handling
user feedbacks. In this paper, we describe 2P-Med architecture and highlight
its main functionalities. We also illustrate the operation of the platform
through personalizing source selection in a travel planning assistant."
"The first Information Retrieval Education through Experimentation track
(EIREX 2010) was run at the University Carlos III of Madrid, during the 2010
spring semester. EIREX 2010 is the first in a series of experiments designed to
foster new Information Retrieval (IR) education methodologies and resources,
with the specific goal of teaching undergraduate IR courses from an
experimental perspective. For an introduction to the motivation behind the
EIREX experiments, see the first sections of [Urbano et al., 2011]. For
information on other editions of EIREX and related data, see the website at
http://ir.kr.inf.uc3m.es/eirex/. The EIREX series have the following goals: a)
to help students get a view of the Information Retrieval process as they would
find it in a real-world scenario, either industrial or academic; b) to make
students realize the importance of laboratory experiments in Computer Science
and have them initiated in their execution and analysis; c) to create a public
repository of resources to teach Information Retrieval courses; d) to seek the
collaboration and active participation of other Universities in this endeavor.
This overview paper summarizes the results of the EIREX 2010 track, focusing on
the creation of the test collection and the analysis to assess its reliability."
"Query sensitive summarization aims at providing the users with the summary of
the contents of single or multiple web pages based on the search query. This
paper proposes a novel idea of generating a comparative summary from a set of
URLs from the search result. User selects a set of web page links from the
search result produced by search engine. Comparative summary of these selected
web sites is generated. This method makes use of HTML DOM tree structure of
these web pages. HTML documents are segmented into set of concept blocks.
Sentence score of each concept block is computed with respect to the query and
feature keywords. The important sentences from the concept blocks of different
web pages are extracted to compose the comparative summary on the fly. This
system reduces the time and effort required for the user to browse various web
sites to compare the information. The comparative summary of the contents would
help the users in quick decision making."
"The main objective of higher education institutions is to provide quality
education to its students. One way to achieve highest level of quality in
higher education system is by discovering knowledge for prediction regarding
enrolment of students in a particular course, alienation of traditional
classroom teaching model, detection of unfair means used in online examination,
detection of abnormal values in the result sheets of the students, prediction
about students' performance and so on. The knowledge is hidden among the
educational data set and it is extractable through data mining techniques.
Present paper is designed to justify the capabilities of data mining techniques
in context of higher education by offering a data mining model for higher
education system in the university. In this research, the classification task
is used to evaluate student's performance and as there are many approaches that
are used for data classification, the decision tree method is used here. By
this task we extract knowledge that describes students' performance in end
semester examination. It helps earlier in identifying the dropouts and students
who need special attention and allow the teacher to provide appropriate
advising/counseling. Keywords-Educational Data Mining (EDM); Classification;
Knowledge Discovery in Database (KDD); ID3 Algorithm."
"Now-a-days the amount of data stored in educational database increasing
rapidly. These databases contain hidden information for improvement of
students' performance. The performance in higher education in India is a
turning point in the academics for all students. This academic performance is
influenced by many factors, therefore it is essential to develop predictive
data mining model for students' performance so as to identify the difference
between high learners and slow learners student. In the present investigation,
an experimental methodology was adopted to generate a database. The raw data
was preprocessed in terms of filling up missing values, transforming values in
one form into another and relevant attribute/ variable selection. As a result,
we had 300 student records, which were used for by Byes classification
prediction model construction. Keywords- Data Mining, Educational Data Mining,
Predictive Model, Classification."
"Every data has a lot of hidden information. The processing method of data
decides what type of information data produce. In India education sector has a
lot of data that can produce valuable information. This information can be used
to increase the quality of education. But educational institution does not use
any knowledge discovery process approach on these data. Information and
communication technology puts its leg into the education sector to capture and
compile low cost information. Now a day a new research community, educational
data mining (EDM), is growing which is intersection of data mining and
pedagogy. In this paper we present roadmap of research done in EDM in various
segment of education sector."
"The second Information Retrieval Education through EXperimentation track
(EIREX 2011) was run at the University Carlos III of Madrid, during the 2011
spring semester. EIREX 2011 is the second in a series of experiments designed
to foster new Information Retrieval (IR) education methodologies and resources,
with the specific goal of teaching undergraduate IR courses from an
experimental perspective. For an introduction to the motivation behind the
EIREX experiments, see the first sections of [Urbano et al., 2011a]. For
information on other editions of EIREX and related data, see the website at
http://ir.kr.inf.uc3m.es/eirex/. The EIREX series have the following goals: a)
to help students get a view of the Information Retrieval process as they would
find it in a real-world scenario, either industrial or academic; b) to make
students realize the importance of laboratory experiments in Computer Science
and have them initiated in their execution and analysis; c) to create a public
repository of resources to teach Information Retrieval courses; d) to seek the
collaboration and active participation of other Universities in this endeavor.
This overview paper summarizes the results of the EIREX 2011 track, focusing on
the creation of the test collection and the analysis to assess its reliability."
"We analyze the state of the art of content-based retrieval in Earth
observation image archives focusing on complete systems showing promise for
operational implementation. The different paradigms at the basis of the main
system families are introduced. The approaches taken are analyzed, focusing in
particular on the phases after primitive feature extraction. The solutions
envisaged for the issues related to feature simplification and synthesis,
indexing, semantic labeling are reviewed. The methodologies for query
specification and execution are analyzed."
"Supervised mapping methods project multi-dimensional labeled data onto a
2-dimensional space attempting to preserve both data similarities and topology
of classes. Supervised mappings are expected to help the user to understand the
underlying original class structure and to classify new data visually. Several
methods have been designed to achieve supervised mapping, but many of them
modify original distances prior to the mapping so that original data
similarities are corrupted and even overlapping classes tend to be separated
onto the map ignoring their original topology. We propose ClassiMap, an
alternative method for supervised mapping. Mappings come with distortions which
can be split between tears (close points mapped far apart) and false
neighborhoods (points far apart mapped as neighbors). Some mapping methods
favor the former while others favor the latter. ClassiMap switches between such
mapping methods so that tears tend to appear between classes and false
neighborhood within classes, better preserving classes' topology. We also
propose two new objective criteria instead of the usual subjective visual
inspection to perform fair comparisons of supervised mapping methods. ClassiMap
appears to be the best supervised mapping method according to these criteria in
our experiments on synthetic and real datasets."
"Probabilistic models require the notion of event space for defining a
probability measure. An event space has a probability measure which ensues the
Kolmogorov axioms. However, the probabilities observed from distinct sources,
such as that of relevance of documents, may not admit a single event space thus
causing some issues. In this article, some results are introduced for ensuring
whether the observed prob- abilities of relevance of documents admit a single
event space. More- over, an alternative framework of probability is introduced,
thus chal- lenging the use of classical probability for ranking documents. Some
reflections on the convenience of extending the classical probabilis- tic
retrieval toward a more general framework which encompasses the issues are
made."
"This thesis consists of four parts: - An analysis of the core functions and
the prerequisites for recommender systems in an industrial context: we identify
four core functions for recommendation systems: Help do Decide, Help to
Compare, Help to Explore, Help to Discover. The implementation of these
functions has implications for the choices at the heart of algorithmic
recommender systems. - A state of the art, which deals with the main techniques
used in automated recommendation system: the two most commonly used algorithmic
methods, the K-Nearest-Neighbor methods (KNN) and the fast factorization
methods are detailed. The state of the art presents also purely content-based
methods, hybridization techniques, and the classical performance metrics used
to evaluate the recommender systems. This state of the art then gives an
overview of several systems, both from academia and industry (Amazon, Google
...). - An analysis of the performances and implications of a recommendation
system developed during this thesis: this system, Reperio, is a hybrid
recommender engine using KNN methods. We study the performance of the KNN
methods, including the impact of similarity functions used. Then we study the
performance of the KNN method in critical uses cases in cold start situation. -
A methodology for analyzing the performance of recommender systems in
industrial context: this methodology assesses the added value of algorithmic
strategies and recommendation systems according to its core functions."
"Recommender systems require their recommendation algorithms to be accurate,
scalable and should handle very sparse training data which keep changing over
time. Inspired by ant colony optimization, we propose a novel collaborative
filtering scheme: Ant Collaborative Filtering that enjoys those favorable
characteristics above mentioned. With the mechanism of pheromone transmission
between users and items, our method can pinpoint most relative users and items
even in face of the sparsity problem. By virtue of the evaporation of existing
pheromone, we capture the evolution of user preference over time. Meanwhile,
the computation complexity is comparatively small and the incremental update
can be done online. We design three experiments on three typical recommender
systems, namely movie recommendation, book recommendation and music
recommendation, which cover both explicit and implicit rating data. The results
show that the proposed algorithm is well suited for real-world recommendation
scenarios which have a high throughput and are time sensitive."
"The Bing Bang of the Internet in the early 90's increased dramatically the
number of images being distributed and shared over the web. As a result, image
information retrieval systems were developed to index and retrieve image files
spread over the Internet. Most of these systems are keyword-based which search
for images based on their textual metadata; and thus, they are imprecise as it
is vague to describe an image with a human language. Besides, there exist the
content-based image retrieval systems which search for images based on their
visual information. However, content-based type systems are still immature and
not that effective as they suffer from low retrieval recall/precision rate.
This paper proposes a new hybrid image information retrieval model for indexing
and retrieving web images published in HTML documents. The distinguishing mark
of the proposed model is that it is based on both graphical content and textual
metadata. The graphical content is denoted by color features and color
histogram of the image; while textual metadata are denoted by the terms that
surround the image in the HTML document, more particularly, the terms that
appear in the tags p, h1, and h2, in addition to the terms that appear in the
image's alt attribute, filename, and class-label. Moreover, this paper presents
a new term weighting scheme called VTF-IDF short for Variable Term
Frequency-Inverse Document Frequency which unlike traditional schemes, it
exploits the HTML tag structure and assigns an extra bonus weight for terms
that appear within certain particular HTML tags that are correlated to the
semantics of the image. Experiments conducted to evaluate the proposed IR model
showed a high retrieval precision rate that outpaced other current models."
"With the advent of the Internet, a new era of digital information exchange
has begun. Currently, the Internet encompasses more than five billion online
sites and this number is exponentially increasing every day. Fundamentally,
Information Retrieval (IR) is the science and practice of storing documents and
retrieving information from within these documents. Mathematically, IR systems
are at the core based on a feature vector model coupled with a term weighting
scheme that weights terms in a document according to their significance with
respect to the context in which they appear. Practically, Vector Space Model
(VSM), Term Frequency (TF), and Inverse Term Frequency (IDF) are among other
long-established techniques employed in mainstream IR systems. However, present
IR models only target generic-type text documents, in that, they do not
consider specific formats of files such as HTML web documents. This paper
proposes a new semantic-sensitive web information retrieval model for HTML
documents. It consists of a vector model called SWVM and a weighting scheme
called BTF-IDF, particularly designed to support the indexing and retrieval of
HTML web documents. The chief advantage of the proposed model is that it
assigns extra weights for terms that appear in certain pre-specified HTML tags
that are correlated to the semantics of the document. Additionally, the model
is semantic-sensitive as it generates synonyms for every term being indexed and
later weights them appropriately to increase the likelihood of retrieving
documents with similar context but different vocabulary terms. Experiments
conducted, revealed a momentous enhancement in the precision of web IR systems
and a radical increase in the number of relevant documents being retrieved. As
further research, the proposed model is to be upgraded so as to support the
indexing and retrieval of web images in multimedia-rich web documents."
"The World Wide Web caters to the needs of billions of users in heterogeneous
groups. Each user accessing the World Wide Web might have his / her own
specific interest and would expect the web to respond to the specific
requirements. The process of making the web to react in a customized manner is
achieved through personalization. This paper proposes a novel model for
extracting keywords from a web page with personalization being incorporated
into it. The keyword extraction problem is approached with the help of web page
segmentation which facilitates in making the problem simpler and solving it
effectively. The proposed model is implemented as a prototype and the
experiments conducted on it empirically validate the model's efficiency."
"The Arabic language is a complex language; it is different from Western
languages especially at the morphological and spelling variations. Indeed, the
performance of information retrieval systems in the Arabic language is still a
problem. For this reason, we are interested in studying the performance of the
most famous search engine, which is a Google Desktop, while searching in Arabic
language documents. Then, we propose an update to the Google Desktop to take
into consideration in search the Arabic words that have the same root. After
that, we evaluate the performance of the Google Desktop in this context. Also,
we are interested in evaluation the performance of peer-to-peer application in
two ways. The first one uses a simple indexation that indexes Arabic documents
without taking in consideration the root of words. The second way takes in
consideration the roots in the indexation of Arabic documents. This evaluation
is done by using a corpus of ten thousand documents and one hundred different
queries."
"Search engine returns thousands of web pages for a single user query, in
which most of them are not relevant. In this context, effective information
retrieval from the expanding web is a challenging task, in particular, if the
query is ambiguous. The major question arises here is that how to get the
relevant pages for an ambiguous query. We propose an approach for the effective
result of an ambiguous query by forming community vector based on association
concept of data minning using vector space model and the freedictionary. We
develop clusters by computing the similarity between community vectors and
document vectors formed from the extracted web pages by the search engine. We
use Gensim package to implement the algorithm because of its simplicity and
robust nature. Analysis shows that our approach is an effective way to form
clusters for an ambiguous query."
"With the increasing popularity of location-based social media applications
and devices that automatically tag generated content with locations, large
repositories of collaborative geo-referenced data are appearing on-line.
Efficiently extracting user preferences from these data to determine what
information to recommend is challenging because of the sheer volume of data as
well as the frequency of updates. Traditional recommender systems focus on the
interplay between users and items, but ignore contextual parameters such as
location. In this paper we take a geospatial approach to determine locational
preferences and similarities between users. We propose to capture the
geographic context of user preferences for items using a relational graph,
through which we are able to derive many new and state-of-the-art
recommendation algorithms, including combinations of them, requiring changes
only in the definition of the edge weights. Furthermore, we discuss several
solutions for cold-start scenarios. Finally, we conduct experiments using two
real-world datasets and provide empirical evidence that many of the proposed
algorithms outperform existing location-aware recommender algorithms."
"Due to the development of social media technology, it becomes easier for
users to gather together to form groups. Take the Last.fm for example, users
can join groups they may be interested where they can share their loved songs
and discuss topics about songs and singers. However, the number of groups grows
over time, users need effective groups recommendations in order to meet more
like-minded users."
"The corpus reported in this paper was developed for the evaluation of a
domain-specific Text to Knowledge Mapping (TKM) prototype. The TKM prototype
operates on the basis of both a combinatory categorical grammar (CCG)
linguistic model and a knowledge model that consists of three layers: ontology,
qualitative and quantitative layers. In the course of this evaluation it was
necessary to populate these initial models with lexical items and semantic
relations. Both elements, the lexicon and semantic relations, are meant to
reflect the domain of the prototype; hence both had to be extracted from the
corpus. While dealing with the lexicon was straight forward, the identification
and extraction of appropriate semantic relations was much more involved. It was
necessary, therefore, to manually develop a conceptual structure for the domain
which was then used to formulate a domain-specific framework of semantic
relations. The conceptual structure was developed using the Cmap tool of IHMC.
The framework of semantic relations- that has resulted from this study
consisted of 55 relations, out of which 42 have inverse relations."
"Web 2.0 applications like Twitter or Facebook create a continuous stream of
information. This demands new ways of analysis in order to offer insight into
this stream right at the moment of the creation of the information, because
lots of this data is only relevant within a short period of time. To address
this problem real time search engines have recently received increased
attention. They take into account the continuous flow of information
differently than traditional web search by incorporating temporal and social
features, that describe the context of the information during its creation.
Standard approaches where data first get stored and then is processed from a
peristent storage suffer from latency. We want to address the fluent and rapid
nature of text stream by providing an event based approach that analyses
directly the stream of information. In a first step we want to define the
difference between real time search and traditional search to clarify the
demands in modern text filtering. In a second step we want to show how event
based features can be used to support the tasks of real time search engines.
Using the example of Twitter we present in this paper a way how to combine an
event based approach with text mining and information filtering concepts in
order to classify incoming information based on stream features. We calculate
stream dependant features and feed them into a neural network in order to
classify the text streams. We show the separative capabilities of event based
features as the foundation for a real time search engine."
"Performance comparisons between File Signatures and Inverted Files for text
retrieval have previously shown several significant shortcomings of file
signatures relative to inverted files. The inverted file approach underpins
most state-of-the-art search engine algorithms, such as Language and
Probabilistic models. It has been widely accepted that traditional file
signatures are inferior alternatives to inverted files. This paper describes
TopSig, a new approach to the construction of file signatures. Many advances in
semantic hashing and dimensionality reduction have been made in recent times,
but these were not so far linked to general purpose, signature file based,
search engines. This paper introduces a different signature file approach that
builds upon and extends these recent advances. We are able to demonstrate
significant improvements in the performance of signature file based indexing
and retrieval, performance that is comparable to that of state of the art
inverted file based systems, including Language models and BM25. These findings
suggest that file signatures offer a viable alternative to inverted files in
suitable settings and from the theoretical perspective it positions the file
signatures model in the class of Vector Space retrieval models."
"Recent work in machine learning for information extraction has focused on two
distinct sub-problems: the conventional problem of filling template slots from
natural language text, and the problem of wrapper induction, learning simple
extraction procedures (""wrappers"") for highly structured text such as Web
pages. For suitable regular domains, existing wrapper induction algorithms can
efficiently learn wrappers that are simple and highly accurate, but the
regularity bias of these algorithms makes them unsuitable for most conventional
information extraction tasks. This paper describes a new approach for wrapping
semistructured Web pages. The wrapper is capable of learning how to extract
relevant information from Web resources on the basis of user supplied examples.
It is based on inductive learning techniques as well as fuzzy logic rules.
Experimental results show that our approach achieves noticeably better
precision and recall coefficient performance measures than SoftMealy, which is
one of the most recently reported wrappers capable of wrapping semi-structured
Web pages with missing attributes, multiple attributes, variant attribute
permutations, exceptions, and typos."
"In this paper we present a synthesis of work performed on tow information
retrieval models: Bayesian network information retrieval model witch encode
(in) dependence relation between terms and possibilistic network information
retrieval model witch make use of necessity and possibility measures to
represent the fuzziness of pertinence measure. It is known that the use of a
general Bayesian network methodology as the basis for an IR system is difficult
to tackle. The problem mainly appears because of the large number of variables
involved and the computational efforts needed to both determine the
relationships between variables and perform the inference processes. To resolve
these problems, many models have been proposed such as BNR model. Generally,
Bayesian network models doesn't consider the fuzziness of natural language in
the relevance measure of a document to a given query and possibilistic models
doesn't undertake the dependence relations between terms used to index
documents. As a first solution we propose a hybridization of these two models
in one that will undertake both the relationship between terms and the
intrinsic fuzziness of natural language. We believe that the translation of
Bayesian network model from the probabilistic framework to possibilistic one
will allow a performance improvement of BNRM."
"In this paper we present a short survey of fuzzy and Semantic approaches to
Knowledge Extraction. The goal of such approaches is to define flexible
Knowledge Extraction Systems able to deal with the inherent vagueness and
uncertainty of the Extraction process. It has long been recognised that
interactivity improves the effectiveness of Knowledge Extraction systems.
Novice user's queries is the most natural and interactive medium of
communication and recent progress in recognition is making it possible to build
systems that interact with the user. However, given the typical novice user's
queries submitted to Knowledge Extraction systems, it is easy to imagine that
the effects of goal recognition errors in novice user's queries must be
severely destructive on the system's effectiveness. The experimental work
reported in this paper shows that the use of classical Knowledge Extraction
techniques for novice user's query processing is robust to considerably high
levels of goal recognition errors. Moreover, both standard relevance feedback
and pseudo relevance feedback can be effectively employed to improve the
effectiveness of novice user's query processing."
"Web search engines have become the dominant tools for finding information on
the Internet. Due to their popularity, users apply them to a wide range of
search needs, from simple look-ups to rather complex information tasks. This
paper presents the results of a study to investigate the characteristics of
these complex information needs in the context of Web search engines. The aim
of the study is to find out more about (1) what makes complex search tasks
distinct from simple tasks and if it is possible to find simple measures for
describing their complexity, (2) if search success for a task can be predicted
by means of unique measures, and (3) if successful searchers show a different
behavior than unsuccessful ones. The study includes 60 people who carried out a
set of 12 search tasks with current commercial search engines. Their behavior
was logged with the Search-Logger tool. The results confirm that complex tasks
show significantly different characteristics than simple tasks. Yet it seems to
be difficult to distinguish successful from unsuccessful search behaviors. Good
searchers can be differentiated from bad searchers by means of measurable
parameters. The implications of these findings for search engine vendors are
discussed."
"In this paper we present a short survey of fuzzy and Semantic approaches to
Knowledge Extraction. The goal of such approaches is to define flexible
Knowledge Extraction Systems able to deal with the inherent vagueness and
uncertainty of the Extraction process. In this survey we address if and how
some approaches met their goal."
"This paper presents a method to measure the similarity between different
fuzzy concepts in order to optimize Semantic networks. The problem approached
is the minimization of the time of research and identification of user's
Objects and Goals. Indeed, it concerns to determine to each instant the
totality of Objects (respectively Goals) among which one can identify rapidly
the most satisfactory for the user's Object and Goal. Alone Objects and most
similar Goals to Objects and researched Goals of the viewpoint of attribute
values will be processed, what will avoid the analysis of all Objects and
system Goals far of needs of the user."
"Internet advertising is a fast growing business which has proved to be
significantly important in digital economics. It is vitally important for both
web search engines and online content providers and publishers because web
advertising provides them with major sources of revenue. Its presence is
increasingly important for the whole media industry due to the influence of the
Web. For advertisers, it is a smarter alternative to traditional marketing
media such as TVs and newspapers. As the web evolves and data collection
continues, the design of methods for more targeted, interactive, and friendly
advertising may have a major impact on the way our digital economy evolves, and
to aid societal development.
  Towards this goal mathematically well-grounded Computational Advertising
methods are becoming necessary and will continue to develop as a fundamental
tool towards the Web. As a vibrant new discipline, Internet advertising
requires effort from different research domains including Information
Retrieval, Machine Learning, Data Mining and Analytic, Statistics, Economics,
and even Psychology to predict and understand user behaviours. In this paper,
we provide a comprehensive survey on Internet advertising, discussing and
classifying the research issues, identifying the recent technologies, and
suggesting its future directions. To have a comprehensive picture, we first
start with a brief history, introduction, and classification of the industry
and present a schematic view of the new advertising ecosystem. We then
introduce four major participants, namely advertisers, online publishers, ad
exchanges and web users; and through analysing and discussing the major
research problems and existing solutions from their perspectives respectively,
we discover and aggregate the fundamental problems that characterise the
newly-formed research field and capture its potential future prospects."
"This paper presents a method of optimization, based on both Bayesian Analysis
technical and Galois Lattice of Fuzzy Semantic Network. The technical System we
use learns by interpreting an unknown word using the links created between this
new word and known words. The main link is provided by the context of the
query. When novice's query is confused with an unknown verb (goal) applied to a
known noun denoting either an object in the ideal user's Network or an object
in the user's Network, the system infer that this new verb corresponds to one
of the known goal. With the learning of new words in natural language as the
interpretation, which was produced in agreement with the user, the system
improves its representation scheme at each experiment with a new user and, in
addition, takes advantage of previous discussions with users. The semantic Net
of user objects thus obtained by learning is not always optimal because some
relationships between couple of user objects can be generalized and others
suppressed according to values of forces that characterize them. Indeed, to
simplify the obtained Net, we propose to proceed to an Inductive Bayesian
Analysis, on the Net obtained from Galois lattice. The objective of this
analysis can be seen as an operation of filtering of the obtained descriptive
graph."
"Search engines are the preferred tools for finding information on the Web.
They are advancing to be the common helpers to answer any of our search needs.
We use them to carry out simple look-up tasks and also to work on rather time
consuming and more complex search tasks. Yet, we do not know very much about
the user performance while carrying out those tasks -- especially not for
ordinary users. The aim of this study was to get more insight into whether Web
users manage to assess difficulty, time effort, query effort, and task outcome
of search tasks, and if their judging performance relates to task complexity.
Our study was conducted with a systematically selected sample of 56 people with
a wide demographic background. They carried out a set of 12 search tasks with
commercial Web search engines in a laboratory environment. The results confirm
that it is hard for normal Web users to judge the difficulty and effort to
carry out complex search tasks. The judgments are more reliable for simple
tasks than for complex ones. Task complexity is an indicator for judging
performance."
"In the last two decades, number of Higher Education Institutions (HEI) grows
rapidly in India. Since most of the institutions are opened in private mode
therefore, a cut throat competition rises among these institutions while
attracting the student to got admission. This is the reason for institutions to
focus on the strength of students not on the quality of education. This paper
presents a data mining application to generate predictive models for
engineering student's dropout management. Given new records of incoming
students, the predictive model can produce short accurate prediction list
identifying students who tend to need the support from the student dropout
program most. The results show that the machine learning algorithm is able to
establish effective predictive model from the existing student dropout data."
"Recommender systems can change our life a lot and help us select suitable and
favorite items much more conveniently and easily. As a consequence, various
kinds of algorithms have been proposed in last few years to improve the
performance. However, all of them face one critical problem: data sparsity. In
this paper, we proposed a two-step recommendation algorithm via iterative local
least squares (ILLS). Firstly, we obtain the ratings matrix which is
constructed via users' behavioral records, and it is normally very sparse.
Secondly, we preprocess the ""ratings"" matrix through ProbS which can convert
the sparse data to a dense one. Then we use ILLS to estimate those missing
values. Finally, the recommendation list is generated. Experimental results on
the three datasets: MovieLens, Netflix, RYM, suggest that the proposed method
can enhance the algorithmic accuracy of AUC. Especially, it performs much
better in dense datasets. Furthermore, since this methods can improve those
missing value more accurately via iteration which might show light in
discovering those inactive users' purchasing intention and eventually solving
cold-start problem."
"During the last three years we conducted several information retrieval
evaluation series with more than 180 LIS students who made relevance
assessments on the outcomes of three specific retrieval services. In this study
we do not focus on the retrieval performance of our system but on the relevance
assessments and the inter-assessor reliability. To quantify the agreement we
apply Fleiss' Kappa and Krippendorff's Alpha. When we compare these two
statistical measures on average Kappa values were 0.37 and Alpha values 0.15.
We use the two agreement measures to drop too unreliable assessments from our
data set. When computing the differences between the unfiltered and the
filtered data set we see a root mean square error between 0.02 and 0.12. We see
this as a clear indicator that disagreement affects the reliability of
retrieval evaluations. We suggest not to work with unfiltered results or to
clearly document the disagreement rates."
"This article deals with the semantic Web and ontologies. It addresses the
issue of the classification of multilingual Web documents, based on domain
ontology. The objective is being able, using a model, to classify documents in
different languages. We will try to solve this problematic using two different
approaches. The two approaches will have two elementary stages: the creation of
the model using machine learning algorithms on a labeled corpus, then the
classification of documents after detecting their languages and mapping their
terms into the concepts of the language of reference (English). But each one
will deal with the multilingualism with a different approach. One supposes the
ontology is monolingual, whereas the other considers it multilingual. To show
the feasibility and the importance of our work, we implemented it on a domain
that attracts nowadays a lot of attention from the data mining community: the
biomedical domain. The selected documents are from the biomedical benchmark
corpus Ohsumed, and the associated ontology is the thesaurus MeSH (Medical
Subject Headings). The main idea in our work is a new document representation,
the masterpiece of all good classification, based on concept. The experimental
results show that the recommended ideas are promising."
"Web services are playing an important role in e-business and e-commerce
applications. As web service applications are interoperable and can work on any
platform, large scale distributed systems can be developed easily using web
services. Finding most suitable web service from vast collection of web
services is very crucial for successful execution of applications. Traditional
web service discovery approach is a keyword based search using UDDI. Various
other approaches for discovering web services are also available. Some of the
discovery approaches are syntax based while other are semantic based. Having
system for service discovery which can work automatically is also the concern
of service discovery approaches. As these approaches are different, one
solution may be better than another depending on requirements. Selecting a
specific service discovery system is a hard task. In this paper, we give an
overview of different approaches for web service discovery described in
literature. We present a survey of how these approaches differ from each other."
"Search Engine is a Web-page retrieval tool. Nowadays Web searchers utilize
their time using an efficient search engine. To improve the performance of the
search engine, we are introducing a unique mechanism which will give Web
searchers more prominent search results. In this paper, we are going to discuss
a domain specific Web search prototype which will generate the predicted
Web-page list for user given search string using Boolean bit mask."
"Web search engines apply a variety of ranking signals to achieve user
satisfaction, i.e., results pages that provide the best-possible results to the
user. While these ranking signals implicitly consider credibility (e.g., by
measuring popularity), explicit measures of credibility are not applied. In
this chapter, credibility in Web search engines is discussed in a broad
context: credibility as a measure for including documents in a search engine's
index, credibility as a ranking signal, credibility in the context of universal
search results, and the possibility of using credibility as an explicit measure
for ranking purposes. It is found that while search engines-at least to a
certain extent-show credible results to their users, there is no fully
integrated credibility framework for Web search engines."
"Use of web pages providing unstructured information poses variety of problems
to the user, such as use of arbitrary formats, unsuitability for machine
processing and likely incompleteness of information. Structured data alleviates
these problems but we require more. Very often yellow page systems are
implemented using a centralized database. In some cases, human intermediaries
accessible over the phone network examine a centralized database and use their
reasoning ability to deal with the user's need for information. Scaling up such
systems is difficult. This paper explores an alternative - a highly distributed
system design meeting a variety of needs - considerably reducing efforts
required at a central organization, enabling large numbers of vendors to enter
information about their own products and services, enabling end-users to
contribute information such as their own ratings, using an ontology to describe
each domain of application in a flexible manner for uses foreseen and
unforeseen, enabling distributed search and mash-ups, use of vendor independent
standards, using reasoning to find the best matches to a given query,
geo-spatial reasoning and a simple, interactive, mobile application/interface.
We give importance to geo-spatial information and mobile applications because
of the very wide-spread use of mobile phones and their inherent ability to
provide some information about the current location of the user. We have
created a prototype using the Jena Toolkit and geo-spatial extensions to
SPARQL. We have tested this prototype by asking a group of typical users to use
it and to provide structured feedback. We have summarized this feedback in the
paper. We believe that the technology can be applied in many contexts in
addition to yellow page systems."
"As the use of web is increasing more day by day, the web users get easily
lost in the web's rich hyper structure. The main aim of the owner of the
website is to give the relevant information according their needs to the users.
We explained the Web mining is used to categorize users and pages by analyzing
user's behavior, the content of pages and then describe Web Structure mining.
This paper includes different Page Ranking algorithms and compares those
algorithms used for Information Retrieval. Different Page Rank based algorithms
like Page Rank (PR), WPR (Weighted Page Rank), HITS (Hyperlink Induced Topic
Selection), Distance Rank and EigenRumor algorithms are discussed and compared.
Simulation Interface has been designed for PageRank algorithm and Weighted
PageRank algorithm but PageRank is the only ranking algorithm on which Google
search engine works."
"The evaluation of web pages against a query is the pivot around which the
Information Retrieval domain revolves around. The context sensitive, semantic
evaluation of web pages is a non-trivial problem which needs to be addressed
immediately. This research work proposes a model to evaluate the web pages by
cumulating the segment scores which are computed by multidimensional evaluation
methodology. The model proposed is hybrid since it utilizes both the structural
semantics and content semantics in the evaluation process. The score of the web
page is computed in a bottom-up process by evaluating individual segment's
score through a multi-dimensional approach. The model incorporates an approach
for segment level annotation. The proposed model is prototyped for evaluation;
experiments conducted on the prototype confirm the model's efficiency in
semantic evaluation of pages."
"A lot of sensor network applications are data-driven. We believe that query
is the most preferred way to discover sensor services. Normally users are
unaware of available sensors. Thus users need to pose different types of query
over the sensor network to get the desired information. Even users may need to
input more complicated queries with higher levels of aggregations, and requires
more complex interactions with the system. As the users have no prior knowledge
of the sensor data or services our aim is to develop a visual query interface
where users can feed more user friendly queries and machine can understand
those. In this paper work, we have developed an Interactive visual query
interface for the users. To accomplish this we have considered several use
cases and we have derived graphical representation of query from their text
based format for those use case scenario. We have facilitated the user by
extracting class, subclass and properties from Ontology. To do so we have
parsed OWL file in the user interface and based upon the parsed information
users build visual query. Later on we have translated the visual query
languages into SPARQL query, a machine understandable format which helps the
machine to communicate with the underlying technology."
"We will report on the participation of GESIS at the first CHiC workshop
(Cultural Heritage in CLEF). Being held for the first time, no prior experience
with the new data set, a document dump of Europeana with ca. 23 million
documents, exists. The most prominent issues that arose from pretests with this
test collection were the very unspecific topics and sparse document
representations. Only half of the topics (26/50) contained a description and
the titles were usually short with just around two words. Therefore we focused
on three different term suggestion and query expansion mechanisms to surpass
the sparse topical description. We used two methods that build on concept
extraction from Wikipedia and on a method that applied co-occurrence statistics
on the available Europeana corpus. In the following paper we will present the
approaches and preliminary results from their assessments."
"TrackMeNot is a Firefox plugin with laudable intentions - protecting your
privacy. By issuing a customizable stream of random search queries on its
users' behalf, TrackMeNot surmises that enough search noise will prevent its
users' true query profiles from being discerned. However, we find that
clustering queries by semantic relatedness allows us to disentangle a
nontrivial subset of true user queries from TrackMeNot issued noise."
"The size of web has increased exponentially over the past few years with
thousands of documents related to a subject available to the user. With this
much amount of information available, it is not possible to take the full
advantage of the World Wide Web without having a proper framework to search
through the available data. This requisite organization can be done in many
ways. In this paper we introduce a combine approach to cluster the web pages
which first finds the frequent sets and then clusters the documents. These
frequent sets are generated by using Frequent Pattern growth technique. Then by
applying Fuzzy C- Means algorithm on it, we found clusters having documents
which are highly related and have similar features. We used Gensim package to
implement our approach because of its simplicity and robust nature. We have
compared our results with the combine approach of (Frequent Pattern growth,
K-means) and (Frequent Pattern growth, Cosine_Similarity). Experimental results
show that our approach is more efficient then the above two combine approach
and can handles more efficiently the serious limitation of traditional Fuzzy
C-Means algorithm, which is sensitiveto initial centroid and the number of
clusters to be formed."
"The term legal research generally refers to the process of identifying and
retrieving appropriate information necessary to support legal decision making
from past case records. At present, the process is mostly manual, but some
traditional technologies such as keyword searching are commonly used to speed
the process up. But a keyword search is not a comprehensive search to cater to
the requirements of legal research as the search result includes too many false
hits in terms of irrelevant case records. Hence the present generic tools
cannot be used to automate legal research.
  This paper presents a framework which was developed by combining several Text
Mining techniques to automate the process overcoming the difficulties in the
existing methods. Further, the research also identifies the possible
enhancements that could be done to enhance the effectiveness of the framework."
"Employers collect a large number of resumes from job portals, or from the
company's own website. These documents are used for an automated selection of
candidates satisfying the requirements and therefore reducing recruitment
costs. Various approaches for process documents have already been developed for
recruitment. In this paper we present an approach based on semantic annotation
of resumes for e-recruitment process. The most important task consists on
modelling the semantic content of these documents using ontology. The ontology
is built taking into account the most significant components of resumes
inspired from the structure of EUROPASS CV. This ontology is thereafter used to
annotate automatically the resumes."
"The majority of Semantic Web search engines retrieve information by focusing
on the use of concepts and relations restricted to the query provided by the
user. By trying to guess the implicit meaning between these concepts and
relations, probabilities are calculated to give the pages a score for ranking.
In this study, I propose a relation-based page rank algorithm to be used as a
Semantic Web search engine. Relevance is measured as the probability of finding
the connections made by the user at the time of the query, as well as the
information contained in the base knowledge of the Semantic Web environment. By
the use of ""virtual links"" between the concepts in a page, which are obtained
from the knowledge base, we can connect concepts and components of a page and
increase the probability score for a better ranking. By creating these
connections, this study also looks to eliminate the possibility of getting
results equal to zero, and to provide a tie-breaker solution when two or more
pages obtain the same score."
"Nowadays, the Web has become one of the most widespread platforms for
information change and retrieval. As it becomes easier to publish documents, as
the number of users, and thus publishers, increases and as the number of
documents grows, searching for information is turning into a cumbersome and
time-consuming operation. Due to heterogeneity and unstructured nature of the
data available on the WWW, Web mining uses various data mining techniques to
discover useful knowledge from Web hyperlinks, page content and usage log. The
main uses of web content mining are to gather, categorize, organize and provide
the best possible information available on the Web to the user requesting the
information. The mining tools are imperative to scanning the many HTML
documents, images, and text. Then, the result is used by the search engines. In
this paper, we first introduce the concepts related to web mining; we then
present an overview of different Web Content Mining tools. We conclude by
presenting a comparative table of these tools based on some pertinent criteria."
"Introduction: Before embarking on the design of any computer system it is
first necessary to assess the magnitude of the problem. In the case of a web
search engine this assessment amounts to determining the current size of the
web, the growth rate of the web, and the quantity of computing resource
necessary to search it, and projecting the historical growth of this into the
future. Method: The over 20 year history of the web makes it possible to make
short-term projections on future growth. The longer history of hard disk drives
(and smart phone memory card) makes it possible to make short-term hardware
projections. Analysis: Historical data on Internet uptake and hardware growth
is extrapolated. Results: It is predicted that within a decade the storage
capacity of a single hard drive will exceed the size of the index of the web at
that time. Within another decade it will be possible to store the entire
searchable text on the same hard drive. Within another decade the entire
searchable web (including images) will also fit. Conclusion: This result raises
questions about the future architecture of search engines. Several new models
are proposed. In one model the user's computer is an active part of the
distributed search architecture. They search a pre-loaded snapshot (back-file)
of the web on their local device which frees up the online data centre for
searching just the difference between the snapshot and the current time.
Advantageously this also makes it possible to search when the user is
disconnected from the Internet. In another model all changes to all files are
broadcast to all users (forming a star-like network) and no data centre is
needed."
"We propose a novel graph-based approach for constructing concept hierarchy
from a large text corpus. Our algorithm, GraBTax, incorporates both statistical
co-occurrences and lexical similarity in optimizing the structure of the
taxonomy. To automatically generate topic-dependent taxonomies from a large
text corpus, GraBTax first extracts topical terms and their relationships from
the corpus. The algorithm then constructs a weighted graph representing topics
and their associations. A graph partitioning algorithm is then used to
recursively partition the topic graph into a taxonomy. For evaluation, we apply
GraBTax to articles, primarily computer science, in the CiteSeerX digital
library and search engine. The quality of the resulting concept hierarchy is
assessed by both human judges and comparison with Wikipedia categories."
"We present a supervised learning algorithm for text categorization which has
brought the team of authors the 2nd place in the text categorization division
of the 2012 Cybersecurity Data Mining Competition (CDMC'2012) and a 3rd prize
overall. The algorithm is quite different from existing approaches in that it
is based on similarity search in the metric space of measure distributions on
the dictionary. At the preprocessing stage, given a labeled learning sample of
texts, we associate to every class label (document category) a point in the
space of question. Unlike it is usual in clustering, this point is not a
centroid of the category but rather an outlier, a uniform measure distribution
on a selection of domain-specific words. At the execution stage, an unlabeled
text is assigned a text category as defined by the closest labeled neighbour to
the point representing the frequency distribution of the words in the text. The
algorithm is both effective and efficient, as further confirmed by experiments
on the Reuters 21578 dataset."
"Online advertising has become a key source of revenue for both web search
engines and online publishers. For them, the ability of allocating right ads to
right webpages is critical because any mismatched ads would not only harm web
users' satisfactions but also lower the ad income. In this paper, we study how
online publishers could optimally select ads to maximize their ad incomes over
time. The conventional offline, content-based matching between webpages and ads
is a fine start but cannot solve the problem completely because good matching
does not necessarily lead to good payoff. Moreover, with the limited display
impressions, we need to balance the need of selecting ads to learn true ad
payoffs (exploration) with that of allocating ads to generate high immediate
payoffs based on the current belief (exploitation). In this paper, we address
the problem by employing Partially observable Markov decision processes
(POMDPs) and discuss how to utilize the correlation of ads to improve the
efficiency of the exploration and increase ad incomes in a long run. Our
mathematical derivation shows that the belief states of correlated ads can be
naturally updated using a formula similar to collaborative filtering. To test
our model, a real world ad dataset from a major search engine is collected and
categorized. Experimenting over the data, we provide an analyse of the effect
of the underlying parameters, and demonstrate that our algorithms significantly
outperform other strong baselines."
"Domain name registrars and URL shortener service providers place
advertisements on the parked domains (Internet domain names which are not in
service) in order to generate profits. As the web contents have been removed,
it is critical to make sure the displayed ads are directly related to the
intents of the visitors who have been directed to the parked domains. Because
of the missing contents in these domains, it is non-trivial to generate the
keywords to describe the previous contents and therefore the users intents. In
this paper we discuss the adaptive keywords extraction problem and introduce an
algorithm based on the BM25F term weighting and linear multi-armed bandits. We
built a prototype over a production domain registration system and evaluated it
using crowdsourcing in multiple iterations. The prototype is compared with
other popular methods and is shown to be more effective."
"Recommender systems are frequently used in domains in which users express
their preferences in the form of graded judgments, such as ratings. If accurate
top-N recommendation lists are to be produced for such graded relevance
domains, it is critical to generate a ranked list of recommended items directly
rather than predicting ratings. Current techniques choose one of two
sub-optimal approaches: either they optimize for a binary metric such as
Average Precision, which discards information on relevance grades, or they
optimize for Normalized Discounted Cumulative Gain (NDCG), which ignores the
dependence of an item's contribution on the relevance of more highly ranked
items.
  In this paper, we address the shortcomings of existing approaches by
proposing the Graded Average Precision factor model (GAPfm), a latent factor
model that is particularly suited to the problem of top-N recommendation in
domains with graded relevance data. The model optimizes for Graded Average
Precision, a metric that has been proposed recently for assessing the quality
of ranked results list for graded relevance. GAPfm learns a latent factor model
by directly optimizing a smoothed approximation of GAP. GAPfm's advantages are
twofold: it maintains full information about graded relevance and also
addresses the limitations of models that optimize NDCG. Experimental results
show that GAPfm achieves substantial improvements on the top-N recommendation
task, compared to several state-of-the-art approaches. In order to ensure that
GAPfm is able to scale to very large data sets, we propose a fast learning
algorithm that uses an adaptive item selection strategy. A final experiment
shows that GAPfm is useful not only for generating recommendation lists, but
also for ranking a given list of rated items."
"The web is trapped in the ""perpetual now"", and when users traverse from page
to page, they are seeing the state of the web resource (i.e., the page) as it
exists at the time of the click and not necessarily at the time when the link
was made. Thus, a temporal discrepancy can arise between the resource at the
time the page author created a link to it and the time when a reader follows
the link. This is especially important in the context of social media: the ease
of sharing links in a tweet or Facebook post allows many people to author web
content, but the space constraints combined with poor awareness by authors
often prevents sufficient context from being generated to determine the intent
of the post. If the links are clicked as soon as they are shared, the temporal
distance between sharing and clicking is so small that there is little to no
difference in content. However, not all clicks occur immediately, and a delay
of days or even hours can result in reading something other than what the
author intended. We introduce the concept of a user's temporal intention upon
publishing a link in social media. We investigate the features that could be
extracted from the post, the linked resource, and the patterns of social
dissemination to model this user intention. Finally, we analyze the historical
integrity of the shared resources in social media across time. In other words,
how much is the knowledge of the author's intent beneficial in maintaining the
consistency of the story being told through social posts and in enriching the
archived content coverage and depth of vulnerable resources?"
"Nowadays, more and more people use the Web as their primary source of
up-to-date information. In this context, fast crawling and indexing of newly
created Web pages has become crucial for search engines, especially because
user traffic to a significant fraction of these new pages (like news, blog and
forum posts) grows really quickly right after they appear, but lasts only for
several days.
  In this paper, we study the problem of timely finding and crawling of such
ephemeral new pages (in terms of user interest). Traditional crawling policies
do not give any particular priority to such pages and may thus crawl them not
quickly enough, and even crawl already obsolete content. We thus propose a new
metric, well thought out for this task, which takes into account the decrease
of user interest for ephemeral pages over time.
  We show that most ephemeral new pages can be found at a relatively small set
of content sources and present a procedure for finding such a set. Our idea is
to periodically recrawl content sources and crawl newly created pages linked
from them, focusing on high-quality (in terms of user interest) content. One of
the main difficulties here is to divide resources between these two activities
in an efficient way. We find the adaptive balance between crawls and recrawls
by maximizing the proposed metric. Further, we incorporate search engine click
logs to give our crawler an insight about the current user demands. Efficiency
of our approach is finally demonstrated experimentally on real-world data."
"We propose in this paper a method for measuring the similarity between
ontological concepts and terms. Our metric can take into account not only the
common words of two strings to compare but also other features such as the
position of the words in these strings, or the number of deletion, insertion or
replacement of words required for the construction of one of the two strings
from each other. The proposed method was then used to determine the ontological
concepts which are equivalent to the terms that qualify toponymes. It aims to
find the topographical type of the toponyme."
"Tagging is nowadays the most prevalent and practical way to make images
searchable. However, in reality many manually-assigned tags are irrelevant to
image content and hence are not reliable for applications. A lot of recent
efforts have been conducted to refine image tags. In this paper, we propose to
do tag refinement from the angle of topic modeling and present a novel
graphical model, regularized Latent Dirichlet Allocation (rLDA). In the
proposed approach, tag similarity and tag relevance are jointly estimated in an
iterative manner, so that they can benefit from each other, and the multi-wise
relationships among tags are explored. Moreover, both the statistics of tags
and visual affinities of images in the corpus are explored to help topic
modeling. We also analyze the superiority of our approach from the deep
structure perspective. The experiments on tag ranking and image retrieval
demonstrate the advantages of the proposed method."
"Web content quality estimation is crucial to various web content processing
applications. Our previous work applied Bagging + C4.5 to achive the best
results on the ECML/PKDD Discovery Challenge 2010, which is the comibination of
many point-wise rankinig models. In this paper, we combine multiple pair-wise
bipartite ranking learner to solve the multi-partite ranking problems for the
web quality estimation. In encoding stage, we present the ternary encoding and
the binary coding extending each rank value to $L - 1$ (L is the number of the
different ranking value). For the decoding, we discuss the combination of
multiple ranking results from multiple bipartite ranking models with the
predefined weighting and the adaptive weighting. The experiments on ECML/PKDD
2010 Discovery Challenge datasets show that \textit{binary coding} +
\textit{predefined weighting} yields the highest performance in all four
combinations and furthermore it is better than the best results reported in
ECML/PKDD 2010 Discovery Challenge competition."
"The contribution of this paper is two-fold. First, we present Indexing by
Latent Dirichlet Allocation (LDI), an automatic document indexing method. The
probability distributions in LDI utilize those in Latent Dirichlet Allocation
(LDA), a generative topic model that has been previously used in applications
for document retrieval tasks. However, the ad hoc applications, or their
variants with smoothing techniques as prompted by previous studies in LDA-based
language modeling, result in unsatisfactory performance as the document
representations do not accurately reflect concept space. To improve
performance, we introduce a new definition of document probability vectors in
the context of LDA and present a novel scheme for automatic document indexing
based on LDA. Second, we propose an Ensemble Model (EnM) for document
retrieval. The EnM combines basis indexing models by assigning different
weights and attempts to uncover the optimal weights to maximize the Mean
Average Precision (MAP). To solve the optimization problem, we propose an
algorithm, EnM.B, which is derived based on the boosting method. The results of
our computational experiments on benchmark data sets indicate that both the
proposed approaches are viable options for document retrieval."
"This paper introduces a project of advanced system of music retrieval from
the Internet. The system uses combination of text search (by author, title and
other information about the music file included in id3 tag description or
similar for other file types) with more intuitive and novel method of melody
search using query by humming. The patterns for storing text and melody
information as well as improved clustering algorithm for the pattern space were
proposed. The search engine is planned to optimise the query due to the data
input by user, thanks to the structure of text and melody index database. The
system is planned to be a plug-in for popular digital music players or an
independent player. An advanced system of recommendation based on information
gathered from user's profile and search history is an integral part of the
system. The recommendation mechanism uses scrobbling methods and is responsible
for making suggestions of songs unknown to the user but similar to his
preferred music styles and positioning search results."
"This study proposes a new way of using WordNet for Query Expansion (QE). We
choose candidate expansion terms, as usual, from a set of pseudo relevant
documents; however, the usefulness of these terms is measured based on their
definitions provided in a hand-crafted lexical resource like WordNet.
Experiments with a number of standard TREC collections show that this method
outperforms existing WordNet based methods. It also compares favorably with
established QE methods such as KLD and RM3. Leveraging earlier work in which a
combination of QE methods was found to outperform each individual method (as
well as other well-known QE methods), we next propose a combination-based QE
method that takes into account three different aspects of a candidate expansion
term's usefulness: (i) its distribution in the pseudo relevant documents and in
the target corpus, (ii) its statistical association with query terms, and (iii)
its semantic relation with the query, as determined by the overlap between the
WordNet definitions of the term and query terms. This combination of diverse
sources of information appears to work well on a number of test collections,
viz., TREC123, TREC5, TREC678, TREC robust new and TREC910 collections, and
yields significant improvements over competing methods on most of these
collections."
"In management education programmes today, students face a difficult time in
choosing electives as the number of electives available are many. As the range
and diversity of different elective courses available for selection have
increased, course recommendation systems that help students in making choices
about courses have become more relevant. In this paper we extend the concept of
collaborative filtering approach to develop a course recommendation system. The
proposed approach provides student an accurate prediction of the grade they may
get if they choose a particular course, which will be helpful when they decide
on selecting elective courses, as grade is an important parameter for a student
while deciding on an elective course. We experimentally evaluate the
collaborative filtering approach on a real life data set and show that the
proposed system is effective in terms of accuracy."
"Tag recommendation is a major aspect of collaborative tagging systems. It
aims to recommend tags to a user for tagging an item. In this paper we present
a part of our work in progress which is a novel improvement of recommendations
by re-ranking the output of a tag recommender. We mine association rules
between candidates tags in order to determine a more consistent list of tags to
recommend.
  Our method is an add-on one which leads to better recommendations as we show
in this paper. It is easily parallelizable and morever it may be applied to a
lot of tag recommenders. The experiments we did on five datasets with two kinds
of tag recommender demonstrated the efficiency of our method."
"Yelp online reviews are invaluable source of information for users to choose
where to visit or what to eat among numerous available options. But due to
overwhelming number of reviews, it is almost impossible for users to go through
all reviews and find the information they are looking for. To provide a
business overview, one solution is to give the business a 1-5 star(s). This
rating can be subjective and biased toward users personality. In this paper, we
predict a business rating based on user-generated reviews texts alone. This not
only provides an overview of plentiful long review texts but also cancels out
subjectivity. Selecting the restaurant category from Yelp Dataset Challenge, we
use a combination of three feature generation methods as well as four machine
learning models to find the best prediction result. Our approach is to create
bag of words from the top frequent words in all raw text reviews, or top
frequent words/adjectives from results of Part-of-Speech analysis. Our results
show Root Mean Square Error (RMSE) of 0.6 for the combination of Linear
Regression with either of the top frequent words from raw data or top frequent
adjectives after Part-of-Speech (POS)."
"Novelty detection in text streams is a challenging task that emerges in quite
a few different scenarios, ranging from email thread filtering to RSS news feed
recommendation on a smartphone. An efficient novelty detection algorithm can
save the user a great deal of time and resources when browsing through relevant
yet usually previously-seen content. Most of the recent research on detection
of novel documents in text streams has been building upon either geometric
distances or distributional similarities, with the former typically performing
better but being much slower due to the need of comparing an incoming document
with all the previously-seen ones. In this paper, we propose a new approach to
novelty detection in text streams. We describe a resource-aware mechanism that
is able to handle massive text streams such as the ones present today thanks to
the burst of social media and the emergence of the Web as the main source of
information. We capitalize on the historical Inverse Document Frequency (IDF)
that was known for capturing well term specificity and we show that it can be
used successfully at the document level as a measure of document novelty. This
enables us to avoid similarity comparisons with previous documents in the text
stream, thus scaling better and leading to faster execution times. Moreover, as
the collection of documents evolves over time, we use a temporal variant of IDF
not only to maintain an efficient representation of what has already been seen
but also to decay the document frequencies as the time goes by. We evaluate the
performance of the proposed approach on a real-world news articles dataset
created for this task. The results show that the proposed method outperforms
all of the baselines while managing to operate efficiently in terms of time
complexity and memory usage, which are of great importance in a mobile setting
scenario."
"In this work, we conduct a joint analysis of both Vector Space and Language
Models for IR using the mathematical framework of Quantum Theory. We shed light
on how both models allocate the space of density matrices. A density matrix is
shown to be a general representational tool capable of leveraging capabilities
of both VSM and LM representations thus paving the way for a new generation of
retrieval models. We analyze the possible implications suggested by our
findings."
"Currently, most people use PubMed to search the MEDLINE database, an
important bibliographical information source for life science and biomedical
information. However, PubMed has some drawbacks that make it difficult to find
relevant publications pertaining to users' individual intentions, especially
for non-expert users. To ameliorate the disadvantages of PubMed, we developed
G-Bean, a graph based biomedical search engine, to search biomedical articles
in MEDLINE database more efficiently.G-Bean addresses PubMed's limitations with
three innovations: parallel document index creation,ontology-graph based query
expansion, and retrieval and re-ranking of documents based on user's search
intention.Performance evaluation with 106 OHSUMED benchmark queries shows that
G-Bean returns more relevant results than PubMed does when using these queries
to search the MEDLINE database. PubMed could not even return any search result
for some OHSUMED queries because it failed to form the appropriate Boolean
query statement automatically from the natural language query strings. G-Bean
is available at http://bioinformatics.clemson.edu/G-Bean/index.php.G-Bean
addresses PubMed's limitations with ontology-graph based query expansion,
automatic document indexing, and user search intention discovery. It shows
significant advantages in finding relevant articles from the MEDLINE database
to meet the information need of the user."
"Text Document Clustering is one of the fastest growing research areas because
of availability of huge amount of information in an electronic form. There are
several number of techniques launched for clustering documents in such a way
that documents within a cluster have high intra-similarity and low
inter-similarity to other clusters. Many document clustering algorithms provide
localized search in effectively navigating, summarizing, and organizing
information. A global optimal solution can be obtained by applying high-speed
and high-quality optimization algorithms. The optimization technique performs a
globalized search in the entire solution space. In this paper, a brief survey
on optimization approaches to text document clustering is turned out."
"The rising availability of digital music stipulates effective categorization
and retrieval methods. Real world scenarios are characterized by mammoth music
collections through pertinent and non-pertinent songs with reference to the
user input. The primary goal of the research work is to counter balance the
perilous impact of non-relevant songs through Progressive Filtering (PF) for
Query by Humming (QBH) system. PF is a technique of problem solving through
reduced space. This paper presents the concept of PF and its efficient design
based on Multi-Resolution Histograms (MRH) to accomplish searching in
manifolds. Initially the entire music database is searched to obtain high
recall rate and narrowed search space. Later steps accomplish slow search in
the reduced periphery and achieve additional accuracy.
  Experimentation on large music database using recursive programming
substantiates the potential of the method. The outcome of proposed strategy
glimpses that MRH effectively locate the patterns. Distances of MRH at lower
level are the lower bounds of the distances at higher level, which guarantees
evasion of false dismissals during PF. In due course, proposed method helps to
strike a balance between efficiency and effectiveness. The system is scalable
for large music retrieval systems and also data driven for performance
optimization as an added advantage."
"Internet and electronic media gaining more popularity due to ease and speed,
the count of Internet users has increased tremendously. The world is moving
faster each day with several events taking place at once and the Internet is
flooded with information in every field. There are categories of information
ranging from most relevant to user, to the information totally irrelevant or
less relevant to specific users. In such a scenario getting the information
which is most relevant to the user is indispensable to save time. The
motivation of our solution is based on the idea of optimizing the search for
information automatically. This information is delivered to user in the form of
an interactive GUI. The optimization of the contents or information served to
him is based on his social networking profiles and on his reading habits on the
proposed solution. The aim is to get the user's profile information based on
his social networking profile considering that almost every Internet user has
one. This helps us personalize the contents delivered to the user in order to
produce what is most relevant to him, in the form of a personalized e-magazine.
Further the proposed solution learns user's reading habits for example the news
he saves or clicks the most and makes a decision to provide him with the best
contents."
"Clustering has been widely applied to Information Retrieval (IR) on the
grounds of its potential improved effectiveness over inverted file search.
Clustering is a mostly unsupervised procedure and the majority of the
clustering algorithms depend on certain assumptions in order to define the
subgroups present in a data set .A clustering quality measure is a function
that, given a data set and its partition into clusters, returns a non-negative
real number representing the quality of that clustering. Moreover, they may
behave in a different way depending on the features of the data set and their
input parameters values. Therefore, in most applications the resulting
clustering scheme requires some sort of evaluation as regards its validity. The
quality of clustering can be enhanced by using a Cellular Automata Classifier
for information retrieval. In this study we take the view that if cellular
automata with clustering is applied to search results (query-specific
clustering), then it has the potential to increase the retrieval effectiveness
compared both to that of static clustering and of conventional inverted file
search. We conducted a number of experiments using ten document collections and
eight hierarchic clustering methods. Our results show that the effectiveness of
query-specific clustering with cellular automata is indeed higher and suggest
that there is scope for its application to IR."
"Methods for fusing document lists that were retrieved in response to a query
often utilize the retrieval scores and/or ranks of documents in the lists. We
present a novel fusion approach that is based on using, in addition,
information induced from inter-document similarities. Specifically, our methods
let similar documents from different lists provide relevance-status support to
each other. We use a graph-based method to model relevance-status propagation
between documents. The propagation is governed by inter-document-similarities
and by retrieval scores of documents in the lists. Empirical evaluation
demonstrates the effectiveness of our methods in fusing TREC runs. The
performance of our most effective methods transcends that of effective fusion
methods that utilize only retrieval scores or ranks."
"Exploiting information induced from (query-specific) clustering of
top-retrieved documents has long been proposed as a means for improving
precision at the very top ranks of the returned results. We present a novel
language model approach to ranking query-specific clusters by the presumed
percentage of relevant documents that they contain. While most previous cluster
ranking approaches focus on the cluster as a whole, our model utilizes also
information induced from documents associated with the cluster. Our model
substantially outperforms previous approaches for identifying clusters
containing a high relevant-document percentage. Furthermore, using the model to
produce document ranking yields precision-at-top-ranks performance that is
consistently better than that of the initial ranking upon which clustering is
performed. The performance also favorably compares with that of a
state-of-the-art pseudo-feedback-based retrieval method."
"Multimedia collections are more than ever growing in size and diversity.
Effective multimedia retrieval systems are thus critical to access these
datasets from the end-user perspective and in a scalable way. We are interested
in repositories of image/text multimedia objects and we study multimodal
information fusion techniques in the context of content based multimedia
information retrieval. We focus on graph based methods which have proven to
provide state-of-the-art performances. We particularly examine two of such
methods : cross-media similarities and random walk based scores. From a
theoretical viewpoint, we propose a unifying graph based framework which
encompasses the two aforementioned approaches. Our proposal allows us to
highlight the core features one should consider when using a graph based
technique for the combination of visual and textual information. We compare
cross-media and random walk based results using three different real-world
datasets. From a practical standpoint, our extended empirical analysis allow us
to provide insights and guidelines about the use of graph based methods for
multimodal information fusion in content based multimedia information
retrieval."
"Similarity search is an important problem in information retrieval. This
similarity is based on a distance. Symbolic representation of time series has
attracted many researchers recently, since it reduces the dimensionality of
these high dimensional data objects. We propose a new distance metric that is
applied to symbolic data objects and we test it on time series data bases in a
classification task. We compare it to other distances that are well known in
the literature for symbolic data objects. We also prove, mathematically, that
our distance is metric."
"Query expansion is a well known method to improve the performance of
information retrieval systems. In this work we have tested different approaches
to extract the candidate query terms from the top ranked documents returned by
the first-pass retrieval.
  One of them is the cooccurrence approach, based on measures of cooccurrence
of the candidate and the query terms in the retrieved documents. The other one,
the probabilistic approach, is based on the probability distribution of terms
in the collection and in the top ranked set.
  We compare the retrieval improvement achieved by expanding the query with
terms obtained with different methods belonging to both approaches. Besides, we
have developed a na\""ive combination of both kinds of method, with which we
have obtained results that improve those obtained with any of them separately.
This result confirms that the information provided by each approach is of a
different nature and, therefore, can be used in a combined manner."
"We propose specific data structures designed to the indexing and retrieval of
information elements in heterogeneous XML data bases. The indexing scheme is
well suited to the management of various contextual searches, expressed either
at a structural level or at an information content level. The approximate
search mechanisms are based on a modified Levenshtein editing distance and
information fusion heuristics. The implementation described highlights the
mixing of structured information presented as field/value instances and free
text elements. The retrieval performances of the proposed approach are
evaluated within the INEX 2005 evaluation campaign. The evaluation results rank
the proposed approach among the best evaluated XML IR systems for the VVCAS
task."
"In the era of the information society, the impact of the information systems
on the economy of material and immaterial is certainly perceptible. With
regards to the information resources of an organization, the annotation
involved to enrich informational content, to track the intellectual activities
on a document and to set the added value on information for the benefit of
solving a decision-making problem in the context of economic intelligence. Our
contribution is distinguished by the representation of an annotation process
and its inherent concepts to lead the decisionmaker to an anticipated decision:
the provision of relevant and annotated information. Such information in the
system is made easy by taking into account the diversity of resources and those
that are well annotated so formally and informally by the EI actors. A capital
research framework consist of integrating in the decision-making process the
annotator activity, the software agent (or the reasoning mechanisms) and the
information resources enhancement."
"Personalized recommender systems are confronting great challenges of
accuracy, diversification and novelty, especially when the data set is sparse
and lacks accessorial information, such as user profiles, item attributes and
explicit ratings. Collaborative tags contain rich information about
personalized preferences and item contents, and are therefore potential to help
in providing better recommendations. In this paper, we propose a recommendation
algorithm based on an integrated diffusion on user-item-tag tripartite graphs.
We use three benchmark data sets, Del.icio.us, MovieLens and BibSonomy, to
evaluate our algorithm. Experimental results demonstrate that the usage of tag
information can significantly improve accuracy, diversification and novelty of
recommendations."
"We present a method to geometrize massive data sets from search engines query
logs. For this purpose, a macrodynamic-like quantitative model of the
Information Retrieval (IR) process is developed, whose paradigm is inspired by
basic constructions of Einstein's general relativity theory in which all IR
objects are uniformly placed in a common Room. The Room has a structure similar
to Einsteinian spacetime, namely that of a smooth manifold. Documents and
queries are treated as matter objects and sources of material fields.
Relevance, the central notion of IR, becomes a dynamical issue controlled by
both gravitation (or, more precisely, as the motion in a curved spacetime) and
forces originating from the interactions of matter fields. The spatio-temporal
description ascribes dynamics to any document or query, thus providing a
uniform description for documents of both initially static and dynamical
nature. Within the IR context, the techniques presented are based on two ideas.
The first is the placement of all objects participating in IR into a common
continuous space. The second idea is the `objectivization' of the IR process;
instead of expressing users' wishes, we consider the overall IR as an objective
physical process, representing the IR process in terms of motion in a given
external-fields configuration. Various semantic environments are treated as
various IR universes."
"We study the properties of the Google matrix generated by a coarse-grained
Perron-Frobenius operator of the Chirikov typical map with dissipation. The
finite size matrix approximant of this operator is constructed by the Ulam
method. This method applied to the simple dynamical model creates the directed
Ulam networks with approximate scale-free scaling and characteristics being
rather similar to those of the World Wide Web. The simple dynamical attractors
play here the role of popular web sites with a strong concentration of
PageRank. A variation of the Google parameter $\alpha$ or other parameters of
the dynamical map can drive the PageRank of the Google matrix to a delocalized
phase with a strange attractor where the Google search becomes inefficient."
"In this paper, by applying a diffusion process, we propose a new index to
quantify the similarity between two users in a user-object bipartite graph. To
deal with the discrete ratings on objects, we use a multi-channel
representation where each object is mapped to several channels with the number
of channels being equal to the number of different ratings. Each channel
represents a certain rating and a user having voted an object will be connected
to the channel corresponding to the rating. Diffusion process taking place on
such a user-channel bipartite graph gives a new similarity measure of user
pairs, which is further demonstrated to be more accurate than the classical
Pearson correlation coefficient under the standard collaborative filtering
framework."
"In this paper we are using the poset representation to describe the complex
answers given by IR systems after a clustering and ranking processes. The
answers considered may be given by cartographical representations or by
thematic sub-lists of documents. The poset representation, with the graph
theory and the relational representation opens many perspectives in the
definition of new similarity measures capable of taking into account both the
clustering and ranking processes. We present a general method for constructing
new similarity measures and give several examples. These measures can be used
for semi-ordered partitions; moreover, in the comparison of two sets of
answers, the corresponding similarity indicator is an increasing function of
the ranks of presentation of common answers."
"Text summarization can be classified into two approaches: extraction and
abstraction. This paper focuses on extraction approach. The goal of text
summarization based on extraction approach is sentence selection. One of the
methods to obtain the suitable sentences is to assign some numerical measure of
a sentence for the summary called sentence weighting and then select the best
ones. The first step in summarization by extraction is the identification of
important features. In our experiment, we used 125 test documents in DUC2002
data set. Each document is prepared by preprocessing process: sentence
segmentation, tokenization, removing stop word, and word stemming. Then, we use
8 important features and calculate their score for each sentence. We propose
text summarization based on fuzzy logic to improve the quality of the summary
created by the general statistic method. We compare our results with the
baseline summarizer and Microsoft Word 2007 summarizers. The results show that
the best average precision, recall, and f-measure for the summaries were
obtained by fuzzy method."
"Collaborative tags are playing more and more important role for the
organization of information systems. In this paper, we study a personalized
recommendation model making use of the ternary relations among users, objects
and tags. We propose a measure of user similarity based on his preference and
tagging information. Two kinds of similarities between users are calculated by
using a diffusion-based process, which are then integrated for recommendation.
We test the proposed method in a standard collaborative filtering framework
with three metrics: ranking score, Recall and Precision, and demonstrate that
it performs better than the commonly used cosine similarity."
"A focused crawler traverses the web selecting out relevant pages to a
predefined topic and neglecting those out of concern. While surfing the
internet it is difficult to deal with irrelevant pages and to predict which
links lead to quality pages. In this paper a technique of effective focused
crawling is implemented to improve the quality of web navigation. To check the
similarity of web pages w.r.t. topic keywords a similarity function is used and
the priorities of extracted out links are also calculated based on meta data
and resultant pages generated from focused crawler. The proposed work also uses
a method for traversing the irrelevant pages that met during crawling to
improve the coverage of a specific topic."
"For users, recommendations can sometimes seem odd or counterintuitive.
Visualizing recommendations can remove some of this mystery, showing how a
recommendation is grouped with other choices. A drawing can also lead a user's
eye to other options. Traditional 2D-embeddings of points can be used to create
a basic layout, but these methods, by themselves, do not illustrate clusters
and neighborhoods very well. In this paper, we propose the use of geographic
maps to enhance the definition of clusters and neighborhoods, and consider the
effectiveness of this approach in visualizing similarities and recommendations
arising from TV shows and music selections. All the maps referenced in this
paper can be found in http://www.research.att.com/~volinsky/maps"
"Traditional Information Retrieval (IR) research has focussed on a single user
interaction modality, where a user searches to satisfy an information need.
Recent advances in web technologies and computer hardware have enabled multiple
users to collaborate on many computer-supported tasks, therefore there is an
increasing opportunity to support two or more users searching together at the
same time in order to satisfy a shared information need, which we refer to as
Synchronous Collaborative Information Retrieval (SCIR). SCIR systems represent
a significant paradigmatic shift from traditional IR systems. In order to
support effective SCIR, new techniques are required to coordinate users'
activities. In addition, the novel domain of SCIR presents challenges for
effective evaluations of these systems. In this paper we will propose an
effective and re-usable evaluation methodology based on simulating users
searching together. We will outline how we have used this evaluation in
empirical studies of the effects of different division of labour and sharing of
knowledge techniques for SCIR."
"In this paper we present an approach for supporting users in the difficult
task of searching for video. We use collaborative feedback mined from the
interactions of earlier users of a video search system to help users in their
current search tasks. Our objective is to improve the quality of the results
that users find, and in doing so also assist users to explore a large and
complex information space. It is hoped that this will lead to them considering
search options that they may not have considered otherwise. We performed a user
centred evaluation. The results of our evaluation indicate that we achieved our
goals, the performance of the users in finding relevant video clips was
enhanced with our system; users were able to explore the collection of video
clips more and users demonstrated a preference for our system that provided
recommendations."
"In order to protect an image search engine's users from undesirable results
adult images' classifier should be built. The information about links from
websites to images is employed to create such a classifier. These links are
represented as a bipartite website-image graph. Each vertex is equipped with
scores of adultness and decentness. The scores for image vertexes are
initialized with zero, those for website vertexes are initialized according to
a text-based website classifier. An iterative algorithm that propagates scores
within a website-image graph is described. The scores obtained are used to
classify images by choosing an appropriate threshold. The experiments on
Internet-scale data have shown that the algorithm under consideration increases
classification recall by 17% in comparison with a simple algorithm which
classifies an image as adult if it is connected with at least one adult site
(at the same precision level)."
"With the advancement of technology and reduced storage costs, individuals and
organizations are tending towards the usage of electronic media for storing
textual information and documents. It is time consuming for readers to retrieve
relevant information from unstructured document collection. It is easier and
less time consuming to find documents from a large collection when the
collection is ordered or classified by group or category. The problem of
finding best such grouping is still there. This paper discusses the
implementation of k-Means clustering algorithm for clustering unstructured text
documents that we implemented, beginning with the representation of
unstructured text and reaching the resulting set of clusters. Based on the
analysis of resulting clusters for a sample set of documents, we have also
proposed a technique to represent documents that can further improve the
clustering result."
"In this work we have compared two indexing algorithms that have been used to
index and retrieve Carnatic music songs. We have compared a modified algorithm
of the Dual ternary indexing algorithm for music indexing and retrieval with
the multi-key hashing indexing algorithm proposed by us. The modification in
the dual ternary algorithm was essential to handle variable length query phrase
and to accommodate features specific to Carnatic music. The dual ternary
indexing algorithm is adapted for Carnatic music by segmenting using the
segmentation technique for Carnatic music. The dual ternary algorithm is
compared with the multi-key hashing algorithm designed by us for indexing and
retrieval in which features like MFCC, spectral flux, melody string and
spectral centroid are used as features for indexing data into a hash table. The
way in which collision resolution was handled by this hash table is different
than the normal hash table approaches. It was observed that multi-key hashing
based retrieval had a lesser time complexity than dual-ternary based indexing
The algorithms were also compared for their precision and recall in which
multi-key hashing had a better recall than modified dual ternary indexing for
the sample data considered."
"Caching is an effective mechanism for reducing bandwidth usage and
alleviating server load. However, the use of caching entails a compromise
between content freshness and refresh cost. An excessive refresh allows a high
degree of content freshness at a greater cost of system resource. Conversely, a
deficient refresh inhibits content freshness but saves the cost of resource
usages. To address the freshness-cost problem, we formulate the refresh
scheduling problem with a generic cost model and use this cost model to
determine an optimal refresh frequency that gives the best tradeoff between
refresh cost and content freshness. We prove the existence and uniqueness of an
optimal refresh frequency under the assumptions that the arrival of content
update is Poisson and the age-related cost monotonically increases with
decreasing freshness. In addition, we provide an analytic comparison of system
performance under fixed refresh scheduling and random refresh scheduling,
showing that with the same average refresh frequency two refresh schedulings
are mathematically equivalent in terms of the long-run average cost."
"Cross-lingual adaptation, a special case of domain adaptation, refers to the
transfer of classification knowledge between two languages. In this article we
describe an extension of Structural Correspondence Learning (SCL), a recently
proposed algorithm for domain adaptation, for cross-lingual adaptation. The
proposed method uses unlabeled documents from both languages, along with a word
translation oracle, to induce cross-lingual feature correspondences. From these
correspondences a cross-lingual representation is created that enables the
transfer of classification knowledge from the source to the target language.
The main advantages of this approach over other approaches are its resource
efficiency and task specificity.
  We conduct experiments in the area of cross-language topic and sentiment
classification involving English as source language and German, French, and
Japanese as target languages. The results show a significant improvement of the
proposed method over a machine translation baseline, reducing the relative
error due to cross-lingual adaptation by an average of 30% (topic
classification) and 59% (sentiment classification). We further report on
empirical analyses that reveal insights into the use of unlabeled data, the
sensitivity with respect to important hyperparameters, and the nature of the
induced cross-lingual correspondences."
"In this paper based on agent and semantic web technologies we propose an
approach .i.e., Semantic Oriented Agent Based Search (SOAS), to cope with
currently existing challenges of Meta data extraction, modeling and information
retrieval over the web. SOAS is designed by keeping four major requirements
.i.e., Automatic user request handling, Dynamic unstructured full text reading,
Analysing and modeling, Semantic query generation and optimized result
classifier. The architecture of SOAS is consisting of an agent called Personal
Agent (PA) and five dynamic components .i.e., Request Processing Unit (RPU),
Agent Locator (AL), Agent Communicator (AC), List Builder (LB) and Result
Generator (RG). Furthermore, in this paper we briefly discuss Semantic Web and
some already existing in time proposed and implemented semantic based
approaches."
"In this paper we present a method for reformulating the Recommender Systems
problem in an Information Retrieval one. In our tests we have a dataset of
users who give ratings for some movies; we hide some values from the dataset,
and we try to predict them again using its remaining portion (the so-called
""leave-n-out approach""). In order to use an Information Retrieval algorithm, we
reformulate this Recommender Systems problem in this way: a user corresponds to
a document, a movie corresponds to a term, the active user (whose rating we
want to predict) plays the role of the query, and the ratings are used as
weigths, in place of the weighting schema of the original IR algorithm. The
output is the ranking list of the documents (""users"") relevant for the query
(""active user""). We use the ratings of these users, weighted according to the
rank, to predict the rating of the active user. We carry out the comparison by
means of a typical metric, namely the accuracy of the predictions returned by
the algorithm, and we compare this to the real ratings from users. In our first
tests, we use two different Information Retrieval algorithms: LSPR, a recently
proposed model based on Discrete Fourier Transform, and a simple vector space
model."
"Semantic Web is, without a doubt, gaining momentum in both industry and
academia. The word ""Semantic"" refers to ""meaning"" - a semantic web is a web of
meaning. In this fast changing and result oriented practical world, gone are
the days where an individual had to struggle for finding information on the
Internet where knowledge management was the major issue. The semantic web has a
vision of linking, integrating and analysing data from various data sources and
forming a new information stream, hence a web of databases connected with each
other and machines interacting with other machines to yield results which are
user oriented and accurate. With the emergence of Semantic Web framework the
na\""ive approach of searching information on the syntactic web is clich\'e.
This paper proposes an optimised semantic searching of keywords exemplified by
simulation an ontology of Indian universities with a proposed algorithm which
ramifies the effective semantic retrieval of information which is easy to
access and time saving."
"Email Retrieval task has recently taken much attention to help the user
retrieve the email(s) related to the submitted query. Up to our knowledge,
existing email retrieval ranking approaches sort the retrieved emails based on
some heuristic rules, which are either search clues or some predefined user
criteria rooted in email fields. Unfortunately, the user usually does not know
the effective rule that acquires best ranking related to his query. This paper
presents a new email retrieval ranking approach to tackle this problem. It
ranks the retrieved emails based on a scoring function that depends on crucial
email fields, namely subject, content, and sender. The paper also proposes an
architecture to allow every user in a network/group of users to be able, if
permissible, to know the most important network senders who are interested in
his submitted query words. The experimental evaluation on Enron corpus prove
that our approach outperforms known email retrieval ranking approaches"
"Email Retrieval task has recently taken much attention to help the user
retrieve the email(s) related to the submitted query. Up to our knowledge,
existing email retrieval ranking approaches sort the retrieved emails based on
some heuristic rules, which are either search clues or some predefined user
criteria rooted in email fields. Unfortunately, the user usually does not know
the effective rule that acquires best ranking related to his query. This paper
presents a new email retrieval ranking approach to tackle this problem. It
ranks the retrieved emails based on a scoring function that depends on crucial
email fields, namely subject, content, and sender. The paper also proposes an
architecture to allow every user in a network/group of users to be able, if
permissible, to know the most important network senders who are interested in
his submitted query words. The experimental evaluation on Enron corpus prove
that our approach outperforms known email retrieval ranking approaches."
"This paper addresses the question of automatic data extraction from the
Wiktionary, which is a multilingual and multifunctional dictionary. Wiktionary
is a collaborative project working on the same principles as the Wikipedia. The
Wiktionary entry is a plain text from the text processing point of view.
Wiktionary guidelines prescribe the entry layout and rules, which should be
followed by editors of the dictionary. The presence of the structure of a
Wiktionary article and formatting rules allows transforming the Wiktionary
entry structure into tables and relations in a relational database schema,
which is a part of a machine-readable dictionary (MRD). The paper describes how
the flat text of the Wiktionary entry was extracted, converted, and stored in
the specially designed relational database. The MRD contains the definitions,
semantic relations, and translations extracted from the English and Russian
Wiktionaries. The parser software is released under the open source license
agreement (GPL), to facilitate its dissemination, modification and upgrades, to
draw researchers and programmers into parsing other Wiktionaries, not only
Russian and English."
"We want to find the optimal strategy for displaying advertisements e.g.
banners, videos, in given locations at given times under some realistic dynamic
constraints. Our primary goal is to maximize the expected revenue in a given
period of time, i.e. the total profit produced by the impressions, which
depends on profit-generating events such as the impressions themselves, the
ensuing clicks and registrations. Moreover we must take into consideration the
possibility that the constraints could change in time in a way that cannot
always be foreseen."
"Today World Wide Web (WWW) has become a huge ocean of information and it is
growing in size everyday. Downloading even a fraction of this mammoth data is
like sailing through a huge ocean and it is a challenging task indeed. In order
to download a large portion of data from WWW, it has become absolutely
essential to make the crawling process parallel. In this paper we offer the
architecture of a dynamic parallel Web crawler, christened as ""WEB-SAILOR,""
which presents a scalable approach based on Client-Server model to speed up the
download process on behalf of a Web Search Engine in a distributed Domain-set
specific environment. WEB-SAILOR removes the possibility of overlapping of
downloaded documents by multiple crawlers without even incurring the cost of
communication overhead among several parallel ""client"" crawling processes."
"The existing search engines sometimes give unsatisfactory search result for
lack of any categorization of search result. If there is some means to know the
preference of user about the search result and rank pages according to that
preference, the result will be more useful and accurate to the user. In the
present paper a web page ranking algorithm is being proposed based on syntactic
classification of web pages. Syntactic Classification does not bother about the
meaning of the content of a web page. The proposed approach mainly consists of
three steps: select some properties of web pages based on user's demand,
measure them, and give different weightage to each property during ranking for
different types of pages. The existence of syntactic classification is
supported by running fuzzy c-means algorithm and neural network classification
on a set of web pages. The change in ranking for difference in type of pages
but for same query string is also being demonstrated."
"Since its emergence in the 1990s the World Wide Web (WWW) has rapidly evolved
into a huge mine of global information and it is growing in size everyday. The
presence of huge amount of resources on the Web thus poses a serious problem of
accurate search. This is mainly because today's Web is a human-readable Web
where information cannot be easily processed by machine. Highly sophisticated,
efficient keyword based search engines that have evolved today have not been
able to bridge this gap. So comes up the concept of the Semantic Web which is
envisioned by Tim Berners-Lee as the Web of machine interpretable information
to make a machine processable form for expressing information. Based on the
semantic Web technologies we present in this paper the design methodology and
development of a semantic Web search engine which provides exact search results
for a domain specific search. This search engine is developed for an
agricultural Website which hosts agricultural information about the state of
West Bengal."
"This paper develops a flexible methodology to analyze the effectiveness of
different variables on various dependent variables which all are times series
and especially shows how to use a time series regression on one of the most
important and primary index (page views per visit) on Google analytic and in
conjunction it shows how to use the most suitable data to gain a more accurate
result. Search engine visitors have a variety of impact on page views which
cannot be described by single regression. On one hand referral visitors are
well-fitted on linear regression with low impact. On the other hand, direct
visitors made a huge impact on page views. The higher connection speed does not
simply imply higher impact on page views and the content of web page and the
territory of visitors can help connection speed to describe user behavior.
Returning visitors have some similarities with direct visitors."
"Collaborative tagging has emerged as a popular and effective method for
organizing and describing pages on the Web. We present Treelicious, a system
that allows hierarchical navigation of tagged web pages. Our system enriches
the navigational capabilities of standard tagging systems, which typically
exploit only popularity and co-occurrence data. We describe a prototype that
leverages the Wikipedia category structure to allow a user to semantically
navigate pages from the Delicious social bookmarking service. In our system a
user can perform an ordinary keyword search and browse relevant pages but is
also given the ability to broaden the search to more general topics and narrow
it to more specific topics. We show that Treelicious indeed provides an
intuitive framework that allows for improved and effective discovery of
knowledge."
"Search engine has become an inevitable tool for retrieving information from
the WWW. Web researchers introduce lots of algorithms to modify search engine
based on different features. Sometimes those algorithms are domain related,
sometimes they are Web page ranking related, and sometimes they are efficiency
related and so on. We are introducing such a type of algorithm which is
multiple domains as well as efficiency related. In this paper, we are providing
multilevel indexing on top of Index Based Acyclic Graph (IBAG) which support
multiple Ontologies as well as reduce search time. IBAG contains only domains
related pages and are constructed from Relevant Page Graph (RPaG). We have also
provided a comparative study of time complexity for the various models."
"The first step to handle semantic heterogeneity should be the attempt to
enrich the semantic information about documents, i.e. to fill up the gaps in
the documents meta-data automatically. Section 2 describes a set of cascading
deductive and heuristic extraction rules, which were developed in the project
CARMEN for the domain of Social Sciences. The mapping between different
terminologies can be done by using intellectual, statistical and/or neural
network transfer modules. Intellectual transfers use cross-concordances between
different classification schemes or thesauri. Section 3 describes the creation,
storage and handling of such transfers."
"Tag-based retrieval of multimedia content is a difficult problem, not only
because of the shorter length of tags associated with images and videos, but
also due to mismatch in the terminologies used by searcher and content creator.
To alleviate this problem, we propose a simple concept-driven probabilistic
model for improving text-based rich-media search. While our approach is similar
to existing topic-based retrieval and cluster-based language modeling work,
there are two important differences: (1) our proposed model considers not only
the query-generation likelihood from cluster, but explicitly accounts for the
overall ""popularity"" of the cluster or underlying concept, and (2) we explore
the possibility of inferring the likely concept relevant to a rich-media
content through the user-created communities that the content belongs to.
  We implement two methods of concept extraction: a traditional cluster based
approach, and the proposed community based approach. We evaluate these two
techniques for how effectively they capture the intended meaning of a term from
the content creator and searcher, and their overall value in improving image
search. Our results show that concept-driven search, though simple, clearly
outperforms plain search. Among the two techniques for concept-driven search,
community-based approach is more successful, as the concepts generated from
user communities are found to be more intuitive and appealing."
"Personalized article recommendation is important to improve user engagement
on news sites. Existing work quantifies engagement primarily through click
rates. We argue that quality of recommendations can be improved by
incorporating different types of ""post-read"" engagement signals like sharing,
commenting, printing and e-mailing article links. More specifically, we propose
a multi-faceted ranking problem for recommending news articles where each facet
corresponds to a ranking problem to maximize actions of a post-read action
type. The key technical challenge is to estimate the rates of post-read action
types by mitigating the impact of enormous data sparsity, we do so through
several variations of factor models. To exploit correlations among post-read
action types we also introduce a novel variant called locally augmented tensor
(LAT) model. Through data obtained from a major news site in the US, we show
that factor models significantly outperform a few baseline IR models and the
LAT model significantly outperforms several other variations of factor models.
Our findings show that it is possible to incorporate post-read signals that are
commonly available on online news sites to improve quality of recommendations."
"Web services are accessed via query interfaces which hide databases
containing thousands of relevant information. User's side, distant database is
a black box which accepts query and returns results, there is no way to access
database schema which reflect data and query meanings. Hence, web services are
very autonomous. Users view this autonomy as a major drawback because they need
often to combine query capabilities of many web services at the same time. In
this work, we will present a new approach which allows users to benefit of
query capabilities of many web services while respecting autonomy of each
service. This solution is a new contribution in Information Retrieval research
axe and has proven good performances on two standard datasets."
"The continuous information explosion through the Internet and all information
sources makes it necessary to perform all information processing activities
automatically in quick and reliable manners. In this paper, we proposed and
implemented a method to automatically create and Index for books written in
Arabic language. The process depends largely on text summarization and
abstraction processes to collect main topics and statements in the book. The
process is developed in terms of accuracy and performance and results showed
that this process can effectively replace the effort of manually indexing books
and document, a process that can be very useful in all information processing
and retrieval applications."
"Question Answering (QA) is not a new research field in Natural Language
Processing (NLP). However in recent years, QA has been a subject of growing
study. Nowadays, most of the QA systems have a similar pipelined architecture
and each system use a set of unique techniques to accomplish its state of the
art results. However, many things are not clear in the QA processing. It is not
clear the extend of the impact of tasks performed in earlier stages in
following stages of the pipelining process. It is not clear, if techniques used
in a QA system can be used in another QA system to improve its results. And
finally, it is not clear in what setting should be these systems tested in
order to properly analyze their results."
"World Wide Web consists of more than 50 billion pages online. It is highly
dynamic i.e. the web continuously introduces new capabilities and attracts many
people. Due to this explosion in size, the effective information retrieval
system or search engine can be used to access the information. In this paper we
have proposed the EPOW (Effective Performance of WebCrawler) architecture. It
is a software agent whose main objective is to minimize the overload of a user
locating needed information. We have designed the web crawler by considering
the parallelization policy. Since our EPOW crawler has a highly optimized
system it can download a large number of pages per second while being robust
against crashes. We have also proposed to use the data structure concepts for
implementation of scheduler & circular Queue to improve the performance of our
web crawler."
"The paper provided a description of a new model of information retrieval,
which is an extension of vector-space model and is based on the principles of
the theory of hypercomplex numerical systems. The model allows to some extent
realize the idea of fuzzy search and allows you to apply in practice the model
of information retrieval practical developments in the field of hypercomplex
numerical systems."
"In this work, we propose a theory for information matching. It is motivated
by the observation that retrieval is about the relevance matching between two
sets of properties (features), namely, the information need representation and
information item representation. However, many probabilistic retrieval models
rely on fixing one representation and optimizing the other (e.g. fixing the
single information need and tuning the document) but not both. Therefore, it is
difficult to use the available related information on both the document and the
query at the same time in calculating the probability of relevance. In this
paper, we address the problem by hypothesizing the relevance as a logical
relationship between the two sets of properties; the relationship is defined on
two separate mappings between these properties. By using the hypothesis we
develop a unified probabilistic relevance model which is capable of using all
the available information. We validate the proposed theory by formulating and
developing probabilistic relevance ranking functions for both ad-hoc text
retrieval and collaborative filtering. Our derivation in text retrieval
illustrates the use of the theory in the situation where no relevance
information is available. In collaborative filtering, we show that the
resulting recommender model unifies the user and item information into a
relevance ranking function without applying any dimensionality reduction
techniques or computing explicit similarity between two different users (or
items), in contrast to the state-of-the-art recommender models."
"Document ranking based on probabilistic evaluations of relevance is known to
exhibit non-classical correlations, which may be explained by admitting a
complex structure of the event space, namely, by assuming the events to emerge
from multiple sample spaces. The structure of event space formed by overlapping
sample spaces is known in quantum mechanics, they may exhibit some
counter-intuitive features, called quantum contextuality. In this Note I
observe that from the structural point of view quantum contextuality looks
similar to personalization of information retrieval scenarios. Along these
lines, Knowledge Revision is treated as operationalistic measurement and a way
to quantify the rate of personalization of Information Retrieval scenarios is
suggested."
"We present a new approach to the matching of 2D shapes using XML language and
dynamic programming. Given a 2D shape, we extract its contour and which is
represented by set of points. The contour is divided into curves using corner
detection. After, each curve is described by local and global features; these
features are coded in a string of symbols and stored in a XML file. Finally,
using the dynamic programming, we find the optimal alignment between sequences
of symbols. Results are presented and compared with existing methods using
MATLAB for KIMIA-25 database and MPEG7 databases."
"We present techniques to characterize which data is important to a
recommender system and which is not. Important data is data that contributes
most to the accuracy of the recommendation algorithm, while less important data
contributes less to the accuracy or even decreases it. Characterizing the
importance of data has two potential direct benefits: (1) increased privacy and
(2) reduced data management costs, including storage. For privacy, we enable
increased recommendation accuracy for comparable privacy levels using existing
data obfuscation techniques. For storage, our results indicate that we can
achieve large reductions in recommendation data and yet maintain recommendation
accuracy.
  Our main technique is called differential data analysis. The name is inspired
by other sorts of differential analysis, such as differential power analysis
and differential cryptanalysis, where insight comes through analysis of
slightly differing inputs. In differential data analysis we chunk the data and
compare results in the presence or absence of each chunk. We present results
applying differential data analysis to two datasets and three different kinds
of attributes. The first attribute is called user hardship. This is a novel
attribute, particularly relevant to location datasets, that indicates how
burdensome a data point was to achieve. The second and third attributes are
more standard: timestamp and user rating. For user rating, we confirm previous
work concerning the increased importance to the recommender of data
corresponding to high and low user ratings."
"The information contained in social tagging systems is often modelled as a
graph of connections between users, items and tags. Recommendation algorithms
such as FolkRank, have the potential to leverage complex relationships in the
data, corresponding to multiple hops in the graph. We present an in-depth
analysis and evaluation of graph models for social tagging data and propose
novel adaptations and extensions of FolkRank to improve tag recommendations. We
highlight implicit assumptions made by the widely used folksonomy model, and
propose an alternative and more accurate graph-representation of the data. Our
extensions of FolkRank address the new item problem by incorporating content
data into the algorithm, and significantly improve prediction results on
unpruned datasets. Our adaptations address issues in the iterative weight
spreading calculation that potentially hinder FolkRank's ability to leverage
the deep graph as an information source. Moreover, we evaluate the benefit of
considering each deeper level of the graph, and present important insights
regarding the characteristics of social tagging data in general. Our results
suggest that the base assumption made by conventional weight propagation
methods, that closeness in the graph always implies a positive relationship,
does not hold for the social tagging domain."
"This paper focuses on building a blog search engine which doesn't focus only
on keyword search but includes extended search capabilities. It also
incorporates the blog-post concept clustering which is based on the category
extracted from the blog post semantic content analysis. The proposed approach
is titled as ""BloSen (Blog Search Engine)"". It involves in extracting the posts
from blogs and parsing them to extract the blog elements and store them as
fields in a document format. Inverted index is being built on the fields of the
documents. Search is induced on the index and requested query is processed
based on the documents so far made from blog posts. It currently focuses on
Blogger and Wordpress hosted blogs since both these hosting services are the
most popular ones in the blogosphere. The proposed BloSen model is experimented
with a prototype implementation and the results of the experiments with the
user's relevance cumulative metric value of 95.44% confirms the efficiency of
the proposed model."
"In this paper we review studies of the growth of the Internet and
technologies that are useful for information search and retrieval on the Web.
Search engines are retrieve the efficient information. We collected data on the
Internet from several different sources, e.g., current as well as projected
number of users, hosts, and Web sites. The trends cited by the sources are
consistent and point to exponential growth in the past and in the coming
decade. Hence it is not surprising that about 85% of Internet users surveyed
claim using search engines and search services to find specific information and
users are not satisfied with the performance of the current generation of
search engines; the slow retrieval speed, communication delays, and poor
quality of retrieved results. Web agents, programs acting autonomously on some
task, are already present in the form of spiders, crawler, and robots. Agents
offer substantial benefits and hazards, and because of this, their development
must involve attention to technical details. This paper illustrates the
different types of agents,crawlers, robots,etc for mining the contents of web
in a methodical, automated manner, also discusses the use of crawler to gather
specific types of information from Web pages, such as harvesting e-mail
addresses"
"Keyword based search engines have problems with term ambiguity and vocabulary
mismatch. In this paper, we propose a query expansion technique that enriches
queries expressed as keywords and short natural language descriptions. We
present a new massive query expansion strategy that enriches queries using a
knowledge base by identifying the query concepts, and adding relevant synonyms
and semantically related terms. We propose two approaches: (i) lexical
expansion that locates the relevant concepts in the knowledge base; and, (ii)
topological expansion that analyzes the network of relations among the
concepts, and suggests semantically related terms by path and community
analysis of the knowledge graph. We perform our expansions by using two
versions of the Wikipedia as knowledge base, concluding that the combination of
both lexical and topological expansion provides improvements of the system's
precision up to more than 27%."
"When a user finds an interesting recommendation in a recommender system, the
user may want to recall related items recommended in the past to reconsider or
to enjoy them again. If the system can pick up such ""recalled"" items at each
user's request, it must deepen the user experience.
  We propose a model and the algorithm for such personalized ""recalling"" in
conventional recommender systems, which is an application of neural networks
for associative memory. In our model, the ""recalled"" items can reflect each
user's personality beyond naive similarities between items."
"Web usage mining is a process of extracting useful information from server
logs i.e. users history. Web usage mining is a process of finding out what
users are looking for on the internet. Some users might be looking at only
textual data, where as some others might be interested in multimedia data. One
would retrieve the data by copying it and pasting it to the relevant document.
But this is tedious and time consuming as well as difficult when the data to be
retrieved is plenty. Extracting structured data from a web page is challenging
problem due to complicated structured pages. Earlier they were used web page
programming language dependent; the main problem is to analyze the html source
code. In earlier they were considered the scripts such as java scripts and
cascade styles in the html files. When it makes different for existing
solutions to infer the regularity of the structure of the Web Pages only by
analyzing the tag structures. To overcome this problem we are using a new
algorithm called VIPS algorithm i.e. independent language. This approach
primary utilizes the visual features on the webpage to implement web data
extraction."
"This paper presents a set of algorithms used for music recommendations and
personalization in a general purpose social network www.ok.ru, the second
largest social network in the CIS visited by more then 40 millions users per
day. In addition to classical recommendation features like ""recommend a
sequence"" and ""find similar items"" the paper describes novel algorithms for
construction of context aware recommendations, personalization of the service,
handling of the cold-start problem, and more. All algorithms described in the
paper are working on-line and are able to detect and address changes in the
user's behavior and needs in the real time.
  The core component of the algorithms is a taste graph containing information
about different entities (users, tracks, artists, etc.) and relations between
them (for example, user A likes song B with certainty X, track B created by
artist C, artist C is similar to artist D with certainty Y and so on). Using
the graph it is possible to select tracks a user would most probably like, to
arrange them in a way that they match each other well, to estimate which items
from a fixed list are most relevant for the user, and more.
  In addition, the paper describes the approach used to estimate algorithms
efficiency and analyze the impact of different recommendation related features
on the users' behavior and overall activity at the service."
"Social tagging, as a novel approach to information organization and
discovery, has been widely adopted in many Web2.0 applications. The tags
provide a new type of information that can be exploited by recommender systems.
Nevertheless, the sparsity of ternary <user, tag, item> interaction data limits
the performance of tag-based collaborative filtering. This paper proposes a
random-walk-based algorithm to deal with the sparsity problem in social tagging
data, which captures the potential transitive associations between users and
items through their interaction with tags. In particular, two smoothing
strategies are presented from both the user-centric and item-centric
perspectives. Experiments on real-world data sets empirically demonstrate the
efficacy of the proposed algorithm."
"Interestingness is an important criterion by which we judge knowledge
discovery. But, interestingness has escaped all attempts to capture its
intuitive meaning into a concise and comprehensive form. A unifying paradigm is
formulated by function composition. We claim that composition is bipolar, i.e.
composition of exactly two functions, whose two semantic poles are relevance
and unexpectedness. The paradigm generality is demonstrated by case studies of
new interestingness functions, examples of known functions that fit the
framework, and counter-examples for which the paradigm points out to the
lacking pole."
"Recommender systems, which can significantly help users find their interested
items from the information era, has attracted an increasing attention from both
the scientific and application society. One of the widest applied
recommendation methods is the Matrix Factorization (MF). However, most of MF
based approaches focus on the user-item rating matrix, but ignoring the
ingredients which may have significant influence on users' preferences on
items. In this paper, we propose a multi-linear interactive MF algorithm
(MLIMF) to model the interactions between the users and each event associated
with their final decisions. Our model considers not only the user-item rating
information but also the pairwise interactions based on some empirically
supported factors. In addition, we compared the proposed model with three
typical other methods: user-based collaborative filtering (UCF), item-based
collaborative filtering (ICF) and regularized MF (RMF). Experimental results on
two real-world datasets, \emph{MovieLens} 1M and \emph{MovieLens} 100k, show
that our method performs much better than other three methods in the accuracy
of recommendation. This work may shed some light on the in-depth understanding
of modeling user online behaviors and the consequent decisions."
"Socially-based recommendation systems have recently attracted significant
interest, and a number of studies have shown that social information can
dramatically improve a system's predictions of user interests. Meanwhile, there
are now many potential applications that involve aspects of both recommendation
and information retrieval, and the task of collaborative retrieval---a
combination of these two traditional problems---has recently been introduced.
Successful collaborative retrieval requires overcoming severe data sparsity,
making additional sources of information, such as social graphs, particularly
valuable. In this paper we propose a new model for collaborative retrieval, and
show that our algorithm outperforms current state-of-the-art approaches by
incorporating information from social networks. We also provide empirical
analyses of the ways in which cultural interests propagate along a social graph
using a real-world music dataset."
"The Web is a potentially huge source of medical drug leads. But despite the
significant amount of multi- dimensional information about drugs, currently
commercial search engines accept only linear keyword strings as inputs. This
work uses linearized fragments of molecular structures as knowledge
representation units to serve as inputs to search engines. It is shown that
quite arbitrary fragments are surprisingly free of ambiguity, obtaining
relatively small result sets, which are both manageable and rich in novel
potential drug leads."
"Recommender systems research has experienced different stages such as from
user preference understanding to content analysis. Typical recommendation
algorithms were built on the following bases: (1) assuming users and items are
IID, namely independent and identically distributed, and (2) focusing on
specific aspects such as user preferences or contents. In reality, complex
recommendation tasks involve and request (1) personalized outcomes to tailor
heterogeneous subjective preferences; and (2) explicit and implicit objective
coupling relationships between users, items, and ratings to be considered as
intrinsic forces driving preferences. This inevitably involves the non-IID
complexity and the need of combining subjective preference with objective
couplings hidden in recommendation applications. In this paper, we propose a
novel generic coupled matrix factorization (CMF) model by incorporating non-IID
coupling relations between users and items. Such couplings integrate the
intra-coupled interactions within an attribute and inter-coupled interactions
among different attributes. Experimental results on two open data sets
demonstrate that the user/item couplings can be effectively applied in RS and
CMF outperforms the benchmark methods."
"In this note, we show how to marginalize over the damping parameter of the
PageRank equation so as to obtain a parameter-free version known as TotalRank.
Our discussion is meant as a reference and intended to provide a guided tour
towards an interesting result that has applications in information retrieval
and classification."
"Faceted arrangement of entities and typed relations for representing
different associations between the entities are established tools in knowledge
representation. In this paper, a proposal is being discussed combining both
tools to draw inferences along relational paths. This approach may yield new
benefit for information retrieval processes, especially when modeled for
heterogeneous environments in the Semantic Web. Faceted arrangement can be used
as a se-lection tool for the semantic knowledge modeled within the knowledge
repre-sentation. Typed relations between the entities of different facets can
be used as restrictions for selecting them across the facets."
"In this paper we present a generic framework for ontology-based information
retrieval. We focus on the recognition of semantic information extracted from
data sources and the mapping of this knowledge into ontology. In order to
achieve more scalability, we propose an approach for semantic indexing based on
entity retrieval model. In addition, we have used ontology of public
transportation domain in order to validate these proposals. Finally, we
evaluated our system using ontology mapping and real world data sources.
Experiments show that our framework can provide meaningful search results."
"An important aspect of a researcher's activities is to find relevant and
related publications. The task of a recommender system for scientific
publications is to provide a list of papers that match these criteria. Based on
the collection of publications managed by Mendeley, four data sets have been
assembled that reflect different aspects of relatedness. Each of these
relatedness scenarios reflect a user's search strategy. These scenarios are
public groups, venues, author publications and user libraries. The first three
of these data sets are being made publicly available for other researchers to
compare algorithms against. Three recommender systems have been implemented: a
collaborative filtering system; a content-based filtering system; and a hybrid
of these two systems. Results from testing demonstrate that collaborative
filtering slightly outperforms the content-based approach, but fails in some
scenarios. The hybrid system, that combines the two recommendation methods,
provides the best performance, achieving a precision of up to 70%. This
suggests that both techniques contribute complementary information in the
context of recommending scientific literature and different approaches suite
for different information needs."
"Common difficulties like the cold-start problem and a lack of sufficient
information about users due to their limited interactions have been major
challenges for most recommender systems (RS). To overcome these challenges and
many similar ones that result in low accuracy (precision and recall)
recommendations, we propose a novel system that extracts semantically-related
search keywords based on the aggregate behavioral data of many users. These
semantically-related search keywords can be used to substantially increase the
amount of knowledge about a specific user's interests based upon even a few
searches and thus improve the accuracy of the RS. The proposed system is
capable of mining aggregate user search logs to discover semantic relationships
between key phrases in a manner that is language agnostic, human
understandable, and virtually noise-free. These semantically related keywords
are obtained by looking at the links between queries of similar users which, we
believe, represent a largely untapped source for discovering latent semantic
relationships between search terms."
"Template extraction is the process of isolating the template of a given
webpage. It is widely used in several disciplines, including webpages
development, content extraction, block detection, and webpages indexing. One of
the main goals of template extraction is identifying a set of webpages with the
same template without having to load and analyze too many webpages prior to
identifying the template. This work introduces a new technique to automatically
discover a reduced set of webpages in a website that implement the template.
This set is computed with an hyperlink analysis that computes a very small set
with a high level of confidence."
"In this paper we present a preliminary analysis over the largest publicly
accessible web dataset: the Common Crawl Corpus. We measure nine web
characteristics from two levels of granularity using MapReduce and we comment
on the initial observations over a fraction of it. To the best of our knowledge
two of the characteristics, the language distribution and the HTML version of
pages have not been analyzed in previous work, while the specific dataset has
been only analyzed on page level."
"Template detection and content extraction are two of the main areas of
information retrieval applied to the Web. They perform different analyses over
the structure and content of webpages to extract some part of the document.
However, their objective is different. While template detection identifies the
template of a webpage (usually comparing with other webpages of the same
website), content extraction identifies the main content of the webpage
discarding the other part. Therefore, they are somehow complementary, because
the main content is not part of the template. It has been measured that
templates represent between 40% and 50% of data on the Web. Therefore,
identifying templates is essential for indexing tasks because templates usually
contain irrelevant information such as advertisements, menus and banners.
Processing and storing this information is likely to lead to a waste of
resources (storage space, bandwidth, etc.). Similarly, identifying the main
content is essential for many information retrieval tasks. In this paper, we
present a benchmark suite to test different approaches for template detection
and content extraction. The suite is public, and it contains real heterogeneous
webpages that have been labelled so that different techniques can be suitable
(and automatically) compared."
"In light of the tremendous amount of data produced by social media, a large
body of research have revisited the relevance estimation of the users'
generated content. Most of the studies have stressed the multidimensional
nature of relevance and proved the effectiveness of combining the different
criteria that it embodies. Traditional relevance estimates combination methods
are often based on linear combination schemes. However, despite being
effective, those aggregation mechanisms are not effective in real-life
applications since they heavily rely on the non-realistic independence property
of the relevance dimensions. In this paper, we propose to tackle this issue
through the design of a novel fuzzy-based document ranking model. We also
propose an automated methodology to capture the importance of relevance
dimensions, as well as information about their interaction. This model, based
on the Choquet Integral, allows to optimize the aggregated documents relevance
scores using any target information retrieval relevance metric. Experiments
within the TREC Microblog task and a social personalized information retrieval
task highlighted that our model significantly outperforms a wide range of
state-of-the-art aggregation operators, as well as a representative learning to
rank methods."
"Context-Based Information Retrieval is recently modelled as an exploration/
exploitation trade-off (exr/exp) problem, where the system has to choose
between maximizing its expected rewards dealing with its current knowledge
(exploitation) and learning more about the unknown user's preferences to
improve its knowledge (exploration). This problem has been addressed by the
reinforcement learning community but they do not consider the risk level of the
current user's situation, where it may be dangerous to explore the
non-top-ranked documents the user may not desire in his/her current situation
if the risk level is high. We introduce in this paper an algorithm named
CBIR-R-greedy that considers the risk level of the user's situation to
adaptively balance between exr and exp."
"Data fusion is the combination of the results of independent searches on a
document collection into one single output result set. It has been shown in the
past that this can greatly improve retrieval effectiveness over that of the
individual results.
  This paper presents probFuse, a probabilistic approach to data fusion.
ProbFuse assumes that the performance of the individual input systems on a
number of training queries is indicative of their future performance. The fused
result set is based on probabilities of relevance calculated during this
training process. Retrieval experiments using data from the TREC ad hoc
collection demonstrate that probFuse achieves results superior to that of the
popular CombMNZ fusion algorithm."
"We perform a deeper analysis of an axiomatic approach to the concept of
intrinsic dimension of a dataset proposed by us in the IJCNN'07 paper
(arXiv:cs/0703125). The main features of our approach are that a high intrinsic
dimension of a dataset reflects the presence of the curse of dimensionality (in
a certain mathematically precise sense), and that dimension of a discrete
i.i.d. sample of a low-dimensional manifold is, with high probability, close to
that of the manifold. At the same time, the intrinsic dimension of a sample is
easily corrupted by moderate high-dimensional noise (of the same amplitude as
the size of the manifold) and suffers from prohibitevely high computational
complexity (computing it is an $NP$-complete problem). We outline a possible
way to overcome these difficulties."
"In the era of the information society, the impact of the information systems
on the economy of material and immaterial is certainly perceptible. With
regards to the information resources of an organization, the annotation
involved to enrich informational content, to track the intellectual activities
on a document and to set the added value on information for the benefit of
solving a decision-making problem in the context of economic intelligence. Our
contribution is distinguished by the representation of an annotation process
and its inherent concepts to lead the decisionmaker to an anticipated decision:
the provision of relevant and annotated information. Such information in the
system is made easy by taking into account the diversity of resources and those
that are well annotated so formally and informally by the EI actors. A capital
research framework consist of integrating in the decision-making process the
annotator activity, the software agent (or the reasoning mechanisms) and the
information resources enhancement."
"The proliferation of media sharing and social networking websites has brought
with it vast collections of site-specific user generated content. The result is
a Social Networking Divide in which the concepts and structure common across
different sites are hidden. The knowledge and structures from one social site
are not adequately exploited to provide new information and resources to the
same or different users in comparable social sites. For music bloggers, this
latent structure, forces bloggers to select sub-optimal blogrolls. However, by
integrating the social activities of music bloggers and listeners, we are able
to overcome this limitation: improving the quality of the blogroll
neighborhoods, in terms of similarity, by 85 percent when using tracks and by
120 percent when integrating tags from another site."
"In addition to the frequency of terms in a document collection, the
distribution of terms plays an important role in determining the relevance of
documents for a given search query. In this paper, term distribution analysis
using Fourier series expansion as a novel approach for calculating an abstract
representation of term positions in a document corpus is introduced. Based on
this approach, two methods for improving the evaluation of document relevance
are proposed: (a) a function-based ranking optimization representing a user
defined document region, and (b) a query expansion technique based on
overlapping the term distributions in the top-ranked documents. Experimental
results demonstrate the effectiveness of the proposed approach in providing new
possibilities for optimizing the retrieval process."
"When searching the web, it is often possible that there are too many results
available for ambiguous queries. Text snippets, extracted from the retrieved
pages, are an indicator of the pages' usefulness to the query intention and can
be used to focus the scope of search results. In this paper, we propose a novel
method for automatically extracting web page snippets that are highly relevant
to the query intention and expressive of the pages' entire content. We show
that the usage of semantics, as a basis for focused retrieval, produces high
quality text snippet suggestions. The snippets delivered by our method are
significantly better in terms of retrieval performance compared to those
derived using the pages' statistical content. Furthermore, our study suggests
that semantically-driven snippet generation can also be used to augment
traditional passage retrieval algorithms based on word overlap or statistical
weights, since they typically differ in coverage and produce different results.
User clicks on the query relevant snippets can be used to refine the query
results and promote the most comprehensive among the relevant documents."
"A large part of the hidden web resides in weblog servers. New content is
produced in a daily basis and the work of traditional search engines turns to
be insufficient due to the nature of weblogs. This work summarizes the
structure of the blogosphere and highlights the special features of weblogs. In
this paper we present a method for ranking weblogs based on the link graph and
on several similarity characteristics between weblogs. First we create an
enhanced graph of connected weblogs and add new types of edges and weights
utilising many weblog features. Then, we assign a ranking to each weblog using
our algorithm, BlogRank, which is a modified version of PageRank. For the
validation of our method we run experiments on a weblog dataset, which we
process and adapt to our search engine. (http://spiderwave.aueb.gr/Blogwave).
The results suggest that the use of the enhanced graph and the BlogRank
algorithm is preferred by the users."
"Plenty of algorithms for link prediction have been proposed and were applied
to various real networks. Among these works, the weights of links are rarely
taken into account. In this paper, we use local similarity indices to estimate
the likelihood of the existence of links in weighted networks, including Common
Neighbor, Adamic-Adar Index, Resource Allocation Index, and their weighted
versions. In both the unweighted and weighted cases, the resource allocation
index performs the best. To our surprise, the weighted indices perform worse,
which reminds us of the well-known Weak Tie Theory. Further extensive
experimental study shows that the weak ties play a significant role in the link
prediction problem, and to emphasize the contribution of weak ties can
remarkably enhance the predicting accuracy."
"A set of ontology matching algorithms (for finding correspondences between
concepts) is based on a thesaurus that provides the source data for the
semantic distance calculations. In this wiki era, new resources may spring up
and improve this kind of semantic search. In the paper a solution of this task
based on Russian Wiktionary is compared to WordNet based algorithms. Metrics
are estimated using the test collection, containing 353 English word pairs with
a relatedness score assigned by human evaluators. The experiment shows that the
proposed method is capable in principle of calculating a semantic distance
between pair of words in any language presented in Russian Wiktionary. The
calculation of Wiktionary based metric had required the development of the
open-source Wiktionary parser software."
"Recently, collaborative tagging systems have attracted more and more
attention and have been widely applied in web systems. Tags provide highly
abstracted information about personal preferences and item content, and are
therefore potential to help in improving better personalized recommendations.
In this paper, we propose a tag-based recommendation algorithm considering the
personal vocabulary and evaluate it in a real-world dataset: Del.icio.us.
Experimental results demonstrate that the usage of tag information can
significantly improve the accuracy of personalized recommendations."
"Huge amount of information is present in the World Wide Web and a large
amount is being added to it frequently. A query-specific summary of multiple
documents is very helpful to the user in this context. Currently, few systems
have been proposed for query-specific, extractive multi-document summarization.
If a summary is available for a set of documents on a given query and if a new
document is added to the corpus, generating an updated summary from the scratch
is time consuming and many a times it is not practical/possible. In this paper
we propose a solution to this problem. This is especially useful in a scenario
where the source documents are not accessible. We cleverly embed the sentences
of the current summary into the new document and then perform query-specific
summary generation on that document. Our experimental results show that the
performance of the proposed approach is good in terms of both quality and
efficiency."
Paper has been withdrawn.
"In addition to the frequency of terms in a document collection, the
distribution of terms plays an important role in determining the relevance of
documents. In this paper, a new approach for representing term positions in
documents is presented. The approach allows an efficient evaluation of
term-positional information at query evaluation time. Three applications are
investigated: a function-based ranking optimization representing a user-defined
document region, a query expansion technique based on overlapping the term
distributions in the top-ranked documents, and cluster analysis of terms in
documents. Experimental results demonstrate the effectiveness of the proposed
approach."
"Information retrieval (IR) is a user approach to obtain relevant information
which meets needs with the help of a IR system (IRS). However, the IRS shows
certain differences between user relevance and system relevance. These gaps are
essentially related to the imperfection of the indexing process (as approach
related to the IR), to problems related to the misunderstanding of the natural
language and the non correspondence between the real needs of the user and the
results of his query. As idea is to think about an ?intellectual? indexing that
takes into account the point of view of the user. By consulting the document,
user can build information as added-value on the existing content: new
information which grows contents and allows the semantic visibility or
facilitates the reading by the annotations, by links to other content, by new
descriptors, specific new abstracts of users: it is the reindexing of the
contents by the contribution or the vote of the uses"
"In Bioinformatics, text mining and text data mining sometimes interchangeably
used is a process to derive high-quality information from text. Perl Status
Reporter (SRr) is a data fetching tool from a flat text file and in this
research paper we illustrate the use of SRr in text or data mining. SRr needs a
flat text input file where the mining process to be performed. SRr reads input
file and derives the high quality information from it. Typically text mining
tasks are text categorization, text clustering, concept and entity extraction,
and document summarization. SRr can be utilized for any of these tasks with
little or none customizing efforts. In our implementation we perform text
categorization mining operation on input file. The input file has two
parameters of interest (firstKey and secondKey). The composition of these two
parameters describes the uniqueness of entries in that file in the similar
manner as done by composite key in database. SRr reads the input file line by
line and extracts the parameters of interest and form a composite key by
joining them together. It subsequently generates an output file consisting of
the name as firstKey secondKey. SRr reads the input file and tracks the
composite key. It further stores all that data lines, having the same composite
key, in output file generated by SRr based on that composite key."
"Emergence of various vertical search engines highlights the fact that a
single ranking technology cannot deal with the complexity and scale of search
problems. For example, technology behind video and image search is very
different from general web search. Their ranking functions share few features.
Question answering websites (e.g., Yahoo! Answer) can make use of text matching
and click features developed for general web, but they have unique page
structures and rich user feedback, e.g., thumbs up and thumbs down ratings in
Yahoo! answer, which greatly benefit their own ranking. Even for those features
shared by answer and general web, the correlation between features and
relevance could be very different. Therefore, dedicated functions are needed in
order to better rank documents within individual domains. These dedicated
functions are defined on distinct feature spaces. However, having one search
box for each domain, is neither efficient nor scalable. Rather than typing the
same query two times into both Yahoo! Search and Yahoo! Answer and retrieving
two ranking lists, we would prefer putting it only once but receiving a
comprehensive list of documents from both domains on the subject. This
situation calls for new technology that blends documents from different sources
into a single ranking list. Despite the content richness of the blended list,
it has to be sorted by relevance none the less. We call such technology
blending, which is the main subject of this paper."
"Consider a family of sets and a single set, called the query set. How can one
quickly find a member of the family which has a maximal intersection with the
query set? Time constraints on the query and on a possible preprocessing of the
set family make this problem challenging. Such maximal intersection queries
arise in a wide range of applications, including web search, recommendation
systems, and distributing on-line advertisements. In general, maximal
intersection queries are computationally expensive. We investigate two
well-motivated distributions over all families of sets and propose an algorithm
for each of them. We show that with very high probability an almost optimal
solution is found in time which is logarithmic in the size of the family.
Moreover, we point out a threshold phenomenon on the probabilities of
intersecting sets in each of our two input models which leads to the efficient
algorithms mentioned above."
"Micro-blogging services such as Twitter allow anyone to publish anything,
anytime. Needless to say, many of the available contents can be diminished as
babble or spam. However, given the number and diversity of users, some valuable
pieces of information should arise from the stream of tweets. Thus, such
services can develop into valuable sources of up-to-date information (the
so-called real-time web) provided a way to find the most
relevant/trustworthy/authoritative users is available. Hence, this makes a
highly pertinent question for which graph centrality methods can provide an
answer. In this paper the author offers a comprehensive survey of feasible
algorithms for ranking users in social networks, he examines their
vulnerabilities to linking malpractice in such networks, and suggests an
objective criterion against which to compare such algorithms. Additionally, he
suggests a first step towards ""desensitizing"" prestige algorithms against
cheating by spammers and other abusive users."
"World Wide Web is a huge repository of web pages and links. It provides
abundance of information for the Internet users. The growth of web is
tremendous as approximately one million pages are added daily. Users' accesses
are recorded in web logs. Because of the tremendous usage of web, the web log
files are growing at a faster rate and the size is becoming huge. Web data
mining is the application of data mining techniques in web data. Web Usage
Mining applies mining techniques in log data to extract the behavior of users
which is used in various applications like personalized services, adaptive web
sites, customer profiling, prefetching, creating attractive web sites etc., Web
usage mining consists of three phases preprocessing, pattern discovery and
pattern analysis. Web log data is usually noisy and ambiguous and preprocessing
is an important process before mining. For discovering patterns sessions are to
be constructed efficiently. This paper reviews existing work done in the
preprocessing stage. A brief overview of various data mining techniques for
discovering patterns, and pattern analysis are discussed. Finally a glimpse of
various applications of web usage mining is also presented."
"This paper illustrates the Principal Direction Divisive Partitioning (PDDP)
algorithm and describes its drawbacks and introduces a combinatorial framework
of the Principal Direction Divisive Partitioning (PDDP) algorithm, then
describes the simplified version of the EM algorithm called the spherical
Gaussian EM (sGEM) algorithm and Information Bottleneck method (IB) is a
technique for finding accuracy, complexity and time space. The PDDP algorithm
recursively splits the data samples into two sub clusters using the hyper plane
normal to the principal direction derived from the covariance matrix, which is
the central logic of the algorithm. However, the PDDP algorithm can yield poor
results, especially when clusters are not well separated from one another. To
improve the quality of the clustering results problem, it is resolved by
reallocating new cluster membership using the IB algorithm with different
settings. IB Method gives accuracy but time consumption is more. Furthermore,
based on the theoretical background of the sGEM algorithm and sequential
Information Bottleneck method(sIB), it can be obvious to extend the framework
to cover the problem of estimating the number of clusters using the Bayesian
Information Criterion. Experimental results are given to show the effectiveness
of the proposed algorithm with comparison to the existing algorithm."
"Missing web pages, URIs that return the 404 ""Page Not Found"" error or the
HTTP response code 200 but dereference unexpected content, are ubiquitous in
today's browsing experience. We use Internet search engines to relocate such
missing pages and provide means that help automate the rediscovery process. We
propose querying web pages' titles against search engines. We investigate the
retrieval performance of titles and compare them to lexical signatures which
are derived from the pages' content. Since titles naturally represent the
content of a document they intuitively change over time. We measure the edit
distance between current titles and titles of copies of the same pages obtained
from the Internet Archive and display their evolution. We further investigate
the correlation between title changes and content modifications of a web page
over time. Lastly we provide a predictive model for the quality of any given
web page title in terms of its discovery performance. Our results show that
titles return more than 60% URIs top ranked and further relevant content
returned in the top 10 results. We show that titles decay slowly but are far
more stable than the pages' content. We further distill stop titles than can
help identify insufficiently performing search engine queries."
"Keyphrases provide a simple way of describing a document, giving the reader
some clues about its contents. Keyphrases can be useful in a various
applications such as retrieval engines, browsing interfaces, thesaurus
construction, text mining etc.. There are also other tasks for which keyphrases
are useful, as we discuss in this paper. This paper describes a neural network
based approach to keyphrase extraction from scientific articles. Our results
show that the proposed method performs better than some state-of-the art
keyphrase extraction approaches."
"This paper describes a method for multi-document update summarization that
relies on a double maximization criterion. A Maximal Marginal Relevance like
criterion, modified and so called Smmr, is used to select sentences that are
close to the topic and at the same time, distant from sentences used in already
read documents. Summaries are then generated by assembling the high ranked
material and applying some ruled-based linguistic post-processing in order to
obtain length reduction and maintain coherency. Through a participation to the
Text Analysis Conference (TAC) 2008 evaluation campaign, we have shown that our
method achieves promising results."
"Web search engines retrieve a vast amount of information for a given search
query. But the user needs only trustworthy and high-quality information from
this vast retrieved data. The response time of the search engine must be a
minimum value in order to satisfy the user. An optimum level of response time
should be maintained even when the system is overloaded. This paper proposes an
optimal Load Shedding algorithm which is used to handle overload conditions in
real-time data stream applications and is adapted to the Information Retrieval
System of a web search engine. Experiment results show that the proposed
algorithm enables a web search engine to provide trustworthy search results to
the user within an optimum response time, even during overload conditions."
"This paper addresses the design and implementation of BiLingual Information
Retrieval system on the domain, Festivals. A generic platform is built for
BiLingual Information retrieval which can be extended to any foreign or Indian
language working with the same efficiency. Search for the solution of the query
is not done in a specific predefined set of standard languages but is chosen
dynamically on processing the user's query. This paper deals with Indian
language Tamil apart from English. The task is to retrieve the solution for the
user given query in the same language as that of the query. In this process, a
Ontological tree is built for the domain in such a way that there are entries
in the above listed two languages in every node of the tree. A Part-Of-Speech
(POS) Tagger is used to determine the keywords from the given query. Based on
the context, the keywords are translated to appropriate languages using the
Ontological tree. A search is performed and documents are retrieved based on
the keywords. With the use of the Ontological tree, Information Extraction is
done. Finally, the solution for the query is translated back to the query
language (if necessary) and produced to the user."
"We propose to use MapReduce to quickly test new retrieval approaches on a
cluster of machines by sequentially scanning all documents. We present a small
case study in which we use a cluster of 15 low cost ma- chines to search a web
crawl of 0.5 billion pages showing that sequential scanning is a viable
approach to running large-scale information retrieval experiments with little
effort. The code is available to other researchers at:
http://mirex.sourceforge.net"
"The TREC 2009 web ad hoc and relevance feedback tasks used a new document
collection, the ClueWeb09 dataset, which was crawled from the general Web in
early 2009. This dataset contains 1 billion web pages, a substantial fraction
of which are spam --- pages designed to deceive search engines so as to deliver
an unwanted payload. We examine the effect of spam on the results of the TREC
2009 web ad hoc and relevance feedback tasks, which used the ClueWeb09 dataset.
We show that a simple content-based classifier with minimal training is
efficient enough to rank the ""spamminess"" of every page in the dataset using a
standard personal computer in 48 hours, and effective enough to yield
significant and substantive improvements in the fixed-cutoff precision (estP10)
as well as rank measures (estR-Precision, StatMAP, MAP) of nearly all submitted
runs. Moreover, using a set of ""honeypot"" queries the labeling of training data
may be reduced to an entirely automatic process. The results of classical
information retrieval methods are particularly enhanced by filtering --- from
among the worst to among the best."
"The ability of fast similarity search at large scale is of great importance
to many Information Retrieval (IR) applications. A promising way to accelerate
similarity search is semantic hashing which designs compact binary codes for a
large number of documents so that semantically similar documents are mapped to
similar codes (within a short Hamming distance). Although some recently
proposed techniques are able to generate high-quality codes for documents known
in advance, obtaining the codes for previously unseen documents remains to be a
very challenging problem. In this paper, we emphasise this issue and propose a
novel Self-Taught Hashing (STH) approach to semantic hashing: we first find the
optimal $l$-bit binary codes for all documents in the given corpus via
unsupervised learning, and then train $l$ classifiers via supervised learning
to predict the $l$-bit code for any query document unseen before. Our
experiments on three real-world text datasets show that the proposed approach
using binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine
(SVM) outperforms state-of-the-art techniques significantly."
"The use of domain knowledge is generally found to improve query efficiency in
content filtering applications. In particular, tangible benefits have been
achieved when using knowledge-based approaches within more specialized fields,
such as medical free texts or legal documents. However, the problem is that
sources of domain knowledge are time-consuming to build and equally costly to
maintain. As a potential remedy, recent studies on Wikipedia suggest that this
large body of socially constructed knowledge can be effectively harnessed to
provide not only facts but also accurate information about semantic
concept-similarities. This paper describes a framework for document filtering,
where Wikipedia's concept-relatedness information is combined with a domain
ontology to produce semantic content classifiers. The approach is evaluated
using Reuters RCV1 corpus and TREC-11 filtering task definitions. In a
comparative study, the approach shows robust performance and appears to
outperform content classifiers based on Support Vector Machines (SVM) and C4.5
algorithm."
"The establishment of links between data (e.g., patient records) and Web
resources (e.g., literature) and the proper visualization of such discovered
knowledge is still a challenge in most Life Science domains (e.g.,
biomedicine). In this paper we present our contribution to the community in the
form of an infrastructure to annotate information resources, to discover
relationships among them, and to represent and visualize the new discovered
knowledge. Furthermore, we have also implemented a Web-based prototype tool
which integrates the proposed infrastructure."
"Because of the increasing number of electronic data, designing efficient
tools to retrieve and exploit documents is a major challenge. Current search
engines suffer from two main drawbacks: there is limited interaction with the
list of retrieved documents and no explanation for their adequacy to the query.
Users may thus be confused by the selection and have no idea how to adapt their
query so that the results match their expectations. This paper describes a
request method and an environment based on aggregating models to assess the
relevance of documents annotated by concepts of ontology. The selection of
documents is then displayed in a semantic map to provide graphical indications
that make explicit to what extent they match the user's query; this man/machine
interface favors a more interactive exploration of data corpus."
"Unstructured information comprises a valuable source of data in clinical
records. For text mining in clinical records, concept extraction is the first
step in finding assertions and relationships. This study presents a system
developed for the annotation of medical concepts, including medical problems,
tests, and treatments, mentioned in clinical records. The system combines six
publicly available named entity recognition system into one framework, and uses
a simple voting scheme that allows to tune precision and recall of the system
to specific needs. The system provides both a web service interface and a UIMA
interface which can be easily used by other systems. The system was tested in
the fourth i2b2 challenge and achieved an F-score of 82.1% for the concept
exact match task, a score which is among the top-ranking systems. To our
knowledge, this is the first publicly available clinical record concept
annotation system."
"SPARQL query composition is difficult for the lay-person or even the
experienced bioinformatician in cases where the data model is unfamiliar.
Established best-practices and internationalization concerns dictate that
semantic web ontologies should use terms with opaque identifiers, further
complicating the task. We present SPARQL Assist: a web application that
addresses these issues by providing context-sensitive type-ahead completion to
existing web forms. Ontological terms are suggested using their labels and
descriptions, leveraging existing XML support for internationalization and
language-neutrality."
"The final goal of Information Retrieval (IR) is knowledge production.
However, it has been argued that knowledge production is not an individual
effort but a collaborative effort. Collaboration in information retrieval is
geared towards knowledge sharing and creation of new knowledge by users. This
paper discusses Collaborative Information Retrieval (CIR) and how it culminates
to knowledge creation. It explains how created knowledge is organized and
structured. It describes a functional architecture for the development of a CIR
prototype called MECOCIR. Some of the features of the prototype are presented
as well as how they facilitate collaborative knowledge exploitation. Knowledge
creation is explained through the knowledge conversion/transformation processes
proposed by Nonaka and CIR activities that facilitate these processes are
high-lighted and discussed"
"Querying over XML elements using keyword search is steadily gaining
popularity. The traditional similarity measure is widely employed in order to
effectively retrieve various XML documents. A number of authors have already
proposed different similarity-measure methods that take advantage of the
structure and content of XML documents. They do not, however, consider the
similarity between latent semantic information of element texts and that of
keywords in a query. Although many algorithms on XML element search are
available, some of them have the high computational complexity due to searching
a huge number of elements. In this paper, we propose a new algorithm that makes
use of the semantic similarity between elements instead of between entire XML
documents, considering not only the structure and content of an XML document,
but also semantic information of namespaces in elements. We compare our
algorithm with the three other algorithms by testing on the real datasets. The
experiments have demonstrated that our proposed method is able to improve the
query accuracy, as well as to reduce the running time."
"The many metrics employed for the evaluation of search engine results have
not themselves been conclusively evaluated. We propose a new measure for a
metric's ability to identify user preference of result lists. Using this
measure, we evaluate the metrics Discounted Cumulated Gain, Mean Average
Precision and classical precision, finding that the former performs best. We
also show that considering more results for a given query can impair rather
than improve a metric's ability to predict user preferences."
"Information exchange among many sources in Internet is more autonomous,
dynamic and free. The situation drive difference view of concepts among
sources. For example, word 'bank' has meaning as economic institution for
economy domain, but for ecology domain it will be defined as slope of river or
lake. In this aper, we will evaluate latent semantic and WordNet approach to
calculate semantic similarity. The evaluation will be run for some concepts
from different domain with reference by expert or human. Result of the
evaluation can provide a contribution for mapping of concept, query rewriting,
interoperability, etc."
"In this paper we analyze the efficiency of various search results
diversification methods. While efficacy of diversification approaches has been
deeply investigated in the past, response time and scalability issues have been
rarely addressed. A unified framework for studying performance and feasibility
of result diversification solutions is thus proposed. First we define a new
methodology for detecting when, and how, query results need to be diversified.
To this purpose, we rely on the concept of ""query refinement"" to estimate the
probability of a query to be ambiguous. Then, relying on this novel ambiguity
detection method, we deploy and compare on a standard test set, three different
diversification methods: IASelect, xQuAD, and OptSelect. While the first two
are recent state-of-the-art proposals, the latter is an original algorithm
introduced in this paper. We evaluate both the efficiency and the effectiveness
of our approach against its competitors by using the standard TREC Web
diversification track testbed. Results shown that OptSelect is able to run two
orders of magnitude faster than the two other state-of-the-art approaches and
to obtain comparable figures in diversification effectiveness."
"Information retrieval is not only the most frequent application executed on
the Web but it is also the base of different types of applications. Considering
collective intelligence of groups of individuals as a framework for evaluating
and incorporating new experiences and information we often cannot retrieve such
knowledge being tacit. Tacit knowledge underlies many competitive capabilities
and it is hard to articulate on discrete ontology structure. It is unstructured
or unorganized, and therefore remains hidden. Developing generic solutions that
can find the hidden knowledge is extremely complex. Moreover this will be a
great challenge for the developers of semantic technologies. This work aims to
explore ways to make explicit and available the tacit knowledge hidden in the
collective intelligence of a collaborative environment within organizations.
The environment was defined by folksonomies supported by a faceted semantic
search. Vector space model which incorporates an analogy with the mathematical
apparatus of quantum theory is adopted for the representation and manipulation
of the meaning of folksonomy. Vector space retrieval has been proven efficiency
when there isn't a data behavioural because it bears ranking algorithms
involving a small number of types of elements and few operations. A solution to
find what the user has in mind when posing a query could be based on ""joint
meaning"" understood as a joint construal of the creator of the contents and the
reader of the contents. The joint meaning was proposed to deal with vagueness
on ontology of folksonomy indeterminacy, incompleteness and inconsistencies on
collective intelligence. A proof-of concept prototype was built for
collaborative environment as evolution of the actual social networks (like
Facebook, LinkedIn,..) using the information visualization on a RIA application
with Semantic Web techniques and technologies."
"The crucial role of the evaluation in the development of the information
retrieval tools is useful evidence to improve the performance of these tools
and the quality of results that they return. However, the classic evaluation
approaches have limitations and shortcomings especially regarding to the user
consideration, the measure of the adequacy between the query and the returned
documents and the consideration of characteristics, specifications and
behaviors of the search tool. Therefore, we believe that the exploitation of
contextual elements could be a very good way to evaluate the search tools. So,
this paper presents a new approach that takes into account the context during
the evaluation process at three complementary levels. The experiments gives at
the end of this article has shown the applicability of the proposed approach to
real research tools. The tests were performed with the most popular searching
engine (i.e. Google, Bing and Yahoo) selected in particular for their high
selectivity. The obtained results revealed that the ability of these engines to
rejecting dead links, redundant results and parasites pages depends strongly to
how queries are formulated, and to the political of sites offering this
information to present their content. The relevance evaluation of results
provided by these engines, using the user's judgments, then using an automatic
manner to take into account the query context has also shown a general decline
in the perceived relevance according to the number of the considered results."
"Web Warehouse is a read only repository maintained on the web to effectively
handle the relevant data. Web warehouse is a system comprised of various
subsystems and process. It supports the organizations in decision making.
Quality of data store in web warehouse can affect the quality of decision made.
For a valuable decision making it is required to consider the quality aspects
in designing and modelling of a web warehouse. Thus data quality is one of the
most important issues of the web warehousing system. Quality must be
incorporated at different stages of the web warehousing system development. It
is necessary to enhance existing data warehousing system to increase the data
quality. It results in the storage of high quality data in the repository and
efficient decision making. In this paper a Quality Evaluation Framework is
proposed keeping in view the quality dimensions associated with different
phases of a web warehouse. Further more, the proposed framework is validated
empirically with the help of quantitative analysis."
"Traditional search engines on World Wide Web (WWW) focus essentially on
relevance ranking at the page level. But this lead to missing innumerable
structured information about real-world objects embedded in static Web pages
and online Web databases. Page-level information retrieval (IR) can
unfortunately lead to highly inaccurate relevance ranking in answering
object-oriented queries. On the other hand, Object Oriented Information
Computing (OOIC) is promising and greatly reduces the complexity of the system
while improving reusability and manageability. The most distinguishing
requirement of today's complex heterogeneous systems is the need of the
computing system to instantly adapt to vigorously changing conditions. OOIC
allows reflecting the dynamic characteristics of the applications by
instantiating objects dynamically. In this paper, major challenges of OOIC as
well as its rudiments are recapped. The review includes the insight to PopRank
Model and comparison analysis of conventional page rank based IR with OOIC"
"The performance of processing search queries depends heavily on the stored
index size. Accordingly, considerable research efforts have been devoted to the
development of efficient compression techniques for inverted indexes. Roughly,
index compression relies on two factors: the ordering of the indexed documents,
which strives to position similar documents in proximity, and the encoding of
the inverted lists that result from the ordered stream of documents. Large
commercial search engines index tens of billions of pages of the ever growing
Web. The sheer size of their indexes dictates the distribution of documents
among thousands of servers in a scheme called local index-partitioning, such
that each server indexes only several millions pages. Due to engineering and
runtime performance considerations, random distribution of documents to servers
is common. However, random index-partitioning among many servers adversely
impacts the resulting index sizes, as it decreases the effectiveness of
document ordering schemes. We study the impact of random index-partitioning on
document ordering schemes. We show that index-partitioning decreases the
aggregated size of the inverted lists logarithmically with the number of
servers, when documents within each server are randomly reordered. On the other
hand, the aggregated partitioned index size increases logarithmically with the
number of servers, when state-of-the-art document ordering schemes, such as
lexical URL sorting and clustering with TSP, are applied. Finally, we justify
the common practice of randomly distributing documents to servers, as we
qualitatively show that despite its ill-effects on the ensuing compression, it
decreases key factors in distributed query evaluation time by an order of
magnitude as compared with partitioning techniques that compress better."
"This paper presents the first evaluation framework for Web search query
segmentation based directly on IR performance. In the past, segmentation
strategies were mainly validated against manual annotations. Our work shows
that the goodness of a segmentation algorithm as judged through evaluation
against a handful of human annotated segmentations hardly reflects its
effectiveness in an IR-based setup. In fact, state-of the-art algorithms are
shown to perform as good as, and sometimes even better than human annotations
-- a fact masked by previous validations. The proposed framework also provides
us an objective understanding of the gap between the present best and the best
possible segmentation algorithm. We draw these conclusions based on an
extensive evaluation of six segmentation strategies, including three most
recent algorithms, vis-a-vis segmentations from three human annotators. The
evaluation framework also gives insights about which segments should be
necessarily detected by an algorithm for achieving the best retrieval results.
The meticulously constructed dataset used in our experiments has been made
public for use by the research community."
"The continuous growth in the XML information repositories has been matched by
increasing efforts in development of XML retrieval systems, in large parts
aiming at supporting content-oriented XML retrieval. These systems exploit the
available structural information, as market up in XML documents, in order to
return documents components- the so called XML elements-instead of the
complement documents in repose to the user query. In this paper, we provide an
overview of the different XML information retrieval systems and classify them
according to their storage and query evaluation strategies."
"This paper proposes a novel statistical approach to intelligent document
retrieval. It seeks to offer a more structured and extensible mathematical
approach to the term generalization done in the popular Latent Semantic
Analysis (LSA) approach to document indexing. A Markov Random Field (MRF) is
presented that captures relationships between terms and documents as
probabilistic dependence assumptions between random variables. From there, it
uses the MRF-Gibbs equivalence to derive joint probabilities as well as local
probabilities for document variables. A parameter learning method is proposed
that utilizes rank reduction with singular value decomposition in a matter
similar to LSA to reduce dimensionality of document-term relationships to that
of a latent topic space. Experimental results confirm the ability of this
approach to effectively and efficiently retrieve documents from substantial
data sets."
"In the view of massive content explosion in World Wide Web through diverse
sources, it has become mandatory to have content filtering tools. The filtering
of contents of the web pages holds greater significance in cases of access by
minor-age people. The traditional web page blocking systems goes by the Boolean
methodology of either displaying the full page or blocking it completely. With
the increased dynamism in the web pages, it has become a common phenomenon that
different portions of the web page holds different types of content at
different time instances. This paper proposes a model to block the contents at
a fine-grained level i.e. instead of completely blocking the page it would be
efficient to block only those segments which holds the contents to be blocked.
The advantages of this method over the traditional methods are fine-graining
level of blocking and automatic identification of portions of the page to be
blocked. The experiments conducted on the proposed model indicate 88% of
accuracy in filtering out the segments."
"The evaluation of a web page with respect to a query is a vital task in the
web information retrieval domain. This paper proposes the evaluation of a web
page as a bottom-up process from the segment level to the page level. A model
for evaluating the relevancy is proposed incorporating six different
dimensions. An algorithm for evaluating the segments of a web page, using the
above mentioned six dimensions is proposed. The benefits of fine-granining the
evaluation process to the segment level instead of the page level are explored.
The proposed model can be incorporated for various tasks like web page
personalization, result re-ranking, mobile device page rendering etc."
"The result listing from search engines includes a link and a snippet from the
web page for each result item. The snippet in the result listing plays a vital
role in assisting the user to click on it. This paper proposes a novel approach
to construct the snippets based on a semantic evaluation of the segments in the
page. The target segment(s) is/are identified by applying a model to evaluate
segments present in the page and selecting the segments with top scores. The
proposed model makes the user judgment to click on a result item easier since
the snippet is constructed semantically after a critical evaluation based on
multiple factors. A prototype implementation of the proposed model confirms the
empirical validation."
"The tremendous amount of increase in the quantity of information resources
available on the web has made the total time that the user spends on a single
page very minimal. Users revisiting the same page would be able to fetch the
required information much faster if the information that they consumed during
the previous visit(s) gets presented to them with a special style. This paper
proposes a model which empowers the users to mark the content interesting to
them, so that it can be identified easily during successive visits. In addition
to the explicit marking by the users, the model facilitates implicit marking
based on the user preferences. The prototype implementation based on proposed
model validates the model's efficiency."
"The results rendered by the search engines are mostly a linear snippet list.
With the prolific increase in the dynamism of web pages there is a need for
enhanced result lists from search engines in order to cope-up with the
expectations of the users. This paper proposes a model for dynamic construction
of a resultant page from various results fetched by the search engine, based on
the web page segmentation approach. With the incorporation of personalization
through user profile during the candidate segment selection, the enriched
resultant page is constructed. The benefits of this approach include instant,
one-shot navigation to relevant portions from various result items, in contrast
to a linear page-by-page visit approach. The experiments conducted on the
prototype model with various levels of users, quantifies the improvements in
terms of amount of relevant information fetched."
"With the phenomenal growth of content in the World Wide Web, the diversity of
user supplied queries have become vivid. Searching for people on the web has
become an important type of search activity in the web search engines. This
paper proposes a model named ""We.I.Pe"" to identify people on the World Wide Web
using e-mail Id as the primary input. The approach followed in this research
work provides the collected information, based on the user supplied e-mail id,
in an easier to navigate manner. The grouping of collected information based on
various sources makes the result visualization process more effective. The
proposed model is validated by a prototype implementation. Experiments
conducted on the prototype implementation provide encouraging results"
"The web page usage mining plays a vital role in enriching the page's content
and structure based on the feedbacks received from the user's interactions with
the page. This paper proposes a model for micro-managing the tracking
activities by fine-tuning the mining from the page level to the segment level.
The proposed model enables the web-master to identify the segments which
receives more focus from users comparing with others. The segment level
analytics of user actions provides an important metric to analyse the factors
which facilitate the increase in traffic for the page. The empirical validation
of the model is performed through prototype implementation."
"Recall, the proportion of relevant documents retrieved, is an important
measure of effectiveness in information retrieval, particularly in the legal,
patent, and medical domains. Where document sets are too large for exhaustive
relevance assessment, recall can be estimated by assessing a random sample of
documents; but an indication of the reliability of this estimate is also
required. In this article, we examine several methods for estimating two-tailed
recall confidence intervals. We find that the normal approximation in current
use provides poor coverage in many circumstances, even when adjusted to correct
its inappropriate symmetry. Analytic and Bayesian methods based on the ratio of
binomials are generally more accurate, but are inaccurate on small populations.
The method we recommend derives beta-binomial posteriors on retrieved and
unretrieved yield, with fixed hyperparameters, and a Monte Carlo estimate of
the posterior distribution of recall. We demonstrate that this method gives
mean coverage at or near the nominal level, across several scenarios, while
being balanced and stable. We offer advice on sampling design, including the
allocation of assessments to the retrieved and unretrieved segments, and
compare the proposed beta-binomial with the officially reported normal
intervals for recent TREC Legal Track iterations."
"Actual social networks (like Facebook, Twitter, Linkedin, ...) need to deal
with vagueness on ontological indeterminacy. In this paper is analyzed the
prototyping of a faceted semantic search for personalized social search using
the ""joint meaning"" in a community environment. User researches in a
""collaborative"" environment defined by folksonomies can be supported by the
most common features on the faceted semantic search. A solution for the
context-aware personalized search is based on ""joint meaning"" understood as a
joint construal of the creators of the contents and the user of the contents
using the faced taxonomy with the Semantic Web. A proof-of concept prototype
shows how the proposed methodological approach can also be applied to existing
presentation components, built with different languages and/or component
technologies."
"Web Data Extraction is an important problem that has been studied by means of
different scientific tools and in a broad range of applications. Many
approaches to extracting data from the Web have been designed to solve specific
problems and operate in ad-hoc domains. Other approaches, instead, heavily
reuse techniques and algorithms developed in the field of Information
Extraction.
  This survey aims at providing a structured and comprehensive overview of the
literature in the field of Web Data Extraction. We provided a simple
classification framework in which existing Web Data Extraction applications are
grouped into two main classes, namely applications at the Enterprise level and
at the Social Web level. At the Enterprise level, Web Data Extraction
techniques emerge as a key tool to perform data analysis in Business and
Competitive Intelligence systems as well as for business process
re-engineering. At the Social Web level, Web Data Extraction techniques allow
to gather a large amount of structured data continuously generated and
disseminated by Web 2.0, Social Media and Online Social Network users and this
offers unprecedented opportunities to analyze human behavior at a very large
scale. We discuss also the potential of cross-fertilization, i.e., on the
possibility of re-using Web Data Extraction techniques originally designed to
work in a given domain, in other domains."
"This paper addresses the problem of classifying web documents using domain
ontology. Our goal is to provide a method for improving the classification of
medical documents by exploiting the MeSH thesaurus (Medical Subject Headings)
which will allow us to generate a new representation based on concepts. This
approach was tested with two well-known data mining algorithms C4.5 and KNN,
and a comparison was made with the usual representation using stems. The
enrichment of vectors using the concepts and the hyperonyms drawn from the
domain ontology has significantly boosted their representation, something
essential for good classification. The results of our experiments on the
benchmark biomedical collection Ohsumed confirm the importance of the approach
by a very significant improvement in the performance of the ontology-based
classification compared to the classical representation (Stems) by 30%."
"We present Broccoli, a fast and easy-to-use search engine for what we call
semantic full-text search. Semantic full-text search combines the capabilities
of standard full-text search and ontology search. The search operates on four
kinds of objects: ordinary words (e.g., edible), classes (e.g., plants),
instances (e.g., Broccoli), and relations (e.g., occurs-with or native-to).
Queries are trees, where nodes are arbitrary bags of these objects, and arcs
are relations. The user interface guides the user in incrementally constructing
such trees by instant (search-as-you-type) suggestions of words, classes,
instances, or relations that lead to good hits. Both standard full-text search
and pure ontology search are included as special cases. In this paper, we
describe the query language of Broccoli, the main idea behind a new kind of
index that enables fast processing of queries from that language as well as
fast query suggestion, the natural language processing required, and the user
interface. We evaluated query times and result quality on the full version of
the English Wikipedia (40 GB XML dump) combined with the YAGO ontology (26
million facts). We have implemented a fully functional prototype based on our
ideas and provide a web application to reproduce our quality experiments. Both
are accessible via http://broccoli.informatik.uni-freiburg.de/repro-corr/ ."
"Future Information Retrieval, especially in connection with the internet,
will incorporate the content descriptions that are generated with social
network extraction technologies and preferably incorporate the probability
theory for assigning the semantic. Although there is an increasing interest
about social network extraction, but a little of them has a significant impact
to infomation retrieval. Therefore this paper proposes a model of information
retrieval from the social network extraction."
"Meaning of Web-page content plays a big role while produced a search result
from a search engine. Most of the cases Web-page meaning stored in title or
meta-tag area but those meanings do not always match with Web-page content. To
overcome this situation we need to go through the Web-page content to identify
the Web-page meaning. In such cases, where Webpage content holds dual meaning
words that time it is really difficult to identify the meaning of the Web-page.
In this paper, we are introducing a new design and development mechanism of
identifying the Web-page content meaning which holds dual meaning words in
their Web-page content."
"Today's conventional search engines hardly do provide the essential content
relevant to the user's search query. This is because the context and semantics
of the request made by the user is not analyzed to the full extent. So here the
need for a semantic web search arises. SWS is upcoming in the area of web
search which combines Natural Language Processing and Artificial Intelligence.
The objective of the work done here is to design, develop and implement a
semantic search engine- SIEU(Semantic Information Extraction in University
Domain) confined to the university domain. SIEU uses ontology as a knowledge
base for the information retrieval process. It is not just a mere keyword
search. It is one layer above what Google or any other search engines retrieve
by analyzing just the keywords. Here the query is analyzed both syntactically
and semantically. The developed system retrieves the web results more relevant
to the user query through keyword expansion. The results obtained here will be
accurate enough to satisfy the request made by the user. The level of accuracy
will be enhanced since the query is analyzed semantically. The system will be
of great use to the developers and researchers who work on web. The Google
results are re-ranked and optimized for providing the relevant links. For
ranking an algorithm has been applied which fetches more apt results for the
user query."
"This paper describes the work towards Gujarati Ad hoc Monolingual Retrieval
task for widely used Information Retrieval (IR) models. We present an indexing
baseline for the Gujarati Language represented by Mean Average Precision (MAP)
values. Our objective is to obtain a relative picture of a better IR model for
Gujarati Language. Results show that Classical IR models like Term Frequency
Inverse Document Frequency (TF_IDF) performs better when compared to few recent
probabilistic IR models. The experiments helped to identify the outperforming
IR models for Gujarati Language."
"Research has been devoted in the past few years to relevance feedback as an
effective solution to improve performance of information retrieval systems.
Relevance feedback refers to an interactive process that helps to improve the
retrieval performance. In this paper we propose the use of relevance feedback
to improve document image retrieval System (DIRS) performance. This paper
compares a variety of strategies for positive and negative feedback. In
addition, feature subspace is extracted and updated during the feedback process
using a Principal Component Analysis (PCA) technique and based on user's
feedback. That is, in addition to reducing the dimensionality of feature
spaces, a proper subspace for each type of features is obtained in the feedback
process to further improve the retrieval accuracy. Experiments show that using
relevance Feedback in DIR achieves better performance than common DIR."
"In this paper, we define a new metric family based on two concepts: The
definition of the stopping criterion and the notion of satisfaction, where the
former depends on the willingness and expectation of a user exploring search
results. Both concepts have been discussed so far in the IR literature, but we
argue in this paper that defining a proper single valued metric depends on
merging them into a single conceptual framework."
"Finding desired information from large data set is a difficult problem.
Information retrieval is concerned with the structure, analysis, organization,
storage, searching, and retrieval of information. Index is the main constituent
of an IR system. Now a day exponential growth of information makes the index
structure large enough affecting the IR system's quality. So compressing the
Index structure is our main contribution in this paper. We compressed the
document number in inverted file entries using a new coding technique based on
run-length encoding. Our coding mechanism uses a specified code which acts over
run-length coding. We experimented and found that our coding mechanism on an
average compresses 67.34% percent more than the other techniques."
"Internet is one of the main sources of information for millions of people.
One can find information related to practically all matters on internet.
Moreover if we want to retrieve information about some particular topic we may
find thousands of Web Pages related to that topic. But our main concern is to
find relevant Web Pages from among that collection. So in this paper I have
discussed that how information is retrieved from the web and the efforts
required for retrieving this information in terms of system and users efforts."
"On the Web, visits of a page are often introduced by one or more valuable
linking sources. Indeed, good back links are valuable resources for Web pages
and sites. We propose to discovering and leveraging the best backlinks of pages
for ranking. Similar to PageRank, MaxRank scores are updated {recursively}. In
particular, with probability $\lambda$, the MaxRank of a document is updated
from the backlink source with the maximum score; with probability $1-\lambda$,
the MaxRank of a document is updated from a random backlink source. MaxRank has
an interesting relation to PageRank. When $\lambda=0$, MaxRank reduces to
PageRank; when $\lambda=1$, MaxRank only looks at the best backlink it thinks.
Empirical results on Wikipedia shows that the global authorities are very
influential; Overall large $\lambda$s (but smaller than 1) perform best: the
convergence is dramatically faster than PageRank, but the performance is still
comparable. We study the influence of these sources and propose a few measures
such as the times of being the best backlink for others, and related properties
of the proposed algorithm. The introduction of best backlink sources provides
new insights for link analysis. Besides ranking, our method can be used to
discover the most valuable linking sources for a page or Website, which is
useful for both search engines and site owners."
"The main information of a webpage is usually mixed between menus,
advertisements, panels, and other not necessarily related information; and it
is often difficult to automatically isolate this information. This is precisely
the objective of content extraction, a research area of widely interest due to
its many applications. Content extraction is useful not only for the final
human user, but it is also frequently used as a preprocessing stage of
different systems that need to extract the main content in a web document to
avoid the treatment and processing of other useless information. Other
interesting application where content extraction is particularly used is
displaying webpages in small screens such as mobile phones or PDAs. In this
work we present a new technique for content extraction that uses the DOM tree
of the webpage to analyze the hierarchical relations of the elements in the
webpage. Thanks to this information, the technique achieves a considerable
recall and precision. Using the DOM structure for content extraction gives us
the benefits of other approaches based on the syntax of the webpage (such as
characters, words and tags), but it also gives us a very precise information
regarding the related components in a block, thus, producing very cohesive
blocks."
"This paper is a survey discussing Information Retrieval concepts, methods,
and applications. It goes deep into the document and query modelling involved
in IR systems, in addition to pre-processing operations such as removing stop
words and searching by synonym techniques. The paper also tackles text
categorization along with its application in neural networks and machine
learning. Finally, the architecture of web crawlers is to be discussed shedding
the light on how internet spiders index web documents and how they allow users
to search for items on the web."
"Recommender systems have emerged as a new weapon to help online firms to
realize many of their strategic goals (e.g., to improve sales, revenue,
customer experience etc.). However, many existing techniques commonly approach
these goals by seeking to recover preference (e.g., estimating ratings) in a
matrix completion framework. This paper aims to bridge this significant gap
between the clearly-defined strategic objectives and the not-so-well-justified
proxy.
  We show it is advantageous to think of a recommender system as an analogy to
a monopoly economic market with the system as the sole seller, users as the
buyers and items as the goods. This new perspective motivates a game-theoretic
formulation for recommendation that enables us to identify the optimal
recommendation policy by explicit optimizing certain strategic goals. In this
paper, we revisit and extend our prior work, the Collaborative-Competitive
Filtering preference model, towards a game-theoretic framework. The proposed
framework consists of two components. First, a conditional preference model
that characterizes how a user would respond to a recommendation action; Second,
knowing in advance how the user would respond, how a recommender system should
act (i.e., recommend) strategically to maximize its goals. We show how
objectives such as click-through rate, sales revenue and consumption diversity
can be optimized explicitly in this framework. Experiments are conducted on a
commercial recommender system and demonstrate promising results."
"Collaborative filtering is a very useful general technique for exploiting the
preference patterns of a group of users to predict the utility of items to a
particular user. Previous research has studied several probabilistic graphic
models for collaborative filtering with promising results. However, while these
models have succeeded in capturing the similarity among users and items in one
way or the other, none of them has considered the fact that users with similar
interests in items can have very different rating patterns; some users tend to
assign a higher rating to all items than other users. In this paper, we propose
and study of two new graphic models that address the distinction between user
preferences and ratings. In one model, called the decoupled model, we introduce
two different variables to decouple a users preferences FROM his ratings. IN
the other, called the preference model, we model the orderings OF items
preferred BY a USER, rather than the USERs numerical ratings of items.
Empirical study over two datasets of movie ratings shows that appropriate
modeling of the distinction between user preferences and ratings improves the
performance substantially and consistently. Specifically, the proposed
decoupled model outperforms all five existing approaches that we compare with
significantly, but the preference model is not very successful. These results
suggest that explicit modeling of the underlying user preferences is very
important for collaborative filtering, but we can not afford ignoring the
rating information completely."
"This work falls in the areas of information retrieval and semantic web, and
aims to improve the evaluation of web search tools. Indeed, the huge number of
information on the web as well as the growth of new inexperienced users creates
new challenges for information retrieval; certainly the current search engines
(such as Google, Bing and Yahoo) offer an efficient way to browse the web
content. However, this type of tool does not take into account the semantic
driven by the query terms and document words. This paper proposes a new
semantic based approach for the evaluation of information retrieval systems;
the goal is to increase the selectivity of search tools and to improve how
these tools are evaluated. The test of the proposed approach for the evaluation
of search engines has proved its applicability to real search tools. The
results showed that semantic evaluation is a promising way to improve the
performance and behavior of search engines as well as the relevance of the
results that they return."
"In this paper we study the relationship between query and search engine by
exploring the adaptive properties based on a simple search engine. We used set
theory and utilized the words and terms for defining singleton space of event
in a search engine model, and then provided the inclusion between one singleton
to another."
"Applications involving telecommunication call data records, web pages, online
transactions, medical records, stock markets, climate warning systems, etc.,
necessitate efficient management and processing of such massively exponential
amount of data from diverse sources. De-duplication or Intelligent Compression
in streaming scenarios for approximate identification and elimination of
duplicates from such unbounded data stream is a greater challenge given the
real-time nature of data arrival. Stable Bloom Filters (SBF) addresses this
problem to a certain extent. .
  In this work, we present several novel algorithms for the problem of
approximate detection of duplicates in data streams. We propose the Reservoir
Sampling based Bloom Filter (RSBF) combining the working principle of reservoir
sampling and Bloom Filters. We also present variants of the novel Biased
Sampling based Bloom Filter (BSBF) based on biased sampling concepts. We also
propose a randomized load balanced variant of the sampling Bloom Filter
approach to efficiently tackle the duplicate detection. In this work, we thus
provide a generic framework for de-duplication using Bloom Filters. Using
detailed theoretical analysis we prove analytical bounds on the false positive
rate, false negative rate and convergence rate of the proposed structures. We
exhibit that our models clearly outperform the existing methods. We also
demonstrate empirical analysis of the structures using real-world datasets (3
million records) and also with synthetic datasets (1 billion records) capturing
various input distributions."
"Online forums or message boards are rich knowledge-based communities. In
these communities, thread retrieval is an essential tool facilitating
information access. However, the issue on thread search is how to combine
evidence from text units(messages) to estimate thread relevance. In this paper,
we first rank a list of messages, then we score threads by aggregating their
ranked messages' scores. To aggregate the message scores, we adopt several
voting techniques that have been applied in ranking aggregates tasks such as
blog distillation and expert finding. The experimental result shows that many
voting techniques should be preferred over a baseline that treats a thread as a
concatenation of its message texts."
"This article is the result of the analysis of various bibliographic reference
management tools, especially those that are free. The use of editorial tools by
bibliographic editors has evolved rapidly since 2007. But, until recently, free
software has fallen short when it comes to ergonomics or use. The functional
and technical panorama offered by free software is the result of the comparison
of JabRef, Mendeley Desktop, BibDesk and Zotero software undertaken in January
2012 by two research professors affiliated with the Institut national
fran\c{c}ais des techniques de la documentation (INTD)."
"Online forums facilitate knowledge seeking and sharing on the Web. However,
the shared knowledge is not fully utilized due to information overload. Thread
retrieval is one method to overcome information overload. In this paper, we
propose a model that combines two existing approaches: the Pseudo Cluster
Selection and the Voting Techniques. In both, a retrieval system first scores a
list of messages and then ranks threads by aggregating their scored messages.
They differ on what and how to aggregate. The pseudo cluster selection focuses
on input, while voting techniques focus on the aggregation method. Our combined
models focus on the input and the aggregation methods. The result shows that
some combined models are statistically superior to baseline methods."
"We describe a novel, ""focusable"", scalable, distributed web crawler based on
GNU/Linux and PostgreSQL that we designed to be easily extendible and which we
have released under a GNU public licence. We also report a first use case
related to an analysis of Twitter's streams about the french 2012 presidential
elections and the URL's it contains."
"Evaluation metrics are an essential part of a ranking system, and in the past
many evaluation metrics have been proposed in information retrieval and Web
search. Discounted Cumulated Gains (DCG) has emerged as one of the evaluation
metrics widely adopted for evaluating the performance of ranking functions used
in Web search. However, the two sets of parameters, gain values and discount
factors, used in DCG are determined in a rather ad-hoc way. In this paper we
first show that DCG is generally not coherent, meaning that comparing the
performance of ranking functions using DCG very much depends on the particular
gain values and discount factors used. We then propose a novel methodology that
can learn the gain values and discount factors from user preferences over
rankings. Numerical simulations illustrate the effectiveness of our proposed
methods. Please contact the authors for the full version of this work."
"Thanks to information extraction and semantic Web efforts, search on
unstructured text is increasingly refined using semantic annotations and
structured knowledge bases. However, most users cannot become familiar with the
schema of knowledge bases and ask structured queries. Interpreting free-format
queries into a more structured representation is of much current interest. The
dominant paradigm is to segment or partition query tokens by purpose
(references to types, entities, attribute names, attribute values, relations)
and then launch the interpreted query on structured knowledge bases. Given that
structured knowledge extraction is never complete, here we use a data
representation that retains the unstructured text corpus, along with structured
annotations (mentions of entities and relationships) on it. We propose two new,
natural formulations for joint query interpretation and response ranking that
exploit bidirectional flow of information between the knowledge base and the
corpus.One, inspired by probabilistic language models, computes expected
response scores over the uncertainties of query interpretation. The other is
based on max-margin discriminative learning, with latent variables representing
those uncertainties. In the context of typed entity search, both formulations
bridge a considerable part of the accuracy gap between a generic query that
does not constrain the type at all, and the upper bound where the ""perfect""
target entity type of each query is provided by humans. Our formulations are
also superior to a two-stage approach of first choosing a target type using
recent query type prediction techniques, and then launching a type-restricted
entity search query."
"The growth of Internet commerce has stimulated the use of collaborative
filtering (CF) algorithms as recommender systems. Such systems leverage
knowledge about the known preferences of multiple users to recommend items of
interest to other users. CF methods have been harnessed to make recommendations
about such items as web pages, movies, books, and toys. Researchers have
proposed and evaluated many approaches for generating recommendations. We
describe and evaluate a new method called emph{personality diagnosis (PD)}.
Given a user's preferences for some items, we compute the probability that he
or she is of the same ""personality type"" as other users, and, in turn, the
probability that he or she will like new items. PD retains some of the
advantages of traditional similarity-weighting techniques in that all data is
brought to bear on each prediction and new data can be added easily and
incrementally. Additionally, PD has a meaningful probabilistic interpretation,
which may be leveraged to justify, explain, and augment results. We report
empirical results on the EachMovie database of movie ratings, and on user
profile data collected from the CiteSeer digital library of Computer Science
research papers. The probabilistic framework naturally supports a variety of
descriptive measurements - in particular, we consider the applicability of a
value of information (VOI) computation."
"The third Information Retrieval Education through EXperimentation track
(EIREX 2012) was run at the University Carlos III of Madrid, during the 2012
spring semester. EIREX 2012 is the third in a series of experiments designed to
foster new Information Retrieval (IR) education methodologies and resources,
with the specific goal of teaching undergraduate IR courses from an
experimental perspective. For an introduction to the motivation behind the
EIREX experiments, see the first sections of [Urbano et al., 2011a]. For
information on other editions of EIREX and related data, see the website at
http://ir.kr.inf.uc3m.es/eirex/. The EIREX series have the following goals: a)
to help students get a view of the Information Retrieval process as they would
find it in a real-world scenario, either industrial or academic; b) to make
students realize the importance of laboratory experiments in Computer Science
and have them initiated in their execution and analysis; c) to create a public
repository of resources to teach Information Retrieval courses; d) to seek the
collaboration and active participation of other Universities in this endeavor.
This overview paper summarizes the results of the EIREX 2012 track, focusing on
the creation of the test collection and the analysis to assess its reliability."
"In this paper, we describe an approach to populate an existing ontology with
instance information present in the natural language text provided as input. An
ontology is defined as an explicit conceptualization of a shared domain. This
approach starts with a list of relevant domain ontologies created by human
experts, and techniques for identifying the most appropriate ontology to be
extended with information from a given text. Then we demonstrate heuristics to
extract information from the unstructured text and for adding it as structured
information to the selected ontology. This identification of the relevant
ontology is critical, as it is used in identifying relevant information in the
text. We extract information in the form of semantic triples from the text,
guided by the concepts in the ontology. We then convert the extracted
information about the semantic class instances into Resource Description
Framework (RDF3) and append it to the existing domain ontology. This enables us
to perform more precise semantic queries over the semantic triple store thus
created. We have achieved 95% accuracy of information extraction in our
implementation."
"With the dramatic increase in the number of websites on the internet, tagging
has become popular for finding related, personal and important documents. When
the potentially increasing internet markets are analyzed, Turkey, in which most
of the people use Turkish language on the internet, found to be exponentially
increasing. In this paper, a tag-based website recommendation method is
presented, where similarity measures are combined with semantic relationships
of tags. In order to evaluate the system, an experiment with 25 people from
Turkey is undertaken and participants are firstly asked to provide websites and
tags in Turkish and then they are asked to evaluate recommended websites."
"The search engine evaluation research has quite a lot metrics available to
it. Only recently, the question of the significance of individual metrics
started being raised, as these metrics' correlations to real-world user
experiences or performance have generally not been well-studied. The first part
of this thesis provides an overview of previous literature on the evaluation of
search engine evaluation metrics themselves, as well as critiques of and
comments on individual studies and approaches. The second part introduces a
meta-evaluation metric, the Preference Identification Ratio (PIR), that
quantifies the capacity of an evaluation metric to capture users' preferences.
Also, a framework for simultaneously evaluating many metrics while varying
their parameters and evaluation standards is introduced. Both PIR and the
meta-evaluation framework are tested in a study which shows some interesting
preliminary results; in particular, the unquestioning adherence to metrics or
their ad hoc parameters seems to be disadvantageous. Instead, evaluation
methods should themselves be rigorously evaluated with regard to goals set for
a particular study."
"Since very recently, users on the social bookmarking service Delicious can
stack web pages in addition to tagging them. Stacking enables users to group
web pages around specific themes with the aim of recommending to others.
However, users still stack a small subset of what they tag, and thus many web
pages remain unstacked. This paper presents early research towards
automatically clustering web pages from tags to find stacks and extend
recommendations."
"The arrangement of things in n-dimensional space is specified as Spatial.
Spatial data consists of values that denote the location and shape of objects
and areas on the earths surface. Spatial information includes facts such as
location of features, the relationship of geographic features and measurements
of geographic features. The spatial cognition is a primal area of study in
various other fields such as Robotics, Psychology, Geosciences, Geography,
Political Sciences, Geographic Economy, Environmental, Mining and Petroleum
Engineering, Natural Resources, Epidemiology, Demography etc., Any text
document which contains physical location specifications such as place names,
geographic coordinates, landmarks, country names etc., are supposed to contain
the spatial information. The spatial information may also be represented using
vague or fuzzy descriptions involving linguistic terms such as near to, far
from, to the east of, very close. Given a query involving events, the aim of
this ongoing research work is to extract the relevant information from multiple
text documents, resolve the uncertainty and vagueness and translate them in to
locations in a map. The input to the system would be a text Corpus and a
Spatial Query event. The output of the system is a map showing the most
possible, disambiguated location of the event queried. The author proposes
Fuzzy Logic Techniques for resolving the uncertainty in the spatial
expressions."
"Recently, group recommendations have attracted considerable attention. Rather
than recommending items to individual users, group recommenders recommend items
to groups of users. In this position paper, we introduce the problem of forming
an appropriate group of users to recommend an item when constraints apply to
the members of the group. We present a formal model of the problem and an
algorithm for its solution. Finally, we identify several directions for future
work."
"The research that has been carried out on blogs focused on blog posts only,
ignoring the title of the blog page. Also, in summarization only a set of
representative sentences are extracted. Some analysis has been done and it has
been found that the blog post contains the content that is likely to be related
to the topic of the blog post. Thus, proposed system of summarization makes use
of title contained in a blog page. The approach makes use of the Presence
factor that indicates the presence of each term of the title in each sentence
of the blog post. This is a key feature because it considers those sentences as
more relevant for summarization that contain each of the term present in the
title. The system has been implemented and evaluated experimentally. The system
has shown promising results."
"Tokenization is the task of chopping it up into pieces, called tokens,
perhaps at the same time throwing away certain characters, such as punctuation.
A token is an instance of token a sequence of characters in some particular
document that are grouped together as a useful semantic unit for processing.
New software tool and algorithm to support the IRS at tokenization process are
presented. Our proposed tool will filter out the three computer character
Sequences: IP-Addresses, Web URLs, Date, and Email Addresses. Our tool will use
the pattern matching algorithms and filtration methods. After this process, the
IRS can start a new tokenization process on the new retrieved text which will
be free of these sequences."
"In this paper, we introduce a novel situation aware approach to improve a
context based recommender system. To build situation aware user profiles, we
rely on evidence issued from retrieval situations. A retrieval situation refers
to the social spatio temporal context of the user when he interacts with the
recommender system. A situation is represented as a combination of social
spatio temporal concepts inferred from ontological knowledge given social
group, location and time information. User's interests are inferred from past
user's interaction with the recommender system related to the identified
situations. They are represented using concepts issued from a domain ontology.
We also propose a method to dynamically adapt the system to the user's
interest's evolution."
"In this paper, we develop a dynamic exploration/ exploitation (exr/exp)
strategy for contextual recommender systems (CRS). Specifically, our methods
can adaptively balance the two aspects of exr/exp by automatically learning the
optimal tradeoff. This consists of optimizing a utility function represented by
a linearized form of the probability distributions of the rewards of the
clicked and the non-clicked documents already recommended. Within an offline
simulation framework we apply our algorithms to a CRS and conduct an evaluation
with real event log data. The experimental results and detailed analysis
demonstrate that our algorithms outperform existing algorithms in terms of
click-through-rate (CTR)."
"In this paper, we propose a system for contextual and semantic Arabic
documents classification by improving the standard fuzzy model. Indeed,
promoting neighborhood semantic terms that seems absent in this model by using
a radial basis modeling. In order to identify the relevant documents to the
query. This approach calculates the similarity between related terms by
determining the relevance of each relative to documents (NEAR operator), based
on a kernel function. The use of sliding window improves the process of
classification. The results obtained on a arabic dataset of press show very
good performance compared with the literature."
"Good term selection is an important issue for an automatic query expansion
(AQE) technique. AQE techniques that select expansion terms from the target
corpus usually do so in one of two ways. Distribution based term selection
compares the distribution of a term in the (pseudo) relevant documents with
that in the whole corpus / random distribution. Two well-known
distribution-based methods are based on Kullback-Leibler Divergence (KLD) and
Bose-Einstein statistics (Bo1). Association based term selection, on the other
hand, uses information about how a candidate term co-occurs with the original
query terms. Local Context Analysis (LCA) and Relevance-based Language Model
(RM3) are examples of association-based methods. Our goal in this study is to
investigate how these two classes of methods may be combined to improve
retrieval effectiveness. We propose the following combination-based approach.
Candidate expansion terms are first obtained using a distribution based method.
This set is then refined based on the strength of the association of terms with
the original query terms. We test our methods on 11 TREC collections. The
proposed combinations generally yield better results than each individual
method, as well as other state-of-the-art AQE approaches. En route to our
primary goal, we also propose some modifications to LCA and Bo1 which lead to
improved performance."
"Query in a search engine is generally based on natural language. A query can
be expressed in more than one way without changing its meaning as it depends on
thinking of human being at a particular moment. Aim of the searcher is to get
most relevant results immaterial of how the query has been expressed. In the
present paper, we have examined the results of search engine for change in
coverage and similarity of first few results when a query is entered in two
semantically same but in different formats. Searching has been made through
Google search engine. Fifteen pairs of queries have been chosen for the study.
The t-test has been used for the purpose and the results have been checked on
the basis of total documents found, similarity of first five and first ten
documents found in the results of a query entered in two different formats. It
has been found that the total coverage is same but first few results are
significantly different."
"The purpose of the Switching Detection Challenge in the 2013 WSCD workshop
was to predict users' search engine swithcing actions given records about
search sessions and logs.Our solution adopted the powerful prediction model
Adpredictor and utilized the method of feature engineering. We successfully
applied the click through rate (CTR) prediction model Adpredicitor into our
solution framework, and then the discovery of effective features and the
multiple classification of different switching type make our model outperforms
many competitors. We achieved an AUC score of 0.84255 on the private
leaderboard and ranked the 5th among all the competitors in the competition."
"The Learning to Rank (L2R) research field has experienced a fast paced growth
over the last few years, with a wide variety of benchmark datasets and
baselines available for experimentation. We here investigate the main
assumption behind this field, which is that, the use of sophisticated L2R
algorithms and models, produce significant gains over more traditional and
simple information retrieval approaches. Our experimental results surprisingly
indicate that many L2R algorithms, when put up against the best individual
features of each dataset, may not produce statistically significant
differences, even if the absolute gains may seem large. We also find that most
of the reported baselines are statistically tied, with no clear winner."
"The wide development of mobile applications provides a considerable amount of
data of all types (images, texts, sounds, videos, etc.). Thus, two main issues
have to be considered: assist users in finding information and reduce search
and navigation time. In this sense, context-based recommender systems (CBRS)
propose the user the adequate information depending on her/his situation. Our
work consists in applying machine learning techniques and reasoning process in
order to bring a solution to some of the problems concerning the acceptance of
recommender systems by users, namely avoiding the intervention of experts,
reducing cold start problem, speeding learning process and adapting to the
user's interest. To achieve this goal, we propose a fundamental modification in
terms of how we model the learning of the CBRS. Inspired by models of human
reasoning developed in robotic, we combine reinforcement learning and
case-based reasoning to define a contextual recommendation process based on
different context dimensions (cognitive, social, temporal, geographic). This
paper describes an ongoing work on the implementation of a CBRS based on a
hybrid Q-learning (HyQL) algorithm which combines Q-learning, collaborative
filtering and case-based reasoning techniques. It also presents preliminary
results by comparing HyQL and the standard Q-Learning w.r.t. solving the cold
start problem."
"Hidden links are designed solely for search engines rather than visitors. To
get high search engine rankings, link hiding techniques are usually used for
the profitability of black industries, such as illicit game servers, false
medical services, illegal gambling, and less attractive high-profit industry,
etc. This paper investigates hyperlink hiding techniques on the Web, and gives
a detailed taxonomy. We believe the taxonomy can help develop appropriate
countermeasures. Study on 5,583,451 Chinese sites' home pages indicate that
link hidden techniques are very prevalent on the Web. We also tried to explore
the attitude of Google towards link hiding spam by analyzing the PageRank
values of relative links. The results show that more should be done to punish
the hidden link spam."
"We focus on two research issues in entity search: scoring a document or
snippet that potentially supports a candidate entity, and aggregating scores
from different snippets into an entity score. Proximity scoring has been
studied in IR outside the scope of entity search. However, aggregation has been
hardwired except in a few cases where probabilistic language models are used.
We instead explore simple, robust, discriminative ranking algorithms, with
informative snippet features and broad families of aggregation functions. Our
first contribution is a study of proximity-cognizant snippet features. In
contrast with prior work which uses hardwired ""proximity kernels"" that
implement a fixed decay with distance, we present a ""universal"" feature
encoding which jointly expresses the perplexity (informativeness) of a query
term match and the proximity of the match to the entity mention. Our second
contribution is a study of aggregation functions. Rather than train the ranking
algorithm on snippets and then aggregate scores, we directly train on entities
such that the ranking algorithm takes into account the aggregation function
being used. Our third contribution is an extensive Web-scale evaluation of the
above algorithms on two data sets having quite different properties and
behavior. The first one is the W3C dataset used in TREC-scale enterprise
search, with pre-annotated entity mentions. The second is a Web-scale
open-domain entity search dataset consisting of 500 million Web pages, which
contain about 8 billion token spans annotated automatically with two million
entities from 200,000 entity types in Wikipedia. On the TREC dataset, the
performance of our system is comparable to the currently prevalent systems. On
the much larger and noisier Web dataset, our system delivers significantly
better performance than all other systems, with 8% MAP improvement over the
closest competitor."
"In this paper we study the relationship between query and search engine by
exploring the selective properties based on a simple search engine. We used the
set theory and utilized the words and terms for defining singleton and
doubleton in the event spaces and then provided their implementation for
proving the existence of the shadow of micro-cluster."
"A major computational burden, while performing document clustering, is the
calculation of similarity measure between a pair of documents. Similarity
measure is a function that assigns a real number between 0 and 1 to a pair of
documents, depending upon the degree of similarity between them. A value of
zero means that the documents are completely dissimilar whereas a value of one
indicates that the documents are practically identical. Traditionally,
vector-based models have been used for computing the document similarity. The
vector-based models represent several features present in documents. These
approaches to similarity measures, in general, cannot account for the semantics
of the document. Documents written in human languages contain contexts and the
words used to describe these contexts are generally semantically related.
Motivated by this fact, many researchers have proposed seman-tic-based
similarity measures by utilizing text annotation through external thesauruses
like WordNet (a lexical database). In this paper, we define a semantic
similarity measure based on documents represented in topic maps. Topic maps are
rapidly becoming an industrial standard for knowledge representation with a
focus for later search and extraction. The documents are transformed into a
topic map based coded knowledge and the similarity between a pair of documents
is represented as a correlation between the common patterns (sub-trees). The
experimental studies on the text mining datasets reveal that this new
similarity measure is more effective as compared to commonly used similarity
measures in text clustering."
"Many Information Retrieval (IR) models make use of offline statistical
techniques to score documents for ranking over a single period, rather than use
an online, dynamic system that is responsive to users over time. In this paper,
we explicitly formulate a general Multi Period Information Retrieval problem,
where we consider retrieval as a stochastic yet controllable process. The
ranking action during the process continuously controls the retrieval system's
dynamics, and an optimal ranking policy is found in order to maximise the
overall users' satisfaction over the multiple periods as much as possible. Our
derivations show interesting properties about how the posterior probability of
the documents relevancy evolves from users feedbacks through clicks, and
provides a plug-in framework for incorporating different click models. Based on
the Multi-Armed Bandit theory, we propose a simple implementation of our
framework using a dynamic ranking rule that takes rank bias and exploration of
documents into account. We use TREC data to learn a suitable exploration
parameter for our model, and then analyse its performance and a number of
variants using a search log data set; the experiments suggest an ability to
explore document relevance dynamically over time using user feedback in a way
that can handle rank bias."
"Information seeking process is an important topic in information seeking
behavior research. Both qualitative and empirical methods have been adopted in
analyzing information seeking processes, with major focus on uncovering the
latent search tactics behind user behaviors. Most of the existing works require
defining search tactics in advance and coding data manually. Among the few
works that can recognize search tactics automatically, they missed making sense
of those tactics. In this paper, we proposed using an automatic technique, i.e.
the Hidden Markov Model (HMM), to explicitly model the search tactics. HMM
results show that the identified search tactics of individual information
seeking behaviors are consistent with Marchioninis Information seeking process
model. With the advantages of showing the connections between search tactics
and search actions and the transitions among search tactics, we argue that HMM
is a useful tool to investigate information seeking process, or at least it
provides a feasible way to analyze large scale dataset."
"Ontologies have become the effective modeling for various applications and
significantly in the semantic web. The difficulty of extracting information
from the web, which was created mainly for visualising information, has driven
the birth of the semantic web, which will contain much more resources than the
web and will attach machine-readable semantic information to these resources.
Ontological bootstrapping on a set of predefined sources, such as web services,
must address the problem of multiple, largely unrelated concepts. The web
services consist of basically two components, Web Services Description Language
(WSDL) descriptors and free text descriptors. The WSDL descriptor is evaluated
using two methods, namely Term Frequency/Inverse Document Frequency (TF/IDF)
and web context generation. The proposed bootstrapping ontological process
integrates TF/IDF and web context generation and applies validation using the
free text descriptor service, so that, it offers more accurate definition of
ontologies. This paper uses ranking adaption model which predicts the rank for
a collection of web service documents which leads to the automatic
construction, enrichment and adaptation of ontologies."
"This paper shows that the problem of web services representation is crucial
and analyzes the various factors that influence on it. It presents the
traditional representation of web services considering traditional textual
descriptions based on the information contained in WSDL files. Unfortunately,
textual web services descriptions are dirty and need significant cleaning to
keep only useful information. To deal with this problem, we introduce rules
based text tagging method, which allows filtering web service description to
keep only significant information. A new representation based on such filtered
data is then introduced. Many web services have empty descriptions. Also, we
consider web services representations based on the WSDL file structure (types,
attributes, etc.). Alternatively, we introduce a new representation called
symbolic reputation, which is computed from relationships between web services.
The impact of the use of these representations on web service discovery and
recommendation is studied and discussed in the experimentation using real world
web services."
"The Information and Communication Technologies revolution brought a digital
world with huge amounts of data available. Enterprises use mining technologies
to search vast amounts of data for vital insight and knowledge. Mining tools
such as data mining, text mining, and web mining are used to find hidden
knowledge in large databases or the Internet."
"Most existing approaches in Context-Aware Recommender Systems (CRS) focus on
recommending relevant items to users taking into account contextual
information, such as time, location, or social aspects. However, few of them
have considered the problem of user's content dynamicity. We introduce in this
paper an algorithm that tackles the user's content dynamicity by modeling the
CRS as a contextual bandit algorithm and by including a situation clustering
algorithm to improve the precision of the CRS. Within a deliberately designed
offline simulation framework, we conduct evaluations with real online event log
data. The experimental results and detailed analysis reveal several important
discoveries in context aware recommender system."
"Web content quality measurement is crucial to various web content processing
applications. This paper will explore multi-scale features which may affect the
quality of a host, and develop automatic statistical methods to evaluate the
Web content quality. The extracted properties include statistical content
features, page and host level link features and TFIDF features. The experiments
on ECML/PKDD 2010 Discovery Challenge data set show that the algorithm is
effective and feasible for the quality tasks of multiple languages, and the
multi-scale features have different identification ability and provide good
complement to each other for most tasks."
"Recommendation systems have received considerable attention in the recent
decades. Yet with the development of information technology and social media,
the risk in revealing private data to service providers has been a growing
concern to more and more users. Trade-offs between quality and privacy in
recommendation systems naturally arise. In this paper, we present a privacy
preserving recommendation framework based on groups. The main idea is to use
groups as a natural middleware to preserve users' privacy. A distributed
preference exchange algorithm is proposed to ensure the anonymity of data,
wherein the effective size of the anonymity set asymptotically approaches the
group size with time. We construct a hybrid collaborative filtering model based
on Markov random walks to provide recommendations and predictions to group
members. Experimental results on the MovieLens and Epinions datasets show that
our proposed methods outperform the baseline methods, L+ and ItemRank, two
state-of-the-art personalized recommendation algorithms, for both
recommendation precision and hit rate despite the absence of personal
preference information."
"Considering today's web scenario, there is a need of effective and meaningful
search over the web which is provided by Semantic Web. Existing search engines
are keyword based. They are vulnerable in answering intelligent queries from
the user due to the dependence of their results on information available in web
pages. While semantic search engines provides efficient and relevant results as
the semantic web is an extension of the current web in which information is
given well defined meaning. MetaCrawler is a search tool that uses several
existing search engines and provides combined results by using their own page
ranking algorithm. This paper proposes development of a meta-semantic-search
engine called SemanTelli which works within cloud. SemanTelli fetches results
from different semantic search engines such as Hakia, DuckDuckGo, SenseBot with
the help of intelligent agents that eliminate the limitations of existing
search engines."
"The notion of profile appeared in the 1970s decade, which was mainly due to
the need to create custom applications that could be adapted to the user. In
this paper, we treat the different aspects of the user's profile, defining it,
profile, its features and its indicators of interest, and then we describe the
different approaches of modelling and acquiring the user's interests."
"Recommender systems are popular in e-commerce as they suggest items of
interest to users. Researchers have addressed the cold-start problem where
either the user or the item is new. However, the situation with both new user
and new item has seldom been considered. In this paper, we propose a cold-start
recommendation approach to this situation based on granular association rules.
Specifically, we provide a means for describing users and items through
information granules, a means for generating association rules between users
and items, and a means for recommending items to users using these rules.
Experiments are undertaken on a publicly available dataset MovieLens. Results
indicate that rule sets perform similarly on the training and the testing sets,
and the appropriate setting of granule is essential to the application of
granular association rules."
"Along with the rapid development of information technology, the amount of
information generated at a given time far exceeds human's ability to organize,
search, and manipulate without the help of automatic systems. Now a days so
many tools and techniques are available for storage and retrieval of
information. User uses interface to interact with these techniques, mostly text
user interface (TUI) or graphical user interface (GUI). Here, I am trying to
introduce a new interface i.e. speech for information retrieval. The goal of
this project is to develop a speech interface that can search and read the
required information from the database effectively, efficiently and more
friendly. This tool will be highly useful to blind people, they will able to
demand the information to the computer by giving voice command/s (keyword)
through microphone and listen the required information using speaker or
headphones."
"The information that mobiles can access becomes very wide nowadays, and the
user is faced with a dilemma: there is an unlimited pool of information
available to him but he is unable to find the exact information he is looking
for. This is why the current research aims to design Recommender Systems (RS)
able to continually send information that matches the user's interests in order
to reduce his navigation time. In this paper, we treat the different approaches
to recommend."
"The evolution of the user's content still remains a problem for an accurate
recommendation.This is why the current research aims to design Recommender
Systems (RS) able to continually adapt information that matches the user's
interests. This paper aims to explain this problematic point in outlining the
proposals that have been made in research with their advantages and
disadvantages."
"Crawler-based search engines are the mostly used search engines among web and
Internet users, involve web crawling, storing in database, ranking, indexing
and displaying to the user. But it is noteworthy that because of increasing
changes in web sites search engines suffer high time and transfers costs which
are consumed to investigate the existence of each page in database while
crawling, updating database and even investigating its existence in any
crawling operations. ""Exclusive Web Crawler"" proposes guidelines for crawling
features, links, media and other elements and to store crawling results in a
certain table in its database on the web. With doing this, search engines store
each site's tables in their databases and implement their ranking results on
them. Thus, accuracy of data in every table (and its being up-to-date) is
ensured and no 404 result is shown in search results since, in fact, this data
crawler crawls data entered by webmaster and the database stores whatever he
wants to display."
"The process of browsing Search Results is one of the major problems with
traditional Web search engines for English, European, and any other languages
generally, and for Arabic Language particularly. This process is absolutely
time consuming and the browsing style seems to be unattractive. Organizing Web
search results into clusters facilitates users quick browsing through search
results. Traditional clustering techniques (data-centric clustering algorithms)
are inadequate since they don't generate clusters with highly readable names or
cluster labels. To solve this problem, Description-centric algorithms such as
Suffix Tree Clustering (STC) algorithm have been introduced and used
successfully and extensively with different adapted versions for English,
European, and Chinese Languages. However, till the day of writing this paper,
in our knowledge, STC algorithm has been never applied for Arabic Web Snippets
Search Results Clustering.In this paper, we propose first, to study how STC can
be applied for Arabic Language? We then illustrate by example that is
impossible to apply STC after Arabic Snippets pre-processing (stem or root
extraction) because the Merging process yields many redundant clusters.
Secondly, to overcome this problem, we propose to integrate STC in a new scheme
taking into a count the Arabic language properties in order to get the web more
and more adapted to Arabic users. The proposed approach automatically clusters
the web search results into high quality, and high significant clusters labels.
The obtained clusters not only are coherent, but also can convey the contents
to the users concisely and accurately. Therefore the Arabic users can decide at
a glance whether the contents of a cluster are of interest....."
"Text Categorization is the task of automatically sorting a set of documents
into categories from a predefined set and Text Summarization is a brief and
accurate representation of input text such that the output covers the most
important concepts of the source in a condensed manner. Document Summarization
is an emerging technique for understanding the main purpose of any kind of
documents. This paper presents a model that uses text categorization and text
summarization for searching a document based on user query."
"Recommender systems are important for e-commerce companies as well as
researchers. Recently, granular association rules have been proposed for
cold-start recommendation. However, existing approaches reserve only globally
strong rules; therefore some users may receive no recommendation at all. In
this paper, we propose to mine the top-k granular association rules for each
user. First we define three measures of granular association rules. These are
the source coverage which measures the user granule size, the target coverage
which measures the item granule size, and the confidence which measures the
strength of the association. With the confidence measure, rules can be ranked
according to their strength. Then we propose algorithms for training the
recommender and suggesting items to each user. Experimental are undertaken on a
publicly available data set MovieLens. Results indicate that the appropriate
setting of granule can avoid over-fitting and at the same time, help obtaining
high recommending accuracy."
"In folksonomies, users use to share objects (movies, books, bookmarks, etc.)
by annotating them with a set of tags of their own choice. With the rise of the
Web 2.0 age, users become the core of the system since they are both the
contributors and the creators of the information. Yet, each user has its own
profile and its own ideas making thereby the strength as well as the weakness
of folksonomies. Indeed, it would be helpful to take account of users' profile
when suggesting a list of tags and resources or even a list of friends, in
order to make a personal recommandation, instead of suggesting the more used
tags and resources in the folksonomy. In this paper, we consider users' profile
as a new dimension of a folksonomy classically composed of three dimensions
<users, tags, ressources> and we propose an approach to group users with
equivalent profiles and equivalent interests as quadratic concepts. Then, we
use such structures to propose our personalized recommendation system of users,
tags and resources according to each user's profile. Carried out experiments on
two real-world datasets, i.e., MovieLens and BookCrossing highlight encouraging
results in terms of precision as well as a good social evaluation."
"Recent numerical results show that non-Bayesian knowledge revision may be
helpful in search engine training and optimization. In order to demonstrate how
basic assumption about about the physical nature (and hence the observed
statistics) of retrieved documents can affect the performance of search engines
we suggest an idealized toy model with minimal number of parameters."
"Archiving the web is socially and culturally critical, but presents problems
of scale. The Internet Archive's Wayback Machine can replay captured web pages
as they existed at a certain point in time, but it has limited ability to
provide extensive content and structural metadata about the web graph. While
the live web has developed a rich ecosystem of APIs to facilitate web
applications (e.g., APIs from Google and Twitter), the web archiving community
has not yet broadly implemented this level of access.
  We present ArcLink, a proof-of-concept system that complements open source
Wayback Machine installations by optimizing the construction, storage, and
access to the temporal web graph. We divide the web graph construction into
four stages (filtering, extraction, storage, and access) and explore
optimization for each stage. ArcLink extends the current Web archive interfaces
to return content and structural metadata for each URI. We show how this API
can be applied to such applications as retrieving inlinks, outlinks,
anchortext, and PageRank."
"LETOR is a package of benchmark data sets for research on LEarning TO Rank,
which contains standard features, relevance judgments, data partitioning,
evaluation tools, and several baselines. Version 1.0 was released in April
2007. Version 2.0 was released in Dec. 2007. Version 3.0 was released in Dec.
2008. This version, 4.0, was released in July 2009. Very different from
previous versions (V3.0 is an update based on V2.0 and V2.0 is an update based
on V1.0), LETOR4.0 is a totally new release. It uses the Gov2 web page
collection (~25M pages) and two query sets from Million Query track of TREC
2007 and TREC 2008. We call the two query sets MQ2007 and MQ2008 for short.
There are about 1700 queries in MQ2007 with labeled documents and about 800
queries in MQ2008 with labeled documents. If you have any questions or
suggestions about the datasets, please kindly email us (letor@microsoft.com).
Our goal is to make the dataset reliable and useful for the community."
"In Information Retrieval System (IRS), the Automatic Relevance Feedback (ARF)
is a query reformulation technique that modifies the initial one without the
user intervention. It is applied mainly through the addition of terms coming
from the external resources such as the ontologies and or the results of the
current research. In this context we are mainly interested in the local
analysis technique for the ARF in ad-hoc IRS on Arabic documents. In this
article, we have examined the impact of the variation of the two parameters
implied in this technique, that is to say, the number of the documents
{\guillemotleft}D{\guillemotright} and the number of terms
{\guillemotleft}T{\guillemotright}, on an Arabic IRS performance. The
experimentation, carried out on an Arabic corpus text, enables us to deduce
that there are queries which are not easily improvable with the query
reformulation. In addition, the success of the ARF is due mainly to the
selection of a sufficient number of documents D and to the extraction of a very
reduced set of relevant terms T for retrieval."
"Personalization is being applied to great extend in many systems. This paper
presents a multi-dimensional user data model and its application in web search.
Online and Offline activities of the user are tracked for creating the user
model. The main phases are identification of relevant documents and the
representation of relevance and similarity of the documents. The concepts
Keywords, Topics, URLs and clusters are used in the implementation. The
algorithms for profiling, grading and clustering the concepts in the user model
and algorithm for determining the personalized search results by re-ranking the
results in a search bank are presented in this paper. Simple experiments for
evaluation of the model and their results are described."
"This paper describes an enhanced automatic keyphrase extraction method
applied to Broadcast News. The keyphrase extraction process is used to create a
concept level for each news. On top of words resulting from a speech
recognition system output and news indexation and it contributes to the
generation of a tag/keyphrase cloud of the top news included in a Multimedia
Monitoring Solution system for TV and Radio news/programs, running daily, and
monitoring 12 TV channels and 4 Radios."
"The process of predicting news stories popularity from several news sources
has become a challenge of great importance for both news producers and readers.
In this paper, we investigate methods for automatically predicting the number
of clicks on a news story during one hour. Our approach is a combination of
additive regression and bagging applied over a M5P regression tree using a
logarithmic scale (log10). The features included are social-based (social
network metadata from Facebook), content-based (automatically extracted
keyphrases, and stylometric statistics from news titles), and time-based. In
1st Sapo Data Challenge we obtained 11.99% as mean relative error value which
put us in the 4th place out of 26 participants."
"The increasing popularity of Twitter renders improved trustworthiness and
relevance assessment of tweets much more important for search. However, given
the limitations on the size of tweets, it is hard to extract measures for
ranking from the tweets' content alone. We present a novel ranking method,
called RAProp, which combines two orthogonal measures of relevance and
trustworthiness of a tweet. The first, called Feature Score, measures the
trustworthiness of the source of the tweet. This is done by extracting features
from a 3-layer twitter ecosystem, consisting of users, tweets and the pages
referred to in the tweets. The second measure, called agreement analysis,
estimates the trustworthiness of the content of the tweet, by analyzing how and
whether the content is independently corroborated by other tweets. We view the
candidate result set of tweets as the vertices of a graph, with the edges
measuring the estimated agreement between each pair of tweets. The feature
score is propagated over this agreement graph to compute the top-k tweets that
have both trustworthy sources and independent corroboration. The evaluation of
our method on 16 million tweets from the TREC 2011 Microblog Dataset shows that
for top-30 precision we achieve 53% higher than current best performing method
on the Dataset and over 300% over current Twitter Search. We also present a
detailed internal empirical evaluation of RAProp in comparison to several
alternative approaches proposed by us."
"With the rapid growth of the Internet and overwhelming amount of information
and choices that people are confronted with, recommender systems have been
developed to effectively support users' decision-making process in the online
systems. However, many recommendation algorithms suffer from the data sparsity
problem, i.e. the user-object bipartite networks are so sparse that algorithms
cannot accurately recommend objects for users. This data sparsity problem makes
many well-known recommendation algorithms perform poorly. To solve the problem,
we propose a recommendation algorithm based on the semi-local diffusion process
on a user-object bipartite network. The numerical simulation on two sparse
datasets, Amazon and Bookcross, show that our method significantly outperforms
the state-of-the-art methods especially for those small-degree users. Two
personalized semi-local diffusion methods are proposed which further improve
the recommendation accuracy. Finally, our work indicates that sparse online
systems are essentially different from the dense online systems, all the
algorithms and conclusions based on dense data should be rechecked again in
sparse data."
"Nowadays, web archives preserve the history of large portions of the web. As
medias are shifting from printed to digital editions, accessing these huge
information sources is drawing increasingly more attention from national and
international institutions, as well as from the research community. These
collections are intrinsically big, leading to index files that do not fit into
the memory and an increase query response time. Decreasing the index size is a
direct way to decrease this query response time.
  Static index pruning methods reduce the size of indexes by removing a part of
the postings. In the context of web archives, it is necessary to remove
postings while preserving the temporal diversity of the archive. None of the
existing pruning approaches take (temporal) diversification into account.
  In this paper, we propose a diversification-based static index pruning
method. It differs from the existing pruning approaches by integrating
diversification within the pruning context. We aim at pruning the index while
preserving retrieval effectiveness and diversity by pruning while maximizing a
given IR evaluation metric like DCG. We show how to apply this approach in the
context of web archives. Finally, we show on two collections that search
effectiveness in temporal collections after pruning can be improved using our
approach rather than diversity oblivious approaches."
"The Hidden Web is the vast repository of informational databases available
only through search form interfaces, accessible by therein typing a set of
keywords in the search forms. Typically, a Hidden Web crawler is employed to
autonomously discover and download pages from the Hidden Web. Traditional
hidden web crawlers do not provide the search engines with an optimal search
experience because of the excessive number of search requests posed through the
form interface so as to exhaustively crawl and retrieve the contents of the
target hidden web database. Here in our work, we provide a framework to
investigate the problem of optimal search and curtail it by proposing an
effective query term selection approach based on the frequency & distribution
of terms in the document database. The paper focuses on developing a
term-weighing scheme called VarDF (acronym for variable document frequency)
that can ease the identification of optimal terms to be used as queries on the
interface for maximizing the achieved coverage of the crawler which in turn
will facilitate the search engine to have a diversified and expanded index. We
experimentally evaluate the effectiveness of our approach on a manually created
database of documents in the area of Information Retrieval."
"Myers-Briggs Type Indicator (MBTI) types depict the psychological preferences
by which a person perceives the world and make decisions. There are 4 principal
functions through which the people see the world: sensation, intuition,
feeling, and thinking. These functions along with the Introverted\Extroverted
nature of the person, there are 16 personalities types, the humans are divided
into. Here an idea is presented where a user can get recommendations for books,
web media content, music and movies on the basis of the users' MBTI type. Only
things like books and other media content has been chosen because the
preferences in such things are mostly subjective. Apart from the recommended
content that is generally generated on the basis of the previous purchases,
searches can be enhanced by using the MBTI. A minimalist survey was designed
for collecting the data. This has a more than 100 features that show the
preference of a personality type. Those include preferences in book genres,
music genres, movie genres and even video games genres. After analyzing the
data that is collected from the survey, some inferences were drawn from it
which can be used to design a new recommendation engine for recommending the
content that coincides with the personality of the user."
"In this paper, we propose a new text categorization framework based on
Concepts Lattice and cellular automata. In this framework, concept structure
are modeled by a Cellular Automaton for Symbolic Induction (CASI). Our
objective is to reduce time categorization caused by the Concept Lattice. We
examine, by experiments the performance of the proposed approach and compare it
with other algorithms such as Naive Bayes and k nearest neighbors. The results
show performance improvement while reducing time categorization."
"Cloud Mining technique can be applied to various documents. Acquisition and
storage of video data is an easy task but retrieval of information from video
data is a challenging task. So video Cloud Mining plays an important role in
efficient video data management for information retrieval. This paper proposes
a Cellular Automata based framework for video Cloud Mining to extract the
information from video data. This includes developing the technique for shot
detection then key frame analysis is considered to compare the frames of each
shot to each others to define the relationship between shots. Cellular automata
based hierarchical clustering technique is adopted to make a group of similar
shots to detect the particular event on some requirement as per user demand."
"The modern geographic information retrieval technology is based on
quantitative models and methods. The semantic information in web documents and
queries cannot be effectively represented, leading to information lost or
misunderstanding so that the results are either unreliable or inconsistent. A
new qualitative approach is thus proposed for supporting geographic information
retrieval based on qualitative representation, semantic matching, and
qualitative reasoning. A qualitative representation model and the corresponding
similarity measurement method are defined. Information in documents and user
queries are represented using propositional logic, which considers the thematic
and geographic semantics synthetically. Thematic information is represented as
thematic propositions on the base of domain ontology. Similarly, spatial
information is represented as geo-spatial propositions with the support of
geographic knowledge base. Then the similarity is divided into thematic
similarity and spatial similarity. The former is calculated by the weighted
distance of proposition keywords in the domain ontology, and the latter
similarity is further divided into conceptual similarity and spatial
similarity. Represented by propositions and information units, the similarity
measurement can take evidence theory and fuzzy logic to combine all sub
similarities to get the final similarity between documents and queries. This
novel retrieval method is mainly used to retrieve the qualitative geographic
information to support the semantic matching and results ranking. It does not
deal with geometric computation and is consistent with human commonsense
cognition, and thus can improve the efficiency of geographic information
retrieval technology."
"Text Categorization is traditionally done by using the term frequency and
inverse document frequency.This type of method is not very good because, some
words which are not so important may appear in the document .The term frequency
of unimportant words may increase and document may be classified in the wrong
category.For reducing the error of classifying of documents in wrong category.
The Distributional features are introduced. In the Distribuional Features, the
Distribution of the words in the whole document is analyzed. Whole Document is
very closely analyzed for different measures like FirstAppearence, Last
Appearance, Centriod, Count, etc.The measures are calculated and they are used
in tf*idf equation and result is used in k- nearest neighbor and K-means
algorithm for classifying the documents."
"Thinking of todays web search scenario which is mainly keyword based, leads
to the need of effective and meaningful search provided by Semantic Web.
Existing search engines are vulnerable to provide relevant answers to users
query due to their dependency on simple data available in web pages. On other
hand, semantic search engines provide efficient and relevant results as the
semantic web manages information with well defined meaning using ontology. A
Meta-Search engine is a search tool that forwards users query to several
existing search engines and provides combined results by using their own page
ranking algorithm. SemanTelli is a meta semantic search engine that fetches
results from different semantic search engines such as Hakia, DuckDuckGo,
SenseBot through intelligent agents. This paper proposes enhancement of
SemanTelli with improved snippet analysis based page ranking algorithm and
support for image and news search."
"With the increase in the number of web services, many web services are
available on internet providing the same functionality, making it difficult to
choose the best one, fulfilling users all requirements. This problem can be
solved by considering the quality of web services to distinguish functionally
similar web services. Nine different quality parameters are considered. Web
services can be classified and ranked using decision tree approach since they
do not require long training period and can be easily interpreted. Various
decision tree and rules approaches available are applied and tested to find the
optimal decision method to correctly classify functionally similar web services
considering their quality parameters."
"In this world, globalization has become a basic and most popular human trend.
To globalize information, people are going to publish the documents in the
internet. As a result, information volume of internet has become huge. To
handle that huge volume of information, Web searcher uses search engines. The
Webpage indexing mechanism of a search engine plays a big role to retrieve Web
search results in a faster way from the huge volume of Web resources. Web
researchers have introduced various types of Web-page indexing mechanism to
retrieve Webpages from Webpage repository. In this paper, we have illustrated a
new approach of design and development of Webpage indexing. The proposed
Webpage indexing mechanism has applied on domain specific Webpages and we have
identified the Webpage domain based on an Ontology. In our approach, first we
prioritize the Ontology terms that exist in the Webpage content then apply our
own indexing mechanism to index that Webpage. The main advantage of storing an
index is to optimize the speed and performance while finding relevant documents
from the domain specific search engine storage area for a user given search
query."
"As todays world grows with the technology on the other hand it seems to be
small with the World Wide Web. With the use of Internet more and more
information can be search from the web. When Users fires a query they want
relevancy in obtained results. In general, search engines perform the ranking
of web pages in an offline mode, which is after the web pages have been
retrieved and stored in the database. But most of the time this method does not
provide relevant results as most of the search engines were using some ranking
algorithms like page Rank, HITS, SALSA and Hilltop. Where these algorithms does
not always provides the results based on the semantic web. So a concept of
Ontology is been introduced in search engines to get more meaningful and
relevant results with respect to the users query.Ontologies are used to capture
knowledge about some domain of interest. Ontology describes the concepts in the
domain and also the relationships that hold between those concepts. Different
ontology languages provide different facilities. The most recent development in
standard ontology languages is OWL (Ontology Web Language) from the World Wide
Web Consortium. OWL makes it possible to describe concept to its full extent
and enables the search engines to provide accurate results to the user."
"As the growing interest of web recommendation systems those are applied to
deliver customized data for their users, we started working on this system.
Generally the recommendation systems are divided into two major categories such
as collaborative recommendation system and content based recommendation system.
In case of collaborative recommen-dation systems, these try to seek out users
who share same tastes that of given user as well as recommends the websites
according to the liking given user. Whereas the content based recommendation
systems tries to recommend web sites similar to those web sites the user has
liked. In the recent research we found that the efficient technique based on
asso-ciation rule mining algorithm is proposed in order to solve the problem of
web page recommendation. Major problem of the same is that the web pages are
given equal importance. Here the importance of pages changes according to the
fre-quency of visiting the web page as well as amount of time user spends on
that page. Also recommendation of newly added web pages or the pages those are
not yet visited by users are not included in the recommendation set. To
over-come this problem, we have used the web usage log in the adaptive
association rule based web mining where the asso-ciation rules were applied to
personalization. This algorithm was purely based on the Apriori data mining
algorithm in order to generate the association rules. However this method also
suffers from some unavoidable drawbacks. In this paper we are presenting and
investigating the new approach based on weighted Association Rule Mining
Algorithm and text mining. This is improved algorithm which adds semantic
knowledge to the results, has more efficiency and hence gives better quality
and performances as compared to existing approaches."
"Today web is the best medium of communication in modern business. Many
companies are redefining their business strategies to improve the business
output. Business over internet provides the opportunity to customers and
partners where their products and specific business can be found. Nowadays
online business breaks the barrier of time and space as compared to the
physical office. Big companies around the world are realizing that e-commerce
is not just buying and selling over Internet, rather it improves the efficiency
to compete with other giants in the market. For this purpose data mining
sometimes called as knowledge discovery is used. Web mining is data mining
technique that is applied to the WWW. There are vast quantities of information
available over the Internet."
"In this paper, we try to answer the question of how to improve the
state-of-the-art methods for relevance ranking in web search by query
segmentation. Here, by query segmentation it is meant to segment the input
query into segments, typically natural language phrases, so that the
performance of relevance ranking in search is increased. We propose employing
the re-ranking approach in query segmentation, which first employs a generative
model to create top $k$ candidates and then employs a discriminative model to
re-rank the candidates to obtain the final segmentation result. The method has
been widely utilized for structure prediction in natural language processing,
but has not been applied to query segmentation, as far as we know. Furthermore,
we propose a new method for using the result of query segmentation in relevance
ranking, which takes both the original query words and the segmented query
phrases as units of query representation. We investigate whether our method can
improve three relevance models, namely BM25, key n-gram model, and dependency
model. Our experimental results on three large scale web search datasets show
that our method can indeed significantly improve relevance ranking in all the
three cases."
"Recommender systems are needed to find food items of ones interest. We review
recommender systems and recommendation methods. We propose a food
personalization framework based on adaptive hypermedia. We extend Hermes
framework with food recommendation functionality. We combine TF-IDF term
extraction method with cosine similarity measure. Healthy heuristics and
standard food database are incorporated into the knowledgebase. Based on the
performed evaluation, we conclude that semantic recommender systems in general
outperform traditional recommenders systems with respect to accuracy,
precision, and recall, and that the proposed recommender has a better F-measure
than existing semantic recommenders."
"The query suggestion or auto-completion mechanisms help users to type less
while interacting with a search engine. A basic approach that ranks suggestions
according to their frequency in the query logs is suboptimal. Firstly, many
candidate queries with the same prefix can be removed as redundant. Secondly,
the suggestions can also be personalised based on the user's context. These two
directions to improve the aforementioned mechanisms' quality can be in
opposition: while the latter aims to promote suggestions that address search
intents that a user is likely to have, the former aims to diversify the
suggestions to cover as many intents as possible. We introduce a
contextualisation framework that utilises a short-term context using the user's
behaviour within the current search session, such as the previous query, the
documents examined, and the candidate query suggestions that the user has
discarded. This short-term context is used to contextualise and diversify the
ranking of query suggestions, by modelling the user's information need as a
mixture of intent-specific user models. The evaluation is performed offline on
a set of approximately 1.0M test user sessions. Our results suggest that the
proposed approach significantly improves query suggestions compared to the
baseline approach."
"Some of the main ranking features of today's search engines reflect result
popularity and are based on ranking models, such as PageRank, implicit feedback
aggregation, and more. While such features yield satisfactory results for a
wide range of queries, they aggravate the problem of search for ambiguous
entities: Searching for a person yields satisfactory results only if the person
we are looking for is represented by a high-ranked Web page and all required
information are contained in this page. Otherwise, the user has to either
reformulate/refine the query or manually inspect low-ranked results to find the
person in question. A possible approach to solve this problem is to cluster the
results, so that each cluster represents one of the persons occurring in the
answer set. However clustering search results has proven to be a difficult
endeavor by itself, where the clusters are typically of moderate quality.
  A wealth of useful information about persons occurs in Web 2.0 platforms,
such as LinkedIn, Wikipedia, Facebook, etc. Being human-generated, the
information on these platforms is clean, focused, and already disambiguated. We
show that when searching for ambiguous person names the information from such
platforms can be bootstrapped to group the results according to the individuals
occurring in them. We have evaluated our methods on a hand-labeled dataset of
around 5,000 Web pages retrieved from Google queries on 50 ambiguous person
names."
"This report describes metrics for the evaluation of the effectiveness of
segment-based retrieval based on existing binary information retrieval metrics.
This metrics are described in the context of a task for the hyperlinking of
video segments. This evaluation approach re-uses existing evaluation measures
from the standard Cranfield evaluation paradigm. Our adaptation approach can in
principle be used with any kind of effectiveness measure that uses binary
relevance, and for other segment-baed retrieval tasks. In our video
hyperlinking setting, we use precision at a cut-off rank n and mean average
precision."
"Machine learning for text classification is the underpinning of document
cataloging, news filtering, document steering and exemplification. In text
mining realm, effective feature selection is significant to make the learning
task more accurate and competent. One of the traditional lazy text classifier
k-Nearest Neighborhood (kNN) has a major pitfall in calculating the similarity
between all the objects in training and testing sets, there by leads to
exaggeration of both computational complexity of the algorithm and massive
consumption of main memory. To diminish these shortcomings in viewpoint of a
data-mining practitioner an amalgamative technique is proposed in this paper
using a novel restructured version of kNN called AugmentedkNN(AkNN) and
k-Medoids(kMdd) clustering.The proposed work comprises preprocesses on the
initial training set by imposing attribute feature selection for reduction of
high dimensionality, also it detects and excludes the high-fliers samples in
the initial training set and restructures a constrictedtraining set. The kMdd
clustering algorithm generates the cluster centers (as interior objects) for
each category and restructures the constricted training set with centroids.
This technique is amalgamated with AkNNclassifier that was prearranged with
text mining similarity measures. Eventually, significantweights and ranks were
assigned to each object in the new training set based upon their accessory
towards the object in testing set. Experiments conducted on Reuters-21578 a UCI
benchmark text mining data set, and comparisons with traditional kNNclassifier
designates the referredmethod yieldspreeminentrecitalin both clustering and
classification."
"Voice search is becoming a popular mode for interacting with search engines.
As a result, research has gone into building better voice transcription
engines, interfaces, and search engines that better handle inherent verbosity
of queries. However, when one considers its use by non- native speakers of
English, another aspect that becomes important is the formulation of the query
by users. In this paper, we present the results of a preliminary study that we
conducted with non-native English speakers who formulate queries for given
retrieval tasks. Our results show that the current search engines are sensitive
in their rankings to the query formulation, and thus highlights the need for
developing more robust ranking methods."
"Starting from an unsolved problem of information retrieval this paper
presents an ontology-based model for indexing and retrieval. The model combines
the methods and experiences of cognitive-to-interpret indexing languages with
the strengths and possibilities of formal knowledge representation. The core
component of the model uses inferences along the paths of typed relations
between the entities of a knowledge representation for enabling the
determination of hit quantities in the context of retrieval processes. The
entities are arranged in aspect-oriented facets to ensure a consistent
hierarchical structure. The possible consequences for indexing and retrieval
are discussed."
"In this paper, we introduce a tag recommendation algorithm that mimics the
way humans draw on items in their long-term memory. This approach uses the
frequency and recency of previous tag assignments to estimate the probability
of reusing a particular tag. Using three real-world folksonomies gathered from
bookmarks in BibSonomy, CiteULike and Flickr, we show how adding a
time-dependent component outperforms conventional ""most popular tags""
approaches and another existing and very effective but less theory-driven,
time-dependent recommendation mechanism. By combining our approach with a
simple resource-specific frequency analysis, our algorithm outperforms other
well-established algorithms, such as FolkRank, Pairwise Interaction Tensor
Factorization and Collaborative Filtering. We conclude that our approach
provides an accurate and computationally efficient model of a user's temporal
tagging behavior. We show how effective principles for information retrieval
can be designed and implemented if human memory processes are taken into
account."
"We assume that recommender systems are more successful, when they are based
on a thorough understanding of how people process information. In the current
paper we test this assumption in the context of social tagging systems.
Cognitive research on how people assign tags has shown that they draw on two
interconnected levels of knowledge in their memory: on a conceptual level of
semantic fields or topics, and on a lexical level that turns patterns on the
semantic level into words. Another strand of tagging research reveals a strong
impact of time dependent forgetting on users' tag choices, such that recently
used tags have a higher probability being reused than ""older"" tags. In this
paper, we align both strands by implementing a computational theory of human
memory that integrates the two-level conception and the process of forgetting
in form of a tag recommender and test it in three large-scale social tagging
datasets (drawn from BibSonomy, CiteULike and Flickr).
  As expected, our results reveal a selective effect of time: forgetting is
much more pronounced on the lexical level of tags. Second, an extensive
evaluation based on this observation shows that a tag recommender
interconnecting both levels and integrating time dependent forgetting on the
lexical level results in high accuracy predictions and outperforms other
well-established algorithms, such as Collaborative Filtering, Pairwise
Interaction Tensor Factorization, FolkRank and two alternative time dependent
approaches. We conclude that tag recommenders can benefit from going beyond the
manifest level of word co-occurrences, and from including forgetting processes
on the lexical level."
"This presentation focuses on the automatic expansion of Arabic request using
morphological analyzer and Arabic Wordnet. The expanded request is sent to
Google."
"Authors of biomedical publications use gel images to report experimental
results such as protein-protein interactions or protein expressions under
different conditions. Gel images offer a concise way to communicate such
findings, not all of which need to be explicitly discussed in the article text.
This fact together with the abundance of gel images and their shared common
patterns makes them prime candidates for automated image mining and parsing. We
introduce an approach for the detection of gel images, and present a workflow
to analyze them. We are able to detect gel segments and panels at high
accuracy, and present preliminary results for the identification of gene names
in these images. While we cannot provide a complete solution at this point, we
present evidence that this kind of image mining is feasible."
"Content-based and collaborative filtering methods are the most successful
solutions in recommender systems. Content based method is based on items
attributes. This method checks the features of users favourite items and then
proposes the items which have the most similar characteristics with those
items. Collaborative filtering method is based on the determination of similar
items or similar users, which are called item-based and user-based
collaborative filtering, respectively.In this paper we propose a hybrid method
that integrates collaborative filtering and content-based methods. The proposed
method can be viewed as user-based Collaborative filtering technique. However
to find users with similar taste with active user, we used content features of
the item under investigation to put more emphasis on users rating for similar
items. In other words two users are similar if their ratings are similar on
items that have similar context. This is achieved by assigning a weight to each
rating when calculating the similarity of two users.We used movielens data set
to access the performance of the proposed method in comparison with basic
user-based collaborative filtering and other popular methods."
"Recent decade has witnessed the increasing popularity of recommender systems,
which help users acquire relevant commodities and services from overwhelming
resources on Internet. Some simple physical diffusion processes have been used
to design effective recommendation algorithms for user-object bipartite
networks, typically mass diffusion (MD) and heat conduction (HC) algorithms
which have different advantages respectively on accuracy and diversity. In this
paper, we investigate the effect of weight assignment in the hybrid of MD and
HC, and find that a new hybrid algorithm of MD and HC with balanced weights
will achieve the optimal recommendation results, we name it balanced diffusion
(BD) algorithm. Numerical experiments on three benchmark data sets, MovieLens,
Netflix and RateYourMusic (RYM), show that the performance of BD algorithm
outperforms the existing diffusion-based methods on the three important
recommendation metrics, accuracy, diversity and novelty. Specifically, it can
not only provide accurately recommendation results, but also yield higher
diversity and novelty in recommendations by accurately recommending unpopular
objects."
"With the rapid growth of the Internet and overwhelming amount of information
that people are confronted with, recommender systems have been developed to
effiectively support users' decision-making process in online systems. So far,
much attention has been paid to designing new recommendation algorithms and
improving existent ones. However, few works considered the different
contributions from different users to the performance of a recommender system.
Such studies can help us improve the recommendation efficiency by excluding
irrelevant users. In this paper, we argue that in each online system there
exists a group of core users who carry most of the information for
recommendation. With them, the recommender systems can already generate
satisfactory recommendation. Our core user extraction method enables the
recommender systems to achieve 90% of the accuracy by taking only 20% of the
data into account."
"The WorldWideWeb (WWW) is a huge conservatory of web pages. Search Engines
are key applications that fetch web pages for the user query. In the current
generation web architecture, search engines treat keywords provided by the user
as isolated keywords without considering the context of the user query. This
results in a lot of unrelated pages or links being displayed to the user.
Semantic Web is based on the current web with a revised framework to display a
more precise result set as response to a user query. The current web pages need
to be annotated by finding relevant meta data to be added to each of them, so
that they become useful to Semantic Web search engines. Semantic Look explores
the context of user query by processing the Semantic information recorded in
the web pages. It is compared with an existing algorithm called OntoLook and it
is shown that Semantic Look is a better optimized search engine by being more
than twice as fast as OntoLook."
"The semantic mapping problem is probably the main obstacle to
computer-to-computer communication. If computer A knows that its concept X is
the same as computer B's concept Y, then the two machines can communicate. They
will in effect be talking the same language. This paper describes a relatively
straightforward way of enhancing the semantic descriptions of Web Service
interfaces by using online sources of keyword definitions. Method interface
descriptions can be enhanced using these standard dictionary definitions.
Because the generated metadata is now standardised, this means that any other
computer that has access to the same source, or understands standard language
concepts, can now understand the description. This helps to remove a lot of the
heterogeneity that would otherwise build up though humans creating their own
descriptions independently of each other. The description comes in the form of
an XML script that can be retrieved and read through the Web Service interface
itself. An additional use for these scripts would be for adding descriptions in
different languages, which would mean that human users that speak a different
language would also understand what the service was about."
"The information available on web pages mostly contains semi-structured text
documents which are represented either in XML, or HTML, or XHTML format that
lacks formatted document structure. The document does not discriminate between
the text and the schema that represent the text. Also the amount of structure
used to represent the text depends on the purpose and size of text document. No
semantic is applied to semi-structured documents. This requires extracting core
contents of text document to analyse words or sentences to generate useful
knowledge. This paper discusses several techniques and approaches useful for
extracting core content from semi-structured text documents and their merits
and demerits"
"This paper describes a method for creating structure from heterogeneous
sources, as part of an information database, or more specifically, a 'concept
base'. Structures called 'concept trees' can grow from the semi-structured
sources when consistent sequences of concepts are presented. They might be
considered to be dynamic databases, possibly a variation on the distributed
Agent-Based or Cellular Automata models, or even related to Markov models.
Semantic comparison of text is required, but the trees can be built more, from
automatic knowledge and statistical feedback. This reduced model might also be
attractive for security or privacy reasons, as not all of the potential data
gets saved. The construction process maintains the key requirement of
generality, allowing it to be used as part of a generic framework. The nature
of the method also means that some level of optimisation or normalisation of
the information will occur. This gives comparisons with databases or
knowledge-bases, but a database system would firstly model its environment or
datasets and then populate the database with instance values. The concept base
deals with a more uncertain environment and therefore cannot fully model it
beforehand. The model itself therefore evolves over time. Similar to databases,
it also needs a good indexing system, where the construction process provides
memory and indexing structures. These allow for more complex concepts to be
automatically created, stored and retrieved, possibly as part of a more
cognitive model. There are also some arguments, or more abstract ideas, for
merging physical-world laws into these automatic processes."
"Sponsored search adopts generalized second price (GSP) auction mechanism
which works on the concept of pay per click which is most commonly used for the
allocation of slots in the searched page. Two main aspects associated with GSP
are the bidding amount and the click through rate (CTR). The CTR learning
algorithms currently being used works on the basic principle of (#clicks_i/
#impressions_i) under a fixed window of clicks or impressions or time. CTR are
prone to fraudulent clicks, resulting in sudden increase of CTR. The current
algorithms are unable to find the solutions to stop this, although with the use
of machine learning algorithms it can be detected that fraudulent clicks are
being generated. In our paper, we have used the concept of relative ranking
which works on the basic principle of (#clicks_i /#clicks_t). In this
algorithm, both the numerator and the denominator are linked. As #clicks_t is
higher than previous algorithms and is linked to the #clicks_i, the small
change in the clicks which occurs in the normal scenario have a very small
change in the result but in case of fraudulent clicks the number of clicks
increases or decreases rapidly which will add up with the normal clicks to
increase the denominator, thereby decreasing the CTR."
"A large amount of data is present on the web. It contains huge number of web
pages and to find suitable information from them is very cumbersome task. There
is need to organize data in formal manner so that user can easily access and
use them. To retrieve information from documents, we have many Information
Retrieval (IR) techniques. Current IR techniques are not so advanced that they
can be able to exploit semantic knowledge within documents and give precise
results. IR technology is major factor responsible for handling annotations in
Semantic Web (SW) languages and in the present paper knowledgeable
representation languages used for retrieving information are discussed."
"Recently, there is a surge of interests on heterogeneous information network
analysis. As a newly emerging network model, heterogeneous information networks
have many unique features (e.g., complex structure and rich semantics) and a
number of interesting data mining tasks have been exploited in this kind of
networks, such as similarity measure, clustering, and classification. Although
evaluating the importance of objects has been well studied in homogeneous
networks, it is not yet exploited in heterogeneous networks. In this paper, we
study the ranking problem in heterogeneous networks and propose the HRank
framework to evaluate the importance of multiple types of objects and meta
paths. Since the importance of objects depends upon the meta paths in
heterogeneous networks, HRank develops a path based random walk process.
Moreover, a constrained meta path is proposed to subtly capture the rich
semantics in heterogeneous networks. Furthermore, HRank can simultaneously
determine the importance of objects and meta paths through applying the tensor
analysis. Extensive experiments on three real datasets show that HRank can
effectively evaluate the importance of objects and paths together. Moreover,
the constrained meta path shows its potential on mining subtle semantics by
obtaining more accurate ranking results."
"In the recent years there has been an increase in scientific papers
publications in Albania and its neighboring countries that have large
communities of Albanian speaking researchers. Many of these papers are written
in Albanian. It is a very time consuming task to find papers related to the
researchers' work, because there is no concrete system that facilitates this
process. In this paper we present the design of a modular intelligent search
system for articles written in Albanian. The main part of it is the recommender
module that facilitates searching by providing relevant articles to the users
(in comparison with a given one). We used a cosine similarity based heuristics
that differentiates the importance of term frequencies based on their location
in the article. We did not notice big differences on the recommendation results
when using different combinations of the importance factors of the keywords,
title, abstract and body. We got similar results when using only the title and
abstract in comparison with the other combinations. Because we got fairly good
results in this initial approach, we believe that similar recommender systems
for documents written in Albanian can be build also in contexts not related to
scientific publishing."
"As the information contained within the web is increasing day by day,
organizing this information could be a necessary requirement.The data mining
process is to extract information from a data set and transform it into an
understandable structure for further use. Classification of web page content is
essential to many tasks in web information retrieval such as maintaining web
directories and focused crawling.The uncontrolled type of nature of web content
presents additional challenges to web page classification as compared to the
traditional text classification, but the interconnected nature of hypertext
also provides features that can assist the process. In this paper the web
classification is discussed in detail and its importance in field of data
mining is explored."
"Web crawlers visit internet applications, collect data, and learn about new
web pages from visited pages. Web crawlers have a long and interesting history.
Early web crawlers collected statistics about the web. In addition to
collecting statistics about the web and indexing the applications for search
engines, modern crawlers can be used to perform accessibility and vulnerability
checks on the application. Quick expansion of the web, and the complexity added
to web applications have made the process of crawling a very challenging one.
Throughout the history of web crawling many researchers and industrial groups
addressed different issues and challenges that web crawlers face. Different
solutions have been proposed to reduce the time and cost of crawling.
Performing an exhaustive crawl is a challenging question. Additionally
capturing the model of a modern web application and extracting data from it
automatically is another open question. What follows is a brief history of
different technique and algorithms used from the early days of crawling up to
the recent days. We introduce criteria to evaluate the relative performance of
web crawlers. Based on these criteria we plot the evolution of web crawlers and
compare their performance"
"Recommender system has attracted lots of attentions since it helps users
alleviate the information overload problem. Matrix factorization technique is
one of the most widely employed collaborative filtering techniques in the
research of recommender systems due to its effectiveness and efficiency in
dealing with very large user-item rating matrices. Recently, based on the
intuition that additional information provides useful insights for matrix
factorization techniques, several recommendation algorithms have utilized
additional information to improve the performance of matrix factorization
methods. However, the majority focus on dealing with the cold start user
problem and ignore the cold start item problem. In addition, there are few
suitable similarity measures for these content enhanced matrix factorization
approaches to compute the similarity between categorical items. In this paper,
we propose attributes coupling based item enhanced matrix factorization method
by incorporating item attribute information into matrix factorization technique
as well as adapting the coupled object similarity to capture the relationship
between items. Item attribute information is formed as an item relationship
regularization term to regularize the process of matrix factorization.
Specifically, the similarity between items is measured by the Coupled Object
Similarity considering coupling between items. Experimental results on two real
data sets show that our proposed method outperforms state-of-the-art
recommendation algorithms and can effectively cope with the cold start item
problem when more item attribute information is available."
"We used Lemur Toolkit, an open source toolkit designed for Information
Retrieval (IR) research, for our automated indexing and retrieval experiments
on a TREC-like test collection for Turkish. We study and compare three
retrieval models Lemur supports, especially Language modeling approach to IR,
combined with language specific preprocessing techniques. Our experiments show
that all retrieval models benefits from language specific preprocessing in
terms of retrieval quality. Also Language Modeling approach is the best
performing retrieval model when language specific preprocessing applied."
"Recent research has unveiled the importance of online social networks for
improving the quality of recommender systems and encouraged the research
community to investigate better ways of exploiting the social information for
recommendations. To contribute to this sparse field of research, in this paper
we exploit users' interactions along three data sources (marketplace, social
network and location-based) to assess their performance in a barely studied
domain: recommending products and domains of interests (i.e., product
categories) to people in an online marketplace environment. To that end we
defined sets of content- and network-based user similarity features for each
data source and studied them isolated using an user-based Collaborative
Filtering (CF) approach and in combination via a hybrid recommender algorithm,
to assess which one provides the best recommendation performance.
Interestingly, in our experiments conducted on a rich dataset collected from
SecondLife, a popular online virtual world, we found that recommenders relying
on user similarity features obtained from the social network data clearly
yielded the best results in terms of accuracy in case of predicting products,
whereas the features obtained from the marketplace and location-based data
sources also obtained very good results in case of predicting categories. This
finding indicates that all three types of data sources are important and should
be taken into account depending on the level of specialization of the
recommendation task."
"In this paper, we present work-in-progress on SocRecM, a novel social
recommendation framework for online marketplaces. We demonstrate that SocRecM
is not only easy to integrate with existing Web technologies through a RESTful,
scalable and easy-to-extend service-based architecture but also reveal the
extent to which various social features and recommendation approaches are
useful in an online social marketplace environment."
"Search engine retrieval effectiveness studies are usually small-scale, using
only limited query samples. Furthermore, queries are selected by the
researchers. We address these issues by taking a random representative sample
of 1,000 informational and 1,000 navigational queries from a major German
search engine and comparing Google's and Bing's results based on this sample.
Jurors were found through crowdsourcing, data was collected using specialised
software, the Relevance Assessment Tool (RAT). We found that while Google
outperforms Bing in both query types, the difference in the performance for
informational queries was rather low. However, for navigational queries, Google
found the correct answer in 95.3 per cent of cases whereas Bing only found the
correct answer 76.6 per cent of the time. We conclude that search engine
performance on navigational queries is of great importance, as users in this
case can clearly identify queries that have returned correct results. So,
performance on this query type may contribute to explaining user satisfaction
with search engines."
"Mathematical content is a valuable information source and retrieving this
content has become an important issue. This paper compares two searching
strategies for math expressions: presentation-based and content-based
approaches. Presentation-based search uses state-of-the-art math search system
while content-based search uses semantic enrichment of math expressions to
convert math expressions into their content forms and searching is done using
these content-based expressions. By considering the meaning of math
expressions, the quality of search system is improved over presentation-based
systems."
"Interestingness,as the composition of Relevance and Unexpectedness, has been
tested by means of Web search cases studies and led to promising results. But
for thorough investigation and routine practical application one needs a
flexible and robust tool. This work describes such an Interestingness based
search tool, its software architecture and actual implementation. One of its
flexibility traits is the choice of Interestingness functions: it may work with
Match-Mismatch and Tf-Idf, among other functions. The tool has been
experimentally verified by application to various domains of interest. It has
been validated by comparison of results with those of commercial search engines
and results from differing Interestingness functions."
"World Wide Web is a huge repository of information and there is a tremendous
increase in the volume of information daily. The number of users are also
increasing day by day. To reduce users browsing time lot of research is taken
place. Web Usage Mining is a type of web mining in which mining techniques are
applied in log data to extract the behaviour of users. Clustering plays an
important role in a broad range of applications like Web analysis, CRM,
marketing, medical diagnostics, computational biology, and many others.
Clustering is the grouping of similar instances or objects. The key factor for
clustering is some sort of measure that can determine whether two objects are
similar or dissimilar . In this paper a novel clustering method to partition
user sessions into accurate clusters is discussed. The accuracy and various
performance measures of the proposed algorithm shows that the proposed method
is a better method for web log mining."
"In the context of business information systems, e-commerce and access to
knowledge, the relevance of the information provided to use is a key fact to
the success of information systems. Therefore the quality of access is
determined by access to the right information at the right time, at the right
place. In this context, it is important to consider the users needs when access
to information and his contextual situation in order to provide relevant
information, tailored to their needs and context use. In what follows we
describe the prelude to a project that tries to combine all of these needs to
improve information systems."
"In this paper, we study a cold-start problem in recommendation systems where
we have completely new users entered the systems. There is not any interaction
or feedback of the new users with the systems previoustly, thus no ratings are
available. Trivial approaches are to select ramdom items or the most popular
ones to recommend to the new users. However, these methods perform poorly in
many case. In this research, we provide a new look of this cold-start problem
in recommendation systems. In fact, we cast this cold-start problem as a
contextual-bandit problem. No additional information on new users and new items
is needed. We consider all the past ratings of previous users as contextual
information to be integrated into the recommendation framework. To solve this
type of the cold-start problems, we propose a new efficient method which is
based on the LinUCB algorithm for contextual-bandit problems. The experiments
were conducted on three different publicly-available data sets, namely
Movielens, Netflix and Yahoo!Music. The new proposed methods were also compared
with other state-of-the-art techniques. Experiments showed that our new method
significantly improves upon all these methods."
"This paper presents an architecture of an information retrieval system that
use the advantages offered by mobile agents to collect information from
different sources and bring the result to the calling user. Mobile agent
technology will be used for determine the traceability of a product and also
for searching information about a specific entity."
"Basically information means selection within a domain (value or definition
set) of possibilities. For objectifiable, comparable and precise information
the domain should be the same for all. Therefore the global (online) definition
of the domain is proposed here. It is advantageous to define an ordered domain,
because this allows using numbers for addressing the elements and because
nature is ordered in many respects. The original data can be ordered in
multiple independent ways. We can define a domain with multiple independent
numeric dimensions to reflect this. Because we want to search information in
the domain, for quantification of similarity we define a distance function or
metric. Therefore we propose ""Domain Spaces"" (DSs) which are online defined
nestable metric spaces. Their elements are called ""Domain Vectors"" (DVs) and
have the simple form:
  URL (of common DS definition) plus sequence of numbers
  At this the sequence must be given so that the mapping of numbers to the DS
dimensions is clear. By help of appropriate software DVs can be represented
e.g. as words and numbers. Compared to words, however, DVs have (as original
information) important objectifiable advantages (clear definition, objectivity,
information content, range, resolution, efficiency, searchability). Using DSs
users can define which information they make searchable and how it is
searchable. DSs can be also used to make quantitative (numeric) data as uniform
DVs interoperable, comparable and searchable. The approach is demonstrated in
an online database with search engine (http://NumericSearch.com). The search
procedure is called ""Numeric Search"". It consists of two systematic steps: 1.
Selection of the appropriate DS e.g. by conventional word based search within
the DS definitions. 2. Range and/or similarity search of DVs in the selected
DS."
"The conventional clustering algorithms have difficulties in handling the
challenges posed by the collection of natural data which is often vague and
uncertain. Fuzzy clustering methods have the potential to manage such
situations efficiently. Fuzzy clustering method is offered to construct
clusters with uncertain boundaries and allows that one object belongs to one or
more clusters with some membership degree. In this paper, an algorithm and
experimental results are presented for fuzzy clustering of web documents using
equivalence relations and fuzzy hierarchical clustering."
"Service oriented architecture integrated with text mining allows services to
extract information in a well defined manner. In this paper, it is proposed to
design a knowledge extracting system for the Ocean Information Data System.
Deployed ARGO floating sensors of INCOIS (Indian National Council for Ocean
Information Systems) organization reflects the characteristics of ocean. This
is forwarded to the OIDS (Ocean Information Data System). For the data received
from OIDS, pre-processing techniques are applied. Pre-processing involves the
header retrieval and data separation. Header information is used to identify
the region of sensor, whereas data is used in the analysis process of Ocean
Information System. Analyzed data is segmented based on the region, by the
header value. Mining technique and composition principle is applied on the
segments for further analysis. Index Terms-- Service oriented architecture;
Text Mining; ARGO floating sensor; INCOIS; OIDS; Pre-processing."
"We present the Bullseye system for scholarly search. Given a collection of
research papers, Bullseye: 1) identifies relevant passages using any
on-the-shelf algorithm; 2) automatically detects document structure and
restricts retrieved passages to user-specifed sections; and 3) highlights those
passages for each PDF document retrieved. We evaluate Bullseye with regard to
three aspects: system effectiveness, user effectiveness, and user effort. In a
system-blind evaluation, users were asked to compare passage retrieval using
Bullseye vs. a baseline which ignores document structure, in regard to four
types of graded assessments. Results show modest improvement in system
effectiveness while both user effectiveness and user effort show substantial
improvement. Users also report very strong demand for passage highlighting in
scholarly search across both systems considered."
"This paper describes our approach towards the ECML/PKDD Discovery Challenge
2010. The challenge consists of three tasks: (1) a Web genre and facet
classification task for English hosts, (2) an English quality task, and (3) a
multilingual quality task (German and French). In our approach, we create an
ensemble of three classifiers to predict unseen Web hosts whereas each
classifier is trained on a different feature set. Our final NDCG on the whole
test set is 0:575 for Task 1, 0:852 for Task 2, and 0:81 (French) and 0:77
(German) for Task 3, which ranks second place in the ECML/PKDD Discovery
Challenge 2010."
"Online forums enable users to discuss together around various topics. One of
the serious problems of these environments is high volume of discussions and
thus information overload problem. Unfortunately without considering the users
interests, traditional Information Retrieval (IR) techniques are not able to
solve the problem. Therefore, employment of a Recommender System (RS) that
could suggest favorite's topics of users according to their tastes could
increases the dynamism of forum and prevent the users from duplicate posts. In
addition, consideration of semantics can be useful for increasing the
performance of IR based RS. Our goal is study of impact of ontology and data
mining techniques on improving of content-based RS. For this purpose, at first,
three type of ontologies will be constructed from the domain corpus with
utilization of text mining, Natural Language Processing (NLP) and Wordnet and
then they will be used as an input in two kind of RS: one, fully ontology-based
and one with enriching the user profile vector with ontology in vector space
model (VSM) (proposed method). Afterward the results will be compared with the
simple VSM based RS. Given results show that the proposed RS presents the
highest performance."
"The similarity searches that use high-dimensional feature vectors consisting
of a vast amount of data have a wide range of application. One way of
conducting a fast similarity search is to transform the feature vectors into
binary vectors and perform the similarity search by using the Hamming distance.
Such a transformation is a hashing method, and the choice of hashing function
is important. Hashing methods using hyperplanes or hyperspheres are proposed.
One study reported here is inspired by Spherical LSH, and we use hypersperes to
hash the feature vectors. Our method, called Eclipse-hashing, performs a
compactification of R^n by using the inverse stereographic projection, which is
a kind of Alexandrov compactification. By using Eclipse-hashing, one can obtain
the hypersphere-hash function without explicitly using hyperspheres. Hence, the
number of nonlinear operations is reduced and the processing time of hashing
becomes shorter. Furthermore, we also show that as a result of improving the
approximation accuracy, Eclipse-hashing is more accurate than
hyperplane-hashing."
"Exponential growth of the web increased the importance of web document
classification and data mining. To get the exact information, in the form of
knowing what classes a web document belongs to, is expensive. Automatic
classification of web document is of great use to search engines which provides
this information at a low cost. In this paper, we propose an approach for
classifying the web document using the frequent item word sets generated by the
Frequent Pattern (FP) Growth which is an association analysis technique of data
mining. These set of associated words act as feature set. The final
classification obtained after Na\""ive Bayes classifier used on the feature set.
For the experimental work, we use Gensim package, as it is simple and robust.
Results show that our approach can be effectively classifying the web document."
"The dynamic web has increased exponentially over the past few years with more
than thousands of documents related to a subject available to the user now.
Most of the web documents are unstructured and not in an organized manner and
hence user facing more difficult to find relevant documents. A more useful and
efficient mechanism is combining clustering with ranking, where clustering can
group the similar documents in one place and ranking can be applied to each
cluster for viewing the top documents at the beginning.. Besides the particular
clustering algorithm, the different term weighting functions applied to the
selected features to represent web document is a main aspect in clustering
task. Keeping this approach in mind, here we proposed a new mechanism called
Tf-Idf based Apriori for clustering the web documents. We then rank the
documents in each cluster using Tf-Idf and similarity factor of documents based
on the user query. This approach will helps the user to get all his relevant
documents in one place and can restrict his search to some top documents of his
choice. For experimental purpose, we have taken the Classic3 and Classic4
datasets of Cornell University having more than 10,000 documents and use gensim
toolkit to carry out our work. We have compared our approach with traditional
apriori algorithm and found that our approach is giving better results for
higher minimum support. Our ranking mechanism is also giving a good F-measure
of 78%."
"With the ever proliferating size and scale of the WWW [1] efficient ways of
exploring content are of increasing importance. How can we efficiently retrieve
information from it through crawling? And in this era of tera and multi-core
processors, we ought to think of multi-threaded processes as a serving
solution. So, even better how can we improve the crawling performance by using
parallel crawlers that work independently? The paper devotes to the fundamental
development in the field of parallel crawlers [4] highlighting the advantages
and challenges arising from its design. The paper also focuses on the aspect of
URL distribution among the various parallel crawling processes or threads and
ordering the URLs within each distributed set of URLs. How to distribute URLs
from the URL frontier to the various concurrently executing crawling process
threads is an orthogonal problem. The paper provides a solution to the problem
by designing a framework WebParF that partitions the URL frontier into a
several URL queues while considering the various design issues."
"Online encyclopedia such as Wikipedia has become one of the best sources of
knowledge. Much effort has been devoted to expanding and enriching the
structured data by automatic information extraction from unstructured text in
Wikipedia. Although remarkable progresses have been made, their effectiveness
and efficiency is still limited as they try to tackle an extremely difficult
natural language understanding problems and heavily relies on supervised
learning approaches which require large amount effort to label the training
data. In this paper, instead of performing information extraction over
unstructured natural language text directly, we focus on a rich set of
semi-structured data in Wikipedia articles: linked entities. The idea of this
paper is the following: If we can summarize the relationship between the entity
and its linked entities, we immediately harvest some of the most important
information about the entity. To this end, we propose a novel rank aggregation
approach to remove noise, an effective clustering and labeling algorithm to
extract knowledge."
"Semantic search technology has received more attention in the last years.
Compared with the keyword based search, semantic search is used to excavate the
latent semantics information and help users find the information items that
they want indeed. In this paper, we present a novel approach for semantic
search which combines Multi-Categorization Semantic Analysis with
personalization technology. The MCSA approach can classify documents into
multiple categories, which is distinct from the existing approaches of
classifying documents into a single category. Then, the search history and
personal information for users are significantly considered in analysing and
matching the original search result by Term Vector DataBase. A series of
personalization algorithms are proposed to match personal information and
search history. At last, the related experiments are made to validate the
effectiveness and efficiency of our method. The experimental results show that
our method based on MCSA and personalization outperforms some existing methods
with the higher search accuracy and the lower extra time cost."
"In this work we present a novel item recommendation approach that aims at
improving Collaborative Filtering (CF) in social tagging systems using the
information about tags and time. Our algorithm follows a two-step approach,
where in the first step a potentially interesting candidate item-set is found
using user-based CF and in the second step this candidate item-set is ranked
using item-based CF. Within this ranking step we integrate the information of
tag usage and time using the Base-Level Learning (BLL) equation coming from
human memory theory that is used to determine the reuse-probability of words
and tags using a power-law forgetting function.
  As the results of our extensive evaluation conducted on data-sets gathered
from three social tagging systems (BibSonomy, CiteULike and MovieLens) show,
the usage of tag-based and time information via the BLL equation also helps to
improve the ranking and recommendation process of items and thus, can be used
to realize an effective item recommender that outperforms two alternative
algorithms which also exploit time and tag-based information."
"In todays era with the rapid growth of information on the web, makes users
turn to search engines as a replacement of traditional media. This makes
sorting of particular information through billions of webpages and displaying
the relevant data makes the task tough for the search engine. Remedy for this
is SEO i.e. having a website optimized in such a way that it will display the
relevant webpages based on ranking. This is the main reason that makes search
engine optimization a prominent position in online world. This paper present a
synonym based data mining approach for SEO that makes the task of improving the
ranking of the website much easier way and user will get answer to their query
easily through any of search engine available in market."
"In this paper we present the IRSA framework that enables the automatic
creation of search term suggestion or recommendation systems (TS). Such TS are
used to operationalize interactive query expansion and help users in refining
their information need in the query formulation phase. Our recent research has
shown TS to be more effective when specific to a certain domain. The presented
technical framework allows owners of Digital Libraries to create their own
specific TS constructed via OAI-harvested metadata with very little effort."
"A large amount of data on the WWW remains inaccessible to crawlers of Web
search engines because it can only be exposed on demand as users fill out and
submit forms. The Hidden web refers to the collection of Web data which can be
accessed by the crawler only through an interaction with the Web-based search
form and not simply by traversing hyperlinks. Research on Hidden Web has
emerged almost a decade ago with the main line being exploring ways to access
the content in online databases that are usually hidden behind search forms.
The efforts in the area mainly focus on designing hidden Web crawlers that
focus on learning forms and filling them with meaningful values. The paper
gives an insight into the various Hidden Web crawlers developed for the purpose
giving a mention to the advantages and shortcoming of the techniques employed
in each."
"Search engines are the most commonly used type of tool for finding relevant
information on the Internet. However, today's search engines are far from
perfect. Typical search queries are short, often one or two words, and can be
ambiguous therefore returning inappropriate results. Contextual information
retrieval (CIR) is a critical technique for these search engines to facilitate
queries and return relevant information. Despite its importance, little
progress has been made in CIR due to the difficulty of capturing and
representing contextual information about users. Numerous contextual
information retrieval approaches exist today, but to the best of our knowledge
none of them offer a similar service to the one proposed in this paper.
  This paper proposes an alternative framework for contextual information
retrieval from the WWW. The framework aims to improve query results (or make
search results more relevant) by constructing a contextual profile based on a
user's behaviour, their preferences, and a shared knowledge base, and using
this information in the search engine framework to find and return relevant
information."
"Contextual retrieval is a critical technique for today's search engines in
terms of facilitating queries and returning relevant information. This paper
reports on the development and evaluation of a system designed to tackle some
of the challenges associated with contextual information retrieval from the
World Wide Web (WWW). The developed system has been designed with a view to
capturing both implicit and explicit user data which is used to develop a
personal contextual profile. Such profiles can be shared across multiple users
to create a shared contextual knowledge base. These are used to refine search
queries and improve both the search results for a user as well as their search
experience. An empirical study has been undertaken to evaluate the system
against a number of hypotheses. In this paper, results related to one are
presented that support the claim that users can find information more readily
using the contextual search system."
"Fuzzy co-clustering can be improved if we handle two main problem first is
outlier and second curse of dimensionality .outlier problem can be reduce by
implementing page replacement algorithm like FIFO, LRU or priority algorithm in
a set of frame of web pages efficiently through a search engine. The web page
which has zero priority (outlier) can be represented in separate slot of frame.
Whereas curse of dimensionality problem can be improved by implementing FCC_STF
algorithm for web pages obtain by search engine that reduce the outlier problem
first. The algorithm FCCM and FUZZY CO-DOK are compared with FCC_STF algorithm
with merit and demerits on the bases of different fuzzifier used. FCC_STF
algorithm in which fuzzifier fused into one entity who have shown high
performance by experiment result of values (A1,B1,Vcj,A2,B2) seem to less
sensitive to local maxima and obtain optimization search space in 2-D for web
pages by plotting graph between J(fcc_stf) and Vcj."
"A key challenge of the collaborative filtering (CF) information filtering is
how to obtain the reliable and accurate results with the help of peers'
recommendation. Since the similarities from small-degree users to large-degree
users would be larger than the ones opposite direction, the large-degree users'
selections are recommended extensively by the traditional second-order CF
algorithms. By considering the users' similarity direction and the second-order
correlations to depress the influence of mainstream preferences, we present the
directed second-order CF (HDCF) algorithm specifically to address the challenge
of accuracy and diversity of the CF algorithm. The numerical results for two
benchmark data sets, MovieLens and Netflix, show that the accuracy of the new
algorithm outperforms the state-of-the-art CF algorithms. Comparing with the CF
algorithm based on random-walks proposed in the Ref.7, the average ranking
score could reach 0.0767 and 0.0402, which is enhanced by 27.3\% and 19.1\% for
MovieLens and Netflix respectively. In addition, the diversity, precision and
recall are also enhanced greatly. Without relying on any context-specific
information, tuning the similarity direction of CF algorithms could obtain
accurate and diverse recommendations. This work suggests that the user
similarity direction is an important factor to improve the personalized
recommendation performance."
"This paper presents an integrated multi-agents architecture for indexing and
retrieving video information.The focus of our work is to elaborate an
extensible approach that gathers a priori almost of the mandatory tools which
palliate to the major intertwining problems raised in the whole process of the
video lifecycle (classification, indexing and retrieval). In fact, effective
and optimal retrieval video information needs a collaborative approach based on
multimodal aspects. Clearly, it must to take into account the distributed
aspect of the data sources, the adaptation of the contents, semantic
annotation, personalized request and active feedback which constitute the
backbone of a vigorous system which improve its performances in a smart way"
"Personalized PageRank is an algorithm to classify the improtance of web pages
on a user-dependent basis. We introduce two generalizations of Personalized
PageRank with node-dependent restart. The first generalization is based on the
proportion of visits to nodes before the restart, whereas the second
generalization is based on the probability of visited node just before the
restart. In the original case of constant restart probability, the two measures
coincide. We discuss interesting particular cases of restart probabilities and
restart distributions. We show that the both generalizations of Personalized
PageRank have an elegant expression connecting the so-called direct and reverse
Personalized PageRanks that yield a symmetry property of these Personalized
PageRanks."
"Rank similarity measures provide a method for quantifying differences between
search engine results without the need for relevance judgments. For example,
the providers of a search service might use such measures to estimate the
impact of a proposed algorithmic change across a large number of queries -
perhaps millions - identifying those queries where the impact is greatest. In
this paper, we propose and validate a family of rank similarity measures, each
derived from an associated effectiveness measure. Each member of the family is
based on the maximization of effectiveness difference under this associated
measure. Computing this maximized effectiveness difference (MED) requires the
solution of an optimization problem that varies in difficulty, depending on the
associated measure. We present solutions for several standard effectiveness
measures, including nDCG, MAP, and ERR. Through an experimental validation, we
show that MED reveals meaningful differences between retrieval runs.
Mathematically, MED is a metric, regardless of the associated measure. Prior
work has established a number of other desiderata for rank similarity in the
context of search, and we demonstrate that MED satisfies these requirements.
Unlike previous proposals, MED allows us to directly translate assumptions
about user behavior from any established effectiveness measure to create a
corresponding rank similarity measure. In addition, MED cleanly accommodates
partial relevance judgments, and if complete relevance information is
available, it reduces to a simple difference between effectiveness values."
"People searching for information occasionally experience difficulties finding
what they want on the Web. This might happen if they cannot quite come up with
the right search terms. What do searchers do when this happens? Intuitively one
imagines that they will try a number of associated search terms to zero in on
their intended search target. Certainly the provision of spelling suggestions
and related search terms assume that frustrated searchers will use these to
implement this strategy. Is this assumption correct? What do people really do?
  We ran an experiment where we asked people to find some relevant links, but
we prevented them from using the most obvious search terms, which we termed
taboo words. To make the experiment more interesting we also provided the
traditional forms of assistance: spelling suggestions and related search
suggestions. We assigned participants using a magic square to get no
assistance, one kind of assistance, or both. Forty eight people participated in
the experiment.
  What emerged from the analysis was that when people are frustrated in their
searching attempts, a minority soldier on, attempting to find other terms, but
the majority will stick with their original query term and simply progress from
page to page in a vain attempt to find something relevant. This confirms
findings by other researchers about the difficulties of query re-formulation.
Our finding will serve to inform the developers of user interfaces to search
engines, since it would be helpful if we could find a better way of supporting
frustrated searchers."
"The use of mobile devices in combination with the rapid growth of the
internet has generated an information overload problem. Recommender systems is
a necessity to decide which of the data are relevant to the user. However in
mobile devices there are different factors who are crucial to information
retrieval, such as the location, the screen size and the processor speed. This
paper gives an overview of the technologies related to mobile recommender
systems and a more detailed description of the challenged faced."
"Nonnegative matrix factorization (NMF) has been successfully applied to many
areas for classification and clustering. Commonly-used NMF algorithms mainly
target on minimizing the $l_2$ distance or Kullback-Leibler (KL) divergence,
which may not be suitable for nonlinear case. In this paper, we propose a new
decomposition method by maximizing the correntropy between the original and the
product of two low-rank matrices for document clustering. This method also
allows us to learn the new basis vectors of the semantic feature space from the
data. To our knowledge, we haven't seen any work has been done by maximizing
correntropy in NMF to cluster high dimensional document data. Our experiment
results show the supremacy of our proposed method over other variants of NMF
algorithm on Reuters21578 and TDT2 databasets."
"Web spam is a big challenge for quality of search engine results. It is very
important for search engines to detect web spam accurately. In this paper we
present 32 low cost quality factors to classify spam and ham pages on real time
basis. These features can be divided in to three categories: (i) URL features,
(ii) Content features, and (iii) Link features. We developed a classifier using
Resilient Back-propagation learning algorithm of neural network and obtained
good accuracy. This classifier can be applied to search engine results on real
time because calculation of these features require very little CPU resources."
"Recent developments in the field of data fusion have seen a focus on
techniques that use training queries to estimate the probability that various
documents are relevant to a given query and use that information to assign
scores to those documents on which they are subsequently ranked. This paper
introduces SlideFuse, which builds on these techniques, introducing a sliding
window in order to compensate for situations where little relevance information
is available to aid in the estimation of probabilities.
  SlideFuse is shown to perform favourably in comparison with CombMNZ, ProbFuse
and SegFuse. CombMNZ is the standard baseline technique against which data
fusion algorithms are compared whereas ProbFuse and SegFuse represent the
state-of-the-art for probabilistic data fusion methods."
"JavaScript engines inside modern browsers are capable of running
sophisticated multi-player games, rendering impressive 3D scenes, and
supporting complex, interactive visualizations. Can this processing power be
harnessed for information retrieval? This paper explores the feasibility of
building a JavaScript search engine that runs completely self-contained on the
client side within the browser---this includes building the inverted index,
gathering terms statistics for scoring, and performing query evaluation. The
design takes advantage of the IndexDB API, which is implemented by the LevelDB
key-value store inside Google's Chrome browser. Experiments show that although
the performance of the JavaScript prototype falls far short of the open-source
Lucene search engine, it is sufficiently responsive for interactive
applications. This feasibility demonstration opens the door to interesting
applications in offline and private search across multiple platforms as well as
hybrid split-execution architectures whereby clients and servers
collaboratively perform query evaluation. One possible future scenario is the
rise of an online search marketplace in which commercial search engine
companies and individual users participate as rational economic actors,
balancing privacy, resource usage, latency, and other factors based on
customizable utility profiles."
"Associating geo-coordinates with the content of social media posts can
enhance many existing applications and services and enable a host of new ones.
Unfortunately, a majority of social media posts are not tagged with
geo-coordinates. Even when location data is available, it may be inaccurate,
very broad or sometimes fictitious. Contemporary location estimation approaches
based on analyzing the content of these posts can identify only broad areas
such as a city, which limits their usefulness. To address these shortcomings,
this paper proposes a methodology to narrowly estimate the geo-coordinates of
social media posts with high accuracy. The methodology relies solely on the
content of these posts and prior knowledge of the wide geographical region from
where the posts originate. An ensemble of language models, which are smoothed
over non-overlapping sub-regions of a wider region, lie at the heart of the
methodology. Experimental evaluation using a corpus of over half a million
tweets from New York City shows that the approach, on an average, estimates
locations of tweets to within just 2.15km of their actual positions."
"Search engines are a combination of hardware and computer software supplied
by a particular company through the website which has been determined. Search
engines collect information from the web through bots or web crawlers that
crawls the web periodically. The process of retrieval of information from
existing websites is called ""web scraping."" Web scraping is a technique of
extracting information from websites. Web scraping is closely related to Web
indexing, as for how to develop a web scraping technique that is by first
studying the program makers HTML document from the website will be taken to the
information in the HTML tag flanking the aim is for information collected after
the program makers learn navigation techniques on the website information will
be taken to a web application mimicked the scraping that we will create. It
should also be noted that the implementation of this writing only scraping
involves a free search engine such as: portal garuda, Indonesian scientific
journal databases (ISJD), google scholar."
"Locating and distilling the valuable relevant information continued to be the
major challenges of Information Retrieval (IR) Systems owing to the explosive
growth of online web information. These challenges can be considered the XML
Information Retrieval challenges as XML has become a de facto standard over the
Web. The research on XML IR starts with the classical IR strategies customized
to XML IR. Later novel IR strategies specific to XML IR are evolved. Meanwhile
literatures reveal development of the rapid and intelligent IR systems. Despite
their success in their specified constrained domains, they have additional
limitations in the complex information space. The effectiveness of IR systems
is thus unsolved in satisfying the most. This article attemptsan overview of
earlier efforts and the gaps in XML IR."
"Searching health information on web has become an integral part of today's
world, and many people turn to the Web for healthcare information and
healthcare assessment. Our pilot study investigates users' preferences for the
type of search results (image, news, video, etc.), and investigates users'
ability to accurately interpret online health information for the purpose of
self diagnosis. The preliminary results reveal that blog and news articles are
most sought by users when searching online information and there exist
challenges in the use of online health information for self-diagnosis."
"While contemporary semantic search systems offer to improve classical
keyword-based search, they are not always adequate for complex domain specific
information needs. The domain of prescription drug abuse, for example, requires
knowledge of both ontological concepts and 'intelligible constructs' not
typically modeled in ontologies. These intelligible constructs convey essential
information that include notions of intensity, frequency, interval, dosage and
sentiments, which could be important to the holistic needs of the information
seeker. We present a hybrid approach to domain specific information retrieval
(or knowledge-aware search) that integrates ontology-driven query
interpretation with synonym-based query expansion and domain specific rules, to
facilitate search in social media. Our framework is based on a context-free
grammar (CFG) that defines the query language of constructs interpretable by
the search system. The grammar provides two levels of semantic interpretation:
1) a top-level CFG that facilitates retrieval of diverse textual patterns,
which belong to broad templates and 2) a low-level CFG that enables
interpretation of certain specific expressions that belong to such patterns.
These low-level expressions occur as concepts from four different categories of
data: 1) ontological concepts, 2) concepts in lexicons (such as emotions and
sentiments), 3) concepts in lexicons with only partial ontology representation,
called lexico-ontology concepts (such as side effects and routes of
administration (ROA)), and 4) domain specific expressions (such as date, time,
interval, frequency and dosage) derived solely through rules. Our approach is
embodied in a novel Semantic Web platform called PREDOSE developed for
prescription drug abuse epidemiology.
  Keywords: Knowledge-Aware Search, Ontology, Semantic Search, Background
Knowledge, Context-Free Grammar"
"An operationalistic scheme, called Melucci metaphor, is suggested
representing Information Retrieval as physical measurements with beam of
particles playing the role of the flow of retrieved documents. The
possibilities of query expansion by extra term are studied from this
perspective, when the particles-`docuscles' are assumed to be of classical or
quantum nature. It is shown that in both cases the choice of an extra term
based on Bayesian belief revision is still valid on the qualitative level for
boosting the relevance of the retrieved documents."
"Majority of the computer or mobile phone enthusiasts make use of the web for
searching activity. Web search engines are used for the searching; The results
that the search engines get are provided to it by a software module known as
the Web Crawler. The size of this web is increasing round-the-clock. The
principal problem is to search this huge database for specific information. To
state whether a web page is relevant to a search topic is a dilemma. This paper
proposes a crawler called as PDD crawler which will follow both a link based as
well as a content based approach. This crawler follows a completely new
crawling strategy to compute the relevance of the page. It analyses the content
of the page based on the information contained in various tags within the HTML
source code and then computes the total weight of the page. The page with the
highest weight, thus has the maximum content and highest relevance."
"Online reputation systems are commonly used by e-commerce providers nowadays.
In order to generate an objective ranking of online items' quality according to
users' ratings, many sophisticated algorithms have been proposed in the
literature. In this paper, instead of proposing new algorithms we focus on a
more fundamental problem: the rating projection. The basic idea is that even
though the rating values given by users are linearly separated, the real
preference of users to items between different values gave is nonlinear. We
thus design an approach to project the original ratings of users to more
representative values. This approach can be regarded as a data pretreatment
method. Simulation in both artificial and real networks shows that the
performance of the ranking algorithms can be improved when the projected
ratings are used."
"Automatic summarization generation of sports video content has been object of
great interest for many years. Although semantic descriptions techniques have
been proposed, many of the approaches still rely on low-level video descriptors
that render quite limited results due to the complexity of the problem and to
the low capability of the descriptors to represent semantic content. In this
paper, a new approach for automatic highlights summarization generation of
soccer videos using audio-visual descriptors is presented. The approach is
based on the segmentation of the video sequence into shots that will be further
analyzed to determine its relevance and interest. Of special interest in the
approach is the use of the audio information that provides additional
robustness to the overall performance of the summarization system. For every
video shot a set of low and mid level audio-visual descriptors are computed and
lately adequately combined in order to obtain different relevance measures
based on empirical knowledge rules. The final summary is generated by selecting
those shots with highest interest according to the specifications of the user
and the results of relevance measures. A variety of results are presented with
real soccer video sequences that prove the validity of the approach."
"Search engines play a vital role in day to day life on internet. People use
search engines to find content on internet. Cloud computing is the computing
concept in which data is stored and accessed with the help of a third party
server called as cloud. Data is not stored locally on our machines and the
softwares and information are provided to user if user demands for it. Search
queries are the most important part in searching data on internet. A search
query consists of one or more than one keywords. A search query is searched
from the database for exact match, and the traditional searchable schemes do
not tolerate minor typos and format inconsistencies, which happen quite
frequently. This drawback makes the existing techniques unsuitable and they
offer very low efficiency. In this paper, we will for the first time formulate
the problem of effective fuzzy search by introducing tree search methodologies.
We will explore the benefits of B trees in search mechanism and use them to
have an efficient keyword search. We have taken into consideration the security
analysis strictly so as to get a secure and privacy-preserving system."
"Document clustering is an unsupervised approach in which a large collection
of documents (corpus) is subdivided into smaller, meaningful, identifiable, and
verifiable sub-groups (clusters). Meaningful representation of documents and
implicitly identifying the patterns, on which this separation is performed, is
the challenging part of document clustering. We have proposed a document
clustering technique using graph based document representation with
constraints. A graph data structure can easily capture the non-linear
relationships of nodes, document contains various feature terms that can be
non-linearly connected hence a graph can easily represents this information.
Constrains, are explicit conditions for document clustering where background
knowledge is use to set the direction for Linking or Not-Linking a set of
documents for a target clusters, thus guiding the clustering process. We deemed
clustering is an ill-define problem, there can be many clustering results.
Background knowledge can be used to drive the clustering algorithm in the right
direction. We have proposed three different types of constraints, Instance
level, corpus level and cluster level constraints. A new algorithm Constrained
HAC is also proposed which will incorporate Instance level constraints as prior
knowledge; it will guide the clustering process leading to better results.
Extensive set of experiments have been performed on both synthetic and standard
document clustering datasets, results are compared on standard clustering
measures like: purity, entropy and F-measure. Results clearly establish that
our proposed approach leads to improvement in cluster quality."
"All pairs similarity search is a problem where a set of data objects is given
and the task is to find all pairs of objects that have similarity above a
certain threshold for a given similarity measure-of-interest. When the number
of points or dimensionality is high, standard solutions fail to scale
gracefully. Approximate solutions such as Locality Sensitive Hashing (LSH) and
its Bayesian variants (BayesLSH and BayesLSHLite) alleviate the problem to some
extent and provides substantial speedup over traditional index based
approaches. BayesLSH is used for pruning the candidate space and computation of
approximate similarity, whereas BayesLSHLite can only prune the candidates, but
similarity needs to be computed exactly on the original data. Thus where ever
the explicit data representation is available and exact similarity computation
is not too expensive, BayesLSHLite can be used to aggressively prune candidates
and provide substantial speedup without losing too much on quality. However,
the loss in quality is higher in the BayesLSH variant, where explicit data
representation is not available, rather only a hash sketch is available and
similarity has to be estimated approximately. In this work we revisit the LSH
problem from a Frequentist setting and formulate sequential tests for composite
hypothesis (similarity greater than or less than threshold) that can be
leveraged by such LSH algorithms for adaptively pruning candidates
aggressively. We propose a vanilla sequential probability ration test (SPRT)
approach based on this idea and two novel variants. We extend these variants to
the case where approximate similarity needs to be computed using fixed-width
sequential confidence interval generation technique."
"Interactions between search and recommendation have recently attracted
significant attention, and several studies have shown that many potential
applications involve with a joint problem of producing recommendations to users
with respect to a given query, termed $Collaborative$ $Retrieval$ (CR).
Successful algorithms designed for CR should be potentially flexible at dealing
with the sparsity challenges since the setup of collaborative retrieval
associates with a given $query$ $\times$ $user$ $\times$ $item$ tensor instead
of traditional $user$ $\times$ $item$ matrix. Recently, several works are
proposed to study CR task from users' perspective. In this paper, we aim to
sufficiently explore the sophisticated relationship of each $query$ $\times$
$user$ $\times$ $item$ triple from items' perspective. By integrating
item-based collaborative information for this joint task, we present an
alternative factorized model that could better evaluate the ranks of those
items with sparse information for the given query-user pair. In addition, we
suggest to employ a recently proposed scalable ranking learning algorithm,
namely BPR, to optimize the state-of-the-art approach, $Latent$ $Collaborative$
$Retrieval$ model, instead of the original learning algorithm. The experimental
results on two real-world datasets, (i.e. \emph{Last.fm}, \emph{Yelp}),
demonstrate the efficiency and effectiveness of our proposed approach."
"In this paper we address the following problem in web document and
information retrieval (IR): How can we use long-term context information to
gain better IR performance? Unlike common IR methods that use bag of words
representation for queries and documents, we treat them as a sequence of words
and use long short term memory (LSTM) to capture contextual dependencies. To
the best of our knowledge, this is the first time that LSTM is applied to
information retrieval tasks. Unlike training traditional LSTMs, the training
strategy is different due to the special nature of information retrieval
problem. Experimental evaluation on an IR task derived from the Bing web search
demonstrates the ability of the proposed method in addressing both lexical
mismatch and long-term context modelling issues, thereby, significantly
outperforming existing state of the art methods for web document retrieval
task."
"Collaborative filtering algorithms haven been widely used in recommender
systems. However, they often suffer from the data sparsity and cold start
problems. With the increasing popularity of social media, these problems may be
solved by using social-based recommendation. Social-based recommendation, as an
emerging research area, uses social information to help mitigate the data
sparsity and cold start problems, and it has been demonstrated that the
social-based recommendation algorithms can efficiently improve the
recommendation performance. However, few of the existing algorithms have
considered using multiple types of relations within one social network. In this
paper, we investigate the social-based recommendation algorithms on
heterogeneous social networks and proposed Hete-CF, a Social Collaborative
Filtering algorithm using heterogeneous relations. Distinct from the exiting
methods, Hete-CF can effectively utilize multiple types of relations in a
heterogeneous social network. In addition, Hete-CF is a general approach and
can be used in arbitrary social networks, including event based social
networks, location based social networks, and any other types of heterogeneous
information networks associated with social information. The experimental
results on two real-world data sets, DBLP (a typical heterogeneous information
network) and Meetup (a typical event based social network) show the
effectiveness and efficiency of our algorithm."
"In this paper, the Simulated Annealing (SA) based biclustering approach is
proposed in which SA is used as an optimization tool for biclustering of web
usage data to identify the optimal user profile from the given web usage data.
Extracted biclusters are consists of correlated users whose usage behaviors are
similar across the subset of web pages of a web site where as these users are
uncorrelated for remaining pages of a web site. These results are very useful
in web personalization so that it communicates better with its users and for
making customized prediction. Also useful for providing customized web service
too. Experiment was conducted on the real web usage dataset called CTI dataset.
Results show that proposed SA based biclustering approach can extract highly
correlated user groups from the preprocessed web usage data."
"Most existing content-based filtering approaches learn user profiles
independently without capturing the similarity among users. Bayesian
hierarchical models \cite{Zhang:Efficient} learn user profiles jointly and have
the advantage of being able to borrow discriminative information from other
users through a Bayesian prior. However, the standard Bayesian hierarchical
models assume all user profiles are generated from the same prior. Considering
the diversity of user interests, this assumption could be improved by
introducing more flexibility. Besides, most existing content-based filtering
approaches implicitly assume that each user profile corresponds to exactly one
user interest and fail to capture a user's multiple interests (information
needs).
  In this paper, we present a flexible Bayesian hierarchical modeling approach
to model both commonality and diversity among users as well as individual
users' multiple interests. We propose two models each with different
assumptions, and the proposed models are called Discriminative Factored Prior
Models (DFPM). In our models, each user profile is modeled as a discriminative
classifier with a factored model as its prior, and different factors contribute
in different levels to each user profile. Compared with existing content-based
filtering models, DFPM are interesting because they can 1) borrow
discriminative criteria of other users while learning a particular user profile
through the factored prior; 2) trade off well between diversity and commonality
among users; and 3) handle the challenging classification situation where each
class contains multiple concepts. The experimental results on a dataset
collected from real users on digg.com show that our models significantly
outperform the baseline models of L-2 regularized logistic regression and
traditional Bayesian hierarchical model with logistic regression."
"Existing document filtering systems learn user profiles based on user
relevance feedback on documents. In some cases, users may have prior knowledge
about what features are important. For example, a Spanish speaker may only want
news written in Spanish, and thus a relevant document should contain the
feature ""Language: Spanish""; a researcher focusing on HIV knows an article with
the medical subject ""Subject: AIDS"" is very likely to be relevant to him/her.
  Semi-structured documents with rich metadata are increasingly prevalent on
the Internet. Motivated by the well-adopted faceted search interface in
e-commerce, we study the exploitation of user prior knowledge on faceted
features for semi-structured document filtering. We envision two faceted
feedback mechanisms, and propose a novel user profile learning algorithm that
can incorporate user feedback on features. To evaluate the proposed work, we
use two data sets from the TREC filtering track, and conduct a user study on
Amazon Mechanical Turk. Our experiment results show that user feedback on
faceted features is useful for filtering. The proposed user profile learning
algorithm can effectively learn from user feedback on both documents and
features, and performs better than several existing methods."
"This paper presents a new user feedback mechanism based on Wikipedia concepts
for interactive retrieval. In this mechanism, the system presents to the user a
group of Wikipedia concepts, and the user can choose those relevant to refine
his/her query. To realize this mechanism, we propose methods to address two
problems: 1) how to select a small number of possibly relevant Wikipedia
concepts to show the user, and 2) how to re-rank retrieved documents given the
user-identified Wikipedia concepts. Our methods are evaluated on three TREC
data sets. The experiment results show that our methods can dramatically
improve retrieval performances."
"As structured documents with rich metadata (such as products, movies, etc.)
become increasingly prevalent, searching those documents has become an
important IR problem. Although advanced search interfaces are widely available,
most users still prefer to use keyword-based queries to search those documents.
Query keywords often imply some hidden restrictions on the desired documents,
which can be represented as document facet-value pairs. To achieve high
retrieval performance, it's important to be able to identify the relevant
facet-value pairs hidden in a query. In this paper, we study the problem of
identifying document facet-value pairs that are relevant to a keyword-based
search query. We propose a machine learning approach and a set of useful
features, and evaluate our approach using a movie data set from INEX."
"Recommendation systems have wide-spread applications in both academia and
industry. Traditionally, performance of recommendation systems has been
measured by their precision. By introducing novelty and diversity as key
qualities in recommender systems, recently increasing attention has been
focused on this topic. Precision and novelty of recommendation are not in the
same direction, and practical systems should make a trade-off between these two
quantities. Thus, it is an important feature of a recommender system to make it
possible to adjust diversity and accuracy of the recommendations by tuning the
model. In this paper, we introduce a probabilistic structure to resolve the
diversity-accuracy dilemma in recommender systems. We propose a hybrid model
with adjustable level of diversity and precision such that one can perform this
by tuning a single parameter. The proposed recommendation model consists of two
models: one for maximization of the accuracy and the other one for
specification of the recommendation list to tastes of users. Our experiments on
two real datasets show the functionality of the model in resolving
accuracy-diversity dilemma and outperformance of the model over other classic
models. The proposed method could be extensively applied to real commercial
systems due to its low computational complexity and significant performance."
"Web templates are one of the main development resources for website
engineers. Templates allow them to increase productivity by plugin content into
already formatted and prepared pagelets. For the final user templates are also
useful, because they provide uniformity and a common look and feel for all
webpages. However, from the point of view of crawlers and indexers, templates
are an important problem, because templates usually contain irrelevant
information such as advertisements, menus, and banners. Processing and storing
this information is likely to lead to a waste of resources (storage space,
bandwidth, etc.). It has been measured that templates represent between 40% and
50% of data on the Web. Therefore, identifying templates is essential for
indexing tasks. In this work we propose a novel method for automatic template
extraction that is based on similarity analysis between the DOM trees of a
collection of webpages that are detected using menus information. Our
implementation and experiments demonstrate the usefulness of the technique."
"Retrieval and content management are assumed to be mutually exclusive. In
this paper we suggest that they need not be so. In the usual information
retrieval scenario, some information about queries leading to a website (due to
`hits' or `visits') is available to the server administrator of the concerned
website. This information can used to better present the content on the
website. Further, we suggest that some more information can be shared by the
retrieval system with the content provider. This will enable the content
provider (any website) to have a more dynamic presentation of the content that
is in tune with the query trends, without violating the privacy of the querying
user. The result will be a better synchronization between retrieval systems and
content providers, with the purpose of improving the user's web search
experience. This will also give the content provider a say in this process,
given that the content provider is the one who knows much more about the
content than the retrieval system. It also means that the content presentation
may change in response to a query. In the end, the user will be able to find
the relevant content more easily and quickly."
"Expert finding is an information retrieval task that is concerned with the
search for the most knowledgeable people with respect to a specific topic, and
the search is based on documents that describe people's activities. The task
involves taking a user query as input and returning a list of people who are
sorted by their level of expertise with respect to the user query. Despite
recent interest in the area, the current state-of-the-art techniques lack in
principled approaches for optimally combining different sources of evidence.
This article proposes two frameworks for combining multiple estimators of
expertise. These estimators are derived from textual contents, from
graph-structure of the citation patterns for the community of experts, and from
profile information about the experts. More specifically, this article explores
the use of supervised learning to rank methods, as well as rank aggregation
approaches, for combing all of the estimators of expertise. Several supervised
learning algorithms, which are representative of the pointwise, pairwise and
listwise approaches, were tested, and various state-of-the-art data fusion
techniques were also explored for the rank aggregation framework. Experiments
that were performed on a dataset of academic publications from the Computer
Science domain attest the adequacy of the proposed approaches."
"The task of expert finding has been getting increasing attention in
information retrieval literature. However, the current state-of-the-art is
still lacking in principled approaches for combining different sources of
evidence. This paper explores the usage of unsupervised rank aggregation
methods as a principled approach for combining multiple estimators of
expertise, derived from the textual contents, from the graph-structure of the
citation patterns for the community of experts, and from profile information
about the experts. We specifically experimented two unsupervised rank
aggregation approaches well known in the information retrieval literature,
namely CombSUM and CombMNZ. Experiments made over a dataset of academic
publications for the area of Computer Science attest for the adequacy of these
methods."
"In this position paper we argue that certain aspects of relevance assessment
in the evaluation of IR systems are oversimplified and that human assessments
represented by qrels should be augmented to take account of contextual factors
and the subjectivity of the task at hand. We propose enhancing test collections
used in evaluation with information related to human assessors and their
interpretation of the task. Such augmented collections would provide a more
realistic and user-focused evaluation, enabling us to better understand the
evaluation process, the performance of systems and user interactions. A first
step is to conduct user studies to examine in more detail what people actually
do when we ask them to judge the relevance of a document."
"This paper reports the use of a document distance-based approach to
automatically expand the number of available relevance judgements when these
are limited and reduced to only positive judgements. This may happen, for
example, when the only available judgements are extracted from a list of
references in a published review paper. We compare the results on two document
sets: OHSUMED, based on medical research publications, and TREC-8, based on
news feeds. We show that evaluations based on these expanded relevance
judgements are more reliable than those using only the initially available
judgements, especially when the number of available judgements is very limited."
"Currently, the quality of a search engine is often determined using so-called
topical relevance, i.e., the match between the user intent (expressed as a
query) and the content of the document. In this work we want to draw attention
to two aspects of retrieval system performance affected by the presentation of
results: result attractiveness (""perceived relevance"") and immediate usefulness
of the snippets (""snippet relevance""). Perceived relevance may influence
discoverability of good topical documents and seemingly better rankings may in
fact be less useful to the user if good-looking snippets lead to irrelevant
documents or vice-versa. And result items on a search engine result page (SERP)
with high snippet relevance may add towards the total utility gained by the
user even without the need to click those items.
  We start by motivating the need to collect different aspects of relevance
(topical, perceived and snippet relevances) and how these aspects can improve
evaluation measures. We then discuss possible ways to collect these relevance
aspects using crowdsourcing and the challenges arising from that."
"Microblogging services like Twitter and Facebook collect millions of user
generated content every moment about trending news, occurring events, and so
on. Nevertheless, it is really a nightmare to find information of interest
through the huge amount of available posts that are often noise and redundant.
In general, social media analytics services have caught increasing attention
from both side research and industry. Specifically, the dynamic context of
microblogging requires to manage not only meaning of information but also the
evolution of knowledge over the timeline. This work defines Time Aware
Knowledge Extraction (briefly TAKE) methodology that relies on temporal
extension of Fuzzy Formal Concept Analysis. In particular, a microblog
summarization algorithm has been defined filtering the concepts organized by
TAKE in a time-dependent hierarchy. The algorithm addresses topic-based
summarization on Twitter. Besides considering the timing of the concepts,
another distinguish feature of the proposed microblog summarization framework
is the possibility to have more or less detailed summary, according to the
user's needs, with good levels of quality and completeness as highlighted in
the experimental results."
"Classic resource recommenders like Collaborative Filtering (CF) treat users
as being just another entity, neglecting non-linear user-resource dynamics
shaping attention and interpretation. In this paper, we propose a novel hybrid
recommendation strategy that refines CF by capturing these dynamics. The
evaluation results reveal that our approach substantially improves CF and,
depending on the dataset, successfully competes with a computationally much
more expensive Matrix Factorization variant."
"We present our solution to the Yandex Personalized Web Search Challenge. The
aim of this challenge was to use the historical search logs to personalize
top-N document rankings for a set of test users. We used over 100 features
extracted from user- and query-depended contexts to train neural net and
tree-based learning-to-rank and regression models. Our final submission, which
was a blend of several different models, achieved an NDCG@10 of 0.80476 and
placed 4'th amongst the 194 teams winning 3'rd prize."
"The multinomial language model has been one of the most effective models of
retrieval for over a decade. However, the multinomial distribution does not
model one important linguistic phenomenon relating to term-dependency, that is
the tendency of a term to repeat itself within a document (i.e. word
burstiness). In this article, we model document generation as a random process
with reinforcement (a multivariate Polya process) and develop a Dirichlet
compound multinomial language model that captures word burstiness directly.
  We show that the new reinforced language model can be computed as efficiently
as current retrieval models, and with experiments on an extensive set of TREC
collections, we show that it significantly outperforms the state-of-the-art
language model for a number of standard effectiveness metrics. Experiments also
show that the tuning parameter in the proposed model is more robust than in the
multinomial language model. Furthermore, we develop a constraint for the
verbosity hypothesis and show that the proposed model adheres to the
constraint. Finally, we show that the new language model essentially introduces
a measure closely related to idf which gives theoretical justification for
combining the term and document event spaces in tf-idf type schemes."
"Compression algorithms are important for data oriented tasks, especially in
the era of Big Data. Modern processors equipped with powerful SIMD instruction
sets, provide us an opportunity for achieving better compression performance.
Previous research has shown that SIMD-based optimizations can multiply decoding
speeds. Following these pioneering studies, we propose a general approach to
accelerate compression algorithms. By instantiating the approach, we have
developed several novel integer compression algorithms, called Group-Simple,
Group-Scheme, Group-AFOR, and Group-PFD, and implemented their corresponding
vectorized versions. We evaluate the proposed algorithms on two public TREC
datasets, a Wikipedia dataset and a Twitter dataset. With competitive
compression ratios and encoding speeds, our SIMD-based algorithms outperform
state-of-the-art non-vectorized algorithms with respect to decoding speeds."
"The ability to browse an information space in a structured way by exploiting
similarities and dissimilarities between information objects is crucial for
knowledge discovery. Knowledge maps use visualizations to gain insights into
the structure of large-scale information spaces, but are still far away from
being applicable for searching. The paper proposes a use case for enhancing
search term recommendations by heat map visualizations of co-word
relation-ships taken from indexing vocabulary. By contrasting areas of
different ""heat"" the user is enabled to indicate mainstream areas of the field
in question more easily."
"We investigate exact indexing for high dimensional Lp norms based on the
1-Lipschitz property and projection operators. The orthogonal projection that
satisfies the 1-Lipschitz property for the Lp norm is described. The adaptive
projection defined by the first principal component is introduced."
"The standard approach for term frequency normalization is based only on the
document length. However, it does not distinguish the verbosity from the scope,
these being the two main factors determining the document length. Because the
verbosity and scope have largely different effects on the increase in term
frequency, the standard approach can easily suffer from insufficient or
excessive penalization depending on the specific type of long document. To
overcome these problems, this paper proposes two-stage normalization by
performing verbosity and scope normalization separately, and by employing
different penalization functions. In verbosity normalization, each document is
pre-normalized by dividing the term frequency by the verbosity of the document.
In scope normalization, an existing retrieval model is applied in a
straightforward manner to the pre-normalized document, finally leading us to
formulate our proposed verbosity normalized (VN) retrieval model. Experimental
results carried out on standard TREC collections demonstrate that the VN model
leads to marginal but statistically significant improvements over standard
retrieval models."
"Successfully modeling state and analytics-based semantic relationships of
documents enhances representation, importance, relevancy, provenience, and
priority of the document. These attributes are the core elements that form the
machine-based knowledge representation for documents. However, modeling
document relationships that can change over time can be inelegant, limited,
complex or overly burdensome for semantic technologies. In this paper, we
present Direct Qualification (DQ), an approach for modeling any semantically
referenced document, concept, or named graph with results from associated
applied analytics. The proposed approach supplements the traditional
subject-object relationships by providing a third leg to the relationship; the
qualification of how and why the relationship exists. To illustrate, we show a
prototype of an event-based system with a realistic use case for applying DQ to
relevancy analytics of PageRank and Hyperlink-Induced Topic Search (HITS)."
"Successfully managing analytics-based semantic relationships and their
provenance enables determinations of document importance and priority,
furthering capabilities for machine-based relevancy scoring operations.
Semantic technologies are well suited for modeling explicit and fully qualified
relationships but struggle with modeling relationships that are qualified in
nature, or resultant from applied analytics. Our work seeks to implement the
autonomous Directed Qualification of analytic-based relationships by pairing
the Prov-O Ontology (W3C Recommendation) with a relevancy ontology supporting
analytics terminology. This work results in the capability for any semantically
referenced document, concept, or named graph to be associated with the results
of applied analytics as Direct Qualification (DQ) modeled relational nodes.
This new capability will enable role, identity, or any other content-based
measures of relevancy and analytics-based metrics for semantically described
documents."
"Despite limited success, information retrieval (IR) systems today are not
intelligent or reliable. IR systems return poor search results when users
formulate their information needs into incomplete or ambiguous queries (i.e.,
weak queries). Therefore, one of the main challenges in modern IR research is
to provide consistent results across all queries by improving the performance
on weak queries. However, existing IR approaches such as query expansion are
not overly effective because they make little effort to analyze and exploit the
meanings of the queries. Furthermore, word sense disambiguation approaches,
which rely on textual context, are ineffective against weak queries that are
typically short. Motivated by the demand for a robust IR system that can
consistently provide highly accurate results, the proposed study implemented a
novel topic detection that leveraged both the language model and structural
knowledge of Wikipedia and systematically evaluated the effect of query
disambiguation and topic-based retrieval approaches on TREC collections. The
results not only confirm the effectiveness of the proposed topic detection and
topic-based retrieval approaches but also demonstrate that query disambiguation
does not improve IR as expected."
"Much of the appeal of music lies in its power to convey emotions/moods and to
evoke them in listeners. In consequence, the past decade witnessed a growing
interest in modeling emotions from musical signals in the music information
retrieval (MIR) community. In this article, we present a novel generative
approach to music emotion modeling, with a specific focus on the
valence-arousal (VA) dimension model of emotion. The presented generative
model, called \emph{acoustic emotion Gaussians} (AEG), better accounts for the
subjectivity of emotion perception by the use of probability distributions.
Specifically, it learns from the emotion annotations of multiple subjects a
Gaussian mixture model in the VA space with prior constraints on the
corresponding acoustic features of the training music pieces. Such a
computational framework is technically sound, capable of learning in an online
fashion, and thus applicable to a variety of applications, including
user-independent (general) and user-dependent (personalized) emotion
recognition and emotion-based music retrieval. We report evaluations of the
aforementioned applications of AEG on a larger-scale emotion-annotated corpora,
AMG1608, to demonstrate the effectiveness of AEG and to showcase how
evaluations are conducted for research on emotion-based MIR. Directions of
future work are also discussed."
"Query Expansion using Pseudo Relevance Feedback is a useful and a popular
technique for reformulating the query. In our proposed query expansion method,
we assume that relevant information can be found within a document near the
central idea. The document is normally divided into sections, paragraphs and
lines. The proposed method tries to extract keywords that are closer to the
central theme of the document. The expansion terms are obtained by
equi-frequency partition of the documents obtained from pseudo relevance
feedback and by using tf-idf scores. The idf factor is calculated for number of
partitions in documents. The group of words for query expansion is selected
using the following approaches: the highest score, average score and a group of
words that has maximum number of keywords. As each query behaved differently
for different methods, the effect of these methods in selecting the words for
query expansion is investigated. From this initial study, we extend the
experiment to develop a rule-based statistical model that automatically selects
the best group of words incorporating the tf-idf scoring and the 3 approaches
explained here, in the future. The experiments were performed on FIRE 2011
Adhoc Hindi and English test collections on 50 queries each, using Terrier as
retrieval engine."
"In computer interfaces in general, especially in information retrieval tasks,
it is important to be able to quickly find and retrieve information. State of
the art approach, used, for example, in search engines, is not effective as it
introduces losses of meanings due to context to keywords back and forth
translation. Authors argue it increases the time and reduces the accuracy of
information retrieval compared to what it could be in the system that employs
modern information retrieval and text mining methods while presenting results
in an adaptive human- computer interface where system effectively learns what
operator needs through iterative interaction. In current work, a combination of
adaptive navigational interface and real time collaborative feedback analysis
for documents relevance weighting is proposed as an viable alternative to
prevailing ""telegraphic"" approach in information retrieval systems. Adaptive
navigation is provided through a dynamic links panel controlled by an
evolutionary algorithm. Documents relevance is initially established with
standard information retrieval techniques and is further refined in real time
through interaction of users with the system. Introduced concepts of
multidimensional Knowledge Map and Weighted Point of Interest allow finding
relevant documents and users with common interests through a trivial
calculation. Browsing search approach, the ability of the algorithm to adapt
navigation to users interests, collaborative refinement and the self-organising
features of the system are the main factors making such architecture effective
in various fields where non-structured knowledge shall be represented to the
users."
"In this paper, we study idea mining from crowdsourcing applications which
encourage a group of people, who are usually undefined and very large sized, to
generate ideas for new product development (NPD). In order to isolate the
relatively small number of potential ones among ideas from crowd, decision
makers not only have to identify the key textual information representing the
ideas, but they also need to consider online opinions of people who gave
comments and votes on the ideas. Due to the extremely large size of text data
generated by people on the Internet, identifying textual information has been
carried out in manual ways, and has been considered very time consuming and
costly. To overcome the ineffectiveness, this paper introduces a novel
framework that can help decision makers discover ideas having the potential to
be used in an NPD process. To achieve this, a semi-automatic text mining
technique that retrieves useful text patterns from ideas posted on
crowdsourcing application is proposed. Then, we provide an online learning
algorithm to evaluate whether the idea is potential or not. Finally to verify
the effectiveness of our algorithm, we conducted experiments on the data, which
are collected from an existing crowd sourcing website."
"With the huge upsurge of information in day-to-days life, it has become
difficult to assemble relevant information in nick of time. But people, always
are in dearth of time, they need everything quick. Hence clustering was
introduced to gather the relevant information in a cluster. There are several
algorithms for clustering information out of which in this paper, we accomplish
K-means and K-Medoids clustering algorithm and a comparison is carried out to
find which algorithm is best for clustering. On the best clusters formed,
document summarization is executed based on sentence weight to focus on key
point of the whole document, which makes it easier for people to ascertain the
information they want and thus read only those documents which is relevant in
their point of view."
"This paper proposes a decentralized recommender system by formulating the
popular collaborative filleting (CF) model into a decentralized matrix
completion form over a set of users. In such a way, data storages and
computations are fully distributed. Each user could exchange limited
information with its local neighborhood, and thus it avoids the centralized
fusion. Advantages of the proposed system include a protection on user privacy,
as well as better scalability and robustness. We compare our proposed algorithm
with several state-of-the-art algorithms on the FlickerUserFavor dataset, and
demonstrate that the decentralized algorithm can gain a competitive performance
to others."
"Text Clustering is a text mining technique which divides the given set of
text documents into significant clusters. It is used for organizing a huge
number of text documents into a well-organized form. In the majority of the
clustering algorithms, the number of clusters must be specified apriori, which
is a drawback of these algorithms. The aim of this paper is to show
experimentally how to determine the number of clusters based on cluster
quality. Since partitional clustering algorithms are well-suited for clustering
large document datasets, we have confined our analysis to a partitional
clustering algorithm."
"In most practical applications of image retrieval, high-dimensional feature
vectors are required, but current multi-dimensional indexing structures lose
their efficiency with growth of dimensions. Our goal is to propose a divisive
hierarchical clustering-based multi-dimensional indexing structure which is
efficient in high-dimensional feature spaces. A projection pursuit method has
been used for finding a component of the data, which data's projections onto it
maximizes the approximation of negentropy for preparing essential information
in order to partitioning of the data space. Various tests and experimental
results on high-dimensional datasets indicate the performance of proposed
method in comparison with others."
"This paper exemplifies the implementation of an efficient Information
Retrieval (IR) System to compute the similarity between a dataset and a query
using Fuzzy Logic. TREC dataset has been used for the same purpose. The dataset
is parsed to generate keywords index which is used for the similarity
comparison with the user query. Each query is assigned a score value based on
its fuzzy similarity with the index keywords. The relevant documents are
retrieved based on the score value. The performance and accuracy of the
proposed fuzzy similarity model is compared with Cosine similarity model using
Precision-Recall curves. The results prove the dominance of Fuzzy Similarity
based IR system."
"Since the length of microblog texts, such as tweets, is strictly limited to
140 characters, traditional Information Retrieval techniques suffer from the
vocabulary mismatch problem severely and cannot yield good performance in the
context of microblogosphere. To address this critical challenge, in this paper,
we propose a new language modeling approach for microblog retrieval by
inferring various types of context information. In particular, we expand the
query using knowledge terms derived from Freebase so that the expanded one can
better reflect users' search intent. Besides, in order to further satisfy
users' real-time information need, we incorporate temporal evidences into the
expansion method, which can boost recent tweets in the retrieval results with
respect to a given topic. Experimental results on two official TREC Twitter
corpora demonstrate the significant superiority of our approach over baseline
methods."
"Microblog classification has received a lot of attention in recent years.
Different classification tasks have been investigated, most of them focusing on
classifying microblogs into a small number of classes (five or less) using a
training set of manually annotated tweets. Unfortunately, labelling data is
tedious and expensive, and finding tweets that cover all the classes of
interest is not always straightforward, especially when some of the classes do
not frequently arise in practice. In this paper we study an approach to tweet
classification based on distant supervision, whereby we automatically transfer
labels from one social medium to another for a single-label multi-class
classification task. In particular, we apply YouTube video classes to tweets
linking to these videos. This provides for free a virtually unlimited number of
labelled instances that can be used as training data. The classification
experiments we have run show that training a tweet classifier via these
automatically labelled data achieves substantially better performance than
training the same classifier with a limited amount of manually labelled data;
this is advantageous, given that the automatically labelled data come at no
cost. Further investigation of our approach shows its robustness when applied
with different numbers of classes and across different languages."
"We present a system that constructs and maintains an up-to-date co-occurrence
network of medical concepts based on continuously mining the latest biomedical
literature. Users can explore this network visually via a concise online
interface to quickly discover important and novel relationships between medical
entities. This enables users to rapidly gain contextual understanding of their
medical topics of interest, and we believe this constitutes a significant user
experience improvement over contemporary search engines operating in the
biomedical literature domain."
"Collaborative Filtering is the most widely used prediction technique in
Recommendation System. Most of the current CF recommender systems maintains
single criteria user rating in user item matrix. However, recent studies
indicate that recommender system depending on multi criteria can improve
prediction and accuracy levels of recommendation by considering the user
preferences in multi aspects of items. This gives birth to Multi Criteria
Collaborative Filtering. In MC CF users provide the rating on multiple aspects
of an item in new dimensions,thereby increasing the size of rating matrix,
sparsity and scalability problem. Appropriate dimensionality reduction
techniques are thus needed to take care of these challenges to reduce the
dimension of user item rating matrix to improve the prediction accuracy and
efficiency of CF recommender system. The process of dimensionality reduction
maps the high dimensional input space into lower dimensional space. Thus, the
objective of this paper is to propose an efficient MC CF algorithm using
dimensionality reduction technique to improve the recommendation quality and
prediction accuracy. Dimensionality reduction techniques such as Singular Value
Decomposition and Principal Component Analysis are used to solve the
scalability and alleviate the sparsity problems in overall rating. The proposed
MC CF approach will be implemented using Apache Mahout, which allows processing
of massive dataset stored in distributed/non-distributed file system."
"The abundance of the data in the Internet facilitates the improvement of
extraction and processing tools. The trend in the open data publishing
encourages the adoption of structured formats like CSV and RDF. However, there
is still a plethora of unstructured data on the Web which we assume contain
semantics. For this reason, we propose an approach to derive semantics from web
tables which are still the most popular publishing tool on the Web. The paper
also discusses methods and services of unstructured data extraction and
processing as well as machine learning techniques to enhance such a workflow.
The eventual result is a framework to process, publish and visualize linked
open data. The software enables tables extraction from various open data
sources in the HTML format and an automatic export to the RDF format making the
data linked. The paper also gives the evaluation of machine learning techniques
in conjunction with string similarity functions to be applied in a tables
recognition task."
"Clustering of web search result document has emerged as a promising tool for
improving retrieval performance of an Information Retrieval (IR) system. Search
results often plagued by problems like synonymy, polysemy, high volume etc.
Clustering other than resolving these problems also provides the user the
easiness to locate his/her desired information. In this paper, a method, called
WSRDC-CSCC, is introduced to cluster web search result using cuckoo search
meta-heuristic method and Consensus clustering. Cuckoo search provides a solid
foundation for consensus clustering. As a local clustering function, k-means
technique is used. The final number of cluster is not depended on this k.
Consensus clustering finds the natural grouping of the objects. The proposed
algorithm is compared to another clustering method which is based on cuckoo
search and Bayesian Information Criterion. The experimental results show that
proposed algorithm finds the actual number of clusters with great value of
precision, recall and F-measure as compared to the other method"
"We consider the ubiquitous technique of VByte compression, which represents
each integer as a variable length sequence of bytes. The low 7 bits of each
byte encode a portion of the integer, and the high bit of each byte is reserved
as a continuation flag. This flag is set to 1 for all bytes except the last,
and the decoding of each integer is complete when a byte with a high bit of 0
is encountered. VByte decoding can be a performance bottleneck especially when
the unpredictable lengths of the encoded integers cause frequent branch
mispredictions. Previous attempts to accelerate VByte decoding using SIMD
vector instructions have been disappointing, prodding search engines such as
Google to use more complicated but faster-to-decode formats for
performance-critical code. Our decoder (Masked VByte) is 2 to 4 times faster
than a conventional scalar VByte decoder, making the format once again
competitive with regard to speed."
"The Personalization of information has taken recommender systems at a very
high level. With personalization these systems can generate user specific
recommendations accurately and efficiently. User profiling helps
personalization, where information retrieval is done to personalize a scenario
which maintains a separate user profile for individual user. The main objective
of this paper is to explore this field of personalization in context of user
profiling, to help researchers make aware of the user profiling. Various
trends, techniques and Applications have been discussed in paper which will
fulfill this motto."
"Recommendation Systems apply Information Retrieval techniques to select the
online information relevant to a given user. Collaborative Filtering is
currently most widely used approach to build Recommendation System. CF
techniques uses the user behavior in form of user item ratings as their
information source for prediction. There are major challenges like sparsity of
rating matrix and growing nature of data which is faced by CF algorithms. These
challenges are been well taken care by Matrix Factorization. In this paper we
attempt to present an overview on the role of different MF model to address the
challenges of CF algorithms, which can be served as a roadmap for research in
this area."
"The traditional apriori algorithm can be used for clustering the web
documents based on the association technique of data mining. But this algorithm
has several limitations due to repeated database scans and its weak association
rule analysis. In modern world of large databases, efficiency of traditional
apriori algorithm would reduce manifolds. In this paper, we proposed a new
modified apriori approach by cutting down the repeated database scans and
improving association analysis of traditional apriori algorithm to cluster the
web documents. Further we improve those clusters by applying Fuzzy C-Means
(FCM), K-Means and Vector Space Model (VSM) techniques separately. For
experimental purpose, we use Classic3 and Classic4 datasets of Cornell
University having more than 10,000 documents and run both traditional apriori
and our modified apriori approach on it. Experimental results show that our
approach outperforms the traditional apriori algorithm in terms of database
scan and improvement on association of analysis. We found out that FCM is
better than K-Means and VSM in terms of F-measure of clusters of different
sizes."
"With the rising quantity of textual data available in electronic format, the
need to organize it become a highly challenging task. In the present paper, we
explore a document organization framework that exploits an intelligent
hierarchical clustering algorithm to generate an index over a set of documents.
The framework has been designed to be scalable and accurate even with large
corpora. The advantage of the proposed algorithm lies in the need for minimal
inputs, with much of the hierarchy attributes being decided in an automated
manner using statistical methods. The use of topic modeling in a pre-processing
stage ensures robustness to a range of variations in the input data. For
experimental work 20-Newsgroups dataset has been used. The F- measure of the
proposed approach has been compared with the traditional K-Means and K-Medoids
clustering algorithms. Test results demonstrate the applicability, efficiency
and effectiveness of our proposed approach. After extensive experimentation, we
conclude that the framework shows promise for further research and specialized
commercial applications."
"This article presents results of experimental studies the effectiveness of
the genetic algorithm that was applied to effective queries creation and
relevant document selection. Studies were carried out to the comparative
analysis of the semantic relevance and quality ranking of the documents found
on the Internet in various ways. Analysis of the results shows that the
greatest effect of presented technology is achieved by finding new documents
for skilled users in the initial stages of the study of the topic.
Additionally, the number of unique and relevant results is significantly
increased."
"People are always in search of matters for which they are prone to use
internet, but again it has huge assemblage of data due to which it becomes
difficult for the reader to get the most accurate data. To make it easier for
people to gather accurate data, similar information has to be clustered at one
place. There are many algorithms used for clustering of relevant information in
one platform. In this paper, K-Medoids clustering algorithm has been employed
for formation of clusters which is further used for document summarization."
"Nowadays, the majority of RSS feeds provide incomplete information about
their news items. The lack of information leads to engagement loss in users. We
present a new automated system for improving the RSS feeds' data quality. RSS
feeds provide a list of the latest news items ordered by date. Therefore, it
makes it easy for a web crawler to precisely locate the item and extract its
raw content. Then it identifies where the main content is located and extracts:
main text corpus, relevant keywords, bigrams, best image and predicts the
category of the item. The output of the system is an enhanced RSS feed. The
proposed system showed an average item data quality improvement from 39.98% to
95.62%."
"This article presents main results of the pilot study of approaches to the
subject information search based on automated semantic processing of mass
scientific and technical data. The authors focus on technology of building and
qualification of search queries with the following filtering and ranking of
search data. Software architecture, specific features of subject search and
research results application are considered."
"With a vast number of items, web-pages, and news to choose from, online
services and the customers both benefit tremendously from personalized
recommender systems. Such systems however provide great opportunities for
targeted advertisements, by displaying ads alongside genuine recommendations.
We consider a biased recommendation system where such ads are displayed without
any tags (disguised as genuine recommendations), rendering them
indistinguishable to a single user. We ask whether it is possible for a small
subset of collaborating users to detect such a bias. We propose an algorithm
that can detect such a bias through statistical analysis on the collaborating
users' feedback. The algorithm requires only binary information indicating
whether a user was satisfied with each of the recommended item or not. This
makes the algorithm widely appealing to real world issues such as
identification of search engine bias and pharmaceutical lobbying. We prove that
the proposed algorithm detects the bias with high probability for a broad class
of recommendation systems when sufficient number of users provide feedback on
sufficient number of recommendations. We provide extensive simulations with
real data sets and practical recommender systems, which confirm the trade offs
in the theoretical guarantees."
"Relevance and diversity are both crucial criteria for an effective search
system. In this paper, we propose a unified learning framework for
simultaneously optimizing both relevance and diversity. Specifically, the
problem is formalized as a structural learning framework optimizing
Diversity-Correlated Evaluation Measures (DCEM), such as ERR-IA, a-NDCG and
NRBP. Within this framework, the discriminant function is defined to be a
bi-criteria objective maximizing the sum of the relevance scores and
dissimilarities (or diversity) among the documents. Relevance and diversity
features are utilized to define the relevance scores and dissimilarities,
respectively. Compared with traditional methods, the advantages of our approach
lie in that: (1) Directly optimizing DCEM as the loss function is more
fundamental for the task; (2) Our framework does not rely on explicit diversity
information such as subtopics, thus is more adaptive to real application; (3)
The representation of diversity as the feature-based scoring function is more
flexible to incorporate rich diversity-based features into the learning
framework. Extensive experiments on the public TREC datasets show that our
approach significantly outperforms state-of-the-art diversification approaches,
which validate the above advantages."
"Efficient similarity retrieval from large-scale multimodal database is
pervasive in modern search engines and social networks. To support queries
across content modalities, the system should enable cross-modal correlation and
computation-efficient indexing. While hashing methods have shown great
potential in achieving this goal, current attempts generally fail to learn
isomorphic hash codes in a seamless scheme, that is, they embed multiple
modalities in a continuous isomorphic space and separately threshold embeddings
into binary codes, which incurs substantial loss of retrieval accuracy. In this
paper, we approach seamless multimodal hashing by proposing a novel Composite
Correlation Quantization (CCQ) model. Specifically, CCQ jointly finds
correlation-maximal mappings that transform different modalities into
isomorphic latent space, and learns composite quantizers that convert the
isomorphic latent features into compact binary codes. An optimization framework
is devised to preserve both intra-modal similarity and inter-modal correlation
through minimizing both reconstruction and quantization errors, which can be
trained from both paired and partially paired data in linear time. A
comprehensive set of experiments clearly show the superior effectiveness and
efficiency of CCQ against the state of the art hashing methods for both
unimodal and cross-modal retrieval."
"With the quick development of online social media such as twitter or sina
weibo in china, many users usually track hot topics to satisfy their desired
information need. For a hot topic, new opinions or ideas will be continuously
produced in the form of online data stream. In this scenario, how to
effectively filter and display information for a certain topic dynamically,
will be a critical problem. We call the problem as Topic-focused Dynamic
Information Filtering (denoted as TDIF for short) in social media. In this
paper, we start open discussions on such application problems. We first analyze
the properties of the TDIF problem, which usually contains several typical
requirements: relevance, diversity, recency and confidence. Recency means that
users want to follow the recent opinions or news. Additionally, the confidence
of information must be taken into consideration. How to balance these factors
properly in online data stream is very important and challenging. We propose a
dynamic preservation strategy on the basis of an existing feature-based utility
function, to solve the TDIF problem. Additionally, we propose new dynamic
diversity measures, to get a more reasonable evaluation for such application
problems. Extensive exploratory experiments have been conducted on TREC public
twitter dataset, and the experimental results validate the effectiveness of our
approach."
"One of the main challenges in Interactive Information Retrieval (IIR)
evaluation is the development and application of re-usable tools that allow
researchers to analyze search behavior of real users in different environments
and different domains, but with comparable results. Furthermore, IIR recently
focuses more on the analysis of whole sessions, which includes all user
interactions that are carried out within a session but also across several
sessions by the same user. Some frameworks have already been proposed for the
evaluation of controlled experiments in IIR, but yet no framework is available
for interactive evaluation of search behavior from real-world information
retrieval (IR) systems with real users. In this paper we present a framework
for whole-session evaluation that can also utilize these uncontrolled data
sets. The logging component can easily be integrated into real-world IR systems
for generating and analyzing new log data. Furthermore, due to a supplementary
mapping it is also possible to analyze existing log data. For every IR system
different actions and filters can be defined. This allows system operators and
researchers to use the framework for the analysis of user search behavior in
their IR systems and to compare it with others. Using a graphical user
interface they have the possibility to interactively explore the data set from
a broad overview down to individual sessions."
"Media sharing applications, such as Flickr and Panoramio, contain a large
amount of pictures related to real life events. For this reason, the
development of effective methods to retrieve these pictures is important, but
still a challenging task. Recognizing this importance, and to improve the
retrieval effectiveness of tag-based event retrieval systems, we propose a new
method to extract a set of geographical tag features from raw geo-spatial
profiles of user tags. The main idea is to use these features to select the
best expansion terms in a machine learning-based query expansion approach.
Specifically, we apply rigorous statistical exploratory analysis of spatial
point patterns to extract the geo-spatial features. We use the features both to
summarize the spatial characteristics of the spatial distribution of a single
term, and to determine the similarity between the spatial profiles of two terms
-- i.e., term-to-term spatial similarity. To further improve our approach, we
investigate the effect of combining our geo-spatial features with temporal
features on choosing the expansion terms. To evaluate our method, we perform
several experiments, including well-known feature analyses. Such analyses show
how much our proposed geo-spatial features contribute to improve the overall
retrieval performance. The results from our experiments demonstrate the
effectiveness and viability of our method."
"Evaluation of recommender systems is typically done with finite datasets.
This means that conventional evaluation methodologies are only applicable in
offline experiments, where data and models are stationary. However, in real
world systems, user feedback is continuously generated, at unpredictable rates.
Given this setting, one important issue is how to evaluate algorithms in such a
streaming data environment. In this paper we propose a prequential evaluation
protocol for recommender systems, suitable for streaming data environments, but
also applicable in stationary settings. Using this protocol we are able to
monitor the evolution of algorithms' accuracy over time. Furthermore, we are
able to perform reliable comparative assessments of algorithms by computing
significance tests over a sliding window. We argue that besides being suitable
for streaming data, prequential evaluation allows the detection of phenomena
that would otherwise remain unnoticed in the evaluation of both offline and
online recommender systems."
"Keeping in consideration the high demand for clustering, this paper focuses
on understanding and implementing K-means clustering using two different
similarity measures. We have tried to cluster the documents using two different
measures rather than clustering it with Euclidean distance. Also a comparison
is drawn based on accuracy of clustering between fuzzy and cosine similarity
measure. The start time and end time parameters for formation of clusters are
used in deciding optimum similarity measure."
"In microblogging, hashtags are used to be topical markers, and they are
adopted by users that contribute similar content or express a related idea.
However, hashtags are created in a free style and there is no domain category
information about them, which make users hard to get access to organized
hashtag presentation. In this paper, we propose an approach that classifies
hashtags with news categories, and then carry out a domain-sensitive popularity
ranking to get hot hashtags in each domain. The proposed approach first trains
a domain classification model with news content and news category information,
then detects microblogs related to a hashtag to be its representative text,
based on which we can classify this hashtag with a domain. Finally, we
calculate the domain-sensitive popularity of each hashtag with multiple
factors, to get most hotly discussed hashtags in each domain. Preliminary
experimental results on a dataset from Sina Weibo, one of the largest Chinese
microblogging websites, show usefulness of the proposed approach on describing
hashtags."
"Knowledge bases are very good sources for knowledge extraction, the ability
to create knowledge from structured and unstructured sources and use it to
improve automatic processes as query expansion. However, extracting knowledge
from unstructured sources is still an open challenge. In this respect,
understanding the structure of knowledge bases can provide significant benefits
for the effectiveness of such purpose. In particular, Wikipedia has become a
very popular knowledge base in the last years because it is a general
encyclopedia that has a large amount of information and thus, covers a large
amount of different topics. In this piece of work, we analyze how articles and
categories of Wikipedia relate to each other and how these relationships can
support a query expansion technique. In particular, we show that the structures
in the form of dense cycles with a minimum amount of categories tend to
identify the most relevant information."
"In this paper, we survey various user-centered or context-based biomedical
health information retrieval systems. We present and discuss the performance of
systems submitted in CLEF eHealth 2014 Task 3 for this purpose. We classify and
focus on comparing the two most prevalent retrieval models in biomedical
information retrieval namely: Language Model (LM) and Vector Space Model (VSM).
We also report on the effectiveness of using external medical resources and
ontologies like MeSH, Metamap, UMLS, etc. We observed that the L.M. based
retrieval systems outperform VSM based systems on various fronts. From the
results we conclude that the state-of-art system scores for MAP was 0.4146,
P@10 was 0.7560 and NDCG@10 was 0.7445, respectively. All of these score were
reported by systems built on language modelling approaches."
"We summarize math search engines and search interfaces produced by the
Document and Pattern Recognition Lab in recent years, and in particular the min
math search interface and the Tangent search engine. Source code for both
systems are publicly available. ""The Masses"" refers to our emphasis on creating
systems for mathematical non-experts, who may be looking to define unfamiliar
notation, or browse documents based on the visual appearance of formulae rather
than their mathematical semantics."
"This paper describes a real world deployment of a context-aware mobile app
recommender system (RS) called Frappe. Utilizing a hybrid-approach, we
conducted a large-scale app market deployment with 1000 Android users combined
with a small-scale local user study involving 33 users. The resulting usage
logs and subjective feedback enabled us to gather key insights into (1)
context-dependent app usage and (2) the perceptions and experiences of
end-users while interacting with context-aware mobile app recommendations.
While Frappe performs very well based on usage-centric evaluation metrics
insights from the small-scale study reveal some negative user experiences. Our
results point to a number of actionable lessons learned specifically related to
designing, deploying and evaluating mobile context-aware RS in-the-wild with
real users."
"Efficient indexing and searching of high dimensional data has been an area of
active research due to the growing exploitation of high dimensional data and
the vulnerability of traditional search methods to the curse of dimensionality.
This paper presents a new approach for fast and effective searching and
indexing of high dimensional features using random partitions of the feature
space. Experiments on both handwritten digits and 3-D shape descriptors have
shown the proposed algorithm to be highly effective and efficient in indexing
and searching real data sets of several hundred dimensions. We also compare its
performance to that of the state-of-the-art locality sensitive hashing
algorithm."
"When dealing with document similarity many methods exist today, like cosine
similarity. More complex methods are also available based on the semantic
analysis of textual information, which are computationally expensive and rarely
used in the real time feeding of content as in enterprise-wide search
environments. To address these real-time constraints, we developed a new
measure of document similarity called Textual Spatial Cosine Similarity, which
is able to detect similitude at the semantic level using word placement
information contained in the document. We will see in this paper that two
degenerate cases exist for this model, which coincide with Cosine Similarity on
one side and with a paraphrasing detection model to the other."
"The vast amount of geo-tagged social images has attracted great attention in
research of predicting location using the plentiful content of images, such as
visual content and textual description. Most of the existing researches use the
text-based or vision-based method to predict location. There still exists a
problem: how to effectively exploit the correlation between different types of
content as well as their geographical distributions for location prediction. In
this paper, we propose to predict image location by learning the latent
relation between geographical location and multiple types of image content. In
particularly, we propose a geographical topic model GTMI (geographical topic
model of social image) to integrate multiple types of image content as well as
the geographical distributions, In GTMI, image topic is modeled on both text
vocabulary and visual feature. Each region has its own distribution over topics
and hence has its own language model and vision pattern. The location of a new
image is estimated based on the joint probability of image content and
similarity measure on topic distribution between images. Experiment results
demonstrate the performance of location prediction based on GTMI."
"Query Understanding concerns about inferring the precise intent of search by
the user with his formulated query, which is challenging because the queries
are often very short and ambiguous. The report discusses the various kind of
queries that can be put to a Search Engine and illustrates the Role of Query
Understanding for return of relevant results. With different advances in
techniques for deep understanding of queries as well as documents, the Search
Technology has witnessed three major era. A lot of interesting real world
examples have been used to illustrate the role of Query Understanding in each
of them. The Query Understanding Module is responsible to correct the mistakes
done by user in the query, to guide him in formulation of query with precise
intent, and to precisely infer the intent of the user query. The report
describes the complete architecture to handle aforementioned three tasks, and
then discusses basic as well as recent advanced techniques for each of the
component, through appropriate papers from reputed conferences and journals."
"Data visualization is one of the major applications of nonlinear
dimensionality reduction. From the information retrieval perspective, the
quality of a visualization can be evaluated by considering the extent that the
neighborhood relation of each data point is maintained while the number of
unrelated points that are retrieved is minimized. This property can be
quantified as a trade-off between the mean precision and mean recall of the
visualization. While there have been some approaches to formulate the
visualization objective directly as a weighted sum of the precision and recall,
there is no systematic way to determine the optimal trade-off between these two
nor a clear interpretation of the optimal value. In this paper, we investigate
the properties of $\alpha$-divergence for information visualization, focusing
our attention on a particular range of $\alpha$ values. We show that the
minimization of the new cost function corresponds to maximizing a geometric
mean between precision and recall, parameterized by $\alpha$. Contrary to some
earlier methods, no hand-tuning is needed, but we can rigorously estimate the
optimal value of $\alpha$ for a given input data. For this, we provide a
statistical framework using a novel distribution called Exponential Divergence
with Augmentation (EDA). By the extensive set of experiments, we show that the
optimal value of $\alpha$, obtained by EDA corresponds to the optimal trade-off
between the precision and recall for a given data distribution."
"Visualization is a powerful paradigm for exploratory data analysis.
Visualizing large graphs, however, often results in a meaningless hairball. In
this paper, we propose a different approach that helps the user adaptively
explore large million-node graphs from a local perspective. For nodes that the
user investigates, we propose to only show the neighbors with the most
subjectively interesting neighborhoods. We contribute novel ideas to measure
this interestingness in terms of how surprising a neighborhood is given the
background distribution, as well as how well it fits the nodes the user chose
to explore. We introduce FACETS, a fast and scalable method for visually
exploring large graphs. By implementing our above ideas, it allows users to
look into the forest through its trees. Empirical evaluation shows that our
method works very well in practice, providing rankings of nodes that match
interests of users. Moreover, as it scales linearly, FACETS is suited for the
exploration of very large graphs."
"Neighbourhood-based Collaborative Filtering (CF) has been applied in the
industry for several decades, because of the easy implementation and high
recommendation accuracy. As the core of neighbourhood-based CF, the task of
dynamically maintaining users' similarity list is challenged by cold-start
problem and scalability problem. Recently, several methods are presented on
solving the two problems. However, these methods applied an $O(n^2)$ algorithm
to compute the similarity list in a special case, where the new users, with
enough recommendation data, have the same rating list. To address the problem
of large computational cost caused by the special case, we design a faster
($O(\frac{1}{125}n^2)$) algorithm, TwinSearch Algorithm, to avoid computing and
sorting the similarity list for the new users repeatedly to save the
computational resources. Both theoretical and experimental results show that
the TwinSearch Algorithm achieves better running time than the traditional
method."
"Similarity search in math is to find mathematical expressions that are
similar to a user's query. We conceptualized the similarity factors between
mathematical expressions, and proposed an approach to math similarity search
(MSS) by defining metrics based on those similarity factors [11]. Our
preliminary implementation indicated the advantage of MSS compared to
non-similarity based search. In order to more effectively and efficiently
search similar math expressions, MSS is further optimized. This paper focuses
on performance evaluation and optimization of MSS. Our results show that the
proposed optimization process significantly improved the performance of MSS
with respect to both relevance ranking and recall."
"Large-scale retrieval systems are often implemented as a cascading sequence
of phases -- a first filtering step, in which a large set of candidate
documents are extracted using a simple technique such as Boolean matching
and/or static document scores; and then one or more ranking steps, in which the
pool of documents retrieved by the filter is scored more precisely using dozens
or perhaps hundreds of different features. The documents returned to the user
are then taken from the head of the final ranked list. Here we examine methods
for measuring the quality of filtering and preliminary ranking stages, and show
how to use these measurements to tune the overall performance of the system.
Standard top-weighted metrics used for overall system evaluation are not
appropriate for assessing filtering stages, since the output is a set of
documents, rather than an ordered sequence of documents. Instead, we use an
approach in which a quality score is computed based on the discrepancy between
filtered and full evaluation. Unlike previous approaches, our methods do not
require relevance judgments, and thus can be used with virtually any query set.
We show that this quality score directly correlates with actual differences in
measured effectiveness when relevance judgments are available. Since the
quality score does not require relevance judgments, it can be used to identify
queries that perform particularly poorly for a given filter. Using these
methods, we explore a wide range of filtering options using thousands of
queries, categorize the relative merits of the different approaches, and
identify useful parameter combinations."
"Recommendation based on user preferences is a common task for e-commerce
websites. New recommendation algorithms are often evaluated by offline
comparison to baseline algorithms such as recommending random or the most
popular items. Here, we investigate how these algorithms themselves perform and
compare to the operational production system in large scale online experiments
in a real-world application. Specifically, we focus on recommending travel
destinations at Booking.com, a major online travel site, to users searching for
their preferred vacation activities. To build ranking models we use
multi-criteria rating data provided by previous users after their stay at a
destination. We implement three methods and compare them to the current
baseline in Booking.com: random, most popular, and Naive Bayes. Our general
conclusion is that, in an online A/B test with live users, our Naive-Bayes
based ranker increased user engagement significantly over the current online
system."
"The participatory Web has enabled the ubiquitous and pervasive access of
information, accompanied by an increase of speed and reach in information
sharing. Data dissemination services such as news aggregators are expected to
provide up-to-date, real-time information to the end users. News aggregators
are in essence recommendation systems that filter and rank news stories in
order to select the few that will appear on the users front screen at any time.
One of the main challenges in such systems is to address the recency and
latency problems, that is, to identify as soon as possible how important a news
story is. In this work we propose an integrated framework that aims at
predicting the importance of news items upon their publication with a focus on
recent and highly popular news, employing resampling strategies, and at
translating the result into concrete news rankings. We perform an extensive
experimental evaluation using real-life datasets of the proposed framework as
both a stand-alone system and when applied to news recommendations from Google
News. Additionally, we propose and evaluate a combinatorial solution to the
augmentation of official media recommendations with social information. Results
show that the proposed approach complements and enhances the news rankings
generated by state-of-the-art systems."
"WDAqua is a Marie Curie Innovative Training Network (ITN) and is funded under
EU grant number 642795 and runs from January 2015 to December 2018. WDAqua aims
at advancing the state of the art by intertwining training, research and
innovation efforts, centered around one service: data-driven question
answering. Question answering is immediately useful to a wide audience of end
users, and we will demonstrate this in settings including e-commerce, public
sector information, publishing and smart cities. Question answering also covers
web science and data science broadly, leading to transferrable research results
and to transferrable skills of the researchers who have finished our training
programme. To ensure that our research improves question answering overall,
every individual research project connects at least two of these steps.
Intersectional secondments (within a consortium covering academia, research
institutes and industrial research as well as network-wide workshops, R and D
challenges and innovation projects further balance ground-breaking research and
the needs of society and industry. Training-wise these offers equip early stage
researchers with the expertise and transferable technical and non-technical
skills that will allow them to pursue a successful career as an academic,
decision maker, practitioner or entrepreneur."
"Personalization collaborative filtering recommender systems (CFRSs) are the
crucial components of popular e-commerce services. In practice, CFRSs are also
particularly vulnerable to ""shilling"" attacks or ""profile injection"" attacks
due to their openness. The attackers can carefully inject chosen attack
profiles into CFRSs in order to bias the recommendation results to their
benefits. To reduce this risk, various detection techniques have been proposed
to detect such attacks, which use diverse features extracted from user
profiles. However, relying on limited features to improve the detection
performance is difficult seemingly, since the existing features can not fully
characterize the attack profiles and genuine profiles. In this paper, we
propose a novel detection method to make recommender systems resistant to the
""shilling"" attacks or ""profile injection"" attacks. The existing features can be
briefly summarized as two aspects including rating behavior based and item
distribution based. We firstly formulate the problem as finding a mapping model
between rating behavior and item distribution by exploiting the least-squares
approximate solution. Based on the trained model, we design a detector by
employing a regressor to detect such attacks. Extensive experiments on both the
MovieLens-100K and MovieLens-ml-latest-small datasets examine the effectiveness
of our proposed detection method. Experimental results were included to
validate the outperformance of our approach in comparison with benchmarked
method including KNN."
"This work describes the theory and the implementation of a new software tool,
the ""Web Topical Discovery System"" (WTDS), which provides an approach to the
automatic discovery and selection of new web pages relevant to specific
analytical needs. We will see how it is possible to specify the research
context with search keywords related to the area of interest and consider the
important problem of removing extraneous data from a web page containing an
article in order to reduce, to a minimum, false positives represented by a
match on a keyword that is showing up on the latest news box of the same page.
The removal of duplicates, the analysis of richness of information contained in
the article and lexical diversity are all taken into consideration in order to
provide the optimum set of recommendations to the end user or system."
"Automatic query reformulation refers to rewriting a user's original query in
order to improve the ranking of retrieval results compared to the original
query. We present a general framework for automatic query reformulation based
on discrete optimization. Our approach, referred to as pseudo-query
reformulation, treats automatic query reformulation as a search problem over
the graph of unweighted queries linked by minimal transformations (e.g. term
additions, deletions). This framework allows us to test existing performance
prediction methods as heuristics for the graph search process. We demonstrate
the effectiveness of the approach on several publicly available datasets."
"Recently bipartite graphs have been widely used to represent the relationship
two sets of items for information retrieval applications. The Web offers a wide
range of data which can be represented by bipartite graphs, such us movies and
reviewers in recomender systems, queries and URLs in search engines, users and
posts in social networks. The size and the dynamic nature of such graphs
generate the need for more efficient ranking methods.
  In this thesis, at first we present the fundamental mathematical backround
that we use subsequently and we describe the basic principles of the
Perron-Frobebius theory for non negative matrices as well as the the basic
principles of the Markov chain theory. Then, we propose a novel algorithm named
BipartiteRank, which is suitable to rank scenarios, that can be represented as
a bipartite graph. This algorithm is based on the random surfer model and
inherits the basic mathematical characteristics of PageRank. What makes it
different, is the fact that it introduces an alternative type of teleportation,
based on the block structure of the bipartite graph in order to achieve more
efficient ranking. Finally, we support this opinion with mathematical arguments
and then we confirm it experimentally through a series of tests on real data."
"With the ever-increasing quantity and variety of data worldwide, the Web has
become a rich repository of mathematical formulae. This necessitates the
creation of robust and scalable systems for Mathematical Information Retrieval,
where users search for mathematical information using individual formulae
(query-by-expression) or a combination of keywords and formulae. Often, the
pages that best satisfy users' information needs contain expressions that only
approximately match the query formulae. For users trying to locate or re-find a
specific expression, browse for similar formulae, or who are mathematical
non-experts, the similarity of formulae depends more on the relative positions
of symbols than on deep mathematical semantics.
  We propose the Maximum Subtree Similarity (MSS) metric for
query-by-expression that produces intuitive rankings of formulae based on their
appearance, as represented by the types and relative positions of symbols.
Because it is too expensive to apply the metric against all formulae in large
collections, we first retrieve expressions using an inverted index over tuples
that encode relationships between pairs of symbols, ranking hits using the Dice
coefficient. The top-k formulae are then re-ranked using MSS. Our approach
obtains state-of-the-art performance on the NTCIR-11 Wikipedia formula
retrieval benchmark and is efficient in terms of both index space and overall
retrieval time. Retrieval systems for other graphical forms, including chemical
diagrams, flowcharts, figures, and tables, may also benefit from adopting our
approach."
"Classical approaches in recommender systems such as collaborative filtering
are concentrated mainly on static user preference extraction. This approach
works well as an example for music recommendations when a user behavior tends
to be stable over long period of time, however the most common situation in
e-commerce is different which requires reactive algorithms based on a
short-term user activity analysis. This paper introduces a small mathematical
framework for short-term user interest detection formulated in terms of item
properties and its application for recommender systems enhancing. The framework
is based on the fundamental concept of information theory --- Kullback-Leibler
divergence."
"Modelling term dependence in IR aims to identify co-occurring terms that are
too heavily dependent on each other to be treated as a bag of words, and to
adapt the indexing and ranking accordingly. Dependent terms are predominantly
identified using lexical frequency statistics, assuming that (a) if terms
co-occur often enough in some corpus, they are semantically dependent; (b) the
more often they co-occur, the more semantically dependent they are. This
assumption is not always correct: the frequency of co-occurring terms can be
separate from the strength of their semantic dependence. E.g. ""red tape"" might
be overall less frequent than ""tape measure"" in some corpus, but this does not
mean that ""red""+""tape"" are less dependent than ""tape""+""measure"". This is
especially the case for non-compositional phrases, i.e. phrases whose meaning
cannot be composed from the individual meanings of their terms (such as the
phrase ""red tape"" meaning bureaucracy). Motivated by this lack of distinction
between the frequency and strength of term dependence in IR, we present a
principled approach for handling term dependence in queries, using both lexical
frequency and semantic evidence. We focus on non-compositional phrases,
extending a recent unsupervised model for their detection [21] to IR. Our
approach, integrated into ranking using Markov Random Fields [31], yields
effectiveness gains over competitive TREC baselines, showing that there is
still room for improvement in the very well-studied area of term dependence in
IR."
"We present two novel models of document coherence and their application to
information retrieval (IR). Both models approximate document coherence using
discourse entities, e.g. the subject or object of a sentence. Our first model
views text as a Markov process generating sequences of discourse entities
(entity n-grams); we use the entropy of these entity n-grams to approximate the
rate at which new information appears in text, reasoning that as more new words
appear, the topic increasingly drifts and text coherence decreases. Our second
model extends the work of Guinaudeau & Strube [28] that represents text as a
graph of discourse entities, linked by different relations, such as their
distance or adjacency in text. We use several graph topology metrics to
approximate different aspects of the discourse flow that can indicate
coherence, such as the average clustering or betweenness of discourse entities
in text. Experiments with several instantiations of these models show that: (i)
our models perform on a par with two other well-known models of text coherence
even without any parameter tuning, and (ii) reranking retrieval results
according to their coherence scores gives notable performance gains, confirming
a relation between document coherence and relevance. This work contributes two
novel models of document coherence, the application of which to IR complements
recent work in the integration of document cohesiveness or comprehensibility to
ranking [5, 56]."
"I present a hybrid matrix factorisation model representing users and items as
linear combinations of their content features' latent factors. The model
outperforms both collaborative and content-based models in cold-start or sparse
interaction data scenarios (using both user and item metadata), and performs at
least as well as a pure collaborative matrix factorisation model where
interaction data is abundant. Additionally, feature embeddings produced by the
model encode semantic information in a way reminiscent of word embedding
approaches, making them useful for a range of related tasks such as tag
recommendations."
"A generalized ensemble model (gEnM) for document ranking is proposed in this
paper. The gEnM linearly combines basis document retrieval models and tries to
retrieve relevant documents at high positions. In order to obtain the optimal
linear combination of multiple document retrieval models or rankers, an
optimization program is formulated by directly maximizing the mean average
precision. Both supervised and unsupervised learning algorithms are presented
to solve this program. For the supervised scheme, two approaches are considered
based on the data setting, namely batch and online setting. In the batch
setting, we propose a revised Newton's algorithm, gEnM.BAT, by approximating
the derivative and Hessian matrix. In the online setting, we advocate a
stochastic gradient descent (SGD) based algorithm---gEnM.ON. As for the
unsupervised scheme, an unsupervised ensemble model (UnsEnM) by iteratively
co-learning from each constituent ranker is presented. Experimental study on
benchmark data sets verifies the effectiveness of the proposed algorithms.
Therefore, with appropriate algorithms, the gEnM is a viable option in diverse
practical information retrieval applications."
"Unstructured data, such as news and blogs, can provide valuable insights into
the financial world. We present the NewsStream portal, an intuitive and
easy-to-use tool for news analytics, which supports interactive querying and
visualizations of the documents at different levels of detail. It relies on a
scalable architecture for real-time processing of a continuous stream of
textual data, which incorporates data acquisition, cleaning, natural-language
preprocessing and semantic annotation components. It has been running for over
two years and collected over 18 million news articles and blog posts. The
NewsStream portal can be used to answer the questions when, how often, in what
context, and with what sentiment was a financial entity or term mentioned in a
continuous stream of news and blogs, and therefore providing a complement to
news aggregators. We illustrate some features of our system in three use cases:
relations between the rating agencies and the PIIGS countries, reflection of
financial news on credit default swap (CDS) prices, the emergence of the
Bitcoin digital currency, and visualizing how the world is connected through
news."
"Many e-commerce websites use recommender systems to recommend items to users.
When a user or item is new, the system may fail because not enough information
is available on this user or item. Various solutions to this `cold-start
problem' have been proposed in the literature. However, many real-life
e-commerce applications suffer from an aggravated, recurring version of
cold-start even for known users or items, since many users visit the website
rarely, change their interests over time, or exhibit different personas. This
paper exposes the `Continuous Cold Start' (CoCoS) problem and its consequences
for content- and context-based recommendation from the viewpoint of typical
e-commerce applications, illustrated with examples from a major travel
recommendation website, Booking.com."
"Available recommender systems mostly provide recommendations based on the
users preferences by utilizing traditional methods such as collaborative
filtering which only relies on the similarities between users and items.
However, collaborative filtering might lead to provide poor recommendation
because it does not rely on other useful available data such as users locations
and hence the accuracy of the recommendations could be very low and
inefficient. This could be very obvious in the systems that locations would
affect users preferences highly such as movie recommender systems. In this
paper a new location-based movie recommender system based on the collaborative
filtering is introduced for enhancing the accuracy and the quality of
recommendations. In this approach, users locations have been utilized and take
in consideration in the entire processing of the recommendations and peer
selections. The potential of the proposed approach in providing novel and
better quality recommendations have been discussed through experiments in real
datasets."
"Specific to Math Information Retrieval is combining text with mathematical
formulae both in documents and in queries. Rigorous evaluation of query
expansion and merging strategies combining math and standard textual keyword
terms in a query are given. It is shown that techniques similar to those known
from textual query processing may be applied in math information retrieval as
well, and lead to a cutting edge performance. Striping and merging partial
results from subqueries is one technique that improves results measured by
information retrieval evaluation metrics like Bpref."
"Deep Web is content hidden behind HTML forms. Since it represents a large
portion of the structured, unstructured and dynamic data on the Web, accessing
Deep-Web content has been a long challenge for the database community. This
paper describes a crawler for accessing Deep-Web using Ontologies. Performance
evaluation of the proposed work showed that this new approach has promising
results."
"Giving user a simple and well organized web search result has been a topic of
active information Retrieval (IR) research. Irrespective of how small or
ambiguous a query is, a user always wants the desired result on the first
display of an IR system. Clustering of an IR system result can render a way,
which fulfills the actual information need of a user. In this paper, an
approach to cluster an IR system result is presented.The approach is a
combination of heuristics and k-means technique using cosine similarity. Our
heuristic approach detects the initial value of k for creating initial
centroids. This eliminates the problem of external specification of the value
k, which may lead to unwanted result if wrongly specified. The centroids
created in this way are more specific and meaningful in the context of web
search result. Another advantage of the proposed method is the removal of the
objective means function of k-means which makes cluster sizes same. The end
result of the proposed approach consists of different clusters of documents
having different sizes."
"In this technical report we present a database schema used to store Wikipedia
so it can be easily used in query-intensive applications. In addition to
storing the information in a way that makes it highly accessible, our schema
enables users to easily formulate complex queries using information such as the
anchor-text of links and their location in the page, the titles and number of
redirect pages for each page and the paragraph structure of entity pages. We
have successfully used the schema in domains such as recommender systems,
information retrieval and sentiment analysis. In order to assist other
researchers, we now make the schema and its content available online."
"Major search engines deploy personalized Web results to enhance users'
experience, by showing them data supposed to be relevant to their interests.
Even if this process may bring benefits to users while browsing, it also raises
concerns on the selection of the search results. In particular, users may be
unknowingly trapped by search engines in protective information bubbles, called
""filter bubbles"", which can have the undesired effect of separating users from
information that does not fit their preferences. This paper moves from early
results on quantification of personalization over Google search query results.
Inspired by previous works, we have carried out some experiments consisting of
search queries performed by a battery of Google accounts with differently
prepared profiles. Matching query results, we quantify the level of
personalization, according to topics of the queries and the profile of the
accounts. This work reports initial results and it is a first step a for more
extensive investigation to measure Web search personalization."
"The article describes a new data structure called neuro-index. It is an
alternative to well-known file indexes. The neuro-index is fundamentally
different because it stores weight coefficients in neural network. It is not a
reference type like ""keyword-position in a file""."
"In this paper we present LocLinkVis (Locate-Link-Visualize); a system which
supports exploratory information access to a document collection based on
geo-referencing and visualization. It uses a gazetteer which contains
representations of places ranging from countries to buildings, and that is used
to recognize toponyms, disambiguate them into places, and to visualize the
resulting spatial footprints."
"The advances of the Linked Open Data (LOD) initiative are giving rise to a
more structured Web of data. Indeed, a few datasets act as hubs (e.g., DBpedia)
connecting many other datasets. They also made possible new Web services for
entity detection inside plain text (e.g., DBpedia Spotlight), thus allowing for
new applications that can benefit from a combination of the Web of documents
and the Web of data. To ease the emergence of these new applications, we
propose a query-biased algorithm (LDRANK) for the ranking of web of data
resources with associated textual data. Our algorithm combines link analysis
with dimensionality reduction. We use crowdsourcing for building a publicly
available and reusable dataset for the evaluation of query-biased ranking of
Web of data resources detected in Web pages. We show that, on this dataset,
LDRANK outperforms the state of the art. Finally, we use this algorithm for the
construction of semantic snippets of which we evaluate the usefulness with a
crowdsourcing-based approach."
"The vocabulary mismatch problem is one of the important challenges facing
traditional keyword-based Information Retrieval Systems. The aim of query
expansion (QE) is to reduce this query-document mismatch by adding related or
synonymous words or phrases to the query.
  Several existing query expansion algorithms have proved their merit, but they
are not uniformly beneficial for all kinds of queries. Our long-term goal is to
formulate methods for applying QE techniques tailored to individual queries,
rather than applying the same general QE method to all queries. As an initial
step, we have proposed a taxonomy of query classes (from a QE perspective) in
this report. We have discussed the properties of each query class with
examples. We have also discussed some QE strategies that might be effective for
each query category.
  In future work, we intend to test the proposed techniques using standard
datasets, and to explore automatic query categorisation methods."
"Typical retrieval systems have three requirements: a) Accurate retrieval
i.e., the method should have high precision, b) Diverse retrieval, i.e., the
obtained set of points should be diverse, c) Retrieval time should be small.
However, most of the existing methods address only one or two of the above
mentioned requirements. In this work, we present a method based on randomized
locality sensitive hashing which tries to address all of the above requirements
simultaneously. While earlier hashing approaches considered approximate
retrieval to be acceptable only for the sake of efficiency, we argue that one
can further exploit approximate retrieval to provide impressive trade-offs
between accuracy and diversity. We extend our method to the problem of
multi-label prediction, where the goal is to output a diverse and accurate set
of labels for a given document in real-time. Moreover, we introduce a new
notion to simultaneously evaluate a method's performance for both the precision
and diversity measures. Finally, we present empirical results on several
different retrieval tasks and show that our method retrieves diverse and
accurate images/labels while ensuring $100x$-speed-up over the existing diverse
retrieval approaches."
"Web is a wide term which mainly consists of surface web and hidden web. One
can easily access the surface web using traditional web crawlers, but they are
not able to crawl the hidden portion of the web. These traditional crawlers
retrieve contents from web pages, which are linked by hyperlinks ignoring the
information hidden behind form pages, which cannot be extracted using simple
hyperlink structure. Thus, they ignore large amount of data hidden behind
search forms. This paper emphasizes on the extraction of hidden data behind
html search forms. The proposed technique makes use of semantic mapping to fill
the html search form using domain specific database. Using semantics to fill
various fields of a form leads to more accurate and qualitative data
extraction."
"Collaborative filtering (CF) is the most widely used and successful approach
for personalized service recommendations. Among the collaborative
recommendation approaches, neighborhood based approaches enjoy a huge amount of
popularity, due to their simplicity, justifiability, efficiency and stability.
Neighborhood based collaborative filtering approach finds K nearest neighbors
to an active user or K most similar rated items to the target item for
recommendation. Traditional similarity measures use ratings of co-rated items
to find similarity between a pair of users. Therefore, traditional similarity
measures cannot compute effective neighbors in sparse dataset. In this paper,
we propose a two-phase approach, which generates user-user and item-item
networks using traditional similarity measures in the first phase. In the
second phase, two hybrid approaches HB1, HB2, which utilize structural
similarity of both the network for finding K nearest neighbors and K most
similar items to a target items are introduced. To show effectiveness of the
measures, we compared performances of neighborhood based CFs using
state-of-the-art similarity measures with our proposed structural similarity
measures based CFs. Recommendation results on a set of real data show that
proposed measures based CFs outperform existing measures based CFs in various
evaluation metrics."
"Fast Nearest Neighbor (NN) search is a fundamental challenge in large-scale
data processing and analytics, particularly for analyzing multimedia contents
which are often of high dimensionality. Instead of using exact NN search,
extensive research efforts have been focusing on approximate NN search
algorithms. In this work, we present ""HDIdx"", an efficient high-dimensional
indexing library for fast approximate NN search, which is open-source and
written in Python. It offers a family of state-of-the-art algorithms that
convert input high-dimensional vectors into compact binary codes, making them
very efficient and scalable for NN search with very low space complexity."
"Trivia is any fact about an entity, which is interesting due to any of the
following characteristics - unusualness, uniqueness, unexpectedness or
weirdness. Such interesting facts are provided in 'Did You Know?' section at
many places. Although trivia are facts of little importance to be known, but we
have presented their usage in user engagement purpose. Such fun facts generally
spark intrigue and draws user to engage more with the entity, thereby promoting
repeated engagement. The thesis has cited some case studies, which show the
significant impact of using trivia for increasing user engagement or for wide
publicity of the product/service.
  In this thesis, we propose a novel approach for mining entity trivia from
their Wikipedia pages. Given an entity, our system extracts relevant sentences
from its Wikipedia page and produces a list of sentences ranked based on their
interestingness as trivia. At the heart of our system lies an interestingness
ranker which learns the notion of interestingness, through a rich set of
domain-independent linguistic and entity based features. Our ranking model is
trained by leveraging existing user-generated trivia data available on the Web
instead of creating new labeled data for movie domain. For other domains like
sports, celebrities, countries etc. labeled data would have to be created as
described in thesis. We evaluated our system on movies domain and celebrity
domain, and observed that the system performs significantly better than the
defined baselines. A thorough qualitative analysis of the results revealed that
our engineered rich set of features indeed help in surfacing interesting trivia
in the top ranks."
"Recently, a Distribution Separation Method (DSM) is proposed for relevant
feedback in information retrieval, which aims to approximate the true relevance
distribution by separating a seed irrelevance distribution from the mixture
one. While DSM achieved a promising empirical performance, theoretical analysis
of DSM is still need further study and comparison with other relative retrieval
model. In this article, we first generalize DSM's theoretical property, by
proving that its minimum correlation assumption is equivalent to the maximum
(original and symmetrized) KL-Divergence assumption. Second, we also
analytically show that the EM algorithm in a well-known Mixture Model is
essentially a distribution separation process and can be simplified using the
linear separation algorithm in DSM. Some empirical results are also presented
to support our theoretical analysis."
"This paper presents a novel research problem on joint discovery of
commonalities and differences between two individual documents (or document
sets), called Comparative Document Analysis (CDA). Given any pair of documents
from a document collection, CDA aims to automatically identify sets of quality
phrases to summarize the commonalities of both documents and highlight the
distinctions of each with respect to the other informatively and concisely. Our
solution uses a general graph-based framework to derive novel measures on
phrase semantic commonality and pairwise distinction}, and guides the selection
of sets of phrases by solving two joint optimization problems. We develop an
iterative algorithm to integrate the maximization of phrase commonality or
distinction measure with the learning of phrase-document semantic relevance in
a mutually enhancing way. Experiments on text corpora from two different
domains---scientific publications and news---demonstrate the effectiveness and
robustness of the proposed method on comparing individual documents. Our case
study on comparing news articles published at different dates shows the power
of the proposed method on comparing document sets."
"Context-aware recommender systems extend traditional recommenders by adapting
their suggestions to users' contextual situations. CARSKit is a Java-based
open-source library specifically designed for the context-aware recommendation,
where the state-of-the-art context-aware recommendation algorithms have been
implemented. This report provides the basic user's guide to CARSKit, including
how to prepare the data set, how to configure the experimental settings, and
how to evaluate the algorithms, as well as interpreting the outputs."
"Many brokers have adapted their operation to exploit the potential of the
web. Despite the importance of the real estate classifieds, there has been
little work in analyzing such data. In this paper we propose a two-stage
regression model that exploits the textual data in real estate classifieds. We
show how our model can be used to predict the price of a real estate
classified. We also show how our model can be used to highlight keywords that
affect the price positively or negatively. To assess our contributions, we
analyze four real world data sets, which we gathered from three different
property websites. The analysis shows that our model (which exploits textual
features) achieves significantly lower root mean squared error across the
different data sets and against variety of regression models."
"Tourism industry is an extremely information-intensive, complex and dynamic
activity. It can benefit from semantic Web technologies, due to the significant
heterogeneity of information sources and the high volume of on-line data. The
management of semantically diverse annotated tourism data is facilitated by
ontologies that provide methods and standards, which allow flexibility and more
intelligent access to on-line data. This paper provides a description of some
of the early results of the Tourinflux project which aims to apply semantic Web
technologies to support tourist actors in effectively finding and publishing
information on the Web."
"Heterogeneous gap among different modalities emerges as one of the critical
issues in modern AI problems. Unlike traditional uni-modal cases, where raw
features are extracted and directly measured, the heterogeneous nature of cross
modal tasks requires the intrinsic semantic representation to be compared in a
unified framework. This paper studies the learning of different representations
that can be retrieved across different modality contents. A novel approach for
mining cross-modal representations is proposed by incorporating explicit linear
semantic projecting in Hilbert space. The insight is that the discriminative
structures of different modality data can be linearly represented in
appropriate high dimension Hilbert spaces, where linear operations can be used
to approximate nonlinear decisions in the original spaces. As a result, an
efficient linear semantic down mapping is jointly learned for multimodal data,
leading to a common space where they can be compared. The mechanism of ""feature
up-lifting and down-projecting"" works seamlessly as a whole, which accomplishes
crossmodal retrieval tasks very well. The proposed method, named as shared
discriminative semantic representation learning (\textbf{SDSRL}), is tested on
two public multimodal dataset for both within- and inter- modal retrieval. The
experiments demonstrate that it outperforms several state-of-the-art methods in
most scenarios."
"Purpose - To test the ability of major search engines, Google, Yahoo, MSN,
and Ask, to distinguish between German and English-language documents
  Design/methodology/approach - 50 queries, using words common in German and in
English, were posed to the engines. The advanced search option of language
restriction was used, once in German and once in English. The first 20 results
per engine in each language were investigated.
  Findings - While none of the search engines faces problems in providing
results in the language of the interface that is used, both Google and MSN face
problems when the results are restricted to a foreign language.
  Research limitations/implications - Search engines were only tested in German
and in English. We have only anecdotal evidence that the problems are the same
with other languages.
  Practical implications - Searchers should not use the language restriction in
Google and MSN when searching for foreign-language documents. Instead,
searchers should use Yahoo or Ask. If searching for foreign language documents
in Google or MSN, the interface in the target language/country should be used.
  Value of paper - Demonstrates a problem with search engines that has not been
previously investigated."
"Purpose: To compare five major Web search engines (Google, Yahoo, MSN,
Ask.com, and Seekport) for their retrieval effectiveness, taking into account
not only the results but also the results descriptions.
  Design/Methodology/Approach: The study uses real-life queries. Results are
made anonymous and are randomised. Results are judged by the persons posing the
original queries.
  Findings: The two major search engines, Google and Yahoo, perform best, and
there are no significant differences between them. Google delivers
significantly more relevant result descriptions than any other search engine.
This could be one reason for users perceiving this engine as superior.
  Research Limitations: The study is based on a user model where the user takes
into account a certain amount of results rather systematically. This may not be
the case in real life.
  Practical Implications: Implies that search engines should focus on relevant
descriptions. Searchers are advised to use other search engines in addition to
Google.
  Originality/Value: This is the first major study comparing results and
descriptions systematically and proposes new retrieval measures to take into
account results descriptions"
"This paper investigates the composition of search engine results pages. We
define what elements the most popular web search engines use on their results
pages (e.g., organic results, advertisements, shortcuts) and to which degree
they are used for popular vs. rare queries. Therefore, we send 500 queries of
both types to the major search engines Google, Yahoo, Live.com and Ask. We
count how often the different elements are used by the individual engines. In
total, our study is based on 42,758 elements. Findings include that search
engines use quite different approaches to results pages composition and
therefore, the user gets to see quite different results sets depending on the
search engine and search query used. Organic results still play the major role
in the results pages, but different shortcuts are of some importance, too.
Regarding the frequency of certain host within the results sets, we find that
all search engines show Wikipedia results quite often, while other hosts shown
depend on the search engine used. Both Google and Yahoo prefer results from
their own offerings (such as YouTube or Yahoo Answers). Since we used the .com
interfaces of the search engines, results may not be valid for other
country-specific interfaces."
"We carried out a retrieval effectiveness test on the three major web search
engines (i.e., Google, Microsoft and Yahoo). In addition to relevance
judgments, we classified the results according to their commercial intent and
whether or not they carried any advertising. We found that all search engines
provide a large number of results with a commercial intent. Google provides
significantly more commercial results than the other search engines do.
However, the commercial intent of a result did not influence jurors in their
relevance judgments."
"Purpose - To test major Web search engines on their performance on
navigational queries, i.e. searches for homepages. Design/methodology/approach
- 100 real user queries are posed to six search engines (Google, Yahoo, MSN,
Ask, Seekport, and Exalead). Users described the desired pages, and the results
position of these is recorded. Measured success N and mean reciprocal rank are
calculated. Findings - Performance of the major search engines Google, Yahoo,
and MSN is best, with around 90 percent of queries answered correctly. Ask and
Exalead perform worse but receive good scores as well. Research
limitations/implications - All queries were in German, and the German-language
interfaces of the search engines were used. Therefore, the results are only
valid for German queries. Practical implications - When designing a search
engine to compete with the major search engines, care should be taken on the
performance on navigational queries. Users can be influenced easily in their
quality ratings of search engines based on this performance. Originality/value
- This study systematically compares the major search engines on navigational
queries and compares the findings with studies on the retrieval effectiveness
of the engines on informational queries. Paper type - research paper"
"This chapter presents a theoretical framework for evaluating next generation
search engines. We focus on search engines whose results presentation is
enriched with additional information and does not merely present the usual list
of 10 blue links, that is, of ten links to results, accompanied by a short
description. While Web search is used as an example here, the framework can
easily be applied to search engines in any other area. The framework not only
addresses the results presentation, but also takes into account an extension of
the general design of retrieval effectiveness tests. The chapter examines the
ways in which this design might influence the results of such studies and how a
reliable test is best designed."
"It is widely known that people become better at an activity if they perform
this activity long and often. Yet, the question is whether being active in
related areas like communicating online, writing blog articles or commenting on
community forums have an impact on a persons ability to perform Web searches,
is still unanswered. Web searching has become a key task conducted online; in
this paper we present our findings on whether the user type, which categorises
a persons online activities, has an impact on her or his search capabilities.
We show (1) the characteristics of different user types when carrying out
simple search tasks; (2) their characteristics when carrying out complex search
tasks; and, (3) the significantly different user type characteristics between
simple and complex search tasks. The results are based on an experiment with 56
ordinary Web users in a laboratory environment. The Search-Logger study
framework was used to analyze and measure user behavior when carrying out a set
of 12 predefined search tasks. Our findings include the fact that depending on
task type (simple or complex) significant differences can be observed between
users of different types."
"Evaluation of search engines relies on assessments of search results for
selected test queries, from which we would ideally like to draw conclusions in
terms of relevance of the results for general (e.g., future, unknown) users. In
practice however, most evaluation scenarios only allow us to conclusively
determine the relevance towards the particular assessor that provided the
judgments. A factor that cannot be ignored when extending conclusions made from
assessors towards users, is the possible disagreement on relevance, assuming
that a single gold truth label does not exist. This paper presents and analyzes
the Predicted Relevance Model (PRM), which allows predicting a particular
result's relevance for a random user, based on an observed assessment and
knowledge on the average disagreement between assessors. With the PRM, existing
evaluation metrics designed to measure binary assessor relevance, can be
transformed into more robust and effectively graded measures that evaluate
relevance towards a random user. It also leads to a principled way of
quantifying multiple graded or categorical relevance levels for use as gains in
established graded relevance measures, such as normalized discounted cumulative
gain (nDCG), which nowadays often use heuristic and data-independent gain
values. Given a set of test topics with graded relevance judgments, the PRM
allows evaluating systems on different scenarios, such as their capability of
retrieving top results, or how well they are able to filter out non-relevant
ones. Its use in actual evaluation scenarios is illustrated on several
information retrieval test collections."
"Entity suggestion by example (ESbE) refers to a type of entity acquisition
query in which a user provides a set of example entities as the query and
obtains in return some entities that best complete the concept underlying the
given query. Such entity acquisition queries can be useful in many applications
such as related-entity recommendation and query expansion. A number of ESbE
query processing solutions exist in the literature. However, they mostly build
only on the idea of entity co-occurrences either in text or web lists, without
taking advantage of the existence of many web-scale conceptual taxonomies that
consist of hierarchical isA relationships between entity-concept pairs. This
paper provides a query processing method based on the relevance models between
entity sets and concepts. These relevance models can be used to obtain the
fine-grained concepts implied by the query entity set, and the entities that
belong to a given concept, thereby providing the entity suggestions. Extensive
evaluations with real data sets show that the accuracy of the queries processed
with this new method is significantly higher than that of existing solutions."
"This paper studies the problem of finding typical entities when the concept
is given as a query. For a short concept such as university, this is a
well-studied problem of retrieving knowledge base such as Microsoft's Probase
and Google's isA database pre-materializing entities found for the concept in
Hearst patterns of the web corpus. However, we find most real-life queries are
long concept queries (LCQs), such as top American private university, which
cannot and should not be pre-materialized. Our goal is an online construction
of entity retrieval for LCQs. We argue a naive baseline of rewriting LCQs into
an intersection of an expanded set of composing short concepts leads to highly
precise results with extremely low recall. Instead, we propose to augment the
concept list, by identifying related concepts of the query concept. However, as
such increase of recall often invites false positives and decreases precision
in return, we propose the following two techniques: First, we identify concepts
with different relatedness to generate linear orderings and pairwise ordering
constraints. Second, we rank entities trying to avoid conflicts with these
constraints, to prune out lowly ranked one (likely false positives). With these
novel techniques, our approach significantly outperforms state-of-the-arts."
"Encyclopedic queries express the intent of obtaining information typically
available in encyclopedias, such as biographical, geographical or historical
facts. In this paper, we train a classifier for detecting the encyclopedic
intent of web queries. For training such a classifier, we automatically label
training data from raw query logs. We use click-through data to select positive
examples of encyclopedic queries as those queries that mostly lead to Wikipedia
articles. We investigated a large set of features that can be generated to
describe the input query. These features include both term-specific patterns as
well as query projections on knowledge bases items (e.g. Freebase). Results
show that using these feature sets it is possible to achieve an F1 score above
87%, competing with a Google-based baseline, which uses a much wider set of
signals to boost the ranking of Wikipedia for potential encyclopedic queries.
The results also show that both query projections on Wikipedia article titles
and Freebase entity match represent the most relevant groups of features. When
the training set contains frequent positive examples (i.e rare queries are
excluded) results tend to improve."
"Nowadays adult content represents a non negligible proportion of the Web
content. It is of the utmost importance to protect children from this content.
Search engines, as an entry point for Web navigation are ideally placed to deal
with this issue.
  In this paper, we propose a method that builds a safe index i.e.
adult-content free for search engines. This method is based on a filter that
uses only textual information from the web page and the associated URL."
"The continuous increasing in the amount of the published and stored
information requires a special Information Retrieval (IR) frameworks to search
and get information accurately and speedily. Currently, keywords-based
techniques are commonly used in information retrieval. However, a major
drawback of the keywords approach is its inability of handling the polysemy and
synonymy phenomenon of the natural language. For instance, the meanings of
words and understanding of concepts differ in different communities. Same word
use for different concepts (polysemy) or use different words for the same
concept (synonymy). Most of information retrieval frameworks have a weakness to
deal with the semantics of the words in term of (indexing, Boolean model,
Latent Semantic Analysis (LSA) , Latent semantic Index (LSI) and semantic
ranking, etc.). Traditional Arabic Information Retrieval (AIR) models
performance insufficient with semantic queries, which deal with not only the
keywords but also with the context of these keywords. Therefore, there is a
need for a semantic information retrieval model with a semantic index structure
and ranking algorithm based on semantic index."
"Arabic language is one of the most widely spoken languages. This language has
a complex morphological structure and is considered as one of the most prolific
languages in terms of article linguistic. Therefore, Arabic Information
Retrieval (AIR) models need specific techniques to deal with this complex
morphological structure. This paper aims to develop an integrate AIR
frameworks. It lists and analysis the different Information Retrieval (IR)
methods and techniques such as query processing, stemming and indexing which
are used in AIR systems. We conclude that AIR frameworks have a weakness to
deal with semantic in term of indexing, Boolean model, Latent Semantic Analysis
(LSA), Latent Semantic Index (LSI) and semantic ranking. Therefore, semantic
Boolean IR framework is proposed in this paper. This model is implemented and
the precision, recall and run time are measured and compared with the
traditional IR model."
"Although there has been a great deal of interest in analyzing customer
opinions and breaking news in microblogs, progress has been hampered by the
lack of an effective mechanism to discover and retrieve data of interest from
microblogs. To address this problem, we have developed an uncertainty-aware
visual analytics approach to retrieve salient posts, users, and hashtags. We
extend an existing ranking technique to compute a multifaceted retrieval
result: the mutual reinforcement rank of a graph node, the uncertainty of each
rank, and the propagation of uncertainty among different graph nodes. To
illustrate the three facets, we have also designed a composite visualization
with three visual components: a graph visualization, an uncertainty glyph, and
a flow map. The graph visualization with glyphs, the flow map, and the
uncertainty analysis together enable analysts to effectively find the most
uncertain results and interactively refine them. We have applied our approach
to several Twitter datasets. Qualitative evaluation and two real-world case
studies demonstrate the promise of our approach for retrieving high-quality
microblog data."
"We propose a method for using the scoring values of passages to effectively
retrieve documents in a Question Answering system.
  For this, we suggest evaluation function that considers proximity between
each question terms in passage. And using this evaluation function , we extract
a documents which involves scoring values in the highest collection, as a
suitable document for question.
  The proposed method is very effective in document retrieval of Korean
question answering system."
"Online reviews of businesses have become increasingly important in recent
years, as customers and even competitors use them to judge the quality of a
business. Yelp is one of the most popular websites for users to write such
reviews, and it would be useful for them to be able to predict the sentiment or
even the star rating of a review. In this paper, we develop two classifiers to
perform positive/negative classification and 5-star classification. We use
Naive Bayes, Support Vector Machines, and Logistic Regression as models, and
achieved the best accuracy with Logistic Regression: 92.90% for
positive/negative classification, and 63.92% for 5-star classification. These
results demonstrate the quality of the Logistic Regression model using only the
text of the review, yet there is a promising opportunity for improvement with
more data, more features, and perhaps different models."
"Domain expertise is regarded as one of the key factors impacting search
success: experts are known to write more effective queries, to select the right
results on the result page, and to find answers satisfying their information
needs. Search transaction logs play the crucial role in the result ranking. Yet
despite the variety in expertise levels of users, all prior interactions are
treated alike, suggesting that weighting in expertise can improve the ranking
for informational tasks. The main aim of this paper is to investigate the
impact of high levels of technical domain expertise on both search behavior and
task outcome. We conduct an online user study with searchers proficient in
programming languages. We focus on Java and Javascript, yet we believe that our
study and results are applicable for other expertise-sensitive search tasks.
The main findings are three-fold: First, we constructed expertise tests that
effectively measure technical domain expertise and correlate well with the
self-reported expertise. Second, we showed that there is a clear position bias,
but technical domain experts were less affected by position bias. Third, we
found that general expertise helped finding the correct answers, but the domain
experts were more successful as they managed to detect better answers. Our work
is using explicit tests to determine user expertise levels, which is an
important step toward fully automatic detection of expertise levels based on
interaction behavior. A deeper understanding of the impact of expertise on
search behavior and task outcome can enable more effective use of expert
behavior in search logs - essentially make everyone search as an expert."
"Summarization is a way to represent same information in concise way with
equal sense. This can be categorized in two type Abstractive and Extractive
type. Our work is focused around Extractive summarization. A generic approach
to extractive summarization is to consider sentence as an entity, score each
sentence based on some indicative features to ascertain the quality of sentence
for inclusion in summary. Sort the sentences on the score and consider top n
sentences for summarization. Mostly statistical features have been used for
scoring the sentences. We are proposing a hybrid model for a single text
document summarization. This hybrid model is an extraction based approach,
which is combination of Statistical and semantic technique. The hybrid model
depends on the linear combination of statistical measures : sentence position,
TF-IDF, Aggregate similarity, centroid, and semantic measure. Our idea to
include sentiment analysis for salient sentence extraction is derived from the
concept that emotion plays an important role in communication to effectively
convey any message hence, it can play a vital role in text document
summarization. For comparison we have generated five system summaries Proposed
Work, MEAD system, Microsoft system, OPINOSIS system, and Human generated
summary, and evaluation is done using ROUGE score."
"We present a dynamic web tool that allows interactive search and
visualization of large news archives using an entity-centric approach. Users
are able to search entities using keyword phrases expressing news stories or
events and the system retrieves the most relevant entities to the user query
based on automatically extracted and indexed entity profiles. From the
computational journalism perspective, TimeMachine allows users to explore media
content through time using automatic identification of entity names, jobs,
quotations and relations between entities from co-occurrences networks
extracted from the news articles. TimeMachine demo is available at
http://maquinadotempo.sapo.pt/"
"Retrieval systems for scholarly literature offer the ability for the
scientific community to search, explore and download scholarly articles across
various scientific disciplines. Mostly used by the experts in the particular
field, these systems contain user community logs including information on user
specific downloaded articles. In this paper we present a novel approach for
automatically evaluating document similarity models in large collections of
scholarly publications. Unlike typical evaluation settings that use test
collections consisting of query documents and human annotated relevance
judgments, we use download logs to automatically generate pseudo-relevant set
of similar document pairs. More specifically we show that consecutively
downloaded document pairs, extracted from a scholarly information retrieval
(IR) system, could be utilized as a test collection for evaluating document
similarity models. Another novel aspect of our approach lies in the method that
we employ for evaluating the performance of the model by comparing the
distribution of consecutively downloaded document pairs and random document
pairs in log space. Across two families of similarity models, that represent
documents in the term vector and topic spaces, we show that our evaluation
approach achieves very high correlation with traditional performance metrics
such as Mean Average Precision (MAP), while being more efficient to compute."
"In this thesis, we look at the problem of assigning each identifier of a
document to a namespace. At the moment, there does not exist a special dataset
where all identifiers are grouped to namespaces, and therefore we need to
create such a dataset ourselves.
  To do that, we need to find groups of documents that use identifiers in the
same way. This can be done with cluster analysis methods. We argue that
documents can be represented by the identifiers they contain, and this approach
is similar to representing textual information in the Vector Space Model.
Because of this, we can apply traditional document clustering techniques for
namespace discovery.
  Because the problem is new, there is no gold standard dataset, and it is hard
to evaluate the performance of our method. To overcome it, we first use Java
source code as a dataset for our experiments, since it contains the namespace
information. We verify that our method can partially recover namespaces from
source code using only information about identifiers.
  The algorithms are evaluated on the English Wikipedia, and the proposed
method can extract namespaces on a variety of topics. After extraction, the
namespaces are organized into a hierarchical structure by using existing
classification schemes such as MSC, PACS and ACM. We also apply it to the
Russian Wikipedia, and the results are consistent across the languages.
  To our knowledge, the problem of introducing namespaces to mathematics has
not been studied before, and prior to our work there has been no dataset where
identifiers are grouped into namespaces. Thus, our result is not only a good
start, but also a good indicator that automatic namespace discovery is
possible."
"Question Answering (QA) systems are becoming the inspiring model for the
future of search engines. While recently, underlying datasets for QA systems
have been promoted from unstructured datasets to structured datasets with
highly semantic-enriched metadata, but still question answering systems involve
serious challenges which cause to be far beyond desired expectations. In this
paper, we raise the challenges for building a Question Answering (QA) system
especially with the focus of employing structured data (i.e. knowledge graph).
This paper provide an exhaustive insight of the known challenges, so far. Thus,
it helps researchers to easily spot open rooms for the future research agenda."
"Theoretical frameworks like the Probability Ranking Principle and its more
recent Interactive Information Retrieval variant have guided the development of
ranking and retrieval algorithms for decades, yet they are not capable of
helping us model problems in Dynamic Information Retrieval which exhibit the
following three properties; an observable user signal, retrieval over multiple
stages and an overall search intent. In this paper a new theoretical framework
for retrieval in these scenarios is proposed. We derive a general dynamic
utility function for optimizing over these types of tasks, that takes into
account the utility of each stage and the probability of observing user
feedback. We apply our framework to experiments over TREC data in the dynamic
multi page search scenario as a practical demonstration of its effectiveness
and to frame the discussion of its use, its limitations and to compare it
against the existing frameworks."
"Key to any research involving session search is the understanding of how a
user's queries evolve throughout the session. When a user creates a query
reformulation, he or she is consciously retaining terms from their original
query, removing others and adding new terms. By measuring the similarity
between queries we can make inferences on the user's information need and how
successful their new query is likely to be. By identifying the origins of added
terms we can infer the user's motivations and gain an understanding of their
interactions.
  In this paper we present a novel term-based methodology for understanding and
interpreting query reformulation actions. We use TREC Session Track data to
demonstrate how our technique is able to learn from query logs and we make use
of click data to test user interaction behavior when reformulating queries. We
identify and evaluate a range of term-based query reformulation strategies and
show that our methods provide valuable insight into understanding query
reformulation in session search."
"In this paper, we present a theoretical framework for tackling the cold-start
collaborative filtering problem, where unknown targets (items or users) keep
coming to the system, and there is a limited number of resources (users or
items) that can be allocated and related to them. The solution requires a
trade-off between exploitation and exploration as with the limited
recommendation opportunities, we need to, on one hand, allocate the most
relevant resources right away, but, on the other hand, it is also necessary to
allocate resources that are useful for learning the target's properties in
order to recommend more relevant ones in the future. In this paper, we study a
simple two-stage recommendation combining a sequential and a batch solution
together. We first model the problem with the partially observable Markov
decision process (POMDP) and provide an exact solution. Then, through an
in-depth analysis over the POMDP value iteration solution, we identify that an
exact solution can be abstracted as selecting resources that are not only
highly relevant to the target according to the initial-stage information, but
also highly correlated, either positively or negatively, with other potential
resources for the next stage. With this finding, we propose an approximate
solution to ease the intractability of the exact solution. Our initial results
on synthetic data and the Movie Lens 100K dataset confirm the performance gains
of our theoretical development and analysis."
"We share the implementation details and testing results for video retrieval
system based exclusively on features extracted by convolutional neural
networks. We show that deep learned features might serve as universal signature
for semantic content of video useful in many search and retrieval tasks. We
further show that graph-based storage structure for video index allows to
efficiently retrieving the content with complicated spatial and temporal search
queries."
"A fundamental goal of search engines is to identify, given a query, documents
that have relevant text. This is intrinsically difficult because the query and
the document may use different vocabulary, or the document may contain query
words without being relevant. We investigate neural word embeddings as a source
of evidence in document ranking. We train a word2vec embedding model on a large
unlabelled query corpus, but in contrast to how the model is commonly used, we
retain both the input and the output projections, allowing us to leverage both
the embedding spaces to derive richer distributional relationships. During
ranking we map the query words into the input space and the document words into
the output space, and compute a query-document relevance score by aggregating
the cosine similarities across all the query-document word pairs.
  We postulate that the proposed Dual Embedding Space Model (DESM) captures
evidence on whether a document is about a query term in addition to what is
modelled by traditional term-frequency based approaches. Our experiments show
that the DESM can re-rank top documents returned by a commercial Web search
engine, like Bing, better than a term-matching based signal like TF-IDF.
However, when ranking a larger set of candidate documents, we find the
embeddings-based approach is prone to false positives, retrieving documents
that are only loosely related to the query. We demonstrate that this problem
can be solved effectively by ranking based on a linear mixture of the DESM and
the word counting features."
"Query-expansion via pseudo-relevance feedback is a popular method of
overcoming the problem of vocabulary mismatch and of increasing average
retrieval effectiveness. In this paper, we develop a new method that estimates
a query topic model from a set of pseudo-relevant documents using a new
language modelling framework.
  We assume that documents are generated via a mixture of multivariate Polya
distributions, and we show that by identifying the topical terms in each
document, we can appropriately select terms that are likely to belong to the
query topic model. The results of experiments on several TREC collections show
that the new approach compares favourably to current state-of-the-art expansion
methods."
"Name disambiguation and the subsequent name conflation are essential for the
correct processing of person name queries in a digital library or other
database. It distinguishes each unique person from all other records in the
database. We study inventor name disambiguation for a patent database using
methods and features from earlier work on author name disambiguation and
propose a feature set appropriate for a patent database. A random forest was
selected for the pairwise linking classifier since they outperform Naive Bayes,
Logistic Regression, Support Vector Machines (SVM), Conditional Inference Tree,
and Decision Trees. Blocking size, very important for scaling, was selected
based on experiments that determined feature importance and accuracy. The
DBSCAN algorithm is used for clustering records, using a distance function
derived from random forest classifier. For additional scalability clustering
was parallelized. Tests on the USPTO patent database show that our method
successfully disambiguated 12 million inventor mentions within 6.5 hours.
Evaluation on datasets from USPTO PatentsView inventor name disambiguation
competition shows our algorithm outperforms all algorithms in the competition."
"Due to its low storage cost and fast query speed, cross-modal hashing (CMH)
has been widely used for similarity search in multimedia retrieval
applications. However, almost all existing CMH methods are based on
hand-crafted features which might not be optimally compatible with the
hash-code learning procedure. As a result, existing CMH methods with
handcrafted features may not achieve satisfactory performance. In this paper,
we propose a novel cross-modal hashing method, called deep crossmodal hashing
(DCMH), by integrating feature learning and hash-code learning into the same
framework. DCMH is an end-to-end learning framework with deep neural networks,
one for each modality, to perform feature learning from scratch. Experiments on
two real datasets with text-image modalities show that DCMH can outperform
other baselines to achieve the state-of-the-art performance in cross-modal
retrieval applications."
"In this paper, we introduce the Wikipedia Tools for Google Spreadsheets.
Google Spreadsheets is part of a free, Web-based software office suite offered
by Google within its Google Docs service. It allows users to create and edit
spreadsheets online, while collaborating with other users in realtime.
Wikipedia is a free-access, free-content Internet encyclopedia, whose content
and data is available, among other means, through an API. With the Wikipedia
Tools for Google Spreadsheets, we have created a toolkit that facilitates
working with Wikipedia data from within a spreadsheet context. We make these
tools available as open-source on GitHub
[https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets], released
under the permissive Apache 2.0 license."
"In Twitter, and other microblogging services, the generation of new content
by the crowd is often biased towards immediacy: what is happening now. Prompted
by the propagation of commentary and information through multiple mediums,
users on the Web interact with and produce new posts about newsworthy topics
and give rise to trending topics. This paper proposes to leverage on the
behavioral dynamics of users to estimate the most relevant time periods for a
topic. Our hypothesis stems from the fact that when a real-world event occurs
it usually has peak times on the Web: a higher volume of tweets, new visits and
edits to related Wikipedia articles, and news published about the event. In
this paper, we propose a novel time-aware ranking model that leverages on
multiple sources of crowd signals. Our approach builds on two major novelties.
First, a unifying approach that given query q, mines and represents temporal
evidence from multiple sources of crowd signals. This allows us to predict the
temporal relevance of documents for query q. Second, a principled retrieval
model that integrates temporal signals in a learning to rank framework, to rank
results according to the predicted temporal relevance. Evaluation on the TREC
2013 and 2014 Microblog track datasets demonstrates that the proposed model
achieves a relative improvement of 13.2% over lexical retrieval models and 6.2%
over a learning to rank baseline."
"Re-finding files from a personal computer is a frequent demand to users. When
encountered a difficult re-finding task, people may not recall the attributes
used by conventional re-finding methods, such as a file's path, file name,
keywords etc., the re-finding would fail.
  We proposed a method to support difficult re-finding tasks. By asking the
user a list of questions about the target, such as a document's pages, author
numbers, accumulated reading time, last reading location etc. Then use the
user's answers to filter out the target.
  After the user answered a list of questions about the target file, we
evaluate the user's familiar degree about the target file based on the answers.
We devise a ranking algorithm which sorts the candidates by comparing the
user's familiarity degree about the target and the candidates.
  We also propose a method to generate re-finding tasks artificially based on
the user's own document corpus."
"This paper tries to throw light in the usage of data structures in the field
of information retrieval. Information retrieval is an area of study which is
gaining momentum as the need and urge for sharing and exploring information is
growing day by day. Data structures have been the area of research for a long
period in the arena of computer science. The need to have efficient data
structures has become even more important as the data grows in an exponential
nature."
"In order to find experts, different approaches build rankings of people,
assuming that they are ranked by level of expertise, and use typical
Information Retrieval (IR) measures to evaluate their effectiveness. However,
we figured out that expert rankings (i) tend to be partially ordered, (ii)
incomplete, and (iii) consequently provide more an order rather than absolute
ranks, which is not what usual IR measures exploit. To improve this state of
the art, we propose to revise the formalism used in IR to design proper
measures for comparing expert rankings. In this report, we investigate a first
step by providing mitigation procedures for the three issues, and we analyse IR
measures with the help of these procedures to identify interesting revisions
and remaining limitations. From this analysis, we see that most of the measures
can be exploited for this more generic context because of our mitigation
procedures. Moreover, measures based on precision and recall, usually unable to
consider the order of the ranked items, are of first interest if we represent a
ranking as a set of ordered pairs. Cumulative measures, on the other hand, are
specifically designed for considering the order but suffer from a higher
complexity, motivating the use of precision/recall measures with the right
representation."
"Matrix factorization has been recently utilized for the task of multi-modal
hashing for cross-modality visual search, where basis functions are learned to
map data from different modalities to the same Hamming embedding. In this
paper, we propose a novel cross-modality hashing algorithm termed Supervised
Matrix Factorization Hashing (SMFH) which tackles the multi-modal hashing
problem with a collective non-matrix factorization across the different
modalities. In particular, SMFH employs a well-designed binary code learning
algorithm to preserve the similarities among multi-modal original features
through a graph regularization. At the same time, semantic labels, when
available, are incorporated into the learning procedure. We conjecture that all
these would facilitate to preserve the most relevant information during the
binary quantization process, and hence improve the retrieval accuracy. We
demonstrate the superior performance of SMFH on three cross-modality visual
search benchmarks, i.e., the PASCAL-Sentence, Wiki, and NUS-WIDE, with
quantitative comparison to various state-of-the-art methods"
"A content-based image retrieval system based on multinomial relevance
feedback is proposed. The system relies on an interactive search paradigm where
at each round a user is presented with k images and selects the one closest to
their ideal target. Two approaches, one based on the Dirichlet distribution and
one based the Beta distribution, are used to model the problem motivating an
algorithm that trades exploration and exploitation in presenting the images in
each round. Experimental results show that the new approach compares favourably
with previous work."
"Air travel is one of the most frequently used means of transportation in our
every-day life. Thus, it is not surprising that an increasing number of
travelers share their experiences with airlines and airports in form of online
reviews on the Web. In this work, we thrive to explain and uncover the features
of airline reviews that contribute most to traveler satisfaction. To that end,
we examine reviews crawled from the Skytrax air travel review portal. Skytrax
provides four review categories to review airports, lounges, airlines and
seats. Each review category consists of several five-star ratings as well as
free-text review content. In this paper, we conducted a comprehensive feature
study and we find that not only five-star rating information such as airport
queuing time and lounge comfort highly correlate with traveler satisfaction but
also textual features in the form of the inferred review text sentiment. Based
on our findings, we created classifiers to predict traveler satisfaction using
the best performing rating features. Our results reveal that given our
methodology, traveler satisfaction can be predicted with high accuracy.
Additionally, we find that training a model on the sentiment of the review text
provides a competitive alternative when no five star rating information is
available. We believe that our work is of interest for researchers in the area
of modeling and predicting user satisfaction based on available review data on
the Web."
"Finding relevant publications is important for scientists who have to cope
with exponentially increasing numbers of scholarly material. Algorithms can
help with this task as they help for music, movie, and product recommendations.
However, we know little about the performance of these algorithms with
scholarly material. Here, we develop an algorithm, and an accompanying Python
library, that implements a recommendation system based on the content of
articles. Design principles are to adapt to new content, provide near-real time
suggestions, and be open source. We tested the library on 15K posters from the
Society of Neuroscience Conference 2015. Human curated topics are used to cross
validate parameters in the algorithm and produce a similarity metric that
maximally correlates with human judgments. We show that our algorithm
significantly outperformed suggestions based on keywords. The work presented
here promises to make the exploration of scholarly material faster and more
accurate."
"We consider the problem of generating interpretable recommendations by
identifying overlapping co-clusters of clients and products, based only on
positive or implicit feedback. Our approach is applicable on very large
datasets because it exhibits almost linear complexity in the input examples and
the number of co-clusters. We show, both on real industrial data and on
publicly available datasets, that the recommendation accuracy of our algorithm
is competitive to that of state-of-art matrix factorization techniques. In
addition, our technique has the advantage of offering recommendations that are
textually and visually interpretable. Finally, we examine how to implement our
technique efficiently on Graphical Processing Units (GPUs)."
"Supervised term weighting could improve the performance of text
categorization. A way proven to be effective is to give more weight to terms
with more imbalanced distributions across categories. This paper shows that
supervised term weighting should not just assign large weights to imbalanced
terms, but should also control the trade-off between over-weighting and
under-weighting. Over-weighting, a new concept proposed in this paper, is
caused by the improper handling of singular terms and too large ratios between
term weights. To prevent over-weighting, we present three regularization
techniques: add-one smoothing, sublinear scaling and bias term. Add-one
smoothing is used to handle singular terms. Sublinear scaling and bias term
shrink the ratios between term weights. However, if sublinear functions scale
down term weights too much, or the bias term is too large, under-weighting
would occur and harm the performance. It is therefore critical to balance
between over-weighting and under-weighting. Inspired by this insight, we also
propose a new supervised term weighting scheme, regularized entropy (re). Our
re employs entropy to measure term distribution, and introduces the bias term
to control over-weighting and under-weighting. Empirical evaluations on topical
and sentiment classification datasets indicate that sublinear scaling and bias
term greatly influence the performance of supervised term weighting, and our re
enjoys the best results in comparison with existing schemes."
"The Count-Min Sketch is a widely adopted structure for approximate event
counting in large scale processing. In a previous work we improved the original
version of the Count-Min-Sketch (CMS) with conservative update using
approximate counters instead of linear counters. These structures are
computationaly efficient and improve the average relative error (ARE) of a CMS
at constant memory footprint. These improvements are well suited for NLP tasks,
in which one is interested by the low-frequency items. However, if Log counters
allow to improve ARE, they produce a residual error due to the approximation.
In this paper, we propose the Count-Min Tree Sketch (Copyright 2016 eXenSa. All
rights reserved) variant with pyramidal counters, which are focused toward
taking advantage of the Zipfian distribution of text data."
"Document retrieval has been an important research problem over many years in
the information retrieval community. State-of-the-art techniques utilize
various methods in matching documents to a given document including keywords,
phrases, and annotations. In this paper, we propose a new approach for document
retrieval that utilizes predications (subject-predicate-object triples)
extracted from the documents. We represent documents as sets of predications.
We measure the similarity between predications to compute the similarity
between documents. Our approach utilizes the hierarchical information available
in ontologies in computing concept-concept similarity, making the approach
flexible. Predication-based document similarity is more precise and forms the
basis for a semantically aware document retrieval system. We show that the
approach is competitive with an existing state-of-the-art related document
retrieval technique in the biomedical domain."
"The relevance between a query and a document in search can be represented as
matching degree between the two objects. Latent space models have been proven
to be effective for the task, which are often trained with click-through data.
One technical challenge with the approach is that it is hard to train a model
for tail queries and tail documents for which there are not enough clicks. In
this paper, we propose to address the challenge by learning a latent matching
model, using not only click-through data but also semantic knowledge. The
semantic knowledge can be categories of queries and documents as well as
synonyms of words, manually or automatically created. Specifically, we
incorporate semantic knowledge into the objective function by including
regularization terms. We develop two methods to solve the learning task on the
basis of coordinate descent and gradient descent respectively, which can be
employed in different settings. Experimental results on two datasets from an
app search engine demonstrate that our model can make effective use of semantic
knowledge, and thus can significantly enhance the accuracies of latent matching
models, particularly for tail queries."
"With the incredibly growing amount of multimedia data shared on the social
media platforms, recommender systems have become an important necessity to ease
users' burden on the information overload. In such a scenario, extensive amount
of heterogeneous information such as tags, image content, in addition to the
user-to-item preferences, is extremely valuable for making effective
recommendations. In this paper, we explore a novel hybrid algorithm termed {\em
STM}, for image recommendation. STM jointly considers the problem of image
content analysis with the users' preferences on the basis of sparse
representation. STM is able to tackle the challenges of highly sparse user
feedbacks and cold-start problmes in the social network scenario. In addition,
our model is based on the classical probabilistic matrix factorization and can
be easily extended to incorporate other useful information such as the social
relationships. We evaluate our approach with a newly collected 0.3 million
social image data set from Flickr. The experimental results demonstrate that
sparse topic modeling of the image content leads to more effective
recommendations, , with a significant performance gain over the
state-of-the-art alternatives."
"Recommender systems usually face the problem of serving the same
recommendations across multiple sessions regardless of whether the user is
interested in them or not, thereby reducing their effectiveness. To add
freshness to the recommended products, we introduce a feedback loop where the
set of recommended products in the current session depend on the user's
interaction with the previously recommended sets. We also describe ways of
addressing freshness when there is little or even no direct user interaction.
We define a metric to quantify freshness by reducing the problem to measuring
temporal diversity."
"Most research into similarity search in metric spaces relies upon the
triangle inequality property. This property allows the space to be arranged
according to relative distances to avoid searching some subspaces. We show that
many common metric spaces, notably including those using Euclidean and
Jensen-Shannon distances, also have a stronger property, sometimes called the
four-point property: in essence, these spaces allow an isometric embedding of
any four points in three-dimensional Euclidean space, as well as any three
points in two-dimensional Euclidean space. In fact, we show that any space
which is isometrically embeddable in Hilbert space has the stronger property.
This property gives stronger geometric guarantees, and one in particular, which
we name the Hilbert Exclusion property, allows any indexing mechanism which
uses hyperplane partitioning to perform better. One outcome of this observation
is that a number of state-of-the-art indexing mechanisms over high dimensional
spaces can be easily extended to give a significant increase in performance;
furthermore, the improvement given is greater in higher dimensions. This
therefore leads to a significant improvement in the cost of metric search in
these spaces."
"General purpose Search Engines (SEs) crawl all domains (e.g., Sports, News,
Entertainment) of the Web, but sometimes the informational need of a query is
restricted to a particular domain (e.g., Medical). We leverage the work of SEs
as part of our effort to route domain specific queries to local Digital
Libraries (DLs). SEs are often used even if they are not the ""best"" source for
certain types of queries. Rather than tell users to ""use this DL for this kind
of query"", we intend to automatically detect when a query could be better
served by a local DL (such as a private, access-controlled DL that is not
crawlable via SEs). This is not an easy task because Web queries are short,
ambiguous, and there is lack of quality labeled training data (or it is
expensive to create). To detect queries that should be routed to local,
specialized DLs, we first send the queries to Google and then examine the
features in the resulting Search Engine Result Pages (SERPs), and then classify
the query as belonging to either the scholar or non-scholar domain. Using
400,000 AOL queries for the non-scholar domain and 400,000 queries from the
NASA Technical Report Server (NTRS) for the scholar domain, our classifier
achieved a precision of 0.809 and F-measure of 0.805."
"Advances in smartphone technology have promoted the rapid development of
mobile apps. However, the availability of a huge number of mobile apps in
application stores has imposed the challenge of finding the right apps to meet
the user needs. Indeed, there is a critical demand for personalized app
recommendations. Along this line, there are opportunities and challenges posed
by two unique characteristics of mobile apps. First, app markets have organized
apps in a hierarchical taxonomy. Second, apps with similar functionalities are
competing with each other. While there are a variety of approaches for mobile
app recommendations, these approaches do not have a focus on dealing with these
opportunities and challenges. To this end, in this paper, we provide a
systematic study for addressing these challenges. Specifically, we develop a
Structural User Choice Model (SUCM) to learn fine-grained user preferences by
exploiting the hierarchical taxonomy of apps as well as the competitive
relationships among apps. Moreover, we design an efficient learning algorithm
to estimate the parameters for the SUCM model. Finally, we perform extensive
experiments on a large app adoption data set collected from Google Play. The
results show that SUCM consistently outperforms state-of-the-art top-N
recommendation methods by a significant margin."
"Most of the fastest-growing string collections today are repetitive, that is,
most of the constituent documents are similar to many others. As these
collections keep growing, a key approach to handling them is to exploit their
repetitiveness, which can reduce their space usage by orders of magnitude. We
study the problem of indexing repetitive string collections in order to perform
efficient document retrieval operations on them. Document retrieval problems
are routinely solved by search engines on large natural language collections,
but the techniques are less developed on generic string collections. The case
of repetitive string collections is even less understood, and there are very
few existing solutions. We develop two novel ideas, {\em interleaved LCPs} and
{\em precomputed document lists}, that yield highly compressed indexes solving
the problem of document listing (find all the documents where a string
appears), top-$k$ document retrieval (find the $k$ documents where a string
appears most often), and document counting (count the number of documents where
a string appears). We also show that a classical data structure supporting the
latter query becomes highly compressible on repetitive data. Finally, we show
how the tools we developed can be combined to solve ranked conjunctive and
disjunctive multi-term queries under the simple tf-idf model of relevance. We
thoroughly evaluate the resulting techniques in various real-life
repetitiveness scenarios, and recommend the best choices for each case."
"Low-dimensional word vectors have long been used in a wide range of
applications in natural language processing. In this paper we shed light on
estimating query vectors in ad-hoc retrieval where a limited information is
available in the original query. Pseudo-relevance feedback (PRF) is a
well-known technique for updating query language models and expanding the
queries with a number of relevant terms. We formulate the query updating in
low-dimensional spaces first with rotating the query vector and then with
scaling. These consequential steps are embedded in a query-specific projection
matrix capturing both angle and scaling. In this paper we propose a new but not
the most effective technique necessarily for PRF in language modeling, based on
the query projection algorithm. We learn an embedded coefficient matrix for
each query, whose aim is to improve the vector representation of the query by
transforming it to a more reliable space, and then update the query language
model. The proposed embedded coefficient divergence minimization model (ECDMM)
takes top-ranked documents retrieved by the query and obtains a couple of
positive and negative sample sets; these samples are used for learning the
coefficient matrix which will be used for projecting the query vector and
updating the query language model using a softmax function. Experimental
results on several TREC and CLEF data sets in several languages demonstrate
effectiveness of ECDMM. The experimental results reveal that the new
formulation for the query works as well as state-of-the-art PRF techniques and
outperforms state-of-the-art PRF techniques in a TREC collection in terms of
MAP,P@5, and P@10 significantly."
"We examine the effects of different latency penalties in the evaluation of
push notification systems, as operationalized in the TREC 2015 Microblog track
evaluation. The purpose of this study is to inform the design of metrics for
the TREC 2016 Real-Time Summarization track, which is largely modeled after the
TREC 2015 evaluation design."
"Most Information Retrieval models compute the relevance score of a document
for a given query by summing term weights specific to a document or a query.
Heuristic approaches, like TF-IDF, or probabilistic models, like BM25, are used
to specify how a term weight is computed. In this paper, we propose to leverage
learning-to-rank principles to learn how to compute a term weight for a given
document based on the term occurrence pattern."
"Deep neural networks have been successfully applied to many text matching
tasks, such as paraphrase identification, question answering, and machine
translation. Although ad-hoc retrieval can also be formalized as a text
matching task, few deep models have been tested on it. In this paper, we study
a state-of-the-art deep matching model, namely MatchPyramid, on the ad-hoc
retrieval task. The MatchPyramid model employs a convolutional neural network
over the interactions between query and document to produce the matching score.
We conducted extensive experiments to study the impact of different pooling
sizes, interaction functions and kernel sizes on the retrieval performance.
Finally, we show that the MatchPyramid models can significantly outperform
several recently introduced deep matching models on the retrieval task, but
still cannot compete with the traditional retrieval models, such as BM25 and
language models."
"Alice' is submitting one web search per five minutes, for three hours in a
row - is it normal? How to detect abnormal search behaviors, among Alice and
other users? Is there any distinct pattern in Alice's (or other users') search
behavior? We studied what is probably the largest, publicly available, query
log that contains more than 30 million queries from 0.6 million users. In this
paper, we present a novel, user-and group-level framework, M3A: Model,
MetaModel and Anomaly detection. For each user, we discover and explain a
surprising, bi-modal pattern of the inter-arrival time (IAT) of landed queries
(queries with user click-through). Specifically, the model Camel-Log is
proposed to describe such an IAT distribution; we then notice the correlations
among its parameters at the group level. Thus, we further propose the metamodel
Meta-Click, to capture and explain the two-dimensional, heavy-tail distribution
of the parameters. Combining Camel-Log and Meta-Click, the proposed M3A has the
following strong points: (1) the accurate modeling of marginal IAT
distribution, (2) quantitative interpretations, and (3) anomaly detection."
"Users of web search engines are known to mostly focus on the top ranked
results of the search engine result page. While many studies support this well
known information seeking pattern only few studies concentrate on the question
what users are missing by neglecting lower ranked results. To learn more about
the relevance distributions in the so-called long tail we conducted a relevance
assessment study with the Million Short long-tail web search engine. While we
see a clear difference in the content between the head and the tail of the
search engine result list we see no statistical significant differences in the
binary relevance judgments and weak significant differences when using graded
relevance. The tail contains different but still valuable results. We argue
that the long tail can be a rich source for the diversification of web search
engine result lists but it needs more evaluation to clearly describe the
differences."
"Mobile search has recently been shown to be the major contributor to the
growing search market. The key difference between mobile search and desktop
search is that information presentation is limited to the screen space of the
mobile device. Thus, major search engines have adopted a new type of search
result presentation, known as \textit{information cards}, in which each card
presents summarized results from one domain/vertical, for a given query, to
augment the standard blue-links search results. While it has been widely
acknowledged that information cards are particularly suited to mobile user
experience, it is also challenging to optimize such result sets. Typically,
user engagement metrics like query reformulation are based on whole ranked list
of cards for each query and most traditional learning to rank algorithms
require per-item relevance labels. In this paper, we investigate the
possibility of interpreting query reformulation into effective relevance labels
for query-card pairs. We inherit the concept of conventional learning-to-rank,
and propose pointwise, pairwise and listwise interpretations for query
reformulation. In addition, we propose a learning-to-label strategy that learns
the contribution of each card, with respect to a query, where such
contributions can be used as labels for training card ranking models. We
utilize a state-of-the-art ranking model and demonstrate the effectiveness of
proposed mechanisms on a large-scale mobile data from a major search engine,
showing that models trained from labels derived from user engagement can
significantly outperform ones trained from human judgment labels."
"When two terms occur together in a document, the probability of a close
relationship between them and the document itself is greater if they are in
nearby positions. However, ranking functions including term proximity (TP)
require larger indexes than traditional document-level indexing, which slows
down query processing. Previous studies also show that this technique is not
effective for all types of queries. Here we propose a document ranking model
which decides for which queries it would be beneficial to use a proximity-based
ranking, based on a collection of features of the query. We use a machine
learning approach in determining whether utilizing TP will be beneficial.
Experiments show that the proposed model returns improved rankings while also
reducing the overhead incurred as a result of using TP statistics."
"Social media platforms are a rich source of information these days, however,
of all the available information, only a small fraction is of users' interest.
To help users catch up with the latest topics of their interests from the large
amount of information available in social media, we present a relevant content
filtering based framework for data stream summarization. More specifically,
given the topic or event of interest, this framework can dynamically discover
and filter out relevant information from irrelevant information in the stream
of text provided by social media platforms. It then further captures the most
representative and up-to-date information to generate a sequential summary or
event story line along with the evolution of the topic or event. Our framework
does not depend on any labeled data, it instead uses the weak supervision
provided by the user, which matches the real scenarios of users searching for
information about an ongoing event. We experimented on two real events traced
by a Twitter dataset from TREC 2011. The results verified the effectiveness of
relevant content filtering and sequential summary generation of the proposed
framework. It also shows its robustness of using the most easy-to-obtain weak
supervision, i.e., trending topic or hashtag. Thus, this framework can be
easily integrated into social media platforms such as Twitter to generate
sequential summaries for the events of interest. We also make the manually
generated gold-standard sequential summaries of the two test events publicly
available for future use in the community."
"Folksonomy is a non-hierarchical document categorizing system, that treats
every category in a flat manner, dan every category is entered freely by anyone
who submitted a document in these categories. Categorization is done
automatically at the time a document is submitted, by entering the list of
categories that best fit the document. del.icio.us (http://del.icio.us) site is
one of the most popular social bookmarking sites that uses folksonomy.
  Usage of folksonomy, although very easy, also has its weaknesses, such as use
of different tags for the same concept, use of the same tag for different
concepts, no quality control, etc. We try to provide a solution for some of
these problems by analyzing Web documents' contents and categorizing them
automatically using multinomial naive Bayes algorithm.
  Bayes classifier works by using a set of evidences and a set of classes. By
training the system using sample data, we can determine the probability of an
evidence given a particular class. Bayes classifier also uses prior probability
of a class, which can be calculated from sample data. From these analysis, when
given a new document which is formed by a set of evidences (words), the
probabilities of each class given that document (posterior probabilities) can
be determined.
  This system is implemented using PHP 5, Apache, and MySQL. The conclusion
from building this system is that the Bayes method can be used to automatically
categorize documents and also as an assistive tool for manual categorization.
  -----
  Folksonomy merupakan metode kategorisasi dokumen yang tidak hierarkis,
menyamaratakan kedudukan setiap kategori, dan judul kategori ditentukan secara
bebas oleh siapa saja yang memasukkan sebuah dokumen di dalam kategori-kategori
tersebut."
"In this paper a framework for Automatic Query Expansion (AQE) is proposed
using distributed neural language model word2vec. Using semantic and contextual
relation in a distributed and unsupervised framework, word2vec learns a low
dimensional embedding for each vocabulary entry. Using such a framework, we
devise a query expansion technique, where related terms to a query are obtained
by K-nearest neighbor approach. We explore the performance of the AQE methods,
with and without feedback query expansion, and a variant of simple K-nearest
neighbor in the proposed framework. Experiments on standard TREC ad-hoc data
(Disk 4, 5 with query sets 301-450, 601-700) and web data (WT10G data with
query set 451-550) shows significant improvement over standard term-overlapping
based retrieval methods. However the proposed method fails to achieve
comparable performance with statistical co-occurrence based feedback method
such as RM3. We have also found that the word2vec based query expansion methods
perform similarly with and without any feedback information."
"What if Information Retrieval (IR) systems did not just retrieve relevant
information that is stored in their indices, but could also ""understand"" it and
synthesise it into a single document? We present a preliminary study that makes
a first step towards answering this question. Given a query, we train a
Recurrent Neural Network (RNN) on existing relevant information to that query.
We then use the RNN to ""deep learn"" a single, synthetic, and we assume,
relevant document for that query. We design a crowdsourcing experiment to
assess how relevant the ""deep learned"" document is, compared to existing
relevant documents. Users are shown a query and four wordclouds (of three
existing relevant documents and our deep learned synthetic document). The
synthetic document is ranked on average most relevant of all."
"This paper proposes implicit CF-NADE, a neural autoregressive model for
collaborative filtering tasks using implicit feedback ( e.g. click, watch,
browse behaviors). We first convert a users implicit feedback into a like
vector and a confidence vector, and then model the probability of the like
vector, weighted by the confidence vector. The training objective of implicit
CF-NADE is to maximize a weighted negative log-likelihood. We test the
performance of implicit CF-NADE on a dataset collected from a popular digital
TV streaming service. More specifically, in the experiments, we describe how to
convert watch counts into implicit relative rating, and feed into implicit
CF-NADE. Then we compare the performance of implicit CF-NADE model with the
popular implicit matrix factorization approach. Experimental results show that
implicit CF-NADE significantly outperforms the baseline."
"Venue recommendation aims to assist users by making personalised suggestions
of venues to visit, building upon data available from location-based social
networks (LBSNs) such as Foursquare. A particular challenge for this task is
context-aware venue recommendation (CAVR), which additionally takes the
surrounding context of the user (e.g. the user's location and the time of day)
into account in order to provide more relevant venue suggestions. To address
the challenges of CAVR, we describe two approaches that exploit word embedding
techniques to infer the vector-space representations of venues, users' existing
preferences, and users' contextual preferences. Our evaluation upon the test
collection of the TREC 2015 Contextual Suggestion track demonstrates that we
can significantly enhance the effectiveness of a state-of-the-art venue
recommendation approach, as well as produce context-aware recommendations that
are at least as effective as the top TREC 2015 systems."
"A major difficulty in applying word vector embeddings in IR is in devising an
effective and efficient strategy for obtaining representations of compound
units of text, such as whole documents, (in comparison to the atomic words),
for the purpose of indexing and scoring documents. Instead of striving for a
suitable method for obtaining a single vector representation of a large
document of text, we rather aim for developing a similarity metric that makes
use of the similarities between the individual embedded word vectors in a
document and a query. More specifically, we represent a document and a query as
sets of word vectors, and use a standard notion of similarity measure between
these sets, computed as a function of the similarities between each constituent
word pair from these sets. We then make use of this similarity measure in
combination with standard IR based similarities for document ranking. The
results of our initial experimental investigations shows that our proposed
method improves MAP by up to $5.77\%$, in comparison to standard text-based
language model similarity, on the TREC ad-hoc dataset."
"We tackle the blog recommendation problem in Tumblr for mobile users in this
paper. Blog recommendation is challenging since most mobile users would suffer
from the cold start when there are only a limited number of blogs followed by
the user. Specifically to address this problem in the mobile domain, we take
into account mobile apps, which typically provide rich information from the
users. Based on the assumption that the user interests can be reflected from
their app usage patterns, we propose to exploit the app usage data for
improving blog recommendation. Building on the state-of-the-art recommendation
framework, Factorization Machines (FM), we implement app-based FM that
integrates app usage data with the user-blog follow relations. In this approach
the blog recommendation is generated not only based on the blogs that the user
followed before, but also the apps that the user has often used. We demonstrate
in a series of experiments that app-based FM can outperform other alternative
approaches to a significant extent. Our experimental results also show that
exploiting app usage information is particularly effective for improving blog
recommendation quality for cold start users."
"Memory Based Collaborative Filtering is a widely used approach to provide
recommendations. It exploits similarities between ratings across a population
of users by forming a weighted vote to predict unobserved ratings. Bespoke
solutions are frequently adopted to deal with the problem of high quality
recommendations on large data sets. A disadvantage of this approach, however,
is the loss of generality and flexibility of the general collaborative
filtering systems. In this paper, we have developed a methodology that allows
one to build a scalable and effective collaborative filtering system on top of
a conventional full-text search engine such as Apache Lucene."
"Point-of-interest (POI) recommendation that suggests new places for users to
visit arises with the popularity of location-based social networks (LBSNs). Due
to the importance of POI recommendation in LBSNs, it has attracted much
academic and industrial interest. In this paper, we offer a systematic review
of this field, summarizing the contributions of individual efforts and
exploring their relations. We discuss the new properties and challenges in POI
recommendation, compared with traditional recommendation problems, e.g., movie
recommendation. Then, we present a comprehensive review in three aspects:
influential factors for POI recommendation, methodologies employed for POI
recommendation, and different tasks in POI recommendation. Specifically, we
propose three taxonomies to classify POI recommendation systems. First, we
categorize the systems by the influential factors check-in characteristics,
including the geographical information, social relationship, temporal
influence, and content indications. Second, we categorize the systems by the
methodology, including systems modeled by fused methods and joint methods.
Third, we categorize the systems as general POI recommendation and successive
POI recommendation by subtle differences in the recommendation task whether to
be bias to the recent check-in. For each category, we summarize the
contributions and system features, and highlight the representative work.
Moreover, we discuss the available data sets and the popular metrics. Finally,
we point out the possible future directions in this area and conclude this
survey."
"Relevance Models are well-known retrieval models and capable of producing
competitive results. However, because they use query expansion they can be very
slow. We address this slowness by incorporating two variants of locality
sensitive hashing (LSH) into the query expansion process. Results on two
document collections suggest that we can obtain large reductions in the amount
of work, with a small reduction in effectiveness. Our approach is shown to be
additive when pruning query terms."
"Recommender systems are mostly well known for their applications in
e-commerce sites and are mostly static models. Classical personalized
recommender algorithm includes item-based collaborative filtering method
applied in Amazon, matrix factorization based collaborative filtering algorithm
from Netflix, etc. In this article, we hope to combine traditional model with
behavior pattern extraction method. We use desensitized mobile transaction
record provided by T-mall, Alibaba to build a hybrid dynamic recommender
system. The sequential pattern mining aims to find frequent sequential pattern
in sequence database and is applied in this hybrid model to predict customers'
payment behavior thus contributing to the accuracy of the model."
"In this paper, we reflect on ways to improve the quality of bio-medical
information retrieval by drawing implicit negative feedback from negated
information in noisy natural language search queries. We begin by studying the
extent to which negations occur in clinical texts and quantify their
detrimental effect on retrieval performance. Subsequently, we present a number
of query reformulation and ranking approaches that remedy these shortcomings by
resolving natural language negations. Our experimental results are based on
data collected in the course of the TREC Clinical Decision Support Track and
show consistent improvements compared to state-of-the-art methods. Using our
novel algorithms, we are able to reduce the negative impact of negations on
early precision by up to 65%."
"In the last decade new ways of shopping online have increased the possibility
of buying products and services more easily and faster than ever. In this new
context, personality is a key determinant in the decision making of the
consumer when shopping. The two main reasons are: firstly, a person's buying
choices are influenced by psychological factors like impulsiveness, and
secondly, some consumers may be more susceptible to making impulse purchases
than others. To the best of our knowledge, the impact of personality factors on
advertisements has been largely neglected at the level of recommender systems.
This work proposes a highly innovative research which uses a personality
perspective to determine the unique associations among the consumer's buying
tendency and advert recommendations. As a matter of fact, the lack of a
publicly available benchmark for computational advertising do not allow both
the exploration of this intriguing research direction and the evaluation of
state-of-the-art algorithms. We present the ADS Dataset, a publicly available
benchmark for computational advertising enriched with Big-Five users'
personality factors and 1,200 personal users' pictures. The proposed benchmark
allows two main tasks: rating prediction over 300 real advertisements (i.e.,
Rich Media Ads, Image Ads, Text Ads) and click-through rate prediction.
Moreover, this work carries out experiments, reviews various evaluation
criteria used in the literature, and provides a library for each one of them
within one integrated toolbox."
"The name entity disambiguation task aims to partition the records of multiple
real-life persons so that each partition contains records pertaining to a
unique person. Most of the existing solutions for this task operate in a batch
mode, where all records to be disambiguated are initially available to the
algorithm. However, more realistic settings require that the name
disambiguation task be performed in an online fashion, in addition to, being
able to identify records of new ambiguous entities having no preexisting
records. In this work, we propose a Bayesian non-exhaustive classification
framework for solving online name disambiguation task. Our proposed method uses
a Dirichlet process prior with a Normal * Normal * Inverse Wishart data model
which enables identification of new ambiguous entities who have no records in
the training data. For online classification, we use one sweep Gibbs sampler
which is very efficient and effective. As a case study we consider
bibliographic data in a temporal stream format and disambiguate authors by
partitioning their papers into homogeneous groups. Our experimental results
demonstrate that the proposed method is better than existing methods for
performing online name disambiguation task."
"Micro-blogging services can track users' geo-locations when users check-in
their places or use geo-tagging which implicitly reveals locations. This ""geo
tracking"" can help to find topics triggered by some events in certain regions.
However, discovering such topics is very challenging because of the large
amount of noisy messages (e.g. daily conversations). This paper proposes a
method to model geographical topics, which can filter out irrelevant words by
different weights in the local and global contexts. Our method is based on the
Latent Dirichlet Allocation (LDA) model but each word is generated from either
a local or a global topic distribution by its generation probabilities. We
evaluated our model with data collected from Weibo, which is currently the most
popular micro-blogging service for Chinese. The evaluation results demonstrate
that our method outperforms other baseline methods in several metrics such as
model perplexity, two kinds of entropies and KL-divergence of discovered
topics."
"This work is pertaining to the diversified ranking of web-resources and
interconnected documents that rely on a network-like structure, e.g. web-pages.
A practical example of this would be a query for the k most relevant web-pages
that are also in the same time as dissimilar with each other as possible.
Relevance and dissimilarity are quantified using an aggregation of network
distance and context similarity. For example, for a specific configuration of
the problem, we might be interested in web-pages that are similar with the
query in terms of their textual description but distant from each other in
terms of the web-graph, e.g. many clicks away. In retrospect, a dearth of work
can be found in the literature addressing this problem taking the network
structure formed by the document links into consideration.
  In this work, we propose a hill-climbing approach that is seeded with a
document collection which is generated using greedy heuristics to diversify
initially. More importantly, we tackle the problem in the context of web-pages
where there is an underlying network structure connecting the available
documents and resources. This is a significant difference to the majority of
works that tackle the problem in terms of either content definitions, or the
graph structure of the data, but never addressing both aspects simultaneously.
To the best of our knowledge, this is the very first effort that can be found
to combine both aspects of this important problem in an elegant fashion by also
allowing a great degree of flexibility on how to configure the trade-offs of
(i) document relevance over result-items' dissimilarity, and (ii) network
distance over content relevance or dissimilarity. Last but not least, we
present an extensive evaluation of our methods that demonstrate the
effectiveness and efficiency thereof."
"Many e-commerce websites use recommender systems or personalized rankers to
personalize search results based on their previous interactions. However, a
large fraction of users has no prior inter-actions, making it impossible to use
collaborative filtering or rely on user history for personalization. Even the
most active users mayvisit only a few times a year and may have volatile needs
or different personas, making their personal history a sparse and noisy signal
at best. This paper investigates how, when we cannot rely on the user history,
the large scale availability of other user interactions still allows us to
build meaningful profiles from the contextual data and whether such contextual
profiles are useful to customize the ranking, exemplified by data from a major
online travel agentBooking.com.Our main findings are threefold: First, we
characterize the Continuous Cold Start Problem(CoCoS) from the viewpoint of
typical e-commerce applications. Second, as explicit situational con-text is
not available in typical real world applications, implicit cues from
transaction logs used at scale can capture essential features of situational
context. Third, contextual user profiles can be created offline, resulting in a
set of smaller models compared to a single huge non-contextual model, making
contextual ranking available with negligible CPU and memory footprint. Finally
we conclude that, in an online A/B test on live users, our contextual ranker
in-creased user engagement substantially over a non-contextual base-line, with
click-through-rate (CTR) increased by 20%. This clearly demonstrates the value
of contextual user profiles in a real world application."
"A click on an item is arguably the most widely used feature in recommender
systems. However, a click is one out of 174 events a browser can trigger. This
paper presents a framework to effectively collect and store data from event
streams. A set of mining methods is provided to extract user engagement
features such as: attention span, scrolling depth and visible impressions. In
this work, we present an experiment where recommendations based on attention
span drove 340% higher click-through-rate than clicks."
"Recommender systems leverage both content and user interactions to generate
recommendations that fit users' preferences. The recent surge of interest in
deep learning presents new opportunities for exploiting these two sources of
information. To recommend items we propose to first learn a user-independent
high-dimensional semantic space in which items are positioned according to
their substitutability, and then learn a user-specific transformation function
to transform this space into a ranking according to the user's past
preferences. An advantage of the proposed architecture is that it can be used
to effectively recommend items using either content that describes the items or
user-item ratings. We show that this approach significantly outperforms
state-of-the-art recommender systems on the MovieLens 1M dataset."
"Document coherence describes how much sense text makes in terms of its
logical organisation and discourse flow. Even though coherence is a relatively
difficult notion to quantify precisely, it can be approximated automatically.
This type of coherence modelling is not only interesting in itself, but also
useful for a number of other text processing tasks, including Information
Retrieval (IR), where adjusting the ranking of documents according to both
their relevance and their coherence has been shown to increase retrieval
effectiveness [34,37].
  The state of the art in unsupervised coherence modelling represents documents
as bipartite graphs of sentences and discourse entities, and then projects
these bipartite graphs into one-mode undirected graphs. However, one-mode
projections may incur significant loss of the information present in the
original bipartite structure. To address this we present three novel graph
metrics that compute document coherence on the original bipartite graph of
sentences and entities. Evaluation on standard settings shows that: (i) one of
our coherence metrics beats the state of the art in terms of coherence
accuracy; and (ii) all three of our coherence metrics improve retrieval
effectiveness because, as closer analysis reveals, they capture aspects of
document quality that go undetected by both keyword-based standard ranking and
by spam filtering. This work contributes document coherence metrics that are
theoretically principled, parameter-free, and useful to IR."
"Online ranker evaluation is a key challenge in information retrieval. An
important task in the online evaluation of rankers is using implicit user
feedback for inferring preferences between rankers. Interleaving methods have
been found to be efficient and sensitive, i.e. they can quickly detect even
small differences in quality. It has recently been shown that multileaving
methods exhibit similar sensitivity but can be more efficient than interleaving
methods. This paper presents empirical results demonstrating that existing
multileaving methods either do not scale well with the number of rankers, or,
more problematically, can produce results which substantially differ from
evaluation measures like NDCG. The latter problem is caused by the fact that
they do not correctly account for the similarities that can occur between
rankers being multileaved. We propose a new multileaving method for handling
this problem and demonstrate that it substantially outperforms existing
methods, in some cases reducing errors by as much as 50%."
"Entity search is a new application meeting either precise or vague
requirements from the search engines users. Baidu Cup 2016 Challenge just
provided such a chance to tackle the problem of the entity search. We achieved
the first place with the average MAP scores on 4 tasks including movie, tvShow,
celebrity and restaurant. In this paper, we propose a series of similarity
features based on both of the word frequency features and the word semantic
features and describe our ranking architecture and experiment details."
"In this article we show how power transformations can be used as a common
framework for the derivation of local term weights. We found that under some
parametric conditions, BM25 and inverse regression produce equivalent results.
As a special case of inverse regression, we show that the largest increment in
term weight occurs when a term is mentioned for the second time. A model based
on inverse regression (BM25IR) is presented. Simulations suggest that BM25IR
works fairly well for different BM25 parametric conditions and document
lengths."
"In this study, a novel metacrawling method is proposed for discovering and
monitoring linked data sources on the Web. We implemented the method in a
prototype system, named SPARQL Endpoints Discovery (SpEnD). SpEnD starts with a
""search keyword"" discovery process for finding relevant keywords for the linked
data domain and specifically SPARQL endpoints. Then, these search keywords are
utilized to find linked data sources via popular search engines (Google, Bing,
Yahoo, Yandex). By using this method, most of the currently listed SPARQL
endpoints in existing endpoint repositories, as well as a significant number of
new SPARQL endpoints, have been discovered. Finally, we have developed a new
SPARQL endpoint crawler (SpEC) for crawling and link analysis."
"We propose a document retrieval method for question answering that represents
documents and questions as weighted centroids of word embeddings and reranks
the retrieved documents with a relaxation of Word Mover's Distance. Using
biomedical questions and documents from BIOASQ, we show that our method is
competitive with PUBMED. With a top-k approximation, our method is fast, and
easily portable to other domains and languages."
"This paper presents our method to retrieve relevant queries given a new
question in the context of Discovery Challenge: Learning to Re-Ranking
Questions for Community Question Answering competition. In order to do that, a
set of learning to rank methods was investigated to select an appropriate
method. The selected method was optimized on training data by using a search
strategy. After optimizing, the method was applied to development and test set.
Results from the competition indicate that the performance of our method
outperforms almost participants and show that Ranking SVM is efficient for
retrieving relevant queries in community question answering."
"Given a large network and a query node, finding its top-k similar nodes is a
primitive operation in many graph-based applications. Recently enhancing search
results with diversification have received much attention. In this paper, we
explore an novel problem of searching for top-k diversified similar nodes in
attributed networks, with the motivation that modeling diversification in an
attributed network should consider both the emergence of network links and the
attribute features of nodes such as user profile information. We formulate this
practical problem as two optimization problems: the Attributed Coverage
Diversification (ACD) problem and the r-Dissimilar Attributed Coverage
Diversification (r-DACD) problem. Based on the submodularity and the
monotonicity of ACD, we propose an efficient greedy algorithm achieving a tight
approximation guarantee of 1-1/e. Unlike the expension based methods only
considering nodes' neighborhood, ACD generalize the definition of
diversification to nodes' own features. To capture diversification in
topological structure of networks, the r-DACD problem introduce a dissimilarity
constraint. We refer to this problem as the Dissimilarity Constrained
Non-monotone Submodular Maximization (DCNSM) problem. We prove that there is no
constant-factor approximation for DCNSM, and also present an efficient greedy
algorithms achieving $1/\rho$ approximation, where $\rho\le\Delta$, $\Delta$ is
the maximum degree of its dissimilarity based graph. To the best of our
knowledge, it is the first approximation algorithm for the Submodular
Maximization problem with a distance constraint. The experimental results on
real-world attributed network datasets demonstrate the effectiveness of our
methods, and confirm that adding dissimilarity constraint can significantly
enhance the performance of diversification."
"Recommender systems assist users in navigating complex information spaces and
focus their attention on the content most relevant to their needs. Often these
systems rely on user activity or descriptions of the content. Social annotation
systems, in which users collaboratively assign tags to items, provide another
means to capture information about users and items. Each of these data sources
provides unique benefits, capturing different relationships.
  In this paper, we propose leveraging multiple sources of data: ratings data
as users report their affinity toward an item, tagging data as users assign
annotations to items, and item data collected from an online database. Taken
together, these datasets provide the opportunity to learn rich distributed
representations by exploiting recent advances in neural network architectures.
We first produce representations that subjectively capture interesting
relationships among the data. We then empirically evaluate the utility of the
representations to predict a user's rating on an item and show that it
outperforms more traditional representations. Finally, we demonstrate that
traditional representations can be combined with representations trained
through a neural network to achieve even better results."
"We present Sedano, a system for processing and indexing a continuous stream
of business-related news. Sedano defines pipelines whose stages analyze and
enrich news items (e.g., newspaper articles and press releases). News data
coming from several content sources are stored, processed and then indexed in
order to be consumed by Atoka, our business intelligence product. Atoka users
can retrieve news about specific companies, filtering according to various
facets. Sedano features both an entity-linking phase, which finds mentions of
companies in news, and a classification phase, which classifies news according
to a set of business events. Its flexible architecture allows Sedano to be
deployed on commodity machines while being scalable and fault-tolerant."
"With the rapid growth of Internet applications, sequential prediction in
collaborative filtering has become an emerging and crucial task. Given the
behavioral history of a specific user, predicting his or her next choice plays
a key role in improving various online services. Meanwhile, there are more and
more scenarios with multiple types of behaviors, while existing works mainly
study sequences with a single type of behavior. As a widely used approach,
Markov chain based models are based on a strong independence assumption. As two
classical neural network methods for modeling sequences, recurrent neural
networks cannot well model short-term contexts, and the log-bilinear model is
not suitable for long-term contexts. In this paper, we propose a Recurrent
Log-BiLinear (RLBL) model. It can model multiple types of behaviors in
historical sequences with behavior-specific transition matrices. RLBL applies a
recurrent structure for modeling long-term contexts. It models several items in
each hidden layer and employs position-specific transition matrices for
modeling short-term contexts. Moreover, considering continuous time difference
in behavioral history is a key factor for dynamic prediction, we further extend
RLBL and replace position-specific transition matrices with time-specific
transition matrices, and accordingly propose a Time-Aware Recurrent
Log-BiLinear (TA-RLBL) model. Experimental results show that the proposed RLBL
model and TA-RLBL model yield significant improvements over the competitive
compared methods on three datasets, i.e., Movielens-1M dataset, Global
Terrorism Database and Tmall dataset with different numbers of behavior types."
"Fine-grained user profile generation approaches have made it increasingly
feasible to display on a profile page in which topics a user has expertise or
interest. Earlier work on topical user profiling has been directed at enhancing
search and personalization functionality, but making such profiles useful for
human consumption presents new challenges. With this work, we have taken a
first step toward a semantic layout mode for topical user profiles. We have
developed a topical generalization approach which finds coherent groups of
topics and adds labels to them, based on their association with broader topics
in the Wikipedia category graph. A nested layout mode, employing topical
generalization, is compared with a simpler flat layout mode in our user study.
The results indicate that users favor the nested structure over flat profiles,
but tend to overlook the specific topics on the lower level. We propose a third
layout mode to address this issue."
"Recommender system data presents unique challenges to the data mining,
machine learning, and algorithms communities. The high missing data rate, in
combination with the large scale and high dimensionality that is typical of
recommender systems data, requires new tools and methods for efficient data
analysis. Here, we address the challenge of evaluating similarity between two
users in a recommender system, where for each user only a small set of ratings
is available. We present a new similarity score, that we call LiRa, based on a
statistical model of user similarity, for large-scale, discrete valued data
with many missing values. We show that this score, based on a ratio of
likelihoods, is more effective at identifying similar users than traditional
similarity scores in user-based collaborative filtering, such as the Pearson
correlation coefficient. We argue that our approach has significant potential
to improve both accuracy and scalability in collaborative filtering."
"Modern search engine result pages often provide immediate value to users and
organize information in such a way that it is easy to navigate. The core
ranking function contributes to this and so do result snippets, smart
organization of result blocks and extensive use of one-box answers or side
panels. While they are useful to the user and help search engines to stand out,
such features present two big challenges for evaluation. First, the presence of
such elements on a search engine result page (SERP) may lead to the absence of
clicks, which is, however, not related to dissatisfaction, so-called ""good
abandonments."" Second, the non-linear layout and visual difference of SERP
items may lead to non-trivial patterns of user attention, which is not captured
by existing evaluation metrics.
  In this paper we propose a model of user behavior on a SERP that jointly
captures click behavior, user attention and satisfaction, the CAS model, and
demonstrate that it gives more accurate predictions of user actions and
self-reported satisfaction than existing models based on clicks alone. We use
the CAS model to build a novel evaluation metric that can be applied to
non-linear SERP layouts and that can account for the utility that users obtain
directly on a SERP. We demonstrate that this metric shows better agreement with
user-reported satisfaction than conventional evaluation metrics."
"Divergence From Randomness (DFR) ranking models assume that informative terms
are distributed in a corpus differently than non-informative terms. Different
statistical models (e.g. Poisson, geometric) are used to model the distribution
of non-informative terms, producing different DFR models. An informative term
is then detected by measuring the divergence of its distribution from the
distribution of non-informative terms. However, there is little empirical
evidence that the distributions of non-informative terms used in DFR actually
fit current datasets. Practically this risks providing a poor separation
between informative and non-informative terms, thus compromising the
discriminative power of the ranking model. We present a novel extension to DFR,
which first detects the best-fitting distribution of non-informative terms in a
collection, and then adapts the ranking computation to this best-fitting
distribution. We call this model Adaptive Distributional Ranking (ADR) because
it adapts the ranking to the statistics of the specific dataset being processed
each time. Experiments on TREC data show ADR to outperform DFR models (and
their extensions) and be comparable in performance to a query likelihood
language model (LM)."
"In this work, we propose a joint audio-video fingerprint Automatic Content
Recognition (ACR) technology for media retrieval. The problem is focused on how
to balance the query accuracy and the size of fingerprint, and how to allocate
the bits of the fingerprint to video frames and audio frames to achieve the
best query accuracy. By constructing a novel concept called Coverage, which is
highly correlated to the query accuracy, we are able to form a rate-coverage
model to translate the original problem into an optimization problem that can
be resolved by dynamic programming. To the best of our knowledge, this is the
first work that uses joint audio-video fingerprint ACR technology for media
retrieval with a theoretical problem formulation. Experimental results indicate
that compared to reference algorithms, the proposed method has up to 25% query
accuracy improvement while using 60% overall bit-rates, and 25% bit-rate
reduction while achieving 85% accuracy, and it significantly outperforms the
solution with single audio or video source fingerprint."
"Information retrieval (IR) and recommender systems (RS) have been employed
for addressing search tasks executed during literature review and the overall
scholarly communication lifecycle. Majority of the studies have concentrated on
algorithm design for improving the accuracy and usefulness of these systems.
Contextual elements related to the scholarly tasks have been largely ignored.
In this paper, we propose a framework called the Scientific Paper Recommender
and Retrieval Framework (SPRRF) that combines aspects of user role modeling and
user-interface features with IR/RS components. The framework is based on eight
emergent themes identified from participants feedback in a user evaluation
study conducted with a prototype assistive system. 119 researchers participated
in the study for evaluating the prototype system that provides recommendations
for two literature review and one manuscript writing tasks. This holistic
framework is meant to guide future studies in this area."
"Recommendation systems are being explored by Cable TV operators to improve
user satisfaction with services, such as Live TV and Video on Demand (VOD)
services. More recently, Catch-up TV has been introduced, allowing users to
watch recent broadcast content whenever they want to. These services give users
a large set of options from which they can choose from, creating an information
overflow problem. Thus, recommendation systems arise as essential tools to
solve this problem by helping users in their selection, which increases not
only user satisfaction but also user engagement and content consumption. In
this paper we present a learning to rank approach that uses contextual
information and implicit feedback to improve recommendation systems for a Cable
TV operator that provides Live and Catch-up TV services. We compare our
approach with existing state-of-the-art algorithms and show that our approach
is superior in accuracy, while maintaining high scores of diversity and
serendipity."
"Nowadays, Cable TV operators provide their users multiple ways to watch TV
content, such as Live TV and Video on Demand (VOD) services. In the last years,
Catch-up TV has been introduced, allowing users to watch recent broadcast
content whenever they want to. Understanding how the users interact with such
services is important to develop solutions that may increase user satisfaction
, user engagement and user consumption. In this paper, we characterize, for the
first time, how users interact with a large European Cable TV operator that
provides Live TV, Catch-up TV and VOD services. We analyzed many
characteristics, such as the service usage, user engagement, program type,
program genres and time periods. This characterization will help us to have a
deeper understanding on how users interact with these different services, that
may be used to enhance the recommendation systems of Cable TV providers."
"Layouts and sub-layouts constitute an important clue while searching a
document on the basis of its structure, or when textual content is
unknown/irrelevant. A sub-layout specifies the arrangement of document entities
within a smaller portion of the document. We propose an efficient graph-based
matching algorithm, integrated with hash-based indexing, to prune a possibly
large search space. A user can specify a combination of sub-layouts of interest
using sketch-based queries. The system supports partial matching for
unspecified layout entities. We handle cases of segmentation pre-processing
errors (for text/non-text blocks) with a symmetry maximization-based strategy,
and accounting for multiple domain-specific plausible segmentation hypotheses.
We show promising results of our system on a database of unstructured entities,
containing 4776 newspaper images."
"Objective: Automatic text summarization tools can help users in the
biomedical domain to access information efficiently from a large volume of
scientific literature and other sources of text documents. In this paper, we
propose a summarization method that combines itemset mining and domain
knowledge to construct a concept-based model and to extract the main subtopics
from an input document. Our summarizer quantifies the informativeness of each
sentence using the support values of itemsets appearing in the sentence.
Methods: To address the concept-level analysis of text, our method initially
maps the original document to biomedical concepts using the UMLS. Then, it
discovers the essential subtopics of the text using a data mining technique,
namely itemset mining, and constructs the summarization model. The employed
itemset mining algorithm extracts a set of frequent itemsets containing
correlated and recurrent concepts of the input document. The summarizer selects
the most related and informative sentences and generates the final summary.
Results: We evaluate the performance of our itemset-based summarizer using the
Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics, performing a
set of experiments. The results show that the itemset-based summarizer performs
better than the compared methods. The itemset-based summarizer achieves the
best scores for all the assessed ROUGE metrics . Conclusion: Compared to the
statistical, similarity, and word frequency methods, the proposed method
demonstrates that the summarization model obtained from the concept extraction
and itemset mining provides the summarizer with an effective metric for
measuring the informative content of sentences. This can lead to an improvement
in the performance of biomedical literature summarization."
"This paper presents a novel system E3 for extracting keyphrases from news
content for the purpose of offering the news audience a broad overview of news
events, with especially high content volume. Given an input query, E3 extracts
keyphrases and enrich them by tagging, ranking and finding role for frequently
associated keyphrases. Also, E3 finds the novelty and activeness of keyphrases
using news publication date, to identify the most interesting and informative
keyphrases."
"Filtering relevant documents with respect to entities is an essential task in
the context of knowledge base construction and maintenance. It entails
processing a time-ordered stream of documents that might be relevant to an
entity in order to select only those that contain vital information.
State-of-the-art approaches to document filtering for popular entities are
entity-dependent: they rely on and are also trained on the specifics of
differentiating features for each specific entity. Moreover, these approaches
tend to use so-called extrinsic information such as Wikipedia page views and
related entities which is typically only available only for popular head
entities. Entity-dependent approaches based on such signals are therefore
ill-suited as filtering methods for long-tail entities. In this paper we
propose a document filtering method for long-tail entities that is
entity-independent and thus also generalizes to unseen or rarely seen entities.
It is based on intrinsic features, i.e., features that are derived from the
documents in which the entities are mentioned. We propose a set of features
that capture informativeness, entity-saliency, and timeliness. In particular,
we introduce features based on entity aspect similarities, relation patterns,
and temporal expressions and combine these with standard features for document
filtering. Experiments following the TREC KBA 2014 setup on a publicly
available dataset show that our model is able to improve the filtering
performance for long-tail entities over several baselines. Results of applying
the model to unseen entities are promising, indicating that the model is able
to learn the general characteristics of a vital document. The overall
performance across all entities---i.e., not just long-tail entities---improves
upon the state-of-the-art without depending on any entity-specific training
data."
"A publicly available dataset for federated search reflecting a real web
environment has long been absent, making it difficult for researchers to test
the validity of their federated search algorithms for the web setting. We
present several experiments and analyses on resource selection on the web using
a recently released test collection containing the results from more than a
hundred real search engines, ranging from large general web search engines such
as Google, Bing and Yahoo to small domain-specific engines. First, we
experiment with estimating the size of uncooperative search engines on the web
using query based sampling and propose a new method using the ClueWeb09
dataset. We find the size estimates to be highly effective in resource
selection. Second, we show that an optimized federated search system based on
smaller web search engines can be an alternative to a system using large web
search engines. Third, we provide an empirical comparison of several popular
resource selection methods and find that these methods are not readily suitable
for resource selection on the web. Challenges include the sparse resource
descriptions and extremely skewed sizes of collections."
"It has been previously noted that optimization of the n-call@k relevance
objective (i.e., a set-based objective that is 1 if at least n documents in a
set of k are relevant, otherwise 0) encourages more result set diversification
for smaller n, but this statement has never been formally quantified. In this
work, we explicitly derive the mathematical relationship between expected
n-call@k and the relevance vs. diversity trade-off --- through fortuitous
cancellations in the resulting combinatorial optimization, we show the
trade-off is a simple and intuitive function of n (notably independent of the
result set size k e n), where diversification increases as n approaches 1."
"Predicting personalized sequential behavior is a key task for recommender
systems. In order to predict user actions such as the next product to purchase,
movie to watch, or place to visit, it is essential to take into account both
long-term user preferences and sequential patterns (i.e., short-term dynamics).
Matrix Factorization and Markov Chain methods have emerged as two separate but
powerful paradigms for modeling the two respectively. Combining these ideas has
led to unified methods that accommodate long- and short-term dynamics
simultaneously by modeling pairwise user-item and item-item interactions.
  In spite of the success of such methods for tackling dense data, they are
challenged by sparsity issues, which are prevalent in real-world datasets. In
recent years, similarity-based methods have been proposed for
(sequentially-unaware) item recommendation with promising results on sparse
datasets. In this paper, we propose to fuse such methods with Markov Chains to
make personalized sequential recommendations. We evaluate our method, Fossil,
on a variety of large, real-world datasets. We show quantitatively that Fossil
outperforms alternative algorithms, especially on sparse datasets, and
qualitatively that it captures personalized dynamics and is able to make
meaningful recommendations."
"Probabilistic language models are widely used in Information Retrieval (IR)
to rank documents by the probability that they generate the query. However, the
implementation of the probabilistic representations with programming languages
that favor matrix calculations is challenging. In this paper, we utilize matrix
representations to reformulate the probabilistic language models. The matrix
representation is a superstructure for the probabilistic language models to
organize the calculated probabilities and a potential formalism for
standardization of language models and for further mathematical analysis. It
facilitates implementations by matrix friendly programming languages. In this
paper, we consider the matrix formulation of conventional language model with
Dirichlet smoothing, and two language models based on Latent Dirichlet
Allocation (LDA), i.e., LBDM and LDI. We release a Java software
package--MatLM--implementing the proposed models. Code is available at:
https://github.com/yanshanwang/JGibbLDA-v.1.0-MatLM."
"Much of the information processed by Information Retrieval (IR) systems is
unreliable, biased, and generally untrustworthy [1], [2], [3]. Yet, factuality
& objectivity detection is not a standard component of IR systems, even though
it has been possible in Natural Language Processing (NLP) in the last decade.
Motivated by this, we ask if and how factuality & objectivity detection may
benefit IR. We answer this in two parts. First, we use state-of-the-art NLP to
compute the probability of document factuality & objectivity in two TREC
collections, and analyse its relation to document relevance. We find that
factuality is strongly and positively correlated to document relevance, but
objectivity is not. Second, we study the impact of factuality & objectivity to
retrieval effectiveness by treating them as query independent features that we
combine with a competitive language modelling baseline. Experiments with 450
TREC queries show that factuality improves precision >10% over strong
baselines, especially for uncurated data used in web search; objectivity gives
mixed results. An overall clear trend is that document factuality & objectivity
is much more beneficial to IR when searching uncurated (e.g. web) documents vs.
curated (e.g. state documentation and newswire articles). To our knowledge,
this is the first study of factuality & objectivity for back-end IR,
contributing novel findings about the relation between relevance and
factuality/objectivity, and statistically significant gains to retrieval
effectiveness in the competitive web search task."
"We estimate sentiment categories proportions for retrieval within large
retrieval sets. In general, estimates are produced by counting the
classification outcomes and then by adjusting such category sizes taking into
account misclassification error matrix. However, both the accuracy of the
classifier and the precision of the retrieval produce a large number of errors
that makes difficult the application of an aggregative approach to sentiment
analysis as a reliable and efficient estimation of proportions for sentiment
categories.
  The challenge for real time analytics during retrieval is thus to overcome
misclassification errors, and more importantly, to apply sentiment
classification or any other similar post-processing analytics at retrieval
time. We present a non-aggregative approach that can be applied to very large
retrieval sets of queries."
"We propose a framework for discriminative Information Retrieval (IR) atop
linguistic features, trained to improve the recall of tasks such as answer
candidate passage retrieval, the initial step in text-based Question Answering
(QA). We formalize this as an instance of linear feature-based IR (Metzler and
Croft, 2007), illustrating how a variety of knowledge discovery tasks are
captured under this approach, leading to a 44% improvement in recall for
candidate triage for QA."
"Modern multi-stage retrieval systems are comprised of a candidate generation
stage followed by one or more reranking stages. In such an architecture, the
quality of the final ranked list may not be sensitive to the quality of initial
candidate pool, especially in terms of early precision. This provides several
opportunities to increase retrieval efficiency without significantly
sacrificing effectiveness. In this paper, we explore a new approach to
dynamically predicting two different parameters in the candidate generation
stage which can directly affect the overall efficiency and effectiveness of the
entire system. Previous work exploring this tradeoff has focused on global
parameter settings that apply to all queries, even though optimal settings vary
across queries. In contrast, we propose a technique which makes a parameter
prediction that maximizes efficiency within a effectiveness envelope on a per
query basis, using only static pre-retrieval features. The query-specific
tradeoff point between effectiveness and efficiency is decided using a
classifier cascade that weighs possible efficiency gains against effectiveness
losses over a range of possible parameter cutoffs to make the prediction. The
interesting twist in our new approach is to train classifiers without requiring
explicit relevance judgments. We show that our framework is generalizable by
applying it to two different retrieval parameters - selecting k in common top-k
query retrieval algorithms, and setting a quality threshold, $\rho$, for
score-at-a-time approximate query evaluation algorithms. Experimental results
show that substantial efficiency gains are achievable depending on the dynamic
parameter choice. In addition, our framework provides a versatile tool that can
be used to estimate the effectiveness-efficiency tradeoffs that are possible
before selecting and tuning algorithms to make machine learned predictions."
"The idea of representation has been used in various fields of study from data
analysis to political science. In this paper, we define representativeness and
describe a method to isolate data points that can represent the entire data
set. Also, we show how the minimum set of representative data points can be
generated. We use data from GLOBE (a project to study the effects on Land
Change based on a set of parameters that include temperature, forest cover,
human population, atmospheric parameters and many other variables) to test &
validate the algorithm. Principal Component Analysis (PCA) is used to reduce
the dimensions of the multivariate data set, so that the representative points
can be generated efficiently and its Representativeness has been compared
against Random Sampling of points from the data set."
"Similarity search on time series is a frequent operation in large-scale
data-driven applications. Sophisticated similarity measures are standard for
time series matching, as they are usually misaligned. Dynamic Time Warping or
DTW is the most widely used similarity measure for time series because it
combines alignment and matching at the same time. However, the alignment makes
DTW slow. To speed up the expensive similarity search with DTW, branch and
bound based pruning strategies are adopted. However, branch and bound based
pruning are only useful for very short queries (low dimensional time series),
and the bounds are quite weak for longer queries. Due to the loose bounds
branch and bound pruning strategy boils down to a brute-force search.
  To circumvent this issue, we design SSH (Sketch, Shingle, & Hashing), an
efficient and approximate hashing scheme which is much faster than the
state-of-the-art branch and bound searching technique: the UCR suite. SSH uses
a novel combination of sketching, shingling and hashing techniques to produce
(probabilistic) indexes which align (near perfectly) with DTW similarity
measure. The generated indexes are then used to create hash buckets for
sub-linear search. Our results show that SSH is very effective for longer time
sequence and prunes around 95% candidates, leading to the massive speedup in
search with DTW. Empirical results on two large-scale benchmark time series
data show that our proposed method can be around 20 times faster than the
state-of-the-art package (UCR suite) without any significant loss in accuracy."
"Models such as latent semantic analysis and those based on neural embeddings
learn distributed representations of text, and match the query against the
document in the latent semantic space. In traditional information retrieval
models, on the other hand, terms have discrete or local representations, and
the relevance of a document is determined by the exact matches of query terms
in the body text. We hypothesize that matching with distributed representations
complements matching with traditional local representations, and that a
combination of the two is favorable. We propose a novel document ranking model
composed of two separate deep neural networks, one that matches the query and
the document using a local representation, and another that matches the query
and the document using learned distributed representations. The two networks
are jointly trained as part of a single neural network. We show that this
combination or `duet' performs significantly better than either neural network
individually on a Web page ranking task, and also significantly outperforms
traditional baselines and other recently proposed models based on neural
networks."
"Internet data has surfaced as a primary source for investigation of different
aspects of human behavior. A crucial step in such studies is finding a suitable
cohort (i.e., a set of users) that shares a common trait of interest to
researchers. However, direct identification of users sharing this trait is
often impossible, as the data available to researchers is usually anonymized to
preserve user privacy. To facilitate research on specific topics of interest,
especially in medicine, we introduce an algorithm for identifying a trait of
interest in anonymous users. We illustrate how a small set of labeled examples,
together with statistical information about the entire population, can be
aggregated to obtain labels on unseen examples. We validate our approach using
labeled data from the political domain.
  We provide two applications of the proposed algorithm to the medical domain.
In the first, we demonstrate how to identify users whose search patterns
indicate they might be suffering from certain types of cancer. In the second,
we detail an algorithm to predict the distribution of diseases given their
incidence in a subset of the population at study, making it possible to predict
disease spread from partial epidemiological data."
"Retrieval pipelines commonly rely on a term-based search to obtain candidate
records, which are subsequently re-ranked. Some candidates are missed by this
approach, e.g., due to a vocabulary mismatch. We address this issue by
replacing the term-based search with a generic k-NN retrieval algorithm, where
a similarity function can take into account subtle term associations. While an
exact brute-force k-NN search using this similarity function is slow, we
demonstrate that an approximate algorithm can be nearly two orders of magnitude
faster at the expense of only a small loss in accuracy. A retrieval pipeline
using an approximate k-NN search can be more effective and efficient than the
term-based pipeline. This opens up new possibilities for designing effective
retrieval pipelines. Our software (including data-generating code) and
derivative data based on the Stack Overflow collection is available online."
"Online recommender systems often deal with continuous, potentially fast and
unbounded flows of data. Ensemble methods for recommender systems have been
used in the past in batch algorithms, however they have never been studied with
incremental algorithms, that are capable of processing those data streams on
the fly. We propose online bagging, using an incremental matrix factorization
algorithm for positive-only data streams. Using prequential evaluation, we show
that bagging is able to improve accuracy more than 35% over the baseline with
small computational overhead."
"With the increase in size of web, the information is also spreading at large
scale. Search Engines are the medium to access this information. Crawler is the
module of search engine which is responsible for download the web pages. In
order to download the fresh information and get the database rich, crawler
should crawl the web in some order. This is called as ordering of URLs. URL
ordering should be done in efficient and effective manner in order to crawl the
web in proficient manner. In this paper, a survey is done on some existing
methods of URL ordering and at the end of this paper comparison is also carried
out among them."
"Web recommendation services bear great importance in e-commerce, as they aid
the user in navigating through the items that are most relevant to her needs.
In a typical Web site, long history of previous activities or purchases by the
user is rarely available. Hence in most cases, recommenders propose items that
are similar to the most recent ones viewed in the current user session. The
corresponding task is called session based item-to-item recommendation. For
frequent items, it is easy to present item-to-item recommendations by ""people
who viewed this, also viewed"" lists. However, most of the items belong to the
long tail, where previous actions are sparsely available. Another difficulty is
the so-called cold start problem, when the item has recently appeared and had
no time yet to accumulate sufficient number of transactions. In order to
recommend a next item in a session in sparse or cold start situations, we also
have to incorporate item similarity models. In this paper we describe a
probabilistic similarity model based on Random Fields to approximate
item-to-item transition probabilities. We give a generative model for the item
interactions based on arbitrary distance measures over the items including
explicit, implicit ratings and external metadata. The model may change in time
to fit better recent events and recommend the next item based on the updated
Fisher Information. Our new model outperforms both simple similarity baseline
methods and recent item-to-item recommenders, under several different
performance metrics and publicly available data sets. We reach significant
gains in particular for recommending a new item following a rare item."
"Microblogging is a model of content sharing in which the temporal locality of
posts with respect to important events, either of foreseeable or unforeseeable
nature, makes applica- tions of real-time filtering of great practical
interest. We propose the use of Entity Linking (EL) in order to improve the
retrieval effectiveness, by enriching the representation of microblog posts and
filtering queries. EL is the process of recognizing in an unstructured text the
mention of relevant entities described in a knowledge base. EL of short pieces
of text is a difficult task, but it is also a scenario in which the information
EL adds to the text can have a substantial impact on the retrieval process. We
implement a start-of-the-art filtering method, based on the best systems from
the TREC Microblog track realtime adhoc retrieval and filtering tasks , and
extend it with a Wikipedia-based EL method. Results show that the use of EL
significantly improves over non-EL based versions of the filtering methods."
"Sequential prediction is a fundamental task for Web applications. Due to the
insufficiency of user feedbacks, sequential prediction usually suffers from the
cold start problem. There are two kinds of popular approaches based on matrix
factorization (MF) and Markov chains (MC) for item prediction. MF methods
factorize the user-item matrix to learn general tastes of users. MC methods
predict the next behavior based on recent behaviors. However, they have
limitations. MF methods can merge additional information to address cold start
but could not capture dynamic properties of user's interest, and MC based
sequential methods have difficulty in addressing cold start and has a strong
Markov assumption that the next state only depends on the last state. In this
work, to deal with the cold start problem of sequential prediction, we propose
a RNN model adopting visual and textual content of items, which is named as
$\mathbf{V}$isual and $\mathbf{T}$extual $\mathbf{R}$ecurrent $\mathbf{N}$eural
$\mathbf{N}$etwork ($\mathbf{VT}$-$\mathbf{RNN}$). We can simultaneously learn
the sequential latent vectors that dynamically capture the user's interest, as
well as content-based representations that contribute to address the cold
start. Experiments on two real-world datasets show that our proposed VT-RNN
model can effectively generate the personalized ranking list and significantly
alleviate the cold start problem."
"A recent ""third wave"" of Neural Network (NN) approaches now delivers
state-of-the-art performance in many machine learning tasks, spanning speech
recognition, computer vision, and natural language processing. Because these
modern NNs often comprise multiple interconnected layers, this new NN research
is often referred to as deep learning. Stemming from this tide of NN work, a
number of researchers have recently begun to investigate NN approaches to
Information Retrieval (IR). While deep NNs have yet to achieve the same level
of success in IR as seen in other areas, the recent surge of interest and work
in NNs for IR suggest that this state of affairs may be quickly changing. In
this work, we survey the current landscape of Neural IR research, paying
special attention to the use of learned representations of queries and
documents (i.e., neural embeddings). We highlight the successes of neural IR
thus far, catalog obstacles to its wider adoption, and suggest potentially
promising directions for future research."
"It is well known that learning customers' preference and making
recommendations to them from today's information-exploded environment is
critical and non-trivial in an on-line system. There are two different modes of
recommendation systems, namely pull-mode and push-mode. The majority of the
recommendation systems are pull-mode, which recommend items to users only when
and after users enter Application Market. While push-mode works more actively
to enhance or re-build connection between Application Market and users. As one
of the most successful phone manufactures,both the number of users and apps
increase dramatically in Huawei Application Store (also named Hispace Store),
which has approximately 0.3 billion registered users and 1.2 million apps until
2016 and whose number of users is growing with high-speed. For the needs of
real scenario, we establish a Push Service Platform (shortly, PSP) to discover
the target user group automatically from web-scale user operation log data with
an additional small set of labelled apps (usually around 10 apps),in Hispace
Store. As presented in this work,PSP includes distributed storage layer,
application layer and evaluation layer. In the application layer, we design a
practical graph-based algorithm (named A-PARW) for user group discovery, which
is an approximate version of partially absorbing random walk. Based on I mode
of A-PARW, the effectiveness of our system is significantly improved, compared
to the predecessor to presented system, which uses Personalized Pagerank in its
application layer."
"In the data deluge context, pattern recognition or labeling in streams is
becoming quite an essential and pressing task as data flows inside always
bigger streams. The assessment of such tasks is not so easy when dealing with
temporal data, namely patterns that have a duration (a beginning and an end
time-stamp). This paper details an approach based on an editing distance to
first align a sequence of labeled temporal segments with a ground truth
sequence, and then, by back-tracing an optimal alignment path, to provide a
confusion matrix at the label level. From this confusion matrix, standard
evaluation measures can easily be derived as well as other measures such as the
""latency"" that can be quite important in (early) pattern detection
applications."
"The ubiquitous nature of modern Information Retrieval and Virtual World give
rise to new realities. To what extent are these ""realities"" real? Which
""physics"" should be applied to quantitatively describe them? In this essay I
dwell on few examples. The first is Adaptive neural networks, which are not
networks and not neural, but still provide service similar to classical ANNs in
extended fashion. The second is the emergence of objects looking like
Einsteinian spacetime, which describe the behavior of an Internet surfer like
geodesic motion. The third is the demonstration of nonclassical and even
stronger-than-quantum probabilities in Information Retrieval, their use.
  Immense operable datasets provide new operationalistic environments, which
become to greater and greater extent ""realities"". In this essay, I consider the
overall Information Retrieval process as an objective physical process,
representing it according to Melucci metaphor in terms of physical-like
experiments. Various semantic environments are treated as analogs of various
realities. The readers' attention is drawn to topos approach to physical
theories, which provides a natural conceptual and technical framework to cope
with the new emerging realities."
"There is a renaissance in visual analytics systems for data analysis and
sharing, in particular, in the current wave of big data applications. We
introduce RAVE, a prototype that automates the generation of an interface that
uses facets and visualization techniques for exploring and analyzing relevance
assessments data sets collected via crowdsourcing. We present a technical
description of the main components and demonstrate its use."
"While advances in computing resources have made processing enormous amounts
of data possible, human ability to identify patterns in such data has not
scaled accordingly. Thus, efficient computational methods for condensing and
simplifying data are becoming vital for extracting actionable insights. In
particular, while data summarization techniques have been studied extensively,
only recently has summarizing interconnected data, or graphs, become popular.
This survey is a structured, comprehensive overview of the state-of-the-art
methods for summarizing graph data. We first broach the motivation behind and
the challenges of graph summarization. We then categorize summarization
approaches by the type of graphs taken as input and further organize each
category by core methodology. Finally, we discuss applications of summarization
on real-world graphs and conclude by describing some open problems in the
field."
"Ranking evaluation metrics are a fundamental element of design and
improvement efforts in information retrieval. We observe that most popular
metrics disregard information portrayed in the scores used to derive rankings,
when available. This may pose a numerical scaling problem, causing an under- or
over-estimation of the evaluation depending on the degree of divergence between
the scores of ranked items. The purpose of this work is to propose a principled
way of quantifying multi-graded relevance judgments of items and enable a more
accurate penalization of ordering errors in rankings. We propose a data-driven
generation of relevance functions based on the degree of the divergence amongst
a set of items' scores and its application in the evaluation metric Normalized
Discounted Cumulative Gain (nDCG). We use synthetic data to demonstrate the
interest of our proposal and a combination of data on news items from Google
News and their respective popularity in Twitter to show its performance in
comparison to the standard nDCG. Results show that our proposal is capable of
providing a more fine-grained evaluation of rankings when compared to the
standard nDCG, and that the latter frequently under- or over-estimates its
evaluation scores in light of the divergence of items' scores."
"The recent development of Audio-based Distributional Semantic Models (ADSMs)
enables the computation of audio and lexical vector representations in a joint
acoustic-semantic space. In this work, these joint representations are applied
to the problem of automatic tag generation. The predicted tags together with
their corresponding acoustic representation are exploited for the construction
of acoustic-semantic clip embeddings. The proposed algorithms are evaluated on
the task of similarity measurement between music clips. Acoustic-semantic
models are shown to outperform the state-of-the-art for this task and produce
high quality tags for audio/music clips."
"Summary: Abstracts in biomedical articles can provide a quick overview of the
articles but detailed information cannot be obtained without reading full-text
contents. Full-text articles certainly generate more information and contents;
however, accessing full-text documents is usually time consuming. Condensedly
is a web-based application, which provides readers an easy and efficient way to
access full-text paragraphs using sentences in abstracts as fishing bait to
retrieve the big fish reside in full-text. Condensedly is based on the
paragraph ranking algorithm, which evaluates and ranks full-text paragraphs
based on their association scores with sentences in abstracts.
  Availability: http://140.116.247.185/~research/Condensedly"
"With the ever increasing number of filed patent applications every year, the
need for effective and efficient systems for managing such tremendous amounts
of data becomes inevitably important. Patent Retrieval (PR) is considered is
the pillar of almost all patent analysis tasks. PR is a subfield of Information
Retrieval (IR) which is concerned with developing techniques and methods that
effectively and efficiently retrieve relevant patent documents in response to a
given search request. In this paper we present a comprehensive review on PR
methods and approaches. It is clear that, recent successes and maturity in IR
applications such as Web search can not be transferred directly to PR without
deliberate domain adaptation and customization. Furthermore, state-of-the-art
performance in automatic PR is still around average. These observations
motivates the need for interactive search tools which provide cognitive
assistance to patent professionals with minimal effort. These tools must also
be developed in hand with patent professionals considering their practices and
expectations. We additionally touch on related tasks to PR such as patent
valuation, litigation, licensing, and highlight potential opportunities and
open directions for computational scientists in these domains."
"Probabilistic graphic model is an elegant framework to compactly present
complex real-world observations by modeling uncertainty and logical flow
(conditionally independent factors). In this paper, we present a probabilistic
framework of neighborhood-based recommendation methods (PNBM) in which
similarity is regarded as an unobserved factor. Thus, PNBM leads the estimation
of user preference to maximizing a posterior over similarity. We further
introduce a novel multi-layer similarity descriptor which models and learns the
joint influence of various features under PNBM, and name the new framework
MPNBM. Empirical results on real-world datasets show that MPNBM allows very
accurate estimation of user preferences."
"Hashtags have become a powerful tool in social platforms such as Twitter to
categorize and search for content, and to spread short messages across members
of the social network. In this paper, we study temporal hashtag usage practices
in Twitter with the aim of designing a cognitive-inspired hashtag
recommendation algorithm we call BLLi,s. Our main idea is to incorporate the
effect of time on (i) individual hashtag reuse (i.e., reusing own hashtags),
and (ii) social hashtag reuse (i.e., reusing hashtags, which has been
previously used by a followee) into a predictive model. For this, we turn to
the Base-Level Learning (BLL) equation from the cognitive architecture ACT-R,
which accounts for the time-dependent decay of item exposure in human memory.
We validate BLLi,s using two crawled Twitter datasets in two evaluation
scenarios: firstly, only temporal usage patterns of past hashtag assignments
are utilized and secondly, these patterns are combined with a content-based
analysis of the current tweet. In both scenarios, we find not only that
temporal effects play an important role for both individual and social hashtag
reuse but also that BLLi,s provides significantly better prediction accuracy
and ranking results than current state-of-the-art hashtag recommendation
methods."
"Newswire and Social Media are the major sources of information in our time.
While the topical demographic of Western Media was subjects of studies in the
past, less is known about Chinese Media. In this paper, we apply event
detection and tracking technology to examine the information overlap and
differences between Chinese and Western - Traditional Media and Social Media.
Our experiments reveal a biased interest of China towards the West, which
becomes particularly apparent when comparing the interest in celebrities."
"One of the main challenges in Recommender Systems (RSs) is the New User
problem which happens when the system has to generate personalised
recommendations for a new user whom the system has no information about. Active
Learning tries to solve this problem by acquiring user preference data with the
maximum quality, and with the minimum acquisition cost. Although there are
variety of works in active learning for RSs research area, almost all of them
have focused only on the single-domain recommendation scenario. However,
several real-world RSs operate in the cross-domain scenario, where the system
generates recommendations in the target domain by exploiting user preferences
in both the target and auxiliary domains. In such a scenario, the performance
of active learning strategies can be significantly influenced and typical
active learning strategies may fail to perform properly. In this paper, we
address this limitation, by evaluating active learning strategies in a novel
evaluation framework, explicitly suited for the cross-domain recommendation
scenario. We show that having access to the preferences of the users in the
auxiliary domain may have a huge impact on the performance of active learning
strategies w.r.t. the classical, single-domain scenario."
"Recent research has shown the usefulness of using collective user interaction
data (e.g., query logs) to recommend query modification suggestions for
Intranet search. However, most of the query suggestion approaches for Intranet
search follow an ""one size fits all"" strategy, whereby different users who
submit an identical query would get the same query suggestion list. This is
problematic, as even with the same query, different users may have different
topics of interest, which may change over time in response to the user's
interaction with the system. We address the problem by proposing a personalised
query suggestion framework for Intranet search. For each search session, we
construct two temporal user profiles: a click user profile using the user's
clicked documents and a query user profile using the user's submitted queries.
We then use the two profiles to re-rank the non-personalised query suggestion
list returned by a state-of-the-art query suggestion method for Intranet
search. Experimental results on a large-scale query logs collection show that
our personalised framework significantly improves the quality of suggested
queries."
"Trending topics in microblogs such as Twitter are valuable resources to
understand social aspects of real-world events. To enable deep analyses of such
trends, semantic annotation is an effective approach; yet the problem of
annotating microblog trending topics is largely unexplored by the research
community. In this work, we tackle the problem of mapping trending Twitter
topics to entities from Wikipedia. We propose a novel model that complements
traditional text-based approaches by rewarding entities that exhibit a high
temporal correlation with topics during their burst time period. By exploiting
temporal information from the Wikipedia edit history and page view logs, we
have improved the annotation performance by 17-28\%, as compared to the
competitive baselines."
"Recent advances of preservation technologies have led to an increasing number
of Web archive systems and collections. These collections are valuable to
explore the past of the Web, but their value can only be uncovered with
effective access and exploration mechanisms. Ideal search and rank- ing methods
must be robust to the high redundancy and the temporal noise of contents, as
well as scalable to the huge amount of data archived. Despite several attempts
in Web archive search, facilitating access to Web archive still remains a
challenging problem.
  In this work, we conduct a first analysis on different ranking strategies
that exploit evidences from metadata instead of the full content of documents.
We perform a first study to compare the usefulness of non-content evidences to
Web archive search, where the evidences are mined from the metadata of file
headers, links and URL strings only. Based on these findings, we propose a
simple yet surprisingly effective learning model that combines multiple
evidences to distinguish ""good"" from ""bad"" search results. We conduct empirical
experiments quantitatively as well as qualitatively to confirm the validity of
our proposed method, as a first step towards better ranking in Web archives
taking meta- data into account."
"A high degree of topical diversity is often considered to be an important
characteristic of interesting text documents. A recent proposal for measuring
topical diversity identifies three elements for assessing diversity: words,
topics, and documents as collections of words. Topic models play a central role
in this approach. Using standard topic models for measuring diversity of
documents is suboptimal due to generality and impurity. General topics only
include common information from a background corpus and are assigned to most of
the documents in the collection. Impure topics contain words that are not
related to the topic; impurity lowers the interpretability of topic models and
impure topics are likely to get assigned to documents erroneously. We propose a
hierarchical re-estimation approach for topic models to combat generality and
impurity; the proposed approach operates at three levels: words, topics, and
documents. Our re-estimation approach for measuring documents' topical
diversity outperforms the state of the art on PubMed dataset which is commonly
used for diversity experiments."
"Research shows that many like-minded people use popular microblogging
websites for posting hateful speech against various religions and race.
Automatic identification of racist and hate promoting posts is required for
building social media intelligence and security informatics based solutions.
However, just keyword spotting based techniques cannot be used to accurately
identify the intent of a post. In this paper, we address the challenge of the
presence of ambiguity in such posts by identifying the intent of author. We
conduct our study on Tumblr microblogging website and develop a cascaded
ensemble learning classifier for identifying the posts having racist or
radicalized intent. We train our model by identifying various semantic,
sentiment and linguistic features from free-form text. Our experimental results
shows that the proposed approach is effective and the emotion tone, social
tendencies, language cues and personality traits of a narrative are
discriminatory features for identifying the racist intent behind a post."
"Word obfuscation or substitution means replacing one word with another word
in a sentence to conceal the textual content or communication. Word obfuscation
is used in adversarial communication by terrorist or criminals for conveying
their messages without getting red-flagged by security and intelligence
agencies intercepting or scanning messages (such as emails and telephone
conversations). ConceptNet is a freely available semantic network represented
as a directed graph consisting of nodes as concepts and edges as assertions of
common sense about these concepts. We present a solution approach exploiting
vast amount of semantic knowledge in ConceptNet for addressing the technically
challenging problem of word substitution in adversarial communication. We frame
the given problem as a textual reasoning and context inference task and utilize
ConceptNet's natural-language-processing tool-kit for determining word
substitution. We use ConceptNet to compute the conceptual similarity between
any two given terms and define a Mean Average Conceptual Similarity (MACS)
metric to identify out-of-context terms. The test-bed to evaluate our proposed
approach consists of Enron email dataset (having over 600000 emails generated
by 158 employees of Enron Corporation) and Brown corpus (totaling about a
million words drawn from a wide variety of sources). We implement word
substitution techniques used by previous researches to generate a test dataset.
We conduct a series of experiments consisting of word substitution methods used
in the past to evaluate our approach. Experimental results reveal that the
proposed approach is effective."
"By the growing trend of online shopping and e-commerce websites,
recommendation systems have gained more importance in recent years in order to
increase the sales ratios of companies. Different algorithms on recommendation
systems are used and every one produce different results. Every algorithm on
this area have positive and negative attributes. The purpose of the research is
to test the different algorithms for choosing the best one according as
structure of dataset and aims of developers. For this purpose, threshold and
k-means based collaborative filtering and content-based filtering algorithms
are utilized on the dataset contains 100*73421 matrix length. What are the
differences and effects of these different algorithms on the same dataset? What
are the challenges of the algorithms? What criteria are more important in order
to evaluate a recommendation systems? In the study, we answer these crucial
problems with the case study."
"Image retrieval is a complex task that differs according to the context and
the user requirements in any specific field, for example in a medical
environment. Search by text is often not possible or optimal and retrieval by
the visual content does not always succeed in modelling high-level concepts
that a user is looking for. Modern image retrieval techniques consist of
multiple steps and aim to retrieve information from large--scale datasets and
not only based on global image appearance but local features and if possible in
a connection between visual features and text or semantics. This paper presents
the Parallel Distributed Image Search Engine (ParaDISE), an image retrieval
system that combines visual search with text--based retrieval and that is
available as open source and free of charge. The main design concepts of
ParaDISE are flexibility, expandability, scalability and interoperability.
These concepts constitute the system, able to be used both in real-world
applications and as an image retrieval research platform. Apart from the
architecture and the implementation of the system, two use cases are described,
an application of ParaDISE in retrieval of images from the medical literature
and a visual feature evaluation for medical image retrieval. Future steps
include the creation of an open source community that will contribute and
expand this platform based on the existing parts."
"While test collections are commonly employed to evaluate the effectiveness of
information retrieval (IR) systems, constructing these collections has become
increasingly expensive in recent years as collection sizes have grown
ever-larger. To address this, we propose a new learning-to-rank topic selection
method which reduces the number of search topics needed for reliable evaluation
of IR systems. As part of this work, we also revisit the deep vs. shallow
judging debate: whether it is better to collect many relevance judgments for a
few topics or a few judgments for many topics. We consider a number of factors
impacting this trade-off: how topics are selected, topic familiarity to judges,
and how topic generation cost may impact both budget utilization and the
resultant quality of judgments. Experiments on NIST TREC Robust 2003 and Robust
2004 test collections show not only our method's ability to reliably evaluate
IR systems using fewer topics, but also that when topics are intelligently
selected, deep judging is often more cost-effective than shallow judging in
achieving the same level of evaluation reliability. Topic familiarity and
construction costs are also seen to notably impact the evaluation cost vs.
reliability tradeoff and provide further evidence supporting deep judging in
practice."
"Item recommendation task predicts a personalized ranking over a set of items
for individual user. One paradigm is the rating-based methods that concentrate
on explicit feedbacks and hence face the difficulties in collecting them.
Meanwhile, the ranking-based methods are presented with rated items and then
rank the rated above the unrated. This paradigm uses widely available implicit
feedback but it usually ignores some important information: item reviews. Item
reviews not only justify the preferences of users, but also help alleviate the
cold-start problem that fails the collaborative filtering. In this paper, we
propose two novel and simple models to integrate item reviews into matrix
factorization based Bayesian personalized ranking (BPR-MF). In each model, we
make use of text features extracted from item reviews via word embeddings. On
top of text features we uncover the review dimensions that explain the
variation in users' feedback and these review factors represent a prior
preference of a user. Experiments on real-world data sets show the benefits of
leveraging item reviews on ranking prediction. We also conduct analyses to
understand the proposed models."
"Searching patients based on the relevance of their medical records is
challenging because of the inherent implicit knowledge within the patients'
medical records and queries. Such knowledge is known to the medical
practitioners but may be hidden from a search system. For example, when
searching for the patients with a heart disease, medical practitioners commonly
know that patients who are taking the amiodarone medicine are relevant, since
this drug is used to combat heart disease. In this article, we argue that
leveraging such implicit knowledge improves the retrieval effectiveness, since
it provides new evidence to infer the relevance of patients' medical records
towards a query. Specifically, built upon existing conceptual representation
for both medical records and queries, we proposed a novel expansion of queries
that infers additional conceptual relationships from domain-specific resources
as well as by extracting informative concepts from the top-ranked patients'
medical records. We evaluate the retrieval effectiveness of our proposed
approach in the context of the TREC 2011 and 2012 Medical Records track. Our
results show the effectiveness of our approach to model the implicit knowledge
in patient search, whereby the retrieval performance is significantly improved
over both an effective conceptual representation baseline and an existing
semantic query expansion baseline. In addition, we provide an analysis of the
types of queries that the proposed approach is likely to be effective."
"Long-term Web archives comprise Web documents gathered over longer time
periods and can easily reach hundreds of terabytes in size. Semantic
annotations such as named entities can facilitate intelligent access to the Web
archive data. However, the annotation of the entire archive content on this
scale is often infeasible. The most efficient way to access the documents
within Web archives is provided through their URLs, which are typically stored
in dedicated index files.The URLs of the archived Web documents can contain
semantic information and can offer an efficient way to obtain initial semantic
annotations for the archived documents. In this paper, we analyse the
applicability of semantic analysis techniques such as named entity extraction
to the URLs in a Web archive. We evaluate the precision of the named entity
extraction from the URLs in the Popular German Web dataset and analyse the
proportion of the archived URLs from 1,444 popular domains in the time interval
from 2000 to 2012 to which these techniques are applicable. Our results
demonstrate that named entity recognition can be successfully applied to a
large number of URLs in our Web archive and provide a good starting point to
efficiently annotate large scale collections of Web documents."
"In the area of ad-targeting, predicting user responses is essential for many
applications such as Real-Time Bidding (RTB). Many of the features available in
this domain are sparse categorical features. This presents a challenge
especially when the user responses to be predicted are rare, because each
feature will only have very few positive examples. Recently, neural embedding
techniques such as word2vec which learn distributed representations of words
using occurrence statistics in the corpus have been shown to be effective in
many Natural Language Processing tasks. In this paper, we use real-world data
set to show that a similar technique can be used to learn distributed
representations of features from users' web history, and that such
representations can be used to improve the accuracy of commonly used models for
predicting rare user responses."
"Limited search and access patterns over Web archives have been well
documented. One of the key reasons is the lack of understanding of the user
access patterns over such collections, which in turn is attributed to the lack
of effective search interfaces. Current search interfaces for Web archives are
(a) either purely navigational or (b) have sub-optimal search experience due to
ineffective retrieval models or query modeling. We identify that external
longitudinal resources, such as social bookmarking data, are crucial sources to
identify important and popular websites in the past. To this extent we present
Tempas, a tag-based temporal search engine for Web archives.
  Websites are posted at specific times of interest on several external
platforms, such as bookmarking sites like Delicious. Attached tags not only act
as relevant descriptors useful for retrieval, but also encode the time of
relevance. With Tempas we tackle the challenge of temporally searching a Web
archive by indexing tags and time. We allow temporal selections for search
terms, rank documents based on their popularity and also provide meaningful
query recommendations by exploiting tag-tag and tag-document co-occurrence
statistics in arbitrary time windows. Finally, Tempas operates as a fairly
non-invasive indexing framework. By not dealing with contents from the actual
Web archive it constitutes an attractive and low-overhead approach for quick
access into Web archives."
"Web archives are large longitudinal collections that store webpages from the
past, which might be missing on the current live Web. Consequently, temporal
search over such collections is essential for finding prominent missing
webpages and tasks like historical analysis. However, this has been challenging
due to the lack of popularity information and proper ground truth to evaluate
temporal retrieval models. In this paper we investigate the applicability of
external longitudinal resources to identify important and popular websites in
the past and analyze the social bookmarking service Delicious for this purpose.
  The timestamped bookmarks on Delicious provide explicit cues about popular
time periods in the past along with relevant descriptors. These are valuable to
identify important documents in the past for a given temporal query. Focusing
purely on recall, we analyzed more than 12,000 queries and find that using
Delicious yields average recall values from 46% up to 100%, when limiting
ourselves to the best represented queries in the considered dataset. This
constitutes an attractive and low-overhead approach for quick access into Web
archives by not dealing with the actual contents."
"Top-$N$ recommender systems typically utilize side information to address the
problem of data sparsity. As nowadays side information is growing towards high
dimensionality, the performances of existing methods deteriorate in terms of
both effectiveness and efficiency, which imposes a severe technical challenge.
In order to take advantage of high-dimensional side information, we propose in
this paper an embedded feature selection method to facilitate top-$N$
recommendation. In particular, we propose to learn feature weights of side
information, where zero-valued features are naturally filtered out. We also
introduce non-negativity and sparsity to the feature weights, to facilitate
feature selection and encourage low-rank structure. Two optimization problems
are accordingly put forward, respectively, where the feature selection is
tightly or loosely coupled with the learning procedure. Augmented Lagrange
Multiplier and Alternating Direction Method are applied to efficiently solve
the problems. Experiment results demonstrate the superior recommendation
quality of the proposed algorithm to that of the state-of-the-art alternatives."
"Traditionally a document is visualized by a word cloud. Recently, distributed
representation methods for documents have been developed, which map a document
to a set of topic embeddings. Visualizing such a representation is useful to
present the semantics of a document in higher granularity; it is also
challenging, as there are multiple topics, each containing multiple words. We
propose to visualize a set of topics using Topic Cloud, which is a pie chart
consisting of topic slices, where each slice contains important words in this
topic. To make important topics/words visually prominent, the sizes of topic
slices and word fonts are proportional to their importance in the document. A
topic cloud can help the user quickly evaluate the quality of derived document
representations. For NLP practitioners, It can be used to qualitatively compare
the topic quality of different document representation algorithms, or to
inspect how model parameters impact the derived representations."
"One of the most used approaches for providing recommendations in various
online environments such as e-commerce is collaborative filtering. Although,
this is a simple method for recommending items or services, accuracy and
quality problems still exist. Thus, we propose a dynamic multi-level
collaborative filtering method that improves the quality of the
recommendations. The proposed method is based on positive and negative
adjustments and can be used in different domains that utilize collaborative
filtering to increase the quality of the user experience. Furthermore, the
effectiveness of the proposed method is shown by providing an extensive
experimental evaluation based on three real datasets and by comparisons to
alternative methods."
"The continuously increasing cost of the US healthcare system has received
significant attention. Central to the ideas aimed at curbing this trend is the
use of technology, in the form of the mandate to implement electronic health
records (EHRs). EHRs consist of patient information such as demographics,
medications, laboratory test results, diagnosis codes and procedures. Mining
EHRs could lead to improvement in patient health management as EHRs contain
detailed information related to disease prognosis for large patient
populations. In this manuscript, we provide a structured and comprehensive
overview of data mining techniques for modeling EHR data. We first provide a
detailed understanding of the major application areas to which EHR mining has
been applied and then discuss the nature of EHR data and its accompanying
challenges. Next, we describe major approaches used for EHR mining, the metrics
associated with EHRs, and the various study designs. With this foundation, we
then provide a systematic and methodological organization of existing data
mining techniques used to model EHRs and discuss ideas for future research. We
conclude this survey with a comprehensive summary of clinical data mining
applications of EHR data, as illustrated in the online supplement."
"In this paper we examine the existence of correlation between movie
similarity and low level features from respective movie content. In particular,
we demonstrate the extraction of multi-modal representation models of movies
based on subtitles, audio and metadata mining. We emphasize our research in
topic modeling of movies based on their subtitles. In order to demonstrate the
proposed content representation approach, we have built a small dataset of 160
widely known movies. We assert movie similarities, as propagated by the
singular modalities and fusion models, in the form of recommendation rankings.
We showcase a novel topic model browser for movies that allows for exploration
of the different aspects of similarities between movies and an information
retrieval system for movie similarity based on multi-modal content."
"In recent years, the information retrieval (IR) community has witnessed the
first successful applications of deep neural network models to short-text
matching and ad-hoc retrieval. It is exciting to see the research on deep
neural networks and IR converge on these tasks of shared interest. However, the
two communities have less in common when it comes to the choice of programming
languages. Indri, an indexing framework popularly used by the IR community, is
written in C++, while Torch, a popular machine learning library for deep
learning, is written in the light-weight scripting language Lua. To bridge this
gap, we introduce Luandri (pronounced ""laundry""), a simple interface for
exposing the search capabilities of Indri to Torch models implemented in Lua."
"Related Pins is the Web-scale recommender system that powers over 40% of user
engagement on Pinterest. This paper is a longitudinal study of three years of
its development, exploring the evolution of the system and its components from
prototypes to present state. Each component was originally built with many
constraints on engineering effort and computational resources, so we
prioritized the simplest and highest-leverage solutions. We show how organic
growth led to a complex system and how we managed this complexity. Many
challenges arose while building this system, such as avoiding feedback loops,
evaluating performance, activating content, and eliminating legacy heuristics.
Finally, we offer suggestions for tackling these challenges when engineering
Web-scale recommender systems."
"For tackling the well known cold-start user problem in model-based
recommender systems, one approach is to recommend a few items to a cold-start
user and use the feedback to learn a profile. The learned profile can then be
used to make good recommendations to the cold user. In the absence of a good
initial profile, the recommendations are like random probes, but if not chosen
judiciously, both bad recommendations and too many recommendations may turn off
a user. We formalize the cold-start user problem by asking what are the $b$
best items we should recommend to a cold-start user, in order to learn her
profile most accurately, where $b$, a given budget, is typically a small
number. We formalize the problem as an optimization problem and present
multiple non-trivial results, including NP-hardness as well as hardness of
approximation. We furthermore show that the objective function, i.e., the least
square error of the learned profile w.r.t. the true user profile, is neither
submodular nor supermodular, suggesting efficient approximations are unlikely
to exist. Finally, we discuss several scalable heuristic approaches for
identifying the $b$ best items to recommend to the user and experimentally
evaluate their performance on 4 real datasets. Our experiments show that our
proposed accelerated algorithms significantly outperform the prior art in
runnning time, while achieving similar error in the learned user profile as
well as in the rating predictions."
"Collaborative filtering (CF) is a powerful recommender system that generates
a list of recommended items for an active user based on the ratings of similar
users. This paper presents a novel approach to CF by first finding the set of
users similar to the active user by adopting self-organizing maps (SOM),
followed by k-means clustering. Then, the ratings for each item in the cluster
closest to the active user are mapped to the frequency domain using the
Discrete Fourier Transform (DFT). The power spectra of the mapped ratings are
generated, and a new similarity measure based on the coherence of these power
spectra is calculated. The proposed similarity measure is more time efficient
than current state-of-the-art measures. Moreover, it can capture the global
similarity between the profiles of users. Experimental results show that the
proposed approach overcomes the major problems in existing CF algorithms as
follows: First, it mitigates the scalability problem by creating clusters of
similar users and applying the time-efficient similarity measure. Second, its
frequency-based similarity measure is less sensitive to sparsity problems
because the DFT performs efficiently even with sparse data. Third, it
outperforms standard similarity measures in terms of accuracy."
"Entity information network is used to describe structural relationships
between entities. Taking advantage of its extension and heterogeneity, entity
information network is more and more widely applied to relationship modeling.
Recent years, lots of researches about entity information network modeling have
been proposed, while seldom of them concentrate on equipment-standard system
with properties of multi-layer, multi-dimension and multi-scale. In order to
efficiently deal with some complex issues in equipment-standard system such as
standard revising, standard controlling, and production designing, a
heterogeneous information network model for equipment-standard system is
proposed in this paper. Three types of entities and six types of relationships
are considered in the proposed model. Correspondingly, several different
similarity-measuring methods are used in the modeling process. The experiments
show that the heterogeneous information network model established in this paper
can reflect relationships between entities accurately. Meanwhile, the modeling
process has a good performance on time consumption."
"With hundreds, even thousands, of hotels to choose from at every destination,
it's difficult to know which will suit your personal preferences. Expedia wants
to take the proverbial rabbit hole out of hotel search by providing
personalized hotel recommendations to their users. This is no small task for a
site with hundreds of millions of visitors every month! Currently, Expedia uses
search parameters to adjust their hotel recommendations, but there aren't
enough customer specific data to personalize them for each user. In this
project, we have taken up the challenge to contextualize customer data and
predict the likelihood a user will stay at 100 different hotel groups."
"Recommender systems have been actively and extensively studied over past
decades. In the meanwhile, the boom of Big Data is driving fundamental changes
in the development of recommender systems. In this paper, we propose a dynamic
intention-aware recommender system to better facilitate users to find desirable
products and services. Compare to prior work, our proposal possesses the
following advantages: (1) it takes user intentions and demands into account
through intention mining techniques. By unearthing user intentions from the
historical user-item interactions, and various user digital traces harvested
from social media and Internet of Things, it is capable of delivering more
satisfactory recommendations by leveraging rich online and offline user data;
(2) it embraces the benefits of embedding heterogeneous source information and
shared representations of multiple domains to provide accurate and effective
recommendations comprehensively; (3) it recommends products or services
proactively and timely by capturing the dynamic influences, which can
significantly reduce user involvements and efforts."
"Due to the availability of references of research papers and the rich
information contained in papers, various citation analysis approaches have been
proposed to identify similar documents for scholar recommendation. Despite of
the success of previous approaches, they are, however, based on co-occurrence
of items. Once there are no co-occurrence items available in documents, they
will not work well. Inspired by distributed representations of words in the
literature of natural language processing, we propose a novel approach to
measuring the similarity of papers based on distributed representations learned
from the citation context of papers. We view the set of papers as the
vocabulary, define the weighted citation context of papers, and convert it to
weight matrix similar to the word-word cooccurrence matrix in natural language
processing. After that we explore a variant of matrix factorization approach to
train distributed representations of papers on the matrix, and leverage the
distributed representations to measure similarities of papers. In the
experiment, we exhibit that our approach outperforms state-of-theart
citation-based approaches by 25%, and better than other distributed
representation based methods."
"For the past few years, we used Apache Lucene as recommendation frame-work in
our scholarly-literature recommender system of the reference-management
software Docear. In this paper, we share three lessons learned from our work
with Lucene. First, recommendations with relevance scores below 0.025 tend to
have significantly lower click-through rates than recommendations with
relevance scores above 0.025. Second, by picking ten recommendations randomly
from Lucene's top50 search results, click-through rate decreased by 15%,
compared to recommending the top10 results. Third, the number of returned
search results tend to predict how high click-through rates will be: when
Lucene returns less than 1,000 search results, click-through rates tend to be
around half as high as if 1,000+ results are returned."
"While user-modeling and recommender systems successfully utilize items like
emails, news, and movies, they widely neglect mind-maps as a source for user
modeling. We consider this a serious shortcoming since we assume user modeling
based on mind maps to be equally effective as user modeling based on other
items. Hence, millions of mind-mapping users could benefit from user-modeling
applications such as recommender systems. The objective of this doctoral thesis
is to develop an effective user-modeling approach based on mind maps. To
achieve this objective, we integrate a recommender system in our mind-mapping
and reference-management software Docear. The recommender system builds user
models based on the mind maps, and recommends research papers based on the user
models. As part of our research, we identify several variables relating to
mind-map-based user modeling, and evaluate the variables' impact on
user-modeling effectiveness with an offline evaluation, a user study, and an
online evaluation based on 430,893 recommendations displayed to 4,700 users. We
find, among others, that the number of analyzed nodes, modification time,
visibility of nodes, relations between nodes, and number of children and
siblings of a node affect the effectiveness of user modeling. When all
variables are combined in a favorable way, this novel approach achieves
click-through rates of 7.20%, which is nearly twice as effective as the best
baseline. In addition, we show that user modeling based on mind maps performs
about as well as user modeling based on other items, namely the research
articles users downloaded or cited. Our findings let us to conclude that user
modeling based on mind maps is a promising research field, and that developers
of mind-mapping applications should integrate recommender systems into their
applications. Such systems could create additional value for millions of
mind-mapping users."
"Wikipedia, rich in entities and events, is an invaluable resource for various
knowledge harvesting, extraction and mining tasks. Numerous resources like
DBpedia, YAGO and other knowledge bases are based on extracting entity and
event based knowledge from it. Online news, on the other hand, is an
authoritative and rich source for emerging entities, events and facts relating
to existing entities. In this work, we study the creation of entities in
Wikipedia with respect to news by studying how entity and event based
information flows from news to Wikipedia.
  We analyze the lag of Wikipedia (based on the revision history of the English
Wikipedia) with 20 years of \emph{The New York Times} dataset (NYT). We model
and analyze the lag of entities and events, namely their first appearance in
Wikipedia and in NYT, respectively. In our extensive experimental analysis, we
find that almost 20\% of the external references in entity pages are news
articles encoding the importance of news to Wikipedia. Second, we observe that
the entity-based lag follows a normal distribution with a high standard
deviation, whereas the lag for news-based events is typically very low.
Finally, we find that events are responsible for creation of emergent entities
with as many as 12\% of the entities mentioned in the event page are created
after the creation of the event page."
"The increasing amount of data on the Web, in particular of Linked Data, has
led to a diverse landscape of datasets, which make entity retrieval a
challenging task. Explicit cross-dataset links, for instance to indicate
co-references or related entities can significantly improve entity retrieval.
However, only a small fraction of entities are interlinked through explicit
statements. In this paper, we propose a two-fold entity retrieval approach. In
a first, offline preprocessing step, we cluster entities based on the
\emph{x--means} and \emph{spectral} clustering algorithms. In the second step,
we propose an optimized retrieval model which takes advantage of our
precomputed clusters. For a given set of entities retrieved by the BM25F
retrieval approach and a given user query, we further expand the result set
with relevant entities by considering features of the queries, entities and the
precomputed clusters. Finally, we re-rank the expanded result set with respect
to the relevance to the query. We perform a thorough experimental evaluation on
the Billions Triple Challenge (BTC12) dataset. The proposed approach shows
significant improvements compared to the baseline and state of the art
approaches."
"Research on recommender systems is a challenging task, as is building and
operating such systems. Major challenges include non-reproducible research
results, dealing with noisy data, and answering many questions such as how many
recommendations to display, how often, and, of course, how to generate
recommendations most effectively. In the past six years, we built three
research-article recommender systems for digital libraries and reference
managers, and conducted research on these systems. In this paper, we share some
experiences we made during that time. Among others, we discuss the required
skills to build recommender systems, and why the literature provides little
help in identifying promising recommendation approaches. We explain the
challenge in creating a randomization engine to run A/B tests, and how low data
quality impacts the calculation of bibliometrics. We further discuss why
several of our experiments delivered disappointing results, and provide
statistics on how many researchers showed interest in our recommendation
dataset."
"We investigate the problem of choice overload - the difficulty of making a
decision when faced with many options - when displaying related-article
recommendations in digital libraries. So far, research regarding to how many
items should be displayed has mostly been done in the fields of media
recommendations and search engines. We analyze the number of recommendations in
current digital libraries. When browsing fullscreen with a laptop or desktop
PC, all display a fixed number of recommendations. 72% display three, four, or
five recommendations, none display more than ten. We provide results from an
empirical evaluation conducted with GESIS' digital library Sowiport, with
recommendations delivered by recommendations-as-a-service provider Mr. DLib. We
use click-through rate as a measure of recommendation effectiveness based on
3.4 million delivered recommendations. Our results show lower click-through
rates for higher numbers of recommendations and twice as many clicked
recommendations when displaying ten instead of one related-articles. Our
results indicate that users might quickly feel overloaded by choice."
"Collaborative filtering (CF) has been successfully used to provide users with
personalized products and services. However, dealing with the increasing
sparseness of user-item matrix still remains a challenge. To tackle such issue,
hybrid CF such as combining with content based filtering and leveraging side
information of users and items has been extensively studied to enhance
performance. However, most of these approaches depend on hand-crafted feature
engineering, which are usually noise-prone and biased by different feature
extraction and selection schemes. In this paper, we propose a new hybrid model
by generalizing contractive auto-encoder paradigm into matrix factorization
framework with good scalability and computational efficiency, which jointly
model content information as representations of effectiveness and compactness,
and leverage implicit user feedback to make accurate recommendations. Extensive
experiments conducted over three large scale real datasets indicate the
proposed approach outperforms the compared methods for item recommendation."
"Online marketplaces, search engines, and databases employ aggregated social
information to rank their content for users. Two ranking heuristics commonly
implemented to order the available options are the average review score and
item popularity-that is, the number of users who have experienced an item.
These rules, although easy to implement, only partly reflect actual user
preferences, as people may assign values to both average scores and popularity
and trade off between the two. How do people integrate these two pieces of
social information when making choices? We present two experiments in which we
asked participants to choose 200 times among options drawn directly from two
widely used online venues: Amazon and IMDb. The only information presented to
participants was the average score and the number of reviews, which served as a
proxy for popularity. We found that most people are willing to settle for items
with somewhat lower average scores if they are more popular. Yet, our study
uncovered substantial diversity of preferences among participants, which
indicates a sizable potential for personalizing ranking schemes that rely on
social information."
"According to the principle of polyrepresentation, retrieval accuracy may
improve through the combination of multiple and diverse information object
representations about e.g. the context of the user, the information sought, or
the retrieval system. Recently, the principle of polyrepresentation was
mathematically expressed using subjective logic, where the potential
suitability of each representation for improving retrieval performance was
formalised through degrees of belief and uncertainty. No experimental evidence
or practical application has so far validated this model. We extend the work of
Lioma et al. (2010), by providing a practical application and analysis of the
model. We show how to map the abstract notions of belief and uncertainty to
real-life evidence drawn from a retrieval dataset. We also show how to estimate
two different types of polyrepresentation assuming either (a) independence or
(b) dependence between the information objects that are combined. We focus on
the polyrepresentation of different types of context relating to user
information needs (i.e. work task, user background knowledge, ideal answer) and
show that the subjective logic model can predict their optimal combination
prior and independently to the retrieval process."
"Interactive Information Retrieval refers to the branch of Information
Retrieval that considers the retrieval process with respect to a wide range of
contexts, which may affect the user's information seeking experience. The
identification and representation of such contexts has been the object of the
principle of Polyrepresentation, a theoretical framework for reasoning about
different representations arising from interactive information retrieval in a
given context. Although the principle of Polyrepresentation has received
attention from many researchers, not much empirical work has been done based on
it. One reason may be that it has not yet been formalised mathematically. In
this paper we propose an up-to-date and exible mathematical formalisation of
the principle of Polyrepresentation for information needs. Specifically, we
apply Subjective Logic to model different representations of information needs
as beliefs marked by degrees of uncertainty. We combine such beliefs using
different logical operators, and we discuss these combinations with respect to
different retrieval scenarios and situations. A formal model is introduced and
discussed, with illustrative applications to the modelling of information
needs."
"Automatic language processing tools typically assign to terms so-called
weights corresponding to the contribution of terms to information content.
Traditionally, term weights are computed from lexical statistics, e.g., term
frequencies. We propose a new type of term weight that is computed from part of
speech (POS) n-gram statistics. The proposed POS-based term weight represents
how informative a term is in general, based on the POS contexts in which it
generally occurs in language. We suggest five different computations of
POS-based term weights by extending existing statistical approximations of term
information measures. We apply these POS-based term weights to information
retrieval, by integrating them into the model that matches documents to
queries. Experiments with two TREC collections and 300 queries, using TF-IDF &
BM25 as baselines, show that integrating our POS-based term weights to
retrieval always leads to gains (up to +33.7% from the baseline). Additional
experiments with a different retrieval model as baseline (Language Model with
Dirichlet priors smoothing) and our best performing POS-based term weight, show
retrieval gains always and consistently across the whole smoothing range of the
baseline."
"The ECIR half-day workshop on Task-Based and Aggregated Search (TBAS) was
held in Barcelona, Spain on 1 April 2012. The program included a keynote talk
by Professor Jarvelin, six full paper presentations, two poster presentations,
and an interactive discussion among the approximately 25 participants. This
report overviews the aims and contents of the workshop and outlines the major
outcomes."
"TextRank is a variant of PageRank typically used in graphs that represent
documents, and where vertices denote terms and edges denote relations between
terms. Quite often the relation between terms is simple term co-occurrence
within a fixed window of k terms. The output of TextRank when applied
iteratively is a score for each vertex, i.e. a term weight, that can be used
for information retrieval (IR) just like conventional term frequency based term
weights. So far, when computing TextRank term weights over co- occurrence
graphs, the window of term co-occurrence is al- ways ?xed. This work departs
from this, and considers dy- namically adjusted windows of term co-occurrence
that fol- low the document structure on a sentence- and paragraph- level. The
resulting TextRank term weights are used in a ranking function that re-ranks
1000 initially returned search results in order to improve the precision of the
ranking. Ex- periments with two IR collections show that adjusting the vicinity
of term co-occurrence when computing TextRank term weights can lead to gains in
early precision."
"Using only implicit data, many recommender systems fail in general to provide
a precise set of recommendations to users with limited interaction history.
This issue is regarded as the ""Cold Start"" problem and is typically resolved by
switching to content-based approaches where extra costly information is
required. In this paper, we use a dimensionality reduction algorithm, Word2Vec
(W2V), originally applied in Natural Language Processing problems under the
framework of Collaborative Filtering (CF) to tackle the ""Cold Start"" problem
using only implicit data. This combined method is named Embedded Collaborative
Filtering (ECF). An experiment is conducted to determine the performance of ECF
on two different implicit data sets. We show that the ECF approach outperforms
other popular and state-of-the-art approaches in ""Cold Start"" scenarios."
"Many social network applications depend on robust representations of
spatio-temporal data. In this work, we present an embedding model based on
feed-forward neural networks which transforms social media check-ins into dense
feature vectors encoding geographic, temporal, and functional aspects for
modelling places, neighborhoods, and users. On the basis of this model, we
further propose a Spatio-Temporal Embedding Similarity algorithm (STES) for
location recommendation.
  In a range of experiments on real life data collected from Foursquare,
Twitter, and Gowalla, we demonstrate our model's effectiveness at
characterizing places and people, resulting in a significantly increased
recommendation and classification performance as compared to the state of the
art. Finally, we select eight major cities around the globe and verify the
robustness and generality of our model by porting pre-trained models from one
city to another, and thereby alleviating the need for costly local training."
"Modern social networks have become sources for vast quantities of data.
Having access to such big data can be very useful for various researchers and
data scientists. In this paper we describe Loklak, an open source distributed
peer to peer crawler and scraper for supporting such research on platforms like
Twitter, Weibo and other social networks. Social networks such as Twitter and
Weibo pose various limitations to the user on the rate at which one could
freely collect such data for research. Our crawler enables researchers to
continuously collect data while overcoming the barriers of authentication and
rate limits imposed to provide a repository of open data as a service."
"Scalable web search systems typically employ multi-stage retrieval
architectures, where an initial stage generates a set of candidate documents
that are then pruned and re-ranked. Since subsequent stages typically exploit a
multitude of features of varying costs using machine-learned models, reducing
the number of documents that are considered at each stage improves latency. In
this work, we propose and validate a unified framework that can be used to
predict a wide range of performance-sensitive parameters which minimize
effectiveness loss, while simultaneously minimizing query latency, across all
stages of a multi-stage search architecture. Furthermore, our framework can be
easily applied in large-scale IR systems, can be trained without explicitly
requiring relevance judgments, and can target a variety of different
efficiency-effectiveness trade-offs, making it well suited to a wide range of
search scenarios. Our results show that we can reliably predict a number of
different parameters on a per-query basis, while simultaneously detecting and
minimizing the likelihood of tail-latency queries that exceed a pre-specified
performance budget. As a proof of concept, we use the prediction framework to
help alleviate the problem of tail-latency queries in early stage retrieval. On
the standard ClueWeb09B collection and 31k queries, we show that our new hybrid
system can reliably achieve a maximum query time of 200 ms with a 99.99%
response time guarantee without a significant loss in overall effectiveness.
The solutions presented are practical, and can easily be used in large-scale
distributed search engine deployments with a small amount of additional
overhead."
"Search engines play an important role in our everyday lives by assisting us
in finding the information we need. When we input a complex query, however,
results are often far from satisfactory. In this work, we introduce a query
reformulation system based on a neural network that rewrites a query to
maximize the number of relevant documents returned. We train this neural
network with reinforcement learning. The actions correspond to selecting terms
to build a reformulated query, and the reward is the document recall. We
evaluate our approach on three datasets against strong baselines and show a
relative improvement of 5-20% in terms of recall. Furthermore, we present a
simple method to estimate a conservative upper-bound performance of a model in
a particular environment and verify that there is still large room for
improvements."
"The task of next POI recommendation has been studied extensively in recent
years. However, developing an unified recommendation framework to incorporate
multiple factors associated with both POIs and users remains challenging,
because of the heterogeneity nature of these information. Further, effective
mechanisms to handle cold-start and endow the system with interpretability are
also difficult topics. Inspired by the recent success of neural networks in
many areas, in this paper, we present a simple but effective neural network
framework for next POI recommendation, named NEXT. NEXT is an unified framework
to learn the hidden intent regarding user's next move, by incorporating
different factors in an unified manner. Specifically, in NEXT, we incorporate
meta-data information and two kinds of temporal contexts (i.e., time interval
and visit time). To leverage sequential relations and geographical influence,
we propose to adopt DeepWalk, a network representation learning technique, to
encode such knowledge. We evaluate the effectiveness of NEXT against
state-of-the-art alternatives and neural networks based solutions. Experimental
results over three publicly available datasets demonstrate that NEXT
significantly outperforms baselines in real-time next POI recommendation.
Further experiments demonstrate the superiority of NEXT in handling cold-start.
More importantly, we show that NEXT provides meaningful explanation of the
dimensions in hidden intent space."
"We design a recommender system for research papers based on topic-modeling.
The users feedback to the results is used to make the results more relevant the
next time they fire a query. The user's needs are understood by observing the
change in the themes that the user shows a preference for over time."
"Enticing users into exploring Open Data remains an important challenge for
the whole Open Data paradigm. Standard stock interfaces often used by Open Data
portals are anything but inspiring even for tech-savvy users, let alone those
without an articulated interest in data science. To address a broader range of
citizens, we designed an open data search interface supporting natural language
interactions via popular platforms like Facebook and Skype. Our data-aware
chatbot answers search requests and suggests relevant open datasets, bringing
fun factor and a potential of viral dissemination into Open Data exploration.
The current system prototype is available for Facebook
(https://m.me/OpenDataAssistant) and Skype
(https://join.skype.com/bot/6db830ca-b365-44c4-9f4d-d423f728e741) users."
"Neural ranking models for information retrieval (IR) use shallow or deep
neural networks to rank search results in response to a query. Traditional
learning to rank models employ machine learning techniques over hand-crafted IR
features. By contrast, neural models learn representations of language from raw
text that can bridge the gap between query and document vocabulary. Unlike
classical IR models, these new machine learning based approaches are
data-hungry, requiring large scale training data before they can be deployed.
This tutorial introduces basic concepts and intuitions behind neural IR models,
and places them in the context of traditional retrieval models. We begin by
introducing fundamental concepts of IR and different neural and non-neural
approaches to learning vector representations of text. We then review shallow
neural IR methods that employ pre-trained neural term embeddings without
learning the IR task end-to-end. We introduce deep neural networks next,
discussing popular deep architectures. Finally, we review the current DNN
models for information retrieval. We conclude with a discussion on potential
future directions for neural IR."
"Collaborative filtering (CF) is one of the most efficient ways for
recommender systems. Typically, CF-based algorithms analyze users' preferences
and items' attributes using one of two types of feedback: \textit{explicit
feedback} (e.g., ratings given to item by users, like/dislike) or
\textit{implicit feedback} (e.g., clicks, views, purchases). Explicit feedback
is reliable but is extremely sparse; whereas implicit feedback is abundant but
is not reliable. To leverage the sparsity of explicit feedback, in this paper,
we propose a model that efficiently combines explicit and implicit feedback in
a unified model for rating prediction. This model is a combination of
\textit{matrix factorization} and \textit{item embedding}, a similar concept
with word-embedding in natural language processing. The experiments on three
real-datasets (\textit{Movilens 1M}, \textit{Movielens 20M}, and
\textit{Bookcrossing}) demonstrate that our method can efficiently predict
ratings for items even if the ratings data is not available for them. The
experimental results also show that our method outperforms competing methods on
rating prediction task in general as well as for users and items which have few
ratings."
"Corporations spend millions of dollars on developing creative image-based
promotional content to advertise to their user-base on platforms like Twitter.
Our paper is an initial study, where we propose a novel method to evaluate and
improve outreach of promotional images from corporations on Twitter, based
purely on their describable aesthetic attributes. Existing works in aesthetic
based image analysis exclusively focus on the attributes of digital
photographs, and are not applicable to advertisements due to the influences of
inherent content and context based biases on outreach.
  Our paper identifies broad categories of biases affecting such images,
describes a method for normalization to eliminate effects of those biases and
score images based on their outreach, and examines the effects of certain
handcrafted describable aesthetic features on image outreach. Optimizing on the
describable aesthetic features resulting from this research is a simple method
for corporations to complement their existing marketing strategy to gain
significant improvement in user engagement on social media for promotional
images."
"In order to disseminate the exponential extent of knowledge being produced in
the form of scientific publications, it would be best to design mechanisms that
connect it with already existing rich repository of concepts -- the Wikipedia.
Not only does it make scientific reading simple and easy (by connecting the
involved concepts used in the scientific articles to their Wikipedia
explanations) but also improves the overall quality of the article. In this
paper, we present a novel metapath based method, WikiM, to efficiently wikify
scientific abstracts -- a topic that has been rarely investigated in the
literature. One of the prime motivations for this work comes from the
observation that, wikified abstracts of scientific documents help a reader to
decide better, in comparison to the plain abstracts, whether (s)he would be
interested to read the full article. We perform mention extraction mostly
through traditional tf-idf measures coupled with a set of smart filters. The
entity linking heavily leverages on the rich citation and author publication
networks. Our observation is that various metapaths defined over these networks
can significantly enhance the overall performance of the system. For mention
extraction and entity linking, we outperform most of the competing
state-of-the-art techniques by a large margin arriving at precision values of
72.42% and 73.8% respectively over a dataset from the ACL Anthology Network. In
order to establish the robustness of our scheme, we wikify three other datasets
and get precision values of 63.41%-94.03% and 67.67%-73.29% respectively for
the mention extraction and the entity linking phase."
"With the advent of numerous community forums, tasks associated with the same
have gained importance in the recent past. With the influx of new questions
every day on these forums, the issues of identifying methods to find answers to
said questions, or even trying to detect duplicate questions, are of practical
importance and are challenging in their own right. This paper aims at surveying
some of the aforementioned issues, and methods proposed for tackling the same."
"Retrieving paragraphs to populate a Wikipedia article is a challenging task.
The new TREC Complex Answer Retrieval (TREC CAR) track introduces a
comprehensive dataset that targets this retrieval scenario. We present early
results from a variety of approaches -- from standard information retrieval
methods (e.g., tf-idf) to complex systems that using query expansion using
knowledge bases and deep neural networks. The goal is to offer future
participants of this track an overview of some promising approaches to tackle
this problem."
"We tackle the novel problem of navigational voice queries posed against an
entertainment system, where viewers interact with a voice-enabled remote
controller to specify the program to watch. This is a difficult problem for
several reasons: such queries are short, even shorter than comparable voice
queries in other domains, which offers fewer opportunities for deciphering user
intent. Furthermore, ambiguity is exacerbated by underlying speech recognition
errors. We address these challenges by integrating word- and character-level
representations of the queries and by modeling voice search sessions to capture
the contextual dependencies in query sequences. Both are accomplished with a
probabilistic framework in which recurrent and feedforward neural network
modules are organized in a hierarchical manner. From a raw dataset of 32M voice
queries from 2.5M viewers on the Comcast Xfinity X1 entertainment system, we
extracted data to train and test our models. We demonstrate the benefits of our
hybrid representation and context-aware model, which significantly outperforms
models without context as well as the current deployed product."
"Identifying the target types of entity-bearing queries can help improve
retrieval performance as well as the overall search experience. In this work,
we address the problem of automatically detecting the target types of a query
with respect to a type taxonomy. We propose a supervised learning approach with
a rich variety of features. Using a purpose-built test collection, we show that
our approach outperforms existing methods by a remarkable margin. This is an
extended version of the article published with the same title in the
Proceedings of SIGIR'17."
"The absence of an appropriate text classification corpus makes the massive
amount of online job information unusable for labor market analysis. This paper
presents JCTC, a large job posting corpus for text classification. In JCTC
construction framework, a formal specification issued by the Chinese central
government is chosen as the classification standard. The unsupervised learning
(WE-cos), supervised learning algorithm (SVM) and human judgements are all used
in the construction process. JCTC has 102581 online job postings distributed in
465 categories. The method proposed here can not only ameliorate the high
demands on people's skill and knowledge, but reduce the subjective influences
as well. Besides, the method is not limited in Chinese. We benchmark five
state-of-the-art deep learning approaches on JCTC providing baseline results
for future studies. JCTC might be the first job posting corpus for text
classification and the largest one in Chinese. With the help of JCTC, related
organizations are able to monitor, analyze and predict the labor market in a
comprehensive, accurate and timely manner."
"Tabular data is difficult to analyze and to search through, yielding for new
tools and interfaces that would allow even non tech-savvy users to gain
insights from open datasets without resorting to specialized data analysis
tools or even without having to fully understand the dataset structure. The
goal of our demonstration is to showcase answering natural language questions
from tabular data, and to discuss related system configuration and model
training aspects. Our prototype is publicly available and open-sourced (see
https://svakulenko.ai.wu.ac.at/tableqa)."
"Cross-modality retrieval encompasses retrieval tasks where the fetched items
are of a different type than the search query, e.g., retrieving pictures
relevant to a given text query. The state-of-the-art approach to cross-modality
retrieval relies on learning a joint embedding space of the two modalities,
where items from either modality are retrieved using nearest-neighbor search.
In this work, we introduce a neural network layer based on Canonical
Correlation Analysis (CCA) that learns better embedding spaces by analytically
computing projections that maximize correlation. In contrast to previous
approaches, the CCA layer allows us to combine existing objectives for
embedding space learning, such as pairwise ranking losses, with the optimal
projections of CCA. We show the effectiveness of our approach for
cross-modality retrieval on three datasets, surpassing both Deep CCA and a
multi-view network using freely learned projections optimized by a pairwise
ranking loss, especially when few training data is available."
"The paper discusses issues related to the use of faceted classifications in
an online environment. The author argues that knowledge organization systems
can be fully utilized in information retrieval only if they are exposed and
made available for machine processing. The experience with classification
automation to date may be used to speed up and ease the conversion of existing
faceted schemes or the creation of management tools for new systems. The author
suggests that it is possible to agree on a set of functional requirements for
supporting faceted classifications online that are equally relevant for the
maintenance of classifications, the creation of classification indexing tools,
or the management of classifications in an authority file. It is suggested that
a set of requirements for analytico-synthetic classifications may be put
forward to improve standards for the use and exchange of knowledge organization
systems."
"Recommender systems play an important role in many scenarios where users are
overwhelmed with too many choices to make. In this context, Collaborative
Filtering (CF) arises by providing a simple and widely used approach for
personalized recommendation. Memory-based CF algorithms mostly rely on
similarities between pairs of users or items, which are posteriorly employed in
classifiers like k-Nearest Neighbor (kNN) to generalize for unknown ratings. A
major issue regarding this approach is to build the similarity matrix.
Depending on the dimensionality of the rating matrix, the similarity
computations may become computationally intractable. To overcome this issue, we
propose to represent users by their distances to preselected users, namely
landmarks. This procedure allows to drastically reduce the computational cost
associated with the similarity matrix. We evaluated our proposal on two
distinct distinguishing databases, and the results showed our method has
consistently and considerably outperformed eight CF algorithms (including both
memory-based and model-based) in terms of computational performance."
"The vision of the Semantic Web (SW) is gradually unfolding and taking shape
through a web of linked data, a part of which is built by capturing semantics
stored in existing knowledge organization systems (KOS), subject metadata and
resource metadata. The content of vast bibliographic collections is currently
categorized by some widely used bibliographic classification and we may soon
see them being mined for information and linked in a meaningful way across the
Web. Bibliographic classifications are designed for knowledge mediation which
offers both a rich terminology and different ways in which concepts can be
categorized and related to each other in the universe of knowledge. From
1990-2010 they have been used in various resource discovery services on the Web
and continue to be used to support information integration in a number of
international digital library projects. In this chapter we will revisit some of
the ways in which universal classifications, as language independent concept
schemes, can assist humans and computers in structuring and presenting
information and formulating queries. Most importantly, we highlight issues
important to understanding bibliographic classifications, both in terms of
their unused potential and technical limitations."
"Making personalized and context-aware suggestions of venues to the users is
very crucial in venue recommendation. These suggestions are often based on
matching the venues' features with the users' preferences, which can be
collected from previously visited locations. In this paper we present a novel
user-modeling approach which relies on a set of scoring functions for making
personalized suggestions of venues based on venues content and reviews as well
as users context. Our experiments, conducted on the dataset of the TREC
Contextual Suggestion Track, prove that our methodology outperforms
state-of-the-art approaches by a significant margin."
"Many learning-to-rank (LtR) algorithms focus on query-independent model, in
which query and document do not lie in the same feature space, and the rankers
rely on the feature ensemble about query-document pair instead of the
similarity between query instance and documents. However, existing algorithms
do not consider local structures in query-document feature space, and are
fragile to irrelevant noise features. In this paper, we propose a novel
Riemannian metric learning algorithm to capture the local structures and
develop a robust LtR algorithm. First, we design a concept called \textit{ideal
candidate document} to introduce metric learning algorithm to query-independent
model. Previous metric learning algorithms aiming to find an optimal metric
space are only suitable for query-dependent model, in which query instance and
documents belong to the same feature space and the similarity is directly
computed from the metric space. Then we extend the new and extremely fast
global Geometric Mean Metric Learning (GMML) algorithm to develop a localized
GMML, namely L-GMML. Based on the combination of local learned metrics, we
employ the popular Normalized Discounted Cumulative Gain~(NDCG) scorer and
Weighted Approximate Rank Pairwise (WARP) loss to optimize the \textit{ideal
candidate document} for each query candidate set. Finally, we can quickly
evaluate all candidates via the similarity between the \textit{ideal candidate
document} and other candidates. By leveraging the ability of metric learning
algorithms to describe the complex structural information, our approach gives
us a principled and efficient way to perform LtR tasks. The experiments on
real-world datasets demonstrate that our proposed L-GMML algorithm outperforms
the state-of-the-art metric learning to rank methods and the stylish
query-independent LtR algorithms regarding accuracy and computational
efficiency."
"Automated music playlist generation is a specific form of music
recommendation. Generally stated, the user receives a set of song suggestions
defining a coherent listening session. We hypothesize that the best way to
convey such playlist coherence to new recommendations is by learning it from
actual curated examples, in contrast to imposing ad hoc constraints. Examples
of thoroughly curated playlists are rather scarce, especially compared to the
amount of listening logs available (e.g., in music streaming services).
Collaborative filtering methods can be used to capture underlying patterns in
hand-curated playlists, but their performance is severely affected by the size
of the data and the low number of song observations. We propose an alternative
method based on a song-to-playlist classifier, which learns the underlying
structure from actual playlist examples, while leveraging song features based
on audio, social tags and independent listening logs. Experiments on two
datasets of hand-curated playlists show competitive performance compared to
collaborative filtering when enough training data is available and also more
robust performance in cold-start situations. For example, both methods achieve
a recall@100 of roughly 35% for songs observed 5 or more times in the training
set, whereas the proposed model achieves a recall@100 of roughly 15% for songs
observed 4 or less times in the training set, compared to the 3% achieved by
collaborative filtering."
"We describe the results of a qualitative study on journalists' information
seeking behavior on social media. Based on interviews with eleven journalists
along with a study of a set of university level journalism modules, we
determined the categories of information need types that lead journalists to
social media. We also determined the ways that social media is exploited as a
tool to satisfy information needs and to define influential factors, which
impacted on journalists' information seeking behavior. We find that not only is
social media used as an information source, but it can also be a supplier of
stories found serendipitously. We find seven information need types that expand
the types found in previous work. We also find five categories of influential
factors that affect the way journalists seek information."
"Graph structured data on the web is now massive as well as diverse, ranging
from social networks, web graphs to knowledge-bases. Effectively querying this
graph structured data is non-trivial and has led to research in a variety of
directions -- structured queries, keyword and natural language queries,
automatic translation of these queries to structured queries, etc. We are
concerned with a class of queries called relationship queries, which are
usually expressed as a set of keywords (each keyword denoting a named entity).
The results returned are a set of ranked trees, each of which denotes
relationships among the various keywords. The result list could consist of
hundreds of answers. The problem of keyword search on graphs has been explored
for over a decade now, but an important aspect that is not as extensively
studied is that of user experience. We propose KlusTree, which presents
clustered results to the users instead of a list of all the results. In our
approach, the result trees are represented using language models and are
clustered using JS divergence as a distance measure. We compare KlusTree with
the well-known approaches based on isomorphism and tree-edit distance based
clustering. The user evaluations show that KlusTree outperforms the other two
in providing better clustering, thereby enriching user experience, revealing
interesting patterns and improving result interpretation by the user."
"Many online services, such as search engines, social media platforms, and
digital marketplaces, are advertised as being available to any user, regardless
of their age, gender, or other demographic factors. However, there are growing
concerns that these services may systematically underserve some groups of
users. In this paper, we present a framework for internally auditing such
services for differences in user satisfaction across demographic groups, using
search engines as a case study. We first explain the pitfalls of na\""ively
comparing the behavioral metrics that are commonly used to evaluate search
engines. We then propose three methods for measuring latent differences in user
satisfaction from observed differences in evaluation metrics. To develop these
methods, we drew on ideas from the causal inference literature and the
multilevel modeling literature. Our framework is broadly applicable to other
online services, and provides general insight into interpreting their
evaluation metrics."
"Online music services have tens of millions of tracks. The content itself is
broad and covers various musical genres as well as non-musical audio content
such as radio plays and podcasts. The sheer scale and diversity of content
makes it difficult for a user to find relevant tracks. Relevant recommendations
are therefore crucial for a good user experience. Here we present a method to
compute track-track similarities using collaborative filtering signals with
side information. On a data set from music streaming service SoundCloud, the
method here outperforms the widely adopted implicit matrix factorization
technique. The implementation of our method is open sourced and can be applied
to related item-item recommendation tasks with side information."
"Compression models represent an interesting approach for different
classification tasks and have been used widely across many research fields. We
adapt compression models to the field of authorship verification (AV), a branch
of digital text forensics. The task in AV is to verify if a questioned document
and a reference document of a known author are written by the same person. We
propose an intrinsic AV method, which yields competitive results compared to a
number of current state-of-the-art approaches, based on support vector machines
or neural networks. However, in contrast to these approaches our method does
not make use of machine learning algorithms, natural language processing
techniques, feature engineering, hyperparameter optimization or external
documents (a common strategy to transform AV from a one-class to a multi-class
classification problem). Instead, the only three key components of our method
are a compressing algorithm, a dissimilarity measure and a threshold, needed to
accept or reject the authorship of the questioned document. Due to its
compactness, our method performs very fast and can be reimplemented with
minimal effort. In addition, the method can handle complicated AV cases where
both, the questioned and the reference document, are not related to each other
in terms of topic or genre. We evaluated our approach against publicly
available datasets, which were used in three international AV competitions.
Furthermore, we constructed our own corpora, where we evaluated our method
against state-of-the-art approaches and achieved, in both cases, promising
results."
"Social media information distributes in different Online Social Networks
(OSNs). This paper addresses the problem integrating the cross-OSN information
to facilitate an immersive social media search experience. We exploit hashtag,
which is widely used to annotate and organize multi-modal items in different
OSNs, as the bridge for information aggregation and organization. A three-stage
solution framework is proposed for hashtag representation, clustering and
demonstration. Given an event query, the related items from three OSNs,
Twitter, Flickr and YouTube, are organized in cluster-hashtag-item hierarchy
for display. The effectiveness of the proposed solution is validated by
qualitative and quantitative experiments on hundreds of trending event queries."
"Vector representations and vector space modeling (VSM) play a central role in
modern machine learning. We propose a novel approach to `vector similarity
searching' over dense semantic representations of words and documents that can
be deployed on top of traditional inverted-index-based fulltext engines, taking
advantage of their robustness, stability, scalability and ubiquity.
  We show that this approach allows the indexing and querying of dense vectors
in text domains. This opens up exciting avenues for major efficiency gains,
along with simpler deployment, scaling and monitoring.
  The end result is a fast and scalable vector database with a tunable
trade-off between vector search performance and quality, backed by a standard
fulltext engine such as Elasticsearch.
  We empirically demonstrate its querying performance and quality by applying
this solution to the task of semantic searching over a dense vector
representation of the entire English Wikipedia."
"Recent years have witnessed some convergence in the architecture of entity
search systems driven by a knowledge graph (KG) and a corpus with annotated
entity mentions. However, each specific system has some limitations. We present
AQQUCN, an entity search system that combines the best design principles into a
public reference implementation. AQQUCN does not depend on well formed question
syntax, but works equally well with syntax poor keyword queries. It uses
several convolutional networks (convnets) to extract subtle, overlapping roles
of query words. Instead of ranking structured query interpretations, which are
then executed on the KG to return unranked sets, AQQUCN directly ranks response
entities, by closely integrating coarse grained predicates from the KG with
fine grained scoring from the corpus, into a single ranking model. Over and
above competitive F1 score, AQQUCN gets the best entity ranking accuracy on two
syntax rich and two syntax poor public query workloads amounting to over 8,000
queries, with 16 to 18 percent absolute improvement in mean average precision
(MAP), compared to recent systems."
"The session search task aims at best serving the user's information need
given her previous search behavior during the session. We propose an extended
relevance model that captures the user's dynamic information need in the
session. Our relevance modelling approach is directly driven by the user's
query reformulation (change) decisions and the estimate of how much the user's
search behavior affects such decisions. Overall, we demonstrate that, the
proposed approach significantly boosts session search performance."
"This paper presents an evaluation and an analysis of some selected
information retrieval models for Bengali monolingual information retrieval
task. Two models, TF-IDF model and the Okapi BM25 model have been considered
for our study. The developed IR models are tested on FIRE ad hoc retrieval data
sets released for different years from 2008 to 2012 and the obtained results
have been reported in this paper."
"Recommender-system datasets are used for recommender-system evaluations,
training machine-learning algorithms, and exploring user behavior. While there
are many datasets for recommender systems in the domains of movies, books, and
music, there are rather few datasets from research-paper recommender systems.
In this paper, we introduce RARD, the Related-Article Recommendation Dataset,
from the digital library Sowiport and the recommendation-as-a-service provider
Mr. DLib. The dataset contains information about 57.4 million recommendations
that were displayed to the users of Sowiport. Information includes details on
which recommendation approaches were used (e.g. content-based filtering,
stereotype, most popular), what types of features were used in content based
filtering (simple terms vs. keyphrases), where the features were extracted from
(title or abstract), and the time when recommendations were delivered and
clicked. In addition, the dataset contains an implicit item-item rating matrix
that was created based on the recommendation click logs. RARD enables
researchers to train machine learning algorithms for research-paper
recommendations, perform offline evaluations, and do research on data from Mr.
DLib's recommender system, without implementing a recommender system
themselves. In the field of scientific recommender systems, our dataset is
unique. To the best of our knowledge, there is no dataset with more (implicit)
ratings available, and that many variations of recommendation algorithms. The
dataset is available at http://data.mr-dlib.org, and published under the
Creative Commons Attribution 3.0 Unported (CC-BY) license."
"Improvements of entity-relationship (E-R) search techniques have been
hampered by a lack of test collections, particularly for complex queries
involving multiple entities and relationships. In this paper we describe a
method for generating E-R test queries to support comprehensive E-R search
experiments. Queries and relevance judgments are created from content that
exists in a tabular form where columns represent entity types and the table
structure implies one or more relationships among the entities. Editorial work
involves creating natural language queries based on relationships represented
by the entries in the table. We have publicly released the RELink test
collection comprising 600 queries and relevance judgments obtained from a
sample of Wikipedia List-of-lists-of-lists tables. The latter comprise tuples
of entities that are extracted from columns and labelled by corresponding
entity types and relationships they represent. In order to facilitate research
in complex E-R retrieval, we have created and released as open source the
RELink Framework that includes Apache Lucene indexing and search specifically
tailored to E-R retrieval. RELink includes entity and relationship indexing
based on the ClueWeb-09-B Web collection with FACC1 text span annotations
linked to Wikipedia entities. With ready to use search resources and a
comprehensive test collection, we support community in pursuing E-R research at
scale."
"In this paper, we first present a novel structure of AutoEncoder, namely
Semi-AutoEncoder. We generalize it into a designated hybrid collaborative
filtering model, which is able to predict ratings as well as to generate
personalized top-N recommendations. Experimental results on two real-world
datasets demonstrate its state-of-the-art performances."
"Now more than ever, it is becoming critical for businesses in the tourism
industry to have a strong online presence. To achieve this goal, the proper
usage of latest Web technologies, particularly schema.org annotations is
crucial. In this paper, we present our effort to improve the online visibility
of Tourismusverband (TVB) Mayrhofen-Hippach and all touristic service providers
in the Mayrhofen-Hippach region by creating and deploying massive semantic
annotations according to schema.org, the de facto standard for structured data
on the Web. The rationale for doing this is straightforward. Having schema.org
annotations enables search engines to understand the content better, and
provide better results for end users. As a direct consequence,
Mayrhofen-Hippach region and its touristic service increase their online
visibility and decrease the dependency on intermediaries i.e. Online Travel
Agency (OTA)."
"Since its unveiling in 2011, schema.org has become the de facto standard for
publishing semantically described structured data on the web, typically in the
form of web page annotations. The increasing adoption of schema.org facilitates
the growth of the web of data, as well as the development of automated agents
that operate on this data. Schema.org is a large heterogeneous vocabulary that
covers many domains. This is obviously not a bug, but a feature, since
schema.org aims to describe almost everything on the web, and the web is huge.
However, the heterogeneity of schema.org may cause a side effect, which is the
challenge of picking the right classes and properties for an annotation in a
certain domain, as well as keeping the annotation semantically consistent. In
this work, we introduce our rule based approach and an implementation of it for
validating schema.org annotations from two aspects: (a) the completeness of the
annotations in terms of a specified domain, (b) the semantic consistency of the
values based on pre-defined rules. We demonstrate our approach in the tourism
domain."
"Understanding users' behavior and predicting their future purchase are
critical for e-commerce companies to boost their revenue. While explicit user
feedback such as ratings plays the most significant role in eliciting users'
preferences, such feedback is scarce, which prompts the need for leveraging
more abundant implicit user feedback such as purchase record. Consequently,
recent studies focused on leveraging users' past purchase record to predict
their purchase. However, their performance is not satisfactory due to 1) the
lack of purchase history of users, and 2) more importantly the ill-posed
assumption of non-purchased items equally being considered as negative
feedback. In this paper, we define new pairwise relationships among items
aiming at overcoming the limitations of existing works, and propose a novel
method called P3S that stands for modeling pairwise relationships among three
disjoint item sets, which leverages users' click record in conjunction with
their purchase record. Precisely, we partition the items into three disjoint
sets based on users' purchase and click record, define new pairwise
relationships among them with respect to users, and reflect these relationships
into our pairwise learning-to-rank method. Experiments on two real-world
datasets demonstrate that our proposed method outperforms the state-of-the-art
baselines in the task of predicting users' future purchase in e-commerce."
"Extending TREC-style test collections by incorporating external resources is
a time consuming and challenging task. Making use of freely available web data
requires technical skills to work with APIs or to create a web scraping program
specifically tailored to the task at hand. We present a light-weight
alternative that employs the web data extraction language OXPath to harvest
data to be added to an existing test collection from web resources. We
demonstrate this by creating an extended version of GIRT4 called GIRT4-XT with
additional metadata fields harvested via OXPath from the social sciences portal
Sowiport. This allows the re-use of this collection for other evaluation
purposes like bibliometrics-enhanced retrieval. The demonstrated method can be
applied to a variety of similar scenarios and is not limited to extending
existing collections but can also be used to create completely new ones with
little effort."
"Large-scale recommender systems often face severe latency and storage
constraints at prediction time. These are particularly acute when the number of
items that could be recommended is large, and calculating predictions for the
full set is computationally intensive. In an attempt to relax these
constraints, we train recommendation models that use binary rather than
real-valued user and item representations, and show that while they are
substantially faster to evaluate, the gains in speed come at a large cost in
accuracy. In our Movielens 1M experiments, we show that reducing the latent
dimensionality of traditional models offers a more attractive accuracy/speed
trade-off than using binary representations."
"Review-based recommender systems have gained noticeable ground in recent
years. In addition to the rating scores, those systems are enriched with
textual evaluations of items by the users. Neural language processing models,
on the other hand, have already found application in recommender systems,
mainly as a means of encoding user preference data, with the actual textual
description of items serving only as side information. In this paper, a novel
approach to incorporating the aforementioned models into the recommendation
process is presented. Initially, a neural language processing model and more
specifically the paragraph vector model is used to encode textual user reviews
of variable length into feature vectors of fixed length. Subsequently this
information is fused along with the rating scores in a probabilistic matrix
factorization algorithm, based on maximum a-posteriori estimation. The
resulting system, ParVecMF, is compared to a ratings' matrix factorization
approach on a reference dataset. The obtained preliminary results on a set of
two metrics are encouraging and may stimulate further research in this area."
"We propose a unified product embedded representation that is optimized for
the task of retrieval-based product recommendation. To this end, we introduce a
new way to fuse modality-specific product embeddings into a joint product
embedding, in order to leverage both product content information, such as
textual descriptions and images, and product collaborative filtering signal. By
introducing the fusion step at the very end of our architecture, we are able to
train each modality separately, allowing us to keep a modular architecture that
is preferable in real-world recommendation deployments. We analyze our
performance on normal and hard recommendation setups such as cold-start and
cross-category recommendations and achieve good performance on a large product
shopping dataset."
"Recommendations are treatments. While todays recommender systems attempt to
emulate the naturally occurring user behaviour by predicting either missing
entries in the user-item matrix or computing the most likely continuation of
user sessions, we need to start thinking of recommendations in terms of optimal
interventions with respect to specific goals, such as the increase of number of
user conversions on a E-Commerce website. This objective is known as
Incremental Treatment Effect prediction (ITE) in the causal community. We
propose a new way of factorizing user-item matrices created from a large sample
of biased data collected using a control recommendation policy and from limited
randomized recommendation data collected using a treatment recommendation
policy in order to jointly optimize the prediction of outcomes of the treatment
policy and its incremental treatment effect with respect to the control policy.
We compare our method against both state-of-the-art factorization methods and
against new approaches of causal recommendation and show significant
improvements in performance."
"Recommendations can greatly benefit from good representations of the user
state at recommendation time. Recent approaches that leverage Recurrent Neural
Networks (RNNs) for session-based recommendations have shown that Deep Learning
models can provide useful user representations for recommendation. However,
current RNN modeling approaches summarize the user state by only taking into
account the sequence of items that the user has interacted with in the past,
without taking into account other essential types of context information such
as the associated types of user-item interactions, the time gaps between events
and the time of day for each interaction. To address this, we propose a new
class of Contextual Recurrent Neural Networks for Recommendation (CRNNs) that
can take into account the contextual information both in the input and output
layers and modifying the behavior of the RNN by combining the context embedding
with the item embedding and more explicitly, in the model dynamics, by
parametrizing the hidden unit transitions as a function of context information.
We compare our CRNNs approach with RNNs and non-sequential baselines and show
good improvements on the next event prediction task."
"In the last decade, driven also by the availability of an unprecedented
computational power and storage capabilities in cloud environments we assisted
to the proliferation of new algorithms, methods, and approaches in two areas of
artificial intelligence: knowledge representation and machine learning. On the
one side, the generation of a high rate of structured data on the Web led to
the creation and publication of the so-called knowledge graphs. On the other
side, deep learning emerged as one of the most promising approaches in the
generation and training of models that can be applied to a wide variety of
application fields. More recently, autoencoders have proven their strength in
various scenarios, playing a fundamental role in unsupervised learning. In this
paper, we instigate how to exploit the semantic information encoded in a
knowledge graph to build connections between units in a Neural Network, thus
leading to a new method, SEM-AUTO, to extract and weigh semantic features that
can eventually be used to build a recommender system. As adding content-based
side information may mitigate the cold user problems, we tested how our
approach behave in the presence of a few rating from a user on the Movielens 1M
dataset and compare results with BPRSLIM."
"With an exponentially growing number of scientific papers published each
year, advanced tools for exploring and discovering publications of interest are
becoming indispensable. To empower users beyond a simple keyword search
provided e.g. by Google Scholar, we present the novel web application PubVis.
Powered by a variety of machine learning techniques, it combines essential
features to help researchers find the content most relevant to them. An
interactive visualization of a large collection of scientific publications
provides an overview of the field and encourages the user to explore articles
beyond a narrow research focus. This is augmented by personalized content based
article recommendations as well as an advanced full text search to discover
relevant references. The open sourced implementation of the app can be easily
set up and run locally on a desktop computer to provide access to content
tailored to the specific needs of individual users. Additionally, a PubVis demo
with access to a collection of 10,000 papers can be tested online."
"Current recommender systems largely focus on static, unstructured content. In
many scenarios, we would like to recommend content that has structure, such as
a trajectory of points-of-interests in a city, or a playlist of songs. Dubbed
Structured Recommendation, this problem differs from the typical structured
prediction problem in that there are multiple correct answers for a given
input. Motivated by trajectory recommendation, we focus on sequential
structures but in contrast to classical Viterbi decoding we require that valid
predictions are sequences with no repeated elements. We propose an approach to
sequence recommendation based on the structured support vector machine. For
prediction, we modify the inference procedure to avoid predicting loops in the
sequence. For training, we modify the objective function to account for the
existence of multiple ground truths for a given input. We also modify the
loss-augmented inference procedure to exclude the known ground truths.
Experiments on real-world trajectory recommendation datasets show the benefits
of our approach over existing, non-structured recommendation approaches."
"Internet as become the way of life in the fast growing digital life.Even with
the increase in the internet speed, higher latency time is still a challenge.
To reduce latency, caching and pre fetching techniques can be used. However,
caching fails for dynamic websites which keeps on changing rapidly. Another
technique is web prefetching, which prefetches the web pages that the user is
likely to request for in the future. Semantic web prefetching makes use of
keywords and descriptive texts like anchor text, titles, text surrounding
anchor text of the present web pages for predicting users future requests.
Semantic information is embedded within the web pages during their designing
for the purpose of reflecting the relationship between the web pages. The
client can fetch this information from the server. However, this technique
involves load on web designers for adding external tags and on server for
providing this information along with the desired page, which is not desirable.
This paper is an effort to find the semantic relation between web pages using
the keywords provided by the user and the anchor texts of the hyperlinks on the
present web page.It provides algorithms for sequential and similar semantic
relations. These algorithms will be implemented on the client side which will
not cause overhead on designers and load on server for semantic information."
"The application of semantic technologies to content on the web is, in many
regards, as important as urgent. Search engines, chatbots, intelligent personal
assistants and other technologies increasingly rely on content published as
semantic structured data. Yet the process of creating this kind of data is
still complicated and the necessity of it is widely unknown. The semantify.it
platform implements an approach to solve three of the most challenging question
regarding the publication of structured semantic data, namely: a) what
vocabulary to use, b) how to create annotation files and c) how to publish or
integrate annotations within a website without programming. This paper presents
the idea and the development of the semantify.it platform. It demonstrates,
that the creation process of semantically annotated data must not be hard,
shows use cases and pilot users of the created software and presents where the
development of this platform or alike projects could go to in the future."
"Massive open online courses (MOOC) describe platforms where users with
completely different backgrounds subscribe to various courses on offer. MOOC
forums and discussion boards offer learners a medium to communicate with each
other and maximize their learning outcomes. However, oftentimes learners are
hesitant to approach each other for different reasons (being shy, don't know
the right match, etc.). In this paper, we propose a reciprocal recommender
system which matches learners who are mutually interested in, and likely to
communicate with each other based on their profile attributes like age,
location, gender, qualification, interests, etc. We test our algorithm on data
sampled using the publicly available MITx-Harvardx dataset and demonstrate that
both attribute importance and reciprocity play an important role in forming the
final recommendation list of learners. Our approach provides promising results
for such a system to be implemented within an actual MOOC."
"Contextual Suggestion deals with search techniques for complex information
needs that are highly focused on context and user needs. In this paper, we
propose \emph{R-Rec}, a novel rule-based technique to identify and recommend
appropriate points-of-interest to a user given her past preferences. We try to
embody the information that the user shares in the form of rating and tags of
any previous point(s)-of-interest and use it to rank the unrated candidate
suggestions. The ranking function is computed based on the similarity between a
suggestion and the places that the user like and the dissimilarity between the
suggestion and the places disliked by the user. Experiments carried out on
TREC-Contextual Suggestion 2015 dataset reveal the efficacy of our method."
"Modeling the complex interactions between users and items as well as amongst
items themselves is at the core of designing successful recommender systems.
One classical setting is predicting users' personalized sequential behavior (or
`next-item' recommendation), where the challenges mainly lie in modeling
`third-order' interactions between a user, her previously visited item(s), and
the next item to consume. Existing methods typically decompose these
higher-order interactions into a combination of pairwise relationships, by way
of which user preferences (user-item interactions) and sequential patterns
(item-item interactions) are captured by separate components. In this paper, we
propose a unified method, TransRec, to model such third-order relationships for
large-scale sequential prediction. Methodologically, we embed items into a
`transition space' where users are modeled as translation vectors operating on
item sequences. Empirically, this approach outperforms the state-of-the-art on
a wide spectrum of real-world datasets. Data and code are available at
https://sites.google.com/a/eng.ucsd.edu/ruining-he/."
"This paper presents a procedure to retrieve subsets of relevant documents
from large text collections for Content Analysis, e.g. in social sciences.
Document retrieval for this purpose needs to take account of the fact that
analysts often cannot describe their research objective with a small set of key
terms, especially when dealing with theoretical or rather abstract research
interests. Instead, it is much easier to define a set of paradigmatic documents
which reflect topics of interest as well as targeted manner of speech. Thus, in
contrast to classic information retrieval tasks we employ manually compiled
collections of reference documents to compose large queries of several hundred
key terms, called dictionaries. We extract dictionaries via Topic Models and
also use co-occurrence data from reference collections. Evaluations show that
the procedure improves retrieval results for this purpose compared to
alternative methods of key term extraction as well as neglecting co-occurrence
data."
"In the e-commerce world, the follow-up of prices in detail web pages is of
great interest for things like buying a product when it falls below some
threshold. For doing this task, instead of bookmarking the pages and revisiting
them, in this paper we propose a novel web data extraction system, called
Wextractor. It consists of an extraction method and a web app for listing the
retrieved prices. As for the final user, the main feature of Wextractor is
usability because (s)he only has to signal the pages of interest and our system
automatically extracts the price from the page."
"Tables are common and important in scientific documents, yet most text-based
document search systems do not capture structures and semantics specific to
tables. How to bridge different types of mismatch between keywords queries and
scientific tables and what influences ranking quality needs to be carefully
investigated. This paper considers the structure of tables and gives different
emphasis to table components. On the query side, thanks to external knowledge
such as knowledge bases and ontologies, key concepts are extracted and used to
build structured queries, and target quantity types are identified and used to
expand original queries. A probabilistic framework is proposed to incorporate
structural and semantic information from both query and table sides. We also
construct and release TableArXiv, a high quality dataset with 105 queries and
corresponding relevance judgements for scientific table search. Experiments
demonstrate significantly higher accuracy overall and at the top of the
rankings than several baseline methods."
"Music genres allow to categorize musical items that share common
characteristics. Although these categories are not mutually exclusive, most
related research is traditionally focused on classifying tracks into a single
class. Furthermore, these categories (e.g., Pop, Rock) tend to be too broad for
certain applications. In this work we aim to expand this task by categorizing
musical items into multiple and fine-grained labels, using three different data
modalities: audio, text, and images. To this end we present MuMu, a new dataset
of more than 31k albums classified into 250 genre classes. For every album we
have collected the cover image, text reviews, and audio tracks. Additionally,
we propose an approach for multi-label genre classification based on the
combination of feature embeddings learned with state-of-the-art deep learning
methodologies. Experiments show major differences between modalities, which not
only introduce new baselines for multi-label genre classification, but also
suggest that combining them yields improved results."
"While neural network approaches are achieving breakthrough performance in the
natural language related fields, there have been few similar attempts at
mathematical language related tasks. In this study, we explore the potential of
applying neural representation techniques to Mathematical Information Retrieval
(MIR) tasks. In more detail, we first briefly analyze the characteristic
differences between natural language and mathematical language. Then we design
a ""symbol2vec"" method to learn the vector representations of formula symbols
(numbers, variables, operators, functions, etc.) Finally, we propose a
""formula2vec"" based MIR approach and evaluate its performance. Preliminary
experiment results show that there is a promising potential for applying
formula embedding models to mathematical language representation and MIR tasks."
"The recent boom of AI has seen the emergence of many human-computer
conversation systems such as Google Assistant, Microsoft Cortana, Amazon Echo
and Apple Siri. We introduce and formalize the task of predicting questions in
conversations, where the goal is to predict the new question that the user will
ask, given the past conversational context. This task can be modeled as a
""sequence matching"" problem, where two sequences are given and the aim is to
learn a model that maps any pair of sequences to a matching probability. Neural
matching models, which adopt deep neural networks to learn sequence
representations and matching scores, have attracted immense research interests
of information retrieval and natural language processing communities. In this
paper, we first study neural matching models for the question retrieval task
that has been widely explored in the literature, whereas the effectiveness of
neural models for this task is relatively unstudied. We further evaluate the
neural matching models in the next question prediction task in conversations.
We have used the publicly available Quora data and Ubuntu chat logs in our
experiments. Our evaluations investigate the potential of neural matching
models with representation learning for question retrieval and next question
prediction in conversations. Experimental results show that neural matching
models perform well for both tasks."
"With the popularity of Linked Open Data (LOD) and the associated rise in
freely accessible knowledge that can be accessed via LOD, exploiting LOD for
recommender systems has been widely studied based on various approaches such as
graph-based or using different machine learning models with LOD-enabled
features. Many of the previous approaches require construction of an additional
graph to run graph-based algorithms or to extract path-based features by
combining user- item interactions (e.g., likes, dislikes) and background
knowledge from LOD. In this paper, we investigate Factorization Machines (FMs)
based on particularly lightweight LOD-enabled features which can be directly
obtained via a public SPARQL Endpoint without any additional effort to
construct a graph. Firstly, we aim to study whether using FM with these
lightweight LOD-enabled features can provide competitive performance compared
to a learning-to-rank approach leveraging LOD as well as other well-established
approaches such as kNN-item and BPRMF. Secondly, we are interested in finding
out to what extent each set of LOD-enabled features contributes to the
recommendation performance. Experimental evaluation on a standard dataset shows
that our proposed approach using FM with lightweight LOD-enabled features
provides the best performance compared to other approaches in terms of five
evaluation metrics. In addition, the study of the recommendation performance
based on different sets of LOD-enabled features indicate that property-object
lists and PageRank scores of items are useful for improving the performance,
and can provide the best performance through using them together for FM. We
observe that subject-property lists of items does not contribute to the
recommendation performance but rather decreases the performance."
"Most of the existing recommender systems assume that user's visiting history
can be constantly recorded. However, in recent online services, the user
identification may be usually unknown and only limited online user behaviors
can be used. It is of great importance to model the temporal online user
behaviors and conduct recommendation for the anonymous users. In this paper, we
propose a list-wise deep neural network based architecture to model the limited
user behaviors within each session. To train the model efficiently, we first
design a session embedding method to pre-train a session representation, which
incorporates different kinds of user search behaviors such as clicks and views.
Based on the learnt session representation, we further propose a list-wise
ranking model to generate the recommendation result for each anonymous user
session. We conduct quantitative experiments on a recently published dataset
from an e-commerce company. The evaluation results validate the effectiveness
of the proposed method, which can outperform the state-of-the-art
significantly."
"Microblogging sites like Twitter and Weibo have emerged as important
sourcesof real-time information on ongoing events, including socio-political
events, emergency events, and so on. For instance, during emergency events
(such as earthquakes, floods, terror attacks), microblogging sites are very
useful for gathering situational information in real-time. During such an
event, typically only a small fraction of the microblogs (tweets) posted are
relevant to the information need. Hence, it is necessary to design effective
methodologies for microblog retrieval, so that the relevant tweets can be
automatically extracted from large sets of documents (tweets).
  In this work, we apply and compare various neural network-based IR models for
microblog retrieval for a specific application, as follows. In a disaster
situation, one of the primary and practical challenges in coordinating the
post-disaster relief operations is to know about what resources are needed and
what resources are available in the disaster-affected area. Thus, in this
study, we focus on extracting these two specific types of microblogs or tweets
namely need tweets and avail tweets, which are tweets which define some needs
of the people and the tweets which offer some solutions or aid for the people,
respectively."
"We describe a new deep learning architecture for learning to rank question
answer pairs. Our approach extends the long short-term memory (LSTM) network
with holographic composition to model the relationship between question and
answer representations. As opposed to the neural tensor layer that has been
adopted recently, the holographic composition provides the benefits of scalable
and rich representational learning approach without incurring huge parameter
costs. Overall, we present Holographic Dual LSTM (HD-LSTM), a unified
architecture for both deep sentence modeling and semantic matching.
Essentially, our model is trained end-to-end whereby the parameters of the LSTM
are optimized in a way that best explains the correlation between question and
answer representations. In addition, our proposed deep learning architecture
requires no extensive feature engineering. Via extensive experiments, we show
that HD-LSTM outperforms many other neural architectures on two popular
benchmark QA datasets. Empirical studies confirm the effectiveness of
holographic composition over the neural tensor layer."
"With the ever-growing volume, complexity and dynamicity of online
information, recommender system has been an effective key solution to overcome
such information overload. In recent years, deep learning's revolutionary
advances in speech recognition, image analysis and natural language processing
have gained significant attention. Meanwhile, recent studies also demonstrate
its effectiveness in coping with information retrieval and recommendation
tasks. Applying deep learning techniques into recommender system has been
gaining momentum due to its state-of-the-art performances and high-quality
recommendations. In contrast to traditional recommendation models, deep
learning provides a better understanding of user's demands, item's
characteristics and historical interactions between them.
  This article aims to provide a comprehensive review of recent research
efforts on deep learning based recommender systems towards fostering
innovations of recommender system research. A taxonomy of deep learning based
recommendation models is presented and used to categorize the surveyed
articles. Open problems are identified based on the analytics of the reviewed
works and potential solutions discussed."
"The effective of information retrieval (IR) systems have become more
important than ever. Deep IR models have gained increasing attention for its
ability to automatically learning features from raw text; thus, many deep IR
models have been proposed recently. However, the learning process of these deep
IR models resemble a black box. Therefore, it is necessary to identify the
difference between automatically learned features by deep IR models and
hand-crafted features used in traditional learning to rank approaches.
Furthermore, it is valuable to investigate the differences between these deep
IR models. This paper aims to conduct a deep investigation on deep IR models.
Specifically, we conduct an extensive empirical study on two different
datasets, including Robust and LETOR4.0. We first compared the automatically
learned features and hand-crafted features on the respects of query term
coverage, document length, embeddings and robustness. It reveals a number of
disadvantages compared with hand-crafted features. Therefore, we establish
guidelines for improving existing deep IR models. Furthermore, we compare two
different categories of deep IR models, i.e. representation-focused models and
interaction-focused models. It is shown that two types of deep IR models focus
on different categories of words, including topic-related words and
query-related words."
"Push notification is a key component for E-commerce mobile applications,
which has been extensively used for user growth and engagement. The
effectiveness of the push notification is generally measured by message open
rate. A push message can contain a recommended product, a shopping news and
etc., but often only one or two items can be shown in the push message due to
the limit of display space. This paper proposes a mixture model approach for
predicting push message open rate for a post-purchase complementary product
recommendation task. The mixture model is trained to learn latent prediction
contexts, which are determined by user and item profiles, and then make open
rate predictions accordingly. The item with the highest predicted open rate is
then chosen to be included in the push notification message for each user. The
parameters of the mixture model are optimized using an EM algorithm. A set of
experiments are conducted to evaluate the proposed method live with a popular
E-Commerce mobile app. The results show that the proposed method is superior
than several existing solutions by a significant margin."
"We explore different approaches to integrating a simple convolutional neural
network (CNN) with the Lucene search engine in a multi-stage ranking
architecture. Our models are trained using the PyTorch deep learning toolkit,
which is implemented in C/C++ with a Python frontend. One obvious integration
strategy is to expose the neural network directly as a service. For this, we
use Apache Thrift, a software framework for building scalable cross-language
services. In exploring alternative architectures, we observe that once trained,
the feedforward evaluation of neural networks is quite straightforward.
Therefore, we can extract the parameters of a trained CNN from PyTorch and
import the model into Java, taking advantage of the Java Deeplearning4J library
for feedforward evaluation. This has the advantage that the entire end-to-end
system can be implemented in Java. As a third approach, we can extract the
neural network from PyTorch and ""compile"" it into a C++ program that exposes a
Thrift service. We evaluate these alternatives in terms of performance (latency
and throughput) as well as ease of integration. Experiments show that
feedforward evaluation of the convolutional neural network is significantly
slower in Java, while the performance of the compiled C++ network does not
consistently beat the PyTorch implementation."
"Due to its storage and retrieval efficiency, cross-modal hashing~(CMH) has
been widely used for cross-modal similarity search in multimedia applications.
According to the training strategy, existing CMH methods can be mainly divided
into two categories: relaxation-based continuous methods and discrete methods.
In general, the training of relaxation-based continuous methods is faster than
discrete methods, but the accuracy of relaxation-based continuous methods is
not satisfactory. On the contrary, the accuracy of discrete methods is
typically better than relaxation-based continuous methods, but the training of
discrete methods is time-consuming. In this paper, we propose a novel CMH
method, called discrete latent factor model based cross-modal hashing~(DLFH),
for cross modal similarity search. DLFH is a discrete method which can directly
learn the binary hash codes for CMH. At the same time, the training of DLFH is
efficient. Experiments on real datasets show that DLFH can achieve
significantly better accuracy than existing methods, and the training time of
DLFH is comparable to that of relaxation-based continuous methods which are
much faster than existing discrete methods."
"Metric search is concerned with the efficient evaluation of queries in metric
spaces. In general,a large space of objects is arranged in such a way that,
when a further object is presented as a query, those objects most similar to
the query can be efficiently found. Most mechanisms rely upon the triangle
inequality property of the metric governing the space. The triangle inequality
property is equivalent to a finite embedding property, which states that any
three points of the space can be isometrically embedded in two-dimensional
Euclidean space. In this paper, we examine a class of semimetric space which is
finitely four-embeddable in three-dimensional Euclidean space. In mathematics
this property has been extensively studied and is generally known as the
four-point property. All spaces with the four-point property are metric spaces,
but they also have some stronger geometric guarantees. We coin the term
supermetric space as, in terms of metric search, they are significantly more
tractable. Supermetric spaces include all those governed by Euclidean, Cosine,
Jensen-Shannon and Triangular distances, and are thus commonly used within many
domains. In previous work we have given a generic mathematical basis for the
supermetric property and shown how it can improve indexing performance for a
given exact search structure. Here we present a full investigation into its use
within a variety of different hyperplane partition indexing structures, and go
on to show some more of its flexibility by examining a search structure whose
partition and exclusion conditions are tailored, at each node, to suit the
individual reference points and data set present there. Among the results
given, we show a new best performance for exact search using a well-known
benchmark."
"In 1953, Blumenthal showed that every semi-metric space that is isometrically
embeddable in a Hilbert space has the n-point property; we have previously
called such spaces supermetric spaces. Although this is a strictly stronger
property than triangle inequality, it is nonetheless closely related and many
useful metric spaces possess it. These include Euclidean, Cosine and
Jensen-Shannon spaces of any dimension. A simple corollary of the n-point
property is that, for any (n+1) objects sampled from the space, there exists an
n-dimensional simplex in Euclidean space whose edge lengths correspond to the
distances among the objects. We show how the construction of such simplexes in
higher dimensions can be used to give arbitrarily tight lower and upper bounds
on distances within the original space. This allows the construction of an
n-dimensional Euclidean space, from which lower and upper bounds of the
original space can be calculated, and which is itself an indexable space with
the n-point property. For similarity search, the engineering tradeoffs are
good: we show significant reductions in data size and metric cost with little
loss of accuracy, leading to a significant overall improvement in search
performance."
"Recommender systems have been successfully applied to assist decision making
by producing a list of item recommendations tailored to user preferences.
Traditional recommender systems only focus on optimizing the utility of the end
users who are the receiver of the recommendations. By contrast,
multi-stakeholder recommendation attempts to generate recommendations that
satisfy the needs of both the end users and other parties or stakeholders. This
paper provides an overview and discussion about the multi-stakeholder
recommendations from the perspective of practical applications, available data
sets, corresponding research challenges and potential solutions."
"We address the task of entity-relationship (E-R) retrieval, i.e, given a
query characterizing types of two or more entities and relationships between
them, retrieve the relevant tuples of related entities. Answering E-R queries
requires gathering and joining evidence from multiple unstructured documents.
In this work, we consider entity and relationships of any type, i.e,
characterized by context terms instead of pre-defined types or relationships.
We propose a novel IR-centric approach for E-R retrieval, that builds on the
basic early fusion design pattern for object retrieval, to provide extensible
entity-relationship representations, suitable for complex, multi-relationships
queries. We performed experiments with Wikipedia articles as entity
representations combined with relationships extracted from ClueWeb-09-B with
FACC1 entity linking. We obtained promising results using 3 different query
collections comprising 469 E-R queries."
"Recommender systems are personalized information systems. However, in many
settings, the end-user of the recommendations is not the only party whose needs
must be represented in recommendation generation. Incorporating this insight
gives rise to the notion of multistakeholder recommendation, in which the
interests of multiple parties are represented in recommendation algorithms and
evaluation. In this paper, we identify patterns of stakeholder utility that
characterize different multistakeholder recommendation applications, and
provide a taxonomy of the different possible systems, only some of which have
currently been implemented."
"This work addresses the problem of matching short excerpts of audio with
their respective counterparts in sheet music images. We show how to employ
neural network-based cross-modality embedding spaces for solving the following
two sheet music-related tasks: retrieving the correct piece of sheet music from
a database when given a music audio as a search query; and aligning an audio
recording of a piece with the corresponding images of sheet music. We
demonstrate the feasibility of this in experiments on classical piano music by
five different composers (Bach, Haydn, Mozart, Beethoven and Chopin), and
additionally provide a discussion on why we expect multi-modal neural networks
to be a fruitful paradigm for dealing with sheet music and audio at the same
time."
"Hashing has been widely used for large-scale search due to its low storage
cost and fast query speed. By using supervised information, supervised hashing
can significantly outperform unsupervised hashing. Recently, discrete
supervised hashing and deep hashing are two representative progresses in
supervised hashing. On one hand, hashing is essentially a discrete optimization
problem. Hence, utilizing supervised information to directly guide discrete
(binary) coding procedure can avoid sub-optimal solution and improve the
accuracy. On the other hand, deep hashing, which integrates deep feature
learning and hash-code learning into an end-to-end architecture, can enhance
the feedback between feature learning and hash-code learning. The key in
discrete supervised hashing is to adopt supervised information to directly
guide the discrete coding procedure in hashing. The key in deep hashing is to
adopt the supervised information to directly guide the deep feature learning
procedure. However, there have not existed works which can use the supervised
information to directly guide both discrete coding procedure and deep feature
learning procedure in the same framework. In this paper, we propose a novel
deep hashing method, called deep discrete supervised hashing (DDSH), to address
this problem. DDSH is the first deep hashing method which can utilize
supervised information to directly guide both discrete coding procedure and
deep feature learning procedure, and thus enhance the feedback between these
two important procedures. Experiments on three real datasets show that DDSH can
outperform other state-of-the-art baselines, including both discrete hashing
and deep hashing baselines, for image retrieval."
"We consider information retrieval when the data, for instance multimedia, is
coputationally expensive to fetch. Our approach uses ""information filters"" to
considerably narrow the universe of possiblities before retrieval. We are
especially interested in redundant information filters that save time over more
general but more costly filters. Efficient retrieval requires that decision
must be made about the necessity, order, and concurrent processing of proposed
filters (an ""execution plan""). We develop simple polynomial-time local criteria
for optimal execution plans, and show that most forms of concurrency are
suboptimal with information filters. Although the general problem of finding an
optimal execution plan is likely exponential in the number of filters, we show
experimentally that our local optimality criteria, used in a polynomial-time
algorithm, nearly always find the global optimum with 15 filters or less, a
sufficient number of filters for most applications. Our methods do not require
special hardware and avoid the high processor idleness that is characteristic
of massive parallelism solutions to this problem. We apply our ideas to an
important application, information retrieval of cpationed data using
natural-language understanding, a problem for which the natural-language
processing can be the bottleneck if not implemented well."
"We are interested in questions of improving user control in best-match
text-retrieval systems, specifically questions as to whether simple
visualizations that nonetheless go beyond the minimal ones generally available
can significantly help users. Recently, we have been investigating ways to help
users decide-given a set of documents retrieved by a query-which documents and
passages are worth closer examination. We built a document viewer incorporating
a visualization centered around a novel content-displaying scrollbar and color
term highlighting, and studied whether the visualization is helpful to
non-expert searchers. Participants' reaction to the visualization was very
positive, while the objective results were inconclusive."
"Because the World Wide Web is a dynamic collection of information, the Web
search tools (or ""search engines"") that index the Web are dynamic. Traditional
information retrieval evaluation techniques may not provide reliable results
when applied to the Web search tools. This study is the result of ten
replications of the classic 1996 Ding and Marchionini Web search tool research.
It explores the effects that replication can have on transforming unreliable
results from one iteration into replicable and therefore reliable results after
multiple iterations."
"It is shown that personalization of web content can be advantageously viewed
as a form of partial evaluation --- a technique well known in the programming
languages community. The basic idea is to model a recommendation space as a
program, then partially evaluate this program with respect to user preferences
(and features) to obtain specialized content. This technique supports both
content-based and collaborative approaches, and is applicable to a range of
applications that require automatic information integration from multiple web
sources. The effectiveness of this methodology is illustrated by two example
applications --- (i) personalizing content for visitors to the Blacksburg
Electronic Village (http://www.bev.net), and (ii) locating and selecting
scientific software on the Internet. The scalability of this technique is
demonstrated by its ability to interface with online web ontologies that index
thousands of web pages."
"The problem of Information Retrieval is, given a set of documents D and a
query q, providing an algorithm for retrieving all documents in D relevant to
q. However, retrieval should depend and be updated whenever the user is able to
provide as an input a preferred set of relevant documents; this process is
known as em relevance feedback. Recent work in IR has been paying great
attention to models which employ a logical approach; the advantage being that
one can have a simple computable characterization of retrieval on the basis of
a pure logical analysis of retrieval. Most of the logical models make use of
probabilities or similar belief functions in order to introduce the inductive
component whereby uncertainty is treated. Their general paradigm is the
following: em find the nature of conditional $d\imp q$ and then define a
probability on the top of it. We just reverse this point of view; first use the
numerical information, frequencies or probabilities, then define your own
logical consequence. More generally, we claim that retrieval is a form of
deduction. We introduce a simple but powerful logical framework of relevance
feedback, derived from the well founded area of nonmonotonic logic. This
description can help us evaluate, describe and compare from a theoretical point
of view previous approaches based on conditionals or probabilities."
"The performance of CBIR algorithms is usually measured on an isolated
workstation. In a real-world environment the algorithms would only constitute a
minor component among the many interacting components. The Internet
dramati-cally changes many of the usual assumptions about measuring CBIR
performance. Any CBIR benchmark should be designed from a networked systems
standpoint. These benchmarks typically introduce communication overhead because
the real systems they model are distributed applications. We present our
implementation of a client/server benchmark called BIRDS-I to measure image
retrieval performance over the Internet. It has been designed with the trend
toward the use of small personalized wireless systems in mind. Web-based CBIR
implies the use of heteroge-neous image sets, imposing certain constraints on
how the images are organized and the type of performance metrics applicable.
BIRDS-I only requires controlled human intervention for the compilation of the
image collection and none for the generation of ground truth in the measurement
of retrieval accuracy. Benchmark image collections need to be evolved
incrementally toward the storage of millions of images and that scaleup can
only be achieved through the use of computer-aided compilation. Finally, our
scoring metric introduces a tightly optimized image-ranking window."
"Information personalization refers to the automatic adjustment of information
content, structure, and presentation tailored to an individual user. By
reducing information overload and customizing information access,
personalization systems have emerged as an important segment of the Internet
economy. This paper presents a systematic modeling methodology - PIPE
(`Personalization is Partial Evaluation') - for personalization.
Personalization systems are designed and implemented in PIPE by modeling an
information-seeking interaction in a programmatic representation. The
representation supports the description of information-seeking activities as
partial information and their subsequent realization by partial evaluation, a
technique for specializing programs. We describe the modeling methodology at a
conceptual level and outline representational choices. We present two
application case studies that use PIPE for personalizing web sites and describe
how PIPE suggests a novel evaluation criterion for information system designs.
Finally, we mention several fundamental implications of adopting the PIPE model
for personalization and when it is (and is not) applicable."
"The latest generation of Web search tools is beginning to exploit hypertext
link information to improve ranking\cite{Brin98,Kleinberg98} and
crawling\cite{Menczer00,Ben-Shaul99etal,Chakrabarti99} algorithms. The hidden
assumption behind such approaches, a correlation between the graph structure of
the Web and its content, has not been tested explicitly despite increasing
research on Web topology\cite{Lawrence98,Albert99,Adamic99,Butler00}. Here I
formalize and quantitatively validate two conjectures drawing connections from
link information to lexical and semantic Web content. The clink-content
conjecture states that a page is similar to the pages that link to it, i.e.,
one can infer the lexical content of a page by looking at the pages that link
to it. I also show that lexical inferences based on link cues are quite
heterogeneous across Web communities. The link-cluster conjecture states that
pages about the same topic are clustered together, i.e., one can infer the
meaning of a page by looking at its neighbours. These results explain the
success of the newest search technologies and open the way for more dynamic and
scalable methods to locate information in a topic or user driven way."
"Many data types arising from data mining applications can be modeled as
bipartite graphs, examples include terms and documents in a text corpus,
customers and purchasing items in market basket analysis and reviewers and
movies in a movie recommender system. In this paper, we propose a new data
clustering method based on partitioning the underlying bipartite graph. The
partition is constructed by minimizing a normalized sum of edge weights between
unmatched pairs of vertices of the bipartite graph. We show that an approximate
solution to the minimization problem can be obtained by computing a partial
singular value decomposition (SVD) of the associated edge weight matrix of the
bipartite graph. We point out the connection of our clustering algorithm to
correspondence analysis used in multivariate analysis. We also briefly discuss
the issue of assigning data objects to multiple clusters. In the experimental
results, we apply our clustering algorithm to the problem of document
clustering to illustrate its effectiveness and efficiency."
"In this paper we describe the requirements for research information systems
and problems which arise in the development of such system. Here is shown which
problems could be solved by using of knowledge markup technologies. Ontology
for Research Information System offered. Architecture for collecting research
data and providing access to it is described."
"The automated categorization (or classification) of texts into predefined
categories has witnessed a booming interest in the last ten years, due to the
increased availability of documents in digital form and the ensuing need to
organize them. In the research community the dominant approach to this problem
is based on machine learning techniques: a general inductive process
automatically builds a classifier by learning, from a set of preclassified
documents, the characteristics of the categories. The advantages of this
approach over the knowledge engineering approach (consisting in the manual
definition of a classifier by domain experts) are a very good effectiveness,
considerable savings in terms of expert manpower, and straightforward
portability to different domains. This survey discusses the main approaches to
text categorization that fall within the machine learning paradigm. We will
discuss in detail issues pertaining to three different problems, namely
document representation, classifier construction, and classifier evaluation."
"The Web graph is a giant social network whose properties have been measured
and modeled extensively in recent years. Most such studies concentrate on the
graph structure alone, and do not consider textual properties of the nodes.
Consequently, Web communities have been characterized purely in terms of graph
structure and not on page content. We propose that a topic taxonomy such as
Yahoo! or the Open Directory provides a useful framework for understanding the
structure of content-based clusters and communities. In particular, using a
topic taxonomy and an automatic classifier, we can measure the background
distribution of broad topics on the Web, and analyze the capability of recent
random walk algorithms to draw samples which follow such distributions. In
addition, we can measure the probability that a page about one broad topic will
link to another broad topic. Extending this experiment, we can measure how
quickly topic context is lost while walking randomly on the Web graph.
Estimates of this topic mixing distance may explain why a global PageRank is
still meaningful in the context of broad queries. In general, our measurements
may prove valuable in the design of community-specific crawlers and link-based
ranking systems."
"Can a Web crawler efficiently locate an unknown relevant page? While this
question is receiving much empirical attention due to its considerable
commercial value in the search engine community
[Cho98,Chakrabarti99,Menczer00,Menczer01], theoretical efforts to bound the
performance of focused navigation have only exploited the link structure of the
Web graph, neglecting other features [Kleinberg01,Adamic01,Kim02]. Here I
investigate the connection between linkage and a content-induced topology of
Web pages, suggesting that efficient paths can be discovered by decentralized
navigation algorithms based on textual cues."
"Recommender systems attempt to reduce information overload and retain
customers by selecting a subset of items from a universal set based on user
preferences. While research in recommender systems grew out of information
retrieval and filtering, the topic has steadily advanced into a legitimate and
challenging research area of its own. Recommender systems have traditionally
been studied from a content-based filtering vs. collaborative design
perspective. Recommendations, however, are not delivered within a vacuum, but
rather cast within an informal community of users and social context.
Therefore, ultimately all recommender systems make connections among people and
thus should be surveyed from such a perspective. This viewpoint is
under-emphasized in the recommender systems literature. We therefore take a
connection-oriented viewpoint toward recommender systems research. We posit
that recommendation has an inherently social element and is ultimately intended
to connect people either directly as a result of explicit user modeling or
indirectly through the discovery of relationships implicit in extant data.
Thus, recommender systems are characterized by how they model users to bring
people together: explicitly or implicitly. Finally, user modeling and the
connection-centric viewpoint raise broadening and social issues--such as
evaluation, targeting, and privacy and trust--which we also briefly address."
"Although knowledge is one of the most valuable resource of enterprises and an
important production and competition factor, this intellectual potential is
often used (or maintained) only inadequate by the enterprises. Therefore, in a
globalised and growing market the optimal usage of existing knowledge
represents a key factor for enterprises of the future. Here, knowledge
management systems should engage facilitating. Because geographically far
distributed establishments cause, however, a distributed system, this paper
should uncover the spectrum connected with it and present a possible basic
approach which is based on ontologies and modern, platform independent
technologies. Last but not least this attempt, as well as general questions of
the knowledge management, are discussed."
"We introduce an approach to automatic indexing of e-prints based on a
pattern-matching technique making extensive use of an Associative Patterns
Dictionary (APD), developed by us. Entries in the APD consist of natural
language phrases with the same semantic interpretation as a set of keywords
from a controlled vocabulary. The method also allows to recognize within
e-prints formulae written in TeX notations that might also appear as keywords.
We present an automatic indexing system, AUTEX, which we have applied to
keyword index e-prints in selected areas in high energy physics (HEP) making
use of the DESY-HEPI thesaurus as a controlled vocabulary."
"Community identification algorithms have been used to enhance the quality of
the services perceived by its users. Although algorithms for community have a
widespread use in the Web, their application to portals or specific subsets of
the Web has not been much studied. In this paper, we propose a technique for
local community identification that takes into account user access behavior
derived from access logs of servers in the Web. The technique takes a departure
from the existing community algorithms since it changes the focus of in terest,
moving from authors to users. Our approach does not use relations imposed by
authors (e.g. hyperlinks in the case of Web pages). It uses information derived
from user accesses to a service in order to infer relationships. The
communities identified are of great interest to content providers since they
can be used to improve quality of their services. We also propose an evaluation
methodology for analyzing the results obtained by the algorithm. We present two
case studies based on actual data from two services: an online bookstore and an
online radio. The case of the online radio is particularly relevant, because it
emphasizes the contribution of the proposed algorithm to find out communities
in an environment (i.e., streaming media service) without links, that represent
the relations imposed by authors (e.g. hyperlinks in the case of Web pages)."
"We present a new method for segmenting, and a new user interface for indexing
and visualizing, the semantic content of extended instructional videos. Given a
series of key frames from the video, we generate a condensed view of the data
by clustering frames according to media type and visual similarities. Using
various visual filters, key frames are first assigned a media type (board,
class, computer, illustration, podium, and sheet). Key frames of media type
board and sheet are then clustered based on contents via an algorithm with
near-linear cost. A novel user interface, the result of two user studies,
displays related topics using icons linked topologically, allowing users to
quickly locate semantically related portions of the video. We analyze the
accuracy of the segmentation tool on 17 instructional videos, each of which is
from 75 to 150 minutes in duration (a total of 40 hours); the classification
accuracy exceeds 96%."
"We present a new method for segmenting, and a new user interface for indexing
and visualizing, the semantic content of extended instructional videos. Using
various visual filters, key frames are first assigned a media type (board,
class, computer, illustration, podium, and sheet). Key frames of media type
board and sheet are then clustered based on contents via an algorithm with
near-linear cost. A novel user interface, the result of two user studies,
displays related topics using icons linked topologically, allowing users to
quickly locate semantically related portions of the video. We analyze the
accuracy of the segmentation tool on 17 instructional videos, each of which is
from 75 to 150 minutes in duration (a total of 40 hours); it exceeds 96%."
"Multimodal interfaces are becoming increasingly important with the advent of
mobile devices, accessibility considerations, and novel software technologies
that combine diverse interaction media. This article investigates systems
support for web browsing in a multimodal interface. Specifically, we outline
the design and implementation of a software framework that integrates hyperlink
and speech modes of interaction. Instead of viewing speech as merely an
alternative interaction medium, the framework uses it to support out-of-turn
interaction, providing a flexibility of information access not possible with
hyperlinks alone. This approach enables the creation of websites that adapt to
the needs of users, yet permits the designer fine-grained control over what
interactions to support. Design methodology, implementation details, and two
case studies are presented."
"Multimodal interfaces are becoming increasingly ubiquitous with the advent of
mobile devices, accessibility considerations, and novel software technologies
that combine diverse interaction media. In addition to improving access and
delivery capabilities, such interfaces enable flexible and personalized dialogs
with websites, much like a conversation between humans. In this paper, we
present a software framework for multimodal web interaction management that
supports mixed-initiative dialogs between users and websites. A
mixed-initiative dialog is one where the user and the website take turns
changing the flow of interaction. The framework supports the functional
specification and realization of such dialogs using staging transformations --
a theory for representing and reasoning about dialogs based on partial input.
It supports multiple interaction interfaces, and offers sessioning, caching,
and co-ordination functions through the use of an interaction manager. Two case
studies are presented to illustrate the promise of this approach."
"We illustrate the use of machine learning techniques to analyze, structure,
maintain, and evolve a large online corpus of academic literature. An emerging
field of research can be identified as part of an existing corpus, permitting
the implementation of a more coherent community structure for its
practitioners."
"This paper offers a short description of an Internet information field
monitoring system, which places a special module-sensor on the side of the
Web-server to detect changes in information resources and subsequently
reindexes only the resources signalized by the corresponding sensor. Concise
results of simulation research and an implementation attempt of the given
""sensors"" concept are provided."
"Scientific research is highly dynamic. New areas of science continually
evolve;others gain or lose importance, merge or split. Due to the steady
increase in the number of scientific publications it is hard to keep an
overview of the structure and dynamic development of one's own field of
science, much less all scientific domains. However, knowledge of hot topics,
emergent research frontiers, or change of focus in certain areas is a critical
component of resource allocation decisions in research labs, governmental
institutions, and corporations. This paper demonstrates the utilization of
Kleinberg's burst detection algorithm, co-word occurrence analysis, and graph
layout techniques to generate maps that support the identification of major
research topics and trends. The approach was applied to analyze and map the
complete set of papers published in the Proceedings of the National Academy of
Sciences (PNAS) in the years 1982-2001. Six domain experts examined and
commented on the resulting maps in an attempt to reconstruct the evolution of
major research areas covered by PNAS."
"Most previous work on the recently developed language-modeling approach to
information retrieval focuses on document-specific characteristics, and
therefore does not take into account the structure of the surrounding corpus.
We propose a novel algorithmic framework in which information provided by
document-based language models is enhanced by the incorporation of information
drawn from clusters of similar documents. Using this framework, we develop a
suite of new algorithms. Even the simplest typically outperforms the standard
language-modeling approach in precision and recall, and our new interpolation
algorithm posts statistically significant improvements for both metrics over
all three corpora tested."
"Markov models have been widely utilized for modelling user web navigation
behaviour. In this work we propose a dynamic clustering-based method to
increase a Markov model's accuracy in representing a collection of user web
navigation sessions. The method makes use of the state cloning concept to
duplicate states in a way that separates in-links whose corresponding
second-order probabilities diverge. In addition, the new method incorporates a
clustering technique which determines an effcient way to assign in-links with
similar second-order probabilities to the same clone. We report on experiments
conducted with both real and random data and we provide a comparison with the
N-gram Markov concept. The results show that the number of additional states
induced by the dynamic clustering method can be controlled through a threshold
parameter, and suggest that the method's performance is linear time in the size
of the model."
"This paper describes the architecture of MOSE (My Own Search Engine), a
scalable parallel and distributed engine for searching the web. MOSE was
specifically designed to efficiently exploit affordable parallel architectures,
such as clusters of workstations. Its modular and scalable architecture can
easily be tuned to fulfill the bandwidth requirements of the application at
hand. Both task-parallel and data-parallel approaches are exploited within MOSE
in order to increase the throughput and efficiently use communication, storing
and computational resources. We used a collection of html documents as a
benchmark, and conducted preliminary experiments on a cluster of three SMP
Linux PCs."
"The semantic Web initiates new, high level access schemes to online content
and applications. One area of superior need for a redefined content exploration
is given by on-line educational applications and their concepts of
interactivity in the framework of open hypermedia systems. In the present paper
we discuss aspects and opportunities of gaining interactivity schemes from
semantic notions of components. A transition from standard educational
annotation to semantic statements of hyperlinks is discussed. Further on we
introduce the concept of semantic link contexts as an approach to manage a
coherent rhetoric of linking. A practical implementation is introduced, as
well. Our semantic hyperlink implementation is based on the more general
Multimedia Information Repository MIR, an open hypermedia system supporting the
standards XML, Corba and JNDI."
"While eLearning systems become more and more popular in daily education,
available applications lack opportunities to structure, annotate and manage
their contents in a high-level fashion. General efforts to improve these
deficits are taken by initiatives to define rich meta data sets and a
semanticWeb layer. In the present paper we introduce Hylos, an online learning
system. Hylos is based on a cellular eLearning Object (ELO) information model
encapsulating meta data conforming to the LOM standard. Content management is
provisioned on this semantic meta data level and allows for variable,
dynamically adaptable access structures. Context aware multifunctional links
permit a systematic navigation depending on the learners and didactic needs,
thereby exploring the capabilities of the semantic web. Hylos is built upon the
more general Multimedia Information Repository (MIR) and the MIR adaptive
context linking environment (MIRaCLE), its linking extension. MIR is an open
system supporting the standards XML, Corba and JNDI. Hylos benefits from
manageable information structures, sophisticated access logic and high-level
authoring tools like the ELO editor responsible for the semi-manual creation of
meta data and WYSIWYG like content editing."
"We introduce new techniques for extracting, analyzing, and visualizing
textual contents from instructional videos of low production quality. Using
Automatic Speech Recognition, approximate transcripts (H75% Word Error Rate)
are obtained from the originally highly compressed videos of university
courses, each comprising between 10 to 30 lectures. Text material in the form
of books or papers that accompany the course are then used to filter meaningful
phrases from the seemingly incoherent transcripts. The resulting index into the
transcripts is tied together and visualized in 3 experimental graphs that help
in understanding the overall course structure and provide a tool for localizing
certain topics for indexing. We specifically discuss a Transcript Index Map,
which graphically lays out key phrases for a course, a Textbook Chapter to
Transcript Match, and finally a Lecture Transcript Similarity graph, which
clusters semantically similar lectures. We test our methods and tools on 7 full
courses with 230 hours of video and 273 transcripts. We are able to extract up
to 98 unique key terms for a given transcript and up to 347 unique key terms
for an entire course. The accuracy of the Textbook Chapter to Transcript Match
exceeds 70% on average. The methods used can be applied to genres of video in
which there are recurrent thematic words (news, sports, meetings,...)"
"Customer Service Management is one of major business activities to better
serve company customers through the introduction of reliable processes and
procedures. Today this kind of activities is implemented through e-services to
directly involve customers into business processes. Traditionally Customer
Service Management involves application of data mining techniques to discover
usage patterns from the company knowledge memory. Hence grouping of
customers/requests to clusters is one of major technique to improve the level
of company customization. The goal of this paper is to present an efficient for
implementation approach for clustering users and their requests. The approach
uses ontology as knowledge representation model to improve the semantic
interoperability between units of the company and customers. Some fragments of
the approach tested in an industrial company are also presented in the paper."
"Attempts to understand the consequence of any individual scientist's activity
within the long-term trajectory of science is one of the most difficult
questions within the philosophy of science. Because scientific publications
play such as central role in the modern enterprise of science, bibliometric
techniques which measure the ``impact'' of an individual publication as a
function of the number of citations it receives from subsequent authors have
provided some of the most useful empirical data on this question. Until
recently, Thompson/ISI has provided the only source of large-scale ``inverted''
bibliographic data of the sort required for impact analysis. In the end of
2004, Google introduced a new service, GoogleScholar, making much of this same
data available. Here we analyze 203 publications, collectively cited by more
than 4000 other publications. We show surprisingly good agreement between data
citation counts provided by the two services. Data quality across the systems
is analyzed, and potentially useful complementarities between are considered.
The additional robustness offered by multiple sources of such data promises to
increase the utility of these measurements as open citation protocols and open
access increase their impact on electronic scientific publication practices."
"The work presented in this paper is part of the cooperative research project
AUTO-OPT carried out by twelve partners from the automotive industries. One
major work package concerns the application of data mining methods in the area
of automotive design. Suitable methods for data preparation and data analysis
are developed. The objective of the work is the re-use of data stored in the
crash-simulation department at BMW in order to gain deeper insight into the
interrelations between the geometric variations of the car during its design
and its performance in crash testing. In this paper a method for data analysis
of finite element models and results from crash simulation is proposed and
application to recent data from the industrial partner BMW is demonstrated. All
necessary steps from data pre-processing to re-integration into the working
environment of the engineer are covered."
"The problem of known signal detection in Additive White Gaussian Noise is
considered. In previous work, a new detection scheme was introduced by the
authors, and it was demonstrated that optimum performance cannot be reached in
a real implementation. In this paper we analyse Support Vector Machines (SVM)
as an alternative, evaluating the results in terms of Probability of detection
curves for a fixed Probability of false alarm."
"Current approaches in pulse detection use domain transformations so as to
concentrate frequency related information that can be distinguishable from
noise. In real cases we do not know when the pulse will begin, so we need a
time search process in which time windows are scheduled and analysed. Each
window can contain the pulsed signal (either complete or incomplete) and / or
noise. In this paper a simple search process will be introduced, allowing the
algorithm to process more information, upgrading the capabilities in terms of
probability of detection (Pd) and probability of false alarm (Pfa)."
"This paper presents a short evaluation about the integration of information
derived from wavelet non-linear-time-invariant (non-LTI) projection properties
using Support Vector Machines (SVM). These properties may give additional
information for a classifier trying to detect known patterns hidden by noise.
In the experiments we present a simple electromagnetic pulsed signal
recognition scheme, where some improvement is achieved with respect to previous
work. SVMs are used as a tool for information integration, exploiting some
unique properties not easily found in neural networks."
"A neural network works as an associative memory device if it has large
storage capacity and the quality of the retrieval is good enough. The learning
and attractor abilities of the network both can be measured by the mutual
information (MI), between patterns and retrieval states. This paper deals with
a search for an optimal topology, of a Hebb network, in the sense of the
maximal MI. We use small-world topology. The connectivity $\gamma$ ranges from
an extremely diluted to the fully connected network; the randomness $\omega$
ranges from purely local to completely random neighbors. It is found that,
while stability implies an optimal $MI(\gamma,\omega)$ at
$\gamma_{opt}(\omega)\to 0$, for the dynamics, the optimal topology holds at
certain $\gamma_{opt}>0$ whenever $0\leq\omega<0.3$."
"Transitive text mining - also named Swanson Linking (SL) after its primary
and principal researcher - tries to establish meaningful links between
literature sets which are virtually disjoint in the sense that each does not
mention the main concept of the other. If successful, SL may give rise to the
development of new hypotheses. In this communication we describe our approach
to transitive text mining which employs co-occurrence analysis of the medical
subject headings (MeSH), the descriptors assigned to papers indexed in PubMed.
In addition, we will outline the current state of our web-based information
system which will enable our users to perform literature-driven hypothesis
building on their own."
"Extraction of association rules is widely used as a data mining method.
However, one of the limit of this approach comes from the large number of
extracted rules and the difficulty for a human expert to deal with the totality
of these rules. We propose to solve this problem by structuring the set of
rules into hierarchy. The expert can then therefore explore the rules, access
from one rule to another one more general when we raise up in the hierarchy,
and in other hand, or a more specific rules. Rules are structured at two
levels. The global level aims at building a hierarchy from the set of rules
extracted. Thus we define a first type of rule-subsomption relying on Galois
lattices. The second level consists in a local and more detailed analysis of
each rule. It generate for a given rule a set of generalization rules
structured into a local hierarchy. This leads to the definition of a second
type of subsomption. This subsomption comes from inductive logic programming
and integrates a terminological model."
"Sentence level novelty detection aims at reducing redundant sentences from a
sentence list. In the task, sentences appearing later in the list with no new
meanings are eliminated. Aiming at a better accuracy for detecting redundancy,
this paper reveals the nature of the novelty detection task currently
overlooked by the Novelty community $-$ Novelty as a combination of the partial
overlap (PO, two sentences sharing common facts) and complete overlap (CO, the
first sentence covers all the facts of the second sentence) relations. By
formalizing novelty detection as a combination of the two relations between
sentences, new viewpoints toward techniques dealing with Novelty are proposed.
Among the methods discussed, the similarity, overlap, pool and language
modeling approaches are commonly used. Furthermore, a novel approach, selected
pool method is provided, which is immediate following the nature of the task.
Experimental results obtained on all the three currently available novelty
datasets showed that selected pool is significantly better or no worse than the
current methods. Knowledge about the nature of the task also affects the
evaluation methodologies. We propose new evaluation measures for Novelty
according to the nature of the task, as well as possible directions for future
study."
"We discuss two techniques used to characterize bibliographic records based on
their similarity to and relationship with the contents of the NASA Astrophysics
Data System (ADS) databases. The first method has been used to classify input
text as being relevant to one or more subject areas based on an analysis of the
frequency distribution of its individual words. The second method has been used
to classify existing records as being relevant to one or more databases based
on the distribution of the papers citing them. Both techniques have proven to
be valuable tools in assigning new and existing bibliographic records to
different disciplines within the ADS databases."
"PageRank has become a key element in the success of search engines, allowing
to rank the most important hits in the top screen of results. One key aspect
that distinguishes PageRank from other prestige measures such as in-degree is
its global nature. From the information provider perspective, this makes it
difficult or impossible to predict how their pages will be ranked. Consequently
a market has emerged for the optimization of search engine results. Here we
study the accuracy with which PageRank can be approximated by in-degree, a
local measure made freely available by search engines. Theoretical and
empirical analyses lead to conclude that given the weak degree correlations in
the Web link graph, the approximation can be relatively accurate, giving
service and information providers an effective new marketing tool."
"Backup or preservation of websites is often not considered until after a
catastrophic event has occurred. In the face of complete website loss, ""lazy""
webmasters or concerned third parties may be able to recover some of their
website from the Internet Archive. Other pages may also be salvaged from
commercial search engine caches. We introduce the concept of ""lazy
preservation""- digital preservation performed as a result of the normal
operations of the Web infrastructure (search engines and caches). We present
Warrick, a tool to automate the process of website reconstruction from the
Internet Archive, Google, MSN and Yahoo. Using Warrick, we have reconstructed
24 websites of varying sizes and composition to demonstrate the feasibility and
limitations of website reconstruction from the public Web infrastructure. To
measure Warrick's window of opportunity, we have profiled the time required for
new Web resources to enter and leave search engine caches."
"Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web
search, we propose a structural re-ranking approach to ad hoc information
retrieval: we reorder the documents in an initially retrieved set by exploiting
asymmetric relationships between them. Specifically, we consider generation
links, which indicate that the language model induced from one document assigns
high probability to the text of another; in doing so, we take care to prevent
bias against long documents. We study a number of re-ranking criteria based on
measures of centrality in the graphs formed by generation links, and show that
integrating centrality into standard language-model-based retrieval is quite
effective at improving precision at top ranks."
"We present a novel approach to pseudo-feedback-based ad hoc retrieval that
uses language models induced from both documents and clusters. First, we treat
the pseudo-feedback documents produced in response to the original query as a
set of pseudo-queries that themselves can serve as input to the retrieval
process. Observing that the documents returned in response to the
pseudo-queries can then act as pseudo-queries for subsequent rounds, we arrive
at a formulation of pseudo-query-based retrieval as an iterative process.
Experiments show that several concrete instantiations of this idea, when
applied in conjunction with techniques designed to heighten precision, yield
performance results rivaling those of a number of previously-proposed
algorithms, including the standard language-modeling approach. The use of
cluster-based language models is a key contributing factor to our algorithms'
success."
"This paper presents an evolutionary algorithm for modeling the arrival dates
of document streams, which is any time-stamped collection of documents, such as
newscasts, e-mails, IRC conversations, scientific journals archives and weblog
postings. This algorithm assigns frequencies (number of document arrivals per
time unit) to time intervals so that it produces an optimal fit to the data.
The optimization is a trade off between accurately fitting the data and
avoiding too many frequency changes; this way the analysis is able to find fits
which ignore the noise. Classical dynamic programming algorithms are limited by
memory and efficiency requirements, which can be a problem when dealing with
long streams. This suggests to explore alternative search methods which allow
for some degree of uncertainty to achieve tractability. Experiments have shown
that the designed evolutionary algorithm is able to reach the same solution
quality as those classical dynamic programming algorithms in a shorter time. We
have also explored different probabilistic models to optimize the fitting of
the date streams, and applied these algorithms to infer whether a new arrival
increases or decreases {\em interest} in the topic the document stream is
about."
"The design of a publisher's electronic interface can have a measurable effect
on electronic journal usage statistics. A study of journal usage from six
COUNTER-compliant publishers at thirty-two research institutions in the United
States, the United Kingdom and Sweden indicates that the ratio of PDF to HTML
views is not consistent across publisher interfaces, even after controlling for
differences in publisher content. The number of fulltext downloads may be
artificially inflated when publishers require users to view HTML versions
before accessing PDF versions or when linking mechanisms, such as CrossRef,
direct users to the full text, rather than the abstract, of each article. These
results suggest that usage reports from COUNTER-compliant publishers are not
directly comparable in their current form. One solution may be to modify
publisher numbers with adjustment factors deemed to be representative of the
benefit or disadvantage due to its interface. Standardization of some interface
and linking protocols may obviate these differences and allow for more accurate
cross-publisher comparisons."
"This paper explores the system of categories that is used to classify
articles in Wikipedia. It is compared to collaborative tagging systems like
del.icio.us and to hierarchical classification like the Dewey Decimal
Classification (DDC). Specifics and commonalitiess of these systems of subject
indexing are exposed. Analysis of structural and statistical properties
(descriptors per record, records per descriptor, descriptor levels) shows that
the category system of Wikimedia is a thesaurus that combines collaborative
tagging and hierarchical subject indexing in a special way."
"Clickthrough data is a particularly inexpensive and plentiful resource to
obtain implicit relevance feedback for improving and personalizing search
engines. However, it is well known that the probability of a user clicking on a
result is strongly biased toward documents presented higher in the result set
irrespective of relevance. We introduce a simple method to modify the
presentation of search results that provably gives relevance judgments that are
unaffected by presentation bias under reasonable assumptions. We validate this
property of the training data in interactive real world experiments. Finally,
we show that using these unbiased relevance judgments learning methods can be
guaranteed to converge to an ideal ranking given sufficient data."
"In this article, we investigate the use of a probabilistic model for
unsupervised clustering in text collections. Unsupervised clustering has become
a basic module for many intelligent text processing applications, such as
information retrieval, text classification or information extraction. The model
considered in this contribution consists of a mixture of multinomial
distributions over the word counts, each component corresponding to a different
theme. We present and contrast various estimation procedures, which apply both
in supervised and unsupervised contexts. In supervised learning, this work
suggests a criterion for evaluating the posterior odds of new documents which
is more statistically sound than the ""naive Bayes"" approach. In an unsupervised
context, we propose measures to set up a systematic evaluation framework and
start with examining the Expectation-Maximization (EM) algorithm as the basic
tool for inference. We discuss the importance of initialization and the
influence of other features such as the smoothing strategy or the size of the
vocabulary, thereby illustrating the difficulties incurred by the high
dimensionality of the parameter space. We also propose a heuristic algorithm
based on iterative EM with vocabulary reduction to solve this problem. Using
the fact that the latent variables can be analytically integrated out, we
finally show that Gibbs sampling algorithm is tractable and compares favorably
to the basic expectation maximization approach."
"Active polarimetric imagery is a powerful tool for accessing the information
present in a scene. Indeed, the polarimetric images obtained can reveal
polarizing properties of the objects that are not avalaible using conventional
imaging systems. However, when coherent light is used to illuminate the scene,
the images are degraded by speckle noise. The polarization properties of a
scene are characterized by the degree of polarization. In standard polarimetric
imagery system, four intensity images are needed to estimate this degree . If
we assume the uncorrelation of the measurements, this number can be decreased
to two images using the Orthogonal State Contrast Image (OSCI). However, this
approach appears too restrictive in some cases. We thus propose in this paper a
new statistical parametric method to estimate the degree of polarization
assuming correlated measurements with only two intensity images. The estimators
obtained from four images, from the OSCI and from the proposed method, are
compared using simulated polarimetric data degraded by speckle noise."
"The program Synarcher for synonym (and related terms) search in the text
corpus of special structure (Wikipedia) was developed. The results of the
search are presented in the form of graph. It is possible to explore the graph
and search for graph elements interactively. Adapted HITS algorithm for synonym
search, program architecture, and program work evaluation with test examples
are presented in the paper. The proposed algorithm can be applied to a query
expansion by synonyms (in a search engine) and a synonym dictionary forming."
"HITS adapted algorithm for synonym search, the program architecture, and the
program work evaluation with test examples are presented in the paper.
Synarcher program for synonym (and related terms) search in the text corpus of
special structure (Wikipedia) was developed. The results of search are
presented in the form of a graph. It is possible to explore the graph and
search graph elements interactively. The proposed algorithm could be applied to
the search request extending and for synonym dictionary forming."