summary
"We study the configurations of pixels that occur when two digitized straight
lines meet each other."
"This paper presents an efficient method for generating and rendering
photorealistic hair in two dimensional pictures. The method consists of three
major steps. Simulating an artist drawing is used to design the rough hair
shape. A convolution based filter is then used to generate photorealistic hair
patches. A refine procedure is finally used to blend the boundaries of the
patches with surrounding areas. This method can be used to create all types of
photorealistic human hair (head hair, facial hair and body hair). It is also
suitable for fur and grass generation. Applications of this method include:
hairstyle designing/editing, damaged hair image restoration, human hair
animation, virtual makeover of a human, and landscape creation."
"Environment maps are used to simulate reflections off curved objects. We
present a technique to reflect a user, or a group of users, in a real
environment, onto a virtual object, in a virtual reality application, using the
live video feeds from a set of cameras, in real-time. Our setup can be used in
a variety of environments ranging from outdoor or indoor scenes."
"The Persint program is designed for the three-dimensional representation of
objects and for the interfacing and access to a variety of independent
applications, in a fully interactive way. Facilities are provided for the
spatial navigation and the definition of the visualization properties, in order
to interactively set the viewing and viewed points, and to obtain the desired
perspective. In parallel, applications may be launched through the use of
dedicated interfaces, such as the interactive reconstruction and display of
physics events. Recent developments have focalized on the interfacing to the
XML ATLAS General Detector Description AGDD, making it a widely used tool for
XML developers. The graphics capabilities of this program were exploited in the
context of the ATLAS 2002 Muon Testbeam where it was used as an online event
display, integrated in the online software framework and participating in the
commissioning and debug of the detector system."
"Many entities managed by HEP Software Frameworks represent spatial
(3-dimensional) real objects. Effective definition, manipulation and
visualization of such objects is an indispensable functionality.
  GraXML is a modular Geometric Modeling toolkit capable of processing
geometric data of various kinds (detector geometry, event geometry) from
different sources and delivering them in ways suitable for further use.
Geometric data are first modeled in one of the Generic Models. Those Models are
then used to populate powerful Geometric Model based on the Java3D technology.
While Java3D has been originally created just to provide visualization of 3D
objects, its light weight and high functionality allow an effective reuse as a
general geometric component. This is possible also thanks to a large overlap
between graphical and general geometric functionality and modular design of
Java3D itself. Its graphical functionalities also allow a natural visualization
of all manipulated elements.
  All these techniques have been developed primarily (or only) for the Java
environment. It is, however, possible to interface them transparently to
Frameworks built in other languages, like for example C++.
  The GraXML toolkit has been tested with data from several sources, as for
example ATLAS and ALICE detector description and ATLAS event data. Prototypes
for other sources, like Geometry Description Markup Language (GDML) exist too
and interface to any other source is easy to add."
"A new graphics client prototype for the HepRep protocol is presented. Based
on modern toolkits and high level languages (C++ and Ruby), Fred is an
experiment to test applicability of scripting facilities to the high energy
physics event display domain. Its flexible structure, extensibility and the use
of the HepRep protocol are key features for its use in the astroparticle
experiment GLAST."
"HepRep is a generic, hierarchical format for description of graphics
representables that can be augmented by physics information and relational
properties. It was developed for high energy physics event display applications
and is especially suited to client/server or component frameworks. The GLAST
experiment, an international effort led by NASA for a gamma-ray telescope to
launch in 2006, chose HepRep to provide a flexible, extensible and maintainable
framework for their event display without tying their users to any one graphics
application. To support HepRep in their GUADI infrastructure, GLAST developed a
HepRep filler and builder architecture. The architecture hides the details of
XML and CORBA in a set of base and helper classes allowing physics experts to
focus on what data they want to represent. GLAST has two GAUDI services:
HepRepSvc, which registers HepRep fillers in a global registry and allows the
HepRep to be exported to XML, and CorbaSvc, which allows the HepRep to be
published through a CORBA interface and which allows the client application to
feed commands back to GAUDI (such as start next event, or run some GAUDI
algorithm). GLAST's HepRep solution gives users a choice of client
applications, WIRED (written in Java) or FRED (written in C++ and Ruby), and
leaves them free to move to any future HepRep-compliant event display."
"We present an efficient and inexpensive to develop application for
interactive high-performance parallel visualization. We extend popular APIs
such as Open Inventor and VTK to support commodity-based cluster visualization.
Our implementation follows a standard master/slave concept: the general idea is
to have a ``Master'' node, which will intercept a sequential graphical user
interface (GUI) and broadcast it to the ``Slave'' nodes. The interactions
between the nodes are implemented using MPI. The parallel remote rendering uses
Chromium. This paper is mainly the report of our implementation experiences. We
present in detail the proposed model and key aspects of its implementation.
Also, we present performance measurements, we benchmark and quantitatively
demonstrate the dependence of the visualization speed on the data size and the
network bandwidth, and we identify the singularities and draw conclusions on
Chromium's sort-first rendering architecture. The most original part of this
work is the combined use of Open Inventor and Chromium."
"Conventional visualization media such as MRI prints and computer screens are
inherently two dimensional, making them incapable of displaying true 3D volume
data sets. By applying only transparency or intensity projection, and ignoring
light-matter interaction, results will likely fail to give optimal results.
Little research has been done on using reflectance functions to visually
separate the various segments of a MRI volume. We will explore if applying
specific reflectance functions to individual anatomical structures can help in
building an intuitive 2D image from a 3D dataset. We will test our hypothesis
by visualizing a statistical analysis of the genetic influences on variations
in human brain morphology because it inherently contains complex and many
different types of data making it a good candidate for our approach"
"This paper presents an algorithm that transforms color visual images, like
photographs or paintings, into tactile graphics. In the algorithm, the edges of
objects are detected and colors of the objects are estimated. Then, the edges
and the colors are encoded into lines and textures in the output tactile image.
Design of the method is substantiated by various qualities of haptic
recognizing of images. Also, means of presentation of the tactile images in
printouts are discussed. Example translated images are shown."
"We develop multiple view visualization of higher dimensional data. Our work
was chiefly motivated by the need to extract insight from four dimensional
Quantum Chromodynamic (QCD) data. We develop visualization where multiple
views, generally views of 3D projections or slices of a higher dimensional
data, are tightly coupled not only by their specific order but also by a view
synchronizing interaction style, and an internally defined interaction
language. The tight coupling of the different views allows a fast and
well-coordinated exploration of the data. In particular, the visualization
allowed us to easily make consistency checks of the 4D QCD data and to infer
the correctness of particle properties calculations. The software developed was
also successfully applied in material studies, in particular studies of
meteorite properties. Our implementation uses the VTK API. To handle a large
number of views (slices/projections) and to still maintain good resolution, we
use IBM T221 display (3840 X 2400 pixels)."
"The past two decades showed a rapid growing of physically-based modeling of
fluids for computer graphics applications. In this area, a common top down
approach is to model the fluid dynamics by Navier-Stokes equations and apply a
numerical techniques such as Finite Differences or Finite Elements for the
simulation. In this paper we focus on fluid modeling through Lattice Gas
Cellular Automata (LGCA) for computer graphics applications. LGCA are discrete
models based on point particles that move on a lattice, according to suitable
and simple rules in order to mimic a fully molecular dynamics. By
Chapman-Enskog expansion, a known multiscale technique in this area, it can be
demonstrated that the Navier-Stokes model can be reproduced by the LGCA
technique. Thus, with LGCA we get a fluid model that does not require solution
of complicated equations. Therefore, we combine the advantage of the low
computational cost of LGCA and its ability to mimic the realistic fluid
dynamics to develop a new animating framework for computer graphics
applications. In this work, we discuss the theoretical elements of our proposal
and show experimental results."
"Von Neuman's work on universal machines and the hardware development have
allowed the simulation of dynamical systems through a large set of interacting
agents. This is a bottom-up approach which tries to derive global properties of
a complex system through local interaction rules and agent behaviour.
Traditionally, such systems are modeled and simulated through top-down methods
based on differential equations. Agent-Based Modeling has the advantage of
simplicity and low computational cost. However, unlike differential equations,
there is no standard way to express agent behaviour. Besides, it is not clear
how to analytically predict the results obtained by the simulation. In this
paper we survey some of these methods. For expressing agent behaviour formal
methods, like Stochastic Process Algebras have been used. Such approach is
useful if the global properties of interest can be expressed as a function of
stochastic time series. However, if space variables must be considered, we
shall change the focus. In this case, multiscale techniques, based on
Chapman-Enskog expansion, was used to establish the connection between the
microscopic dynamics and the macroscopic observables. Also, we use data mining
techniques,like Principal Component Analysis (PCA), to study agent systems like
Cellular Automata. With the help of these tools we will discuss a simple
society model, a Lattice Gas Automaton for fluid modeling, and knowledge
discovery in CA databases. Besides, we show the capabilities of the NetLogo, a
software for agent simulation of complex system and show our experience about."
"This article introduces a Mathematica package providing a graphics export
function that automatically replaces Mathematica expressions in a graphic by
the corresponding LaTeX constructs and positions them correctly. It thus
facilitates the creation of publication-quality Enscapulated PostScript (EPS)
graphics."
"We define a Graphics Turing Test to measure graphics performance in a similar
manner to the definition of the traditional Turing Test. To pass the test one
needs to reach a computational scale, the Graphics Turing Scale, for which
Computer Generated Imagery becomes comparatively indistinguishable from real
images while also being interactive. We derive an estimate for this
computational scale which, although large, is within reach of todays
supercomputers. We consider advantages and disadvantages of various computer
systems designed to pass the Graphics Turing Test. Finally we discuss
commercial applications from the creation of such a system, in particular
Interactive Cinema."
"We describe a system that lets a designer interactively draw patterns of
strokes in the picture plane, then guide the synthesis of similar patterns over
new picture regions. Synthesis is based on an initial user-assisted analysis
phase in which the system recognizes distinct types of strokes (hatching and
stippling) and organizes them according to perceptual grouping criteria. The
synthesized strokes are produced by combining properties (eg. length,
orientation, parallelism, proximity) of the stroke groups extracted from the
input examples. We illustrate our technique with a drawing application that
allows the control of attributes and scale-dependent reproduction of the
synthesized patterns."
"The paper describes a new image processing for a non-photorealistic
rendering. The algorithm is based on a random generation of gray tones and
competing statistical requirements. The gray tone value of each pixel in the
starting image is replaced selecting among randomly generated tone values,
according to the statistics of nearest-neighbor and next-nearest-neighbor
pixels. Two competing conditions for replacing the tone values - one position
on the local mean value the other on the local variance - produce a peculiar
pattern on the image. This pattern has a labyrinthine tiling aspect. For
certain subjects, the pattern enhances the look of the image."
"We have recently developed an algorithm for vector field visualization with
oriented streamlines, able to depict the flow directions everywhere in a dense
vector field and the sense of the local orientations. The algorithm has useful
applications in the visualization of the director field in nematic liquid
crystals. Here we propose an improvement of the algorithm able to enhance the
visualization of the local magnitude of the field. This new approach of the
algorithm is compared with the same procedure applied to the Line Integral
Convolution (LIC) visualization."
"Shape preservation behavior of a spline consists of criterial conditions for
preserving convexity, inflection, collinearity, torsion and coplanarity shapes
of data polgonal arc. We present our results which acts as an improvement in
the definitions of and provide geometrical insight into each of the above shape
preservation criteria. We also investigate the effect of various results from
the literature on various shape preservation criteria. These results have not
been earlier refered in the context of shape preservation behaviour of splines.
We point out that each curve segment need to satisfy more than one shape
preservation criteria. We investigate the conflict between different shape
preservation criteria 1)on each curve segment and 2)of adjacent curve segments.
We derive simplified formula for shape preservation criteria for cubic curve
segments. We study the shape preservation behavior of cubic Catmull-Rom splines
and see that, though being very simple spline curve, it indeed satisfy all the
shape preservation criteria."
"The next generation of virtual environments for training is oriented towards
collaborative aspects. Therefore, we have decided to enhance our platform for
virtual training environments, adding collaboration opportunities and
integrating humanoids. In this paper we put forward a model of humanoid that
suits both virtual humans and representations of real users, according to
collaborative training activities. We suggest adaptations to the scenario model
of our platform making it possible to write collaborative procedures. We
introduce a mechanism of action selection made up of a global repartition and
an individual choice. These models are currently being integrated and validated
in GVT, a virtual training tool for maintenance of military equipments,
developed in collaboration with the French company NEXTER-Group."
"We investigate raytracing performance that can be achieved on a class of Blue
Gene supercomputers. We measure a 822 times speedup over a Pentium IV on a 6144
processor Blue Gene/L. We measure the computational performance as a function
of number of processors and problem size to determine the scaling performance
of the raytracing calculation on the Blue Gene. We find nontrivial scaling
behavior at large number of processors. We discuss applications of this
technology to scientific visualization with advanced lighting and high
resolution. We utilize three racks of a Blue Gene/L in our calculations which
is less than three percent of the the capacity of the worlds largest Blue Gene
computer."
"This article introduces the next version of MathPSfrag. MathPSfrag is a
Mathematica package that during export automatically replaces all expressions
in a plot by corresponding LaTeX commands. The new version can also produce
LaTeX independent images; e.g., PDF files for inclusion in pdfLaTeX. Moreover
from these files a preview is generated and shown within Mathematica."
"We consider perturbations of the complex quadratic map $ z \to z^2 +c$ and
corresponding changes in their quasi-Mandelbrot sets. Depending on particular
perturbation, visual forms of quasi-Mandelbrot set changes either sharply (when
the perturbation reaches some critical value) or continuously. In the latter
case we have a smooth transition from the classical form of the set to some
forms, constructed from mostly linear structures, as it is typical for
two-dimensional real number dynamics. Two examples of continuous evolution of
the quasi-Mandelbrot set are described."
"Many image watermarking schemes have been proposed in recent years, but they
usually involve embedding a watermark to the entire image without considering
only a particular object in the image, which the image owner may be interested
in. This paper proposes a watermarking scheme that can embed a watermark to an
arbitrarily shaped object in an image. Before embedding, the image owner
specifies an object of arbitrary shape that is of a concern to him. Then the
object is transformed into the wavelet domain using in place lifting shape
adaptive DWT(SADWT) and a watermark is embedded by modifying the wavelet
coefficients. In order to make the watermark robust and transparent, the
watermark is embedded in the average of wavelet blocks using the visual model
based on the human visual system. Wavelet coefficients n least significant bits
(LSBs) are adjusted in concert with the average. Simulation results shows that
the proposed watermarking scheme is perceptually invisible and robust against
many attacks such as lossy compression (e.g.JPEG, JPEG2000), scaling, adding
noise, filtering, etc."
"In this paper a novel spatial domain LSB based watermarking scheme for color
Images is proposed. The proposed scheme is of type blind and invisible
watermarking. Our scheme introduces the concept of storing variable number of
bits in each pixel based on the actual color value of pixel. Equal or higher
the color value of channels with respect to intensity of pixel stores higher
number of watermark bits. The Red, Green and Blue channel of the color image
has been used for watermark embedding. The watermark is embedded into selected
channels of pixel. The proposed method supports high watermark embedding
capacity, which is equivalent to the size of cover image. The security of
watermark is preserved by permuting the watermark bits using secret key. The
proposed scheme is found robust to various image processing operations such as
image compression, blurring, salt and pepper noise, filtering and cropping."
"Separation of the text regions from background texture and graphics is an
important step of any optical character recognition system for the images
containing both texts and graphics. In this paper, we have presented a novel
text/graphics separation technique and a method for skew correction of text
regions extracted from business card images captured with a cell-phone camera.
At first, the background is eliminated at a coarse level based on intensity
variance. This makes the foreground components distinct from each other. Then
the non-text components are removed using various characteristic features of
text and graphics. Finally, the text regions are skew corrected for further
processing. Experimenting with business card images of various resolutions, we
have found an optimum performance of 98.25% (recall) with 0.75 MP images, that
takes 0.17 seconds processing time and 1.1 MB peak memory on a moderately
powerful computer (DualCore 1.73 GHz Processor, 1 GB RAM, 1 MB L2 Cache). The
developed technique is computationally efficient and consumes low memory so as
to be applicable on mobile devices."
"This paper describes visualization of chaotic attractor and elements of the
singularities in 3D space. 3D view of these effects enables to create a
demonstrative projection about relations of chaos generated by physical
circuit, the Chua's circuit. Via macro views on chaotic attractor is obtained
not only visual space illustration of representative point motion in state
space, but also its relation to planes of singularity elements. Our created
program enables view on chaotic attractor both in 2D and 3D space together with
plane objects visualization -- elements of singularities."
"Many times we need to plot 3-D functions e.g., in many scientificc
experiments. To plot this 3-D functions on 2-D screen it requires some kind of
mapping. Though OpenGL, DirectX etc 3-D rendering libraries have made this job
very simple, still these libraries come with many complex pre- operations that
are simply not intended, also to integrate these libraries with any kind of
system is often a tough trial. This article presents a very simple method of
mapping from 3D to 2D, that is free from any complex pre-operation, also it
will work with any graphics system where we have some primitive 2-D graphics
function. Also we discuss the inverse transform and how to do basic computer
graphics transformations using our coordinate mapping system."
"In this thesis a rendering system and an accompanying tool chain for Virtual
Texturing is presented. Our tools allow to automatically retexture existing
geometry in order to apply unique texturing on each face. Furthermore we
investigate several techniques that try to minimize visual artifacts in the
case that only a small amount of pages can be streamed per frame. We analyze
the influence of different heuristics that are responsible for the page
selection. Alongside these results we present a measurement method to allow the
comparison of our heuristics."
"The paper proposes an ancient landscape design as an example of graphic
design for an age and place where no written documents existed. It is created
by a network of earthworks, which constitute the remains of an extensive
ancient agricultural system. It can be seen by means of the Google satellite
imagery on the Peruvian region near the Titicaca Lake, as a texture
superimposed to the background landform. In this texture, many drawings
(geoglyphs) can be observed."
"In this work SVG will be translated into VML or HTML by using Javascript
based on Backbase Client Framework. The target of this project is to implement
SVG to be viewed in Internet Explorer without any plug-in and work together
with other Backbase Client Framework languages. The result of this project will
be added as an extension to the current Backbase Client Framework."
"This paper presents a high speed and area efficient DWT processor based
design for Image Compression applications. In this proposed design, pipelined
partially serial architecture has been used to enhance the speed along with
optimal utilization and resources available on target FPGA. The proposed model
has been designed and simulated using Simulink and System Generator blocks,
synthesized with Xilinx Synthesis tool (XST) and implemented on Spartan 2 and 3
based XC2S100-5tq144 and XC3S500E-4fg320 target device. The results show that
proposed design can operate at maximum frequency 231 MHz in case of Spartan 3
by consuming power of 117mW at 28 degree/c junction temperature. The result
comparison has shown an improvement of 15% in speed."
"Color quantization is an important operation with many applications in
graphics and image processing. Most quantization methods are essentially based
on data clustering algorithms. However, despite its popularity as a general
purpose clustering algorithm, k-means has not received much respect in the
color quantization literature because of its high computational requirements
and sensitivity to initialization. In this paper, we investigate the
performance of k-means as a color quantizer. We implement fast and exact
variants of k-means with several initialization schemes and then compare the
resulting quantizers to some of the most popular quantizers in the literature.
Experiments on a diverse set of images demonstrate that an efficient
implementation of k-means with an appropriate initialization strategy can in
fact serve as a very effective color quantizer."
"We present a novel method of simulating wave effects in graphics using
ray--based renderers with a new function: the Wave BSDF (Bidirectional
Scattering Distribution Function). Reflections from neighboring surface patches
represented by local BSDFs are mutually independent. However, in many surfaces
with wavelength-scale microstructures, interference and diffraction requires a
joint analysis of reflected wavefronts from neighboring patches. We demonstrate
a simple method to compute the BSDF for the entire microstructure, which can be
used independently for each patch. This allows us to use traditional ray--based
rendering pipelines to synthesize wave effects of light and sound. We exploit
the Wigner Distribution Function (WDF) to create transmissive, reflective, and
emissive BSDFs for various diffraction phenomena in a physically accurate way.
In contrast to previous methods for computing interference, we circumvent the
need to explicitly keep track of the phase of the wave by using BSDFs that
include positive as well as negative coefficients. We describe and compare the
theory in relation to well understood concepts in rendering and demonstrate a
straightforward implementation. In conjunction with standard raytracers, such
as PBRT, we demonstrate wave effects for a range of scenarios such as
multi--bounce diffraction materials, holograms and reflection of high frequency
surfaces."
"In this technical note, we present the formulae of the derivatives of the
Mean Value Coordinates based transformations, using an enclosing triangle mesh,
acting as a cage for the deformation of an interior object."
"We describe a technique for bundled curve representations in
parallel-coordinates plots and present a controlled user study evaluating their
effectiveness. Replacing the traditional C^0 polygonal lines by C^1 continuous
piecewise Bezier curves makes it easier to visually trace data points through
each coordinate axis. The resulting Bezier curves can then be bundled to
visualize data with given cluster structures. Curve bundles are efficient to
compute, provide visual separation between data clusters, reduce visual
clutter, and present a clearer overview of the dataset. A controlled user study
with 14 participants confirmed the effectiveness of curve bundling for
parallel-coordinates visualization: 1) compared to polygonal lines, it is
equally capable of revealing correlations between neighboring data attributes;
2) its geometric cues can be effective in displaying cluster information. For
some datasets curve bundling allows the color perceptual channel to be applied
to other data attributes, while for complex cluster patterns, bundling and
color can represent clustering far more clearly than either alone."
"This paper presents a survey of ocean simulation and rendering methods in
computer graphics. To model and animate the ocean's surface, these methods
mainly rely on two main approaches: on the one hand, those which approximate
ocean dynamics with parametric, spectral or hybrid models and use empirical
laws from oceanographic research. We will see that this type of methods
essentially allows the simulation of ocean scenes in the deep water domain,
without breaking waves. On the other hand, physically-based methods use
Navier-Stokes Equations (NSE) to represent breaking waves and more generally
ocean surface near the shore. We also describe ocean rendering methods in
computer graphics, with a special interest in the simulation of phenomena such
as foam and spray, and light's interaction with the ocean surface."
"Without careful long-term preservation digital data may be lost to a number
of factors, including physical media decay, lack of suitable decoding
equipment, and the absence of software. When raw data can be read but lack
suitable annotations as to provenance, the ability to interpret them is more
straightforward if they can be assessed through simple visual techniques. In
this regard digital images are a special case since their data have a natural
representation on two-dimensional media surfaces. This paper presents a novel
binary image pixel encoding that produces an approximate analog rendering of
encoded images when the image bits are arranged spatially in an appropriate
manner. This simultaneous digital and analog representation acts to inseparably
annotate bits as image data, which may contribute to the longevity of
so-encoded images."
"We propose a novel method for fitting planar B-spline curves to unorganized
data points. In traditional methods, optimization of control points and foot
points are performed in two very time-consuming steps in each iteration: 1)
control points are updated by setting up and solving a linear system of
equations; and 2) foot points are computed by projecting each data point onto a
B-spline curve. Our method uses the L-BFGS optimization method to optimize
control points and foot points simultaneously and therefore it does not need to
perform either matrix computation or foot point projection in every iteration.
As a result, our method is much faster than existing methods."
"We present GLSL implementations of Perlin noise and Perlin simplex noise that
run fast enough for practical consideration on current generation GPU hardware.
The key benefits are that the functions are purely computational, i.e. they use
neither textures nor lookup tables, and that they are implemented in GLSL
version 1.20, which means they are compatible with all current GLSL-capable
platforms, including OpenGL ES 2.0 and WebGL 1.0. Their performance is on par
with previously presented GPU implementations of noise, they are very
convenient to use, and they scale well with increasing parallelism in present
and upcoming GPU architectures."
"Image resampling is a necessary component of any operation that changes the
size of an image or its geometry.
  Methods tuned for natural image upsampling (roughly speaking, image
enlargement) are analyzed and developed with a focus on their ability to
preserve diagonal features and suppress overshoots. Monotone, locally bounded
and almost monotone ""direct"" interpolation and filtering methods, as well as
face split and vertex split surface subdivision methods, alone or in
combination, are studied. Key properties are established by way of proofs and
counterexamples as well as numerical experiments involving 1D curve and 2D
diagonal data resampling.
  In addition, the Remez minimax method for the computation of low-cost
polynomial approximations of low-pass filter kernels tuned for natural image
downsampling (roughly speaking, image reduction) is refactored for relative
error minimization in the presence of roots in the interior of the interval of
approximation and so that even and odd functions are approximated with like
polynomials. The accuracy and frequency response of the approximations are
tabulated and plotted against the original, establishing their rapid
convergence."
"We introduce the heat method for computing the shortest geodesic distance to
a specified subset (e.g., point or curve) of a given domain. The heat method is
robust, efficient, and simple to implement since it is based on solving a pair
of standard linear elliptic problems. The method represents a significant
breakthrough in the practical computation of distance on a wide variety of
geometric domains, since the resulting linear systems can be prefactored once
and subsequently solved in near-linear time. In practice, distance can be
updated via the heat method an order of magnitude faster than with
state-of-the-art methods while maintaining a comparable level of accuracy. We
provide numerical evidence that the method converges to the exact geodesic
distance in the limit of refinement; we also explore smoothed approximations of
distance suitable for applications where more regularity is required."
"The Block Transform Coded, JPEG- a lossy image compression format has been
used to keep storage and bandwidth requirements of digital image at practical
levels. However, JPEG compression schemes may exhibit unwanted image artifacts
to appear - such as the 'blocky' artifact found in smooth/monotone areas of an
image, caused by the coarse quantization of DCT coefficients. A number of image
filtering approaches have been analyzed in literature incorporating
value-averaging filters in order to smooth out the discontinuities that appear
across DCT block boundaries. Although some of these approaches are able to
decrease the severity of these unwanted artifacts to some extent, other
approaches have certain limitations that cause excessive blurring to
high-contrast edges in the image. The image deblocking algorithm presented in
this paper aims to filter the blocked boundaries. This is accomplished by
employing smoothening, detection of blocked edges and then filtering the
difference between the pixels containing the blocked edge. The deblocking
algorithm presented has been successful in reducing blocky artifacts in an
image and therefore increases the subjective as well as objective quality of
the reconstructed image."
"This paper presents the nearest neighbor value (NNV) algorithm for high
resolution (H.R.) image interpolation. The difference between the proposed
algorithm and conventional nearest neighbor algorithm is that the concept
applied, to estimate the missing pixel value, is guided by the nearest value
rather than the distance. In other words, the proposed concept selects one
pixel, among four directly surrounding the empty location, whose value is
almost equal to the value generated by the conventional bilinear interpolation
algorithm. The proposed method demonstrated higher performances in terms of
H.R. when compared to the conventional interpolation algorithms mentioned."
"In this paper, we study the generation of maximal Poisson-disk sets with
varying radii. First, we present a geometric analysis of gaps in such disk
sets. This analysis is the basis for maximal and adaptive sampling in Euclidean
space and on manifolds. Second, we propose efficient algorithms and data
structures to detect gaps and update gaps when disks are inserted, deleted,
moved, or have their radius changed. We build on the concepts of the regular
triangulation and the power diagram. Third, we will show how our analysis can
make a contribution to the state-of-the-art in surface remeshing."
"We consider the problem of finding a color scale which performs well when
converted to a grayscale. We assume that each color is converted to a shade of
gray with the same intensity as the color. We also assume that the color scales
have a linear variation of intensity and hue, and find scales which maximize
the average chroma (or ""colorfulness"") of the colors. We find two classes of
solutions, which traverse the color wheel in opposite directions. The two
classes of scales start with hues near cyan and red. The average chroma of the
scales are 65-77% those of the pure colors."
"We establish several fundamental properties of analysis-suitable T-splines
which are important for design and analysis. First, we characterize T-spline
spaces and prove that the space of smooth bicubic polynomials, defined over the
extended T-mesh of an analysis-suitable T-spline, is contained in the
corresponding analysis-suitable T-spline space. This is accomplished through
the theory of perturbed analysis-suitable T-spline spaces and a simple
topological dimension formula. Second, we establish the theory of
analysis-suitable local refinement and describe the conditions under which two
analysis-suitable T-spline spaces are nested. Last, we demonstrate that these
results can be used to establish basic approximation results which are critical
for analysis."
"Real-time generation of natural-looking floor plans is vital in games with
dynamic environments. This paper presents an algorithm to generate suburban
house floor plans in real-time. The algorithm is based on the work presented in
[1]. However, the corridor placement is redesigned to produce floor plans
similar to real houses. Moreover, an optimization stage is added to find a
corridor placement with the minimum used space, an approach that is designed to
mimic the real-life practices to minimize the wasted spaces in the design. The
results show very similar floor plans to the ones designed by an architect."
"We present a full pipeline for computing the medial axis transform of an
arbitrary 2D shape. The instability of the medial axis transform is overcome by
a pruning algorithm guided by a user-defined Hausdorff distance threshold. The
stable medial axis transform is then approximated by spline curves in 3D to
produce a smooth and compact representation. These spline curves are computed
by minimizing the approximation error between the input shape and the shape
represented by the medial axis transform. Our results on various 2D shapes
suggest that our method is practical and effective, and yields faithful and
compact representations of medial axis transforms of 2D shapes."
"We present a novel methodology that utilizes 4-Dimensional (4D) space
deformation to simulate a magnification lens on versatile volume datasets and
textured solid models. Compared with other magnification methods (e.g.,
geometric optics, mesh editing), 4D differential geometry theory and its
practices are much more flexible and powerful for preserving shape features
(i.e., minimizing angle distortion), and easier to adapt to versatile solid
models. The primary advantage of 4D space lies at the following fact: we can
now easily magnify the volume of regions of interest (ROIs) from the additional
dimension, while keeping the rest region unchanged. To achieve this primary
goal, we first embed a 3D volumetric input into 4D space and magnify ROIs in
the 4th dimension. Then we flatten the 4D shape back into 3D space to
accommodate other typical applications in the real 3D world. In order to
enforce distortion minimization, in both steps we devise the high dimensional
geometry techniques based on rigorous 4D geometry theory for 3D/4D mapping back
and forth to amend the distortion. Our system can preserve not only focus
region, but also context region and global shape. We demonstrate the
effectiveness, robustness, and efficacy of our framework with a variety of
models ranging from tetrahedral meshes to volume datasets."
"In this paper we describe a new technique to generate and use surfels for
rendering of highly complex, polygonal 3D scenes in real time. The basic idea
is to approximate complex parts of the scene by rendering a set of points
(surfels). The points are computed in a preprocessing step and offer two
important properties: They are placed only on the visible surface of the
scene's geometry and they are distributed and sorted in such a way, that every
prefix of points is a good visual representation of the approximated part of
the scene. An early evaluation of the method shows that it is capable of
rendering scenes consisting of several billions of triangles with high image
quality."
"In this article we present Zahir, a framework for experimentation in Computer
Graphics that provides a group of object-oriented base components that take
care of common tasks in rendering techniques and algorithms, specially those of
Non Photo-realistic Rendering (NPR). These components allow developers to
implement rendering techniques and algorithms over static and animated meshes.
Currently, Zahir is being used in a Master's Thesis and as support material in
the undergraduate Computer Graphics course in University of Chile."
"A comprehensive framework for detection and characterization of overlapping
intrinsic symmetry over 3D shapes is proposed. To identify prominent symmetric
regions which overlap in space and vary in form, the proposed framework is
decoupled into a Correspondence Space Voting procedure followed by a
Transformation Space Mapping procedure. In the correspondence space voting
procedure, significant symmetries are first detected by identifying surface
point pairs on the input shape that exhibit local similarity in terms of their
intrinsic geometry while simultaneously maintaining an intrinsic distance
structure at a global level. Since different point pairs can share a common
point, the detected symmetric shape regions can potentially overlap. To this
end, a global intrinsic distance-based voting technique is employed to ensure
the inclusion of only those point pairs that exhibit significant symmetry. In
the transformation space mapping procedure, the Functional Map framework is
employed to generate the final map of symmetries between point pairs. The
transformation space mapping procedure ensures the retrieval of the underlying
dense correspondence map throughout the 3D shape that follows a particular
symmetry. Additionally, the formulation of a novel cost matrix enables the
inner product to succesfully indicate the complexity of the underlying symmetry
transformation. The proposed transformation space mapping procedure is shown to
result in the formulation of a semi-metric symmetry space where each point in
the space represents a specific symmetry transformation and the distance
between points represents the complexity between the corresponding
transformations. Experimental results show that the proposed framework can
successfully process complex 3D shapes that possess rich symmetries."
"We propose connectivity-preserving geometry images (CGIMs), which map a
three-dimensional mesh onto a rectangular regular array of an image, such that
the reconstructed mesh produces no sampling errors, but merely round-off
errors. We obtain a V-matrix with respect to the original mesh, whose elements
are vertices of the mesh, which intrinsically preserves the vertex-set and the
connectivity of the original mesh in the sense of allowing round-off errors. We
generate a CGIM array by using the Cartesian coordinates of corresponding
vertices of the V-matrix. To reconstruct a mesh, we obtain a vertex-set and an
edge-set by collecting all the elements with different pixels, and all
different pairwise adjacent elements from the CGIM array respectively. Compared
with traditional geometry images, CGIMs achieve minimum reconstruction errors
with an efficient parametrization-free algorithm via elementary permutation
techniques. We apply CGIMs to lossy compression of meshes, and the experimental
results show that CGIMs perform well in reconstruction precision and detail
preservation."
"In this paper the problem of matching Forward Kinematics (FK) motion of a 3
Dimensional (3D) joint chain to the Inverse Kinematics (IK) movement and vice
versa has been addressed. The problem lies at the heart of animating a 3D
character having controller and manipulator based rig for animation within any
3D modeling and animation software. The seamless matching has been achieved
through the use of pseudo-inverse of Jacobian Matrix. The Jacobian Matrix is
used to determine the rotation values of each joint of character body part such
as arms, between the inverse kinematics and forward kinematics motion. Then
moving the corresponding kinematic joint system to the desired place,
automatically eliminating the jumping or popping effect which would reduce the
complexity of the system."
"A new computer haptics algorithm to be used in general interactive
manipulations of deformable virtual objects is presented. In multimodal
interactive simulations, haptic feedback computation often comes from contact
forces. Subsequently, the fidelity of haptic rendering depends significantly on
contact space modeling. Contact and friction laws between deformable models are
often simplified in up to date methods. They do not allow a ""realistic""
rendering of the subtleties of contact space physical phenomena (such as slip
and stick effects due to friction or mechanical coupling between contacts). In
this paper, we use Signorini's contact law and Coulomb's friction law as a
computer haptics basis. Real-time performance is made possible thanks to a
linearization of the behavior in the contact space, formulated as the so-called
Delassus operator, and iteratively solved by a Gauss-Seidel type algorithm.
Dynamic deformation uses corotational global formulation to obtain the Delassus
operator in which the mass and stiffness ratio are dissociated from the
simulation time step. This last point is crucial to keep stable haptic
feedback. This global approach has been packaged, implemented, and tested.
Stable and realistic 6D haptic feedback is demonstrated through a clipping task
experiment."
"The Phong illumination model is still widely used in realtime 3D
visualization systems. The aim of this article is to document problems with the
Phong illumination model that are encountered by an important professional user
group, namely digital designers. This leads to a visual evaluation of Phong
illumination, which at least in this condensed form seems still to be missing
in the literature. It is hoped that by explicating these flaws, awareness about
the limitations and interdependencies of the model will increase, both among
fellow users, and among researchers and developers."
"Gliomas are the most common primary brain tumors, evolving from the cerebral
supportive cells. For clinical follow-up, the evaluation of the preoperative
tumor volume is essential. Volumetric assessment of tumor volume with manual
segmentation of its outlines is a time-consuming process that can be overcome
with the help of computer-assisted segmentation methods. In this paper, a
semi-automatic approach for World Health Organization (WHO) grade IV glioma
segmentation is introduced that uses balloon inflation forces, and relies on
the detection of high-intensity tumor boundaries that are coupled by using
contrast agent gadolinium. The presented method is evaluated on 27 magnetic
resonance imaging (MRI) data sets and the ground truth data of the tumor
boundaries - for evaluation of the results - are manually extracted by
neurosurgeons."
"Simulation of human motion is the subject of study in a number of
disciplines: Biomechanics, Robotics, Computer Animation, Control Theory,
Neurophysiology, Medicine, Ergonomics. Since the author has never visited any
of these fields, this review is indeed a passer-by's impression. On the other
hand, he happens to be a human (who occasionally is moving) and, as everybody
else, rates himself an expert in Applied Common Sense. Thus the author hopes
that this view from the {\em outside} will be of some interest not only for the
strangers like himself, but for those who are {\em inside} as well.
  Two flaws of the text that follows are inevitable. First, some essential
issues that are too familar to the specialists to discuss them may be missing.
Second, the author probably failed to provide the uniform ""level-of-detail"" for
this wide range of topics."
"Flow fields are often represented by a set of static arrows to illustrate
scientific vulgarization, documentary film, meteorology, etc. This simple
schematic representation lets an observer intuitively interpret the main
properties of a flow: its orientation and velocity magnitude. We propose to
generate dynamic versions of such representations for 2D unsteady flow fields.
Our algorithm smoothly animates arrows along the flow while controlling their
density in the domain over time. Several strategies have been combined to lower
the unavoidable popping artifacts arising when arrows appear and disappear and
to achieve visually pleasing animations. Disturbing arrow rotations in low
velocity regions are also handled by continuously morphing arrow glyphs to
semi-transparent discs. To substantiate our method, we provide results for
synthetic and real velocity field datasets."
"We propose a new gradient-domain technique for processing registered EM image
stacks to remove the inter-image discontinuities while preserving intra-image
detail. To this end, we process the image stack by first performing anisotropic
diffusion to smooth the data along the slice axis and then solving a
screened-Poisson equation within each slice to re-introduce the detail. The
final image stack is both continuous across the slice axis (facilitating the
tracking of information between slices) and maintains sharp details within each
slice (supporting automatic feature detection). To support this editing, we
describe the implementation of the first multigrid solver designed for
efficient gradient domain processing of large, out-of-core, voxel grids."
"This work presents an analysis of Higher Order Singular Value Decomposition
(HO-SVD) applied to lossy compression of 3D mesh animations. We describe
strategies for choosing a number of preserved spatial and temporal components
after tensor decomposition. Compression error is measured using three metrics
(MSE, Hausdorff, MSDM). Results are compared with a method based on Principal
Component Analysis (PCA) and presented on a set of animations with typical mesh
deformations."
"A fundamental tool in shape analysis is the virtual embedding of the
Riemannian manifold describing the geometry of a shape into Euclidean space.
Several methods have been proposed to embed isometric shapes in flat domains
while preserving distances measured on the manifold. Recently, attention has
been given to embedding shapes into the eigenspace of the Lapalce-Beltrami
operator. The Laplace-Beltrami eigenspace preserves the diffusion distance, and
is invariant under isometric transformations. However, Laplace-Beltrami
eigenfunctions computed independently for different shapes are often
incompatible with each other. Applications involving multiple shapes, such as
pointwise correspondence, would greatly benefit if their respective
eigenfunctions were somehow matched. Here, we introduce a statistical approach
for matching eigenfunctions. We consider the values of the eigenfunctions over
the manifold as sampling of random variables, and try to match their
multivariate distributions. Comparing distributions is done indirectly, using
high order statistics. We show that the permutation and sign ambiguities of low
order eigenfunctions, can be inferred by minimizing the difference of their
third order moments. The sign ambiguities of antisymmetric eigenfunctions can
be resolved by exploiting isometric invariant relations between the gradients
of the eigenfunctions and the surface normal. We present experiments
demonstrating the success of the proposed method applied to feature point
correspondence."
"The generation of fractals and study of the dynamics of polynomials is one of
the emerging and interesting field of research nowadays. We introduce in this
paper the dynamics of polynomials z^ n - z + c = 0 for n>=2 and applied Jungck
Ishikawa Iteration to generate new Relative Superior Mandelbrot sets and
Relative Superior Julia sets. In order to solve this function by Jungck type
iterative schemes, we write it in the form of Sz = Tz, where the function T, S
are defined as Tz = z^ n + c and Sz = z. Only mathematical explanations are
derived by applying Jungck Ishikawa Iteration for polynomials in the literature
but in this paper we have generated Relative Mandelbrot sets and Relative Julia
sets."
"This research paper addresses the problem of generating involuntary and
precise animation of quadrupeds with automatic rigging system of various
character types. The technique proposed through this research is based on a two
tier animation control curve with base simulation being driven through dynamic
mathematical model using procedural algorithm and the top layer with a custom
user controlled animation provided with intuitive Graphical User Interface
(GUI). The character rig is based on forward and inverse kinematics driven
through trigonometric based motion equations. The User is provided with various
manipulators and attributes to control and handle the locomotion gaits of the
characters and choose between various types of simulated motions from walking,
running, trotting, ambling and galloping with complete custom controls to
easily extend the base simulation as per requirements."
"A recent development, called isogeometric analysis, provides a unified
approach for design, analysis and optimization of functional products in
industry. Traditional volume rendering methods for inspecting the results from
the numerical simulations cannot be applied directly to isogeometric models. We
present a novel approach for interactive visualization of isogeometric analysis
results, ensuring correct, i.e., pixel-accurate geometry of the volume
including its bounding surfaces. The entire OpenGL pipeline is used in a
multi-stage algorithm leveraging techniques from surface rendering,
order-independent transparency, as well as theory and numerical methods for
ordinary differential equations. We showcase the efficiency of our approach on
different models relevant to industry, ranging from quality inspection of the
parametrization of the geometry, to stress analysis in linear elasticity, to
visualization of computational fluid dynamics results."
"We present Piko, a framework for designing, optimizing, and retargeting
implementations of graphics pipelines on multiple architectures. Piko
programmers express a graphics pipeline by organizing the computation within
each stage into spatial bins and specifying a scheduling preference for these
bins. Our compiler, Pikoc, compiles this input into an optimized implementation
targeted to a massively-parallel GPU or a multicore CPU.
  Piko manages work granularity in a programmable and flexible manner, allowing
programmers to build load-balanced parallel pipeline implementations, to
exploit spatial and producer-consumer locality in a pipeline implementation,
and to explore tradeoffs between these considerations. We demonstrate that Piko
can implement a wide range of pipelines, including rasterization, Reyes, ray
tracing, rasterization/ray tracing hybrid, and deferred rendering. Piko allows
us to implement efficient graphics pipelines with relative ease and to quickly
explore design alternatives by modifying the spatial binning configurations and
scheduling preferences for individual stages, all while delivering real-time
performance that is within a factor six of state-of-the-art rendering systems."
"Continuous collision detection (CCD) and response methods are widely adopted
in dynamics simulation of deformable models. They are history-based, as their
success is strictly based on an assumption of a collision-free state at the
start of each time interval. On the other hand, in many applications surfaces
have normals defined to designate their orientation (i.e. front- and
back-face), yet CCD methods are totally blind to such orientation
identification (thus are orientation-free). We notice that if such information
is utilized, many penetrations can be untangled. In this paper we present a
history-free method for separation of two penetrating meshes, where at least
one of them has clarified surface orientation. This method first computes all
edge-face (E-F) intersections with discrete collision detection (DCD), and then
builds a number of penetration stencils. On response, the stencil vertices are
relocated into a penetration-free state, via a global displacement minimizer.
Our method is very effective for handling penetration between two meshes, being
it an initial configuration or in the middle of physics simulation. The major
limitation is that it is not applicable to self-collision within one mesh at
the time being."
"The physical world consists of spatially varying media, such as the
atmosphere and the ocean, in which light and sound propagates along non-linear
trajectories. This presents a challenge to existing ray-tracing based methods,
which are widely adopted to simulate propagation due to their efficiency and
flexibility, but assume linear rays. We present a novel algorithm that traces
analytic ray curves computed from local media gradients, and utilizes the
closed-form solutions of both the intersections of the ray curves with planar
surfaces, and the travel distance. By constructing an adaptive unstructured
mesh, our algorithm is able to model general media profiles that vary in three
dimensions with complex boundaries consisting of terrains and other scene
objects such as buildings. We trace the analytic ray curves using the adaptive
unstructured mesh, which considerably improves the efficiency over prior
methods. We highlight the algorithm's application on simulation of sound and
visual propagation in outdoor scenes."
"This paper describes the implementation and evaluation of an open source
library for mathematical morphology based on packed binary and run-length
compressed images for document imaging applications. Abstractions and patterns
useful in the implementation of the interval operations are described. A number
of benchmarks and comparisons to bit-blit based implementations on standard
document images are provided."
"In this work, we present a new method for generating a threshold structure.
This kind of structure can be advantageously used in various halftoning
algorithms such as clustered-dot or dispersed-dot dithering, error diffusion
with threshold modulation, etc. The proposed method is based on rectifiable
polyominoes -- a non-periodic hierarchical structure, which tiles the Euclidean
plane with no gaps. Each polyomino contains a fixed number of discrete
threshold values. Thanks to its inherent non-periodic nature combined with
off-line optimization of threshold values, our polyomino-based threshold
structure shows blue-noise spectral properties. The halftone images produced
with this threshold structure have high visual quality. Although the proposed
method is general, and can be applied on any polyomino tiling, we consider one
particular case: tiling with G-hexominoes. We compare our polyomino-based
threshold structure with the best known state-of-the-art methods for generation
threshold matrices, and conclude considerable improvement achieved with our
method."
"This thesis presents a two-layer uniform facet elastic object for real-time
simulation based on physics modeling method. It describes the elastic object
procedural modeling algorithm with particle system from the simplest
one-dimensional object, to more complex two-dimensional and three-dimensional
objects.
  The double-layered elastic object consists of inner and outer elastic mass
spring surfaces and compressible internal pressure. The density of the inner
layer can be set different from the density of the outer layer; the motion of
the inner layer can be opposite to the motion of the outer layer. These special
features, which cannot be achieved by a single layered object, result in
improved imitation of a soft body, such as tissue's liquidity non-uniform
deformation. The construction of the double-layered elastic object is closer to
the real tissue's physical structure.
  The inertial behavior of the elastic object is well illustrated in
environments with gravity and collisions with walls, ceiling, and floor. The
collision detection is defined by elastic collision penalty method and the
motion of the object is guided by the Ordinary Differential Equation
computation.
  Users can interact with the modeled objects, deform them, and observe the
response to their action in real time."
"This game is meant to be extension of the overly-beaten pacman-style game
(code-named ""Yet Another Pacman 3D Adventures"", or YAP3DAD) from the proposed
ideas and other projects with advance visual and computer graphics features,
including a-game-in-a-game approach. The project is an open-source project
published on SourceForge.net for possible future development and extension."
"Removed by arXiv administration. This article was plagiarised from
http://www.dmi.unict.it/~battiato/download/NSIP_2003_VQ.pdf and other
locations."
"In this paper a new watermarking scheme is presented based on log-average
luminance. A colored-image is divided into blocks after converting the RGB
colored image to YCbCr color space. A monochrome image of 1024 bytes is used as
the watermark. To embed the watermark, 16 blocks of size 8X8 are selected and
used to embed the watermark image into the original image. The selected blocks
are chosen spirally (beginning form the center of the image) among the blocks
that have log-average luminance higher than or equal the log-average luminance
of the entire image. Each byte of the monochrome watermark is added by updating
a luminance value of a pixel of the image. If the byte of the watermark image
represented white color (255) a value <alpha> is added to the image pixel
luminance value, if it is black (0) the <alpha> is subtracted from the
luminance value. To extract the watermark, the selected blocks are chosen as
the above, if the difference between the luminance value of the watermarked
image pixel and the original image pixel is greater than 0, the watermark pixel
is supposed to be white, otherwise it supposed to be black. Experimental
results show that the proposed scheme is efficient against changing the
watermarked image to grayscale, image cropping, and JPEG compression."
"Separation of the text regions from background texture and graphics is an
important step of any optical character recognition sytem for the images
containg both texts and graphics. In this paper, we have presented a novel
text/graphics separation technique for business card images captured with a
cell-phone camera. At first, the background is eliminated at a coarse level
based on intensity variance. This makes the foreground components distinct from
each other. Then the non-text components are removed using various
characteristic features of text and graphics. Finally, the text regions are
skew corrected and binarized for further processing. Experimenting with
business card images of various resolutions, we have found an optimum
performance of 98.54% with 0.75 MP images, that takes 0.17 seconds processing
time and 1.1 MB peak memory on a moderately powerful computer (DualCore 1.73
GHz Processor, 1 GB RAM, 1 MB L2 Cache). The developed technique is
computationally efficient and consumes low memory so as to be applicable on
mobile devices."
"We present a novel approach to finding critical points in cell-wise
barycentrically or bilinearly interpolated vector fields on surfaces. The
Poincar\e index of the critical points is determined by investigating the
qualitative behavior of 0-level sets of the interpolants of the vector field
components in parameter space using precomputed combinatorial results, thus
avoiding the computation of the Jacobian of the vector field at the critical
points in order to determine its index. The locations of the critical points
within a cell are determined analytically to achieve accurate results. This
approach leads to a correct treatment of cases with two first-order critical
points or one second-order critical point of bilinearly interpolated vector
fields within one cell, which would be missed by examining the linearized field
only. We show that for the considered interpolation schemes determining the
index of a critical point can be seen as a coloring problem of cell edges. A
complete classification of all possible colorings in terms of the types and
number of critical points yielded by each coloring is given using computational
group theory. We present an efficient algorithm that makes use of these
precomputed classifications in order to find and classify critical points in a
cell-by-cell fashion. Issues of numerical stability, construction of the
topological skeleton, topological simplification, and the statistics of the
different types of critical points are also discussed."
"We present an algorithm for generating Poisson-disc patterns taking O(N) time
to generate $N$ points. The method is based on a grid of regions which can
contain no more than one point in the final pattern, and uses an explicit model
of point arrival times under a uniform Poisson process."
"Compared with full volume rendering, isosurface rendering has several well
recognized advantages in efficiency and accuracy. However, standard isosurface
rendering has some limitations in effectiveness. First, it uses a monotone
colored approach and can only visualize the geometry features of an isosurface.
The lack of the capability to illustrate the material property and the internal
structures behind an isosurface has been a big limitation of this method in
applications. Another limitation of isosurface rendering is the difficulty to
reveal physically meaningful structures, which are hidden in one or multiple
isosurfaces. As such, the application requirements of extract and recombine
structures of interest can not be implemented effectively with isosurface
rendering. In this work, we develop an enhanced isosurface rendering technique
to improve the effectiveness while maintaining the performance efficiency of
the standard isosurface rendering. First, an isosurface color enhancement
method is proposed to illustrate the neighborhood density and to reveal some of
the internal structures. Second, we extend the structure extraction capability
of isosurface rendering by enabling explicit scene exploration within a
3D-view, using surface peeling, voxel-selecting, isosurface segmentation, and
multi-surface-structure visualization. Our experiments show that the color
enhancement not only improves the visual fidelity of the rendering, but also
reveals the internal structures without significant increase of the
computational cost. Explicit scene exploration is also demonstrated as a
powerful tool in some application scenarios, such as displaying multiple
abdominal organs."
"It is a common sense to apply the VFC (view frustum culling) of spatial
object to bounding cube of the object in 3D graphics. The accuracy of VFC can
not be guaranteed even in cube rotated three-dimensionally. In this paper is
proposed a method which is able to carry out more precise and fast VFC of any
spatial object in the image domain of cube by an analytic mapping, and is
demonstrated the effect of the method for terrain block on global surface."
"When representing a solid object there are alternatives to the use of
traditional explicit (surface meshes) or implicit (zero crossing of implicit
functions) methods. Skeletal representations encode shape information in a
mixed fashion: they are composed of a set of explicit primitives, yet they are
able to efficiently encode the shape's volume as well as its topology. I will
discuss, in two dimensions, how symmetry can be used to reduce the
dimensionality of the data (from a 2D solid to a 1D curve), and how this
relates to the classical definition of skeletons by Medial Axis Transform.
While the medial axis of a 2D shape is composed of a set of curves, in 3D it
results in a set of sheets connected in a complex fashion. Because of this
complexity, medial skeletons are difficult to use in practical applications.
Curve skeletons address this problem by strictly requiring their geometry to be
one dimensional, resulting in an intuitive yet powerful shape representation.
In this report I will define both medial and curve skeletons and discuss their
mutual relationship. I will also present several algorithms for their
computation and a variety of scenarios where skeletons are employed, with a
special focus on geometry processing and shape analysis."
"We formalize the notion of sampling a function using k-d darts. A k-d dart is
a set of independent, mutually orthogonal, k-dimensional subspaces called k-d
flats. Each dart has d choose k flats, aligned with the coordinate axes for
efficiency. We show that k-d darts are useful for exploring a function's
properties, such as estimating its integral, or finding an exemplar above a
threshold. We describe a recipe for converting an algorithm from point sampling
to k-d dart sampling, assuming the function can be evaluated along a k-d flat.
  We demonstrate that k-d darts are more efficient than point-wise samples in
high dimensions, depending on the characteristics of the sampling domain: e.g.
the subregion of interest has small volume and evaluating the function along a
flat is not too expensive. We present three concrete applications using line
darts (1-d darts): relaxed maximal Poisson-disk sampling, high-quality
rasterization of depth-of-field blur, and estimation of the probability of
failure from a response surface for uncertainty quantification. In these
applications, line darts achieve the same fidelity output as point darts in
less time. We also demonstrate the accuracy of higher dimensional darts for a
volume estimation problem. For Poisson-disk sampling, we use significantly less
memory, enabling the generation of larger point clouds in higher dimensions."
"Polyhedral meshes (PM) - meshes having planar faces - have enjoyed a rise in
popularity in recent years due to their importance in architectural and
industrial design. However, they are also notoriously difficult to generate and
manipulate. Previous methods start with a smooth surface and then apply
elaborate meshing schemes to create polyhedral meshes approximating the
surface. In this paper, we describe a reverse approach: given the topology of a
mesh, we explore the space of possible planar meshes with that topology.
  Our approach is based on a complete characterization of the maximal linear
spaces of polyhedral meshes contained in the curved manifold of polyhedral
meshes with a given topology. We show that these linear spaces can be described
as nullspaces of differential operators, much like harmonic functions are
nullspaces of the Laplacian operator. An analysis of this operator provides
tools for global and local design of a polyhedral mesh, which fully expose the
geometric possibilities and limitations of the given topology."
"To create a text with graphic instructions for output pictures into LATEX
document, we offer software that allows us to build a picture in WIZIWIG mode
and for setting the text with these graphical instructions."
"Glyph-based visualization is an effective tool for depicting multivariate
information. Since sorting is one of the most common analytical tasks performed
on individual attributes of a multi-dimensional data set, this motivates the
hypothesis that introducing glyph sorting would significantly enhance the
usability of glyph-based visualization. In this paper, we present a glyph-based
conceptual framework as part of a visualization process for interactive sorting
of multivariate data. We examine several technical aspects of glyph sorting and
provide design principles for developing effective, visually sortable glyphs.
Glyphs that are visually sortable provide two key benefits: 1) performing
comparative analysis of multiple attributes between glyphs and 2) to support
multi-dimensional visual search. We describe a system that incorporates focus
and context glyphs to control sorting in a visually intuitive manner and for
viewing sorted results in an Interactive, Multi-dimensional Glyph (IMG) plot
that enables users to perform high-dimensional sorting, analyse and examine
data trends in detail. To demonstrate the usability of glyph sorting, we
present a case study in rugby event analysis for comparing and analysing trends
within matches. This work is undertaken in conjunction with a national rugby
team. From using glyph sorting, analysts have reported the discovery of new
insight beyond traditional match analysis."
"The Generalized Cornu Spiral (GCS) was first proposed by Ali et al. in 1995
[9]. Due to the monotonocity of its curvature function, the surface generated
with GCS segments has been considered as a high quality surface and it has
potential applications in surface design [2]. In this paper, the analysis of
GCS segment is carried out by determining its aesthetic value using the log
curvature Graph (LCG) as proposed by Kanaya et al.[10]. The analysis of LCG
supports the claim that GCS is indeed a generalized aesthetic curve."
"A method to construct transition curves using a family of the quartic Bezier
spiral is described. The transition curves discussed are S-shape and C-shape of
contact, between two separated circles. A spiral is a curve of monotone
increasing or monotone decreasing curvature of one sign. Thus, a spiral cannot
have an inflection point or curvature extreme. The family of quartic Bezier
spiral form which is introduced has more degrees of freedom and will give a
better approximation. It is proved that the methods of constructing transition
curves can be simplified by the transformation process and the ratio of two
radii has no restriction, which extends the application area, and it gives a
family of transition curves that allow more flexible curve designs."
"In this paper, we analyze the planar cubic Alternative curve to determine the
conditions for convex, loops, cusps and inflection points. Thus cubic curve is
represented by linear combination of three control points and basis function
that consist of two shape parameters. By using algebraic manipulation, we can
determine the constraint of shape parameters and sufficient conditions are
derived which ensure that the curve is a strictly convex, loops, cusps and
inflection point. We conclude the result in a shape diagram of parameters. The
simplicity of this form makes characterization more intuitive and efficient to
compute."
"The log-aesthetic curves include the logarithmic (equiangular) spiral,
clothoid, and involute curves. Although most of them are expressed only by an
integral form of the tangent vector, it is possible to interactively generate
and deform them and they are expected to be utilized for practical use of
industrial and graphical design. The discrete log-aesthetic filter based on the
formulation of the log-aesthetic curve has successfully been introduced not to
impose strong constraints on the designer's activity, to let him/her design
freely and to embed the properties of the log-aesthetic curves for complicated
ones with both increasing and decreasing curvature. In this paper, in order to
define the log-aesthetic surface and develop surface filters based on its
formulation, at first we reformulate the log-aesthetic curve with variational
principle. Then we propose several new functionals to be minimized for
free-form surfaces and define the log-aesthetic surface. Furthermore we propose
new discrete surface filters based on the log-aesthetic surface formulation"
"In this paper, we proposed a new form of type-2 fuzzy data points(T2FDPs)
that is normal type-2 data points(NT2FDPs). These brand-new forms of data were
defined by using the definition of normal type-2 triangular fuzzy
number(NT2TFN). Then, we applied fuzzification(alpha-cut) and type-reduction
processes towards NT2FDPs after they had been redefined based on the situation
of NT2FDPs. Furthermore, we redefine the defuzzification definition along with
the new definitions of fuzzification process and type-reduction method to
obtain crisp type-2 fuzzy solution data points. For all these processes from
the defining the NT2FDPs to defuzzification of NT2FDPs, we demonstrate through
curve representation by using the rational B-spline curve function as the
example form modeling these NT2FDPs."
"The research on developing planar curves to produce visually pleasing
products (ranges from electric appliances to car body design) and
indentifying/modifying planar curves for special purposes namely for railway
design, highway design and robot trajectories have been progressing since
1970s. The pattern of research in this field of study has branched to five
major groups namely curve synthesis, fairing process, improvement in control of
natural spiral, construction of new type of planar curves and, natural spiral
fitting & approximation techniques. The purpose of is this paper is to briefly
review recent progresses in Computer Aided Geometric Design (CAGD) focusing on
the topics states above."
"An improvised algorithm is proposed based on the work of Yoshimoto and
Harada. The improvised algorithm results a graph which is called LDGC or
Logarithmic Distribution Graph of Curvature. This graph has the capability to
identify the beauty of monotonic planar curves with less effort as compared to
LDDC by Yoshimoto and Harada."
"In this paper, we proposed another new form of type-2 fuzzy data
points(T2FDPs) that is perfectly normal type-2 data points(PNT2FDPs). These
kinds of brand-new data were defined by using the existing type-2 fuzzy set
theory(T2FST) and type-2 fuzzy number(T2FN) concept since we dealt with the
problem of defining complex uncertainty data. Along with this restructuring, we
included the fuzzification(alpha-cut operation), type-reduction and
defuzzification processes against PNT2FDPs. In addition, we used interpolation
B-soline curve function to demonstrate the PNT2FDPs."
"In many graphics applications, the computation of exact geodesic distance is
very important. However, the high computational cost of the existing geodesic
algorithms means that they are not practical for large-scale models or
time-critical applications. To tackle this challenge, we propose the parallel
Chen-Han (or PCH) algorithm, which extends the classic Chen-Han (CH) discrete
geodesic algorithm to the parallel setting. The original CH algorithm and its
variant both lack a parallel solution because the windows (a key data structure
that carries the shortest distance in the wavefront propagation) are maintained
in a strict order or a tightly coupled manner, which means that only one window
is processed at a time. We propose dividing the CH's sequential algorithm into
four phases, window selection, window propagation, data organization, and
events processing so that there is no data dependence or conflicts in each
phase and the operations within each phase can be carried out in parallel. The
proposed PCH algorithm is able to propagate a large number of windows
simultaneously and independently. We also adopt a simple yet effective strategy
to control the total number of windows. We implement the PCH algorithm on
modern GPUs (such as Nvidia GTX 580) and analyze the performance in detail. The
performance improvement (compared to the sequential algorithms) is highly
consistent with GPU double-precision performance (GFLOPS). Extensive
experiments on real-world models demonstrate an order of magnitude improvement
in execution time compared to the state-of-the-art."
"In this paper we discuss the variety of planar spiral segments and their
applications in objects in both the real and artificial world. The discussed
curves with monotonic curvature function are well-known in geometric modelling
and computer aided geometric design as fair curves, and they are very
significant in aesthetic shape modelling. Fair curve segments are used for
two-point G1 and G2 Hermite interpolation, as well as for generating aesthetic
splines."
"This article studies families of curves with monotonic curvature function
(MC-curves) and their applications in geometric modelling and aesthetic design.
Aesthetic analysis and assessment of the structure and plastic qualities of
pseudospirals, which are curves with monotonic curvature function, are
conducted for the first time in the field of geometric modelling from the
position of technical aesthetics laws. The example of car body surface
modelling with the use of aesthetics splines is given."
"High-dimensional transfer function design is widely used to provide
appropriate data classification for direct volume rendering of various
datasets. However, its design is a complicated task. Parallel coordinate plot
(PCP), as a powerful visualization tool, can efficiently display
high-dimensional geometry and accurately analyze multivariate data. In this
paper, we propose to combine parallel coordinates with dimensional reduction
methods to guide high-dimensional transfer function design. Our pipeline has
two major advantages: (1) combine and display extracted high-dimensional
features in parameter space; and (2) select appropriate high-dimensional
parameters, with the help of dimensional reduction methods, to obtain
sophisticated data classification as transfer function for volume rendering. In
order to efficiently design high-dimensional transfer functions, the
combination of both parallel coordinate components and dimension reduction
results is necessary to generate final visualization results. We demonstrate
the capability of our method for direct volume rendering using various CT and
MRI datasets."
"Parallel coordinates plot (PCP) is an excellent tool for multivariate
visualization and analysis, but it may fail to reveal inherent structures for
datasets with a large number of items. In this paper, we propose a suite of
novel clustering, dimension ordering and visualization techniques based on PCP,
to reveal and highlight hidden structures. First, we propose a continuous
spline based polycurves design to extract and classify different cluster
aspects of the data. Then, we provide an efficient and optimal correlation
based sorting technique to reorder coordinates, as a helpful visualization tool
for data analysis. Various results generated by our framework visually
represent much structure, trend and correlation information to guide the user,
and improve the efficacy of analysis, especially for complex and noisy
datasets."
"We present the first triangle mesh-based technique for tracking the evolution
of general three-dimensional multimaterial interfaces undergoing complex
topology changes induced by deformations and collisions. Our core
representation is a non-manifold triangle surface mesh with material labels
assigned to each half-face to distinguish volumetric regions. We advect the
vertices of the mesh in a Lagrangian manner, and employ a complete set of
collision-safe mesh improvement and topological operations that track and
update material labels. In particular, we develop a unified, collision-safe
strategy for handling complex topological operations acting on evolving triple-
and higher-valence junctions, and a flexible method to merge colliding
multimaterial meshes. We demonstrate our approach with a number of challenging
geometric flows, including passive advection, normal flow, and mean curvature
flow."
"We present a novel methodology based on geometric approach to simulate
magnification lens effects. Our aim is to promote new applications of powerful
geometric modeling techniques in visual computing. Conventional image
processing/visualization methods are computed in two dimensional space (2D). We
examine this conventional 2D manipulation from a completely innovative
perspective of 3D geometric processing. Compared with conventional optical lens
design, 3D geometric method are much more capable of preserving shape features
and minimizing distortion. We magnify an area of interest to better visualize
the interior details, while keeping the rest of area without perceivable
distortion. We flatten the mesh back into 2D space for viewing, and further
applications in the screen space. In both steps, we devise an iterative
deformation scheme to minimize distortion around both focus and context region,
while avoiding the noncontinuous transition region between the focus and
context areas. Particularly, our method allows the user to flexibly modify the
ROI shapes to accommodate complex feature. The user can also easily specify a
spectrum of metrics for different visual effects. Various experimental results
demonstrate the effectiveness, robustness, and efficiency of our framework."
"In this paper, we address the following research problem: How can we generate
a meaningful split grammar that explains a given facade layout? To evaluate if
a grammar is meaningful, we propose a cost function based on the description
length and minimize this cost using an approximate dynamic programming
framework. Our evaluation indicates that our framework extracts meaningful
split grammars that are competitive with those of expert users, while some
users and all competing automatic solutions are less successful."
"The rapid advances in 3D scanning and acquisition techniques have given rise
to the explosive increase of volumetric digital models in recent years. This
dissertation systematically trailblazes a novel volumetric modeling framework
to represent 3D solids. The need to explore more efficient and robust 3D
modeling framework has gained the prominence. Although the traditional surface
representation (e.g., triangle mesh) has many attractive properties, it is
incapable of expressing the interior space and materials. Such a serious
drawback overshadows many potential modeling and analysis applications.
Consequently volumetric modeling techniques become the well-known solution to
this problem. Nevertheless, many unsolved research issues remain when
developing an efficient modeling paradigm for existing 3D models: complex
geometry (fine details and extreme concaveness), arbitrary topology,
heterogenous materials, large-scale data storage and processing, etc."
"In this dissertation, we concentrate on the challenging research issue of
developing a spline-based modeling framework, which converts the conventional
data (e.g., surface meshes) to tensor-product trivariate splines. This
methodology can represent both boundary/volumetric geometry and real volumetric
physical attributes in a compact and continuous fashion. The regular
tensor-product structure enables our new developed methods to be embedded into
the industry standard seamlessly. These properties make our techniques highly
preferable in many physically-based applications including mechanical analysis,
shape deformation and editing, virtual surgery training, etc."
"Barycentric coordinates are frequently used as interpolants to shade computer
graphics images. A simple equation transforms barycentric coordinates from
screen space into eye space in order to undo the perspective transformation and
permit accurate interpolative shading of texture maps. This technique is
amenable to computation using a block-normalized integer representation."
"Volume approximation is an important problem found in many applications of
computer graphics, vision, and image processing. The problem is about computing
an accurate and compact approximate representation of 3D volumes using some
simple primitives. In this study, we propose a new volume representation,
called medial meshes, and present an efficient method for its computation.
Specifically, we use the union of a novel type of simple volume primitives,
which are spheres and the convex hulls of two or three spheres, to approximate
a given 3D shape. We compute such a volume approximation based on a new method
for medial axis simplification guided by Hausdorff errors. We further
demonstrate the superior efficiency and accuracy of our method over existing
methods for medial axis simplification."
"A flexible, scalable and affordable virtual reality software system
architecture is proposed. This solution can be easily implemented on different
hardware configurations: on a single computer or on a computer cluster. The
architecture is aimed to be integrated in the workflow for solving engineering
tasks and oriented towards presenting implicit object properties through
multiple sensorial channels (visual, audio and haptic). Implicit properties
represent hidden object features (i.e. magnetization, radiation, humidity,
toxicity, etc.) which cannot be perceived by the observer through his or her
senses but require specialized equipment in order to expand the sensory ability
of the observer. Our approach extends the underlying general scene graph
structure incorporating additional effects nodes for implicit properties
representation."
"The applicability of Virtual Reality for evaluating engineering analysis
results is beginning to receive increased appreciation in the last years. The
problem many engineers are still facing is how to import their model together
with the analysis results in a virtual reality environment for exploration and
results validation. In this paper we propose an algorithm for transforming
model data and results from finite element analysis (FEA) solving application
to a format easily interpretable by a virtual reality application. The
algorithm includes also steps for reducing the face-count of the resulting mesh
by eliminating faces from the inner part of the model in the cases when only
the surfaces of the model is analyzed. We also describe a possibility for
simultaneously assessing multiple analysis results relying on multimodal
results presentation by stimulating different senses of the operator."
"We present a novel BSSRDF for rendering translucent materials. Angular
effects lacking in previous BSSRDF models are incorporated by using a dual-beam
formulation. We employ a Placzek's Lemma interpretation of the method of images
and discard diffusion theory. Instead, we derive a plane-parallel
transformation of the BSSRDF to form the associated BRDF and optimize the image
confiurations such that the BRDF is close to the known analytic solutions for
the associated albedo problem. This ensures reciprocity, accurate colors, and
provides an automatic level-of-detail transition for translucent objects that
appear at various distances in an image. Despite optimizing the subsurface
fluence in a plane-parallel setting, we find that this also leads to fairly
accurate fluence distributions throughout the volume in the original 3D
searchlight problem. Our method-of-images modifications can also improve the
accuracy of previous BSSRDFs."
"Physics-based animation of soft or rigid bodies for real-time applications
often suffers from numerical instabilities. We analyse one of the most common
sources of unwanted behaviour: the numerical integration strategy. To assess
the impact of popular integration methods, we consider a scenario where soft
and hard constraints are added to a custom designed deformable linear object.
Since the goal for this class of simulation methods is to attain interactive
frame-rates, we present the drawbacks of using explicit integration methods
over inherently stable, implicit integrators. To help numerical solver
designers better understand the impact of an integrator on a certain simulated
world, we have conceived a method of benchmarking the efficiency of an
integrator with respect to its speed, stability and symplecticity."
"In this paper, we present an approach to reconstruct 3-D human motion from
multi-cameras and track human skeleton using the reconstructed human 3-D point
(voxel) cloud. We use an improved and more robust algorithm, probabilistic
shape from silhouette to reconstruct human voxel. In addition, the annealed
particle filter is applied for tracking, where the measurement is computed
using the reprojection of reconstructed voxel. We use two different ways to
accelerate the approach. For the CPU only acceleration, we leverage Intel TBB
to speed up the hot spot of the computational overhead and reached an
accelerating ratio of 3.5 on a 4-core CPU. Moreover, we implement an
intensively paralleled version via GPU acceleration without TBB. Taking account
all data transfer and computing time, the GPU version is about 400 times faster
than the original CPU implementation, leading the approach to run at a
real-time speed."
"This paper presents a method for extraction and analysis of curve--type
structures which consist of disconnected components. Such structures are found
in electron--microscopy (EM) images of metal nanograins, which are widely used
in the field of nanosensor technology.
  The topography of metal nanograins in compound nanomaterials is crucial to
nanosensor characteristics. The method of completing such templates consists of
three steps. In the first step, a local Gaussian filter is used with different
weights for each neighborhood. In the second step, an adaptive morphology
operation is applied to detect the endpoints of curve segments and connect
them. In the last step, pruning is employed to extract a curve which optimally
fits the template."
"A composite quadric model (CQM) is an object modeled by piecewise linear or
quadric patches. We study the continuous detection problem of a special type of
CQM objects which are commonly used in CAD/CAM, that is, the boundary surfaces
of such a CQM intersect only in straight line segments or conic curve segments.
We present a framework for continuous collision detection (CCD) of this special
type of CQM (which we also call CQM for brevity) in motion. We derive algebraic
formulations and compute numerically the first contact time instants and the
contact points of two moving CQMs in $\mathbb R^3$. Since it is difficult to
process CCD of two CQMs in a direct manner because they are composed of
semi-algebraic varieties, we break down the problem into subproblems of solving
CCD of pairs of boundary elements of the CQMs. We present procedures to solve
CCD of different types of boundary element pairs in different dimensions. Some
CCD problems are reduced to their equivalents in a lower dimensional setting,
where they can be solved more efficiently."
"We consider the problem of establishing dense correspondences within a set of
related shapes of strongly varying geometry. For such input, traditional shape
matching approaches often produce unsatisfactory results. We propose an
ensemble optimization method that improves given coarse correspondences to
obtain dense correspondences. Following ideas from minimum description length
approaches, it maximizes the compactness of the induced shape space to obtain
high-quality correspondences. We make a number of improvements that are
important for computer graphics applications: Our approach handles meshes of
general topology and handles partial matching between input of varying
topology. To this end we introduce a novel part-based generative statistical
shape model. We develop a novel analysis algorithm that learns such models from
training shapes of varying topology. We also provide a novel synthesis method
that can generate new instances with varying part layouts and subject to
generic variational constraints. In practical experiments, we obtain a
substantial improvement in correspondence quality over state-of-the-art
methods. As example application, we demonstrate a system that learns shape
families as assemblies of deformable parts and permits real-time editing with
continuous and discrete variability."
"The paper addresses the following problem: given a set of man-made shapes,
e.g., chairs, can we quickly rank and explore the set of shapes with respect to
a given avatar pose? Answering this question requires identifying which shapes
are more suitable for the defined avatar and pose; and moreover, to provide
fast preview of how to alter the input geometry to better fit the deformed
shapes to the given avatar pose? The problem naturally links physical
proportions of human body and its interaction with object shapes in an attempt
to connect ergonomics with shape geometry. We designed an interaction system
that allows users to explore shape collections using the deformation of human
characters while at the same time providing interactive previews of how to
alter the shapes to better fit the user-specified character. We achieve this by
first mapping ergonomics guidelines into a set of simultaneous multi-part
constraints based on target contacts; and then, proposing a novel contact-based
deformation model to realize multi-contact constraints. We evaluate our
framework on various chair models and validate the results via a small user
study."
"For the rendering of multiple scattering effects in participating media,
methods based on the diffusion approximation are an extremely efficient
alternative to Monte Carlo path tracing. However, in sufficiently transparent
regions, classical diffusion approximation suffers from non-physical radiative
fluxes which leads to a poor match to correct light transport. In particular,
this prevents the application of classical diffusion approximation to
heterogeneous media, where opaque material is embedded within transparent
regions. To address this limitation, we introduce flux-limited diffusion, a
technique from the astrophysics domain. This method provides a better
approximation to light transport than classical diffusion approximation,
particularly when applied to heterogeneous media, and hence broadens the
applicability of diffusion-based techniques. We provide an algorithm for
flux-limited diffusion, which is validated using the transport theory for a
point light source in an infinite homogeneous medium. We further demonstrate
that our implementation of flux-limited diffusion produces more accurate
renderings of multiple scattering in various heterogeneous datasets than
classical diffusion approximation, by comparing both methods to ground truth
renderings obtained via volumetric path tracing."
"We present a generalization of the bilateral filter that can be applied to
feature-preserving smoothing of signals on images, meshes, and other domains
within a single unified framework. Our discretization is competitive with
state-of-the-art smoothing techniques in terms of both accuracy and speed, is
easy to implement, and has parameters that are straightforward to understand.
Unlike previous bilateral filters developed for meshes and other irregular
domains, our construction reduces exactly to the image bilateral on rectangular
domains and comes with a rigorous foundation in both the smooth and discrete
settings. These guarantees allow us to construct unconditionally convergent
mean-shift schemes that handle a variety of extremely noisy signals. We also
apply our framework to geometric edge-preserving effects like feature
enhancement and show how it is related to local histogram techniques."
"This paper extends a recently proposed robust computational framework for
constructing the boundary representation (brep) of the volume swept by a given
smooth solid moving along a one parameter family $h$ of rigid motions. Our
extension allows the input solid to have sharp features, i.e., to be of class
G0 wherein, the unit outward normal to the solid may be discontinuous. In the
earlier framework, the solid to be swept was restricted to be G1, and thus this
is a significant and useful extension of that work. This naturally requires a
precise description of the geometry of the surface generated by the sweep of a
sharp edge supported by two intersecting smooth faces. We uncover the geometry
along with the related issues like parametrization, self-intersection and
singularities via a novel mathematical analysis. Correct trimming of such a
surface is achieved by a delicate analysis of the interplay between the cone of
normals at a sharp point and its trajectory under $h$. The overall topology is
explicated by a key lifting theorem which allows us to compute the adjacency
relations amongst entities in the swept volume by relating them to
corresponding adjacencies in the input solid. Moreover, global issues related
to body-check such as orientation are efficiently resolved. Many examples from
a pilot implementation illustrate the efficiency and effectiveness of our
framework."
"Jacobson et al. [JKSH13] hypothesized that the local coherency of the
generalized winding number function could be used to correctly determine
consistent facet orientations in polygon meshes. We report on an approach to
consistently orienting facets in polygon meshes by minimizing the Dirichlet
energy of generalized winding numbers. While the energy can be concisely
formulated and efficiently computed, we found that this approach is
fundamentally flawed and is unfortunately not applicable for most handmade
meshes shared on popular mesh repositories such as Google 3D Warehouse."
"A method for creating 3D texture coordinates for a sequence of polygon meshes
with changing topology and vertex motion vectors."
"We present a sketch-based modeling system suitable for detail editing, based
on a multilevel representation for surfaces. The main advantage of this
representation allowing for the control of local (details) and global changes
of the model. We used an adaptive mesh (4-8 mesh) and developed a label theory
to construct a manifold structure, which is responsible for controlling local
editing of the model. The overall shape and global modifications are defined by
a variational implicit surface (Hermite RBF). Our system assembles the manifold
structures to allow the user to add details without changing the overall shape,
as well as edit the overall shape while repositioning details coherently."
"Search-based texture synthesis algorithms are sensitive to the order in which
texture samples are generated; different synthesis orders yield different
textures. Unfortunately, most polygon rasterizers and ray tracers do not
guarantee the order with which surfaces are sampled. To circumvent this
problem, textures are synthesized beforehand at some maximum resolution and
rendered using texture mapping.
  We describe a search-based texture synthesis algorithm in which samples can
be generated in arbitrary order, yet the resulting texture remains identical.
The key to our algorithm is a pyramidal representation in which each texture
sample depends only on a fixed number of neighboring samples at each level of
the pyramid. The bottom (coarsest) level of the pyramid consists of a noise
image, which is small and predetermined. When a sample is requested by the
renderer, all samples on which it depends are generated at once. Using this
approach, samples can be generated in any order. To make the algorithm
efficient, we propose storing texture samples and their dependents in a
pyramidal cache. Although the first few samples are expensive to generate,
there is substantial reuse, so subsequent samples cost less. Fortunately, most
rendering algorithms exhibit good coherence, so cache reuse is high."
"Background: Visualization of multi-channel microscopy data plays a vital role
in biological research. With the ever-increasing resolution of modern
microscopes the data set size of the scanned specimen grows steadily. On
commodity hardware this size easily exceeds the available main memory and the
even more limited GPU memory. Common volume rendering techniques require the
entire data set to be present in the GPU memory. Existing out-of-core rendering
approaches for large volume data sets either are limited to single-channel
volumes, or require a computer cluster, or have long preprocessing times.
Results: We introduce a ray-casting technique for rendering large volumetric
multi-channel microscopy data streams on commodity hardware. The volumetric
data is managed at different levels of detail by an octree structure. In
contrast to previous octree-based techniques, the octree is built incrementally
and therefore supports streamed microscopy data as well as data set sizes
exceeding the available main memory. Furthermore, our approach allows the user
to interact with the partially rendered data set at all stages of the octree
construction. After a detailed description of our method, we present
performance results for different multi-channel data sets with a size of up to
24 GB on a standard desktop PC. Conclusions: Our rendering technique allows
biologists to visualize their scanned specimen on their standard desktop
computers without high-end hardware requirements. Furthermore, the user can
interact with the data set during the initial loading to explore the already
loaded parts, change rendering parameters like color maps or adjust clipping
planes. Thus, the time of biologists being idle is reduced. Also, streamed data
can be visualized to detect and stop flawed scans early during the scan
process."
"Results: We present an application that enables the quantitative analysis of
multichannel 5-D (x, y, z, t, channel) and large montage confocal fluorescence
microscopy images. The image sequences show stem cells together with blood
vessels, enabling quantification of the dynamic behaviors of stem cells in
relation to their vascular niche, with applications in developmental and cancer
biology. Our application automatically segments, tracks, and lineages the image
sequence data and then allows the user to view and edit the results of
automated algorithms in a stereoscopic 3-D window while simultaneously viewing
the stem cell lineage tree in a 2-D window. Using the GPU to store and render
the image sequence data enables a hybrid computational approach. An
inference-based approach utilizing user-provided edits to automatically correct
related mistakes executes interactively on the system CPU while the GPU handles
3-D visualization tasks. Conclusions: By exploiting commodity computer gaming
hardware, we have developed an application that can be run in the laboratory to
facilitate rapid iteration through biological experiments. There is a pressing
need for visualization and analysis tools for 5-D live cell image data. We
combine accurate unsupervised processes with an intuitive visualization of the
results. Our validation interface allows for each data set to be corrected to
100% accuracy, ensuring that downstream data analysis is accurate and
verifiable. Our tool is the first to combine all of these aspects, leveraging
the synergies obtained by utilizing validation information from stereo
visualization to improve the low level image processing tasks."
"Background: Because of the difficulties involved in learning and using 3D
modeling and rendering software, many scientists hire programmers or animators
to create models and animations. This both slows the discovery process and
provides opportunities for miscommunication. Working with multiple
collaborators, we developed a set of design goals for a tool that would enable
them to directly construct models and animations. Results: We present
SketchBio, a tool that incorporates state-of-the-art bimanual interaction and
drop shadows to enable rapid construction of molecular structures and
animations. It includes three novel features: crystal by example, pose-mode
physics, and spring-based layout that accelerate operations common in the
formation of molecular models. We present design decisions and their
consequences, including cases where iterative design was required to produce
effective approaches. Conclusions: The design decisions, novel features, and
inclusion of state-of-the-art techniques enabled SketchBio to meet all of its
design goals. These features and decisions can be incorporated into existing
and new tools to improve their effectiveness"
"Analysis of high dimensional data is a common task. Often, small multiples
are used to visualize 1 or 2 dimensions at a time, such as in a scatterplot
matrix. Associating data points between different views can be difficult
though, as the points are not fixed. Other times, dimensional reduction
techniques are employed to summarize the whole dataset in one image, but
individual dimensions are lost in this view. In this paper, we present a means
of augmenting a dimensional reduction plot with isocontours to reintroduce the
original dimensions. By applying this to each dimension in the original data,
we create multiple views where the points are consistent, which facilitates
their comparison. Our approach employs a combination of a novel, graph-based
projection technique with a GPU accelerated implementation of moving least
squares to interpolate space between the points. We also present evaluations of
this approach both with a case study and with a user study."
"Blue noise refers to sample distributions that are random and well-spaced,
with a variety of applications in graphics, geometry, and optimization.
However, prior blue noise sampling algorithms typically suffer from the
curse-of-dimensionality, especially when striving to cover a domain maximally.
This hampers their applicability for high dimensional domains.
  We present a blue noise sampling method that can achieve high quality and
performance across different dimensions. Our key idea is spoke-dart sampling,
sampling locally from hyper-annuli centered at prior point samples, using
lines, planes, or, more generally, hyperplanes. Spoke-dart sampling is more
efficient at high dimensions than the state-of-the-art alternatives: global
sampling and advancing front point sampling. Spoke-dart sampling achieves good
quality as measured by differential domain spectrum and spatial coverage. In
particular, it probabilistically guarantees that each coverage gap is small,
whereas global sampling can only guarantee that the sum of gaps is not large.
We demonstrate advantages of our method through empirical analysis and
applications across dimensions 8 to 23 in Delaunay graphs, global optimization,
and motion planning."
"Harmonic surface deformation is a well-known geometric modeling method that
creates plausible deformations in an interactive manner. However, this method
is susceptible to artifacts, in particular close to the deformation handles.
These artifacts often correlate with strong gradients of the deformation
energy.In this work, we propose a novel formulation of harmonic surface
deformation, which incorporates a regularization of the deformation energy. To
do so, we build on and extend a recently introduced generic linear
regularization approach. It can be expressed as a change of norm for the linear
optimization problem, i.e., the regularization is baked into the optimization.
This minimizes the implementation complexity and has only a small impact on
runtime. Our results show that a moderate use of regularization suppresses many
deformation artifacts common to the well-known harmonic surface deformation
method, without introducing new artifacts."
"The production of animation is a resource intensive process in game
companies. Therefore, techniques to synthesize animations have been developed.
However, these procedural techniques offer limited adaptability by animation
artists. In order to solve this, a fuzzy neural network model of the animation
is proposed, where the parameters can be tuned either by machine learning
techniques that use motion capture data as training data or by the animation
artist himself. This paper illustrates how this real time procedural animation
system can be developed, taking the human gait on flat terrain and inclined
surfaces as example. Currently, the parametric model is capable of synthesizing
animations for various limb sizes and step sizes."
"We present some notes on the definition of mathematical design as well as on
the methods of mathematical modeling which are used in the process of the
artistic design of the environment and its components. For the first time in
the field of geometric modeling, we perform an aesthetic analysis of planar
Bernstein-Bezier curves from the standpoint of the laws of technical
aesthetics. The shape features of the curve segments' geometry were evaluated
using the following criteria: conciseness-integrity, expressiveness,
proportional consistency, compositional balance, structural organization,
imagery, rationality, dynamism, scale, flexibility and harmony. In the
non-Russian literature, Bernstein-Bezier curves using a monotonic curvature
function (i.e., a class A Bezier curve) are considered to be fair (i.e.,
beautiful) curves, but their aesthetic analysis has never been performed. The
aesthetic analysis performed by the authors of this work means that this is no
longer the case. To confirm the conclusions of the authors' research, a survey
of the ""aesthetic appropriateness"" of certain Bernstein-Bezier curve segments
was conducted among 240 children, aged 14-17. The results of this survey have
shown themselves to be in full accordance with the authors' results."
"Finding nearly accurate distance between two or more nearly intersecting
three-dimensional (3D) objects is vital especially for collision determination
such as in virtual surgeon simulation and real-time car crash simulation.
Instead of performing broad phase collision detection, we need to check for
accuracy of detection by running narrow phase collision detection. One of the
important elements for narrow phase collision detection is to determine the
precise distance between two or more nearly intersecting objects or polygons in
order to prepare the area for potential colliding. Distance computation plays
important roles in determine the exact point of contact between two or more
nearly intersecting polygons where the preparation for collision detection is
determined at the earlier stage. In this paper, we describes our current works
of determining the distance between objects using dynamic pivot point that will
be used as reference point to reduce the complexity searching for potential
point of contacts. By using Axis-Aligned Bounding Box for each polygon, we
calculate a dynamic pivot point that will become our reference point to
determine the potential candidates for distance computation. The test our
finding distance will be simplified by using our method instead of performing
unneeded operations. Our method provides faster solution than the previous
method where it helps to determine the point of contact efficiently and faster
than the other method."
"One of the most efficient ways of generating goal-directed walking motions is
synthesising the final motion based on footprints. Nevertheless, current
implementations have not examined the generation of continuous motion based on
footprints, where different behaviours can be generated automatically.
Therefore, in this paper a flexible approach for footprint-driven locomotion
composition is presented. The presented solution is based on the ability to
generate footprint-driven locomotion, with flexible features like jumping,
running, and stair stepping. In addition, the presented system examines the
ability of generating the desired motion of the character based on predefined
footprint patterns that determine which behaviour should be performed. Finally,
it is examined the generation of transition patterns based on the velocity of
the root and the number of footsteps required to achieve the target behaviour
smoothly and naturally."
"As humans, we regularly associate shape of an object with its built material.
In the context of geometric modeling, however, this interrelation between form
and material is rarely explored. In this work, we propose a novel data-driven
reforming (i.e., reshaping) algorithm that adapts an input multi-component
model for a target fabrication material. The algorithm adapts both the part
geometry and the inter-part topology of the input shape to better align with
material specific fabrication requirements. As output, we produce the reshaped
model along with respective part dimensions and inter-part junction
specifications. We evaluate our algorithm on a range of man-made models and
demonstrate non-trivial model reshaping examples focusing only on metal and
wooden materials. We also appraise the output of our algorithm using a user
study."
"Being able to reverse engineer from point cloud data to obtain 3D models is
important in modeling. As our main contribution, we present a new method to
obtain a tensor product B-spline representation from point cloud data by
fitting surfaces to appropriately segmented data. By blending multiple local
fits our method is more efficient than existing techniques, with the ability to
deal with more detail by efficiently introducing a high number of knots.
Further point cloud data obtained by digitizing 3D data, typically presents
many associated complications like noise and missing data. As our second
contribution, we propose an end-to-end framework for smoothing, hole filling,
parameterization, knot selection and B-spline fitting that addresses these
issues, works robustly with large irregularly shaped data containing holes and
is straightforward to implement."
"This paper presents an application of photogrammetry on ceramic fragments
from two excavation sites located north-west of France. The restitution by
photogrammetry of these different fragments allowed reconstructions of the
potteries in their original state or at least to get to as close as possible.
We used the 3D reconstructions to compute some metrics and to generate a
presentation support by using a 3D printer. This work is based on affordable
tools and illustrates how 3D technologies can be quite easily integrated in
archaeology process with limited financial resources. 1. INTRODUCTION Today,
photogrammetry and 3D modelling are an integral part of the methods used in
archeology and heritage management. They provide answers to scientific needs in
the fields of conservation, preservation, restoration and mediation of
architectural, archaeological and cultural heritage [2] [6] [7] [9].
Photogrammetry on ceramic fragments was one of the first applications
contemporary of the development of this technique applied in the archaeological
community [3]. More recently and due to its democratization, it was applied
more generally to artifacts [5]. Finally joined today by the rise of 3D
printing [8] [10], it can restore fragmented artifacts [1] [12]. These examples
target one or several particular objects and use different types of equipment
that can be expensive. These aspects can put off uninitiated archaeologists. So
it would be appropriate to see if these techniques could be generalized to a
whole class of geometrically simple and common artifacts, such as ceramics.
From these observations, associated to ceramics specialists with fragments of
broken ceramics, we aimed at arranging different tools and methods, including
photogrammetry, to explore opportunities for a cheap and attainable
reconstruction methodology and its possible applications. Our first objective
was to establish a protocol for scanning fragments with photogrammetry, and for
reconstruction of original ceramics. We used the digital reconstitutions of the
ceramics we got following our process to calculate some metrics and to design
and 3D print a display for the remaining fragments of one pottery."
"In this paper, we present a novel approach to the problem of merging of
B\'ezier curves with respect to the $L_2$-norm. We give illustrative examples
to show that the solution of the conventional merging problem may not be
suitable for further modification and applications. As in the case of the
degree reduction problem, we apply the so-called restricted area approach --
proposed recently in (P. Gospodarczyk, Computer-Aided Design 62 (2015),
143--151) -- to avoid certain defects and make the resulting curve more useful.
A method of solving the new problem is based on box-constrained quadratic
programming approach."
"Halo is one of the most important basic elements in cosmology simulation,
which merges from small clumps to ever larger objects. The processes of the
birth and merging of the halos play a fundamental role in studying the
evolution of large scale cosmological structures. In this paper, a visual
analysis system is developed to interactively identify and explore the
evolution histories of thousands of halos. In this system, an intelligent
structure-aware selection method in What You See Is What You Get manner is
designed to efficiently define the interesting region in 3D space with 2D
hand-drawn lasso input. Then the exact information of halos within this 3D
region is identified by data mining in the merger tree files. To avoid visual
clutter, all the halos are projected in 2D space with a MDS method. Through the
linked view of 3D View and 2D graph, Users can interactively explore these
halos, including the tracing path and evolution history tree."
"We present a new approach to the problem of $G^{k,l}$-constrained ($k,l \leq
3$) multi-degree reduction of B\'{e}zier curves with respect to the least
squares norm. First, to minimize the least squares error, we consider two
methods of determining the values of geometric continuity parameters. One of
them is based on quadratic and nonlinear programming, while the other uses some
simplifying assumptions and solves a system of linear equations. Next, for
prescribed values of these parameters, we obtain control points of the
multi-degree reduced curve, using the properties of constrained dual Bernstein
basis polynomials. Assuming that the input and output curves are of degree $n$
and $m$, respectively, we determine these points with the complexity $O(mn)$,
which is significantly less than the cost of other known methods. Finally, we
give several examples to demonstrate the effectiveness of our algorithms."
"This paper provides a tutorial and survey for a specific kind of illustrative
visualization technique: feature lines. We examine different feature line
methods. For this, we provide the differential geometry behind these concepts
and adapt this mathematical field to the discrete differential geometry. All
discrete differential geometry terms are explained for triangulated surface
meshes. These utilities serve as basis for the feature line methods. We provide
the reader with all knowledge to re-implement every feature line method.
Furthermore, we summarize the methods and suggest a guideline for which kind of
surface which feature line algorithm is best suited. Our work is motivated by,
but not restricted to, medical and biological surface models."
"Marching surfaces is a method for isosurface extraction and approximation
based on a $G^1$ multi-sided patch interpolation scheme. Given a 3D grid of
scalar values, an underlying curve network is formed using second order
information and cubic Hermite splines. Circular arc fitting defines the tangent
vectors for the Hermite curves at specified isovalues. Once the boundary curve
network is formed, a loop of curves is determined for each grid cell and then
interpolated with multi-sided surface patches, which are $G^1$ continuous at
the joins. The data economy of the method and its continuity preserving
properties provide an effective compression scheme, ideal for indirect volume
rendering on mobile devices, or collaborating on the Internet, while enhancing
visual fidelity. The use of multi-sided patches enables a more natural way to
approximate the isosurfaces than using a fixed number of sides or polygons as
is proposed in the literature. This assertion is supported with comparisons to
the traditional Marching Cubes algorithm and other $G^1$ methods."
"When animation of a humanoid figure is to be generated at run-time, instead
of by replaying pre-composed motion clips, some method is required of
specifying the avatar's movements in a form from which the required motion data
can be automatically generated. This form must be of a more abstract nature
than raw motion data: ideally, it should be independent of the particular
avatar's proportions, and both writable by hand and suitable for automatic
generation from higher-level descriptions of the required actions.
  We describe here the development and implementation of such a scripting
language for the particular area of sign languages of the deaf, called SiGML
(Signing Gesture Markup Language), based on the existing HamNoSys notation for
sign languages.
  We conclude by suggesting how this work may be extended to more general
animation for interactive virtual reality applications."
"We present a multi-scale approach to sketch-based shape retrieval. It is
based on a novel multi-scale shape descriptor called Pyramidof- Parts, which
encodes the features and spatial relationship of the semantic parts of query
sketches. The same descriptor can also be used to represent 2D projected views
of 3D shapes, allowing effective matching of query sketches with 3D shapes
across multiple scales. Experimental results show that the proposed method
outperforms the state-of-the-art method, whether the sketch segmentation
information is obtained manually or automatically by considering each stroke as
a semantic part."
"The midpoint method or technique is a measurement and as each measurement it
has a tolerance, but worst of all it can be invalid, called Out-of-Control or
OoC. The core of all midpoint methods is the accurate measurement of the
difference of the squared distances of two points to the polar of their
midpoint with respect to the conic. When this measurement is valid, it also
measures the difference of the squared distances of these points to the conic,
although it may be inaccurate, called Out-of-Accuracy or OoA. The primary
condition is the necessary and sufficient condition that a measurement is
valid. It is comletely new and it can be checked ultra fast and before the
actual measurement starts. Modeling an incremental algorithm, shows that the
curve must be subdivided into piecewise monotonic sections, the start point
must be optimal, and it explains that the 2D-incremental method can find,
locally, the global Least Square Distance. Locally means that there are at most
three candidate points for a given monotonic direction; therefore the
2D-midpoint method has, locally, at most three measurements. When all the
possible measurements are invalid, the midpoint method cannot be applied, and
in that case the ultra fast OoC-rule selects the candidate point. This
guarantees, for the first time, a 100% stable, ultra-fast, berserkless midpoint
algorithm, which can be easily transformed to hardware."
"Character rigging is a process of endowing a character with a set of custom
manipulators and controls making it easy to animate by the animators. These
controls consist of simple joints, handles, or even separate character
selection windows.This research paper present an automated rigging system for
quadruped characters with custom controls and manipulators for animation.The
full character rigging mechanism is procedurally driven based on various
principles and requirements used by the riggers and animators. The automation
is achieved initially by creating widgets according to the character type.
These widgets then can be customized by the rigger according to the character
shape, height and proportion. Then joint locations for each body parts are
calculated and widgets are replaced programmatically.Finally a complete and
fully operational procedurally generated character control rig is created and
attached with the underlying skeletal joints. The functionality and feasibility
of the rig was analyzed from various source of actual character motion and a
requirements criterion was met. The final rigged character provides an
efficient and easy to manipulate control rig with no lagging and at high frame
rate."
"Data-driven methods play an increasingly important role in discovering
geometric, structural, and semantic relationships between 3D shapes in
collections, and applying this analysis to support intelligent modeling,
editing, and visualization of geometric data. In contrast to traditional
approaches, a key feature of data-driven approaches is that they aggregate
information from a collection of shapes to improve the analysis and processing
of individual shapes. In addition, they are able to learn models that reason
about properties and relationships of shapes without relying on hard-coded
rules or explicitly programmed instructions. We provide an overview of the main
concepts and components of these techniques, and discuss their application to
shape classification, segmentation, matching, reconstruction, modeling and
exploration, as well as scene analysis and synthesis, through reviewing the
literature and relating the existing works with both qualitative and numerical
comparisons. We conclude our report with ideas that can inspire future research
in data-driven shape analysis and processing."
"The exploding growth of digital data in the information era and its
immeasurable potential value has called for different types of data-driven
techniques to exploit its value for further applications. Information
visualization and data mining are two research field with such goal. While the
two communities advocates different approaches of problem solving, the vision
of combining the sophisticated algorithmic techniques from data mining as well
as the intuitivity and interactivity of information visualization is tempting.
In this paper, we attempt to survey recent researches and real world systems
integrating the wisdom in two fields towards more effective and efficient data
analytics. More specifically, we study the intersection from a data mining
point of view, explore how information visualization can be used to complement
and improve different stages of data mining through established theories for
optimized visual presentation as well as practical toolsets for rapid
development. We organize the survey by identifying three main stages of typical
process of data mining, the preliminary analysis of data, the model
construction, as well as the model evaluation, and study how each stage can
benefit from information visualization."
"We present a flexible illustrative line style model for the visualization of
streamline data. Our model partitions view-oriented line strips into parallel
bands whose basic visual properties can be controlled independently. We thus
extend previous line stylization techniques specifically for visualization
purposes by allowing the parametrization of these bands based on the local line
data attributes. Moreover, our approach supports emphasis and abstraction by
introducing line style transfer functions that map local line attribute values
to complete line styles. With a flexible GPU implementation of this line style
model we enable the interactive exploration of visual representations of
streamlines. We demonstrate the effectiveness of our model by applying it to 3D
flow field datasets."
"This paper presents a graph bundling algorithm that agglomerates edges taking
into account both spatial proximity as well as user-defined criteria in order
to reveal patterns that were not perceivable with previous bundling techniques.
Each edge belongs to a group that may either be an input of the problem or
found by clustering one or more edge properties such as origin, destination,
orientation, length or domain-specific properties. Bundling is driven by a
stack of density maps, with each map capturing both the edge density of a given
group as well as interactions with edges from other groups. Density maps are
efficiently calculated by smoothing 2D histograms of edge occurrence using
repeated averaging filters based on integral images.
  A CPU implementation of the algorithm is tested on several graphs, and
different grouping criteria are used to illustrate how the proposed technique
can render different visualizations of the same data. Bundling performance is
much higher than on previous approaches, being particularly noticeable on large
graphs, with millions of edges being bundled in seconds."
"This work introduces a novel tool for interactive, real-time transformations
of two dimensional IFS fractals. We assign barycentric coordinates (relative to
an arbitrary affine basis of $\mathbb{R}^2$) to the points that constitute the
image of a fractal. The tool uses some of the nice properties of the
barycentric coordinates, enabling any affine transformation of the basis, done
by click-and-drag, to be immediately followed by the same affine transformation
of the IFS fractal attractor. In order to have a better control over the
fractal, as affine basis we use a kind of minimal simplex that contains the
attractor. We give theoretical grounds of the tool and then the software
application."
"Ray tracing is a technique for generating an image by tracing the path of
light through pixels in an image plane and simulating the effects of
high-quality global illumination at a heavy computational cost. Because of the
high computation complexity, it can't reach the requirement of real-time
rendering. The emergence of many-core architectures, makes it possible to
reduce significantly the running time of ray tracing algorithm by employing the
powerful ability of floating point computation. In this paper, a new GPU
implementation and optimization of the ray tracing to accelerate the rendering
process is presented."
"Wide-angle images gained a huge popularity in the last years due to the
development of computational photography and imaging technological advances.
They present the information of a scene in a way which is more natural for the
human eye but, on the other hand, they introduce artifacts such as bent lines.
These artifacts become more and more unnatural as the field of view increases.
  In this work, we present a technique aimed to improve the perceptual quality
of panorama visualization. The main ingredients of our approach are, on one
hand, considering the viewing sphere as a Riemann sphere, what makes natural
the application of M\""obius (complex) transformations to the input image, and,
on the other hand, a projection scheme which changes in function of the field
of view used.
  We also introduce an implementation of our method, compare it against images
produced with other methods and show that the transformations can be done in
real-time, which makes our technique very appealing for new settings, as well
as for existing interactive panorama applications."
"We introduce Integral Curve Coordinates, which identify each point in a
bounded domain with a parameter along an integral curve of the gradient of a
function $f$ on that domain; suitable functions have exactly one critical
point, a maximum, in the domain, and the gradient of the function on the
boundary points inward. Because every integral curve intersects the boundary
exactly once, Integral Curve Coordinates provide a natural bijective mapping
from one domain to another given a bijection of the boundary. Our approach can
be applied to shapes in any dimension, provided that the boundary of the shape
(or cage) is topologically equivalent to an $n$-sphere. We present a simple
algorithm for generating a suitable function space for $f$ in any dimension. We
demonstrate our approach in 2D and describe a practical (simple and robust)
algorithm for tracing integral curves on a (piecewise-linear) triangulated
regular grid."
"In this paper, we use the blending functions of Lupa\c{s} type (rational)
$(p,q)$-Bernstein operators based on $(p,q)$-integers for construction of
Lupa\c{s} $(p,q)$-B$\acute{e}$zier curves (rational curves) and surfaces
(rational surfaces) with shape parameters. We study the nature of degree
elevation and degree reduction for Lupa\c{s} $(p,q)$-B$\acute{e}$zier Bernstein
functions. Parametric curves are represented using Lupa\c{s} $(p,q)$-Bernstein
basis. We introduce affine de Casteljau algorithm for Lupa\c{s} type
$(p,q)$-Bernstein B$\acute{e}$zier curves. The new curves have some properties
similar to $q$-B$\acute{e}$zier curves. Moreover, we construct the
corresponding tensor product surfaces over the rectangular domain $(u, v) \in
[0, 1] \times [0, 1] $ depending on four parameters. We also study the de
Casteljau algorithm and degree evaluation properties of the surfaces for these
generalization over the rectangular domain. We get $q$-B$\acute{e}$zier
surfaces for $(u, v) \in [0, 1] \times [0, 1] $ when we set the parameter
$p_1=p_2=1.$ In comparison to $q$-B$\acute{e}$zier curves and surfaces based on
Lupa\c{s} $q$-Bernstein polynomials, our generalization gives us more
flexibility in controlling the shapes of curves and surfaces.
  We also show that the $(p,q)$-analogue of Lupa\c{s} Bernstein operator
sequence $L^{n}_{p_n,q_n}(f,x)$ converges uniformly to $f(x)\in C[0,1]$ if and
only if $0<q_n<p_n\leq1$ such that $\lim\limits_{n\to\infty} q_n=1, $
$\lim\limits_{n\to\infty} p_n=1$ and $\lim\limits_{n\to\infty}p_n^n=a,$
$\lim\limits_{n\to\infty}q_n^n=b$ with $0<a,b\leq1.$ On the other hand, for any
$p>0$ fixed and $p \neq 1,$ the sequence $L^{n}_{p,q}(f,x)$ converges uniformly
to $f(x)~ \in C[0,1]$ if and only if $f(x)=ax+b$ for some $a, b \in
\mathbb{R}.$"
"Recent work on octree-based finite-element systems has developed a multigrid
solver for Poisson equations on meshes. While the idea of defining a regularly
indexed function space has been successfully used in a number of applications,
it has also been noted that the richness of the function space is limited
because the function values can be coupled across locally disconnected regions.
In this work, we show how to enrich the function space by introducing functions
that resolve the coupling while still preserving the nesting hierarchy that
supports multigrid. A spectral analysis reveals the superior quality of the
resulting Laplace-Beltrami operator and applications to surface flow
demonstrate that our new solver more efficiently converges to the correct
solution."
"We present in this paper a new family of implicit function for synthesizing a
wide variety of 3D surfaces. The basis of this family consists of the usual
functions that are: the function rectangular pulses, the function saw-tooth
pulses, the function of triangular pulses, the staircase function and the power
function. By combining these common functions, named constituent functions, in
one implicit function and by varying some parameters of this function we can
synthesize a wide variety of 3D surfaces with the possibility to set their
deformations."
"Ray tracing on GPUs is becoming quite common these days. There are many
publicly available documents on how to implement basic ray tracing on GPUs for
spheres and implicit surfaces. We even have some general frameworks for ray
tracing on GPUs. We however hardly find details on how to implement more
complex ray tracing algorithms themselves that are commonly used for
photorealistic rendering. This paper explains an implementation of a
stand-alone rendering system on GPUs which supports the bounding volume
hierarchy and stochastic progressive photon mapping. The key characteristic of
the system is that it uses only GLSL shaders without relying on any platform
dependent feature. The system can thus run on many platforms that support
OpenGL, making photorealistic rendering on GPUs widely accessible. This paper
also sketches practical ideas for stackless traversal and pseudorandom number
generation which both fit well with the limited system configuration."
"Information Visualization techniques are built on a context with many factors
related to both vision and cognition, making it difficult to draw a clear
picture of how data visually turns into comprehension. In the intent of
promoting a better picture, here, we survey concepts on vision, cognition, and
Information Visualization organized in a theorization named Visual Expression
Process. Our theorization organizes the basis of visualization techniques with
a reduced level of complexity; still, it is complete enough to foster
discussions related to design and analytical tasks. Our work introduces the
following contributions: (1) a Theoretical compilation of vision, cognition,
and Information Visualization; (2) Discussions supported by vast literature;
and (3) Reflections on visual-cognitive aspects concerning use and design. We
expect our contributions will provide further clarification about how users and
designers think about InfoVis, leveraging the potential of systems and
techniques."
"We revisit the design space of visualizations aiming at identifying and
relating its components. In this sense, we establish a model to examine the
process through which visualizations become expressive for users. This model
has leaded us to a taxonomy oriented to the human visual perception, a
conceptualization that provides natural criteria in order to delineate a novel
understanding for the visualization design space. The new organization of
concepts that we introduce is our main contribution: a grammar for the
visualization design based on the review of former works and of classical and
state-of-the-art techniques. Like so, the paper is presented as a survey whose
structure introduces a new conceptualization for the space of techniques
concerning visual analysis."
"In this report, we revisit the work of Pilleboue et al. [2015], providing a
representation-theoretic derivation of the closed-form expression for the
expected value and variance in homogeneous Monte Carlo integration. We show
that the results obtained for the variance estimation of Monte Carlo
integration on the torus, the sphere, and Euclidean space can be formulated as
specific instances of a more general theory. We review the related
representation theory and show how it can be used to derive a closed-form
solution."
"We propose a new gradient-domain technique for processing registered EM image
stacks to remove inter-image discontinuities while preserving intra-image
detail. To this end, we process the image stack by first performing anisotropic
smoothing along the slice axis and then solving a Poisson equation within each
slice to re-introduce the detail. The final image stack is continuous across
the slice axis and maintains sharp details within each slice. Adapting existing
out-of-core techniques for solving the linear system, we describe a parallel
algorithm with time complexity that is linear in the size of the data and space
complexity that is sub-linear, allowing us to process datasets as large as five
teravoxels with a 600 MB memory footprint."
"Accurate color reproduction is important in many applications of 3D printing,
from design prototypes to 3D color copies or portraits. Although full color is
available via other technologies, multi-jet printers have greater potential for
graphical 3D printing, in terms of reproducing complex appearance properties.
However, to date these printers cannot produce full color, and doing so poses
substantial technical challenges, from the shear amount of data to the
translucency of the available color materials. In this paper, we propose an
error diffusion halftoning approach to achieve full color with multi-jet
printers, which operates on multiple isosurfaces or layers within the object.
We propose a novel traversal algorithm for voxel surfaces, which allows the
transfer of existing error diffusion algorithms from 2D printing. The resulting
prints faithfully reproduce colors, color gradients and fine-scale details."
"This paper presents an analytical taxonomy that can suitably describe, rather
than simply classify, techniques for data presentation. Unlike previous works,
we do not consider particular aspects of visualization techniques, but their
mechanisms and foundational vision perception. Instead of just adjusting
visualization research to a classification system, our aim is to better
understand its process. For doing so, we depart from elementary concepts to
reach a model that can describe how visualization techniques work and how they
convey meaning."
"In this paper, we present a data-driven approach to generate realistic
steering behaviors for virtual crowds in crowd simulation. We take advantage of
both rule-based models and data-driven models by applying the interaction
patterns discovered from crowd videos. Unlike existing example-based models in
which current states are matched to states extracted from crowd videos
directly, our approach adopts a hierarchical mechanism to generate the steering
behaviors of agents. First, each agent is classified into one of the
interaction patterns that are automatically discovered from crowd video before
simulation. Then the most matched action is selected from the associated
interaction pattern to generate the steering behaviors of the agent. By doing
so, agents can avoid performing a simple state matching as in the traditional
example-based approaches, and can perform a wider variety of steering behaviors
as well as mimic the cognitive process of pedestrians. Simulation results on
scenarios with different crowd densities and main motion directions demonstrate
that our approach performs better than two state-of-the-art simulation models,
in terms of prediction accuracy. Besides, our approach is efficient enough to
run at interactive rates in real time simulation."
"There is an increasing requirement for efficient image retargeting techniques
to adapt the content to various forms of digital media. With rapid growth of
mobile communications and dynamic web page layouts, one often needs to resize
the media content to adapt to the desired display sizes. For various layouts of
web pages and typically small sizes of handheld portable devices, the
importance in the original image content gets obfuscated after resizing it with
the approach of uniform scaling. Thus, there occurs a need for resizing the
images in a content aware manner which can automatically discard irrelevant
information from the image and present the salient features with more
magnitude. There have been proposed some image retargeting techniques keeping
in mind the content awareness of the input image. However, these techniques
fail to prove globally effective for various kinds of images and desired sizes.
The major problem is the inefficiency of these algorithms to process these
images with minimal visual distortion while also retaining the meaning conveyed
from the image. In this dissertation, we present a novel perspective for
content aware image retargeting, which is well implementable in real time. We
introduce a novel method of analysing semantic information within the input
image while also maintaining the important and visually significant features.
We present the various nuances of our algorithm mathematically and logically,
and show that the results prove better than the state-of-the-art techniques."
"The ""Ajeijadinho 3D"" project is an initiative supported by the University of
S\~ao Paulo (Museum of Science and Dean of Culture and Extension), which
involves the 3D digitization of art works of Brazilian sculptor Antonio
Francisco Lisboa, better known as Aleijadinho. The project made use of advanced
acquisition and processing of 3D meshes for preservation and dissemination of
the cultural heritage. The dissemination occurs through a Web portal, so that
the population has the opportunity to meet the art works in detail using 3D
visualization and interaction. The portal address is
http://www.aleijadinho3d.icmc.usp.br. The 3D acquisitions were conducted over a
week at the end of July 2013 in the cities of Ouro Preto, MG, Brazil and
Congonhas do Campo, MG, Brazil. The scanning was done with a special equipment
supplied by company Leica Geosystems, which allowed the work to take place at
distances between 10 and 30 meters, defining a non-invasive procedure,
simplified logistics, and without the need for preparation or isolation of the
sites. In Ouro Preto, we digitized the churches of Francisco of Assis, Our Lady
of Carmo, and Our Lady of Mercy; in Congonhas do Campo we scanned the entire
Sanctuary of Bom Jesus de Matosinhos and his 12 prophets. Once scanned, the art
works went through a long process of preparation, which required careful
handling of meshes done by experts from the University of S\~ao Paulo in
partnership with company Imprimate."
"This paper proposes a simple and efficient method for the reconstruction and
extraction of geometric parameters from 3D tubular objects. Our method
constructs an image that accumulates surface normal information, then peaks
within this image are located by tracking. Finally, the positions of these are
optimized to lie precisely on the tubular shape centerline. This method is very
versatile, and is able to process various input data types like full or partial
mesh acquired from 3D laser scans, 3D height map or discrete volumetric images.
The proposed algorithm is simple to implement, contains few parameters and can
be computed in linear time with respect to the number of surface faces. Since
the extracted tube centerline is accurate, we are able to decompose the tube
into rectilinear parts and torus-like parts. This is done with a new linear
time 3D torus detection algorithm, which follows the same principle of a
previous work on 2D arc circle recognition. Detailed experiments show the
versatility, accuracy and robustness of our new method."
"3D shape creation and modeling remains a challenging task especially for
novice users. Many methods in the field of computer graphics have been proposed
to automate the often repetitive and precise operations needed during the
modeling of detailed shapes. This report surveys different approaches of shape
modeling and correspondence especially for shapes exhibiting topological
complexity. We focus on methods designed to help generate or process shapes
with large number of interconnected components often found in man-made shapes.
We first discuss a variety of modeling techniques, that leverage existing
shapes, in easy to use creative modeling systems. We then discuss possible
correspondence strategies for topologically different shapes as it is a
requirement for such systems. Finally, we look at different shape
representations and tools that facilitate the modification of shape topology
and we focus on those particularly useful in free-form 3D modeling."
"In the last years, Distributed Visualization over Personal Computer (PC)
clusters has become important for research and industrial communities. They
have made large-scale visualizations practical and more accessible. In this
work we survey Distributed Visualization techniques aiming at compiling last
decade's literature on the use of PC clusters as suitable alternatives to
high-end workstations. We review the topic by defining basic concepts,
enumerating system requirements and implementation challenges, and presenting
up-to-date methodologies. Our work fulfills the needs of newcomers and seasoned
professionals as an introductory compilation at the same time that it can help
experienced personnel by organizing ideas."
"Designing programming environments for physical simulation is challenging
because simulations rely on diverse algorithms and geometric domains. These
challenges are compounded when we try to run efficiently on heterogeneous
parallel architectures. We present Ebb, a domain-specific language (DSL) for
simulation, that runs efficiently on both CPUs and GPUs. Unlike previous DSLs,
Ebb uses a three-layer architecture to separate (1) simulation code, (2)
definition of data structures for geometric domains, and (3) runtimes
supporting parallel architectures. Different geometric domains are implemented
as libraries that use a common, unified, relational data model. By structuring
the simulation framework in this way, programmers implementing simulations can
focus on the physics and algorithms for each simulation without worrying about
their implementation on parallel computers. Because the geometric domain
libraries are all implemented using a common runtime based on relations, new
geometric domains can be added as needed, without specifying the details of
memory management, mapping to different parallel architectures, or having to
expand the runtime's interface.
  We evaluate Ebb by comparing it to several widely used simulations,
demonstrating comparable performance to hand-written GPU code where available,
and surpassing existing CPU performance optimizations by up to 9$\times$ when
no GPU code exists."
"One of the most useful techniques to help visual data analysis systems is
interactive filtering (brushing). However, visualization techniques often
suffer from overlap of graphical items and multiple attributes complexity,
making visual selection inefficient. In these situations, the benefits of data
visualization are not fully observable because the graphical items do not pop
up as comprehensive patterns. In this work we propose the use of content-based
data retrieval technology combined with visual analytics. The idea is to use
the similarity query functionalities provided by metric space systems in order
to select regions of the data domain according to user-guidance and interests.
After that, the data found in such regions feed multiple visualization
workspaces so that the user can inspect the correspondent datasets. Our
experiments showed that the methodology can break the visual analysis process
into smaller problems (views) and that the views hold the expectations of the
analyst according to his/her similarity query selection, improving data
perception and analytical possibilities. Our contribution introduces a
principle that can be used in all sorts of visualization techniques and
systems, this principle can be extended with different kinds of integration
visualization-metric-space, and with different metrics, expanding the
possibilities of visual data analysis in aspects such as semantics and
scalability."
"Solving large-scale optimization on-the-fly is often a difficult task for
real-time computer graphics applications. To tackle this challenge, model
reduction is a well-adopted technique. Despite its usefulness, model reduction
often requires a handcrafted subspace that spans a domain that hypothetically
embodies desirable solutions. For many applications, obtaining such subspaces
case-by-case either is impossible or requires extensive human labors, hence
does not readily have a scalable solution for growing number of tasks. We
propose linear variational subspace design for large-scale constrained
quadratic programming, which can be computed automatically without any human
interventions. We provide meaningful approximation error bound that
substantiates the quality of calculated subspace, and demonstrate its empirical
success in interactive deformable modeling for triangular and tetrahedral
meshes."
"In this paper, we present a hybrid graph-drawing algorithm (GDA) for
layouting large, naturally-clustered, disconnected graphs. We called it a
hybrid algorithm because it is an implementation of a series of already known
graph-drawing and graph-theoretic procedures. We remedy in this hybrid the
problematic nature of the current force-based GDA which has the inability to
scale to large, naturally-clustered, and disconnected graphs. These kinds of
graph usually model the complex inter-relationships among entities in social,
biological, natural, and artificial networks. Obviously, the hybrid runs longer
than the current GDAs. By using two extreme cases of graphs as inputs, we
present in this paper the derivation of the time complexity of the hybrid which
we found to be $O(|\V|^3)$."
"Handle-driven deformation based on linear blending is widely used in many
applications because of its merits in intuitiveness, efficiency and easiness of
implementation. We provide a meshfree method to compute the smooth weights of
linear blending for shape deformation. The C2-continuity of weighting is
guaranteed by the carefully formulated basis functions, with which the
computation of weights is in a closed-form. Criteria to ensure the quality of
deformation are preserved by the basis functions after decomposing the shape
domain according to the Voronoi diagram of handles. The cost of inserting a new
handle is only the time to evaluate the distances from the new handle to all
sample points in the space of deformation. Moreover, a virtual handle insertion
algorithm has been developed to allow users freely placing handles while
preserving the criteria on weights. Experimental examples for real-time 2D/3D
deformations are shown to demonstrate the effectiveness of this method."
"The Hermite radial basis functions (HRBFs) implicits have been used to
reconstruct surfaces from scattered Hermite data points. In this work, we
propose a closed-form formulation to construct HRBF-based implicits by a
quasi-solution approximating the exact solution. A scheme is developed to
automatically adjust the support sizes of basis functions to hold the error
bound of a quasi-solution. Our method can generate an implicit function from
positions and normals of scattered points without taking any global operation.
Working together with an adaptive sampling algorithm, the HRBF-based implicits
can also reconstruct surfaces from point clouds with non-uniformity and noises.
Robust and efficient reconstruction has been observed in our experimental tests
on real data captured from a variety of scenes."
"We analyze actual methods that generate smooth frame fields both in 2D and in
3D. We formalize the 2D problem by representing frames as functions (as it was
done in 3D), and show that the derived optimization problem is the one that
previous work obtain via ""representation vectors."" We show (in 2D) why this non
linear optimization problem is easier to solve than directly minimizing the
rotation angle of the field, and observe that the 2D algorithm is able to find
good fields.
  Now, the 2D and the 3D optimization problems are derived from the same
formulation (based on representing frames by functions). Their energies share
some similarities from an optimization point of view (smoothness, local minima,
bounds of partial derivatives, etc.), so we applied the 2D resolution mechanism
to the 3D problem. Our evaluation of all existing 3D methods suggests to
initialize the field by this new algorithm, but possibly use another method for
further smoothing."
"In this paper, a de Casteljau algorithm to compute (p,q)-Bernstein Bezier
curves based on (p,q)-integers is introduced. We study the nature of degree
elevation and degree reduction for (p,q)-Bezier Bernstein functions. The new
curves have some properties similar to q-Bezier curves. Moreover, we construct
the corresponding tensor product surfaces over the rectangular domain (u, v)
\in [0, 1] \times [0, 1] depending on four parameters. We also study the de
Casteljau algorithm and degree evaluation properties of the surfaces for these
generalization over the rectangular domain. Furthermore, some fundamental
properties for (p,q)-Bernstein Bezier curves are discussed. We get q-Bezier
curves and surfaces for (u, v) \in [0, 1] \times [0, 1] when we set the
parameter p1 = p2 = 1."
"Good parametrisations of affine transformations are essential to
interpolation, deformation, and analysis of shape, motion, and animation. It
has been one of the central research topics in computer graphics. However,
there is no single perfect method and each one has both advantages and
disadvantages. In this paper, we propose a novel parametrisation of affine
transformations, which is a generalisation to or an improvement of existing
methods. Our method adds yet another choice to the existing toolbox and shows
better performance in some applications. A C++ implementation is available to
make our framework ready to use in various applications."
"L-BFGS is a hill climbing method that is guarantied to converge only for
convex problems. In computer graphics, it is often used as a black box solver
for a more general class of non linear problems, including problems having many
local minima. Some works obtain very nice results by solving such difficult
problems with L-BFGS. Surprisingly, the method is able to escape local minima:
our interpretation is that the approximation of the Hessian is smoother than
the real Hessian, making it possible to evade the local minima. We analyse the
behavior of L-BFGS on the design of 2D frame fields. It involves an energy
function that is infinitly continuous, strongly non linear and having many
local minima. Moreover, the local minima have a clear visual interpretation:
they corresponds to differents frame field topologies. We observe that the
performances of LBFGS are almost unpredictables: they are very competitive when
the field is sampled on the primal graph, but really poor when they are sampled
on the dual graph."
"The prose storyboard language is a formal language for describing movies shot
by shot, where each shot is described with a unique sentence. The language uses
a simple syntax and limited vocabulary borrowed from working practices in
traditional movie-making, and is intended to be readable both by machines and
humans. The language is designed to serve as a high-level user interface for
intelligent cinematography and editing systems."
"In digital painting software, layers organize paintings. However, layers are
not explicitly represented, transmitted, or published with the final digital
painting. We propose a technique to decompose a digital painting into layers.
In our decomposition, each layer represents a coat of paint of a single paint
color applied with varying opacity throughout the image. Our decomposition is
based on the painting's RGB-space geometry. In RGB-space, a geometric structure
is revealed due to the linear nature of the standard Porter-Duff ""over"" pixel
compositing operation. The vertices of the convex hull of pixels in RGB-space
suggest paint colors. Users choose the degree of simplification to perform on
the convex hull, as well as a layer order for the colors. We solve a
constrained optimization problem to find maximally translucent, spatially
coherent opacity for each layer, such that the composition of the layers
reproduces the original image. We demonstrate the utility of the resulting
decompositions for re-editing."
"Many colour maps provided by vendors have highly uneven perceptual contrast
over their range. It is not uncommon for colour maps to have perceptual flat
spots that can hide a feature as large as one tenth of the total data range.
Colour maps may also have perceptual discontinuities that induce the appearance
of false features. Previous work in the design of perceptually uniform colour
maps has mostly failed to recognise that CIELAB space is only designed to be
perceptually uniform at very low spatial frequencies. The most important factor
in designing a colour map is to ensure that the magnitude of the incremental
change in perceptual lightness of the colours is uniform. The specific
requirements for linear, diverging, rainbow and cyclic colour maps are
developed in detail. To support this work two test images for evaluating colour
maps are presented. The use of colour maps in combination with relief shading
is considered and the conditions under which colour can enhance or disrupt
relief shading are identified. Finally, a set of new basis colours for the
construction of ternary images are presented. Unlike the RGB primaries these
basis colours produce images whereby the salience of structures are consistent
irrespective of the assignment of basis colours to data channels."
"CODYRUN is a software for computational aeraulic and thermal simulation in
buildings developed by the Laboratory of Building Physics and Systems
(L.P.B.S). Numerical simulation codes of artificial lighting have been
introduced to extend the tool capacity. These calculation codes are able to
predict the amount of light received by any point of a given working plane and
from one or more sources installed on the ceiling of the room. The model used
for these calculations is original and semi-detailed (simplified). The test
case references of the task-3 TC-33 International Commission on Illumination
(CIE) were applied to the software to ensure reliability to properly handle
this photometric aspect. This allowed having a precise idea about the
reliability of the results of numerical simulations."
"We present a technique for designing 3D-printed perforated lampshades, which
project continuous grayscale images onto the surrounding walls. Given the
geometry of the lampshade and a target grayscale image, our method computes a
distribution of tiny holes over the shell, such that the combined footprints of
the light emanating through the holes form the target image on a nearby diffuse
surface. Our objective is to approximate the continuous tones and the spatial
detail of the target image, to the extent possible within the constraints of
the fabrication process.
  To ensure structural integrity, there are lower bounds on the thickness of
the shell, the radii of the holes, and the minimal distances between adjacent
holes. Thus, the holes are realized as thin tubes distributed over the
lampshade surface. The amount of light passing through a single tube may be
controlled by the tube's radius and by its direction (tilt angle). The core of
our technique thus consists of determining a suitable configuration of the
tubes: their distribution across the relevant portion of the lampshade, as well
as the parameters (radius, tilt angle) of each tube. This is achieved by
computing a capacity-constrained Voronoi tessellation over a suitably defined
density function, and embedding a tube inside the maximal inscribed circle of
each tessellation cell. The density function for a particular target image is
derived from a series of simulated images, each corresponding to a different
uniform density tube pattern on the lampshade."
"In this paper, we present a surface remeshing method with high approximation
quality based on Principal Component Analysis. Given a triangular mesh and a
user assigned polygon/vertex budget, traditional methods usually require the
extra curvature metric field for the desired anisotropy to best approximate the
surface, even though the estimated curvature metric is known to be imperfect
and already self-contained in the surface. In our approach, this anisotropic
control is achieved through the optimal geometry partition without this
explicit metric field. The minimization of our proposed partition energy has
the following properties: Firstly, on a C2 surface, it is theoretically
guaranteed to have the optimal aspect ratio and cluster size as specified in
approximation theory for L1 piecewise linear approximation. Secondly, it
captures sharp features on practical models without any pre-tagging. We develop
an effective merging-swapping framework to seek the optimal partition and
construct polygonal/triangular mesh afterwards. The effectiveness and
efficiency of our method are demonstrated through the comparison with other
state-of-the-art remeshing methods."
"Screen content coding (SCC) is becoming increasingly important in various
applications, such as desktop sharing, video conferencing, and remote
education. When compared to natural camera- captured content, screen content
has different characteristics, in particular sharper edges. In this paper, we
propose a novel intra prediction scheme for screen content video. In the
proposed scheme, bilinear interpolation in angular intra prediction in HEVC is
selectively replaced by nearest-neighbor intra prediction to preserve the sharp
edges in screen content video. We present three different variants of the
proposed nearest neighbor prediction algorithm: two implicit methods where both
the encoder, and the decoder derive whether to perform nearest neighbor
prediction or not based on either (a) the sum of the absolute difference, or
(b) the difference between the boundary pixels from which prediction is
performed; and another variant where Rate-Distortion-Optimization (RDO) search
is performed at the encoder to decide whether or not to use the nearest
neighbor interpolation, and explicitly signaled to the decoder. We also discuss
the various underlying trade-offs in terms of the complexity of the three
variants. All the three proposed variants provide significant gains over HEVC,
and simulation results show that average gains of 3.3% BD-bitrate in
Intra-frame coding are achieved by the RDO variant for screen content video. To
the best of our knowledge, this is the first paper that 1) points out current
HEVC intra prediction scheme with bilinear interpolation does not work
efficiently for screen content video and 2) uses different filters adaptively
in the HEVC intra prediction interpolation."
"Existing bidirectional reflectance distribution function (BRDF) models are
capable of capturing the distinctive highlights produced by the fibrous nature
of wood. However, capturing parameter textures for even a single specimen
remains a laborious process requiring specialized equipment. In this paper we
take a procedural approach to generating parameters for the wood BSDF. We
characterize the elements of trees that are important for the appearance of
wood, discuss techniques appropriate for representing those features, and
present a complete procedural wood shader capable of reproducing the growth
patterns responsible for the distinctive appearance of highly prized
``figured'' wood. Our procedural wood shader is random-access, 3D, modular, and
is fast enough to generate a preview for design."
"In this paper, we use the blending functions of Bernstein polynomials with
shifted knots for construction of Bezier curves and surfaces. We study the
nature of degree elevation and degree reduction for Bezier Bernstein functions
with shifted knots.
  Parametric curves are represented using these modified Bernstein basis and
the concept of total positivity is applied to investigate the shape properties
of the curve. We get Bezier curve defined on [0, 1] when we set the parameter
\alpha=\beta to the value 0. We also present a de Casteljau algorithm to
compute Bernstein Bezier curves and surfaces with shifted knots. The new curves
have some properties similar to Bezier curves. Furthermore, some fundamental
properties for Bernstein Bezier curves and surfaces are discussed."
"Task mapping in modern high performance parallel computers can be modeled as
a graph embedding problem, which simulates the mapping as embedding one graph
into another and try to find the minimum wirelength for the mapping. Though
embedding problems have been considered for several regular graphs, such as
hypercubes into grids, binary trees into grids, et al, it is still an open
problem for hypercubes into cylinders. In this paper, we consider the problem
of embedding hypercubes into cylinders to minimize the wirelength. We obtain
the exact wirelength formula of embedding hypercube $Q^r$ into cylinder
$C_{2^3}\times P_{2^{r-3}}$ with $r\ge3$."
"Biopsy is commonly used to confirm cancer diagnosis when radiologically
indicated. Given the ability of PET to localize malignancies in heterogeneous
tumors and tumors that do not have a CT correlate, PET/CT guided biopsy may
improve the diagnostic yield of biopsies. To facilitate PET/CT guided needle
biopsy, we developed a workflow that allows us to bring PET image guidance into
the interventional CT suite. In this abstract, we present SlicerPET, a
user-friendly workflow based module developed using open source software
libraries to guide needle biopsy in the interventional suite."
"The idea of style similarity metrics has been recently developed for various
media types such as 2D clip art and 3D shapes. We explore this style metric
problem and improve existing style similarity metrics of 3D shapes in four
novel ways. First, we consider the color and texture of 3D shapes which are
important properties that have not been previously considered. Second, we
explore the effect of clustering a dataset of 3D models by comparing between
style metrics for a single object type and style metrics that combine clusters
of object types. Third, we explore the idea of user-guided learning for this
problem. Fourth, we introduce an iterative approach that can learn a metric
from a general set of 3D models. We demonstrate these contributions with
various classes of 3D shapes and with applications such as style-based
similarity search and scene composition."
"Tone-mapping operators (TMO) are designed to generate perceptually similar
low-dynamic range images from high-dynamic range ones. We studied the
performance of fifteen TMOs in two psychophysical experiments where observers
compared the digitally generated tone-mapped images to their corresponding
physical scenes. All experiments were performed in a controlled environment and
the setups were designed to emphasise different image properties: in the first
experiment we evaluated the local relationships among intensity-levels, and in
the second one we evaluated global visual appearance among physical scenes and
tone-mapped images, which were presented side by side. We ranked the TMOs
according to how well they reproduce the results obtained in the physical
scene. Our results show that ranking position clearly depends on the adopted
evaluation criteria, which implies that, in general, these tone-mapping
algorithms consider either local or global image attributes but rarely both. We
conclude that a more thorough and standardized evaluation criteria are needed
to study all the characteristics of TMOs, as there is ample room for
improvement in future developments."
"The reassembly of a broken archaeological ceramic pottery is an open and
complex problem, which remains a scientific process of extreme interest for the
archaeological community. Usually, the solutions suggested by various research
groups and universities depend on various aspects such as the matching process
of the broken surfaces, the outline of sherds or their colors and geometric
characteris-tics, their axis of symmetry, the corners of their contour, the
theme portrayed on the surface, the concentric circular rills that are left
during the base construction in the inner pottery side by the fingers of the
potter artist etc. In this work the reassembly process is based on a different
and more secure idea, since it is based on the thick-ness profile, which is
appropriately identified in every fragment. Specifically, our approach is based
on information encapsulated in the inner part of the sherd (i.e. thickness),
which is not -or at least not heavily- affected by the presence of harsh
environmental conditions, but is safely kept within the sherd itself. Our
method is verified in various use case experiments, using cutting edge
technologies such as 3D representations and precise measurements on surfaces
from the acquired 3D models."
"Targeted user studies are often employed to measure how well artists can
perform specific tasks. But these studies cannot properly describe editing
workflows as wholes, since they guide the artists both by choosing the tasks
and by using simplified interfaces. In this paper, we investigate digital
sculpting workflows used to produce detailed models. In our experiment design,
artists can choose freely what and how to model. We recover whole-workflow
trends with sophisticated statistical analyzes and validate these trends with
goodness-of-fits measures. We record brush strokes and mesh snapshots by
instrumenting a sculpting program and analyze the distribution of these
properties and their spatial and temporal characteristics. We hired expert
artists that can produce relatively sophisticated models in short time, since
their workflows are representative of best practices. We analyze 13 meshes
corresponding to roughly 25 thousand strokes in total. We found that artists
work mainly with short strokes, with average stroke length dependent on model
features rather than the artist itself. Temporally, artists do not work
coarse-to-fine but rather in bursts. Spatially, artists focus on some selected
regions by dedicating different amounts of edits and by applying different
techniques. Spatio-temporally, artists return to work on the same area multiple
times without any apparent periodicity. We release the entire dataset and all
code used for the analyzes as reference for the community."
"The generalized winding number function measures insideness for arbitrary
oriented triangle meshes. Exploiting this, I similarly generalize binary
boolean operations to act on such meshes. The resulting operations for union,
intersection, difference, etc. avoid volumetric discretization or
pre-processing."
"We present a new method for the interpolation of given data points and
associated normals with surface parametric patches with rational normal fields.
We give some arguments why a dual approach is the most convenient for these
surfaces, which are traditionally called Pythagorean normal vector (PN)
surfaces. Our construction is based on the isotropic model of the dual space to
which the original data are pushed. Then the bicubic Coons patches are
constructed in the isotropic space and then pulled back to the standard three
dimensional space. As a result we obtain the patch construction which is
completely local and produces surfaces with the global G1~continuity."
"Bezigons, i.e., closed paths composed of B\'ezier curves, have been widely
employed to describe shapes in image vectorization results. However, most
existing vectorization techniques infer the bezigons by simply approximating an
intermediate vector representation (such as polygons). Consequently, the
resultant bezigons are sometimes imperfect due to accumulated errors, fitting
ambiguities, and a lack of curve priors, especially for low-resolution images.
In this paper, we describe a novel method for vectorizing clipart images. In
contrast to previous methods, we directly optimize the bezigons rather than
using other intermediate representations; therefore, the resultant bezigons are
not only of higher fidelity compared with the original raster image but also
more reasonable because they were traced by a proficient expert. To enable such
optimization, we have overcome several challenges and have devised a
differentiable data energy as well as several curve-based prior terms. To
improve the efficiency of the optimization, we also take advantage of the local
control property of bezigons and adopt an overlapped piecewise optimization
strategy. The experimental results show that our method outperforms both the
current state-of-the-art method and commonly used commercial software in terms
of bezigon quality."
"Game level editing is the process of constructing a full game level starting
from 3D asset libraries, e.g. 3d models, textures, shaders, scripts. In level
editing, designers define the look and behavior of the whole level by placing
objects, assigning materials and lighting parameters, setting animations and
physics properties and customizing the objects AI and behavior by editing
scripts. The heterogeneity of the task usually translates to a workflow where a
team of people, experts on separate aspects, cooperate to edit the game level,
often working on the same objects (e.g.: a programmer working on the AI of a
character, while an artist works on its 3D model or its materials). Today this
collaboration is established by using version control systems designed for text
documents, such as Git, to manage different versions and share them amongst
users. The merge algorithms used in these systems though does not perform well
in our case since it does not respect the relations between game objects
necessary to maintain the semantic of the game level behavior and look. This is
a known problem and commercial systems for game level merging exists, e.g.
PlasticSCM, but these are only slightly more robust than text-based ones. This
causes designers to often merge scenes manually, essentially reapplying others
edits in the game level editor."
"This paper presents a new method for modelling the dynamic behaviour of
developable ribbons, two dimensional strips with much smaller width than
length. Instead of approximating such surface with a general triangle mesh, we
characterize it by a set of creases and bending angles across them. This
representation allows the developability to be satisfied everywhere while still
leaves enough degree of freedom to represent salient global deformation. We
show how the potential and kinetic energies can be properly discretized in this
configuration space and time integrated in a fully implicit manner. The result
is a dynamic simulator with several desirable features: We can model
non-trivial deformation using much fewer elements than conventional FEM method.
It is stable under extreme deformation, external force or large timestep size.
And we can readily handle various user constraints in Euclidean space."
"This paper deals with the problem of multi-degree reduction of a composite
B\'ezier curve with the parametric continuity constraints at the endpoints of
the segments. We present a novel method which is based on the idea of using
constrained dual Bernstein polynomials to compute the control points of the
reduced composite curve. In contrast to other methods, ours minimizes the
$L_2$-error for the whole composite curve instead of minimizing the
$L_2$-errors for each segment separately. As a result, an additional
optimization is possible. Examples show that the new method gives much better
results than multiple application of the degree reduction of a single B\'ezier
curve."
"In this report we describe a mesh editing system that we implemented that
uses a natural stretching and bending energy defined over smooth surfaces. As
such, this energy behaves uniformly under various mesh resolutions. All of the
elements of our approach already exist in the literature. We hope that our
discussions of these energies helps to shed light on the behaviors of these
methods and provides a unified discussion of these methods."
"Many problems can be presented in an abstract form through a wide range of
binary objects and relations which are defined over problem domain. In these
problems, graphical demonstration of defined binary objects and solutions is
the most suitable representation approach. In this regard, graph drawing
problem discusses the methods for transforming combinatorial graphs to
geometrical drawings in order to visualize them. This paper studies the
force-directed algorithms and multi-surface techniques for drawing general
undirected graphs. Particularly, this research describes force-directed
approach to model the drawing of a general graph as a numerical optimization
problem. So, it can use rich knowledge which is presented as an established
system by the numerical optimization. Moreover, this research proposes the
multi-surface approach as an efficient tool for overcoming local minimums in
standard force-directed algorithms. Next, we introduce a new method for
multi-surface approach based on fuzzy clustering algorithms."
"Man-made objects usually exhibit descriptive curved features (i.e., curve
networks). The curve network of an object conveys its high-level geometric and
topological structure. We present a framework for extracting feature curve
networks from unstructured point cloud data. Our framework first generates a
set of initial curved segments fitting highly curved regions. We then optimize
these curved segments to respect both data fitting and structural regularities.
Finally, the optimized curved segments are extended and connected into curve
networks using a clustering method. To facilitate effectiveness in case of
severe missing data and to resolve ambiguities, we develop a user interface for
completing the curve networks. Experiments on various imperfect point cloud
data validate the effectiveness of our curve network extraction framework. We
demonstrate the usefulness of the extracted curve networks for surface
reconstruction from incomplete point clouds."
"In traditional design, shapes are first conceived, and then fabricated. While
this decoupling simplifies the design process, it can result in inefficient
material usage, especially where off-cut pieces are hard to reuse. The
designer, in absence of explicit feedback on material usage remains helpless to
effectively adapt the design -- even though design variabilities exist. In this
paper, we investigate {\em waste minimizing furniture design} wherein based on
the current design, the user is presented with design variations that result in
more effective usage of materials. Technically, we dynamically analyze material
space layout to determine {\em which} parts to change and {\em how}, while
maintaining original design intent specified in the form of design constraints.
We evaluate the approach on simple and complex furniture design scenarios, and
demonstrate effective material usage that is difficult, if not impossible, to
achieve without computational support."
"In this technical report we derive the analytic form of the Hessian matrix
for shape matching energy. Shape matching is a useful technique for meshless
deformation, which can be easily combined with multiple techniques in real-time
dynamics. Nevertheless, it has been rarely applied in scenarios where implicit
integrators are required, and hence strong viscous damping effect, though
popular in simulation systems nowadays, is forbidden for shape matching. The
reason lies in the difficulty to derive the Hessian matrix of the shape
matching energy. Computing the Hessian matrix correctly, and stably, is the key
to more broadly application of shape matching in implicitly-integrated systems."
"In this paper, a new variant of the blossom, the $(p,q)$-blossom, is
introduced, by altering the diagonal property of the standard blossom. This
$(p,q)$-blossom has been adapted for developing identities and algorithms for
$(p,q)$-Bernstein bases and $(p,q)$-B$\acute{e}$zier curves. We generate
several new identities including an explicit formula representing the monomials
in terms of the $(p,q)$-Bernstein basis functions and a $(p,q)$-variant of
Marsden's identity by applying the $(p,q)$-blossom. We also derive for each
$(p,q)$-B$\acute{e}$zier curve of degree $n,$ a collection of $n!$ new, affine
invariant, recursive evaluation algorithms. Using two of these new recursive
evaluation algorithms, we construct a recursive subdivision algorithm for
$(p,q)$-B$\acute{e}$zier curves."
"We present a new method for real-time physics-based simulation supporting
many different types of hyperelastic materials. Previous methods such as
Position Based or Projective Dynamics are fast, but support only limited
selection of materials; even classical materials such as the Neo-Hookean
elasticity are not supported. Recently, Xu et al. [2015] introduced new
""spline-based materials"" which can be easily controlled by artists to achieve
desired animation effects. Simulation of these types of materials currently
relies on Newton's method, which is slow, even with only one iteration per
timestep. In this paper, we show that Projective Dynamics can be interpreted as
a quasi-Newton method. This insight enables very efficient simulation of a
large class of hyperelastic materials, including the Neo-Hookean, spline-based
materials, and others. The quasi-Newton interpretation also allows us to
leverage ideas from numerical optimization. In particular, we show that our
solver can be further accelerated using L-BFGS updates (Limited-memory
Broyden-Fletcher-Goldfarb-Shanno algorithm). Our final method is typically more
than 10 times faster than one iteration of Newton's method without compromising
quality. In fact, our result is often more accurate than the result obtained
with one iteration of Newton's method. Our method is also easier to implement,
implying reduced software development costs."
"This paper covers the whole process of developing an Augmented Reality
Stereoscopig Render Engine for the Oculus Rift. To capture the real world in
form of a camera stream, two cameras with fish-eye lenses had to be installed
on the Oculus Rift DK1 hardware. The idea was inspired by Steptoe
\cite{steptoe2014presence}. After the introduction, a theoretical part covers
all the most neccessary elements to achieve an AR System for the Oculus Rift,
following the implementation part where the code from the AR Stereo Engine is
explained in more detail. A short conclusion section shows some results,
reflects some experiences and in the final chapter some future works will be
discussed. The project can be accessed via the git repository
https://github.com/MaXvanHeLL/ARift.git."
"We present a new method for performing Boolean operations on volumes
represented as triangle meshes. In contrast to existing methods which treat
meshes as 3D polyhedra and try to partition the faces at their exact
intersection curves, we treat meshes as adaptive surfaces which can be
arbitrarily refined. Rather than depending on computing precise face
intersections, our approach refines the input meshes in the intersection
regions, then discards intersecting triangles and fills the resulting holes
with high-quality triangles. The original intersection curves are approximated
to a user-definable precision, and our method can identify and preserve creases
and sharp features. Advantages of our approach include the ability to trade
speed for accuracy, support for open meshes, and the ability to incorporate
tolerances to handle cases where large numbers of faces are slightly
inter-penetrating or near-coincident."
"Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public."
"We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil."
"We present, SurfCuit, a novel approach to design and construction of electric
circuits on the surface of 3D prints. Our surface mounting technique allows
durable construction of circuits on the surface of 3D prints. SurfCuit does not
require tedious circuit casing design or expensive set-ups, thus we can
expedite the process of circuit construction for 3D models. Our technique
allows the user to construct complex circuits for consumer-level desktop fused
decomposition modeling (FDM) 3D printers. The key idea behind our technique is
that FDM plastic forms a strong bond with metal when it is melted. This
observation enables construction of a robust circuit traces using copper tape
and soldering. We also present an interactive tool to design such circuits on
arbitrary 3D geometry. We demonstrate the effectiveness of our approach through
various actual construction examples."
"A method to visualize polytopes in a four dimensional euclidian space
$(x,y,z,w)$ is proposed. A polytope is sliced by multiple hyperplanes that are
parallel each other and separated by uniform intervals. Since the hyperplanes
are perpendicular to the $w$ axis, the resulting multiple slices appear in the
three-dimensional $(x,y,z)$ space and they are shown by the standard computer
graphics. The polytope is rotated extrinsically in the four dimensional space
by means of a simple input method based on keyboard typings. The multiple
slices are placed on a parabola curve in the three-dimensional world
coordinates. The slices in a view window form an oval appearance. Both the
simple and the double rotations in the four dimensional space are applied to
the polytope. All slices synchronously change their shapes when a rotation is
applied to the polytope. The compact display in the oval of many slices with
the help of quick rotations facilitate a grasp of the four dimensional
configuration of the polytope."
"During medicine studies, visualization of certain elements is common and
indispensable in order to get more information about the way they work.
Currently, we resort to the use of photographs -which are insufficient due to
being static- or tests in patients, which can be invasive or even risky.
Therefore, a low-cost approach is proposed by using a 3D visualization. This
paper presents a holographic system built with low-cost materials for teaching
obstetrics, where student interaction is performed by using voice and gestures.
Our solution, which we called HoloMed, is focused on the projection of a
euthocic normal delivery under a web-based infrastructure which also employs a
Kinect. HoloMed is divided in three (3) essential modules: a gesture analyzer,
a data server, and a holographic projection architecture, which can be executed
in several interconnected computers using different network protocols. Tests
used for determining the user's position, illumination factors, and response
times, demonstrate HoloMed's effectiveness as a low-cost system for teaching,
using a natural user interface and 3D images."
"We present a novel algorithm to control the physically-based animation of
smoke. Given a set of keyframe smoke shapes, we compute a dense sequence of
control force fields that can drive the smoke shape to match several keyframes
at certain time instances. Our approach formulates this control problem as a
PDE constrained spacetime optimization and computes locally optimal control
forces as the stationary point of the Karush-Kuhn-Tucker conditions. In order
to reduce the high complexity of multiple passes of fluid resimulation, we
utilize the coherence between consecutive fluid simulation passes and update
our solution using a novel spacetime full approximation scheme (STFAS). We
demonstrate the benefits of our approach by computing accurate solutions on 2D
and 3D benchmarks. In practice, we observe more than an order of magnitude
improvement over prior methods."
"We introduce geoplotlib, an open-source python toolbox for visualizing
geographical data. geoplotlib supports the development of hardware-accelerated
interactive visualizations in pure python, and provides implementations of dot
maps, kernel density estimation, spatial graphs, Voronoi tesselation,
shapefiles and many more common spatial visualizations. We describe geoplotlib
design, functionalities and use cases."
"Subdivision is a well-known and established method for generating smooth
curves and surfaces from discrete data by repeated refinements. The typical
input for such a process is a mesh of vertices. In this work we propose to
refine 2D data consisting of vertices of a polygon and a normal at each vertex.
Our core refinement procedure is based on a circle average, which is a new
non-linear weighted average of two points and their corresponding normals. The
ability to locally approximate curves by the circle average is demonstrated.
With this ability, the circle average is a candidate for modifying linear
subdivision schemes refining points, to schemes refining point-normal pairs.
This is done by replacing the weighted binary arithmetic means in a linear
subdivision scheme, expressed in terms of repeated binary averages, by circle
averages with the same weights. Here we investigate the modified
Lane-Riesenfeld algorithm and the 4-point scheme. For the case that the initial
data consists of a control polygon only, a naive method for choosing initial
normals is proposed. An example demonstrates the superiority of the above two
modified schemes, with the naive choice of initial normals over the
corresponding linear schemes, when applied to a control polygon with edges of
significantly different lengths."
"Porous structures such as trabecular bone are widely seen in nature. These
structures exhibit superior mechanical properties whilst being lightweight. In
this paper, we present a method to generate bone-like porous structures as
lightweight infill for additive manufacturing. Our method builds upon and
extends voxel-wise topology optimization. In particular, for the purpose of
generating sparse yet stable structures distributed in the interior of a given
shape, we propose upper bounds on the localized material volume in the
proximity of each voxel in the design domain. We then aggregate the local
per-voxel constraints by their p-norm into an equivalent global constraint, in
order to facilitate an efficient optimization process. Implemented on a
high-resolution topology optimization framework, our results demonstrate
mechanically optimized, detailed porous structures which mimic those found in
nature. We further show variants of the optimized structures subject to
different design specifications, and analyze the optimality and robustness of
the obtained structures."
"The Position Based Fluids (PBF) method is a state-of-the-art approach for
fluid simulations in the context of real-time applications like games. It uses
an iterative solver concept that tries to maintain a constant fluid density
(incompressibility) to realize incompressible fluids like water. However,
larger fluid volumes that consist of several hundred thousand particles (e.g.
for the simulation of oceans) require many iterations and a lot of simulation
power. We present a lightweight and easy-to-integrate extension to PBF that
adaptively adjusts the number of solver iterations on a fine-grained basis.
Using a novel adaptive-simulation approach, we are able to achieve significant
improvements in performance on our evaluation scenarios while maintaining
high-quality results in terms of visualization quality, which makes it a
perfect choice for game developers. Furthermore, our method does not weaken the
advantages of prior work and seamlessly integrates into other position-based
methods for physically-based simulations."
"We present a web application for the procedural generation of transformations
of 3D models. We generate the transformations by algorithmically generating the
vertex shaders of the 3D models. The vertex shaders are created with an
interactive genetic algorithm, which displays to the user the visual effect
caused by each vertex shader, allows the user to select the visual effect the
user likes best, and produces a new generation of vertex shaders using the user
feedback as the fitness measure of the genetic algorithm. We use genetic
programming to represent each vertex shader as a computer program. This paper
presents details of requirements specification, software architecture, high and
low-level design, and prototype user interface. We discuss the project's
current status and development challenges."
"A wide variety of color schemes have been devised for mapping scalar data to
color. Some use the data value to index a color scale. Others assign colors to
different, usually blended disjoint materials, to handle areas where materials
overlap. A number of methods can map low-dimensional data to color, however,
these methods do not scale to higher dimensional data. Likewise, schemes that
take a more artistic approach through color mixing and the like also face
limits when it comes to the number of variables they can encode. We address the
challenge of mapping multivariate data to color and avoid these limitations at
the same time. It is a data driven method, which first gauges the similarity of
the attributes and then arranges them according to the periphery of a convex 2D
color space, such as HSL. The color of a multivariate data sample is then
obtained via generalized barycentric coordinate (GBC) interpolation."
"Embedding high-dimensional data into a 2D canvas is a popular strategy for
their visualization."
"We present a novel method to interpolate smoke and liquid simulations in
order to perform data-driven fluid simulations. Our approach calculates a dense
space-time deformation using grid-based signed-distance functions of the
inputs. A key advantage of this implicit Eulerian representation is that it
allows us to use powerful techniques from the optical flow area. We employ a
five-dimensional optical flow solve. In combination with a projection
algorithm, and residual iterations, we achieve a robust matching of the inputs.
Once the match is computed, arbitrary in between variants can be created very
efficiently. To concatenate multiple long-range deformations, we propose a
novel alignment technique. Our approach has numerous advantages, including
automatic matches without user input, volumetric deformations that can be
applied to details around the surface, and the inherent handling of topology
changes. As a result, we can interpolate swirling smoke clouds, and splashing
liquid simulations. We can even match and interpolate phenomena with
fundamentally different physics: a drop of liquid, and a blob of heavy smoke."
"This German paper was written entirely at the University of Duisburg-Essen in
2011 for a 3D modeling masters course in applied computer science. We publish
this paper, thus, interested people can acquire a first impression of the topic
""volume raycasting"". In addition to writing this paper, we developed a
functioning open-source OpenCL raycaster. A video of this raycaster is
available: http://www.youtube.com/watch?v=VMMsQnf4zEY. Additionally, we
archived and published the complete source code of the raycaster in the Google
Code Archive: http://code.google.com/p/gputracer/. If this is no longer the
case, those who are interested can also write an email to the author, hence, we
can provide the source code.
  This paper provides an introduction and overview of the topic ""volume ray
casting with OpenCL"". We show how volume data can be loaded, manipulated, and
visualized by modern GPUs in real time. In addition, we present basic
algorithms and data structures that are necessary for building such a
raycaster. Then, we describe how we built a rudimentary raycaster using OpenCL
and .NET C#. Furthermore, we analyze different gradient operators
(CentralDifference, Sobel3D and Zucker-Hummel) for surface detection and show
an evaluation of these with respect to their performance. Finally, we present
optimization techniques (hitpoint refinement, adaptive sampling, octrees, and
empty-space-skipping) for improving a raycaster."
"Sub-surface scattering is key to our perception of translucent materials.
Models based on diffusion theory are used to render such materials in a
realistic manner by evaluating an approximation of the material BSSRDF at any
two points of the surface. Under the assumption of perpendicular incidence,
this BSSRDF approximation can be tabulated over 2 dimensions to provide fast
evaluation and importance sampling. However, accounting for non-perpendicular
incidence with the same approach would require to tabulate over 4 dimensions,
making the model too large for practical applications. In this report, we
present a method to efficiently evaluate and importance sample the
multi-scattering component of diffusion based BSSRDFs for non-perpendicular
incidence. Our approach is based on tabulating a compressed angular model of
Photon Beam Diffusion. We explain how to generate, evaluate and sample our
model. We show that 1 MiB is enough to store a model of the multi-scattering
BSSRDF that is within $0.5\%$ relative error of Photon Beam Diffusion. Finally,
we present a method to use our model in a Monte Carlo particle tracer and show
results of our implementation in PBRT."
"Layered manufacturing inherently suffers from staircase defects along
surfaces that are gently slopped with respect to the build direction. Reducing
the slice thickness improves the situation but never resolves it completely as
flat layers remain a poor approximation of the true surface in these regions.
In addition, reducing the slice thickness largely increases the print time. In
this work we focus on a simple yet effective technique to improve the print
accuracy for layered manufacturing by filament deposition. Our method works
with standard three-axis 3D filament printers (e.g. the typical, widely
available 3D printers), using standard extrusion nozzles. It better reproduces
the geometry of sloped surfaces without increasing the print time. Our key idea
is to perform a local anti-aliasing, working at a sub-layer accuracy to produce
slightly curved deposition paths and reduce approximation errors. This is
inspired by Computer Graphics anti-aliasing techniques which consider sub-pixel
precision to treat aliasing effects. We show that the necessary deviation in
height compared to standard slicing is bounded by half the layer thickness.
Therefore, the height changes remain small and plastic deposition remains
reliable. We further split and order paths to minimize defects due to the
extruder nozzle shape, avoiding any change to the existing hardware. We apply
and analyze our approach on 3D printed examples, showing that our technique
greatly improves surface accuracy and silhouette quality while keeping the
print time nearly identical."
"Volumetric cloudscapes are prohibitively expensive to render in real time
without extensive optimisations. Previous approaches render the clouds to an
offscreen buffer at one quarter resolution and update a fraction of the pixels
per frame, drawing the remaining pixels by temporal reprojection. We present an
alternative approach, reducing the number of raymarching steps and adding a
randomly jittered offset to the raymarch. We use an analytical integration
technique to make results consistent with a lower number of raymarching steps.
To remove noise from the resulting image we apply a temporal anti-aliasing
implementation. The result is a technique producing visually similar results
with 1/16 the number of steps."
"We present an unsupervised method for co-segmentation of a set of 3D shapes
from the same class with the aim of segmenting the input shapes into consistent
semantic parts and establishing their correspondence across the set. Starting
from meaningful pre-segmentation of all given shapes individually, we construct
the correspondence between same candidate parts and obtain the labels via
functional maps. And then, we use these labels to mark the input shapes and
obtain results of co-segmentation. The core of our algorithm is to seek for an
optimal correspondence between semantically similar parts through functional
maps and mark such shape parts. Experimental results on the benchmark datasets
show the efficiency of this method and comparable accuracy to the
state-of-the-art algorithms."
"We introduce the {\em polygon cloud}, also known as a polygon set or {\em
soup}, as a compressible representation of 3D geometry (including its
attributes, such as color texture) intermediate between polygonal meshes and
point clouds. Dynamic or time-varying polygon clouds, like dynamic polygonal
meshes and dynamic point clouds, can take advantage of temporal redundancy for
compression, if certain challenges are addressed. In this paper, we propose
methods for compressing both static and dynamic polygon clouds, specifically
triangle clouds. We compare triangle clouds to both triangle meshes and point
clouds in terms of compression, for live captured dynamic colored geometry. We
find that triangle clouds can be compressed nearly as well as triangle meshes,
while being far more robust to noise and other structures typically found in
live captures, which violate the assumption of a smooth surface manifold, such
as lines, points, and ragged boundaries. We also find that triangle clouds can
be used to compress point clouds with significantly better performance than
previously demonstrated point cloud compression methods. In particular, for
intra-frame coding of geometry, our method improves upon octree-based
intra-frame coding by a factor of 5-10 in bit rate. Inter-frame coding improves
this by another factor of 2-5. Overall, our dynamic triangle cloud compression
improves over the previous state-of-the-art in dynamic point cloud compression
by 33\% or more."
"The inverse diffusion curve problem focuses on automatic creation of
diffusion curve images that resemble user provided color fields. This problem
is challenging since the 1D curves have a nonlinear and global impact on
resulting color fields via a partial differential equation (PDE). We introduce
a new approach complementary to previous methods by optimizing curve geometry.
In particular, we propose a novel iterative algorithm based on the theory of
shape derivatives. The resulting diffusion curves are clean and well-shaped,
and the final image closely approximates the input. Our method provides a
user-controlled parameter to regularize curve complexity, and generalizes to
handle input color fields represented in a variety of formats."
"A systematic fractal brownian motion approach is proposed for generating
coherent noise, aiming at procedurally generating realistic terrain and
textures. Two models are tested and compared to Perlin noise method for
two-dimensional height map generation. A fractal analysis is performed in order
to compare fractal behaviour of generated data to real terrain coastlines from
the point of view of fractal dimension. Performance analysis show that one of
the described schemes requires half as many primitive operations than Perlin
noise while producing data of equivalent quality."
"We present a novel algorithm to compute multi-scale curvature fields on
triangle meshes. Our algorithm is based on finding robust mean curvatures using
the ball neighborhood, where the radius of a ball corresponds to the scale of
the features. The essential problem is to find a good radius for each ball to
obtain a reliable curvature estimation. We propose an algorithm that finds
suitable radii in an automatic way. In particular, our algorithm is applicable
to meshes produced by image-based reconstruction systems. These meshes often
contain geometric features at various scales, for example if certain regions
have been captured in greater detail. We also show how such a multi-scale
curvature field can be converted to a density field and used to guide
applications like mesh simplification."
"The problem of mesh matching is addressed in this work. For a given n-sided
planar region bounded by one loop of n polylines we are selecting optimal
quadrilateral mesh from existing catalogue of meshes. The formulation of
matching between planar shape and quadrilateral mesh from the catalogue is
based on the problem of finding longest common subsequence (LCS). Theoretical
foundation of mesh matching method is provided. Suggested method represents a
viable technique for selecting best mesh for planar region and stepping stone
for further parametrization of the region."
"A new taxonomy of issues related to CAD model quality is presented, which
distinguishes between explicit and procedural models. For each type of model,
morphologic, syntactic, and semantic errors are characterized. The taxonomy was
validated successfully when used to classify quality testing tools, which are
aimed at detecting and repairing data errors that may affect the
simplification, interoperability, and reusability of CAD models. The study
shows that low semantic level errors that hamper simplification are reasonably
covered in explicit representations, although many CAD quality testers are
still unaffordable for Small and Medium Enterprises, both in terms of cost and
training time. Interoperability has been reasonably solved by standards like
STEP AP 203 and AP214, but model reusability is not feasible in explicit
representations. Procedural representations are promising, as interactive
modeling editors automatically prevent most morphologic errors derived from
unsuitable modeling strategies. Interoperability problems between procedural
representations are expected to decrease dramatically with STEP AP242. Higher
semantic aspects of quality such as assurance of design intent, however, are
hardly supported by current CAD quality testers."
"Fractal image generation algorithms exhibit extreme parallelizability. Using
general purpose graphics processing unit (GPU) programming to implement
escape-time algorithms for Julia sets of functions,parallel methods generate
visually attractive fractal images much faster than traditional methods. Vastly
improved speeds are achieved using this method of computation, which allow
real-time generation and display of images. A comparison is made between
sequential and parallel implementations of the algorithm. An application
created by the authors demonstrates using the increased speed to create dynamic
imaging of fractals where the user may explore paths of parameter values
corresponding to a given function's Mandelbrot set. Examples are given of
artistic and mathematical insights gained by experiencing fractals
interactively and from the ability to sample the parameter space quickly and
comprehensively."
"We apply a novel optimization scheme from the image processing and machine
learning areas, a fast Primal-Dual method, to achieve controllable and
realistic fluid simulations. While our method is generally applicable to many
problems in fluid simulations, we focus on the two topics of fluid guiding and
separating solid-wall boundary conditions. Each problem is posed as an
optimization problem and solved using our method, which contains acceleration
schemes tailored to each problem. In fluid guiding, we are interested in
partially guiding fluid motion to exert control while preserving fluid
characteristics. With our method, we achieve explicit control over both
large-scale motions and small-scale details which is valuable for many
applications, such as level-of-detail adjustment (after running the coarse
simulation), spatially varying guiding strength, domain modification, and
resimulation with different fluid parameters. For the separating solid-wall
boundary conditions problem, our method effectively eliminates unrealistic
artifacts of fluid crawling up solid walls and sticking to ceilings, requiring
few changes to existing implementations. We demonstrate the fast convergence of
our Primal-Dual method with a variety of test cases for both model problems."
"Immersive, stereoscopic viewing enables scientists to better analyze the
spatial structures of visualized physical phenomena. However, their findings
cannot be properly presented in traditional media, which lack these core
attributes. Creating a presentation tool that captures this environment poses
unique challenges, namely related to poor viewing accessibility. Immersive
scientific renderings often require high-end equipment, which can be
impractical to obtain. We address these challenges with our authoring tool and
navigational interface, which is designed for affordable head-mounted displays.
With the authoring tool, scientists can show salient data features as connected
360{\deg} video paths, resulting in a ""choose-your-own-adventure"" experience.
Our navigational interface features bidirectional video playback for added
viewing control when users traverse the tailor-made content. We evaluate our
system's benefits by authoring case studies on several data sets and conducting
a usability study on the navigational interface's design. In summary, our
approach provides scientists an immersive medium to visually present their
research to the intended audience--spanning from students to colleagues--on
affordable virtual reality headsets."
"We provide a qualitative and quantitative evaluation of 8 clear sky models
used in Computer Graphics. We compare the models with each other as well as
with measurements and with a reference model from the physics community. After
a short summary of the physics of the problem, we present the measurements and
the reference model, and how we ""invert"" it to get the model parameters. We
then give an overview of each CG model, and detail its scope, its algorithmic
complexity, and its results using the same parameters as in the reference
model. We also compare the models with a perceptual study. Our quantitative
results confirm that the less simplifications and approximations are used to
solve the physical equations, the more accurate are the results. We conclude
with a discussion of the advantages and drawbacks of each model, and how to
further improve their accuracy."
"The EditLens is an interactive lens technique that supports the editing of
graphs. The user can insert, update, or delete nodes and edges while
maintaining an already existing layout of the graph. For the nodes and edges
that are affected by an edit operation, the EditLens suggests suitable
locations and routes, which the user can accept or adjust. For this purpose,
the EditLens requires an efficient routing algorithm that can compute results
at interactive framerates. Existing algorithms cannot fully satisfy the needs
of the EditLens. This paper describes a novel algorithm that can compute
orthogonal edge routes for incremental edit operations of graphs. Tests
indicate that, in general, the algorithm is better than alternative solutions."
"In this manuscript, inspired by a simpler reformulation of primary sample
space Metropolis light transport, we derive a novel family of general Markov
chain Monte Carlo algorithms called charted Metropolis-Hastings, that
introduces the notion of sampling charts to extend a given sampling domain and
making it easier to sample the desired target distribution and escape from
local maxima through coordinate changes. We further apply the novel algorithms
to light transport simulation, obtaining a new type of algorithm called charted
Metropolis light transport, that can be seen as a bridge between primary sample
space and path space Metropolis light transport. The new algorithms require to
provide only right inverses of the sampling functions, a property that we
believe crucial to make them practical in the context of light transport
simulation. We further propose a method to integrate density estimation into
this framework through a novel scheme that uses it as an independence sampler."
"This paper proposes a shoulder inverse kinematics (IK) technique. Shoulder
complex is comprised of the sternum, clavicle, ribs, scapula, humerus, and four
joints. The shoulder complex shows specific motion pattern, such as Scapulo
humeral rhythm. As a result, if a motion of the shoulder isgenerated without
the knowledge of kinesiology, it will be seen as un-natural. The proposed
technique generates motion of the shoulder complex about the orientation of the
upper arm by interpolating the measurement data. The shoulder IK method allows
novice animators to generate natural shoulder motions easily. As a result, this
technique improves the quality of character animation."
"This article introduces a new notion of optimal transport (OT) between tensor
fields, which are measures whose values are positive semidefinite (PSD)
matrices. This ""quantum"" formulation of OT (Q-OT) corresponds to a relaxed
version of the classical Kantorovich transport problem, where the fidelity
between the input PSD-valued measures is captured using the geometry of the
Von-Neumann quantum entropy. We propose a quantum-entropic regularization of
the resulting convex optimization problem, which can be solved efficiently
using an iterative scaling algorithm. This method is a generalization of the
celebrated Sinkhorn algorithm to the quantum setting of PSD matrices. We extend
this formulation and the quantum Sinkhorn algorithm to compute barycenters
within a collection of input tensor fields. We illustrate the usefulness of the
proposed approach on applications to procedural noise generation, anisotropic
meshing, diffusion tensor imaging and spectral texture synthesis."
"Exploring and editing colors in images is a common task in graphic design and
photography. However, allowing for interactive recoloring while preserving
smooth color blends in the image remains a challenging problem. We present
LayerBuilder, an algorithm that decomposes an image or video into a linear
combination of colored layers to facilitate color-editing applications. These
layers provide an interactive and intuitive means for manipulating individual
colors. Our approach reduces color layer extraction to a fast iterative linear
system. Layer Builder uses locally linear embedding, which represents pixels as
linear combinations of their neighbors, to reduce the number of variables in
the linear solve and extract layers that can better preserve color blending
effects. We demonstrate our algorithm on recoloring a variety of images and
videos, and show its overall effectiveness in recoloring quality and time
complexity compared to previous approaches. We also show how this
representation can benefit other applications, such as automatic recoloring
suggestion, texture synthesis, and color-based filtering."
"This paper presents Poisson vector graphics, an extension of the popular
first-order diffusion curves, for generating smooth-shaded images. Armed with
two new types of primitives, namely Poisson curves and Poisson regions, PVG can
easily produce photorealistic effects such as specular highlights, core
shadows, translucency and halos. Within the PVG framework, users specify color
as the Dirichlet boundary condition of diffusion curves and control tone by
offsetting the Laplacian, where both controls are simply done by mouse click
and slider dragging. The separation of color and tone not only follows the
basic drawing principle that is widely adopted by professional artists, but
also brings three unique features to PVG, i.e., local hue change, ease of
extrema control, and permit of intersection among geometric primitives, making
PVG an ideal authoring tool.
  To render PVG, we develop an efficient method to solve 2D Poisson's equations
with piecewise constant Laplacians. In contrast to the conventional finite
element method that computes numerical solutions only, our method expresses the
solution using harmonic B-spline, whose basis functions can be constructed
locally and the control coefficients are obtained by solving a small sparse
linear system. Our closed-form solver is numerically stable and it supports
random access evaluation, zooming-in of arbitrary resolution and anti-aliasing.
Although the harmonic B-spline based solutions are approximate, computational
results show that the relative mean error is less than 0.3%, which cannot be
distinguished by naked eyes."
"In this paper a new system for piecewise primitive surface recovery on point
clouds is presented, which allows a novice user to sketch areas of interest in
order to guide the fitting process. The algorithm is demonstrated against a
benchmark technique for autonomous surface fitting, and, contrasted against
existing literature in user guided surface recovery, with empirical evidence.
It is concluded that the system is an improvement to the current documented
literature for its visual quality when modelling objects which are composed of
piecewise primitive shapes, and, in its ability to fill large holes on occluded
surfaces using free-form input."
"Photographers routinely compose multiple manipulated photos of the same scene
(layers) into a single image, which is better than any individual photo could
be alone. Similarly, 3D artists set up rendering systems to produce layered
images to contain only individual aspects of the light transport, which are
composed into the final result in post-production. Regrettably, both approaches
either take considerable time to capture, or remain limited to synthetic
scenes. In this paper, we suggest a system to allow decomposing a single image
into a plausible shading decomposition (PSD) that approximates effects such as
shadow, diffuse illumination, albedo, and specular shading. This decomposition
can then be manipulated in any off-the-shelf image manipulation software and
recomposited back. We perform such a decomposition by learning a convolutional
neural network trained using synthetic data. We demonstrate the effectiveness
of our decomposition on synthetic (i.e., rendered) and real data (i.e.,
photographs), and use them for common photo manipulation, which are nearly
impossible to perform otherwise from single images."
"Dealing with visualizations containing large data set is a challenging issue
and, in the field of Information Visualization, almost every visual technique
reveals its drawback when visualizing large number of items. To deal with this
problem we introduce a formal environment, modeling in a virtual space the
image features we are interested in (e.g, absolute and relative density,
clusters, etc.) and we define some metrics able to characterize the image
decay. Such metrics drive our automatic techniques (i.e., not uniform sampling)
rescuing the image features and making them visible to the user. In this paper
we focus on 2D scatter-plots, devising a novel non uniform data sampling
strategy able to preserve in an effective way relative densities."
"This paper presents a realization of the approach to spatial three Dimension
stereo of visualization of three Dimension images with use parallel Graphics
processing unit (GPU). The experiments of realization of synthesis of images of
a 3D stage by a method of trace of beams on GPU with Compute Unified Device
Architecture have shown that 60 % of the time is spent for the decision of a
computing problem approximately, the major part of time (40 %) is spent for
transfer of data between the central processing unit and GPU for calculations
and the organization process of visualization. The study of the influence of
increase in the size of the GPU network at the speed of calculations showed
importance of the correct task of structure of formation of the parallel
computer network and general mechanism of parallelization.
  Keywords: Volumetric three Dimension visualization, stereo three Dimension
visualization, Ray tracing, parallel computing on GPU, Compute Unified Device
Architecture."
"The article presents a general concept of the organization of pseudo three
dimension visualization of graphics and video content for three dimension
visualization systems. The steps of algorithms for solving the problem of
synthesis of three dimension stereo images based on two dimension images are
introduced. The features of synthesis organization of standard format of three
dimension stereo frame are presented. Moreover, the performed experimental
simulation for generating complete stereoframes and the results of its time
complexity are shown. Keywords:Three dimension visualization, pseudo three
dimension stereo, a stereo pair, three dimension stereo format, algorithm,
modeling, time complexity."
"A general concept of 3D volumetric visualization systems is described based
on 3D discrete voxel scenes (worlds) representation. Definitions of 3D discrete
voxel scene (world) basic elements and main steps of the image synthesis
algorithm are formulated. An algorithm for solving the problem of the voxelized
world 3D image synthesis, intended for the systems of volumetric spatial
visualization, is proposed. A computer-based architecture for 3D volumetric
visualization of 3D discrete voxel world is presented. On the basis of the
proposed overall concept of discrete voxel representation, the proposed
architecture successfully adapts the ray tracing technique for the synthesis of
3D volumetric images. Since it is algorithmically simple and effectively
supports parallelism, it can efficiently be implemented.
  Key words:Volumetric spatial visualization, 3D volumetric imagesynthesis,
discrete voxel world, ray tracing."
"This paper presents the three scripting commands and main functionalities of
a novel character animation environment called CHASE. CHASE was developed for
enabling inexperienced programmers, animators, artists, and students to animate
in meaningful ways virtual reality characters. This is achieved by scripting
simple commands within CHASE. The commands identified, which are associated
with simple parameters, are responsible for generating a number of predefined
motions and actions of a character. Hence, the virtual character is able to
animate within a virtual environment and to interact with tasks located within
it. An additional functionality of CHASE is supplied. It provides the ability
to generate multiple tasks of a character, such as providing the user the
ability to generate scenario-related animated sequences. However, since
multiple characters may require simultaneous animation, the ability to script
actions of different characters at the same time is also provided."
"We present here the first systematic treatment of the problems posed by the
visualization and analysis of large-scale, parallel adaptive mesh refinement
(AMR) simulations on an Eulerian grid. When compared to those obtained by
constructing an intermediate unstructured mesh with fully described
connectivity, our primary results indicate a gain of at least 80\% in terms of
memory footprint, with a better rendering while retaining similar execution
speed. In this article, we describe the key concepts that allow us to obtain
these results, together with the methodology that facilitates the design,
implementation, and optimization of algorithms operating directly on such
refined meshes. This native support for AMR meshes has been contributed to the
open source Visualization Toolkit (VTK). This work pertains to a broader
long-term vision, with the dual goal to both improve interactivity when
exploring such data sets in 2 and 3 dimensions, and optimize resource
utilization."
"We present here the result of continuation work, performed to further fulfill
the vision we outlined in [Harel,Lekien,P\'eba\""y-2017] for the visualization
and analysis of tree-based adaptive mesh refinement (AMR) simulations, using
the hypertree grid paradigm which we proposed.
  The first filter presented hereafter implements an adaptive approach in order
to accelerate the rendering of 2-dimensional AMR grids, hereby solving the
problem posed by the loss of interactivity that occurs when dealing with large
and/or deeply refined meshes. Specifically, view parameters are taken into
account, in order to: on one hand, avoid creating surface elements that are
outside of the view area; on the other hand, utilize level-of-detail properties
to cull those cells that are deemed too small to be visible with respect to the
given view parameters. This adaptive approach often results in a massive
increase in rendering performance.
  In addition, two new selection filters provide data analysis capabilities, by
means of allowing for the extraction of those cells within a hypertree grid
that are deemed relevant in some sense, either geometrically or topologically.
After a description of these new algorithms, we illustrate their use within the
Visualization Toolkit (VTK) in which we implemented them. This note ends with
some suggestions for subsequent work."
"A challenge in isogeometric analysis is constructing analysis-suitable
volumetric meshes which can accurately represent the geometry of a given
physical domain. In this paper, we propose a method to derive a spline-based
representation of a domain of interest from voxel-based data. We show an
efficient way to obtain a boundary representation of the domain by a level-set
function. Then, we use the geometric information from the boundary (the normal
vectors and curvature) to construct a matching C1 representation with
hierarchical cubic splines. The approximation is done by a single template and
linear transformations (scaling, translations and rotations) without the need
for solving an optimization problem. We illustrate our method with several
examples in two and three dimensions, and show good performance on some
standard benchmark test problems."
"With virtual reality, digital painting on 2D canvases is now being extended
to 3D spaces. Tilt Brush and Oculus Quill are widely accepted among artists as
tools that pave the way to a new form of art - 3D emmersive painting. Current
3D painting systems are only a start, emitting textured triangular geometries.
In this paper, we advance this new art of 3D painting to 3D volumetric painting
that enables an artist to draw a huge scene with full control of spatial color
fields. Inspired by the fact that 2D paintings often use vast space to paint
background and small but detailed space for foreground, we claim that
supporting a large canvas in varying detail is essential for 3D painting. In
order to help artists focus and audiences to navigate the large canvas space,
we provide small artist-defined areas, called rooms, that serve as beacons for
artist-suggested scales, spaces, locations for intended appreciation view of
the painting. Artists and audiences can easily transport themselves between
different rooms. Technically, our canvas is represented as an array of deep
octrees of depth 24 or higher, built on CPU for volume painting and on GPU for
volume rendering using accurate ray casting. In CPU side, we design an
efficient iterative algorithm to refine or coarsen octree, as a result of
volumetric painting strokes, at highly interactive rates, and update the
corresponding GPU textures. Then we use GPU-based ray casting algorithms to
render the volumetric painting result. We explore precision issues stemming
from ray-casting the octree of high depth, and provide a new analysis and
verification. From our experimental results as well as the positive feedback
from the participating artists, we strongly believe that our new 3D volume
painting system can open up a new possibility for VR-driven digital art medium
to professional artists as well as to novice users."
"This invited talk will present recent projection mapping technologies for
augmented reality. First, fundamental technologies are briefly explained, which
have been proposed to overcome the technical limitations of ordinary
projectors. Second, augmented reality (AR) applications using projection
mapping technologies are introduced."
"Surface reconstruction from an unorganized point cloud is an important
problem due to its widespread applications. White noise, possibly clustered
outliers, and noisy perturbation may be generated when a point cloud is sampled
from a surface. Most existing methods handle limited amount of noise. We
develop a method to denoise a point cloud so that the users can run their
surface reconstruction codes or perform other analyses afterwards. Our
experiments demonstrate that our method is computationally efficient and it has
significantly better noise handling ability than several existing surface
reconstruction codes."
"We study Markov Chain Monte Carlo (MCMC) methods operating in primary sample
space and their interactions with multiple sampling techniques. We observe that
incorporating the sampling technique into the state of the Markov Chain, as
done in Multiplexed Metropolis Light Transport (MMLT), impedes the ability of
the chain to properly explore the path space, as transitions between sampling
techniques lead to disruptive alterations of path samples. To address this
issue, we reformulate Multiplexed MLT in the Reversible Jump MCMC framework
(RJMCMC) and introduce inverse sampling techniques that turn light paths into
the random numbers that would produce them. This allows us to formulate a novel
perturbation that can locally transition between sampling techniques without
changing the geometry of the path, and we derive the correct acceptance
probability using RJMCMC. We investigate how to generalize this concept to
non-invertible sampling techniques commonly found in practice, and introduce
probabilistic inverses that extend our perturbation to cover most sampling
methods found in light transport simulations. Our theory reconciles the
inverses with RJMCMC yielding an unbiased algorithm, which we call Reversible
Jump MLT (RJMLT). We verify the correctness of our implementation in canonical
and practical scenarios and demonstrate improved temporal coherence, decrease
in structured artifacts, and faster convergence on a wide variety of scenes."
"A conformal flattening maps a curved surface to the plane without distorting
angles---such maps have become a fundamental building block for problems in
geometry processing, numerical simulation, and computational design. Yet
existing methods provide little direct control over the shape of the flattened
domain, or else demand expensive nonlinear optimization. Boundary first
flattening (BFF) is a linear method for conformal parameterization which is
faster than traditional linear methods, yet provides control and quality
comparable to sophisticated nonlinear schemes. The key insight is that the
boundary data for many conformal mapping problems can be efficiently
constructed via the Cherrier formula together with a pair of Poincare-Steklov
operators; once the boundary is known, the map can be easily extended over the
rest of the domain. Since computation demands only a single factorization of
the real Laplace matrix, the amortized cost is about 50x less than any
previously published technique for boundary-controlled conformal flattening. As
a result, BFF opens the door to real-time editing or fast optimization of
high-resolution maps, with direct control over boundary length or angle. We
show how this method can be used to construct maps with sharp corners, cone
singularities, minimal area distortion, and uniformization over the unit disk;
we also demonstrate for the first time how a surface can be conformally
flattened directly onto any given target shape."
"To watch 360{\deg} videos on normal 2D displays, we need to project the
selected part of the 360{\deg} image onto the 2D display plane. In this paper,
we propose a fully-automated framework for generating content-aware 2D
normal-view perspective videos from 360{\deg} videos. Especially, we focus on
the projection step preserving important image contents and reducing image
distortion. Basically, our projection method is based on Pannini projection
model. At first, the salient contents such as linear structures and salient
regions in the image are preserved by optimizing the single Panini projection
model. Then, the multiple Panini projection models at salient regions are
interpolated to suppress image distortion globally. Finally, the temporal
consistency for image projection is enforced for producing temporally stable
normal-view videos. Our proposed projection method does not require any
user-interaction and is much faster than previous content-preserving methods.
It can be applied to not only images but also videos taking the temporal
consistency of projection into account. Experiments on various 360{\deg} videos
show the superiority of the proposed projection method quantitatively and
qualitatively."
"While ray tracing has become increasingly common and path tracing is well
understood by now, a major challenge lies in crafting an easy-to-use and
efficient system implementing these technologies. Following a purely
physically-based paradigm while still allowing for artistic workflows, the Iray
light transport simulation and rendering system allows for rendering complex
scenes by the push of a button and thus makes accurate light transport
simulation widely available. In this document we discuss the challenges and
implementation choices that follow from our primary design decisions,
demonstrating that such a rendering system can be made a practical, scalable,
and efficient real-world application that has been adopted by various companies
across many fields and is in use by many industry professionals today."
"We propose a method for converting geometric shapes into hierarchically
segmented parts with part labels. Our key idea is to train category-specific
models from the scene graphs and part names that accompany 3D shapes in public
repositories. These freely-available annotations represent an enormous,
untapped source of information on geometry. However, because the models and
corresponding scene graphs are created by a wide range of modelers with
different levels of expertise, modeling tools, and objectives, these models
have very inconsistent segmentations and hierarchies with sparse and noisy
textual tags. Our method involves two analysis steps. First, we perform a joint
optimization to simultaneously cluster and label parts in the database while
also inferring a canonical tag dictionary and part hierarchy. We then use this
labeled data to train a method for hierarchical segmentation and labeling of
new 3D shapes. We demonstrate that our method can mine complex information,
detecting hierarchies in man-made objects and their constituent parts,
obtaining finer scale details than existing alternatives. We also show that, by
performing domain transfer using a few supervised examples, our technique
outperforms fully-supervised techniques that require hundreds of
manually-labeled models."
"Solving the global method of Weighted Least Squares (WLS) model in image
filtering is both time- and memory-consuming. In this paper, we present an
alternative approximation in a time- and memory- efficient manner which is
denoted as Semi-Global Weighed Least Squares (SG-WLS). Instead of solving a
large linear system, we propose to iteratively solve a sequence of subsystems
which are one-dimensional WLS models. Although each subsystem is
one-dimensional, it can take two-dimensional neighborhood information into
account due to the proposed special neighborhood construction. We show such a
desirable property makes our SG-WLS achieve close performance to the original
two-dimensional WLS model but with much less time and memory cost. While
previous related methods mainly focus on the 4-connected/8-connected
neighborhood system, our SG-WLS can handle a more general and larger
neighborhood system thanks to the proposed fast solution. We show such a
generalization can achieve better performance than the 4-connected/8-connected
neighborhood system in some applications. Our SG-WLS is $\sim20$ times faster
than the WLS model. For an image of $M\times N$, the memory cost of SG-WLS is
at most at the magnitude of $max\{\frac{1}{M}, \frac{1}{N}\}$ of that of the
WLS model. We show the effectiveness and efficiency of our SG-WLS in a range of
applications."
"An algorithm for the computation of global discrete conformal
parametrizations with prescribed global holonomy signatures for triangle meshes
was recently described in [Campen and Zorin 2017]. In this paper we provide a
detailed analysis of convergence and correctness of this algorithm. We
generalize and extend ideas of [Springborn et al. 2008] to show a connection of
the algorithm to Newton's algorithm applied to solving the system of
constraints on angles in the parametric domain, and demonstrate that this
system can be obtained as a gradient of a convex energy."
"Due to the wide diffusion of 3D printing technologies, geometric algorithms
for Additive Manufacturing are being invented at an impressive speed. Each
single step, in particular along the Process Planning pipeline, can now count
on dozens of methods that prepare the 3D model for fabrication, while analysing
and optimizing geometry and machine instructions for various objectives. This
report provides a classification of this huge state of the art, and elicits the
relation between each single algorithm and a list of desirable objectives
during Process Planning. The objectives themselves are listed and discussed,
along with possible needs for tradeoffs. Additive Manufacturing technologies
are broadly categorized to explicitly relate classes of devices and supported
features. Finally, this report offers an analysis of the state of the art while
discussing open and challenging problems from both an academic and an
industrial perspective."
"Scalar features in time-dependent fluid flow are traditionally visualized
using 3D representation, and their topology changes over time are often
conveyed with abstract graphs. Using such techniques, however, the structural
details of feature separation and the temporal evolution of features undergoing
topological changes are difficult to analyze. In this paper, we propose a novel
approach for the spatio-temporal visualization of feature separation that
segments feature volumes into regions with respect to their contribution to
distinct features after separation. To this end, we employ particle-based
feature tracking to find volumetric correspondences between features at two
different instants of time. We visualize this segmentation by constructing mesh
boundaries around each volume segment of a feature at the initial time that
correspond to the separated features at the later time. To convey temporal
evolution of the partitioning within the investigated time interval, we
complement our approach with spatio-temporal separation surfaces. For the
application of our approach to multiphase flow, we additionally present a
feature-based corrector method to ensure phase-consistent particle
trajectories. The utility of our technique is demonstrated by application to
two-phase (liquid-gas) and multi-component (liquid-liquid) flows where the
scalar field represents the fraction of one of the phases."
"Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL."
"The surface of metal, glass and plastic objects is often characterized by
microscopic scratches caused by manufacturing and/or wear. A closer look onto
such scratches reveals iridescent colors with a complex dependency on viewing
and lighting conditions. The physics behind this phenomenon is well understood;
it is caused by diffraction of the incident light by surface features on the
order of the optical wavelength. Existing analytic models are able to reproduce
spatially unresolved microstructure such as the iridescent appearance of
compact disks and similar materials. Spatially resolved scratches, on the other
hand, have proven elusive due to the highly complex wave-optical light
transport simulations needed to account for their appearance. In this paper, we
propose a wave-optical shading model based on non-paraxial scalar diffraction
theory to render this class of effects. Our model expresses surface roughness
as a collection of line segments. To shade a point on the surface, the
individual diffraction patterns for contributing scratch segments are computed
analytically and superimposed coherently. This provides natural transitions
from localized glint-like iridescence to smooth BRDFs representing the
superposition of many reflections at large viewing distances. We demonstrate
that our model is capable of recreating the overall appearance as well as
characteristic detail effects observed on real-world examples."
"Correspondence problems are often modelled as quadratic optimization problems
over permutations. Common scalable methods for approximating solutions of these
NP-hard problems are the spectral relaxation for non-convex energies and the
doubly stochastic (DS) relaxation for convex energies. Lately, it has been
demonstrated that semidefinite programming relaxations can have considerably
improved accuracy at the price of a much higher computational cost. We present
a convex quadratic programming relaxation which is provably stronger than both
DS and spectral relaxations, with the same scalability as the DS relaxation.
The derivation of the relaxation also naturally suggests a projection method
for achieving meaningful integer solutions which improves upon the standard
closest-permutation projection. Our method can be easily extended to
optimization over doubly stochastic matrices, partial or injective matching,
and problems with additional linear constraints. We employ recent advances in
optimization of linear-assignment type problems to achieve an efficient
algorithm for solving the convex relaxation.
  We present experiments indicating that our method is more accurate than local
minimization or competing relaxations for non-convex problems. We successfully
apply our algorithm to shape matching and to the problem of ordering images in
a grid, obtaining results which compare favorably with state of the art
methods. We believe our results indicate that our method should be considered
the method of choice for quadratic optimization over permutations."
"Spectral shape descriptors have been used extensively in a broad spectrum of
geometry processing applications ranging from shape retrieval and segmentation
to classification. In this pa- per, we propose a spectral graph wavelet
approach for 3D shape classification using the bag-of-features paradigm. In an
effort to capture both the local and global geometry of a 3D shape, we present
a three-step feature description framework. First, local descriptors are
extracted via the spectral graph wavelet transform having the Mexican hat
wavelet as a generating ker- nel. Second, mid-level features are obtained by
embedding lo- cal descriptors into the visual vocabulary space using the soft-
assignment coding step of the bag-of-features model. Third, a global descriptor
is constructed by aggregating mid-level fea- tures weighted by a geodesic
exponential kernel, resulting in a matrix representation that describes the
frequency of appearance of nearby codewords in the vocabulary. Experimental
results on two standard 3D shape benchmarks demonstrate the effective- ness of
the proposed classification approach in comparison with state-of-the-art
methods."
"3D mesh segmentation is an important research area in computer graphics, and
there is an increasing interest in applying deep learning to this challenging
area. We observe that 1) existing techniques are either slow to train or
sensitive to feature resizing and sampling, 2) in the literature there are
minimal comparative studies and 3) techniques often suffer from reproducibility
issue. These hinder the research development of supervised segmentation tasks.
This study contributes in two ways. First, we propose a novel convolutional
neural network technique for mesh segmentation, using 1D data and filters, and
a multi-branch network for separate training of features of three different
scales. We also propose a novel way of computing conformal factor, which is
less sensitive to small areas of large curvatures, and improve graph-cut
refinement with the addition of a geometric feature term. The technique gives
better results than the state of the art. Secondly, we provide a comprehensive
study and implementations of several deep learning techniques, namely, neural
networks (NNs), autoencoders (AEs) and convolutional neural networks (CNNs),
which use an architecture of at least two layers deep. The significance of the
study is that it offers a novel fast and accurate CNN technique, and a
comparison of several other deep learning techniques for comparison."
"This paper deals with a kind of design of a ruled surface. It combines
concepts from the fields of computer aided geometric design and kinematics. A
dual unit spherical B\'ezier-like curve on the dual unit sphere (DUS) is
obtained with respect the control points by a new method. So, with the aid of
Study [1] transference principle, a dual unit spherical B\'ezier-like curve
corresponds to a ruled surface. Furthermore, closed ruled surfaces are
determined via control points and integral invariants of these surfaces are
investigated. The results are illustrated by examples."
"This paper introduces a general method to approximate the convolution of an
arbitrary program with a Gaussian kernel. This process has the effect of
smoothing out a program. Our compiler framework models intermediate values in
the program as random variables, by using mean and variance statistics. Our
approach breaks the input program into parts and relates the statistics of the
different parts, under the smoothing process. We give several approximations
that can be used for the different parts of the program. These include the
approximation of Dorn et al., a novel adaptive Gaussian approximation, Monte
Carlo sampling, and compactly supported kernels. Our adaptive Gaussian
approximation is accurate up to the second order in the standard deviation of
the smoothing kernel, and mathematically smooth. We show how to construct a
compiler that applies chosen approximations to given parts of the input
program. Because each expression can have multiple approximation choices, we
use a genetic search to automatically select the best approximations. We apply
this framework to the problem of automatically bandlimiting procedural shader
programs. We evaluate our method on a variety of complex shaders, including
shaders with parallax mapping, animation, and spatially varying statistics. The
resulting smoothed shader programs outperform previous approaches both
numerically, and aesthetically, due to the smoothing properties of our
approximations."
"QuickCSG computes the result for general N-polyhedron boolean expressions
without an intermediate tree of solids. We propose a vertex-centric view of the
problem, which simplifies the identification of final geometric contributions,
and facilitates its spatial decomposition. The problem is then cast in a single
KD-tree exploration, geared toward the result by early pruning of any region of
space not contributing to the final surface. We assume strong regularity
properties on the input meshes and that they are in general position. This
simplifying assumption, in combination with our vertex-centric approach,
improves the speed of the approach. Complemented with a task-stealing
parallelization, the algorithm achieves breakthrough performance, one to two
orders of magnitude speedups with respect to state-of-the-art CPU algorithms,
on boolean operations over two to dozens of polyhedra. The algorithm also
outperforms GPU implementations with approximate discretizations, while
producing an output without redundant facets. Despite the restrictive
assumptions on the input, we show the usefulness of QuickCSG for applications
with large CSG problems and strong temporal constraints, e.g. modeling for 3D
printers, reconstruction from visual hulls and collision detection."
"We present a novel extension of the path tracing algorithm that is capable of
treating highly scattering participating media in the presence of fluorescent
structures. The extension is based on the formulation of the full radiative
transfer equation when solved on a per-wavelength-basis, resulting in accurate
model and unbiased algorithm for rendering highly scattering fluorescent
participating media. The model accounts for the intrinsic properties of
fluorescent dyes including their absorption and emission spectra, molar
absorptivity and quantum yield and also their concentration. Our algorithm is
applied to render highly scattering isotropic fluorescent solutions under
different illumination conditions. The spectral performance of the model is
validated against emission spectra of different fluorescent dyes that are of
significance in spectroscopy."
"The game and movie industries always face the challenge of reproducing
materials. This problem is tackled by combining illumination models and various
textures (painted or procedural patterns). Gnerating stochastic wall patterns
is crucial in the creation of a wide range of backgrounds (castles, temples,
ruins...). A specific Wang tile set was introduced previously to tackle this
problem, in a non-procedural fashion. Long lines may appear as visual
artifacts. We use this tile set in a new procedural algorithm to generate
stochastic wall patterns. For this purpose, we introduce specific hash
functions implementing a constrained Wang tiling. This technique makes possible
the generation of boundless textures while giving control over the maximum line
length. The algorithm is simple and easy to implement, and the wall structure
we get from the tiles allows to achieve visuals that reproduce all the small
details of artist painted walls."
"We present a web application for the procedural generation of perturbations
of 3D models. We generate the perturbations by generating vertex shaders that
change the positions of vertices that make up the 3D model. The vertex shaders
are created with an interactive genetic algorithm, which displays to the user
the visual effect caused by each vertex shader, allows the user to select the
visual effect the user likes best, and produces a new generation of vertex
shaders using the user feedback as the fitness measure of the genetic
algorithm. We use genetic programming to represent each vertex shader as a
computer program. This paper presents details of requirements specification,
software architecture, high and low-level design, and prototype user interface.
We discuss the project's current status and development challenges."
"NURBS curve is widely used in Computer Aided Design and Computer Aided
Geometric Design. When a single weight approaches infinity, the limit of a
NURBS curve tends to the corresponding control point. In this paper, a kind of
control structure of a NURBS curve, called regular control curve, is defined.
We prove that the limit of the NURBS curve is exactly its regular control curve
when all of weights approach infinity, where each weight is multiplied by a
certain one-parameter function tending to infinity, different for each control
point. Moreover, some representative examples are presented to show this
property and indicate its application for shape deformation."
"Wayfinding signs play an important role in guiding users to navigate in a
virtual environment and in helping pedestrians to find their ways in a
real-world architectural site. Conventionally, the wayfinding design of a
virtual environment is created manually, so as the wayfinding design of a
real-world architectural site. The many possible navigation scenarios, as well
as the interplay between signs and human navigation, can make the manual design
process overwhelming and non-trivial. As a result, creating a wayfinding design
for a typical layout can take months to several years. In this paper, we
introduce the Way to Go! approach for automatically generating a wayfinding
design for a given layout. The designer simply has to specify some navigation
scenarios; our approach will automatically generate an optimized wayfinding
design with signs properly placed considering human agents' visibility and
possibility of making mistakes during a navigation. We demonstrate the
effectiveness of our approach in generating wayfinding designs for different
layouts such as a train station, a downtown and a canyon. We evaluate our
results by comparing different wayfinding designs and show that our optimized
wayfinding design can guide pedestrians to their destinations effectively and
efficiently. Our approach can also help the designer visualize the
accessibility of a destination from different locations, and correct any ""blind
zone"" with additional signs."
"We live in a 3D world, performing activities and interacting with objects in
the indoor environments everyday. Indoor scenes are the most familiar and
essential environments in everyone's life. In the virtual world, 3D indoor
scenes are also ubiquitous in 3D games and interior design. With the fast
development of VR/AR devices and the emerging applications, the demand of
realistic 3D indoor scenes keeps growing rapidly. Currently, designing detailed
3D indoor scenes requires proficient 3D designing and modeling skills and is
often time-consuming. For novice users, creating realistic and complex 3D
indoor scenes is even more difficult and challenging.
  Many efforts have been made in different research communities, e.g. computer
graphics, vision and robotics, to capture, analyze and generate the 3D indoor
data. This report mainly focuses on the recent research progress in graphics on
geometry, structure and semantic analysis of 3D indoor data and different
modeling techniques for creating plausible and realistic indoor scenes. We
first review works on understanding and semantic modeling of scenes from
captured 3D data of the real world. Then, we focus on the virtual scenes
composed of 3D CAD models and study methods for 3D scene analysis and
processing. After that, we survey various modeling paradigms for creating 3D
indoor scenes and investigate human-centric scene analysis and modeling, which
bridge indoor scene studies of graphics, vision and robotics. At last, we
discuss open problems in indoor scene processing that might bring interests to
graphics and all related communities."
"Colorization of gray-scale images relies on prior color information.
Examplar-based methods use a color image as source of such information. Then
the colors of the source image are transferred to the gray-scale image. In the
literature, this transfer is mainly guided by texture descriptors. Face images
usually contain few texture so that the common approaches frequently fail. In
this paper we propose a new method based on image morphing. This technique is
able to compute a correspondence map between images with similar shapes. It is
based on the geometric structure of the images rather than textures which is
more reliable for faces. Our numerical experiments show that our morphing based
approach clearly outperforms state-of-the-art methods."
"We present a novel approach enabling interactive visualization of volumetric
Locally Refined B-splines (LR-splines). To this end we propose a highly
efficient algorithm for direct visualization of scalar and vector fields given
by an LR-spline. For the case of scalar fields a volume rendering approach is
designed, along with methods for the necessary adaptive sampling distance,
on-the-fly trimming with a surface geometry given by a STereoLithography
(STL)-file, and local volume illumination based on on-the-fly evaluation of the
derivative of the underlying LR-spline function. For vector fields we design a
two-stage algorithm consisting of the computation of stream lines with adaptive
step size and their rendering with tubes. In both cases, the common basic
ingredient to achieve interactive frame rates is an acceleration structure
based on a k-d forest together with suitable data structures. The algorithms
are designed to fully utilize modern graphics processing unit (GPU)
capabilities. Important applications where LR-spline volumes emerge are given
for instance by approximation of large-scale simulation and sensor data, and
Isogeometric Analysis (IGA). For the first case -- approximation of large three
dimensional point clouds with an associated field -- we provide an extension of
the multilevel B-spline approximation (MBA) algorithm to the case of volumetric
LR-splines. We showcase interactive rendering achieved by our approach on
different representative use cases, stemming from simulations of wind flow
around a telescope, Magnetic Resonance (MR) imaging of a human brain, and
simulations of a fluidized bed used for mixing and coating particles in
industrial processes."
"Human motions (especially dance motions) are very noisy, and it is hard to
analyze and edit the motions. To resolve this problem, we propose a new method
to decompose and modify the motions using the Hilbert-Huang transform (HHT).
First, HHT decomposes a chromatic signal into ""monochromatic"" signals that are
the so-called Intrinsic Mode Functions (IMFs) using an Empirical Mode
Decomposition (EMD) [6]. After applying the Hilbert Transform to each IMF, the
instantaneous frequencies of the ""monochromatic"" signals can be obtained. The
HHT has the advantage to analyze non-stationary and nonlinear signals such as
human-joint-motions over FFT or Wavelet transform.
  In the present paper, we propose a new framework to analyze and extract some
new features from a famous Japanese threesome pop singer group called
""Perfume"", and compare it with Waltz and Salsa dance. Using the EMD, their
dance motions can be decomposed into motion (choreographic) primitives or IMFs.
Therefore we can scale, combine, subtract, exchange, and modify those IMFs, and
can blend them into new dance motions self-consistently. Our analysis and
framework can lead to a motion editing and blending method to create a new
dance motion from different dance motions."
"The use of Laplacian eigenfunctions is ubiquitous in a wide range of computer
graphics and geometry processing applications. In particular, Laplacian
eigenbases allow generalizing the classical Fourier analysis to manifolds. A
key drawback of such bases is their inherently global nature, as the Laplacian
eigenfunctions carry geometric and topological structure of the entire
manifold. In this paper, we introduce a new framework for local spectral shape
analysis. We show how to efficiently construct localized orthogonal bases by
solving an optimization problem that in turn can be posed as the
eigendecomposition of a new operator obtained by a modification of the standard
Laplacian. We study the theoretical and computational aspects of the proposed
framework and showcase our new construction on the classical problems of shape
approximation and correspondence. We obtain significant improvement compared to
classical Laplacian eigenbases as well as other alternatives for constructing
localized bases."
"In geometry processing, smoothness energies are commonly used to model
scattered data interpolation, dense data denoising, and regularization during
shape optimization. The squared Laplacian energy is a popular choice of energy
and has a corresponding standard implementation: squaring the discrete
Laplacian matrix. For compact domains, when values along the boundary are not
known in advance, this construction bakes in low-order boundary conditions.
This causes the geometric shape of the boundary to strongly bias the solution.
For many applications, this is undesirable. Instead, we propose using the
squared Frobenious norm of the Hessian as a smoothness energy. Unlike the
squared Laplacian energy, this energy's natural boundary conditions (those that
best minimize the energy) correspond to meaningful high-order boundary
conditions. These boundary conditions model free boundaries where the shape of
the boundary should not bias the solution locally. Our analysis begins in the
smooth setting and concludes with discretizations using finite-differences on
2D grids or mixed finite elements for triangle meshes. We demonstrate the core
behavior of the squared Hessian as a smoothness energy for various tasks."
"Integration of scalar and vector visualization has been an interesting topic.
This paper presents a technique to appropriately select and display multiple
streamlines while overlaying with isosurfaces, aiming an integrated scalar and
vector field visualization. The technique visualizes a scalar field by multiple
semitransparent isosurfaces, and a vector field by multiple streamlines, while
the technique adequately selects the streamlines considering reduction of
cluttering among the isosurfaces and streamlines. The technique first selects
and renders isosurfaces, and then generates large number of streamlines from
randomly selected seed points. The technique evaluates each of the streamlines
according to their shapes on a 2D display space, distances to critical points
of the given vector fields, and occlusion by isosurfaces. It then selects the
specified number of highly evaluated streamlines. As a result, we can visualize
both scalar and vector fields as a set of view-independently selected
isosurfaces and view-dependently selected streamlines."
"BRDF of most real world materials has two components, the surface BRDF due to
the light reflecting at the surface of the material and the subsurface BRDF due
to the light entering and going through many scattering events inside the
material. Each of these events modifies light's path, power, polarization
state. Computing polarized subsurface BRDF of a material requires simulating
the light transport inside the material. The transport of polarized light is
modeled by the Vector Radiative Transfer Equation (VRTE), an
integro-differential equation. Computing solution to that equation is
expensive. The Discrete Ordinate Method (DOM) is a common approach to solving
the VRTE. Such solvers are very time consuming for complex uses such as BRDF
computation, where one must solve VRTE for surface radiance distribution due to
light incident from every direction of the hemisphere above the surface. In
this paper, we present a GPU based DOM solution of the VRTE to expedite the
subsurface BRDF computation. As in other DOM based solutions, our solution is
based on Fourier expansions of the phase function and the radiance function.
This allows us to independently solve the VRTE for each order of expansion. We
take advantage of those repetitions and of the repetitions in each of the
sub-steps of the solution process. Our solver is implemented to run mainly on
graphics hardware using the OpenCL library and runs up to seven times faster
than its CPU equivalent, allowing the computation of subsurface BRDF in a
matter of minutes. We compute and present the subsurface BRDF lobes due to
powders and paints of a few materials. We also show the rendering of objects
with the computed BRDF. The solver is available for public use through the
authors' web site."
"We propose Steklov geometry processing, an extrinsic approach to spectral
geometry processing and shape analysis. Intrinsic approaches, usually based on
the Laplace-Beltrami operator, cannot capture the spatial embedding of a shape
up to rigid motion, while many previous extrinsic methods lack theoretical
justification. Instead, we propose a systematic approach by considering the
Steklov eigenvalue problem, computing the spectrum of the Dirichlet-to-Neumann
operator of a surface bounding a volume. A remarkable property of this operator
is that it encodes the volumetric geometry. We use the boundary element method
(BEM) to discretize the operator, accelerated by hierarchical numerical schemes
and preconditioning; this pipeline allows us to solve eigenvalue and linear
problems on large-scale meshes despite the density of the Dirichlet-to-Neumann
discretization. We further demonstrate that our operators naturally fit into
existing frameworks for geometry processing, making a shift from intrinsic to
extrinsic geometry as simple as substituting the Laplace-Beltrami operator with
the Dirichlet-to-Neumann operator."
"The colorful appearance of a physical painting is determined by the
distribution of paint pigments across the canvas, which we model as a per-pixel
mixture of a small number of pigments with multispectral absorption and
scattering coefficients. We present an algorithm to efficiently recover this
structure from an RGB image, yielding a plausible set of pigments and a low RGB
reconstruction error. We show that under certain circumstances we are able to
recover pigments that are close to ground truth, while in all cases our results
are always plausible. Using our decomposition, we repose standard digital image
editing operations as operations in pigment space rather than RGB, with
interestingly novel results. We demonstrate tonal adjustments, selection
masking, cut-copy-paste, recoloring, palette summarization, and edge
enhancement."
"We present a discrete theory for modeling developable surfaces as
quadrilateral meshes satisfying simple angle constraints. The basis of our
model is a lesser known characterization of developable surfaces as manifolds
that can be parameterized through orthogonal geodesics. Our model is simple,
local, and, unlike previous works, it does not directly encode the surface
rulings. This allows us to model continuous deformations of discrete
developable surfaces independently of their decomposition into torsal and
planar patches or the surface topology. We prove and experimentally demonstrate
strong ties to smooth developable surfaces, including a theorem stating that
every sampling of the smooth counterpart satisfies our constraints up to second
order. We further present an extension of our model that enables a local
definition of discrete isometry. We demonstrate the effectiveness of our
discrete model in a developable surface editing system, as well as computation
of an isometric interpolation between isometric discrete developable shapes."
"How to establish the matching (or corresponding) between two different 3D
shapes is a classical problem. This paper focused on the research on shape
mapping of 3D mesh models, and proposed a shape mapping algorithm based on
Hidden Markov Random Field and EM algorithm, as introducing a hidden state
random variable associated with the adjacent blocks of shape matching when
establishing HMRF. This algorithm provides a new theory and method to ensure
the consistency of the edge data of adjacent blocks, and the experimental
results show that the algorithm in this paper has a great improvement on the
shape mapping of 3D mesh models."
"We introduce a continuous global optimization method to the field of surface
reconstruction from discrete noisy cloud of points with weak information on
orientation. The proposed method uses an energy functional combining flux-based
data-fit measures and a regularization term. A continuous convex relaxation
scheme assures the global minima of the geometric surface functional. The
reconstructed surface is implicitly represented by the binary segmentation of
vertices of a 3D uniform grid and a triangulated surface can be obtained by
extracting an appropriate isosurface. Unlike the discrete graph-cut solution,
the continuous global optimization entails advantages like memory requirements,
reduction of metrication errors for geometric quantities, allowing globally
optimal surface reconstruction at higher grid resolutions. We demonstrate the
performance of the proposed method on several oriented point clouds captured by
laser scanners. Experimental results confirm that our approach is robust to
noise, large holes and non-uniform sampling density under the condition of very
coarse orientation information."
"The visual representation of concepts or ideas through the use of simple
shapes has always been explored in the history of Humanity, and it is believed
to be the origin of writing. We focus on computational generation of visual
symbols to represent concepts. We aim to develop a system that uses background
knowledge about the world to find connections among concepts, with the goal of
generating symbols for a given concept. We are also interested in exploring the
system as an approach to visual dissociation and visual conceptual blending.
This has a great potential in the area of Graphic Design as a tool to both
stimulate creativity and aid in brainstorming in projects such as logo,
pictogram or signage design."
"Inspired by kernel methods that have been used extensively in achieving
efficient facial animation retargeting, this paper presents a solution to
retargeting facial animation in virtual character's face model based on the
kernel projection of latent structure (KPLS) regression between semantically
similar facial expressions. Specifically, a given number of corresponding
semantically similar facial expressions are projected into the latent space. By
using the Nonlinear Iterative Partial Least Square method, decomposition of the
latent variables is achieved. Finally, the KPLS is achieved by solving a
kernalized version of the eigenvalue problem. By evaluating our methodology
with other kernel-based solutions, the efficiency of the presented methodology
in transferring facial animation to face models with different morphological
variations is demonstrated."
"New generations of neutron scattering sources and instrumentation are
providing challenges in data handling for user software. Time-of-Flight
instruments used at pulsed sources typically produce hundreds or thousands of
channels of data for each detector segment. New instruments are being designed
with thousands to hundreds of thousands of detector segments. High intensity
neutron sources make possible parametric studies and texture studies which
further increase data handling requirements. The Integrated Spectral Analysis
Workbench (ISAW) software developed at Argonne handles large numbers of spectra
simultaneously while providing operations to reduce, sort, combine and export
the data. It includes viewers to inspect the data in detail in real time. ISAW
uses existing software components and packages where feasible and takes
advantage of the excellent support for user interface design and network
communication in Java. The included scripting language simplifies repetitive
operations for analyzing many files related to a given experiment. Recent
additions to ISAW include a contour view, a time-slice table view, routines for
finding and fitting peaks in data, and support for data from other facilities
using the NeXus format. In this paper, I give an overview of features and
planned improvements of ISAW. Details of some of the improvements are covered
in other presentations at this conference."
"This paper solves the problem of computing conformal structures of general
2-manifolds represented as triangle meshes. We compute conformal structures in
the following way: first compute homology bases from simplicial complex
structures, then construct dual cohomology bases and diffuse them to harmonic
1-forms. Next, we construct bases of holomorphic differentials. We then obtain
period matrices by integrating holomorphic differentials along homology bases.
We also study the global conformal mapping between genus zero surfaces and
spheres, and between general meshes and planes. Our method of computing
conformal structures can be applied to tackle fundamental problems in computer
aid design and computer graphics, such as geometry classification and
identification, and surface global parametrization."
"Twenty-five years ago, Crow published the shadow volume approach for
determining shadowed regions in a scene. A decade ago, Heidmann described a
hardware-accelerated stencil buffer-based shadow volume algorithm.
  Unfortunately hardware-accelerated stenciled shadow volume techniques have
not been widely adopted by 3D games and applications due in large part to the
lack of robustness of described techniques. This situation persists despite
widely available hardware support. Specifically what has been lacking is a
technique that robustly handles various ""hard"" situations created by near or
far plane clipping of shadow volumes.
  We describe a robust, artifact-free technique for hardware-accelerated
rendering of stenciled shadow volumes. Assuming existing hardware, we resolve
the issues otherwise caused by shadow volume near and far plane clipping
through a combination of (1) placing the conventional far clip plane ""at
infinity"", (2) rasterization with infinite shadow volume polygons via
homogeneous coordinates, and (3) adopting a zfail stencil-testing scheme. Depth
clamping, a new rasterization feature provided by NVIDIA's GeForce3, preserves
existing depth precision by not requiring the far plane to be placed at
infinity. We also propose two-sided stencil testing to improve the efficiency
of rendering stenciled shadow volumes."
Cg is a language for programming GPUs. This paper describes Cg briefly.
"Massive data sets have radically changed our understanding of how to design
efficient algorithms; the streaming paradigm, whether it in terms of number of
passes of an external memory algorithm, or the single pass and limited memory
of a stream algorithm, appears to be the dominant method for coping with large
data.
  A very different kind of massive computation has had the same effect at the
level of the CPU. The most prominent example is that of the computations
performed by a graphics card. The operations themselves are very simple, and
require very little memory, but require the ability to perform many
computations extremely fast and in parallel to whatever degree possible. What
has resulted is a stream processor that is highly optimized for stream
computations. An intriguing side effect of this is the growing use of a
graphics card as a general purpose stream processing engine. In an
ever-increasing array of applications, researchers are discovering that
performing a computation on a graphics card is far faster than performing it on
a CPU, and so are using a GPU as a stream co-processor."
"The latest Graphics Processing Units (GPUs) are reported to reach up to
  200 billion floating point operations per second (200 Gflops) and to have
price performance of 0.1 cents per M flop. These facts raise great interest in
the plausibility of extending the GPUs' use to non-graphics applications, in
particular numerical simulations on structured grids (lattice).
  We review previous work on using GPUs for non-graphics applications,
implement probability-based simulations on the GPU, namely the
  Ising and percolation models, implement vector operation benchmarks for the
GPU, and finally compare the CPU's and GPU's performance.
  A general conclusion from the results obtained is that moving computations
from the CPU to the GPU is feasible, yielding good time and price performance,
for certain lattice computations.
  Preliminary results also show that it is feasible to use them in parallel"
"A procedure for interpolating between specified points of a curve or surface
is described. The method guarantees slope continuity at all junctions. A
surface panel divided into p x q contiguous patches is completely specified by
the coordinates of (p+1) x (q+1) points. Each individual patch, however,
depends parametrically on the coordinates of 16 points, allowing shape
flexibility and global conformity."
"In this paper, it is presented a methodology to estimate the deformation
involved between two objects attending to its physical properties. This
methodology can be used, for example, in Computational Vision or Computer
Graphics applications, and consists in physically modeling the objects, by
means of the Finite Elements Method, establishing correspondences between some
of its data points, by using Modal Matching, and finally, determining the
displacement field, that is the intermediate shapes, through the resolution of
the Lagrange Dynamic Equilibrium Equation. As in many of the possible
applications of the methodology to present, it is necessary to quantify the
existing deformation, as well as to estimate only the non rigid component of
the involved global deformation. The solutions adopted to satisfy such
intentions will be also presented."
"We present a method to accelerate global illumination computation in dynamic
environments by taking advantage of limitations of the human visual system. A
model of visual attention is used to locate regions of interest in a scene and
to modulate spatiotemporal sensitivity. The method is applied in the form of a
spatiotemporal error tolerance map. Perceptual acceleration combined with good
sampling protocols provide a global illumination solution feasible for use in
animation. Results indicate an order of magnitude improvement in computational
speed. The method is adaptable and can also be used in image-based rendering,
geometry level of detail selection, realistic image synthesis, video telephony
and video compression."
"Modeling, simulation and visualization of three-dimension complex bodies
widely use mathematical model of curves and surfaces. The most important curves
and surfaces for these purposes are curves and surfaces in Hermite and Bezier
forms, splines and NURBS. Article is devoted to survey this way to use
geometrical data in various computer graphics systems and adjacent fields."
"We describe a variation of the iterative closest point (ICP) algorithm for
aligning two point sets under a set of transformations. Our algorithm is
superior to previous algorithms because (1) in determining the optimal
alignment, it identifies and discards likely outliers in a statistically robust
manner, and (2) it is guaranteed to converge to a locally optimal solution. To
this end, we formalize a new distance measure, fractional root mean squared
distance (frmsd), which incorporates the fraction of inliers into the distance
function. We lay out a specific implementation, but our framework can easily
incorporate most techniques and heuristics from modern registration algorithms.
We experimentally validate our algorithm against previous techniques on 2 and 3
dimensional data exposed to a variety of outlier types."
"Programs for complicated engineering and scientific tasks always have to deal
with a problem of showing numerous graphical results. The limits of the screen
space and often opposite requirements from different users are the cause of the
infinite discussions between designers and users, but the source of this
ongoing conflict is not in the level of interface design, but in the basic
principle of current graphical output: user may change some views and details,
but in general the output view is absolutely defined and fixed by the
developer. Author was working for several years on the algorithm that will
allow eliminating this problem thus allowing stepping from designer-driven
applications to user-driven. Such type of applications in which user is
deciding what, when and how to show on the screen, is the dream of scientists
and engineers working on the analysis of the most complicated tasks. The new
paradigm is based on movable and resizable graphics, and such type of graphics
can be widely used not only for scientific and engineering applications."
"Convergence properties of binary stationary subdivision schemes for curves
have been analyzed using the techniques of z-transforms and eigenanalysis.
Eigenanalysis provides a way to determine derivative continuity at specific
points based on the eigenvalues of a finite matrix. None of the well-known
subdivision schemes for curves have complex eigenvalues. We prove when a
convergent scheme with palindromic mask can have complex eigenvalues and that a
lower limit for the size of the mask exists in this case. We find a scheme with
complex eigenvalues achieving this lower bound. Furthermore we investigate this
scheme numerically and explain from a geometric viewpoint why such a scheme has
not yet been used in computer-aided geometric design."
"An efficient computer algorithm is described for the perspective drawing of a
wide class of surfaces. The class includes surfaces corresponding lo
single-valued, continuous functions which are defined over rectangular domains.
The algorithm automatically computes and eliminates hidden lines. The number of
computations in the algorithm grows linearly with the number of sample points
on the surface to be drawn. An analysis of the algorithm is presented, and
extensions lo certain multi-valued functions are indicated. The algorithm is
implemented and tested on .Net 2.0 platform that left interactive use. Running
times are found lo be exceedingly efficient for visualization, where
interaction on-line and view-point control, enables effective and rapid
examination of a surfaces from many perspectives."
"In this paper we demonstrate the use of intelligent optimization
methodologies on the visualization optimization of virtual / simulated
environments. The problem of automatic selection of an optimized set of views,
which better describes an on-going simulation over a virtual environment is
addressed in the context of the RoboCup Rescue Simulation domain. A generic
architecture for optimization is proposed and described. We outline the
possible extensions of this architecture and argue on how several problems
within the fields of Interactive Rendering and Visualization can benefit from
it."
"Despite the recent advances in graphics hardware capabilities, a brute force
approach is incapable of interactively displaying terabytes of data. We have
implemented a system that uses hierarchical level-of-detailing for the results
of cosmological simulations, in order to display visually accurate results
without loading in the full dataset (containing over 10 billion points). The
guiding principle of the program is that the user should not be able to
distinguish what they are seeing from a full rendering of the original data.
Furthermore, by using a tree-based system for levels of detail, the size of the
underlying data is limited only by the capacity of the IO system containing it."
"An algorithm to generate the locus of a circle using the intersection points
of straight lines is proposed. The pixels on the circle are plotted independent
of one another and the operations involved in finding the locus of the circle
from the intersection of straight lines are parallelizable. Integer only
arithmetic and algorithmic optimizations are used for speedup. The proposed
algorithm makes use of an envelope to form a parabolic arc which is consequent
transformed into a circle. The use of parabolic arcs for the transformation
results in higher pixel errors as the radius of the circle to be drawn
increases. At its current state, the algorithm presented may be suitable only
for generating circles for string art."
"The Good is Blondie, a wandering gunman with a strong personal sense of
honor. The Bad is Angel Eyes, a sadistic hitman who always hits his mark. The
Ugly is Tuco, a Mexican bandit who's always only looking out for himself.
Against the backdrop of the BOWS contest, they search for a watermark in gold
buried in three images. Each knows only a portion of the gold's exact location,
so for the moment they're dependent on each other. However, none are
particularly inclined to share..."
"Midpoint subdivision generalizes the Lane-Riesenfeld algorithm for uniform
tensor product splines and can also be applied to non regular meshes. For
example, midpoint subdivision of degree 2 is a specific Doo-Sabin algorithm and
midpoint subdivision of degree 3 is a specific Catmull-Clark algorithm. In
2001, Zorin and Schroeder were able to prove C1-continuity for midpoint
subdivision surfaces analytically up to degree 9. Here, we develop general
analysis tools to show that the limiting surfaces under midpoint subdivision of
any degree >= 2 are C1-continuous at their extraordinary points."
"In this paper we present closed-form solutions for efficiently updating the
principal components of a set of $n$ points, when $m$ points are added or
deleted from the point set. For both operations performed on a discrete point
set in $\mathbb{R}^d$, we can compute the new principal components in $O(m)$
time for fixed $d$. This is a significant improvement over the commonly used
approach of recomputing the principal components from scratch, which takes
$O(n+m)$ time. An important application of the above result is the dynamical
computation of bounding boxes based on principal component analysis. PCA
bounding boxes are very often used in many fields, among others in computer
graphics for collision detection and fast rendering. We have implemented and
evaluated few algorithms for computing dynamically PCA bounding boxes in
$\mathbb{R}^3$. In addition, we present closed-form solutions for computing
dynamically principal components of continuous point sets in $\mathbb{R}^2$ and
$\mathbb{R}^3$. In both cases, discrete and continuous, to compute the new
principal components, no additional data structures or storage are needed."
"This work expands further our earlier poster presentation and integration of
the OpenGL Slides Framework (OGLSF) - to make presentations with real-time
animated graphics where each slide is a scene with tidgets - and physical based
animation of elastic two-, three-layer softbody objects. The whole project is
very interactive, and serves dual purpose - delivering the teaching material in
a classroom setting with real running animated examples as well as releasing
the source code to the students to show how the actual working things are made."
"In this paper, we describe how we can precisely produce complex and various
dynamic morphological features such as structured and chaotic features which
occur in sand pilings (piles, avalanches, internal collapses, arches) , in
flowing fluids (laminar flowing, Kelvin-Helmholtz and Von Karmann eddies), and
in cohesive pastes (twist-and-turn oozing and packing) using only a single
unified model, called ""mesoscopic model"". This model is a physically-based
particle model whose behavior depends on only four simple, but easy to
understand, physically-based parameters : elasticity, viscosity and their local
areas of influence. It is fast to compute and easy to understand by
non-physicist users."
"This paper presents a modeling process in order to produce a realistic
simulation of crowds in the ancient Greek agora of Argos. This place was a
social theater in which two kinds of collective phenomena took place:
interpersonal interactions (small group discussion and negotiation, etc.) and
global collective phenomena, such as flowing and jamming. In this paper, we
focus on the second type of collective human phenomena, called non-deliberative
emergent crowd phenomena. This is a typical case of collective emergent
self-organization. When a great number of individuals move within a confined
environment and under a common fate, collective structures appear
spontaneously: jamming with inner collapses, organized flowing with queues,
curls, and vortices, propagation effects, etc. These are particularly relevant
features to enhance the realism - more precisely the ""truthfulness"" - of models
of this kind of collective phenomena. We assume that this truthfulness is
strongly associated with the concept of emergence: evolutions are not
predetermined by the individual characters, but emerge from the interaction of
numerous characters. The evolutions are not repetitive, and evolve on the basis
of small changes. This paper demonstrates that the physically-based interacting
particles system is an adequate candidate to model emergent crowd effects: it
associates a large number of elementary dynamic actors via elementary
non-linear dynamic interactions. Our model of the scene is regulated as a
large, dynamically coupled network of second order differential automata. We
take advantage of symbolic non-photorealistic and efficient visualization to
render the style of the person, rather than the person itself. As an artistic
representation, NPR reinforces the symbolic acceptance of the scene by the
observer, triggering an immediate and intuitive recognition of the scene as a
plausible scene from ancient Greece."
"This paper is focused on the question of simulation and visualiza- tion of 3D
gel and paste dynamic effects. In a first part, we introduce a 3D physically
based particle (or mass-interaction) model, with a small number of masses and
few powerful interaction parameters, which is able to generate the dynamic
features of both gels and pastes. This model proves that the 3D
mass-interaction method is relevant for the simulation of such phenomena,
without an explicit knowledge of their underly- ing physics. In a second part,
we expose an original rendering process, the Flow Structuring Method that
enhances the dynamic properties of the simulation and offers a realistic
visualization. This process ignores all the properties of the underlying
physical model. It leads to a reconstruction of the spatial structure of the
gel (or paste) flow only through an analysis of the output of the simula- tion
which is a set of unorganized points moving in a 3D space. Finally, the paper
presents realistic renderings obtained by using implicit surfaces and
ray-tracing techniques on the Structured Flow previously obtained."
"The control polygon of a Bezier curve is well-defined and has geometric
significance---there is a sequence of weights under which the limiting position
of the curve is the control polygon. For a Bezier surface patch, there are many
possible polyhedral control structures, and none are canonical. We propose a
not necessarily polyhedral control structure for surface patches, regular
control surfaces, which are certain C^0 spline surfaces. While not unique,
regular control surfaces are exactly the possible limiting positions of a
Bezier patch when the weights are allowed to vary."
"Understanding constellations in large data collections has become a common
task. One obstacle a user has to overcome is the internal complexity of these
repositories. For example, extracting connected data from a normalized
relational database requires knowledge of the table structure which might not
be available for the casual user. In this paper we present a visualization
framework which presents the collection as a set of entities and relations (on
the data level). Using rating functions, we divide large relation networks into
small graphs which resemble ego-centered networks. These graphs are connected
so the user can browse from one to another. To further assist the user, we
present two views which embed information on the evolution of the relations
into the graphs. Each view emphasizes another aspect of temporal development.
The framework can be adapted to any repository by a flexible data interface and
a graph configuration file. We present some first web-based applications
including a visualization of the DBLP data set. We use the DBLP visualization
to evaluate our approach."
"Most of the physically based techniques for rendering translucent objects use
the diffusion theory of light scattering in turbid media. The widely used
dipole diffusion model (Jensen et al. 2001) applies the diffusion-theory
formula derived for a planar interface to objects of arbitrary shapes. This
paper presents first results of our investigation of how surface curvature
affects the diffuse reflectance from translucent materials."
"We discuss a topic on the role of computer graphics in the production of
documentaries, which is often ignored in favor of other topics. Typically,
except for some rare occasions, documentary producers and computer scientists
or digital artists that do computer graphics are relatively far apart in their
domains and rarely intercommunicate to have a joint production; yet it happens,
and perhaps more so in the present and the future.
  We attempt to classify the documentaries on the amount and techniques of
computer graphics used for documentaries. We come up with the initial
categories such as ""plain"" (no graphics), ""in-between"", ""all-out"" -- nearly
100% of the documentary consisting of computer-generated imagery. Computer
graphics can be used to enhance the scenery, fill in the gaps in the missing
storyline pieces, or animate between scenes. It can incorporate stereoscopic
effects for higher viewer impression as well as interactivity aspects. It can
also be used simply in old archived image and film restoration."
"Displays based on organic light-emitting diode (OLED) technology are
appearing on many mobile devices. Unlike liquid crystal displays (LCD), OLED
displays consume dramatically different power for showing different colors. In
particular, OLED displays are inefficient for showing bright colors. This has
made them undesirable for mobile devices because much of the web content is of
bright colors.
  To tackle this problem, we present the motivational studies, design, and
realization of Chameleon, a color adaptive web browser that renders web pages
with power-optimized color schemes under user-supplied constraints. Driven by
the findings from our motivational studies, Chameleon provides end users with
important options, offloads tasks that are not absolutely needed in real-time,
and accomplishes real-time tasks by carefully enhancing the codebase of a
browser engine. According to measure-ments with OLED smartphones, Chameleon is
able to re-duce average system power consumption for web browsing by 41% and
reduce display power consumption by 64% without introducing any noticeable
delay."
"Rational B\'{e}zier functions are widely used as mapping functions in surface
reparameterization, finite element analysis, image warping and morphing. The
injectivity (one-to-one property) of a mapping function is typically necessary
for these applications. Toric B\'{e}zier patches are generalizations of
classical patches (triangular, tensor product) which are defined on the convex
hull of a set of integer lattice points. We give a geometric condition on the
control points that we show is equivalent to the injectivity of every 2D toric
B\'{e}zier patch with those control points for all possible choices of weights.
This condition refines that of Craciun, et al., which only implied injectivity
on the interior of a patch."
"With the rapid advances in mobile technology many mobile devices are capable
of capturing high quality images and video with their embedded camera. This
paper investigates techniques for real-time processing of the resulting images,
particularly on-device utilizing a graphical processing unit. Issues and
limitations of image processing on mobile devices are discussed, and the
performance of graphical processing units on a range of devices measured
through a programmable shader implementation of Canny edge detection."
"Character posing is of interest in computer animation. It is difficult due to
its dependence on inverse kinematics (IK) techniques and articulate property of
human characters . To solve the IK problem, classical methods that rely on
numerical solutions often suffer from the under-determination problem and can
not guarantee naturalness. Existing data-driven methods address this problem by
learning from motion capture data. When facing a large variety of poses
however, these methods may not be able to capture the pose styles or be
applicable in real-time environment. Inspired from the low-rank motion
de-noising and completion model in \cite{lai2011motion}, we propose a novel
model for character posing based on sparse coding. Unlike conventional
approaches, our model directly captures the pose styles in Euclidean space to
provide intuitive training error measurements and facilitate pose synthesis. A
pose dictionary is learned in training stage and based on it natural poses are
synthesized to satisfy users' constraints . We compare our model with existing
models for tasks of pose de-noising and completion. Experiments show our model
obtains lower de-noising and completion error. We also provide User
Interface(UI) examples illustrating that our model is effective for interactive
character posing."
"In this paper, we first give a high-level overview of medical visualization
development over the past 30 years, focusing on key developments and the trends
that they represent. During this discussion, we will refer to a number of key
papers that we have also arranged on the medical visualization research
timeline. Based on the overview and our observations of the field, we then
identify and discuss the medical visualization research challenges that we
foresee for the coming decade."
"Connectomics is a field of neuroscience that analyzes neuronal connections. A
connectome is a complete map of a neuronal system, comprising all neuronal
connections between its structures. The term ""connectome"" is close to the word
""genome"" and implies completeness of all neuronal connections, in the same way
as a genome is a complete listing of all nucleotide sequences. The goal of
connectomics is to create a complete representation of the brain's wiring. Such
a representation is believed to increase our understanding of how functional
brain states emerge from their underlying anatomical structure. Furthermore, it
can provide important information for the cure of neuronal dysfunctions like
schizophrenia or autism. In this paper, we review the current state-of-the-art
of visualization and image processing techniques in the field of connectomics
and describe some remaining challenges."
"Ultrasound is one of the most frequently used imaging modality in medicine.
The high spatial resolution, its interactive nature and non-invasiveness makes
it the first choice in many examinations. Image interpretation is one of
ultrasound's main challenges. Much training is required to obtain a confident
skill level in ultrasound-based diagnostics. State-of-the-art graphics
techniques is needed to provide meaningful visualizations of ultrasound in
real-time. In this paper we present the process-pipeline for ultrasound
visualization, including an overview of the tasks performed in the specific
steps. To provide an insight into the trends of ultrasound visualization
research, we have selected a set of significant publications and divided them
into a technique-based taxonomy covering the topics pre-processing,
segmentation, registration, rendering and augmented reality. For the different
technique types we discuss the difference between ultrasound-based techniques
and techniques for other modalities."
"Color intensity projections (CIP) have been shown to improve the
visualisation of greyscale angiography images by combining greyscale images
into a single color image. A key property of the combined CIP image is the
encoding of the arrival time information from greyscale images into the hue of
the color in the CIP image. A few minor improvements to the calculation of the
CIP image are introduced that substantially improve the quality of the
visualisation. One improvement is interpolating of the greyscale images in time
before calculation of the CIP image. A second is the use of hue cycling - where
the hue of the color is cycled through more than once in an image. The hue
cycling allows the variation of the hue to be concentrated in structures of
interest. If there is a zero time point hue cycling can be applied after zero
time and before zero time can be indicated by greyscale. If there is an end
time point hue cycling can be applied before the end time and pixels can be set
to black after the end time. An angiogram of a brain is used to demonstrate the
substantial improvements hue cycling brings to CIP images. A third improvement
is the use of maximum intensity projection for 2D rendering of a 3D CIP image
volume. A fourth improvement allowing interpreters to interactively adjust the
phase of the hue via standard contrast - brightness controls using lookup
tables. Other potential applications of CIP are also mentioned."
"In this paper, we introduce two generalizations of midpoint subdivision and
analyze the smoothness of the resulting subdivision surfaces at regular and
extraordinary points.
  The smoothing operators used in midpoint and mid-edge subdivision connect the
midpoints of adjacent faces or of adjacent edges, respectively. An arbitrary
combination of these two operators and the refinement operator that splits each
face with m vertices into m quadrilateral subfaces forms a general midpoint
subdivision operator. We analyze the smoothness of the resulting subdivision
surfaces by estimating the norm of a special second order difference scheme and
by using established methods for analyzing midpoint subdivision. The surfaces
are smooth at their regular points and they are also smooth at extraordinary
points for a certain subclass of general midpoint subdivision schemes.
  Generalizing the smoothing rules of non general midpoint subdivision schemes
around extraordinary and regular vertices or faces results in a class of
subdivision schemes, which includes the Catmull-Clark algorithm with restricted
parameters. We call these subdivision schemes generalized Catmull-Clark schemes
and we analyze their smoothness properties."
"Chemists now routinely use software as part of their work. For example,
virtual chemistry allows chemical reactions to be simulated. In particular, a
selection of software is available for the visualisation of complex
3-dimensional molecular structures. Many of these are very beautiful in their
own right. As well as being included as illustrations in academic papers, such
visualisations are often used on the covers of chemistry journals as
artistically decorative and attractive motifs. Chemical images have also been
used as the basis of artworks in exhibitions. This paper explores the
development of the relationship of chemistry, art, and IT. It covers some of
the increasingly sophisticated software used to generate these projections
(e.g., UCSF Chimera) and their progressive use as a visual art form."
"Augmented Reality (AR) is becoming mobile. Mobile devices have many
constraints but also rich new features that traditional desktop computers do
not have. There are several survey papers on AR, but none is dedicated to
Mobile Augmented Reality (MAR). Our work serves the purpose of closing this
gap. The contents are organized with a bottom-up approach. We first present the
state-of-the-art in system components including hardware platforms, software
frameworks and display devices, follows with enabling technologies such as
tracking and data management. We then survey the latest technologies and
methods to improve run-time performance and energy efficiency for practical
implementation. On top of these, we further introduce the application fields
and several typical MAR applications. Finally we conclude the survey with
several challenge problems, which are under exploration and require great
research efforts in the future."
"We are communicating with computers on two different levels. On upper level
we have a very flexible system of windows: we can move them, resize, overlap or
put side by side. At any moment we decide what would be the best view and
reorganize the whole view easily. Then we start any application, go to the
inner level, and everything changes. Here we are stripped of all the
flexibility and can work only inside the scenario, developed by the designer of
the program. Interface will allow us to change some tiny details, but in
general everything is fixed: graphics is neither moveable, nor resizable, and
the same with controls. Author designed an extremely powerful mechanism of
turning graphical objects and controls into moveable and resizable. This can
not only significantly improve the existing applications, but this will bring
the applications to another level. (To estimate the possible difference, try to
imagine the Windows system without its flexibility and compare it with the
current one.) This article explains in details the construction and use of
moveable and resizable graphical objects."
"We present a conceptual model that describes the effect of pixel size on
target acquisition. We demonstrate the use of our conceptual model by applying
it to predict and explain the results of an experiment to evaluate users'
performance in a target acquisition task involving three distinct display
sizes: standard desktop, small and large displays. The results indicate that
users are fastest on standard desktop displays, undershoots are the most common
error on small displays and overshoots are the most common error on large
displays. We propose heuristics to maintain usability when changing displays.
Finally, we contribute to the growing body of evidence that amplitude does
affect performance in a display-based pointing task."
"This paper derives strong relations that boundary curves of a smooth complex
of patches have to obey when the patches are computed by local averaging. These
relations restrict the choice of reparameterizations for geometric continuity.
In particular, when one bicubic tensor-product B-spline patch is associated
with each facet of a quadrilateral mesh with n-valent vertices and we do not
want segments of the boundary curves forced to be linear, then the relations
dictate the minimal number and multiplicity of knots: For general data, the
tensor-product spline patches must have at least two internal double knots per
edge to be able to model a G^1-conneced complex of C^1 splines. This lower
bound on the complexity of any construction is proven to be sharp by suitably
interpreting an existing surface construction. That is, we have a tight bound
on the complexity of smoothing quad meshes with bicubic tensor-product B-spline
patches."
"Many state-of-the art visualization techniques must be tailored to the
specific type of dataset, its modality (CT, MRI, etc.), the recorded object or
anatomical region (head, spine, abdomen, etc.) and other parameters related to
the data acquisition process. While parts of the information (imaging modality
and acquisition sequence) may be obtained from the meta-data stored with the
volume scan, there is important information which is not stored explicitly
(anatomical region, tracing compound). Also, meta-data might be incomplete,
inappropriate or simply missing.
  This paper presents a novel and simple method of determining the type of
dataset from previously defined categories. 2D histograms based on intensity
and gradient magnitude of datasets are used as input to a neural network, which
classifies it into one of several categories it was trained with. The proposed
method is an important building block for visualization systems to be used
autonomously by non-experts. The method has been tested on 80 datasets, divided
into 3 classes and a ""rest"" class.
  A significant result is the ability of the system to classify datasets into a
specific class after being trained with only one dataset of that class. Other
advantages of the method are its easy implementation and its high computational
performance."
"Weierstrass representation is a classical parameterization of minimal
surfaces. However, two functions should be specified to construct the
parametric form in Weierestrass representation. In this paper, we propose an
explicit parametric form for a class of parametric polynomial minimal surfaces
of arbitrary degree. It includes the classical Enneper surface for cubic case.
The proposed minimal surfaces also have some interesting properties such as
symmetry, containing straight lines and self-intersections. According to the
shape properties, the proposed minimal surface can be classified into four
categories with respect to $n=4k-1$ $n=4k+1$, $n=4k$ and $n=4k+2$. The explicit
parametric form of corresponding conjugate minimal surfaces is given and the
isometric deformation is also implemented."
"We show that parametric context-sensitive L-systems with affine geometry
interpretation provide a succinct description of some of the most fundamental
algorithms of geometric modeling of curves. Examples include the
Lane-Riesenfeld algorithm for generating B-splines, the de Casteljau algorithm
for generating Bezier curves, and their extensions to rational curves. Our
results generalize the previously reported geometric-modeling applications of
L-systems, which were limited to subdivision curves."
"In this paper, we consider rendering color videos using a non-photo-realistic
art form technique commonly called stippling. Stippling is the art of rendering
images using point sets, possibly with various attributes like sizes,
elementary shapes, and colors. Producing nice stippling is attractive not only
for the sake of image depiction but also because it yields a compact vectorial
format for storing the semantic information of media. Moreover, stippling is by
construction easily tunable to various device resolutions without suffering
from bitmap sampling artifacts when resizing. The underlying core technique for
stippling images is to compute a centroidal Voronoi tessellation on a
well-designed underlying density. This density relates to the image content,
and is used to compute a weighted Voronoi diagram. By considering videos as
image sequences and initializing properly the stippling of one image by the
result of its predecessor, one avoids undesirable point flickering artifacts
and can produce stippled videos that nevertheless still exhibit noticeable
artifacts. To overcome this, our method improves over the naive scheme by
considering dynamic point creation and deletion according to the current scene
semantic complexity, and show how to effectively vectorize video while
adjusting for both color and contrast characteristics. Furthermore, we explain
how to produce high quality stippled ``videos'' (eg., fully dynamic
spatio-temporal point sets) for media containing various fading effects, like
quick motions of objects or progressive shot changes. We report on practical
performances of our implementation, and present several stippled video results
rendered on-the-fly using our viewer that allows both spatio-temporal dynamic
rescaling (eg., upscale vectorially frame rate)."
"The context of this paper is the use of formal methods for topology-based
geometric modelling. Topology-based geometric modelling deals with objects of
various dimensions and shapes. Usually, objects are defined by a graph-based
topological data structure and by an embedding that associates each topological
element (vertex, edge, face, etc.) with relevant data as their geometric shape
(position, curve, surface, etc.) or application dedicated data (e.g. molecule
concentration level in a biological context). We propose to define
topology-based geometric objects as labelled graphs. The arc labelling defines
the topological structure of the object whose topological consistency is then
ensured by labelling constraints. Nodes have as many labels as there are
different data kinds in the embedding. Labelling constraints ensure then that
the embedding is consistent with the topological structure. Thus,
topology-based geometric objects constitute a particular subclass of a category
of labelled graphs in which nodes have multiple labels."
"We describe a novel integrated algorithm for real-time enhancement of video
acquired under challenging lighting conditions. Such conditions include low
lighting, haze, and high dynamic range situations. The algorithm automatically
detects the dominate source of impairment, then depending on whether it is low
lighting, haze or others, a corresponding pre-processing is applied to the
input video, followed by the core enhancement algorithm. Temporal and spatial
redundancies in the video input are utilized to facilitate real-time processing
and to improve temporal and spatial consistency of the output. The proposed
algorithm can be used as an independent module, or be integrated in either a
video encoder or a video decoder for further optimizations."
"Humans describe images in terms of nouns and adjectives while algorithms
operate on images represented as sets of pixels. Bridging this gap between how
humans would like to access images versus their typical representation is the
goal of image parsing, which involves assigning object and attribute labels to
pixel. In this paper we propose treating nouns as object labels and adjectives
as visual attribute labels. This allows us to formulate the image parsing
problem as one of jointly estimating per-pixel object and attribute labels from
a set of training images. We propose an efficient (interactive time) solution.
Using the extracted labels as handles, our system empowers a user to verbally
refine the results. This enables hands-free parsing of an image into pixel-wise
object/attribute labels that correspond to human semantics. Verbally selecting
objects of interests enables a novel and natural interaction modality that can
possibly be used to interact with new generation devices (e.g. smart phones,
Google Glass, living room devices). We demonstrate our system on a large number
of real-world images with varying complexity. To help understand the tradeoffs
compared to traditional mouse based interactions, results are reported for both
a large scale quantitative evaluation and a user study."
"Decolorization is the process to convert a color image or video to its
grayscale version, and it has received great attention in recent years. An
ideal decolorization algorithm should preserve the original color contrast as
much as possible. Meanwhile, it should provide the final decolorized result as
fast as possible. However, most of the current methods are suffering from
either unsatisfied color information preservation or high computational cost,
limiting their application value. In this paper, a simple but effective
technique is proposed for real-time decolorization. Based on the typical
rgb2gray() color conversion model, which produces a grayscale image by linearly
combining R, G, and B channels, we propose a dominant color hypothesis and a
corresponding distance measurement metric to evaluate the quality of grayscale
conversion. The local optimum scheme provides several ""good"" candidates in a
confidence interval, from which the ""best"" result can be extracted.
Experimental results demonstrate that remarkable simplicity of the proposed
method facilitates the process of high resolution images and videos in
real-time using a common CPU."
"This is a comparative study of the traditional 3D computer graphics technique
of geometric modelling and image-based rendering techniques that were surveyed
and implemented.We have discussed the classifications and representative
methods of both the techniques. The study has shown that there is a strong
continuum between both the techniques and a hybrid of the two is most suitable
for further implementations.This hybridisation study is underway to create
models of real life situations and provide disaster management training."
"One of the big challenges of developing interactive statistical applications
is the management of the data pipeline, which controls transformations from
data to plot. The user's interactions needs to be propagated through these
modules and reflected in the output representation at a fast pace. Each
individual module may be easy to develop and manage, but the dependency
structure can be quite challenging. The MVC (Model/View/Controller) pattern is
an attempt to solve the problem by separating the user's interaction from the
representation of the data. In this paper we discuss the paradigm of reactive
programming in the framework of the MVC architecture and show its applicability
to interactive graphics. Under this paradigm, developers benefit from the
separation of user interaction from the graphical representation, which makes
it easier for users and developers to extend interactive applications. We show
the central role of reactive data objects in an interactive graphics system,
implemented as the R package cranvas, which is freely available on GitHub and
the main developers include the authors of this paper."
"Big data volume continues to grow at unprecedented rates. One of the key
features that makes big data valuable is the promise to find unknown patterns
or correlations that may be able to improve the quality of processes or
systems. Unfortunately, with the exponential growth in data, users often have
difficulty in visualizing the often-unstructured, non-homogeneous data coming
from a variety of sources. The recent growth in popularity of 3D printing has
ushered in a revolutionary way to interact with big data. Using a 3D printed
mockup up a physical or notional environment, one can display data on the
mockup to show real-time data patterns. In this poster and demonstration, we
describe the process of 3D printing and demonstrate an application of
displaying Twitter data on a 3D mockup of the Massachusetts Institute of
Technology (MIT) campus, known as LuminoCity."
"We adapt multilevel, force-directed graph layout techniques to visualizing
dynamic graphs in which vertices and edges are added and removed in an online
fashion (i.e., unpredictably). We maintain multiple levels of coarseness using
a dynamic, randomized coarsening algorithm. To ensure the vertices follow
smooth trajectories, we employ dynamics simulation techniques, treating the
vertices as point particles. We simulate fine and coarse levels of the graph
simultaneously, coupling the dynamics of adjacent levels. Projection from
coarser to finer levels is adaptive, with the projection determined by an
affine transformation that evolves alongside the graph layouts. The result is a
dynamic graph visualizer that quickly and smoothly adapts to changes in a
graph."
"Counting the number of all the matchings on a bipartite graph has been
transformed into calculating the permanent of a matrix obtained from the
extended bipartite graph by Yan Huo, and Rasmussen presents a simple approach
(RM) to approximate the permanent, which just yields a critical ratio
O($n\omega(n)$) for almost all the 0-1 matrices, provided it's a simple
promising practical way to compute this #P-complete problem. In this paper, the
performance of this method will be shown when it's applied to compute all the
matchings based on that transformation. The critical ratio will be proved to be
very large with a certain probability, owning an increasing factor larger than
any polynomial of $n$ even in the sense for almost all the 0-1 matrices. Hence,
RM fails to work well when counting all the matchings via computing the
permanent of the matrix. In other words, we must carefully utilize the known
methods of estimating the permanent to count all the matchings through that
transformation."
"The use of floating bipolar electrodes in electrowinning cells of copper
constitutes a nonconventional technology that promises economic and operational
impacts. This paper presents a computational tool for the simulation and
analysis of such electrochemical cells. A new model is developed for floating
electrodes and a method of finite difference is used to obtain the
threedimensional distribution of the potential and the field of current density
inside the cell. The analysis of the results is based on a technique for the
interactive visualization of three-dimensional vectorial fields as lines of
flow."
"The use of floating bipolar electrodes in electrowinning cells of copper
constitutes a nonconventional technology that promises economic and operational
impacts. This thesis presents a computational tool for the simulation and
analysis of such electrochemical cells. A new model is developed for floating
electrodes and a method of finite difference is used to obtain the
threedimensional distribution of the potential and the field of current density
inside the cell. The analysis of the results is based on a technique for the
interactive visualization of three-dimensional vectorial fields as lines of
flow."
"In this paper we present a framework for the rendering of dynamic 3D virtual
environments which can be integrated in the development of videogames. It
includes methods to manage sounds and particle effects, paged static
geometries, the support of a physics engine and various input systems. It has
been designed with a modular structure to allow future expansions. We exploited
some open-source state-of-the-art components such as OGRE, PhysX,
ParticleUniverse, etc.; all of them have been properly integrated to obtain
peculiar physical and environmental effects. The stand-alone version of the
application is fully compatible with Direct3D and OpenGL APIs and adopts OpenAL
APIs to manage audio cards. Concluding, we devised a showcase demo which
reproduces a dynamic 3D environment, including some particular effects: the
alternation of day and night infuencing the lighting of the scene, the
rendering of terrain, water and vegetation, the reproduction of sounds and
atmospheric agents."
"The process of design and development of virtual environments can be
supported by tools and frameworks, to save time in technical aspects and
focusing on the content. In this paper we present an academic framework which
provides several levels of abstraction to ease this work. It includes
state-of-the-art components we devised or integrated adopting open-source
solutions in order to face specific problems. Its architecture is modular and
customizable, the code is open-source."
"Nowadays three dimension (3D) architectural visualisation has become a
powerful tool in the conceptualisation, design and presentation of
architectural products in the construction industry, providing realistic
interaction and walkthrough on engineering products. Traditional ways of
implementing 3D models involves the use of specialised 3D authoring tools along
with skilled 3D designers with blueprints of the model and this is a slow and
laborious process. The aim of this paper is to automate this process by simply
analyzing the blueprint document and generating the 3D scene automatically. For
this purpose we have devised a 3-Phase recognition approach to pseudo 3D
building generation from 2D floor plan and developed a software accordingly.
Our 3-phased 3D building system has been implemented using C, C++ and OpenCV
library [24] for the Image Processing module; The Save Module generated an XML
file for storing the processed floor plan objects attributes; while the
Irrlitch [14] game engine was used to implement the Interactive 3D module.
Though still at its infancy, our proposed system gave commendable results. We
tested our system on 6 floor plans with complexities ranging from low to high
and the results seems to be very promising with an average processing time of
around 3s and a 3D generation in 4s. In addition the system provides an
interactive walk-though and allows users to modify components."
"With more and more digital media, especially in the field of virtual reality
where detailed and convincing scenes are much required, procedural scene
generation is a big helping tool for artists. A problem is that defining scene
descriptions through these procedures usually requires a knowledge in formal
language grammars, programming theory and manually editing textual files using
a strict syntax, making it less intuitive to use. Luckily, graphical user
interfaces has made a lot of tasks on computers easier to perform and out of
the belief that creating computer programs can also be one of them, visual
programming languages (VPLs) have emerged. The goal in VPLs is to shift more
work from the programmer to the integrated development environment (IDE),
making programming an user-friendlier task.
  In this thesis, an approach of using a VPL for defining procedures that
automatically generate virtual scenes is presented. The methods required to
build a VPL are presented, including a novel method of generating readable code
in a structured programming language. Also, the methods for achieving basic
principles of VPLs will be shown -- suitable visual presentation of information
and guiding the programmer in the right direction using constraints. On the
other hand, procedural generation methods are presented in the context of
visual programming -- adapting the application programming interface (API) of
these methods to better serve the user. The main focus will be on the methods
for urban modeling, such as building, city layout and details generation with
random number generation used to create non-deterministic scenes."
"In this paper we introduce the combined use of Brain-Computer Interfaces
(BCI) and Haptic interfaces. We propose to adapt haptic guides based on the
mental activity measured by a BCI system. This novel approach is illustrated
within a proof-of-concept system: haptic guides are toggled during a
path-following task thanks to a mental workload index provided by a BCI. The
aim of this system is to provide haptic assistance only when the user's brain
activity reflects a high mental workload. A user study conducted with 8
participants shows that our proof-of-concept is operational and exploitable.
Results show that activation of haptic guides occurs in the most difficult part
of the path-following task. Moreover it allows to increase task performance by
53% by activating assistance only 59% of the time. Taken together, these
results suggest that BCI could be used to determine when the user needs
assistance during haptic interaction and to enable haptic guides accordingly."
"The Herschel Interactive Processing Environment (HIPE) was developed by the
European Space Agency (ESA) in collaboration with NASA and the Herschel
Instrument Control Centres to provide the astronomical community a complete
environment to process and analyze the data gathered by the Herschel Space
Observatory. One of the most important components of HIPE is the plotting
system (named PlotXY) that we present here. With PlotXY it is possible to
produce easily high quality publication ready 2D plots. It provides a long list
of features, with fully configurable components, and interactive zooming. The
entire code of HIPE is written in Java and is open source released under the
GNU Lesser General Public License version 3. A new version of PlotXY is being
developed to be independent from the HIPE code base; it is available to the
software development community for the inclusion in other projects at the URL
http://code.google.com/p/jplot2d/."
"Morphology of cardiovascular tissue is influenced by the unsteady behavior of
the blood flow and vice versa. Therefore, the pathogenesis of several
cardiovascular diseases is directly affected by the blood-flow dynamics.
Understanding flow behavior is of vital importance to understand the
cardiovascular system and potentially harbors a considerable value for both
diagnosis and risk assessment. The analysis of hemodynamic characteristics
involves qualitative and quantitative inspection of the blood-flow field.
Visualization plays an important role in the qualitative exploration, as well
as the definition of relevant quantitative measures and its validation. There
are two main approaches to obtain information about the blood flow: simulation
by computational fluid dynamics, and in-vivo measurements. Although research on
blood flow simulation has been performed for decades, many open problems remain
concerning accuracy and patient-specific solutions. Possibilities for real
measurement of blood flow have recently increased considerably by new
developments in magnetic resonance imaging which enable the acquisition of 3D
quantitative measurements of blood-flow velocity fields. This chapter presents
the visualization challenges for both simulation and real measurements of
unsteady blood-flow fields."
"In JPEG (DCT based) compresses image data by representing the original image
with a small number of transform coefficients. It exploits the fact that for
typical images a large amount of signal energy is concentrated in a small
number of coefficients. The goal of DCT transform coding is to minimize the
number of retained transform coefficients while keeping distortion at an
acceptable level.In JPEG, it is done in 8X8 non overlapping blocks. It divides
an image into blocks of equal size and processes each block independently.
Block processing allows the coder to adapt to the local image statistics,
exploit the correlation present among neighboring image pixels, and to reduce
computational and storage requirements. One of the most degradation of the
block transform coding is the blocking artifact. These artifacts appear as a
regular pattern of visible block boundaries. This degradation is a direct
result of the coarse quantization of the coefficients and the independent
processing of the blocks which does not take into account the existing
correlations among adjacent block pixels. In this paper attempt is being made
to reduce the blocking artifact introduced by the Block DCT Transform in JPEG."
"Designing 3D objects from scratch is difficult, especially when the user
intent is fuzzy without a clear target form. In the spirit of
modeling-by-example, we facilitate design by providing reference and
inspiration from existing model contexts. We rethink model design as navigating
through different possible combinations of part assemblies based on a large
collection of pre-segmented 3D models. We propose an interactive
sketch-to-design system, where the user sketches prominent features of parts to
combine. The sketched strokes are analyzed individually and in context with the
other parts to generate relevant shape suggestions via a design gallery
interface. As the session progresses and more parts get selected, contextual
cues becomes increasingly dominant and the system quickly converges to a final
design. As a key enabler, we use pre-learned part-based contextual information
to allow the user to quickly explore different combinations of parts. Our
experiments demonstrate the effectiveness of our approach for efficiently
designing new variations from existing shapes."
"Google Earth (GE) has become a powerful tool for geological, geophysical and
geographical modeling; yet GE can be accepted to acquire elevation data of
terrain. In this paper, we present a real study case of building the discrete
surface model (DSM) at Haut-Barr Castle in France based on the elevation data
of terrain points extracted from GE using the COM API. We first locate the
position of Haut-Barr Castle and determine the region of the study area, then
extract elevation data of terrain at Haut-Barr, and thirdly create a planar
triangular mesh that covers the study area and finally generate the desired DSM
by calculating the elevation of vertices in the planar mesh via interpolating
with Universal Kriging (UK) and Inverse Distance Weighting (IDW). The generated
DSM can reflect the features of the ground surface at Haut-Barr well, and can
be used for constructingthe Sealed Engineering Geological Model (SEGM) in
further step."
"From the beginning the Geant4 Visualisation System was designed to support
several simultaneous graphics systems written to common abstract interfaces.
Today it has matured into a powerful diagnostic and presentational tool. It
comes with a library of models that may be added to the current scene and which
include the representation of the Geant4 geometry hierarchy, simulated
trajectories and user-written hits and digitisations. The workhorse is the
OpenGL suite of drivers for X, Xm, Qt and Win32. There is an Open Inventor
driver. Scenes can be exported in special graphics formats for offline viewing
in the DAWN, VRML, HepRApp and gMocren browsers. PostScript can be generated
through OpenGL, Open Inventor, DAWN and HepRApp. Geant4's own tracking
algorithms are used by the Ray Tracer. Not all drivers support all features but
all drivers bring added functionality of some sort. This paper describes the
interfaces and details the individual drivers."
"Self-Organizing Maps (SOM) are popular unsupervised artificial neural network
used to reduce dimensions and visualize data. Visual interpretation from
Self-Organizing Maps (SOM) has been limited due to grid approach of data
representation, which makes inter-scenario analysis impossible. The paper
proposes a new way to structure SOM. This model reconstructs SOM to show
strength between variables as the threads of a cobweb and illuminate
inter-scenario analysis. While Radar Graphs are very crude representation of
spider web, this model uses more lively and realistic cobweb representation to
take into account the difference in strength and length of threads. This model
allows for visualization of highly unstructured dataset with large number of
dimensions, common in Bigdata sources."
"Google Earth (GE) has become a powerful tool for geological modeling and
visualization. An interesting and useful feature of GE, Google Street View, can
allow the GE users to view geological structure such as layers of rock masses
at a field site. In this paper, we introduce a practical solution for building
3D geological models for rock masses based on the data acquired by use with GE.
A real study case at Haut-Barr, France is presented to demonstrate our
solution. We first locate the position of Haut-Barr in GE, and then determine
the shape and scale of the rock masses in the study area, and thirdly acquire
the layout of layers of rock masses in the Google Street View, and finally
create the approximate 3D geological models by extruding and intersecting. The
generated 3D geological models can simply reflect the basic structure of the
rock masses at Haut-Barr, and can be used for visualizing the rock bodies
interactively."
"We have been developing a visualization application for CAVE-type virtual
reality (VR) systems for more than a decade. This application, VFIVE, is
currently used in several CAVE systems in Japan for routine visualizations. It
is also used as a base system of further developments of advanced
visualizations. The development of VFIVE is summarized."
"VFIVE is a scientific visualization application for CAVE-type immersive
virtual reality systems. The source codes are freely available. VFIVE is used
as a research tool in various VR systems. It also lays the groundwork for
developments of new visualization software for CAVEs. In this paper, we pick up
five CAVE systems in four different institutions in Japan. Applications of
VFIVE in each CAVE system are summarized. Special emphases will be placed on
scientific and technical achievements made possible by VFIVE."
"Volume Rendering applications require sophisticated user interaction for the
definition and refinement of transfer functions. Traditional 2D desktop user
interface elements have been developed to solve this task, but such concepts do
not map well to the interaction devices available in Virtual Reality
environments.
  In this paper, we propose an intuitive user interface for Volume Rendering
specifically designed for Virtual Reality environments. The proposed interface
allows transfer function design and refinement based on intuitive two-handed
operation of Wand-like controllers. Additional interaction modes such as
navigation and clip plane manipulation are supported as well.
  The system is implemented using the Sony PlayStation Move controller system.
This choice is based on controller device capabilities as well as application
and environment constraints.
  Initial results document the potential of our approach."
"This short article is a brief account of the usage of fourth-order curvature
flow in surface modelling."
"In the first part of the paper we present a short review of applications of
digital differential analyzers (DDA) to generation of circles showing that they
can be treated as one-step numerical schemes. In the second part we present and
discuss a novel fast algorithm based on a two-step numerical scheme (explicit
midpoint rule). Although our algorithm is as cheap as the simplest one-step DDA
algoritm (and can be represented in terms of shifts and additions), it
generates circles with maximal accuracy, i.e., it is exact up to round-off
errors."
"A system for live high quality surface reconstruction using a single moving
depth camera on a commodity hardware is presented. High accuracy and real-time
frame rate is achieved by utilizing graphics hardware computing capabilities
via OpenCL and by using sparse data structure for volumetric surface
representation. Depth sensor pose is estimated by combining serial texture
registration algorithm with iterative closest points algorithm (ICP) aligning
obtained depth map to the estimated scene model. Aligned surface is then fused
into the scene. Kalman filter is used to improve fusion quality. Truncated
signed distance function (TSDF) stored as block-based sparse buffer is used to
represent surface. Use of sparse data structure greatly increases accuracy of
scanned surfaces and maximum scanning area. Traditional GPU implementation of
volumetric rendering and fusion algorithms were modified to exploit sparsity to
achieve desired performance. Incorporation of texture registration for sensor
pose estimation and Kalman filter for measurement integration improved accuracy
and robustness of scanning process."
"Animation, which is basically a form of pictorial presentation, has become
the most prominent feature of technology-based learning environments. It refers
to simulated motion pictures showing movement of drawn objects. Recently,
educational computer animation has turned out to be one of the most elegant
tools for presenting multimedia materials for learners, and its significance in
helping to understand and remember information has greatly increased since the
advent of powerful graphics-oriented computers. In this book chapter we
introduce and discuss the history of computer animation, its well-known
fundamental principles and some educational applications. It is however still
debatable if truly educational computer animations help in learning, as the
research on whether animation aids learners' understanding of dynamic phenomena
has come up with positive, negative and neutral results. We have tried to
provide as much detailed information on computer animation as we could, and we
hope that this book chapter will be useful for students who study computer
science, computer-assisted education or some other courses connected with
contemporary education, as well as researchers who conduct their research in
the field of computer animation."
"In this paper we present modern texture mapping techniques and several
applications of polynomial texture mapping in cultural heritage programs. We
also consider some well-known and some new methods for mathematical procedure
that is involved in generation of polynomial texture maps."
"A topologically-informed method is presented for seeding of hyperstreamlines
for visualization of alignment tensor fields. The method is inspired by and
applied to visualization of nematic liquid crystal (LC) reorientation dynamics
simulations. The method distributes hyperstreamlines along domain boundaries
and edges of a nearest-neighbor graph whose vertices are degenerate regions of
the alignment tensor field, which correspond to orientational defects in a
nematic LC domain. This is accomplished without iteration while conforming to a
user-specified spacing between hyperstreamlines and avoids possible failure
modes associated with hyperstreamline integration in the vicinity of
degeneracies of alignment (orientational defects). It is shown that the
presented seeding method enables automated hyperstreamline-based visualization
of a broad range of alignment tensor fields which enhances the ability of
researchers to interpret these fields and provides an alternative to using
glyph-based techniques."
"Markerless motion capture is an active research in 3D virtualization. In
proposed work we presented a system for markerless motion capture for 3D human
character animation, paper presents a survey on motion and skeleton tracking
techniques which are developed or are under development. The paper proposed a
method to transform the motion of a performer to a 3D human character (model),
the 3D human character performs similar movements as that of a performer in
real time. In the proposed work, human model data will be captured by Kinect
camera, processed data will be applied on 3D human model for animation. 3D
human model is created using open source software (MakeHuman). Anticipated
dataset for sport activity is considered as input which can be applied to any
HCI application."
"In this paper a new algorithm has been proposed which can fix the problem of
Weiler Atherton algorithm. The problem of Weiler Atherton algorithm lies in
clipping self intersecting polygon. Clipping self intersecting polygon is not
considered in Weiler Atherton algorithm and hence it is also a main
disadvantage of this algorithm. In our new algorithm a self intersecting
polygon has been divided into non self intersecting contours and then perform
the Weiler Atherton clipping algorithm on those sub polygons. For holes we have
to store the edges that is not own boundary of hole contour from recently
clipped polygon. Thus if both contour is hole then we have to store all the
edges of the recently clipped polygon. Finally the resultant polygon has been
produced by eliminating all the stored edges."
"Real-world images usually contain vivid contents and rich textural details,
which will complicate the manipulation on them. In this paper, we design a new
framework based on content-aware synthesis to enhance content-aware image
retargeting. By detecting the textural regions in an image, the textural image
content can be synthesized rather than simply distorted or cropped. This method
enables the manipulation of textural & non-textural regions with different
strategy since they have different natures. We propose to retarget the textural
regions by content-aware synthesis and non-textural regions by fast
multi-operators. To achieve practical retargeting applications for general
images, we develop an automatic and fast texture detection method that can
detect multiple disjoint textural regions. We adjust the saliency of the image
according to the features of the textural regions. To validate the proposed
method, comparisons with state-of-the-art image targeting techniques and a user
study were conducted. Convincing visual results are shown to demonstrate the
effectiveness of the proposed method."
"In the SIGGRAPH 2014 paper [SvTSH14] an approach for animating deformable
objects using sparse spacetime constraints is introduced. This report contains
the proofs of two theorems presented in the paper."
"Background: The biological world is replete with phenomena that appear to be
ideally modeled and analyzed by one archetypal statistical framework - the
Graphical Probabilistic Model (GPM). The structure of GPMs is a uniquely good
match for biological problems that range from aligning sequences to modeling
the genome-to-phenome relationship. The fundamental questions that GPMs address
involve making decisions based on a complex web of interacting factors.
Unfortunately, while GPMs ideally fit many questions in biology, they are not
an easy solution to apply. Building a GPM is not a simple task for an end user.
Moreover, applying GPMs is also impeded by the insidious fact that the complex
web of interacting factors inherent to a problem might be easy to define and
also intractable to compute upon. Discussion: We propose that the visualization
sciences can contribute to many domains of the bio-sciences, by developing
tools to address archetypal representation and user interaction issues in GPMs,
and in particular a variety of GPM called a Conditional Random Field(CRF). CRFs
bring additional power, and additional complexity, because the CRF dependency
network can be conditioned on the query data. Conclusions: In this manuscript
we examine the shared features of several biological problems that are amenable
to modeling with CRFs, highlight the challenges that existing visualization and
visual analytics paradigms induce for these data, and document an experimental
solution called StickWRLD which, while leaving room for improvement, has been
successfully applied in several biological research projects."
"We introduce a framework for the generation of grid-shell structures that is
based on Voronoi diagrams and allows us to design tessellations that achieve
excellent static performances. We start from an analysis of stress on the input
surface and we use the resulting tensor field to induce an anisotropic
non-Euclidean metric over it. Then we compute a Centroidal Voronoi Tessellation
under the same metric. The resulting mesh is hex-dominant and made of cells
with a variable density, which depends on the amount of stress, and anisotropic
shape, which depends on the direction of maximum stress. This mesh is further
optimized taking into account symmetry and regularity of cells to improve
aesthetics. We demonstrate that our grid-shells achieve better static
performances with respect to quad-based grid shells, while offering an
innovative and aesthetically pleasing look."
"Compression plays a significant role in a data storage and a transmission. If
we speak about a generall data compression, it has to be a lossless one. It
means, we are able to recover the original data 1:1 from the compressed file.
Multimedia data (images, video, sound...), are a special case. In this area, we
can use something called a lossy compression. Our main goal is not to recover
data 1:1, but only keep them visually similar. This article is about an image
compression, so we will be interested only in image compression. For a human
eye, it is not a huge difference, if we recover RGB color with values
[150,140,138] instead of original [151,140,137]. The magnitude of a difference
determines the loss rate of the compression. The bigger difference usually
means a smaller file, but also worse image quality and noticable differences
from the original image. We want to cover compression techniques mainly from
the last decade. Many of them are variations of existing ones, only some of
them uses new principes."
"We introduce linear-state dataflows, a canonical model for a large set of
visualization algorithms that we call data-linear visualizations. Our model
defines a fixed dataflow architecture: partitioning and subpartitioning of
input data, ordering, graphic primitives, and graphic attributes generation.
Local variables and accumulators are specific concepts that extend the
expressiveness of the dataflow to support features of visualization algorithms
that require state handling. We first show the flexibility of our model: it
enables the declarative construction of many common algorithms with just a few
mappings. Furthermore, the model enables easy mixing of visual mappings, such
as creating treemaps of histograms and 2D plots, plots of histograms...
Finally, we introduce our model in a more formal way and present some of its
important properties. We have implemented this model in a visualization
framework built around the concept of linear-state dataflows."
"Sculptors often deviate from geometric accuracy in order to enhance the
appearance of their sculpture. These subtle stylizations may emphasize anatomy,
draw the viewer's focus to characteristic features of the subject, or symbolize
textures that might not be accurately reproduced in a particular sculptural
medium, while still retaining fidelity to the unique proportions of an
individual. In this work we demonstrate an interactive system for enhancing
face geometry using a class of stylizations based on visual decomposition into
abstract semantic regions, which we call sculptural abstraction. We propose an
interactive two-scale optimization framework for stylization based on
sculptural abstraction, allowing real-time adjustment of both global and local
parameters. We demonstrate this system's effectiveness in enhancing physical 3D
prints of scans from various sources."
"Expression cloning plays an important role in facial expression synthesis. In
this paper, a novel algorithm is proposed for facial expression cloning. The
proposed algorithm first introduces a new elastic model to balance the global
and local warping effects, such that the impacts from facial feature diversity
among people can be minimized, and thus more effective geometric warping
results can be achieved. Furthermore, a muscle-distribution-based (MD) model is
proposed, which utilizes the muscle distribution of the human face and results
in more accurate facial illumination details. In addition, we also propose a
new distance-based metric to automatically select the optimal parameters such
that the global and local warping effects in the elastic model can be suitably
balanced. Experimental results show that our proposed algorithm outperforms the
existing methods."
"In this paper we address the problem of interpolating a spline developable
patch bounded by a given spline curve and the first and the last rulings of the
developable surface. In order to complete the boundary of the patch a second
spline curve is to be given. Up to now this interpolation problem could be
solved, but without the possibility of choosing both endpoints for the rulings.
We circumvent such difficulty here by resorting to degree elevation of the
developable surface. This is useful not only to solve this problem, but also
other problems dealing with triangular developable patches."
"Morphing is the process of changing one figure into another. Some numerical
methods of 3D surface morphing by deformable modeling and conformal mapping are
shown in this study. It is well known that there exists a unique Riemann
conformal mapping from a simply connected surface into a unit disk by the
Riemann mapping theorem. The dilation and relative orientations of the 3D
surfaces can be linked through the M\""obius transformation due to the conformal
characteristic of the Riemann mapping. On the other hand, a 3D surface
deformable model can be built via various approaches such as mutual
parameterization from direct interpolation or surface matching using landmarks.
In this paper, we take the advantage of the unique representation of 3D
surfaces by the mean curvatures and the conformal factors associated with the
Riemann mapping. By registering the landmarks on the conformal parametric
domains, the correspondence of the mean curvatures and the conformal factors
for each surfaces can be obtained. As a result, we can construct the 3D
deformation field from the surface reconstruction algorithm proposed by Gu and
Yau. Furthermore, by composition of the M\""obius transformation and the 3D
deformation field, the morphing sequence can be generated from the mean
curvatures and the conformal factors on a unified mesh structure by using the
cubic spline homotopy. Several numerical experiments of the face morphing are
presented to demonstrate the robustness of our approach."
"This is the preprint version of our paper on EUROGRAPHICS 2015. A big city
visual analysis platform based on Web Virtual Reality Geographical Information
System (WEBVRGIS) is presented. Extensive model editing functions and spatial
analysis functions are available, including terrain analysis, spatial analysis,
sunlight analysis, traffic analysis, population analysis and community
analysis."
"In this paper we classify and derive closed formulas for geometric elements
of quadrics in rational B\'ezier triangular form (such as the center, the conic
at infinity, the vertex and the axis of paraboloids and the principal planes),
using just the control vertices and the weights for the quadric patch. The
results are extended also to quadric tensor product patches. Our results rely
on using techniques from projective algebraic geometry to find suitable
bilinear forms for the quadric in a coordinate-free fashion, considering a
pencil of quadrics that are tangent to the given quadric along a conic. Most of
the information about the quadric is encoded in one coefficient, involving the
weights of the patch, which allows us to tell apart oval from ruled quadrics.
This coefficient is also relevant to determine the affine type of the quadric.
Spheres and quadrics of revolution are characterised within this framework."
"Custom optics is a necessity for many imaging applications. Unfortunately,
custom lens design is costly (thousands to tens of thousands of dollars), time
consuming (10-12 weeks typical lead time), and requires specialized optics
design expertise. By using only inexpensive, off-the-shelf lens components the
Lens Factory automatic design system greatly reduces cost and time. Design,
ordering of parts, delivery, and assembly can be completed in a few days, at a
cost in the low hundreds of dollars. Lens design constraints, such as focal
length and field of view, are specified in terms familiar to the graphics
community so no optics expertise is necessary. Unlike conventional lens design
systems, which only use continuous optimization methods, Lens Factory adds a
discrete optimization stage. This stage searches the combinatorial space of
possible combinations of lens elements to find novel designs, evolving simple
canonical lens designs into more complex, better designs. Intelligent pruning
rules make the combinatorial search feasible. We have designed and built
several high performance optical systems which demonstrate the practicality of
the system."
"We explore the use of inexpensive consumer light- field camera technology for
the purpose of light-field mi- croscopy. Our experiments are based on the Lytro
(first gen- eration) camera. Unfortunately, the optical systems of the Lytro
and those of microscopes are not compatible, lead- ing to a loss of light-field
information due to angular and spatial vignetting when directly recording
microscopic pic- tures. We therefore consider an adaptation of the Lytro op-
tical system. We demonstrate that using the Lytro directly as an oc- ular
replacement, leads to unacceptable spatial vignetting. However, we also found a
setting that allows the use of the Lytro camera in a virtual imaging mode which
prevents the information loss to a large extent. We analyze the new vir- tual
imaging mode and use it in two different setups for im- plementing light-field
microscopy using a Lytro camera. As a practical result, we show that the camera
can be used for low magnification work, as e.g. common in quality control,
surface characterization, etc. We achieve a maximum spa- tial resolution of
about 6.25{\mu}m, albeit at a limited SNR for the side views."
"Computer graphics has wide range of applications which are implemented into
computer animation, computer modeling among others. Since the invention of
computer graphics researchers have not paid much of attentions toward the
possibility of converting oral tales otherwise known as folktales into possible
cartoon animated videos. This paper is based on how to develop cartoons of
local folktales that will be of huge benefits to Nigerians. The activities were
divided into 5 stages; analysis, design, development, implementation and
evaluation which involved various processes and use of various specialized
software and hardware. After the implementation of this project, the video
characteristics were evaluated using likert scale. Analysis of 30 user
responses indicated that 17 users (56.7 percent) rated the image quality as
excellent, the video and image synchronization was rated as excellent by 9
users (30 percent), the Background noise was rated excellent by 18 users (60
percent), the Character Impression was rated Excellent by 11 users (36.67
percent), the general assessment of the storyline was rated excellent by 17
users (56.7 percent), the video Impression was rated excellent by 11 users
(36.67 percent) and the voice quality was rated by 10 users (33.33 percent) as
excellent."
"Flutter shutter is a technique in which the exposure is chopped into segments
and light is only integrated part of the time. By carefully selecting the
chopping sequence it is possible to better condition the data for
reconstruction problems such as motion deblurring, focal sweeping, and
compressed sensing. The partial exposure trades better conditioning for less
energy. In problems such as motion deblurring the available energy is what
caused the problem in the first place (as strong illumination allows short
exposure thus eliminates motion blur). It is still beneficial because the
benefit from the better conditioning outweighs the cost in energy.
  This documents is focused on light efficient flutter shutter that provides
better conditioning and better energy utilization than conventional flutter
shutter."
"Light projection is a powerful technique to edit appearances of objects in
the real world. Based on pixel-wise modification of light transport, previous
techniques have successfully modified static surface properties such as surface
color, dynamic range, gloss and shading. Here, we propose an alternative light
projection technique that adds a variety of illusory, yet realistic distortions
to a wide range of static 2D and 3D projection targets. The key idea of our
technique, named Deformation Lamps, is to project only dynamic luminance
information, which effectively activates the motion (and shape) processing in
the visual system, while preserving the color and texture of the original
object. Although the projected dynamic luminance information is spatially
inconsistent with the color and texture of the target object, the observer's
brain automatically com- bines these sensory signals in such a way as to
correct the inconsistency across visual attributes. We conducted a
psychophysical experiment to investigate the characteristics of the
inconsistency correction, and found that the correction was dependent
critically on the retinal magnitude of inconsistency. Another experiment showed
that perceived magnitude of image deformation by our techniques was
underestimated. The results ruled out the possibility that the effect by our
technique stemmed simply from the physical change of object appearance by light
projection. Finally, we discuss how our techniques can make the observers
perceive a vivid and natural movement, deformation, or oscillation of a variety
of static objects, including drawn pictures, printed photographs, sculptures
with 3D shading, objects with natural textures including human bodies."
"We present a geometric surface parameterization algorithm and several
visualization techniques adapted to the problem of understanding the 4D
peristaltic-like motion of the outflow tract (OFT) in an embryonic chick heart.
We illustrated the techniques using data from hearts under normal conditions
(four embryos), and hearts in which blood flow conditions are altered through
OFT banding (four embryos). The overall goal is to create quantitative measures
of the temporal heart-shape change both within a single subject and between
multiple subjects. These measures will help elucidate how altering hemodynamic
conditions changes the shape and motion of the OFT walls, which in turn
influence the stresses and strains on the developing heart, causing it to
develop differently. We take advantage of the tubular shape and periodic motion
of the OFT to produce successively lower dimensional visualizations of the
cardiac motion (e.g. curvature, volume, and cross-section) over time, and
quantifications of such visualizations."
"As humans, we regularly interpret images based on the relations between image
regions. For example, a person riding object X, or a plank bridging two
objects. Current methods provide limited support to search for images based on
such relations. We present RAID, a relation-augmented image descriptor that
supports queries based on inter-region relations. The key idea of our
descriptor is to capture the spatial distribution of simple point-to-region
relationships to describe more complex relationships between two image regions.
We evaluate the proposed descriptor by querying into a large subset of the
Microsoft COCO database and successfully extract nontrivial images
demonstrating complex inter-region relations, which are easily missed or
erroneously classified by existing methods."
"While significant research has been dedicated to the simulation of fluids,
not much attention has been given to exploring new interesting behavior that
can be generated with the different types of non-Newtonian fluids with
non-constant viscosity. Going in this direction, this paper introduces a
computational model for simulating the interesting phenomena observed in
non-Newtonian shear thickening fluids, which are fluids where the viscosity
increases with increased stress. These fluids have unique and unconventional
behavior, and they often appear in real world scenarios such as when sinking in
quicksand or when experimenting with popular cornstarch and water mixtures.
While interesting behavior of shear thickening fluids can be easily observed in
the real world, the most interesting phenomena of these fluids have not been
simulated before in computer graphics. The fluid exhibits unique phase changes
between solid and liquid states, great impact resistance in its solid state and
strong hysteresis effects. Our proposed approach builds on existing
non-Newtonian fluid models in computer graphics and introduces an efficient
history-based stiffness term that is essential to produce the most interesting
shear thickening phenomena. The history-based stiffness is formulated through
the use of fractional derivatives, leveraging the fractional calculus ability
to depict both the viscoelastic behavior and the history effects of
history-dependent systems. Simulations produced by our method are compared
against real experiments and the results demonstrate that the proposed model
successfully captures key phenomena observed in shear thickening fluids."
"We present a novel volumetric animation generation framework to create new
types of animations from raw 3D surface or point cloud sequence of captured
real performances. The framework considers as input time incoherent 3D
observations of a moving shape, and is thus particularly suitable for the
output of performance capture platforms. In our system, a suitable virtual
representation of the actor is built from real captures that allows seamless
combination and simulation with virtual external forces and objects, in which
the original captured actor can be reshaped, disassembled or reassembled from
user-specified virtual physics. Instead of using the dominant surface-based
geometric representation of the capture, which is less suitable for volumetric
effects, our pipeline exploits Centroidal Voronoi tessellation decompositions
as unified volumetric representation of the real captured actor, which we show
can be used seamlessly as a building block for all processing stages, from
capture and tracking to virtual physic simulation. The representation makes no
human specific assumption and can be used to capture and re-simulate the actor
with props or other moving scenery elements. We demonstrate the potential of
this pipeline for virtual reanimation of a real captured event with various
unprecedented volumetric visual effects, such as volumetric distortion,
erosion, morphing, gravity pull, or collisions."
"We introduce a new presentation of the two dimensional rigid transformation
which is more concise and efficient than the standard matrix presentation. By
modifying the ordinary dual number construction for the complex numbers, we
define the ring of the anti-commutative dual complex numbers, which
parametrizes two dimensional rotation and translation all together. With this
presentation, one can easily interpolate or blend two or more rigid
transformations at a low computational cost. We developed a library for C++
with the MIT-licensed source code and demonstrate its facility by an
interactive deformation tool developed for iPad."
"In this paper we review the derivation of implicit equations for
non-degenerate quadric patches in rational Bezier triangular form. These are
the case of Steiner surfaces of degree two. We derive the bilinear forms for
such quadrics in a coordinate-free fashion in terms of their control net and
their list of weights in a suitable form. Our construction relies on projective
geometry and is grounded on the pencil of quadrics circumscribed to a
tetrahedron formed by vertices of the control net and an additional point which
is required for the Steiner surface to be a non-degenerate quadric."
"The As-Rigid-As-Possible (ARAP) shape deformation framework is a versatile
technique for morphing, surface modelling, and mesh editing. We discuss an
improvement of the ARAP framework in a few aspects: 1. Given a triangular mesh
in 3D space, we introduce a method to associate a tetrahedral structure, which
encodes the geometry of the original mesh. 2. We use a Lie algebra based method
to interpolate local transformation, which provides better handling of rotation
with large angle. 3. We propose a new error function to compile local
transformations into a global piecewise linear map, which is rotation invariant
and easy to minimise. We implemented a shape blender based on our algorithm and
its MIT licensed source code is available online."
"Design of false color palette is quite easy but some effort has to be done to
achieve good dynamic range, contrast and overall appearance of the palette.
Such palettes, for instance, are commonly used in scientific papers for
presenting the data. However, to lower the cost of the paper most scientists
decide to let the data to be printed in grayscale. The same applies to e-book
readers based on e-ink where most of them are still grayscale. For majority of
false color palettes reproducing them in grayscale results in ambiguous mapping
of the colors and may be misleading for the reader. In this article design of
false color palettes suitable for grayscale reproduction is described. Due to
the monotonic change of luminance of these palettes grayscale representation is
very similar to the data directly presented with a grayscale palette. Some
suggestions and examples how to design such palettes are provided."
"Digital circles not only play an important role in various technological
settings, but also provide a lively playground for more fundamental
number-theoretical questions. In this paper, we present a new recursive
algorithm for the construction of digital circles on the integer lattice
$\mathbb{Z}^2$, which makes sole use of the signum function. By briefly
elaborating on the nature of discretization of circular paths, we then find
that this algorithm recovers, in a space endowed with $\ell^1$-norm, the
defining constant $\pi$ of a circle in $\mathbb{R}^2$."
"This paper presents an extension to the KinectFusion algorithm which allows
creating simplified 3D models with high quality RGB textures. This is achieved
through (i) creating model textures using images from an HD RGB camera that is
calibrated with Kinect depth camera, (ii) using a modified scheme to update
model textures in an asymmetrical colour volume that contains a higher number
of voxels than that of the geometry volume, (iii) simplifying dense polygon
mesh model using quadric-based mesh decimation algorithm, and (iv) creating and
mapping 2D textures to every polygon in the output 3D model. The proposed
method is implemented in real-time by means of GPU parallel processing.
Visualization via ray casting of both geometry and colour volumes provides
users with a real-time feedback of the currently scanned 3D model. Experimental
results show that the proposed method is capable of keeping the model texture
quality even for a heavily decimated model and that, when reconstructing small
objects, photorealistic RGB textures can still be reconstructed."
"In computer vision, convolutional neural networks (CNNs) have recently
achieved new levels of performance for several inverse problems where RGB pixel
appearance is mapped to attributes such as positions, normals or reflectance.
In computer graphics, screen-space shading has recently increased the visual
quality in interactive image synthesis, where per-pixel attributes such as
positions, normals or reflectance of a virtual 3D scene are converted into RGB
pixel appearance, enabling effects like ambient occlusion, indirect light,
scattering, depth-of-field, motion blur, or anti-aliasing. In this paper we
consider the diagonal problem: synthesizing appearance from given per-pixel
attributes using a CNN. The resulting Deep Shading simulates various
screen-space effects at competitive quality and speed while not being
programmed by human experts but learned from example images."
"Probabilistic inference algorithms such as Sequential Monte Carlo (SMC)
provide powerful tools for constraining procedural models in computer graphics,
but they require many samples to produce desirable results. In this paper, we
show how to create procedural models which learn how to satisfy constraints. We
augment procedural models with neural networks which control how the model
makes random choices based on the output it has generated thus far. We call
such models neurally-guided procedural models. As a pre-computation, we train
these models to maximize the likelihood of example outputs generated via SMC.
They are then used as efficient SMC importance samplers, generating
high-quality results with very few samples. We evaluate our method on
L-system-like models with image-based constraints. Given a desired quality
threshold, neurally-guided models can generate satisfactory results up to 10x
faster than unguided models."
"Collision sequences are commonly used in games and entertainment to add drama
and excitement. Authoring even two body collisions in the real world can be
difficult, as one has to get timing and the object trajectories to be correctly
synchronized. After tedious trial-and-error iterations, when objects can
actually be made to collide, then they are difficult to capture in 3D. In
contrast, synthetically generating plausible collisions is difficult as it
requires adjusting different collision parameters (e.g., object mass ratio,
coefficient of restitution, etc.) and appropriate initial parameters. We
present SMASH to directly read off appropriate collision parameters directly
from raw input video recordings. Technically we enable this by utilizing laws
of rigid body collision to regularize the problem of lifting 2D trajectories to
a physically valid 3D reconstruction of the collision. The reconstructed
sequences can then be modified and combined to easily author novel and
plausible collisions. We evaluate our system on a range of synthetic scenes and
demonstrate the effectiveness of our method by accurately reconstructing
several complex real world collision events."
"Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed
reality and robotic applications. However, scalability brings challenges of
drift in pose estimation, introducing significant errors in the accumulated
model. Approaches often require hours of offline processing to globally correct
model errors. Recent online methods demonstrate compelling results, but suffer
from: (1) needing minutes to perform online correction preventing true
real-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation
resulting in many tracking failures; or (3) supporting only unstructured
point-based representations, which limit scan quality and applicability. We
systematically address these issues with a novel, real-time, end-to-end
reconstruction framework. At its core is a robust pose estimation strategy,
optimizing per frame for a global set of camera poses by considering the
complete history of RGB-D input with an efficient hierarchical approach. We
remove the heavy reliance on temporal tracking, and continually localize to the
globally optimized frames instead. We contribute a parallelizable optimization
framework, which employs correspondences based on sparse features and dense
geometric and photometric matching. Our approach estimates globally optimized
(i.e., bundle adjusted) poses in real-time, supports robust tracking with
recovery from gross tracking failures (i.e., relocalization), and re-estimates
the 3D model in real-time to ensure global consistency; all within a single
framework. Our approach outperforms state-of-the-art online systems with
quality on par to offline methods, but with unprecedented speed and scan
completeness. Our framework leads to a comprehensive online scanning solution
for large indoor environments, enabling ease of use and high-quality results."
"Aiming at applications to the scientific visualization of three dimensional
simulations with time evolution, a keyboard based control method to specify
rotations in four dimensions is proposed. It is known that four dimensional
rotations are generally so-called double rotations, and a double rotation is a
combination of simultaneously applied two simple rotations. The proposed method
can specify both the simple and double rotations by single key typings of the
keyboard. The method is tested in visualizations of a regular pentachoron in
four dimensional space by a hyperplane slicing."
"We propose M\""obius transformations as the natural rotation and scaling tools
for editing spherical images. As an application we produce spherical Droste
images. We obtain other self-similar visual effects using rational functions,
elliptic functions, and Schwarz-Christoffel maps."
"Given a simple graph $G=(V,E)$, a subset of $E$ is called a triangle cover if
it intersects each triangle of $G$. Let $\nu_t(G)$ and $\tau_t(G)$ denote the
maximum number of pairwise edge-disjoint triangles in $G$ and the minimum
cardinality of a triangle cover of $G$, respectively. Tuza conjectured in 1981
that $\tau_t(G)/\nu_t(G)\le2$ holds for every graph $G$. In this paper, using a
hypergraph approach, we design polynomial-time combinatorial algorithms for
finding small triangle covers. These algorithms imply new sufficient conditions
for Tuza's conjecture on covering and packing triangles. More precisely,
suppose that the set $\mathscr T_G$ of triangles covers all edges in $G$. We
show that a triangle cover of $G$ with cardinality at most $2\nu_t(G)$ can be
found in polynomial time if one of the following conditions is satisfied: (i)
$\nu_t(G)/|\mathscr T_G|\ge\frac13$, (ii) $\nu_t(G)/|E|\ge\frac14$, (iii)
$|E|/|\mathscr T_G|\ge2$.
  Keywords: Triangle cover, Triangle packing, Linear 3-uniform hypergraphs,
Combinatorial algorithms"
"The class of models that can be represented by STL files is larger than the
class of models that can be printed using additive manufacturing technologies.
In this paper such a gap is formalized while providing an unambiguous
description of all the mathematical entities involved in the modeling-printing
pipeline. Possible defects of an STL file are formally defined and classified,
and a fully automatic procedure is described to turn ""any"" such file into a
printable model. The procedure is as exact as possible, meaning that no visible
distortion is introduced unless it is strictly imposed by limitations of the
printing device. Thanks to such an unprecedented flexibility and accuracy, this
algorithm is expected to significantly simplify the modeling-printing process,
in particular within the continuously emerging non-professional ""maker""
communities."
"Previous saliency detection research required the reader to evaluate
performance qualitatively, based on renderings of saliency maps on a few
shapes. This qualitative approach meant it was unclear which saliency models
were better, or how well they compared to human perception. This paper provides
a quantitative evaluation framework that addresses this issue. In the first
quantitative analysis of 3D computational saliency models, we evaluate four
computational saliency models and two baseline models against ground-truth
saliency collected in previous work."
"Accurately drawing 3D objects is difficult for untrained individuals, as it
requires an understanding of perspective and its effects on geometry and
proportions. Step-by-step tutorials break the complex task of sketching an
entire object down into easy-to-follow steps that even a novice can follow.
However, creating such tutorials requires expert knowledge and is a
time-consuming task. As a result, the availability of tutorials for a given
object or viewpoint is limited. How2Sketch addresses this problem by
automatically generating easy-to-follow tutorials for arbitrary 3D objects.
Given a segmented 3D model and a camera viewpoint,it computes a sequence of
steps for constructing a drawing scaffold comprised of geometric primitives,
which helps the user draw the final contours in correct perspective and
proportion. To make the drawing scaffold easy to construct, the algorithm
solves for an ordering among the scaffolding primitives and explicitly makes
small geometric modifications to the size and location of the object parts to
simplify relative positioning. Technically, we formulate this scaffold
construction as a single selection problem that simultaneously solves for the
ordering and geometric changes of the primitives. We demonstrate our algorithm
for generating tutorials on a variety of man-made objects and evaluate how
easily the tutorials can be followed with a user study."
"Radiological imaging of the prostate is becoming more popular among
researchers and clinicians in searching for diseases, primarily cancer. Scans
might be acquired with different equipment or at different times for prognosis
monitoring, with patient movement between scans, resulting in multiple datasets
that need to be registered. For these cases, we introduce a method for
volumetric registration using curvature flow. Multiple prostate datasets are
mapped to canonical solid spheres, which are in turn aligned and registered
through the use of identified landmarks on or within the gland. Theoretical
proof and experimental results show that our method produces homeomorphisms
with feature constraints. We provide thorough validation of our method by
registering prostate scans of the same patient in different orientations, from
different days and using different modes of MRI. Our method also provides the
foundation for a general group-wise registration using a standard reference,
defined on the complex plane, for any input. In the present context, this can
be used for registering as many scans as needed for a single patient or
different patients on the basis of age, weight or even malignant and
non-malignant attributes to study the differences in general population. Though
we present this technique with a specific application to the prostate, it is
generally applicable for volumetric registration problems."
"Current connectivity diagrams of human brain image data are either overly
complex or overly simplistic. In this work we introduce simple yet accurate
interactive visual representations of multiple brain image structures and the
connectivity among them. We map cortical surfaces extracted from human brain
magnetic resonance imaging (MRI) data onto 2D surfaces that preserve shape
(angle), extent (area), and spatial (neighborhood) information for 2D (circular
disk) and 3D (spherical) mapping, split these surfaces into separate patches,
and cluster functional and diffusion tractography MRI connections between pairs
of these patches. The resulting visualizations are easier to compute on and
more visually intuitive to interact with than the original data, and facilitate
simultaneous exploration of multiple data sets, modalities, and statistical
maps."
"A transformation based on mean curvature is introduced which morphs
triangulated surfaces into round spheres."
"While the problem of image aesthetics has been well explored, the study of 3D
shape aesthetics has focused on specific manually defined features. In this
paper, we learn an aesthetics measure for 3D shapes autonomously from raw voxel
data and without manually-crafted features by leveraging the strength of deep
learning. We collect data from humans on their aesthetics preferences for
various 3D shape classes. We take a deep convolutional 3D shape ranking
approach to compute a measure that gives an aesthetics score for a 3D shape. We
demonstrate our approach with various types of shapes and for applications such
as aesthetics-based visualization, search, and scene composition."
"A pair of pants is a genus zero orientable surface with three boundary
components. A pants decomposition of a surface is a finite collection of
unordered pairwise disjoint simple closed curves embedded in the surface that
decompose the surface into pants. In this paper we present two Morse theory
based algorithms for pants decomposition of a surface mesh. Both algorithms
operates on a choice of an appropriate Morse function on the surface. The first
algorithm uses this Morse function to identify handles that are glued
systematically to obtain a pant decomposition. The second algorithm uses the
Reeb graph of the Morse function to obtain a pant decomposition. Both
algorithms work for surfaces with or without boundaries. Our preliminary
implementation of the two algorithms shows that both algorithms run in much
less time than an existing state-of-the-art method, and the Reeb graph based
algorithm achieves the best time efficiency. Finally, we demonstrate the
robustness of our algorithms against noise."
"This paper is devoted to the construction of polynomial 2-surfaces which
possess a polynomial area element. In particular we study these surfaces in the
Euclidean space $\mathbb R^3$ (where they are equivalent to the PN surfaces)
and in the Minkowski space $\mathbb R^{3,1}$ (where they provide the MOS
surfaces). We show generally in real vector spaces of any dimension and any
metric that the Gram determinant of a parametric set of subspaces is a perfect
square if and only if the Gram determinant of its orthogonal complement is a
perfect square. Consequently the polynomial surfaces of a given degree with
polynomial area element can be constructed from the prescribed normal fields
solving a system of linear equations. The degree of the constructed surface
depending on the degree and the quality of the prescribed normal field is
investigated and discussed. We use the presented approach to interpolate a
network of points and associated normals with piecewise polynomial surfaces
with polynomial area element and demonstrate our method on a number of examples
(constructions of quadrilateral as well as triangular patches"
"Sleep apnea is a syndrome that is characterized by sudden breathing halts
while sleeping. One of the common treatments involves wearing a mask that
delivers continuous air flow into the nostrils so as to maintain a steady air
pressure. These masks are designed for an average facial model and are often
difficult to adjust due to poor fit to the actual patient. The incompatibility
is characterized by gaps between the mask and the face, which deteriorates the
impermeability of the mask and leads to air leakage. We suggest a fully
automatic approach for designing a personalized nasal mask interface using a
facial depth scan. The interfaces generated by the proposed method accurately
fit the geometry of the scanned face, and are easy to manufacture. The proposed
method utilizes cheap commodity depth sensors and 3D printing technologies to
efficiently design and manufacture customized masks for patients suffering from
sleep apnea."
"Automatic estimation of skinning transformations is a popular way to deform a
single reference shape into a new pose by providing a small number of control
parameters. We generalize this approach by efficiently enabling the use of
multiple exemplar shapes. Using a small set of representative natural poses, we
propose to express an unseen appearance by a low-dimensional linear subspace,
specified by a redundant dictionary of weighted vertex positions. Minimizing a
nonlinear functional that regulates the example manifold, the suggested
approach supports local-rigid deformations of articulated objects, as well as
nearly isometric embeddings of smooth shapes. A real-time non-rigid deformation
system is demonstrated, and a shape completion and partial registration
framework is introduced. These applications can recover a target pose and
implicit inverse kinematics from a small number of examples and just a few
vertex positions. The result reconstruction is more accurate compared to
state-of-the-art reduced deformable models."
"We present a system to capture video footage of human subjects in the real
world. Our system leverages a quadrotor camera to automatically capture
well-composed video of two subjects. Subjects are tracked in a large-scale
outdoor environment using RTK GPS and IMU sensors. Then, given the tracked
state of our subjects, our system automatically computes static shots based on
well-established visual composition principles and canonical shots from
cinematography literature. To transition between these static shots, we
calculate feasible, safe, and visually pleasing transitions using a novel
real-time trajectory planning algorithm. We evaluate the performance of our
tracking system, and experimentally show that RTK GPS significantly outperforms
conventional GPS in capturing a variety of canonical shots. Lastly, we
demonstrate our system guiding a consumer quadrotor camera autonomously
capturing footage of two subjects in a variety of use cases. This is the first
end-to-end system that enables people to leverage the mobility of quadrotors,
as well as the knowledge of expert filmmakers, to autonomously capture
high-quality footage of people in the real world."
"Early hands-on experiences with the Microsoft Hololens augmented/mixed
reality device are reported and discussed, with a general aim of exploring
basic 3D visualization. A range of usage cases are tested, including data
visualization and immersive data spaces, in-situ visualization of 3D models and
full scale architectural form visualization. Ultimately, the Hololens is found
to provide a remarkable tool for moving from traditional visualization of 3D
objects on a 2D screen, to fully experiential 3D visualizations embedded in the
real world."
"Geometric model fitting is a fundamental task in computer graphics and
computer vision. However, most geometric model fitting methods are unable to
fit an arbitrary geometric model (e.g. a surface with holes) to incomplete
data, due to that the similarity metrics used in these methods are unable to
measure the rigid partial similarity between arbitrary models. This paper hence
proposes a novel rigid geometric similarity metric, which is able to measure
both the full similarity and the partial similarity between arbitrary geometric
models. The proposed metric enables us to perform partial procedural geometric
model fitting (PPGMF). The task of PPGMF is to search a procedural geometric
model space for the model rigidly similar to a query of non-complete point set.
Models in the procedural model space are generated according to a set of
parametric modeling rules. A typical query is a point cloud. PPGMF is very
useful as it can be used to fit arbitrary geometric models to non-complete
(incomplete, over-complete or hybrid-complete) point cloud data. For example,
most laser scanning data is non-complete due to occlusion. Our PPGMF method
uses Markov chain Monte Carlo technique to optimize the proposed similarity
metric over the model space. To accelerate the optimization process, the method
also employs a novel coarse-to-fine model dividing strategy to reject
dissimilar models in advance. Our method has been demonstrated on a variety of
geometric models and non-complete data. Experimental results show that the
PPGMF method based on the proposed metric is able to fit non-complete data,
while the method based on other metrics is unable. It is also shown that our
method can be accelerated by several times via early rejection."
"A set of bathymetry point clouds acquired by different measurement techniques
at different times, having different accuracy and varying patterns of points,
are approximated by an LR B-spline surface. The aim is to represent the sea
bottom with good accuracy and at the same time reduce the data size
considerably. In this process the point clouds must be cleaned by selecting the
""best"" points for surface generation. This cleaning process is called
deconfliction, and we use a rough approximation of the combined point clouds as
a reference surface to select a consistent set of points. The reference surface
is updated with the selected points to create an accurate approximation. LR
B-splines is the selected surface format due to its suitability for adaptive
refinement and approximation, and its ability to represent local detail without
a global increase in the data size of the surface"
"Many shape analysis methods treat the geometry of an object as a metric space
that can be captured by the Laplace-Beltrami operator. In this paper, we
propose to adapt the classical Hamiltonian operator from quantum mechanics to
the field of shape analysis. To this end we study the addition of a potential
function to the Laplacian as a generator for dual spaces in which shape
processing is performed. We present a general optimization approach for solving
variational problems involving the basis defined by the Hamiltonian using
perturbation theory for its eigenvectors. The suggested operator is shown to
produce better functional spaces to operate with, as demonstrated on different
shape analysis tasks."
"The typical goal of surface remeshing consists in finding a mesh that is (1)
geometrically faithful to the original geometry, (2) as coarse as possible to
obtain a low-complexity representation and (3) free of bad elements that would
hamper the desired application. In this paper, we design an algorithm to
address all three optimization goals simultaneously. The user specifies desired
bounds on approximation error {\delta}, minimal interior angle {\theta} and
maximum mesh complexity N (number of vertices). Since such a desired mesh might
not even exist, our optimization framework treats only the approximation error
bound {\delta} as a hard constraint and the other two criteria as optimization
goals. More specifically, we iteratively perform carefully prioritized local
operators, whenever they do not violate the approximation error bound and
improve the mesh otherwise. In this way our optimization framework greedily
searches for the coarsest mesh with minimal interior angle above {\theta} and
approximation error bounded by {\delta}. Fast runtime is enabled by a local
approximation error estimation, while implicit feature preservation is obtained
by specifically designed vertex relocation operators. Experiments show that our
approach delivers high-quality meshes with implicitly preserved features and
better balances between geometric fidelity, mesh complexity and element quality
than the state-of-the-art."
"Interference detection of arbitrary geometric objects is not a trivial task
due to the heavy computational load imposed by implementation issues. The
hierarchically structured bounding boxes help us to quickly isolate the contour
of segments in interference. In this paper, a new approach is introduced to
treat the interference detection problem involving the representation of
arbitrary shaped objects. Our proposed method relies upon searching for the
best possible way to represent contours by means of hierarchically structured
rectangular oriented bounding boxes. This technique handles 2D objects
boundaries defined by closed B-spline curves with roughness details. Each
oriented box is adapted and fitted to the segments of the contour using second
order statistical indicators from some elements of the segments of the object
contour in a multiresolution framework. Our method is efficient and robust when
it comes to 2D animations in real time. It can deal with smooth curves and
polygonal approximations as well results are present to illustrate the
performance of the new method."
"Development of additive manufacturing in last decade greatly improves tissue
engineering. During the manufacturing of porous scaffold, simplified but
functionally equivalent models are getting focused for practically reasons.
Scaffolds can be classified into regular porous scaffolds and irregular porous
scaffolds. Several methodologies are developed to design these scaffolds. A
novel method is proposed in this paper using anisotropic radial basis function
(ARBF) interpolation. This is method uses geometric models such as volumetric
meshes as input and proves to be flexible because geometric models are able to
capture the characteristics of complex tissues easily. Moreover, this method is
straightforward and easy to implement."
"In this paper, a new approach to solve the cubic B-spline curve fitting
problem is presented based on a meta-heuristic algorithm called "" dolphin
echolocation "". The method minimizes the proximity error value of the selected
nodes that measured using the least squares method and the Euclidean distance
method of the new curve generated by the reverse engineering. The results of
the proposed method are compared with the genetic algorithm. As a result, this
new method seems to be successful."
"In this paper we address the issue of designing developable surfaces with
Bezier patches. We show that developable surfaces with a polynomial edge of
regression are the set of developable surfaces which can be constructed with
Aumann's algorithm. We also obtain the set of polynomial developable surfaces
which can be constructed using general polynomial curves. The conclusions can
be extended to spline surfaces as well."
"We present a data-driven approach that colorizes 3D furniture models and
indoor scenes by leveraging indoor images on the internet. Our approach is able
to colorize the furniture automatically according to an example image. The core
is to learn image-guided mesh segmentation to segment the model into different
parts according to the image object. Given an indoor scene, the system supports
colorization-by-example, and has the ability to recommend the colorization
scheme that is consistent with a user-desired color theme. The latter is
realized by formulating the problem as a Markov random field model that imposes
user input as an additional constraint. We contribute to the community a
hierarchically organized image-model database with correspondences between each
image and the corresponding model at the part-level. Our experiments and a user
study show that our system produces perceptually convincing results comparable
to those generated by interior designers."
"We present SceneSuggest: an interactive 3D scene design system providing
context-driven suggestions for 3D model retrieval and placement. Using a
point-and-click metaphor we specify regions in a scene in which to
automatically place and orient relevant 3D models. Candidate models are ranked
using a set of static support, position, and orientation priors learned from 3D
scenes. We show that our suggestions enable rapid assembly of indoor scenes. We
perform a user study comparing suggestions to manual search and selection, as
well as to suggestions with no automatic orientation. We find that suggestions
reduce total modeling time by 32%, that orientation priors reduce time spent
re-orienting objects by 27%, and that context-driven suggestions reduce the
number of text queries by 50%."
"Animation is ubiquitous in visualization systems, and a common technique for
creating these animations is the transition. In the transition approach,
animations are created by smoothly interpolating a visual attribute between a
start and end value, reaching the end value after a specified duration. This
approach works well when each transition for an attribute is allowed to finish
before the next is triggered, but performs poorly when a new transition is
triggered before the current transition has finished. In particular,
interruptions introduce velocity discontinuities, and frequent interruptions
can slow down the resulting animation. To solve these problems, we model the
problem of animation as a signal processing problem. In our technique,
animations are produced by transformations of signals, or functions over time.
In particular, an animation is produced by transforming an input signal, a
function from time to target attribute value, into an output signal, a function
from time to displayed attribute value. We show that well-known
signal-processing techniques can be applied to produce animations that are free
from velocity discontinuities even when interrupted."
"Virtual Reality, an immersive technology that replicates an environment via
computer-simulated reality, gets a lot of attention in the entertainment
industry. However, VR has also great potential in other areas, like the medical
domain, Examples are intervention planning, training and simulation. This is
especially of use in medical operations, where an aesthetic outcome is
important, like for facial surgeries. Alas, importing medical data into Virtual
Reality devices is not necessarily trivial, in particular, when a direct
connection to a proprietary application is desired. Moreover, most researcher
do not build their medical applications from scratch, but rather leverage
platforms like MeVisLab, MITK, OsiriX or 3D Slicer. These platforms have in
common that they use libraries like ITK and VTK, and provide a convenient
graphical interface. However, ITK and VTK do not support Virtual Reality
directly. In this study, the usage of a Virtual Reality device for medical data
under the MeVisLab platform is presented. The OpenVR library is integrated into
the MeVisLab platform, allowing a direct and uncomplicated usage of the head
mounted display HTC Vive inside the MeVisLab platform. Medical data coming from
other MeVisLab modules can directly be connected per drag-and-drop to the
Virtual Reality module, rendering the data inside the HTC Vive for immersive
virtual reality inspection."
"Multivariate graphs are prolific across many fields, including transportation
and neuroscience. A key task in graph analysis is the exploration of
connectivity, to, for example, analyze how signals flow through neurons, or to
explore how well different cities are connected by flights. While standard
node-link diagrams are helpful in judging connectivity, they do not scale to
large networks. Adjacency matrices also do not scale to large networks and are
only suitable to judge connectivity of adjacent nodes. A key approach to
realize scalable graph visualization are queries: instead of displaying the
whole network, only a relevant subset is shown. Query-based techniques for
analyzing connectivity in graphs, however, can also easily suffer from
cluttering if the query result is big enough. To remedy this, we introduce
techniques that provide an overview of the connectivity and reveal details on
demand. We have two main contributions: (1) two novel visualization techniques
that work in concert for summarizing graph connectivity; and (2) Graffinity, an
open-source implementation of these visualizations supplemented by detail views
to enable a complete analysis workflow. Graffinity was designed in a close
collaboration with neuroscientists and is optimized for connectomics data
analysis, yet the technique is applicable across domains. We validate the
connectivity overview and our open-source tool with illustrative examples using
flight and connectomics data."
"Digital sculpting is a popular means to create 3D models but remains a
challenging task for many users. This can be alleviated by recent advances in
data-driven and procedural modeling, albeit bounded by the underlying data and
procedures. We propose a 3D sculpting system that assists users in freely
creating models without predefined scope. With a brushing interface similar to
common sculpting tools, our system silently records and analyzes users'
workflows, and predicts what they might or should do in the future to reduce
input labor or enhance output quality. Users can accept, ignore, or modify the
suggestions and thus maintain full control and individual style. They can also
explicitly select and clone past workflows over output model regions. Our key
idea is to consider how a model is authored via dynamic workflows in addition
to what it is shaped in static geometry, for more accurate analysis of user
intentions and more general synthesis of shape structures. The workflows
contain potential repetitions for analysis and synthesis, including user inputs
(e.g. pen strokes on a pressure sensing tablet), model outputs (e.g. extrusions
on an object surface), and camera viewpoints. We evaluate our method via user
feedbacks and authored models."
"This paper proposes a new data-driven approach for modeling detailed splashes
for liquid simulations with neural networks. Our model learns to generate
small-scale splash detail for fluid-implicit-particle methods using training
data acquired from physically accurate, high-resolution simulations. We use
neural networks to model the regression of splash formation using a classifier
together with a velocity modification term. More specifically, we employ a
heteroscedastic model for the velocity updates. Our simulation results
demonstrate that our model significantly improves visual fidelity with a large
amount of realistic droplet formation and yields splash detail much more
efficiently than finer discretizations. We show this for two different spatial
scales and simulation setups."
"In this paper, we present a color transfer algorithm to colorize a broad
range of gray images without any user intervention. The algorithm uses a
machine learning-based approach to automatically colorize grayscale images. The
algorithm uses the superpixel representation of the reference color images to
learn the relationship between different image features and their corresponding
color values. We use this learned information to predict the color value of
each grayscale image superpixel. As compared to processing individual image
pixels, our use of superpixels helps us to achieve a much higher degree of
spatial consistency as well as speeds up the colorization process. The
predicted color values of the gray-scale image superpixels are used to provide
a 'micro-scribble' at the centroid of the superpixels. These color scribbles
are refined by using a voting based approach. To generate the final
colorization result, we use an optimization-based approach to smoothly spread
the color scribble across all pixels within a superpixel. Experimental results
on a broad range of images and the comparison with existing state-of-the-art
colorization methods demonstrate the greater effectiveness of the proposed
algorithm."
"Liquids exhibit highly complex, non-linear behavior under changing simulation
conditions such as user interactions. We propose a method to map this complex
behavior over a parameter range onto a reduced representation based on
space-time deformations. In order to represent the complexity of the full space
of inputs, we use aligned deformations from optical flow solves, and we
leverage the power of generative neural networks to synthesize additional
deformations for refinement. We introduce a novel deformation-aware loss
function, which enables optimization in the highly non-linear space of multiple
deformations. To demonstrate the effectiveness of our approach, we showcase the
method with several complex examples in two and four dimensions. Our
representation makes it possible to generate implicit surfaces of liquids very
efficiently, which allows us to very efficiently display the scene from any
angle, and to add secondary effects such as particle systems. We have
implemented a mobile application with our full pipeline to demonstrate that
real-time interaction is possible with our approach."
"We present a novel data-driven algorithm to synthesize high-resolution flow
simulations with reusable repositories of space-time flow data. In our work, we
employ a descriptor learning approach to encode the similarity between fluid
regions with differences in resolution and numerical viscosity. We use
convolutional neural networks to generate the descriptors from fluid data such
as smoke density and flow velocity. At the same time, we present a deformation
limiting patch advection method which allows us to robustly track deformable
fluid regions. With the help of this patch advection, we generate stable
space-time data sets from detailed fluids for our repositories. We can then use
our learned descriptors to quickly localize a suitable data set when running a
new simulation. This makes our approach very efficient, and resolution
independent. We will demonstrate with several examples that our method yields
volumes with very high effective resolutions, and non-dissipative small scale
details that naturally integrate into the motions of the underlying flow."
"We introduce a novel neural network architecture for encoding and synthesis
of 3D shapes, particularly their structures. Our key insight is that 3D shapes
are effectively characterized by their hierarchical organization of parts,
which reflects fundamental intra-shape relationships such as adjacency and
symmetry. We develop a recursive neural net (RvNN) based autoencoder to map a
flat, unlabeled, arbitrary part layout to a compact code. The code effectively
captures hierarchical structures of man-made 3D objects of varying structural
complexities despite being fixed-dimensional: an associated decoder maps a code
back to a full hierarchy. The learned bidirectional mapping is further tuned
using an adversarial setup to yield a generative model of plausible structures,
from which novel structures can be sampled. Finally, our structure synthesis
framework is augmented by a second trained module that produces fine-grained
part geometry, conditioned on global and local structural context, leading to a
full generative pipeline for 3D shapes. We demonstrate that without
supervision, our network learns meaningful structural hierarchies adhering to
perceptual grouping principles, produces compact codes which enable
applications such as shape classification and partial matching, and supports
shape synthesis and interpolation with significant variations in topology and
geometry."
"This paper presents an automated method for 3D character skeleton extraction
that can be applied for generic 3D shapes. Our work is motivated by the
skeleton-based prior work on automatic rigging focused on skeleton extraction
and can automatically aligns the extracted structure to fit the 3D shape of the
given 3D mesh. The body mesh can be subsequently skinned based on the extracted
skeleton and thus enables rigging process. In the experiment, we apply public
dataset to drive the estimated skeleton from different body shapes, as well as
the real data obtained from 3D scanning systems. Satisfactory results are
obtained compared to the existing approaches."
"Image compositing is a popular and successful method used to generate
realistic yet fake imagery. Much previous work in compositing has focused on
improving the appearance compatibility between a given object segment and a
background image. However, most previous work does not investigate the topic of
automatically selecting semantically compatible segments and predicting their
locations and sizes given a background image. In this work, we attempt to fill
this gap by developing a fully automatic compositing system that learns this
information. To simplify the task, we restrict our problem by focusing on human
instance composition, because human segments exhibit strong correlations with
the background scene and are easy to collect. The first problem we investigate
is determining where should a person segment be placed given a background
image, and what should be its size in the background image. We tackle this by
developing a novel Convolutional Neural Network (CNN) model that jointly
predicts the potential location and size of the person segment. The second
problem we investigate is, given the background image, which person segments
(who) can be composited with the previously predicted locations and sizes,
while retaining compatibility with both the local context and the global scene
semantics? To achieve this, we propose an efficient context-based segment
retrieval method that incorporates pre-trained deep feature representations.
  To demonstrate the effectiveness of the proposed compositing system, we
conduct quantitative and qualitative experiments including a user study.
Experimental results show our system can generate composite images that look
semantically and visually convincing. We also develop a proof-of-concept user
interface to demonstrate the potential application of our method."
"Face modeling has been paid much attention in the field of visual computing.
There exist many scenarios, including cartoon characters, avatars for social
media, 3D face caricatures as well as face-related art and design, where
low-cost interactive face modeling is a popular approach especially among
amateur users. In this paper, we propose a deep learning based sketching system
for 3D face and caricature modeling. This system has a labor-efficient
sketching interface, that allows the user to draw freehand imprecise yet
expressive 2D lines representing the contours of facial features. A novel CNN
based deep regression network is designed for inferring 3D face models from 2D
sketches. Our network fuses both CNN and shape based features of the input
sketch, and has two independent branches of fully connected layers generating
independent subsets of coefficients for a bilinear face representation. Our
system also supports gesture based interactions for users to further manipulate
initial face models. Both user studies and numerical results indicate that our
sketching system can help users create face models quickly and effectively. A
significantly expanded face database with diverse identities, expressions and
levels of exaggeration is constructed to promote further research and
evaluation of face modeling techniques."
"The Japanese comic format known as Manga is popular all over the world. It is
traditionally produced in black and white, and colorization is time consuming
and costly. Automatic colorization methods generally rely on greyscale values,
which are not present in manga. Furthermore, due to copyright protection,
colorized manga available for training is scarce. We propose a manga
colorization method based on conditional Generative Adversarial Networks
(cGAN). Unlike previous cGAN approaches that use many hundreds or thousands of
training images, our method requires only a single colorized reference image
for training, avoiding the need of a large dataset. Colorizing manga using
cGANs can produce blurry results with artifacts, and the resolution is limited.
We therefore also propose a method of segmentation and color-correction to
mitigate these issues. The final results are sharp, clear, and in high
resolution, and stay true to the character's original color scheme."
"Blue Brain has pushed high-performance visualization (HPV) to complement its
HPC strategy since its inception in 2007. In 2011, this strategy has been
accelerated to develop innovative visualization solutions through increased
funding and strategic partnerships with other research institutions.
  We present the key elements of this HPV ecosystem, which integrates C++
visualization applications with novel collaborative display systems. We
motivate how our strategy of transforming visualization engines into services
enables a variety of use cases, not only for the integration with high-fidelity
displays, but also to build service oriented architectures, to link into web
applications and to provide remote services to Python applications."
"Performance is a critical challenge in mobile image processing. Given a
reference imaging pipeline, or even human-adjusted pairs of images, we seek to
reproduce the enhancements and enable real-time evaluation. For this, we
introduce a new neural network architecture inspired by bilateral grid
processing and local affine color transforms. Using pairs of input/output
images, we train a convolutional neural network to predict the coefficients of
a locally-affine model in bilateral space. Our architecture learns to make
local, global, and content-dependent decisions to approximate the desired image
transformation. At runtime, the neural network consumes a low-resolution
version of the input image, produces a set of affine transformations in
bilateral space, upsamples those transformations in an edge-preserving fashion
using a new slicing node, and then applies those upsampled transformations to
the full-resolution image. Our algorithm processes high-resolution images on a
smartphone in milliseconds, provides a real-time viewfinder at 1080p
resolution, and matches the quality of state-of-the-art approximation
techniques on a large class of image operators. Unlike previous work, our model
is trained off-line from data and therefore does not require access to the
original operator at runtime. This allows our model to learn complex,
scene-dependent transformations for which no reference implementation is
available, such as the photographic edits of a human retoucher."
"Topological data analysis is an emerging area in exploratory data analysis
and data mining. Its main tool, persistent homology, has become a popular
technique to study the structure of complex, high-dimensional data. In this
paper, we propose a novel method using persistent homology to quantify the
perturbation of features that occur in time-varying graphs. Specifically, we
slice the graph data into temporal snapshots, transform the snapshots into
metric spaces, extract topological features using persistent homology, and
compare those features over time. We provide a visualization that assists in
time-varying graph exploration, and helps to identify patterns of behavior
within the graph. To validate our approach, we examine the stability of these
metric spaces by performing a comparative analysis of topological and metric
space features under perturbations. Finally, we conduct several case studies on
real world data sets and show how our method can find cyclic patterns,
deviations from those patterns, and one-time events in time-varying graphs."
"In this manuscript, we derive optimal conditions for building function
approximations that minimize variance when used as importance sampling
estimators for Monte Carlo integration problems. Particularly, we study the
problem of finding the optimal projection $g$ of an integrand $f$ onto certain
classes of piecewise constant functions, in order to minimize the variance of
the unbiased importance sampling estimator $E_g[f/g]$, as well as the related
problem of finding optimal mixture weights to approximate and importance sample
a target mixture distribution $f = \sum_i \alpha_i f_i$ with components $f_i$
in a family $\mathcal{F}$, through a corresponding mixture of importance
sampling densities $g_i$ that are only approximately proportional to $f_i$. We
further show that in both cases the optimal projection is different from the
commonly used $\ell_1$ projection, and provide an intuitive explanation for the
difference."
"Sketch-based modeling strives to bring the ease and immediacy of drawing to
the 3D world. However, while drawings are easy for humans to create, they are
very challenging for computers to interpret due to their sparsity and
ambiguity. We propose a data-driven approach that tackles this challenge by
learning to reconstruct 3D shapes from one or more drawings. At the core of our
approach is a deep convolutional neural network (CNN) that predicts occupancy
of a voxel grid from a line drawing. This CNN provides us with an initial 3D
reconstruction as soon as the user completes a single drawing of the desired
shape. We complement this single-view network with an updater CNN that refines
an existing prediction given a new drawing of the shape created from a novel
viewpoint. A key advantage of our approach is that we can apply the updater
iteratively to fuse information from an arbitrary number of viewpoints, without
requiring explicit stroke correspondences between the drawings. We train both
CNNs by rendering synthetic contour drawings from hand-modeled shape
collections as well as from procedurally-generated abstract shapes. Finally, we
integrate our CNNs in a minimal modeling interface that allows users to
seamlessly draw an object, rotate it to see its 3D reconstruction, and refine
it by re-drawing from another vantage point using the 3D reconstruction as
guidance. The main strengths of our approach are its robustness to freehand
bitmap drawings, its ability to adapt to different object categories, and the
continuum it offers between single-view and multi-view sketch-based modeling."
"For a monochrome layer $x$ of opacity $0\le o_x\le1 $ placed on another
monochrome layer of opacity 1, the result given by the standard formula is
$$\small\Pi\left({\bf
C}_\varphi\right)=1+\sum_{n=1}^2\left(2-n-(-1)^no_{\chi(\varphi+1)}\right)\left(\chi(n+\varphi-1)-o_{\chi(n+\varphi-1)}\right),$$
the formula being of course explained in detail in this paper. We will
eventually deduce a very simple theorem, generalize it and then see its
validity with alternative formulas to this standard containing the same main
properties here exposed."
"Applications that require Internet access to remote 3D datasets are often
limited by the storage costs of 3D models. Several compression methods are
available to address these limits for objects represented by triangle meshes.
Many CAD and VRML models, however, are represented as quadrilateral meshes or
mixed triangle/quadrilateral meshes, and these models may also require
compression. We present an algorithm for encoding the connectivity of such
quadrilateral meshes, and we demonstrate that by preserving and exploiting the
original quad structure, our approach achieves encodings 30 - 80% smaller than
an approach based on randomly splitting quads into triangles. We present both a
code with a proven worst-case cost of 3 bits per vertex (or 2.75 bits per
vertex for meshes without valence-two vertices) and entropy-coding results for
typical meshes ranging from 0.3 to 0.9 bits per vertex, depending on the
regularity of the mesh. Our method may be implemented by a rule for a
particular splitting of quads into triangles and by using the compression and
decompression algorithms introduced in [Rossignac99] and
[Rossignac&Szymczak99]. We also present extensions to the algorithm to compress
meshes with holes and handles and meshes containing triangles and other
polygons as well as quads."
"We embark in a program of studying the problem of better approximating
surfaces by triangulations(triangular meshes) by considering the approximating
triangulations as finite metric spaces and the target smooth surface as their
Haussdorff-Gromov limit. This allows us to define in a more natural way the
relevant elements, constants and invariants s.a. principal directions and
principal values, Gaussian and Mean curvature, etc. By a ""natural way"" we mean
an intrinsic, discrete, metric definitions as opposed to approximating or
paraphrasing the differentiable notions. In this way we hope to circumvent
computational errors and, indeed, conceptual ones, that are often inherent to
the classical, ""numerical"" approach. In this first study we consider the
problem of determining the Gaussian curvature of a polyhedral surface, by using
the {\em embedding curvature} in the sense of Wald (and Menger). We present two
modalities of employing these definitions for the computation of Gaussian
curvature."
"During early 1980s, the so-called `escape time' method, developed to display
the Julia sets for complex dynamical systems, was exported to quaternions in
order to draw analogous pictures in this wider numerical field. Despite of the
fine results in the complex plane, where all topological configurations of
Julia sets have been successfully displayed, the `escape time' method fails to
render properly the non-filled-in variety of quaternionic Julia sets. So their
digital visualisation remained an open problem for several years. Both the
solution for extending this old method to non-filled-in quaternionic Julia sets
and its implementation into a program are explained here."
"Unlike static documents, version controlled documents are continuously edited
by one or more authors. Such collaborative revision process makes traditional
modeling and visualization techniques inappropriate. In this paper we propose a
new representation based on local space-time smoothing that captures important
revision patterns. We demonstrate the applicability of our framework using
experiments on synthetic and real-world data."
"Using Atomic Force Microscopes (AFM) to manipulate nano-objects is an actual
challenge for surface scientists. Basic haptic interfacesbetween the AFM and
experimentalists have already been implemented. Themulti-sensory renderings
(seeing, hearing and feeling) studied from acognitive point of view increase
the efficiency of the actual interfaces. Toallow the experimentalist to feel
and touch the nano-world, we add mixedrealities between an AFM and a force
feedback device, enriching thus thedirect connection by a modeling engine. We
present in this paper the firstresults from a real-time remote-control handling
of an AFM by our ForceFeedback Gestural Device through the example of the
approach-retract curve."
"The quality of a simulator equipped with a haptic interface is given by the
dynamical properties of its components: haptic interface, simulator and control
system. Some application areas of such kind of simulator like musical
synthesis, animation or more general, instrumental art have specific
requirements as for the ""haptic rendering"" of small movements that go beyond
the usual haptic interfaces allow. Object properties variability and different
situations of object combination represent important aspects of such type of
application which makes that the user can be interested as much in the
restitution of certain global properties of an entire object domain as in the
restitution of properties that are specific to an isolate object. In the
traditional approaches, the usual criteria are founded on the paradigm of
transparency and are related to the impedance error introduced by the technical
aspects of the system. As a general aim, rather than to minimize these effects,
we look to characterize them by physical metaphors conferring to haptic medium
the role of a tool. This positioning leads to firstly analyze the natural human
object interaction as a simplified evolutive system and then considers its
synthesis in the case of the interactive physical simulation. By means of a
frequential method, this approach is presented for some elementary
configurations of the simulator"
"By tooling an spot-illuminated surface to control the flow of specular glints
under motion, one can produce holographic view-dependent imagery. This paper
presents the differential equation that governs the shape of the specular
surfaces, and illustrates how solutions can be constructed for different kinds
of motion, lighting, host surface geometries, and fabrication constraints,
leading to some novel forms of holography."
"Collaborative data consist of ratings relating two distinct sets of objects:
users and items. Much of the work with such data focuses on filtering:
predicting unknown ratings for pairs of users and items. In this paper we focus
on the problem of visualizing the information. Given all of the ratings, our
task is to embed all of the users and items as points in the same Euclidean
space. We would like to place users near items that they have rated (or would
rate) high, and far away from those they would give a low rating. We pose this
problem as a real-valued non-linear Bayesian network and employ Markov chain
Monte Carlo and expectation maximization to find an embedding. We present a
metric by which to judge the quality of a visualization and compare our results
to local linear embedding and Eigentaste on three real-world datasets."
"This paper presents a new progressive compression method for triangular
meshes. This method, in fact, is based on a schema of irregular
multi-resolution analysis and is centered on the optimization of the
rate-distortion trade-off. The quantization precision is adapted to each vertex
during the encoding / decoding process to optimize the rate-distortion
compromise. The Optimization of the treated mesh geometry improves the
approximation quality and the compression ratio at each level of resolution.
The experimental results show that the proposed algorithm gives competitive
results compared to the previous works dealing with the rate-distortion
compromise."
"We implement for comparative purposes the Feynman algorithm within a
C++-based framework for two-layer uniform facet elastic object for real-time
softbody simulation based on physics modeling methods. To facilitate the
comparison, we implement initial timing measurements on the same hardware
against that of Euler integrator in the softbody framework by varying different
algorithm parameters. Due to a relatively large number of such variations we
implement a GLUI-based user-interface to allow for much more finer control over
the simulation process at real-time, which was lacking completely in the
previous versions of the framework. We show our currents results based on the
enhanced framework. The two-layered elastic object consists of inner and outer
elastic mass-spring surfaces and compressible internal pressure. The density of
the inner layer can be set differently from the density of the outer layer; the
motion of the inner layer can be opposite to the motion of the outer layer.
These special features, which cannot be achieved by a single layered object,
result in improved imitation of a soft body, such as tissue's liquid
non-uniform deformation. The inertial behavior of the elastic object is well
illustrated in environments with gravity and collisions with walls, ceiling,
and floor. The collision detection is defined by elastic collision penalty
method and the motion of the object is guided by the Ordinary Differential
Equation computation. Users can interact with the modeled objects, deform them,
and observe the response to their action in real-time and we provide an
extensible framework and its implementation for comparative studies of
different physical-based modeling and integration algorithm implementations."
"The basic objective of data visualization is to provide an efficient
graphical display for summarizing and reasoning about quantitative information.
During the last decades, political science has accumulated a large corpus of
various kinds of data such as comprehensive factbooks and atlases,
characterizing all or most of existing states by multiple and objectively
assessed numerical indicators within certain time lapse. As a consequence,
there exists a continuous trend for political science to gradually become a
more quantitative scientific field and to use quantitative information in the
analysis and reasoning. It is believed that any objective analysis in political
science must be multidimensional and combine various sources of quantitative
information; however, human capabilities for perception of large massifs of
numerical information are limited. Hence, methods and approaches for
visualization of quantitative and qualitative data (and, especially
multivariate data) is an extremely important topic. Data visualization
approaches can be classified into several groups, starting from creating
informative charts and diagrams (statistical graphics and infographics) and
ending with advanced statistical methods for visualizing multidimensional
tables containing both quantitative and qualitative information. In this
article we provide a short review of existing methods of data visualization
methods with applications in political and social science."
"We present the design and prototype implementation of a scientific
visualization language called Zifazah for composing 3D visualizations of
diffusion tensor magnetic resonance imaging (DT-MRI or DTI) data. Unlike
existing tools allowing flexible customization of data visualizations that are
programmer-oriented, we focus on domain scientists as end users in order to
enable them to freely compose visualizations of their scientific data set. We
analyzed end-user descriptions extracted from interviews with neurologists and
physicians conducting clinical practices using DTI about how they would build
and use DTI visualizations to collect syntax and semantics for the language
design, and have discovered the elements and structure of the proposed
language. Zifazah makes use of the initial set of lexical terms and semantics
to provide a declarative language in the spirit of intuitive syntax and usage.
This work contributes three, among others, main design principles for
scientific visualization language design as well as a practice of such language
for DTI visualization with Zifazah. First, Zifazah incorporated visual symbolic
mapping based on color, size and shape, which is a sub-set of Bertin's taxonomy
migrated to scientific visualizations. Second, Zifazah is defined as a spatial
language whereby lexical representation of spatial relationship for 3D object
visualization and manipulations, which is characteristic of scientific data,
can be programmed. Third, built on top of Bertin's semiology, flexible data
encoding specifically for scientific visualizations is integrated in our
language in order to allow end users to achieve optimal visual composition at
their best. Along with sample scripts representative of our language design
features, some new DTI visualizations as the running results created by end
users using the novel visualization language have also been presented."
"Autoplot is software developed for the Virtual Observatories in Heliophysics
to provide intelligent and automated plotting capabilities for many typical
data products that are stored in a variety of file formats or databases.
Autoplot has proven to be a flexible tool for exploring, accessing, and viewing
data resources as typically found on the web, usually in the form of a
directory containing data files with multiple parameters contained in each
file. Data from a data source is abstracted into a common internal data model
called QDataSet. Autoplot is built from individually useful components, and can
be extended and reused to create specialized data handling and analysis
applications and is being used in a variety of science visualization and
analysis applications. Although originally developed for viewing
heliophysics-related time series and spectrograms, its flexible and generic
data representation model makes it potentially useful for the Earth sciences."
"This work presents new methods and algorithms for tracking the shape and
trajectory of moving reflecting obstacles with broken rays, or rays reflecting
at an obstacle. While in tomography the focus of the reconstruction method is
to recover the velocity structure of the domain, the shape and trajectory
reconstruction procedure directly finds the shape and trajectory of the
obstacle. The physical signal carrier for this innovative method are ultrasonic
beams. When the speed of sound is constant, the rays are straight line segments
and the shape and trajectory of moving objects will be reconstructed with
methods based on the travel time equation and ellipsoid geometry. For variable
speed of sound, we start with the eikonal equation and a system of differential
equations that has its origins in acoustics and seismology. In this case, the
rays are curves that are not necessarily straight line segments and we develop
algorithms for shape and trajectory tracking based on the numerical solution of
these equations. We present methods and algorithms for shape and trajectory
tracking of moving obstacles with reflected rays when the location of the
receiver of the reflected ray is not known in advance. The shape and trajectory
tracking method is very efficient because it is not necessary for the reflected
signal to traverse the whole domain or the same path back to the transmitter.
It could be received close to the point of reflection or far away from the
transmitter. This optimizes the energy spent by transmitters for tracking the
object, reduces signal attenuation and improves image resolution. It is a safe
and secure method. We also present algorithms for tracking the shape and
trajectory of absorbing obstacles. The new methods and algorithms for shape and
trajectory tracking enable new applications and an application to one-hop
Internet routing is presented."
"We present a novel sparse modeling approach to non-rigid shape matching using
only the ability to detect repeatable regions. As the input to our algorithm,
we are given only two sets of regions in two shapes; no descriptors are
provided so the correspondence between the regions is not know, nor we know how
many regions correspond in the two shapes. We show that even with such scarce
information, it is possible to establish very accurate correspondence between
the shapes by using methods from the field of sparse modeling, being this, the
first non-trivial use of sparse models in shape correspondence. We formulate
the problem of permuted sparse coding, in which we solve simultaneously for an
unknown permutation ordering the regions on two shapes and for an unknown
correspondence in functional representation. We also propose a robust variant
capable of handling incomplete matches. Numerically, the problem is solved
efficiently by alternating the solution of a linear assignment and a sparse
coding problem. The proposed methods are evaluated qualitatively and
quantitatively on standard benchmarks containing both synthetic and scanned
objects."
"This paper describes a 2D and 3D simulation engine that quantitatively models
the statics, dynamics, and non-linear deformation of heterogeneous soft bodies
in a computationally efficient manner. There is a large body of work simulating
compliant mechanisms. These normally assume small deformations with homogeneous
material properties actuated with external forces. There is also a large body
of research on physically-based deformable objects for applications in computer
graphics with the purpose of generating realistic appearances at the expense of
accuracy. Here we present a simulation framework in which an object may be
composed of any number of interspersed materials with varying properties
(stiffness, density, etc.) to enable true heterogeneous multi-material
simulation. Collisions are handled to prevent self-penetration due to large
deformation, which also allows multiple bodies to interact. A volumetric
actuation method is implemented to impart motion to the structures which opens
the door to the design of novel structures and mechanisms. The simulator was
implemented efficiently such that objects with thousands of degrees of freedom
can be simulated at suitable framerates for user interaction using a single
thread of a typical desktop computer. The code is written in platform agnostic
C++ and is fully open source. This research opens the door to the dynamic
simulation of freeform 3D multi-material mechanisms and objects in a manner
suitable for design automation."
"Optimization-based filtering smoothes an image by minimizing a fidelity
function and simultaneously preserves edges by exploiting a sparse norm penalty
over gradients. It has obtained promising performance in practical problems,
such as detail manipulation, HDR compression and deblurring, and thus has
received increasing attentions in fields of graphics, computer vision and image
processing. This paper derives a new type of image filter called sparse norm
filter (SNF) from optimization-based filtering. SNF has a very simple form,
introduces a general class of filtering techniques, and explains several
classic filters as special implementations of SNF, e.g. the averaging filter
and the median filter. It has advantages of being halo free, easy to implement,
and low time and memory costs (comparable to those of the bilateral filter).
Thus, it is more generic than a smoothing operator and can better adapt to
different tasks. We validate the proposed SNF by a wide variety of applications
including edge-preserving smoothing, outlier tolerant filtering, detail
manipulation, HDR compression, non-blind deconvolution, image segmentation, and
colorization."
"3D model retrieval techniques can be classified as histogram-based,
view-based and graph-based approaches. We propose a hybrid shape descriptor
which combines the global and local radial distance features by utilizing the
histogram-based and view-based approaches respectively. We define an
area-weighted global radial distance with respect to the center of the bounding
sphere of the model and encode its distribution into a 2D histogram as the
global radial distance shape descriptor. We then uniformly divide the bounding
cube of a 3D model into a set of small cubes and define their centers as local
centers. Then, we compute the local radial distance of a point based on the
nearest local center. By sparsely sampling a set of views and encoding the
local radial distance feature on the rendered views by color coding, we extract
the local radial distance shape descriptor. Based on these two shape
descriptors, we develop a hybrid radial distance shape descriptor for 3D model
retrieval. Experiment results show that our hybrid shape descriptor outperforms
several typical histogram-based and view-based approaches."
"We derive new diffusion solutions to the monoenergetic generalized linear
Boltzmann transport equation (GLBE) for the stationary collision density and
scalar flux about an isotropic point source in an infinite $d$-dimensional
absorbing medium with isotropic scattering. We consider both classical
transport theory with exponentially-distributed free paths in arbitrary
dimensions as well as a number of non-classical transport theories
(non-exponential random flights) that describe a broader class of transport
processes within partially-correlated random media. New rigorous asymptotic
diffusion approximations are derived where possible. We also generalize
Grosjean's moment-preserving approach of separating the first (or uncollided)
distribution from the collided portion and approximating only the latter using
diffusion. We find that for any spatial dimension and for many free-path
distributions Grosjean's approach produces compact, analytic approximations
that are, overall, more accurate for high absorption and for small
source-detector separations than either $P_1$ diffusion or rigorous asymptotic
diffusion. These diffusion-based approximations are exact in the first two even
spatial moments, which we derive explicitly for various non-classical transport
types. We also discuss connections between the random-flight-theory derivation
of the Green's function and the discrete spectrum of the transport operator and
report some new observations regarding the discrete eigenvalues of the
transport operator for general dimensions and free-path distributions."
"Background: Cancers are highly heterogeneous with different subtypes. These
subtypes often possess different genetic variants, present different
pathological phenotypes, and most importantly, show various clinical outcomes
such as varied prognosis and response to treatment and likelihood for
recurrence and metastasis. Recently, integrative genomics (or panomics)
approaches are often adopted with the goal of combining multiple types of omics
data to identify integrative biomarkers for stratification of patients into
groups with different clinical outcomes. Results: In this paper we present a
visual analytic system called Interactive Genomics Patient Stratification
explorer (iGPSe) which significantly reduces the computing burden for
biomedical researchers in the process of exploring complicated integrative
genomics data. Our system integrates unsupervised clustering with graph and
parallel sets visualization and allows direct comparison of clinical outcomes
via survival analysis. Using a breast cancer dataset obtained from the The
Cancer Genome Atlas (TCGA) project, we are able to quickly explore different
combinations of gene expression (mRNA) and microRNA features and identify
potential combined markers for survival prediction. Conclusions: Visualization
plays an important role in the process of stratifying given population
patients. Visual tools allowed for the selection of possibly features across
various datasets for the given patient population. We essentially made a case
for visualization for a very important problem in translational informatics."
"Background: Biological data often originate from samples containing mixtures
of subpopulations, corresponding e.g. to distinct cellular phenotypes. However,
identification of distinct subpopulations may be difficult if biological
measurements yield distributions that are not easily separable. Results: We
present Multiresolution Correlation Analysis (MCA), a method for visually
identifying subpopulations based on the local pairwise correlation between
covariates, without needing to define an a priori interaction scale. We
demonstrate that MCA facilitates the identification of differentially regulated
subpopulations in simulated data from a small gene regulatory network, followed
by application to previously published single-cell qPCR data from mouse
embryonic stem cells. We show that MCA recovers previously identified
subpopulations, provides additional insight into the underlying correlation
structure, reveals potentially spurious compartmentalizations, and provides
insight into novel subpopulations. Conclusions: MCA is a useful method for the
identification of subpopulations in low-dimensional expression data, as
emerging from qPCR or FACS measurements. With MCA it is possible to investigate
the robustness of covariate correlations with respect subpopulations,
graphically identify outliers, and identify factors contributing to
differential regulation between pairs of covariates. MCA thus provides a
framework for investigation of expression correlations for genes of interests
and biological hypothesis generation."
"The novel ""Volume-Enclosing Surface exTraction Algorithm"" (VESTA) generates
triangular isosurfaces from computed tomography volumetric images and/or
three-dimensional (3D) simulation data. Here, we present various benchmarks for
GPU-based code implementations of both VESTA and the current state-of-the-art
Marching Cubes Algorithm (MCA). One major result of this study is that VESTA
runs significantly faster than the MCA."
"We present a method of rendering aerial and volumetric graphics using
femtosecond lasers. A high-intensity laser excites a physical matter to emit
light at an arbitrary 3D position. Popular applications can then be explored
especially since plasma induced by a femtosecond laser is safer than that
generated by a nanosecond laser. There are two methods of rendering graphics
with a femtosecond laser in air: Producing holograms using spatial light
modulation technology, and scanning of a laser beam by a galvano mirror. The
holograms and workspace of the system proposed here occupy a volume of up to 1
cm^3; however, this size is scalable depending on the optical devices and their
setup. This paper provides details of the principles, system setup, and
experimental evaluation, and discussions on scalability, design space, and
applications of this system. We tested two laser sources: an adjustable (30-100
fs) laser which projects up to 1,000 pulses per second at energy up to 7 mJ per
pulse, and a 269-fs laser which projects up to 200,000 pulses per second at an
energy up to 50 uJ per pulse. We confirmed that the spatiotemporal resolution
of volumetric displays, implemented with these laser sources, is 4,000 and
200,000 dots per second. Although we focus on laser-induced plasma in air, the
discussion presented here is also applicable to other rendering principles such
as fluorescence and microbubble in solid/liquid materials."
"Algorithms for laying out large graphs have seen significant progress in the
past decade. However, browsing large graphs remains a challenge. Rendering
thousands of graphical elements at once often results in a cluttered image, and
navigating these elements naively can cause disorientation. To address this
challenge we propose a method called GraphMaps, mimicking the browsing
experience of online geographic maps.
  GraphMaps creates a sequence of layers, where each layer refines the previous
one. During graph browsing, GraphMaps chooses the layer corresponding to the
zoom level, and renders only those entities of the layer that intersect the
current viewport. The result is that, regardless of the graph size, the number
of entities rendered at each view does not exceed a predefined threshold, yet
all graph elements can be explored by the standard zoom and pan operations.
  GraphMaps preprocesses a graph in such a way that during browsing, the
geometry of the entities is stable, and the viewer is responsive. Our case
studies indicate that GraphMaps is useful in gaining an overview of a large
graph, and also in exploring a graph on a finer level of detail."
"We present a novel, log-radius profile representation for convex curves and
define a new operation for combining the shape features of curves. Unlike the
standard, angle profile-based methods, this operation accurately combines the
shape features in a visually intuitive manner. This method have implications in
shape analysis as well as in investigating how the brain perceives and
generates curved shapes and motions."
"We examine an algorithm for the visualization of domain-coloured Riemann
surfaces of plane algebraic curves. The approach faithfully reproduces the
topology and the holomorphic structure of the Riemann surface. We discuss how
the algorithm can be implemented efficiently in OpenGL with geometry shaders,
and (less efficiently) even in WebGL with multiple render targets and floating
point textures. While the generation of the surface takes noticeable time in
both implementations, the visualization of a cached Riemann surface mesh is
possible with interactive performance. This allows us to visually explore
otherwise almost unimaginable mathematical objects. As examples, we look at the
complex square root and the folium of Descartes. For the folium of Descartes,
the visualization reveals features of the algebraic curve which are not obvious
from its equation."
"We present a real-time algorithm that finds the Penetration Depth (PD)
between general polygonal models based on iterative and local optimization
techniques. Given an in-collision configuration of an object in configuration
space, we find an initial collision-free configuration using several methods
such as centroid difference, maximally clear configuration, motion coherence,
random configuration, and sampling-based search. We project this configuration
on to a local contact space using a variant of continuous collision detection
algorithm and construct a linear convex cone around the projected
configuration. We then formulate a new projection of the in-collision
configuration onto the convex cone as a Linear Complementarity Problem (LCP),
which we solve using a type of Gauss-Seidel iterative algorithm. We repeat this
procedure until a locally optimal PD is obtained. Our algorithm can process
complicated models consisting of tens of thousands triangles at interactive
rates."
"Digital images are ubiquitous in our modern lives, with uses ranging from
social media to news, and even scientific papers. For this reason, it is
crucial evaluate how accurate people are when performing the task of identify
doctored images. In this paper, we performed an extensive user study evaluating
subjects capacity to detect fake images. After observing an image, users have
been asked if it had been altered or not. If the user answered the image has
been altered, he had to provide evidence in the form of a click on the image.
We collected 17,208 individual answers from 383 users, using 177 images
selected from public forensic databases. Different from other previously
studies, our method propose different ways to avoid lucky guess when evaluating
users answers. Our results indicate that people show inaccurate skills at
differentiating between altered and non-altered images, with an accuracy of
58%, and only identifying the modified images 46.5% of the time. We also track
user features such as age, answering time, confidence, providing deep analysis
of how such variables influence on the users' performance."
"This paper presents a generalized integrated framework of semi-automatic
surgical template design. Several algorithms were implemented including the
mesh segmentation, offset surface generation, collision detection, ruled
surface generation, etc., and a special software named TemDesigner was
developed. With a simple user interface, a customized template can be semi-
automatically designed according to the preoperative plan. Firstly, mesh
segmentation with signed scalar of vertex is utilized to partition the inner
surface from the input surface mesh based on the indicated point loop. Then,
the offset surface of the inner surface is obtained through contouring the
distance field of the inner surface, and segmented to generate the outer
surface. Ruled surface is employed to connect inner and outer surfaces.
Finally, drilling tubes are generated according to the preoperative plan
through collision detection and merging. It has been applied to the template
design for various kinds of surgeries, including oral implantology, cervical
pedicle screw insertion, iliosacral screw insertion and osteotomy,
demonstrating the efficiency, functionality and generality of our method."
"The fractal structure of real world objects is often analyzed using digital
images. In this context, the compression fractal dimension is put forward. It
provides a simple method for the direct estimation of the dimension of fractals
stored as digital image files. The computational scheme can be implemented
using readily available free software. Its simplicity also makes it very
interesting for introductory elaborations of basic concepts of fractal
geometry, complexity, and information theory. A test of the computational
scheme using limited-quality images of well-defined fractal sets obtained from
the Internet and free software has been performed. Also, a systematic
evaluation of the proposed method using computer generated images of the
Weierstrass cosine function shows an accuracy comparable to those of the
methods most commonly used to estimate the dimension of fractal data sequences
applied to the same test problem."
"This work suggests a new variational approach to the task of computer aided
restoration of incomplete characters, residing in a highly noisy document. We
model character strokes as the movement of a pen with a varying radius.
Following this model, a cubic spline representation is being utilized to
perform gradient descent steps, while maintaining interpolation at some initial
(manually sampled) points. The proposed algorithm was utilized in the process
of restoring approximately 1000 ancient Hebrew characters (dating to ca.
8th-7th century BCE), some of which are presented herein and show that the
algorithm yields plausible results when applied on deteriorated documents."
"Many graphics and vision problems can be expressed as non-linear least
squares optimizations of objective functions over visual data, such as images
and meshes. The mathematical descriptions of these functions are extremely
concise, but their implementation in real code is tedious, especially when
optimized for real-time performance on modern GPUs in interactive applications.
In this work, we propose a new language, Opt (available under
http://optlang.org), for writing these objective functions over image- or
graph-structured unknowns concisely and at a high level. Our compiler
automatically transforms these specifications into state-of-the-art GPU solvers
based on Gauss-Newton or Levenberg-Marquardt methods. Opt can generate
different variations of the solver, so users can easily explore tradeoffs in
numerical precision, matrix-free methods, and solver approaches. In our
results, we implement a variety of real-world graphics and vision applications.
Their energy functions are expressible in tens of lines of code, and produce
highly-optimized GPU solver implementations. These solver have performance
competitive with the best published hand-tuned, application-specific GPU
solvers, and orders of magnitude beyond a general-purpose auto-generated
solver."
"With the advancement in the digital camera technology, the use of high
resolution images and videos has been widespread in the modern society. In
particular, image and video frame registration is frequently applied in
computer graphics and film production. However, the conventional registration
approaches usually require long computational time for high quality images and
video frames. This hinders the applications of the registration approaches in
the modern industries. In this work, we propose a novel approach called {\em
TRIM} to accelerate the computations of the registration by triangulating the
images. More specifically, given a high resolution image or video frame, we
compute an optimal coarse triangulation which captures the important features
of the image. Then, the computation of the registration can be simplified with
the aid of the coarse triangulation. Experimental results suggest that the
computational time of the registration is significantly reduced using our
triangulation-based approach, meanwhile the accuracy of the registration is
well retained when compared with the conventional grid-based approach."
"In order to avoid the curse of dimensionality, frequently encountered in Big
Data analysis, there was a vast development in the field of linear and
non-linear dimension reduction techniques in recent years. These techniques
(sometimes referred to as manifold learning) assume that the scattered input
data is lying on a lower dimensional manifold, thus the high dimensionality
problem can be overcome by learning the lower dimensionality behavior. However,
in real life applications, data is often very noisy. In this work, we propose a
method to approximate a $d$-dimensional $C^{m+1}$ smooth submanifold
$\mathcal{M}$ residing in $\mathbb{R}^n$ ($d << n$) based upon scattered data
points (i.e., a data cloud). We assume that the data points are located ""near""
the noisy lower dimensional manifold and perform a non-linear moving
least-squares projection on an approximating manifold. Under some mild
assumptions, the resulting approximant is shown to be infinitely smooth and of
high approximation order (i.e., $O(h^{m+1})$, where $h$ is the fill distance
and $m$ is the degree of the local polynomial approximation). Furthermore, the
method presented here assumes no analytic knowledge of the approximated
manifold and the approximation algorithm is linear in the large dimension $n$."
"In this paper, we are concerned with geometric constraint solvers, i.e., with
programs that find one or more solutions of a geometric constraint problem. If
no solution exists, the solver is expected to announce that no solution has
been found. Owing to the complexity, type or difficulty of a constraint
problem, it is possible that the solver does not find a solution even though
one may exist. Thus, there may be false negatives, but there should never be
false positives. Intuitively, the ability to find solutions can be considered a
measure of solver's competence. We consider static constraint problems and
their solvers. We do not consider dynamic constraint solvers, also known as
dynamic geometry programs, in which specific geometric elements are moved,
interactively or along prescribed trajectories, while continually maintaining
all stipulated constraints. However, if we have a solver for static constraint
problems that is sufficiently fast and competent, we can build a dynamic
geometry program from it by solving the static problem for a sufficiently dense
sampling of the trajectory of the moving element(s). The work we survey has its
roots in applications, especially in mechanical computer-aided design (MCAD).
The constraint solvers used in MCAD took a quantum leap in the 1990s. These
approaches solve a geometric constraint problem by an initial, graph-based
structural analysis that extracts generic subproblems and determines how they
would combine to form a complete solution. These subproblems are then handed to
an algebraic solver that solves the specific instances of the generic
subproblems and combines them."
"Interactions play a key role in understanding objects and scenes, for both
virtual and real world agents. We introduce a new general representation for
proximal interactions among physical objects that is agnostic to the type of
objects or interaction involved. The representation is based on tracking
particles on one of the participating objects and then observing them with
sensors appropriately placed in the interaction volume or on the interaction
surfaces. We show how to factorize these interaction descriptors and project
them into a particular participating object so as to obtain a new functional
descriptor for that object, its interaction landscape, capturing its observed
use in a spatio-temporal framework. Interaction landscapes are independent of
the particular interaction and capture subtle dynamic effects in how objects
move and behave when in functional use. Our method relates objects based on
their function, establishes correspondences between shapes based on functional
key points and regions, and retrieves peer and partner objects with respect to
an interaction."
"Recent developments in elastic shape analysis (ESA) are motivated by the fact
that it provides comprehensive frameworks for simultaneous registration,
deformation, and comparison of shapes. These methods achieve computational
efficiency using certain square-root representations that transform invariant
elastic metrics into Euclidean metrics, allowing for applications of standard
algorithms and statistical tools. For analyzing shapes of embeddings of
$\mathbb{S}^2$ in $\mathbb{R}^3$, Jermyn et al. introduced square-root normal
fields (SRNFs) that transformed an elastic metric, with desirable invariant
properties, into the $\mathbb{L}^2$ metric. These SRNFs are essentially surface
normals scaled by square-roots of infinitesimal area elements. A critical need
in shape analysis is to invert solutions (deformations, averages, modes of
variations, etc) computed in the SRNF space, back to the original surface space
for visualizations and inferences. Due to the lack of theory for understanding
SRNFs maps and their inverses, we take a numerical approach and derive an
efficient multiresolution algorithm, based on solving an optimization problem
in the surface space, that estimates surfaces corresponding to given SRNFs.
This solution is found effective, even for complex shapes, e.g. human bodies
and animals, that undergo significant deformations including bending and
stretching. Specifically, we use this inversion for computing elastic shape
deformations, transferring deformations, summarizing shapes, and for finding
modes of variability in a given collection, while simultaneously registering
the surfaces. We demonstrate the proposed algorithms using a statistical
analysis of human body shapes, classification of generic surfaces and analysis
of brain structures."
"Recently, methods have been proposed that perform texture synthesis and style
transfer by using convolutional neural networks (e.g. Gatys et al.
[2015,2016]). These methods are exciting because they can in some cases create
results with state-of-the-art quality. However, in this paper, we show these
methods also have limitations in texture quality, stability, requisite
parameter tuning, and lack of user controls. This paper presents a multiscale
synthesis pipeline based on convolutional neural networks that ameliorates
these issues. We first give a mathematical explanation of the source of
instabilities in many previous approaches. We then improve these instabilities
by using histogram losses to synthesize textures that better statistically
match the exemplar. We also show how to integrate localized style losses in our
multiscale framework. These losses can improve the quality of large features,
improve the separation of content and style, and offer artistic controls such
as paint by numbers. We demonstrate that our approach offers improved quality,
convergence in fewer iterations, and more stability over the optimization."
"Unlike the conventional first-order network (FoN), the higher-order network
(HoN) provides a more accurate description of transitions by creating
additional nodes to encode higher-order dependencies. However, there exists no
visualization and exploration tool for the HoN. For applications such as the
development of strategies to control species invasion through global shipping
which is known to exhibit higher-order dependencies, the existing FoN
visualization tools are limited. In this paper, we present HoNVis, a novel
visual analytics framework for exploring higher-order dependencies of the
global ocean shipping network. Our framework leverages coordinated multiple
views to reveal the network structure at three levels of detail (i.e., the
global, local, and individual port levels). Users can quickly identify ports of
interest at the global level and specify a port to investigate its higher-order
nodes at the individual port level. Investigating a larger-scale impact is
enabled through the exploration of HoN at the local level. Using the global
ocean shipping network data, we demonstrate the effectiveness of our approach
with a real-world use case conducted by domain experts specializing in species
invasion. Finally, we discuss the generalizability of this framework to other
real-world applications such as information diffusion in social networks and
epidemic spreading through air transportation."
"Designing 3D scenes is currently a creative task that requires significant
expertise and effort in using complex 3D design interfaces. This effortful
design process starts in stark contrast to the easiness with which people can
use language to describe real and imaginary environments. We present SceneSeer:
an interactive text to 3D scene generation system that allows a user to design
3D scenes using natural language. A user provides input text from which we
extract explicit constraints on the objects that should appear in the scene.
Given these explicit constraints, the system then uses a spatial knowledge base
learned from an existing database of 3D scenes and 3D object models to infer an
arrangement of the objects forming a natural scene matching the input
description. Using textual commands the user can then iteratively refine the
created scene by adding, removing, replacing, and manipulating objects. We
evaluate the quality of 3D scenes generated by SceneSeer in a perceptual
evaluation experiment where we compare against manually designed scenes and
simpler baselines for 3D scene generation. We demonstrate how the generated
scenes can be iteratively refined through simple natural language commands."
"Despite being vastly ignored in the literature, coping with topological noise
is an issue of increasing importance, especially as a consequence of the
increasing number and diversity of 3D polygonal models that are captured by
devices of different qualities or synthesized by algorithms of different
stabilities. One approach for matching 3D shapes under topological noise is to
replace the topology-sensitive geodesic distance with distances that are less
sensitive to topological changes. We propose an alternative approach utilising
gradual deflation (or inflation) of the shape volume, of which purpose is to
bring the pair of shapes to be matched to a \emph{comparable} topology before
the search for correspondences. Illustrative experiments using different
datasets demonstrate that as the level of topological noise increases, our
approach outperforms the other methods in the literature."
"This work presents an evaluation study using a force feedback evaluation
framework for a novel direct needle force volume rendering concept in the
context of liver puncture simulation. PTC/PTCD puncture interventions targeting
the bile ducts have been selected to illustrate this concept. The haptic
algorithms of the simulator system are based on (1) partially segmented patient
image data and (2) a non-linear spring model effective at organ borders. The
primary aim is to quantitatively evaluate force errors caused by our patient
modeling approach, in comparison to haptic force output obtained from using
gold-standard, completely manually-segmented data. The evaluation of the force
algorithms compared to a force output from fully manually segmented
gold-standard patient models, yields a low mean of 0.12 N root mean squared
force error and up to 1.6 N for systematic maximum absolute errors. Force
errors were evaluated on 31,222 preplanned test paths from 10 patients. Only
twelve percent of the emitted forces along these paths were affected by errors.
This is the first study evaluating haptic algorithms with deformable virtual
patients in silico. We prove haptic rendering plausibility on a very high
number of test paths. Important errors are below just noticeable differences
for the hand-arm system."
"Infographics are complex graphic designs integrating text, images, charts and
sketches. Despite the increasing popularity of infographics and the rapid
growth of online design portfolios, little research investigates how we can
take advantage of these design resources. In this paper we present a method for
measuring the style similarity between infographics. Based on human perception
data collected from crowdsourced experiments, we use computer vision and
machine learning algorithms to learn a style similarity metric for infographic
designs. We evaluate different visual features and learning algorithms and find
that a combination of color histograms and Histograms-of-Gradients (HoG)
features is most effective in characterizing the style of infographics. We
demonstrate our similarity metric on a preliminary image retrieval test."
"We present ShapeNet: a richly-annotated, large-scale repository of shapes
represented by 3D CAD models of objects. ShapeNet contains 3D models from a
multitude of semantic categories and organizes them under the WordNet taxonomy.
It is a collection of datasets providing many semantic annotations for each 3D
model such as consistent rigid alignments, parts and bilateral symmetry planes,
physical sizes, keywords, as well as other planned annotations. Annotations are
made available through a public web-based interface to enable data
visualization of object attributes, promote data-driven geometric analysis, and
provide a large-scale quantitative benchmark for research in computer graphics
and vision. At the time of this technical report, ShapeNet has indexed more
than 3,000,000 models, 220,000 models out of which are classified into 3,135
categories (WordNet synsets). In this report we describe the ShapeNet effort as
a whole, provide details for all currently available datasets, and summarize
future plans."
"In this paper a method is introduced, to visualize bifurcated stent grafts in
CT-Data. The aim is to improve therapy planning for minimal invasive treatment
of abdominal aortic aneurysms (AAA). Due to precise measurement of the
abdominal aortic aneurysm and exact simulation of the bifurcated stent graft,
physicians are supported in choosing a suitable stent prior to an intervention.
The presented method can be used to measure the dimensions of the abdominal
aortic aneurysm as well as simulate a bifurcated stent graft. Both of these
procedures are based on a preceding segmentation and skeletonization of the
aortic, right and left iliac. Using these centerlines (aortic, right and left
iliac) a bifurcated initial stent is constructed. Through the implementation of
an ACM method the initial stent is fit iteratively to the vessel walls - due to
the influence of external forces (distance- as well as balloonforce). Following
the fitting process, the crucial values for choosing a bifurcated stent graft
are measured, e.g. aortic diameter, right and left common iliac diameter,
minimum diameter of distal neck. The selected stent is then simulated to the
CT-Data - starting with the initial stent. It hereby becomes apparent if the
dimensions of the bifurcated stent graft are exact, i.e. the fitting to the
arteries was done properly and no ostium was covered."
"This paper presents an approach to a time-dependent variant of the concept of
vector field topology for 2-D vector fields. Vector field topology is defined
for steady vector fields and aims at discriminating the domain of a vector
field into regions of qualitatively different behaviour. The presented approach
represents a generalization for saddle-type critical points and their
separatrices to unsteady vector fields based on generalized streak lines, with
the classical vector field topology as its special case for steady vector
fields. The concept is closely related to that of Lagrangian coherent
structures obtained as ridges in the finite-time Lyapunov exponent field. The
proposed approach is evaluated on both 2-D time-dependent synthetic and vector
fields from computational fluid dynamics."