summary
"Simple economic and performance arguments suggest appropriate lifetimes for
main memory pages and suggest optimal page sizes. The fundamental tradeoffs are
the prices and bandwidths of RAMs and disks. The analysis indicates that with
today's technology, five minutes is a good lifetime for randomly accessed
pages, one minute is a good lifetime for two-pass sequentially accessed pages,
and 16 KB is a good size for index pages. These rules-of-thumb change in
predictable ways as technology ratios change. They also motivate the importance
of the new Kaps, Maps, Scans, and $/Kaps, $/Maps, $/TBscan metrics."
"We study a set of linear transformations on the Fourier series representation
of a sequence that can be used as the basis for similarity queries on
time-series data. We show that our set of transformations is rich enough to
formulate operations such as moving average and time warping. We present a
query processing algorithm that uses the underlying R-tree index of a
multidimensional data set to answer similarity queries efficiently. Our
experiments show that the performance of this algorithm is competitive to that
of processing ordinary (exact match) queries using the index, and much faster
than sequential scanning. We relate our transformations to the general
framework for similarity queries of Jagadish et al."
"We propose an improvement of the known DFT-based indexing technique for fast
retrieval of similar time sequences. We use the last few Fourier coefficients
in the distance computation without storing them in the index since every
coefficient at the end is the complex conjugate of a coefficient at the
beginning and as strong as its counterpart. We show analytically that this
observation can accelerate the search time of the index by more than a factor
of two. This result was confirmed by our experiments, which were carried out on
real stock prices and synthetic data."
"We identify two unreasonable, though standard, assumptions made by database
query optimizers that can adversely affect the quality of the chosen evaluation
plans. One assumption is that it is enough to optimize for the expected
case---that is, the case where various parameters (like available memory) take
on their expected value. The other assumption is that the parameters are
constant throughout the execution of the query. We present an algorithm based
on the ``System R''-style query optimization algorithm that does not rely on
these assumptions. The algorithm we present chooses the plan of the least
expected cost instead of the plan of least cost given some fixed value of the
parameters. In execution environments that exhibit a high degree of
variability, our techniques should result in better performance."
"Complex queries are becoming commonplace, with the growing use of decision
support systems. These complex queries often have a lot of common
sub-expressions, either within a single query, or across multiple such queries
run as a batch. Multi-query optimization aims at exploiting common
sub-expressions to reduce evaluation cost. Multi-query optimization has
hither-to been viewed as impractical, since earlier algorithms were exhaustive,
and explore a doubly exponential search space.
  In this paper we demonstrate that multi-query optimization using heuristics
is practical, and provides significant benefits. We propose three cost-based
heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple
modifications to the Volcano search strategy, and a greedy heuristic. Our
greedy heuristic incorporates novel optimizations that improve efficiency
greatly. Our algorithms are designed to be easily added to existing optimizers.
We present a performance study comparing the algorithms, using workloads
consisting of queries from the TPC-D benchmark. The study shows that our
algorithms provide significant benefits over traditional optimization, at a
very acceptable overhead in optimization time."
"XML is becoming the most relevant new standard for data representation and
exchange on the WWW. Novel languages for extracting and restructuring the XML
content have been proposed, some in the tradition of database query languages
(i.e. SQL, OQL), others more closely inspired by XML. No standard for XML query
language has yet been decided, but the discussion is ongoing within the World
Wide Web Consortium and within many academic institutions and Internet-related
major companies. We present a comparison of five, representative query
languages for XML, highlighting their common features and differences."
"In data warehouse and data mart systems, queries often take a long time to
execute due to their complex nature. Query response times can be greatly
improved by caching final/intermediate results of previous queries, and using
them to answer later queries. In this paper we describe a caching system called
Exchequer which incorporates several novel features including optimization
aware cache maintenance and the use of a cache aware optimizer. In contrast, in
existing work, the module that makes cost-benefit decisions is part of the
cache manager and works independent of the optimizer which essentially
reconsiders these decisions while finding the best plan for a query. In our
work, the optimizer takes the decisions for the cache manager. Furthermore,
existing approaches are either restricted to cube (slice/point) queries, or
cache just the query results. On the other hand, our work is extens ible and in
fact presents a data-model independent framework and algorithm. Our
experimental results attest to the efficacy of our cache management techniques
and show that over a wide range of parameters (a) Exchequer's query response
times are lower by more than 30% compared to the best performing competitor,
and (b) Exchequer can deliver the same response time as its competitor with
just one tenth of the cache size."
"Because the presence of views enhances query performance, materialized views
are increasingly being supported by commercial database/data warehouse systems.
Whenever the data warehouse is updated, the materialized views must also be
updated. However, whereas the amount of data entering a warehouse, the query
loads, and the need to obtain up-to-date responses are all increasing, the time
window available for making the warehouse up-to-date is shrinking. These trends
necessitate efficient techniques for the maintenance of materialized views.
  In this paper, we show how to find an efficient plan for maintenance of a
{\em set} of views, by exploiting common subexpressions between different view
maintenance expressions. These common subexpressions may be materialized
temporarily during view maintenance. Our algorithms also choose
subexpressions/indices to be materialized permanently (and maintained along
with other materialized views), to speed up view maintenance. While there has
been much work on view maintenance in the past, our novel contributions lie in
exploiting a recently developed framework for multiquery optimization to
efficiently find good view maintenance plans as above. In addition to faster
view maintenance, our algorithms can also be used to efficiently select
materialized views to speed up workloads containing queries."
"Recent trends in information management involve the periodic transcription of
data onto secondary devices in a networked environment, and the proper
scheduling of these transcriptions is critical for efficient data management.
To assist in the scheduling process, we are interested in modeling the
reduction of consistency over time between a relation and its replica, termed
obsolescence of data. The modeling is based on techniques from the field of
stochastic processes, and provides several stochastic models for content
evolution in the base relations of a database, taking referential integrity
constraints into account. These models are general enough to accommodate most
of the common scenarios in databases, including batch insertions and life spans
both with and without memory. As an initial ""proof of concept"" of the
applicability of our approach, we validate the insertion portion of our model
framework via experiments with real data feeds. We also discuss a set of
transcription protocols which make use of the proposed stochastic model."
"Queries involving aggregation are typical in database applications. One of
the main ideas to optimize the execution of an aggregate query is to reuse
results of previously answered queries. This leads to the problem of rewriting
aggregate queries using views. Due to a lack of theory, algorithms for this
problem were rather ad-hoc. They were sound, but were not proven to be
complete.
  Recently we have given syntactic characterizations for the equivalence of
aggregate queries and applied them to decide when there exist rewritings.
However, these decision procedures do not lend themselves immediately to an
implementation. In this paper, we present practical algorithms for rewriting
queries with $\COUNT$ and $\SUM$. Our algorithms are sound. They are also
complete for important cases. Our techniques can be used to improve well-known
procedures for rewriting non-aggregate queries. These procedures can then be
adapted to obtain algorithms for rewriting queries with $\MIN$ and $\MAX$. The
algorithms presented are a basis for realizing optimizers that rewrite queries
using views."
"EquiX is a search language for XML that combines the power of querying with
the simplicity of searching. Requirements for such languages are discussed and
it is shown that EquiX meets the necessary criteria. Both a graphical abstract
syntax and a formal concrete syntax are presented for EquiX queries. In
addition, the semantics is defined and an evaluation algorithm is presented.
The evaluation algorithm is polynomial under combined complexity.
  EquiX combines pattern matching, quantification and logical expressions to
query both the data and meta-data of XML documents. The result of a query in
EquiX is a set of XML documents. A DTD describing the result documents is
derived automatically from the query."
"The Harland document management system implements a data model in which
document (object) structure can be altered by mixin-style multiple inheritance
at any time. This kind of structural fluidity has long been supported by
knowledge-base management systems, but its use has primarily been in support of
reasoning and inference. In this paper, we report our experiences building and
supporting several non-trivial applications on top of this data model. Based on
these experiences, we argue that structural fluidity is convenient for
data-intensive applications other than knowledge-base management. Specifically,
we suggest that this flexible data model is a natural fit for the decoupled
programming methodology that arises naturally when using enterprise component
frameworks."
"The need for Knowledge and Data Discovery Management Systems (KDDMS) that
support ad hoc data mining queries has been long recognized. A significant
amount of research has gone into building tightly coupled systems that
integrate association rule mining with database systems. In this paper, we
describe a seamless integration scheme for database queries and association
rule discovery using a common query optimizer for both. Query trees of
expressions in an extended algebra are used for internal representation in the
optimizer. The algebraic representation is flexible enough to deal with
constrained association rule queries and other variations of association rule
specifications. We propose modularization to simplify the query tree for
complex tasks in data mining. It paves the way for making use of existing
algorithms for constructing query plans in the optimization process. How the
integration scheme we present will facilitate greater user control over the
data mining process is also discussed. The work described in this paper forms
part of a larger project for fully integrating data mining with database
management."
"This paper looks at five U.S. cities (Austin, Cleveland, Nashville, Portland,
and Washington, DC) and explores strategies being employed by community
activists and local governments to create and sustain community networking
projects. In some cities, community networking initiatives are relatively
mature, while in others they are in early or intermediate stages. The paper
looks at several factors that help explain the evolution of community networks
in cities:
  1) Local government support; 2) Federal support 3) Degree of community
activism, often reflected by public-private partnerships that help support
community networks.
  In addition to these (more or less) measurable elements of local support, the
case studies enable description of the different objectives of community
networks in different cities. Several community networking projects aim to
improve the delivery of government services (e.g., Portland and Cleveland),
some have a job-training focus (e.g., Austin, Washington, DC), others are
oriented very explicitly toward community building (Nashville, DC), and others
toward neighborhood entrepreneurship (Portland and Cleveland).
  The paper ties the case studies together by asking whether community
technology initiatives contribute to social capital in the cities studied."
"Large organizations today are being served by different types of data
processing and informations systems, ranging from the operational (OLTP)
systems, data warehouse systems, to data mining and business intelligence
applications. It is important to create an integrated repository of what these
systems contain and do in order to use them collectively and effectively. The
repository contains metadata of source systems, data warehouse, and also the
business metadata. Decision support and business analysis require extensive and
in-depth understanding of business entities, tasks, rules and the environment.
The purpose of business metadata is to provide this understanding. Realizing
the importance of metadata, many standardization efforts has been initiated to
define metadata models. In trying to define an integrated metadata and
information systems for a banking application, we discover some important
limitations or inadequacies of the business metadata proposals. They relate to
providing an integrated and flexible inter-operability and navigation between
metadata and data, and to the important issue of systematically handling
temporal characteristics and evolution of the metadata itself.
  In this paper, we study the issue of structuring business metadata so that it
can provide a context for business management and decision support when
integrated with data warehousing. We define temporal object-oriented business
metadata model, and relate it both to the technical metadata and the data
warehouse. We also define ways of accessing and navigating metadata in
conjunction with data."
"EquiX is a search language for XML that combines the power of querying with
the simplicity of searching. Requirements for such languages are discussed and
it is shown that EquiX meets the necessary criteria. Both a graph-based
abstract syntax and a formal concrete syntax are presented for EquiX queries.
In addition, the semantics is defined and an evaluation algorithm is presented.
The evaluation algorithm is polynomial under combined complexity.
  EquiX combines pattern matching, quantification and logical expressions to
query both the data and meta-data of XML documents. The result of a query in
EquiX is a set of XML documents. A DTD describing the result documents is
derived automatically from the query."
"The web, through many search engine sites, has popularized the keyword-based
search paradigm, where a user can specify a string of keywords and expect to
retrieve relevant documents, possibly ranked by their relevance to the query.
Since a lot of information is stored in databases (and not as HTML documents),
it is important to provide a similar search paradigm for databases, where users
can query a database without knowing the database schema and database query
languages such as SQL. In this paper, we propose such a database search system,
which accepts a free-form query as a collection of keywords, translates it into
queries on the database using the database metadata, and presents query results
in a well-structured and browsable form. Th eysytem maps keywords onto the
database schema and uses inter-relationships (i.e., data semantics) among the
referred tables to generate meaningful query results. We also describe our
prototype for database search, called Mragyati. Th eapproach proposed here is
scalable, as it does not build an in-memory graph of the entire database for
searching for relationships among the objects selected by the user's query."
"The Relational Database Aspects of Argonnes ATLAS Control System Argonnes
ATLAS (Argonne Tandem Linac Accelerator System) control system comprises two
separate database concepts. The first is the distributed real-time database
structure provided by the commercial product Vsystem [1]. The second is a more
static relational database archiving system designed by ATLAS personnel using
Oracle Rdb [2] and Paradox [3] software. The configuration of the ATLAS
facility has presented a unique opportunity to construct a control system
relational database that is capable of storing and retrieving complete archived
tune-up configurations for the entire accelerator. This capability has been a
major factor in allowing the facility to adhere to a rigorous operating
schedule. Most recently, a Web-based operator interface to the control systems
Oracle Rdb database has been installed. This paper explains the history of the
ATLAS database systems, how they interact with each other, the design of the
new Web-based operator interface, and future plans."
"Since Self-Describing Data Sets (SDDS) were first introduced, the source code
has been ported to many different operating systems and various languages. SDDS
is now available in C, Tcl, Java, Fortran, and Python. All of these versions
are supported on Solaris, Linux, and Windows. The C version of SDDS is also
supported on VxWorks. With the recent addition of the Java port, SDDS can now
be deployed on virtually any operating system. Due to this proliferation, SDDS
files serve to link not only a collection of C programs, but programs and
scripts in many languages on different operating systems. The platform
independent binary feature of SDDS also facilitates portability among operating
systems. This paper presents an overview of various benefits of SDDS platform
interoperability."
"In optimizing queries, solutions based on AND/OR DAG can generate all
possible join orderings and select placements before searching for optimal
query execution strategy. But as the number of joins and selection conditions
increase, the space and time complexity to generate optimal query plan
increases exponentially. In this paper, we use join graph for a relational
database schema to either pre-compute all possible join orderings that can be
executed and store it as a join DAG or, extract joins in the queries to
incrementally build a history join DAG as and when the queries are executed.
The select conditions in the queries are appropriately placed in the retrieved
join DAG (or, history join DAG) to generate optimal query execution strategy.
We experimentally evaluate our query optimization technique on TPC-D/H query
sets to show their effectiveness over AND/OR DAG query optimization strategy.
Finally, we illustrate how our technique can be used for efficient multiple
query optimization and selection of materialized views in data warehousing
environments."
"We describe a meta-querying system for databases containing queries in
addition to ordinary data. In the context of such databases, a meta-query is a
query about queries. Representing stored queries in XML, and using the standard
XML manipulation language XSLT as a sublanguage, we show that just a few
features need to be added to SQL to turn it into a fully-fledged meta-query
language. The good news is that these features can be directly supported by
extensible database technology."
"We consider here the problem of obtaining reliable, consistent information
from inconsistent databases -- databases that do not have to satisfy given
integrity constraints. We use the notion of consistent query answer -- a query
answer which is true in every (minimal) repair of the database. We provide a
complete classification of the computational complexity of consistent answers
to first-order queries w.r.t. functional dependencies and denial constraints.
We show how the complexity depends on the {\em type} of the constraints
considered, their {\em number}, and the {\em size} of the query. We obtain
several new PTIME cases, using new algorithms."
"Graph simulation (using graph schemata or data guides) has been successfully
proposed as a technique for adding structure to semistructured data. Design
patterns for description (such as meta-classes and homomorphisms between schema
layers), which are prominent in the object-oriented programming community,
constitute a generalization of this graph simulation approach.
  In this paper, we show description applicable to a wide range of data models
that have some notion of object (-identity), and propose to turn it into a data
model primitive much like, say, inheritance. We argue that such an extension
fills a practical need in contemporary data management. Then, we present
algebraic techniques for query optimization (using the notions of described and
description queries). Finally, in the semistructured setting, we discuss the
pruning of regular path queries (with nested conditions) using description
meta-data. In this context, our notion of meta-data extends graph schemata and
data guides by meta-level values, allowing to boost query performance and to
reduce the redundancy of data."
"The handling of user preferences is becoming an increasingly important issue
in present-day information systems. Among others, preferences are used for
information filtering and extraction to reduce the volume of data presented to
the user. They are also used to keep track of user profiles and formulate
policies to improve and automate decision making.
  We propose here a simple, logical framework for formulating preferences as
preference formulas. The framework does not impose any restrictions on the
preference relations and allows arbitrary operation and predicate signatures in
preference formulas. It also makes the composition of preference relations
straightforward. We propose a simple, natural embedding of preference formulas
into relational algebra (and SQL) through a single winnow operator
parameterized by a preference formula. The embedding makes possible the
formulation of complex preference queries, e.g., involving aggregation, by
piggybacking on existing SQL constructs. It also leads in a natural way to the
definition of further, preference-related concepts like ranking. Finally, we
present general algebraic laws governing the winnow operator and its
interaction with other relational algebra operators. The preconditions on the
applicability of the laws are captured by logical formulas. The laws provide a
formal foundation for the algebraic optimization of preference queries. We
demonstrate the usefulness of our approach through numerous examples."
"A relational database is inconsistent if it does not satisfy a given set of
integrity constraints. Nevertheless, it is likely that most of the data in it
is consistent with the constraints. In this paper we apply logic programming
based on answer sets to the problem of retrieving consistent information from a
possibly inconsistent database. Since consistent information persists from the
original database to every of its minimal repairs, the approach is based on a
specification of database repairs using disjunctive logic programs with
exceptions, whose answer set semantics can be represented and computed by
systems that implement stable model semantics. These programs allow us to
declare persistence by defaults and repairing changes by exceptions. We
concentrate mainly on logic programs for binary integrity constraints, among
which we find most of the integrity constraints found in practice."
"Research on information extraction from Web pages (wrapping) has seen much
activity recently (particularly systems implementations), but little work has
been done on formally studying the expressiveness of the formalisms proposed or
on the theoretical foundations of wrapping. In this paper, we first study
monadic datalog over trees as a wrapping language. We show that this simple
language is equivalent to monadic second order logic (MSO) in its ability to
specify wrappers. We believe that MSO has the right expressiveness required for
Web information extraction and propose MSO as a yardstick for evaluating and
comparing wrappers. Along the way, several other results on the complexity of
query evaluation and query containment for monadic datalog over trees are
established, and a simple normal form for this language is presented. Using the
above results, we subsequently study the kernel fragment Elog$^-$ of the Elog
wrapping language used in the Lixto system (a visual wrapper generator).
Curiously, Elog$^-$ exactly captures MSO, yet is easier to use. Indeed,
programs in this language can be entirely visually specified."
"We address the problem of minimal-change integrity maintenance in the context
of integrity constraints in relational databases. We assume that
integrity-restoration actions are limited to tuple deletions. We identify two
basic computational issues: repair checking (is a database instance a repair of
a given database?) and consistent query answers (is a tuple an answer to a
given query in every repair of a given database?). We study the computational
complexity of both problems, delineating the boundary between the tractable and
the intractable. We consider denial constraints, general functional and
inclusion dependencies, as well as key and foreign key constraints. Our results
shed light on the computational feasibility of minimal-change integrity
maintenance. The tractable cases should lead to practical implementations. The
intractability results highlight the inherent limitations of any integrity
enforcement mechanism, e.g., triggers or referential constraint actions, as a
way of performing minimal-change integrity maintenance."
"We present a data model for spatio-temporal databases. In this model
spatio-temporal data is represented as a finite union of objects described by
means of a spatial reference object, a temporal object and a geometric
transformation function that determines the change or movement of the reference
object in time.
  We define a number of practically relevant classes of spatio-temporal
objects, and give complete results concerning closure under Boolean set
operators for these classes. Since only few classes are closed under all set
operators, we suggest an extension of the model, which leads to better closure
properties, and therefore increased practical applicability. We also discuss a
normal form for this extended data model."
"Comprehensive semantic descriptions of Web services are essential to exploit
them in their full potential, that is, discovering them dynamically, and
enabling automated service negotiation, composition and monitoring. The
semantic mechanisms currently available in service registries which are based
on taxonomies fail to provide the means to achieve this. Although the terms
taxonomy and ontology are sometimes used interchangably there is a critical
difference. A taxonomy indicates only class/subclass relationship whereas an
ontology describes a domain completely. The essential mechanisms that ontology
languages provide include their formal specification (which allows them to be
queried) and their ability to define properties of classes. Through properties
very accurate descriptions of services can be defined and services can be
related to other services or resources. In this paper, we discuss the
advantages of describing service semantics through ontology languages and
describe how to relate the semantics defined with the services advertised in
service registries like UDDI and ebXML."
"In this paper we describe a framework for exploiting the semantics of Web
services through UDDI registries. As a part of this framework, we extend the
DAML-S upper ontology to describe the functionality we find essential for
e-businesses. This functionality includes relating the services with electronic
catalogs, describing the complementary services and finding services according
to the properties of products or services. Once the semantics is defined, there
is a need for a mechanism in the service registry to relate it with the service
advertised. The ontology model developed is general enough to be used with any
service registry. However when it comes to relating the semantics with services
advertised, the capabilities provided by the registry effects how this is
achieved. We demonstrate how to integrate the described service semantics to
UDDI registries."
"A Script Language in this paper is designed to transform the original data
into the target data by the computing formula. The Script Language can be
translated into the corresponding SQL Language, and the computation is finally
implemented by the first type of dynamic SQL. The Script Language has the
operations of insert, update, delete, union, intersect, and minus for the table
in the database.The Script Language is edited by a text file and you can easily
modify the computing formula in the text file to deal with the situations when
the computing formula have been changed. So you only need modify the text of
the script language, but needn't change the programs that have complied."
"XML is of great importance in information storage and retrieval because of
its recent emergence as a standard for data representation and interchange on
the Internet. However XML provides little semantic content and as a result
several papers have addressed the topic of how to improve the semantic
expressiveness of XML. Among the most important of these approaches has been
that of defining integrity constraints in XML. In a companion paper we defined
strong functional dependencies in XML(XFDs). We also presented a set of axioms
for reasoning about the implication of XFDs and showed that the axiom system is
sound for arbitrary XFDs. In this paper we prove that the axioms are also
complete for unary XFDs (XFDs with a single path on the l.h.s.). The second
contribution of the paper is to prove that the implication problem for unary
XFDs is decidable and to provide a linear time algorithm for it."
"Databases, collections of related data, are as old as the written word. A
database can be anything from a homemaker's metal recipe file to a
sophisticated data warehouse. Yet today, when we think of a database we
invariably think of computerized data and their DBMSs (database management
systems). How did we go from organizing our data in a simple metal filing box
or cabinet to storing our data in a sophisticated computerized database? How
did the computerized database evolve?
  This paper defines what we mean by a database. It traces the evolution of the
database, from its start as a non-computerized set of related data, to the, now
standard, computerized RDBMS (relational database management system). Early
computerized storage methods are reviewed including both the ISAM (Indexed
Sequential Access Method) and VSAM (Virtual Storage Access Method) storage
methods. Early database models are explored including the network and
hierarchical database models. Eventually, the relational, object-relational and
object-oriented databases models are discussed. An appendix of diagrams,
including hierarchical occurrence tree, network schema, ER (entity
relationship) and UML (unified modeling language) diagrams, is included to
support the text.
  This paper concludes with an exploration of current and future trends in DBMS
development. It discusses the factors affecting these trends. It delves into
the relationship between DBMSs and the increasingly popular object-oriented
development methodologies. Finally, it speculates on the future of the DBMS."
"Conditions Data in high energy physics experiments is frequently seen as
every data needed for reconstruction besides the event data itself. This
includes all sorts of slowly evolving data like detector alignment, calibration
and robustness, and data from detector control system. Also, every Conditions
Data Object is associated with a time interval of validity and a version.
Besides that, quite often is useful to tag collections of Conditions Data
Objects altogether. These issues have already been investigated and a data
model has been proposed and used for different implementations based in
commercial DBMSs, both at CERN and for the BaBar experiment. The special case
of the ATLAS complex trigger that requires online access to calibration and
alignment data poses new challenges that have to be met using a flexible and
customizable solution more in the line of Open Source components. Motivated by
the ATLAS challenges we have developed an alternative implementation, based in
an Open Source RDBMS. Several issues were investigated land will be described
in this paper:
  -The best way to map the conditions data model into the relational database
concept considering what are foreseen as the most frequent queries.
  -The clustering model best suited to address the scalability problem.
-Extensive tests were performed and will be described.
  The very promising results from these tests are attracting the attention from
the HEP community and driving further developments."
"Flexible and performant Persistency Service is a necessary component of any
HEP Software Framework. The building of a modular, non-intrusive and performant
persistency component have been shown to be very difficult task. In the past,
it was very often necessary to sacrifice modularity to achieve acceptable
performance. This resulted in the strong dependency of the overall Frameworks
on their Persistency subsystems.
  Recent development in software technology has made possible to build a
Persistency Service which can be transparently used from other Frameworks. Such
Service doesn't force a strong architectural constraints on the overall
Framework Architecture, while satisfying high performance requirements. Java
Data Object standard (JDO) has been already implemented for almost all major
databases. It provides truly transparent persistency for any Java object (both
internal and external). Objects in other languages can be handled via
transparent proxies. Being only a thin layer on top of a used database, JDO
doesn't introduce any significant performance degradation. Also Aspect-Oriented
Programming (AOP) makes possible to treat persistency as an orthogonal Aspect
of the Application Framework, without polluting it with persistence-specific
concepts.
  All these techniques have been developed primarily (or only) for the Java
environment. It is, however, possible to interface them transparently to
Frameworks built in other languages, like for example C++.
  Fully functional prototypes of flexible and non-intrusive persistency modules
have been build for several other packages, as for example FreeHEP AIDA and LCG
Pool AttributeSet (package Indicium)."
"PHENIX is one of the two large experiments at the Relativistic Heavy Ion
Collider (RHIC) at Brookhaven National Laboratory (BNL) and archives roughly
100TB of experimental data per year. In addition, large volumes of simulated
data are produced at multiple off-site computing centers. For any file catalog
to play a central role in data management it has to face problems associated
with the need for distributed access and updates. To be used effectively by the
hundreds of PHENIX collaborators in 12 countries the catalog must satisfy the
following requirements: 1) contain up-to-date data, 2) provide fast and
reliable access to the data, 3) have write permissions for the sites that store
portions of data. We present an analysis of several available Relational
Database Management Systems (RDBMS) to support a catalog meeting the above
requirements and discuss the PHENIX experience with building and using the
distributed file catalog."
"The BaBar database has pioneered the use of a commercial ODBMS within the HEP
community. The unique object-oriented architecture of Objectivity/DB has made
it possible to manage over 700 terabytes of production data generated since
May'99, making the BaBar database the world's largest known database. The
ongoing development includes new features, addressing the ever-increasing
luminosity of the detector as well as other changing physics requirements.
Significant efforts are focused on reducing space requirements and operational
costs. The paper discusses our experience with developing a large scale
database system, emphasizing universal aspects which may be applied to any
large scale system, independently of underlying technology used."
"The implementation of persistency in the Compact Muon Solenoid (CMS) Software
Framework uses the core I/O functionality of ROOT. We will discuss the current
ROOT/IO implementation, its evolution from the prior Objectivity/DB
implementation, and the plans and ongoing work for the conversion to ""POOL"",
provided by the LHC Computing Grid (LCG) persistency project."
"The simulation of CMS raw data requires the random selection of one hundred
and fifty pileup events from a very large set of files, to be superimposed in
memory to the signal event. The use of ROOT I/O for that purpose is quite
unusual: the events are not read sequentially but pseudo-randomly, they are not
processed one by one in memory but by bunches, and they do not contain orthodox
ROOT objects but many foreign objects and templates. In this context, we have
compared the performance of ROOT containers versus the STL vectors, and the use
of trees versus a direct storage of containers. The strategy with best
performances is by far the one using clones within trees, but it stays hard to
tune and very dependant on the exact use-case. The use of STL vectors could
bring more easily similar performances in a future ROOT release."
"The POOL project is the common persistency framework for the LHC experiments
to store petabytes of experiment data and metadata in a distributed and grid
enabled way. POOL is a hybrid event store consisting of a data streaming layer
and a relational layer. This paper describes the design of file catalog,
collection and metadata components which are not part of the data streaming
layer of POOL and outlines how POOL aims to provide transparent and efficient
data access for a wide range of environments and use cases - ranging from a
large production site down to a single disconnected laptops. The file catalog
is the central POOL component translating logical data references to physical
data files in a grid environment. POOL collections with their associated
metadata provide an abstract way of accessing experiment data via their logical
grouping into sets of related data objects."
"COMPASS, the fixed-target experiment at CERN studying the structure of the
nucleon and spectroscopy, collected over 260 TB during summer 2002 run. All
these data, together with reconstructed events information, were put from the
beginning in a database infrastructure based on Objectivity/DB and on the
hierarchical storage manager CASTOR. The experience in the usage of the
database is reviewed and the evolution of the system outlined."
"In preparation for the planned linear collider TESLA, DESY is designing the
required buildings and facilities. The accelerator and infrastructure
components have to be allocated to buildings, and their required areas for
installation, operation and maintenance have to be determined.
Interdisciplinary working groups specify the project from different viewpoints
and need to develop a common vision as a precondition for an optimal solution.
A commercial requirements database is used as a collaborative tool, enabling
concurrent requirements specification by independent working groups. The
requirements database ensures long term storage and availability of the
emerging knowledge, and it offers a central platform for communication which is
available for all project members. It is successfully operating since summer
2002 and has since then become an important tool for the design team."
"Next-generation projects in High Energy Physics will reach again a new
dimension of complexity. Information management has to ensure an efficient and
economic information flow within the collaborations, offering world-wide
up-to-date information access to the collaborators as one condition for
successful projects. DESY introduces several information systems in preparation
for the planned linear collider TESLA: a Requirements Management System (RMS)
is in production for the TESLA planning group, a Product Data Management System
(PDMS) is in production since the beginning of 2002 and is supporting the
cavity preparation and the general engineering of accelerator components. A
pilot Asset Management System (AMS) is in production for supporting the
management and maintenance of the technical infrastructure, and a Facility
Management System (FMS) with a Geographic Information System (GIS) is currently
being introduced to support civil engineering. Efforts have been started to
integrate the systems with the goal that users can retrieve information through
a single point of access. The paper gives an introduction to information
management and the activities at DESY."
"In the context of the ATLAS experiment there is growing evidence of the
importance of different kinds of Meta-data including all the important details
of the detector and data acquisition that are vital for the analysis of the
acquired data. The Online BookKeeper (OBK) is a component of ATLAS online
software that stores all information collected while running the experiment,
including the Meta-data associated with the event acquisition, triggering and
storage. The facilities for acquisition of control data within the on-line
software framework, together with a full functional Web interface, make the OBK
a powerful tool containing all information needed for event analysis, including
an electronic log book.
  In this paper we explain how OBK plays a role as one of the main collectors
and managers of Meta-data produced on-line, and we'll also focus on the Web
facilities already available. The usage of the web interface as an electronic
run logbook is also explained, together with the future extensions.
  We describe the technology used in OBK development and how we arrived at the
present level explaining the previous experience with various DBMS
technologies. The extensive performance evaluations that have been performed
and the usage in the production environment of the ATLAS test beams are also
analysed."
"We report the development of an open-sourced data warehouse builder,
InterBase Data Warehouse Builder (IB-DWB), based on Borland InterBase 6 Open
Edition Database Server. InterBase 6 is used for its low maintenance and small
footprint. IB-DWB is designed modularly and consists of 5 main components, Data
Plug Platform, Discoverer Platform, Multi-Dimensional Cube Builder, and Query
Supporter, bounded together by a Kernel. It is also an extensible system, made
possible by the Data Plug Platform and the Discoverer Platform. Currently,
extensions are only possible via dynamic linked-libraries (DLLs).
Multi-Dimensional Cube Builder represents a basal mean of data aggregation. The
architectural philosophy of IB-DWB centers around providing a base platform
that is extensible, which is functionally supported by expansion modules.
IB-DWB is currently being hosted by sourceforge.net (Project Unix Name:
ib-dwb), licensed under GNU General Public License, Version 2."
"We present a new application for keyword search within relational databases,
which uses a novel algorithm to solve the join discovery problem by finding
Memex-like trails through the graph of foreign key dependencies. It differs
from previous efforts in the algorithms used, in the presentation mechanism and
in the use of primary-key only database queries at query-time to maintain a
fast response for users. We present examples using the DBLP data set."
"We introduce indexing of tables referencing complex structures such as
digraphs and spatial objects, appearing in genetics and other data intensive
analysis. The indexing is achieved by extracting dimension schemas from the
referenced structures. The schemas and their dimensionality are determined by
proper coloring algorithms and the duality between all such schemas and all
such possible proper colorings is established. This duality, in turn, provides
us with an extensive library of solutions when addressing indexing questions.
It is illustrated how to use the schemas, in connection with additional
relational database technologies, to optimize queries conditioned on the
structural information being referenced. Comparisons using bitmap indexing in
the Oracle 9.2i database, on the one hand, and multidimensional clustering in
DB2 8.1.2, on the other hand, are used to illustrate the applicability of the
indexing to different technology settings. Finally, we illustrate how the
indexing can be used to extract low dimensional schemas from a binary interval
tree in order to resolve efficiently interval and stabbing queries."
"A group of senior database researchers gathers every few years to assess the
state of database research and to point out problem areas that deserve
additional focus. This report summarizes the discussion and conclusions of the
sixth ad-hoc meeting held May 4-6, 2003 in Lowell, Mass. It observes that
information management continues to be a critical component of most complex
software systems. It recommends that database researchers increase focus on:
integration of text, data, code, and streams; fusion of information from
heterogeneous data sources; reasoning about uncertain data; unsupervised data
mining for interesting correlations; information privacy; and self-adaptation
and repair."
"We study the core fragment of the Elog wrapping language used in the Lixto
system (a visual wrapper generator) and formally compare Elog to other wrapping
languages proposed in the literature."
"Given a point query Q in multi-dimensional space, K-Nearest Neighbor (KNN)
queries return the K closest answers according to given distance metric in the
database with respect to Q. In this scenario, it is possible that a majority of
the answers may be very similar to some other, especially when the data has
clusters. For a variety of applications, such homogeneous result sets may not
add value to the user. In this paper, we consider the problem of providing
diversity in the results of KNN queries, that is, to produce the closest result
set such that each answer is sufficiently different from the rest. We first
propose a user-tunable definition of diversity, and then present an algorithm,
called MOTLEY, for producing a diverse result set as per this definition.
Through a detailed experimental evaluation on real and synthetic data, we show
that MOTLEY can produce diverse result sets by reading only a small fraction of
the tuples in the database. Further, it imposes no additional overhead on the
evaluation of traditional KNN queries, thereby providing a seamless interface
between diversity and distance."
"Users of database-centric Web applications, especially in the e-commerce
domain, often resort to exploratory ``trial-and-error'' queries since the
underlying data space is huge and unfamiliar, and there are several
alternatives for search attributes in this space. For example, scouting for
cheap airfares typically involves posing multiple queries, varying flight
times, dates, and airport locations. Exploratory queries are problematic from
the perspective of both the user and the server. For the database server, it
results in a drastic reduction in effective throughput since much of the
processing is duplicated in each successive query. For the client, it results
in a marked increase in response times, especially when accessing the service
through wireless channels.
  In this paper, we investigate the design of automated techniques to minimize
the need for repetitive exploratory queries. Specifically, we present SAUNA, a
server-side query relaxation algorithm that, given the user's initial range
query and a desired cardinality for the answer set, produces a relaxed query
that is expected to contain the required number of answers. The algorithm
incorporates a range-query-specific distance metric that is weighted to produce
relaxed queries of a desired shape (e.g. aspect ratio preserving), and utilizes
multi-dimensional histograms for query size estimation. A detailed performance
evaluation of SAUNA over a variety of multi-dimensional data sets indicates
that its relaxed queries can significantly reduce the costs associated with
exploratory query processing."
"Data mining services require accurate input data for their results to be
meaningful, but privacy concerns may influence users to provide spurious
information. To encourage users to provide correct inputs, we recently proposed
a data distortion scheme for association rule mining that simultaneously
provides both privacy to the user and accuracy in the mining results. However,
mining the distorted database can be orders of magnitude more time-consuming as
compared to mining the original database. In this paper, we address this issue
and demonstrate that by (a) generalizing the distortion process to perform
symbol-specific distortion, (b) appropriately choosing the distortion
parameters, and (c) applying a variety of optimizations in the reconstruction
process, runtime efficiencies that are well within an order of magnitude of
undistorted mining can be achieved."
"We define XPathLog as a Datalog-style extension of XPath. XPathLog provides a
clear, declarative language for querying and manipulating XML whose
perspectives are especially in XML data integration. In our characterization,
the formal semantics is defined wrt. an edge-labeled graph-based model which
covers the XML data model. We give a complete, logic-based characterization of
XML data and the main language concept for XML, XPath. XPath-Logic extends the
XPath language with variable bindings and embeds it into first-order logic.
XPathLog is then the Horn fragment of XPath-Logic, providing a Datalog-style,
rule-based language for querying and manipulating XML data. The model-theoretic
semantics of XPath-Logic serves as the base of XPathLog as a logic-programming
language, whereas also an equivalent answer-set semantics for evaluating
XPathLog queries is given. In contrast to other approaches, the XPath syntax
and semantics is also used for a declarative specification how the database
should be updated: when used in rule heads, XPath filters are interpreted as
specifications of elements and properties which should be added to the
database."
"In this paper we analyze declarative deterministic and non-deterministic
semantics for active rules. In particular we consider several (partial) stable
model semantics, previously defined for deductive rules, such as well-founded,
max deterministic, unique total stable model, total stable model, and maximal
stable model semantics. The semantics of an active program AP is given by first
rewriting it into a deductive program P, then computing a model M defining the
declarative semantics of P and, finally, applying `consistent' updates
contained in M to the source database. The framework we propose permits a
natural integration of deductive and active rules and can also be applied to
queries with function symbols or to queries over infinite databases."
"We propose a framework for modeling uncertainty where both belief and doubt
can be given independent, first-class status. We adopt probability theory as
the mathematical formalism for manipulating uncertainty. An agent can express
the uncertainty in her knowledge about a piece of information in the form of a
confidence level, consisting of a pair of intervals of probability, one for
each of her belief and doubt. The space of confidence levels naturally leads to
the notion of a trilattice, similar in spirit to Fitting's bilattices.
Intuitively, thep oints in such a trilattice can be ordered according to truth,
information, or precision. We develop a framework for probabilistic deductive
databases by associating confidence levels with the facts and rules of a
classical deductive database. While the trilattice structure offers a variety
of choices for defining the semantics of probabilistic deductive databases, our
choice of semantics is based on the truth-ordering, which we find to be closest
to the classical framework for deductive databases. In addition to proposing a
declarative semantics based on valuations and an equivalent semantics based on
fixpoint theory, we also propose a proof procedure and prove it sound and
complete. We show that while classical Datalog query programs have a polynomial
time data complexity, certain query programs in the probabilistic deductive
database framework do not even terminate on some input databases. We identify a
large natural class of query programs of practical interest in our framework,
and show that programs in this class possess polynomial time data complexity,
i.e., not only do they terminate on every input database, they are guaranteed
to do so in a number of steps polynomial in the input database size."
"Relational Databases are universally conceived as an advance over their
predecessors Network and Hierarchical models. Superior in every querying
respect, they turned out to be surprisingly incomplete when modeling transitive
dependencies. Almost every couple of months a question how to model a tree in
the database surfaces at comp.database.theory newsgroup. This article completes
a series of articles exploring Nested Intervals Model. Previous articles
introduced tree encoding with Binary Rational Numbers. However, binary encoding
grows exponentially, both in breadth and in depth. In this article, we'll
leverage Farey fractions in order to overcome this problem. We'll also
demonstrate that our implementation scales to a tree with 1M nodes."
"The notion of preference is becoming more and more ubiquitous in present-day
information systems. Preferences are primarily used to filter and personalize
the information reaching the users of such systems. In database systems,
preferences are usually captured as preference relations that are used to build
preference queries. In our approach, preference queries are relational algebra
or SQL queries that contain occurrences of the winnow operator (""find the most
preferred tuples in a given relation"").
  We present here a number of semantic optimization techniques applicable to
preference queries. The techniques make use of integrity constraints, and make
it possible to remove redundant occurrences of the winnow operator and to apply
a more efficient algorithm for the computation of winnow. We also study the
propagation of integrity constraints in the result of the winnow. We have
identified necessary and sufficient conditions for the applicability of our
techniques, and formulated those conditions as constraint satisfiability
problems."
"We introduce a new variation of Tree Encoding with Nested Intervals, find
connections with Materialized Path, and suggest a method for moving parts of
the hierarchy."
"Similarity searching finds application in a wide variety of domains including
multilingual databases, computational biology, pattern recognition and text
retrieval. Similarity is measured in terms of a distance function, edit
distance, in general metric spaces, which is expensive to compute. Indexing
techniques can be used reduce the number of distance computations. We present
an analysis of various existing similarity indexing structures for the same.
The performance obtained using the index structures studied was found to be
unsatisfactory . We propose an indexing technique that combines the features of
clustering with M tree(MTB) and the results indicate that this gives better
performance."
"The Sloan Digital Sky Survey science database is approaching 2TB. While the
vast majority of queries normally execute in seconds or minutes, this
interactive execution time can be disproportionately increased by a small
fraction of queries that take hours or days to run; either because they require
non-index scans of the largest tables or because they request very large result
sets. In response to this, we added a multi-queue job submission and tracking
system. The transfer of very large result sets from queries over the network is
another serious problem. Statistics suggested that much of this data transfer
is unnecessary; users would prefer to store results locally in order to allow
further cross matching and filtering. To allow local analysis, we implemented a
system that gives users their own personal database (MyDB) at the portal site.
Users may transfer data to their MyDB, and then perform further analysis before
extracting it to their own machine.
  We intend to extend the MyDB and asynchronous query ideas to multiple NVO
nodes. This implies development, in a distributed manner, of several features,
which have been demonstrated for a single node in the SDSS Batch Query System
(CasJobs). The generalization of asynchronous queries necessitates some form of
MyDB storage as well as workflow tracking services on each node and
coordination strategies among nodes."
"Most scientific data will never be directly examined by scientists; rather it
will be put into online databases where it will be analyzed and summarized by
computer programs. Scientists increasingly see their instruments through online
scientific archives and analysis tools, rather than examining the raw data.
Today this analysis is primarily driven by scientists asking queries, but
scientific archives are becoming active databases that self-organize and
recognize interesting and anomalous facts as data arrives. In some fields, data
from many different archives can be cross-correlated to produce new insights.
Astronomy presents an excellent example of these trends; and, federating
Astronomy archives presents interesting challenges for computer scientists."
"The Sloan Digital Sky Survey Science Archive is the first in a series of
multi-Terabyte digital archives in Astronomy and other data-intensive sciences.
To facilitate data mining in the SDSS archive, we adapted a commercial database
engine and built specialized tools on top of it. Originally we chose an
object-oriented database management system due to its data organization
capabilities, platform independence, query performance and conceptual fit to
the data. However, after using the object database for the first couple of
years of the project, it soon began to fall short in terms of its query support
and data mining performance. This was as much due to the inability of the
database vendor to respond our demands for features and bug fixes as it was due
to their failure to keep up with the rapid improvements in hardware
performance, particularly faster RAID disk systems. In the end, we were forced
to abandon the object database and migrate our data to a relational database.
We describe below the technical issues that we faced with the object database
and how and why we migrated to relational technology."
"U-Datalog has been developed with the aim of providing a set-oriented logical
update language, guaranteeing update parallelism in the context of a
Datalog-like language. In U-Datalog, updates are expressed by introducing
constraints (+p(X), to denote insertion, and [minus sign]p(X), to denote
deletion) inside Datalog rules. A U-Datalog program can be interpreted as a CLP
program. In this framework, a set of updates (constraints) is satisfiable if it
does not represent an inconsistent theory, that is, it does not require the
insertion and the deletion of the same fact. This approach resembles a very
simple form of negation. However, on the other hand, U-Datalog does not provide
any mechanism to explicitly deal with negative information, resulting in a
language with limited expressive power. In this paper, we provide a semantics,
based on stratification, handling the use of negated atoms in U-Datalog
programs, and we show which problems arise in defining a compositional
semantics."
"This paper introduces an abductive framework for updating knowledge bases
represented by extended disjunctive programs. We first provide a simple
transformation from abductive programs to update programs which are logic
programs specifying changes on abductive hypotheses. Then, extended abduction,
which was introduced by the same authors as a generalization of traditional
abduction, is computed by the answer sets of update programs. Next, different
types of updates, view updates and theory updates are characterized by
abductive programs and computed by update programs. The task of consistency
restoration is also realized as special cases of these updates. Each update
problem is comparatively assessed from the computational complexity viewpoint.
The result of this paper provides a uniform framework for different types of
knowledge base updates, and each update is computed using existing procedures
of logic programming."
"The financial services industry is rapidly changing. Factors such as
globalization, deregulation, mergers and acquisitions, competition from
non-financial institutions, and technological innovation, have forced companies
to re-think their business.Many large companies have been using Business
Intelligence (BI) computer software for some years to help them gain
competitive advantage. With the introduction of cheaper and more generalized
products to the market place BI is now in the reach of smaller and medium sized
companies. Business Intelligence is also known as knowledge management,
management information systems (MIS), Executive information systems (EIS) and
On-line analytical Processing (OLAP)."
"We introduce an extension of the XQuery language, FluX, that supports
event-based query processing and the conscious handling of main memory buffers.
Purely event-based queries of this language can be executed on streaming XML
data in a very direct way. We then develop an algorithm that allows to
efficiently rewrite XQueries into the event-based FluX language. This algorithm
uses order constraints from a DTD to schedule event handlers and to thus
minimize the amount of buffering required for evaluating a query. We discuss
the various technical aspects of query optimization and query evaluation within
our framework. This is complemented with an experimental evaluation of our
approach."
"In this paper, we motivated the need for relational database systems to
support subset query processing. We defined new operators in relational
algebra, and new constructs in SQL for expressing subset queries. We also
illustrated the applicability of subset queries through different examples
expressed using extended SQL statements and relational algebra expressions. Our
aim is to show the utility of subset queries for next generation applications."
"Database system architectures are undergoing revolutionary changes.
Algorithms and data are being unified by integrating programming languages with
the database system. This gives an extensible object-relational system where
non-procedural relational operators manipulate object sets. Coupled with this,
each DBMS is now a web service. This has huge implications for how we structure
applications. DBMSs are now object containers. Queues are the first objects to
be added. These queues are the basis for transaction processing and workflow
applica-tions. Future workflow systems are likely to be built on this core.
Data cubes and online analytic processing are now baked into most DBMSs. Beyond
that, DBMSs have a framework for data mining and machine learning algorithms.
Decision trees, Bayes nets, clustering, and time series analysis are built in;
new algorithms can be added. Text, temporal, and spatial data access methods,
along with their probabilistic reasoning have been added to database systems.
Allowing approximate and probabilistic answers is essential for many
applications. Many believe that XML and xQuery will be the main data structure
and access pattern. Database systems must accommodate that perspective.These
changes mandate a much more dynamic query optimization strategy. Intelligence
is moving to the periphery of the network. Each disk and each sensor will be a
competent database machine. Relational algebra is a convenient way to program
these systems. Database systems are now expected to be self-managing,
self-healing, and always-up."
"We explored ways of doing spatial search within a relational database: (1)
hierarchical triangular mesh (a tessellation of the sphere), (2) a zoned
bucketing system, and (3) representing areas as disjunctive-normal form
constraints. Each of these approaches has merits. They all allow efficient
point-in-region queries. A relational representation for regions allows Boolean
operations among them and allows quick tests for point-in-region,
regions-containing-point, and region-overlap. The speed of these algorithms is
much improved by a zone and multi-scale zone-pyramid scheme. The approach has
the virtue that the zone mechanism works well on B-Trees native to all SQL
systems and integrates naturally with current query optimizers - rather than
requiring a new spatial access method and concomitant query optimizer
extensions. Over the last 5 years, we have used these techniques extensively in
our work on SkyServer.sdss.org, and SkyQuery.net."
"XSLT is an increasingly popular language for processing XML data. It is
widely supported by application platform software. However, little optimization
effort has been made inside the current XSLT processing engines. Evaluating a
very simple XSLT program on a large XML document with a simple schema may
result in extensive usage of memory. In this paper, we present a novel notion
of \emph{Streaming Processing Model} (\emph{SPM}) to evaluate a subset of XSLT
programs on XML documents, especially large ones. With SPM, an XSLT processor
can transform an XML source document to other formats without extra memory
buffers required. Therefore, our approach can not only tackle large source
documents, but also produce large results. We demonstrate with a performance
study the advantages of the SPM approach. Experimental results clearly confirm
that SPM improves XSLT evaluation typically 2 to 10 times better than the
existing approaches. Moreover, the SPM approach also features high scalability."
"This paper presents a generalization of the disjunctive paraconsistent
relational data model in which disjunctive positive and negative information
can be represented explicitly and manipulated. There are situations where the
closed world assumption to infer negative facts is not valid or undesirable and
there is a need to represent and reason with negation explicitly. We consider
explicit disjunctive negation in the context of disjunctive databases as there
is an interesting interplay between these two types of information. Generalized
disjunctive paraconsistent relation is introduced as the main structure in this
model. The relational algebra is appropriately generalized to work on
generalized disjunctive paraconsistent relations and their correctness is
established."
"The ability to perform meaningful empirical studies is of essence in research
in spatio-temporal query processing. Such studies are often necessary to gain
detailed insight into the functional and performance characteristics of
proposals for new query processing techniques.
  We present a collection of spatio-temporal data, collected during an
intelligent speed adaptation project, termed INFATI, in which some two dozen
cars equipped with GPS receivers and logging equipment took part. We describe
how the data was collected and how it was ""modified"" to afford the drivers some
degree of anonymity.
  We also present the road network in which the cars were moving during data
collection.
  The GPS data is publicly available for non-commercial purposes. It is our
hope that this resource will help the spatio-temporal research community in its
efforts to develop new and better query processing techniques."
"We explore the possibility of applying the framework of frequent pattern
mining to a class of continuous objects appearing in nature, namely knots. We
introduce the frequent knot mining problem and present a solution. The key
observation is that a database consisting of knots can be transformed into a
transactional database. This observation is based on the Prime Decomposition
Theorem of knots."
"This paper presents an extension of generalized disjunctive paraconsistent
relational data model in which pure disjunctive positive and negative
information as well as mixed disjunctive positive and negative information can
be represented explicitly and manipulated. We consider explicit mixed
disjunctive information in the context of disjunctive databases as there is an
interesting interplay between these two types of information. Extended
generalized disjunctive paraconsistent relation is introduced as the main
structure in this model. The relational algebra is appropriately generalized to
work on extended generalized disjunctive paraconsistent relations and their
correctness is established."
"In this paper, we present a generalization of the relational data model based
on paraconsistent intuitionistic fuzzy sets. Our data model is capable of
manipulating incomplete as well as inconsistent information. Fuzzy relation or
intuitionistic fuzzy relation can only handle incomplete information.
Associated with each relation are two membership functions one is called
truth-membership function $T$ which keeps track of the extent to which we
believe the tuple is in the relation, another is called false-membership
function which keeps track of the extent to which we believe that it is not in
the relation. A paraconsistent intuitionistic fuzzy relation is inconsistent if
there exists one tuple $a$ such that $T(a) + F(a) > 1$. In order to handle
inconsistent situation, we propose an operator called split to transform
inconsistent paraconsistent intuitionistic fuzzy relations into
pseudo-consistent paraconsistent intuitionistic fuzzy relations and do the
set-theoretic and relation-theoretic operations on them and finally use another
operator called combine to transform the result back to paraconsistent
intuitionistic fuzzy relation. For this model, we define algebraic operators
that are generalisations of the usual operators such as union, selection, join
on fuzzy relations. Our data model can underlie any database and knowledge-base
management system that deals with incomplete and inconsistent information."
"I have plotted an image by using mathematical functions in the Database ""4th
Dimension"". I'm going to show an alternative method to: detect which sector has
been clicked; highlight it and combine it with other sectors already
highlighted; store the graph information in an efficient way; load and splat
image layers to reconstruct the stored graph."
"The problem of recovering (count and sum) range queries over multidimensional
data only on the basis of aggregate information on such data is addressed. This
problem can be formalized as follows. Suppose that a transformation T producing
a summary from a multidimensional data set is used. Now, given a data set D, a
summary S=T(D) and a range query r on D, the problem consists of studying r by
modelling it as a random variable defined over the sample space of all the data
sets D' such that T(D) = S. The study of such a random variable, done by the
definition of its probability distribution and the computation of its mean
value and variance, represents a well-founded, theoretical probabilistic
approach for estimating the query only on the basis of the available
information (that is the summary S) without assumptions on original data."
"We reduce the set of classic relational algebra operators to two binary
operations: natural join and generalized union. We further demonstrate that
this set of operators is relationally complete and honors lattice axioms."
"We address a fundamental question concerning spatio-temporal database
systems: ``What are exactly spatio-temporal queries?'' We define
spatio-temporal queries to be computable mappings that are also generic,
meaning that the result of a query may only depend to a limited extent on the
actual internal representation of the spatio-temporal data. Genericity is
defined as invariance under groups of geometric transformations that preserve
certain characteristics of spatio-temporal data (e.g., collinearity, distance,
velocity, acceleration, ...). These groups depend on the notions that are
relevant in particular spatio-temporal database applications.
  These transformations also have the distinctive property that they respect
the monotone and unidirectional nature of time.
  We investigate different genericity classes with respect to the constraint
database model for spatio-temporal databases and we identify sound and complete
languages for the first-order and the computable queries in these genericity
classes. We distinguish between genericity determined by time-invariant
transformations, genericity notions concerning physical quantities and
genericity determined by time-dependent transformations."
"Nowadays, tiered architectures are widely accepted for constructing large
scale information systems. In this context application servers often form the
bottleneck for a system's efficiency. An application server exposes an object
oriented interface consisting of set of methods which are accessed by
potentially remote clients. The idea of method caching is to store results of
read-only method invocations with respect to the application server's interface
on the client side. If the client invokes the same method with the same
arguments again, the corresponding result can be taken from the cache without
contacting the server. It has been shown that this approach can considerably
improve a real world system's efficiency.
  This paper extends the concept of method caching by addressing the case where
clients wrap related method invocations in ACID transactions. Demarcating
sequences of method calls in this way is supported by many important
application server standards. In this context the paper presents an
architecture, a theory and an efficient protocol for maintaining full
transactional consistency and in particular serializability when using a method
cache on the client side. In order to create a protocol for scheduling cached
method results, the paper extends a classical transaction formalism. Based on
this extension, a recovery protocol and an optimistic serializability protocol
are derived. The latter one differs from traditional transactional cache
protocols in many essential ways. An efficiency experiment validates the
approach: Using the cache a system's performance and scalability are
considerably improved."
"Motivated by the increasing prominence of loosely-coupled systems, such as
mobile and sensor networks, which are characterised by intermittent
connectivity and volatile data, we study the tagging of data with so-called
expiration times. More specifically, when data are inserted into a database,
they may be tagged with time values indicating when they expire, i.e., when
they are regarded as stale or invalid and thus are no longer considered part of
the database. In a number of applications, expiration times are known and can
be assigned at insertion time. We present data structures and algorithms for
online management of data tagged with expiration times. The algorithms are
based on fully functional, persistent treaps, which are a combination of binary
search trees with respect to a primary attribute and heaps with respect to a
secondary attribute. The primary attribute implements primary keys, and the
secondary attribute stores expiration times in a minimum heap, thus keeping a
priority queue of tuples to expire. A detailed and comprehensive experimental
study demonstrates the well-behavedness and scalability of the approach as well
as its efficiency with respect to a number of competitors."
"The problem of extracting consistent information from relational databases
violating integrity constraints on numerical data is addressed. In particular,
aggregate constraints defined as linear inequalities on aggregate-sum queries
on input data are considered. The notion of repair as consistent set of updates
at attribute-value level is exploited, and the characterization of several
complexity issues related to repairing data and computing consistent query
answers is provided."
"Semistructured databases require tailor-made concurrency control mechanisms
since traditional solutions for the relational model have been shown to be
inadequate. Such mechanisms need to take full advantage of the hierarchical
structure of semistructured data, for instance allowing concurrent updates of
subtrees of, or even individual elements in, XML documents. We present an
approach for concurrency control which is document-independent in the sense
that two schedules of semistructured transactions are considered equivalent if
they are equivalent on all possible documents. We prove that it is decidable in
polynomial time whether two given schedules in this framework are equivalent.
This also solves the view serializability for semistructured schedules
polynomially in the size of the schedule and exponentially in the number of
transactions."
"We study a collection of heterogeneous XML databases maintaining similar and
related information, exchanging data via a peer to peer overlay network. In
this setting, a mediated global schema is unrealistic. Yet, users/applications
wish to query the databases via one peer using its schema. We have recently
developed HepToX, a P2P Heterogeneous XML database system. A key idea is that
whenever a peer enters the system, it establishes an acquaintance with a small
number of peer databases, possibly with different schema. The peer
administrator provides correspondences between the local schema and the
acquaintance schema using an informal and intuitive notation of arrows and
boxes. We develop a novel algorithm that infers a set of precise mapping rules
between the schemas from these visual annotations. We pin down a semantics of
query translation given such mapping rules, and present a novel query
translation algorithm for a simple but expressive fragment of XQuery, that
employs the mapping rules in either direction. We show the translation
algorithm is correct. Finally, we demonstrate the utility and scalability of
our ideas and algorithms with a detailed set of experiments on top of the
Emulab, a large scale P2P network emulation testbed."
"In this paper we study the problem of reducing the evaluation costs of
queries on finite databases in presence of integrity constraints, by designing
and materializing views. Given a database schema, a set of queries defined on
the schema, a set of integrity constraints, and a storage limit, to find a
solution to this problem means to find a set of views that satisfies the
storage limit, provides equivalent rewritings of the queries under the
constraints (this requirement is weaker than equivalence in the absence of
constraints), and reduces the total costs of evaluating the queries. This
problem, database reformulation, is important for many applications, including
data warehousing and query optimization. We give complexity results and
algorithms for database reformulation in presence of constraints, for
conjunctive queries, views, and rewritings and for several types of
constraints, including functional and inclusion dependencies. To obtain better
complexity results, we introduce an unchase technique, which reduces the
problem of query equivalence under constraints to equivalence in the absence of
constraints without increasing query size."
"We study here the impact of priorities on conflict resolution in inconsistent
relational databases. We extend the framework of repairs and consistent query
answers. We propose a set of postulates that an extended framework should
satisfy and consider two instantiations of the framework: (locally preferred)
l-repairs and (globally preferred) g-repairs. We study the relationships
between them and the impact each notion of repair has on the computational
complexity of repair checking and consistent query answers."
"A high-performance algorithm for searching for frequent patterns (FPs) in
transactional databases is presented. The search for FPs is carried out by
using an iterative sieve algorithm by computing the set of enclosed cycles. In
each inner cycle of level FPs composed of elements are generated. The assigned
number of enclosed cycles (the parameter of the problem) defines the maximum
length of the desired FPs. The efficiency of the algorithm is produced by (i)
the extremely simple logical searching scheme, (ii) the avoidance of recursive
procedures, and (iii) the usage of only one-dimensional arrays of integers."
"This article presents earlier results of our research works in the area of
modeling Business Intelligence Systems. The basic idea of this research area is
presented first. We then show the necessity of including certain users'
parameters in Information systems that are used in Business Intelligence
systems in order to integrate a better response from such systems. We
identified two main types of attributes that can be missing from a base and we
showed why they needed to be included. A user model that is based on a
cognitive user evolution is presented. This model when used together with a
good definition of the information needs of the user (decision maker) will
accelerate his decision making process."
"This paper presents our preprocessing and clustering analysis on the
clickstream dataset proposed for the ECMLPKDD 2005 Discovery Challenge. The
main contributions of this article are double. First, after presenting the
clickstream dataset, we show how we build a rich data warehouse based an
advanced preprocesing. We take into account the intersite aspects in the given
ecommerce domain, which offers an interesting data structuration. A preliminary
statistical analysis based on time period clickstreams is given, emphasing the
importance of intersite user visits in such a context. Secondly, we describe
our crossed-clustering method which is applied on data generated from our data
warehouse. Our preliminary results are interesting and promising illustrating
the benefits of our WUM methods, even if more investigations are needed on the
same dataset."
"We study the applicability of XML path summaries in the context of
current-day XML databases. We find that summaries provide an excellent basis
for optimizing data access methods, which furthermore mixes very well with
path-partitioned stores. We provide practical algorithms for building and
exploiting summaries, and prove its benefits through extensive experiments."
"Relational lattice reduces the set of six classic relational algebra
operators to two binary lattice operations: natural join and inner union. We
give an introduction to this theory with emphasis on formal algebraic laws. New
results include Spight distributivity criteria and its applications to query
transformations."
"For several reasons a database may not satisfy a given set of integrity
constraints(ICs), but most likely most of the information in it is still
consistent with those ICs; and could be retrieved when queries are answered.
Consistent answers to queries wrt a set of ICs have been characterized as
answers that can be obtained from every possible minimally repaired consistent
version of the original database. In this paper we consider databases that
contain null values and are also repaired, if necessary, using null values. For
this purpose, we propose first a precise semantics for IC satisfaction in a
database with null values that is compatible with the way null values are
treated in commercial database management systems. Next, a precise notion of
repair is introduced that privileges the introduction of null values when
repairing foreign key constraints, in such a way that these new values do not
create an infinite cycle of new inconsistencies. Finally, we analyze how to
specify this kind of repairs of a database that contains null values using
disjunctive logic programs with stable model semantics."
"SPARQL is the W3C candidate recommendation query language for RDF. In this
paper we address systematically the formal study of SPARQL, concentrating in
its graph pattern facility. We consider for this study a fragment without
literals and a simple version of filters which encompasses all the main issues
yet is simple to formalize. We provide a compositional semantics, prove there
are normal forms, prove complexity bounds, among others that the evaluation of
SPARQL patterns is PSPACE-complete, compare our semantics to an alternative
operational semantics, give simple and natural conditions when both semantics
coincide and discuss optimizations procedures."
"Current systems and formalisms for representing incomplete information
generally suffer from at least one of two weaknesses. Either they are not
strong enough for representing results of simple queries, or the handling and
processing of the data, e.g. for query evaluation, is intractable.
  In this paper, we present a decomposition-based approach to addressing this
problem. We introduce world-set decompositions (WSDs), a space-efficient
formalism for representing any finite set of possible worlds over relational
databases. WSDs are therefore a strong representation system for any relational
query language. We study the problem of efficiently evaluating relational
algebra queries on sets of worlds represented by WSDs. We also evaluate our
technique experimentally in a large census data scenario and show that it is
both scalable and efficient."
"The Health-e-Child project aims to develop an integrated healthcare platform
for European paediatrics. In order to achieve a comprehensive view of childrens
health, a complex integration of biomedical data, information, and knowledge is
necessary. Ontologies will be used to formally define this domain knowledge and
will form the basis for the medical knowledge management system. This paper
introduces an innovative methodology for the vertical integration of biomedical
knowledge. This approach will be largely clinician-centered and will enable the
definition of ontology fragments, connections between them (semantic bridges)
and enriched ontology fragments (views). The strategy for the specification and
capture of fragments, bridges and views is outlined with preliminary examples
demonstrated in the collection of biomedical information from hospital
databases, biomedical ontologies, and biomedical public databases."
"Extensible markup language (XML) is a technology that has been much hyped, so
that XML has become an industry buzzword. Behind the hype is a powerful
technology for data representation in a platform independent manner. As a text
document, however, XML suffers from being too bloated, and requires an XML
parser to access and manipulate it. XString is an encoding method for XML, in
essence, a markup language's markup language. XString gives the benefit of
compressing XML, and allows for easy manipulation and processing of XML source
as a very long string."
"Calculating aggregation operators of moving point objects, using time as a
continuous variable, presents unique problems when querying for congestion in a
moving and changing (or dynamic) query space. We present a set of congestion
query operators, based on a threshold value, that estimate the following 5
aggregation operations in d-dimensions. 1) We call the count of point objects
that intersect the dynamic query space during the query time interval, the
CountRange. 2) We call the Maximum (or Minimum) congestion in the dynamic query
space at any time during the query time interval, the MaxCount (or MinCount).
3) We call the sum of time that the dynamic query space is congested, the
ThresholdSum. 4) We call the number of times that the dynamic query space is
congested, the ThresholdCount. And 5) we call the average length of time of all
the time intervals when the dynamic query space is congested, the
ThresholdAverage. These operators rely on a novel approach to transforming the
problem of selection based on position to a problem of selection based on a
threshold. These operators can be used to predict concentrations of migrating
birds that may carry disease such as Bird Flu and hence the information may be
used to predict high risk areas. On a smaller scale, those operators are also
applicable to maintaining safety in airplane operations. We present the theory
of our estimation operators and provide algorithms for exact operators. The
implementations of those operators, and experiments, which include data from
more than 7500 queries, indicate that our estimation operators produce fast,
efficient results with error under 5%."
"Algorithms that exploit sort orders are widely used to implement joins,
grouping, duplicate elimination and other set operations. Query optimizers
traditionally deal with sort orders by using the notion of interesting orders.
The number of interesting orders is unfortunately factorial in the number of
participating attributes. Optimizer implementations use heuristics to prune the
number of interesting orders, but the quality of the heuristics is unclear.
Increasingly complex decision support queries and increasing use of covering
indices, which provide multiple alternative sort orders for relations, motivate
us to better address the problem of optimization with interesting orders.
  We show that even a simplified version of optimization with sort orders is
NP-hard and provide principled heuristics for choosing interesting orders. We
have implemented the proposed techniques in a Volcano-style cost-based
optimizer, and our performance study shows significant improvements in
estimated cost. We also executed our plans on a widely used commercial database
system, and on PostgreSQL, and found that actual execution times for our plans
were significantly better than for plans generated by those systems in several
cases."
"We show that for every conjunctive query, the complexity of evaluating it on
a probabilistic database is either \PTIME or #\P-complete, and we give an
algorithm for deciding whether a given conjunctive query is \PTIME or
#\P-complete. The dichotomy property is a fundamental result on query
evaluation on probabilistic databases and it gives a complete classification of
the complexity of conjunctive queries."
"We consider the privacy problem in data publishing: given a relation I
containing sensitive information 'anonymize' it to obtain a view V such that,
on one hand attackers cannot learn any sensitive information from V, and on the
other hand legitimate users can use V to compute useful statistics on I. These
are conflicting goals. We use a definition of privacy that is derived from
existing ones in the literature, which relates the a priori probability of a
given tuple t, Pr(t), with the a posteriori probability, Pr(t | V), and propose
a novel and quite practical definition for utility. Our main result is the
following. Denoting n the size of I and m the size of the domain from which I
was drawn (i.e. n < m) then: when the a priori probability is Pr(t) =
Omega(n/sqrt(m)) for some t, there exists no useful anonymization algorithm,
while when Pr(t) = O(n/m) for all tuples t, then we give a concrete
anonymization algorithm that is both private and useful. Our algorithm is quite
different from the k-anonymization algorithm studied intensively in the
literature, and is based on random deletions and insertions to I."
"Several factors are driving high-scale deployments of large data centers
built upon commodity components. These commodity clusters are far cheaper than
mainframe systems of the past but they bring serious heat and power density
issues. Also the high failure rate of the individual components drives
significant administrative costs. This proposal outlines an architecture for
data center design based upon 20'x8'x8' modules that substantially changes how
these systems are acquired, administered, and then later recycled."
"Fragmentation leads to unpredictable and degraded application performance.
While these problems have been studied in detail for desktop filesystem
workloads, this study examines newer systems such as scalable object stores and
multimedia repositories. Such systems use a get/put interface to store objects.
In principle, databases and filesystems can support such applications
efficiently, allowing system designers to focus on complexity, deployment cost
and manageability. Although theoretical work proves that certain storage
policies behave optimally for some workloads, these policies often behave
poorly in practice. Most storage benchmarks focus on short-term behavior or do
not measure fragmentation. We compare SQL Server to NTFS and find that
fragmentation dominates performance when object sizes exceed 256KB-1MB. NTFS
handles fragmentation better than SQL Server. Although the performance curves
will vary with other systems and workloads, we expect the same interactions
between fragmentation and free space to apply. It is well-known that
fragmentation is related to the percentage free space. We found that the ratio
of free space to object size also impacts performance. Surprisingly, in both
systems, storing objects of a single size causes fragmentation, and changing
the size of write requests affects fragmentation. These problems could be
addressed with simple changes to the filesystem and database interfaces. It is
our hope that an improved understanding of fragmentation will lead to
predictable storage systems that require less maintenance after deployment."
"While there are known performance trade-offs between database page buffer
pool and query execution memory allocation policies, little has been written on
the impact of query compilation memory use on overall throughput of the
database management system (DBMS). We present a new aspect of the query
optimization problem and offer a solution implemented in Microsoft SQL Server
2005. The solution provides stable throughput for a range of workloads even
when memory requests outstrip the ability of the hardware to service those
requests."
"In this paper, we propose an approach to providing the benefits of isolation
in service-oriented applications where it is not feasible to hold traditional
locks for ACID transactions. Our technique, called ""Promises"", provides an
uniform view for clients which covers a wide range of implementation techniques
on the service side, all allowing the client to check a condition and then
later rely on that condition still holding."
"This paper gives an overview of Demaq, an XML message processing system
operating on the foundation of transactional XML message queues. We focus on
the syntax and semantics of its fully declarative, rule-based application
language and demonstrate our message-based programming paradigm in the context
of a case study. Further, we discuss optimization opportunities for executing
Demaq programs."
"Event processing will play an increasingly important role in constructing
enterprise applications that can immediately react to business critical events.
Various technologies have been proposed in recent years, such as event
processing, data streams and asynchronous messaging (e.g. pub/sub). We believe
these technologies share a common processing model and differ only in target
workload, including query language features and consistency requirements. We
argue that integrating these technologies is the next step in a natural
progression. In this paper, we present an overview and discuss the foundations
of CEDR, an event streaming system that embraces a temporal stream model to
unify and further enrich query language features, handle imperfections in event
delivery and define correctness guarantees. We describe specific contributions
made so far and outline next steps in developing the CEDR system."
"Biologists are increasingly using databases for storing and managing their
data. Biological databases typically consist of a mixture of raw data,
metadata, sequences, annotations, and related data obtained from various
sources. Current database technology lacks several functionalities that are
needed by biological databases. In this paper, we introduce bdbms, an
extensible prototype database management system for supporting biological data.
bdbms extends the functionalities of current DBMSs to include: (1) Annotation
and provenance management including storage, indexing, manipulation, and
querying of annotation and provenance as first class objects in bdbms, (2)
Local dependency tracking to track the dependencies and derivations among data
items, (3) Update authorization to support data curation via content-based
authorization, in contrast to identity-based authorization, and (4) New access
methods and their supporting operators that support pattern matching on various
types of compressed biological data types. This paper presents the design of
bdbms along with the techniques proposed to support these functionalities
including an extension to SQL. We also outline some open issues in building
bdbms."
"RFID technology is gaining adoption on an increasing scale for tracking and
monitoring purposes. Wide deployments of RFID devices will soon generate an
unprecedented volume of data. Emerging applications require the RFID data to be
filtered and correlated for complex pattern detection and transformed to events
that provide meaningful, actionable information to end applications. In this
work, we design and develop SASE, a com-plex event processing system that
performs such data-information transformation over real-time streams. We design
a complex event language for specifying application logic for such
transformation, devise new query processing techniques to effi-ciently
implement the language, and develop a comprehensive system that collects,
cleans, and processes RFID data for deliv-ery of relevant, timely information
as well as storing necessary data for future querying. We demonstrate an
initial prototype of SASE through a real-world retail management scenario."
"ably successful in building a large market and adapting to the changes of the
last three decades, its impact on the broader market of information management
is surprisingly limited. If we were to design an information management system
from scratch, based upon today's requirements and hardware capabilities, would
it look anything like today's database systems?"" In this paper, we introduce
Impliance, a next-generation information management system consisting of
hardware and software components integrated to form an easy-to-administer
appliance that can store, retrieve, and analyze all types of structured,
semi-structured, and unstructured information. We first summarize the trends
that will shape information management for the foreseeable future. Those trends
imply three major requirements for Impliance: (1) to be able to store, manage,
and uniformly query all data, not just structured records; (2) to be able to
scale out as the volume of this data grows; and (3) to be simple and robust in
operation. We then describe four key ideas that are uniquely combined in
Impliance to address these requirements, namely the ideas of: (a) integrating
software and off-the-shelf hardware into a generic information appliance; (b)
automatically discovering, organizing, and managing all data - unstructured as
well as structured - in a uniform way; (c) achieving scale-out by exploiting
simple, massive parallel processing, and (d) virtualizing compute and storage
resources to unify, simplify, and streamline the management of Impliance.
Impliance is an ambitious, long-term effort to define simpler, more robust, and
more scalable information systems for tomorrow's enterprises."
"This paper introduces the CondorJ2 cluster management system. Traditionally,
cluster management systems such as Condor employ a process-oriented approach
with little or no use of modern database system technology. In contrast,
CondorJ2 employs a data-centric, 3-tier web-application architecture for all
system functions (e.g., job submission, monitoring and scheduling; node
configuration, monitoring and management, etc.) except for job execution.
Employing a data-oriented approach allows the core challenge (i.e., managing
and coordinating a large set of distributed computing resources) to be
transformed from a relatively low-level systems problem into a more abstract,
higher-level data management problem. Preliminary results suggest that
CondorJ2's use of standard 3-tier software represents a significant step
forward to the design and implementation of large clusters (1,000 to 10,000
nodes)."
"Data analysis applications typically aggregate data across many dimensions
looking for anomalies or unusual patterns. The SQL aggregate functions and the
GROUP BY operator produce zero-dimensional or one-dimensional aggregates.
Applications need the N-dimensional generalization of these operators. This
paper defines that operator, called the data cube or simply cube. The cube
operator generalizes the histogram, cross-tabulation, roll-up, drill-down, and
sub-total constructs found in most report writers. The novelty is that cubes
are relations. Consequently, the cube operator can be imbedded in more complex
non-procedural data analysis programs. The cube operator treats each of the N
aggregation attributes as a dimension of N-space. The aggregate of a particular
set of attribute values is a point in this space. The set of points forms an
N-dimensional cube. Super-aggregates are computed by aggregating the N-cube to
lower dimensional spaces. This paper (1) explains the cube and roll-up
operators, (2) shows how they fit in SQL, (3) explains how users can define new
aggregate functions for cubes, and (4) discusses efficient techniques to
compute the cube. Many of these features are being added to the SQL Standard."
"Soon most information will be available at your fingertips, anytime,
anywhere. Rapid advances in storage, communications, and processing allow us
move all information into Cyberspace. Software to define, search, and visualize
online information is also a key to creating and accessing online information.
This article traces the evolution of data management systems and outlines
current trends. Data management systems began by automating traditional tasks:
recording transactions in business, science, and commerce. This data consisted
primarily of numbers and character strings. Today these systems provide the
infrastructure for much of our society, allowing fast, reliable, secure, and
automatic access to data distributed throughout the world. Increasingly these
systems automatically design and manage access to the data. The next steps are
to automate access to richer forms of data: images, sound, video, maps, and
other media. A second major challenge is automatically summarizing and
abstracting data in anticipation of user requests. These multi-media databases
and tools to access them will be a cornerstone of our move to Cyberspace."
"ANSI SQL-92 defines Isolation Levels in terms of phenomena: Dirty Reads,
Non-Repeatable Reads, and Phantoms. This paper shows that these phenomena and
the ANSI SQL definitions fail to characterize several popular isolation levels,
including the standard locking implementations of the levels. Investigating the
ambiguities of the phenomena leads to clearer definitions; in addition new
phenomena that better characterize isolation types are introduced. An important
multiversion isolation type, Snapshot Isolation, is defined."
"Message-oriented-middleware (MOM) has become an small industry. MOM offers
queued transaction processing as an advance over pure client-server transaction
processing. This note makes four points: Queued transaction processing is less
general than direct transaction processing. Queued systems are built on top of
direct systems. You cannot build a direct system atop a queued system. It is
difficult to build direct, conversational, or distributed transactions atop a
queued system. Queues are interesting databases with interesting concurrency
control. It is best to build these mechanisms into a standard database system
so other applications can use these interesting features. Queue systems need
DBMS functionality. Queues need security, configuration, performance
monitoring, recovery, and reorganization utilities. Database systems already
have these features. A full-function MOM system duplicates these database
features. Queue managers are simple TP-monitors managing server pools driven by
queues. Database systems are encompassing many server pool features as they
evolve to TP-lite systems."
"Application designers often face the question of whether to store large
objects in a filesystem or in a database. Often this decision is made for
application design simplicity. Sometimes, performance measurements are also
used. This paper looks at the question of fragmentation - one of the
operational issues that can affect the performance and/or manageability of the
system as deployed long term. As expected from the common wisdom, objects
smaller than 256KB are best stored in a database while objects larger than 1M
are best stored in the filesystem. Between 256KB and 1MB, the read:write ratio
and rate of object overwrite or replacement are important factors. We used the
notion of ""storage age"" or number of object overwrites as way of normalizing
wall clock time. Storage age allows our results or similar such results to be
applied across a number of read:write ratios and object replacement rates."
"This paper presents a simple data dump and load utility for Firebird
databases which mimics mysqldump in MySQL. This utility, fb_dump and fb_load,
for dumping and loading respectively, retrieves each database table using
kinterbasdb and serializes the data using marshal module. This utility has two
advantages over the standard Firebird database backup utility, gbak. Firstly,
it is able to backup and restore single database tables which might help to
recover corrupted databases. Secondly, the output is in text-coded format (from
marshal module) making it more resilient than a compressed text backup, as in
the case of using gbak."
"The normalization of a data cube is the ordering of the attribute values. For
large multidimensional arrays where dense and sparse chunks are stored
differently, proper normalization can lead to improved storage efficiency. We
show that it is NP-hard to compute an optimal normalization even for 1x3
chunks, although we find an exact algorithm for 1x2 chunks. When dimensions are
nearly statistically independent, we show that dimension-wise attribute
frequency sorting is an optimal normalization and takes time O(d n log(n)) for
data cubes of size n^d. When dimensions are not independent, we propose and
evaluate several heuristics. The hybrid OLAP (HOLAP) storage mechanism is
already 19%-30% more efficient than ROLAP, but normalization can improve it
further by 9%-13% for a total gain of 29%-44% over ROLAP."
"Rating-based collaborative filtering is the process of predicting how a user
would rate a given item from other user ratings. We propose three related slope
one schemes with predictors of the form f(x) = x + b, which precompute the
average difference between the ratings of one item and another for users who
rated both. Slope one algorithms are easy to implement, efficient to query,
reasonably accurate, and they support both online queries and dynamic updates,
which makes them good candidates for real-world systems. The basic slope one
scheme is suggested as a new reference scheme for collaborative filtering. By
factoring in items that a user liked separately from items that a user
disliked, we achieve results competitive with slower memory-based schemes over
the standard benchmark EachMovie and Movielens data sets while better
fulfilling the desiderata of CF applications."
This paper has been withdrawn.
"The queries defined on data warehouses are complex and use several join
operations that induce an expensive computational cost. This cost becomes even
more prohibitive when queries access very large volumes of data. To improve
response time, data warehouse administrators generally use indexing techniques
such as star join indexes or bitmap join indexes. This task is nevertheless
complex and fastidious. Our solution lies in the field of data warehouse
auto-administration. In this framework, we propose an automatic index selection
strategy. We exploit a data mining technique ; more precisely frequent itemset
mining, in order to determine a set of candidate indexes from a given workload.
Then, we propose several cost models allowing to create an index configuration
composed by the indexes providing the best profit. These models evaluate the
cost of accessing data using bitmap join indexes, and the cost of updating and
storing these indexes."
"Materialized view selection is a non-trivial task. Hence, its complexity must
be reduced. A judicious choice of views must be cost-driven and influenced by
the workload experienced by the system. In this paper, we propose a framework
for materialized view selection that exploits a data mining technique
(clustering), in order to determine clusters of similar queries. We also
propose a view merging algorithm that builds a set of candidate views, as well
as a greedy process for selecting a set of views to materialize. This selection
is based on cost models that evaluate the cost of accessing data using views
and the cost of storing these views. To validate our strategy, we executed a
workload of decision-support queries on a test data warehouse, with and without
using our strategy. Our experimental results demonstrate its efficiency, even
when storage space is limited."
"In object-oriented or object-relational databases such as multimedia
databases or most XML databases, access patterns are not static, i.e.,
applications do not always access the same objects in the same order
repeatedly. However, this has been the way these databases and associated
optimisation techniques such as clustering have been evaluated up to now. This
paper opens up research regarding this issue by proposing a dynamic object
evaluation framework (DOEF). DOEF accomplishes access pattern change by
defining configurable styles of change. It is a preliminary prototype that has
been designed to be open and fully extensible. Though originally designed for
the object-oriented model, it can also be used within the object-relational
model with few adaptations. Furthermore, new access pattern change models can
be added too. To illustrate the capabilities of DOEF, we conducted two
different sets of experiments. In the first set of experiments, we used DOEF to
compare the performances of four state of the art dynamic clustering
algorithms. The results show that DOEF is effective at determining the
adaptability of each dynamic clustering algorithm to changes in access pattern.
They also led us to conclude that dynamic clustering algorithms can cope with
moderate levels of access pattern change, but that performance rapidly degrades
to be worse than no clustering when vigorous styles of access pattern change
are applied. In the second set of experiments, we used DOEF to compare the
performance of two different object stores: Platypus and SHORE. The use of DOEF
exposed the poor swapping performance of Platypus."
"We present in this paper a new benchmark for evaluating the performances of
data warehouses. Benchmarking is useful either to system users for comparing
the performances of different systems, or to system engineers for testing the
effect of various design choices. While the TPC (Transaction Processing
Performance Council) standard benchmarks address the first point, they are not
tuneable enough to address the second one. Our Data Warehouse Engineering
Benchmark (DWEB) allows to generate various ad-hoc synthetic data warehouses
and workloads. DWEB is fully parameterized. However, two levels of
parameterization keep it easy to tune. Since DWEB mainly meets engineering
benchmarking needs, it is complimentary to the TPC standard benchmarks, and not
a competitor. Finally, DWEB is implemented as a Java free software that can be
interfaced with most existing relational database management systems."
"With the wide development of databases in general and data warehouses in
particular, it is important to reduce the tasks that a database administrator
must perform manually. The idea of using data mining techniques to extract
useful knowledge for administration from the data themselves has existed for
some years. However, little research has been achieved. The aim of this study
is to search for a way of extracting useful knowledge from stored data to
automatically apply performance optimization techniques, and more particularly
indexing techniques. We have designed a tool that extracts frequent itemsets
from a given workload to compute an index configuration that helps optimizing
data access time. The experiments we performed showed that the index
configurations generated by our tool allowed performance gains of 15% to 25% on
a test database and a test data warehouse."
"We present in this paper three dynamic clustering techniques for
Object-Oriented Databases (OODBs). The first two, Dynamic, Statistical &
Tunable Clustering (DSTC) and StatClust, exploit both comprehensive usage
statistics and the inter-object reference graph. They are quite elaborate.
However, they are also complex to implement and induce a high overhead. The
third clustering technique, called Detection & Reclustering of Objects (DRO),
is based on the same principles, but is much simpler to implement. These three
clustering algorithm have been implemented in the Texas persistent object store
and compared in terms of clustering efficiency (i.e., overall performance
increase) and overhead using the Object Clustering Benchmark (OCB). The results
obtained showed that DRO induced a lighter overhead while still achieving
better overall performance."
"Performance of object-oriented database systems (OODBs) is still an issue to
both designers and users nowadays. The aim of this paper is to propose a
generic discrete-event random simulation model, called VOODB, in order to
evaluate the performances of OODBs in general, and the performances of
optimization methods like clustering in particular. Such optimization methods
undoubtedly improve the performances of OODBs. Yet, they also always induce
some kind of overhead for the system. Therefore, it is important to evaluate
their exact impact on the overall performances. VOODB has been designed as a
generic discrete-event random simulation model by putting to use a modelling
approach, and has been validated by simulating the behavior of the O2 OODB and
the Texas persistent object store. Since our final objective is to compare
object clustering algorithms, some experiments have also been conducted on the
DSTC clustering technique, which is implemented in Texas. To validate VOODB,
performance results obtained by simulation for a given experiment have been
compared to the results obtained by benchmarking the real systems in the same
conditions. Benchmarking and simulation performance evaluations have been
observed to be consistent, so it appears that simulation can be a reliable
approach to evaluate the performances of OODBs."
"We present in this paper a generic object-oriented benchmark (the Object
Clustering Benchmark) that has been designed to evaluate the performances of
clustering policies in object-oriented databases. OCB is generic because its
sample database may be customized to fit the databases introduced by the main
existing benchmarks (e.g., OO1). OCB's current form is clustering-oriented
because of its clustering-oriented workload, but it can be easily adapted to
other purposes. Lastly, OCB's code is compact and easily portable. OCB has been
implemented in a real system (Texas, running on a Sun workstation), in order to
test a specific clustering policy called DSTC. A few results concerning this
test are presented."
"It is widely acknowledged that good object clustering is critical to the
performance of object-oriented databases. However, object clustering always
involves some kind of overhead for the system. The aim of this paper is to
propose a modelling methodology in order to evaluate the performances of
different clustering policies. This methodology has been used to compare the
performances of three clustering algorithms found in the literature (Cactis, CK
and ORION) that we considered representative of the current research in the
field of object clustering. The actual performance evaluation was performed
using simulation. Simulation experiments we performed showed that the Cactis
algorithm is better than the ORION algorithm and that the CK algorithm totally
outperforms both other algorithms in terms of response time and clustering
overhead."
"Data warehouse architectural choices and optimization techniques are critical
to decision support query performance. To facilitate these choices, the
performance of the designed data warehouse must be assessed. This is usually
done with the help of benchmarks, which can either help system users comparing
the performances of different systems, or help system engineers testing the
effect of various design choices. While the TPC standard decision support
benchmarks address the first point, they are not tuneable enough to address the
second one and fail to model different data warehouse schemas. By contrast, our
Data Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc
synthetic data warehouses and workloads. DWEB is fully parameterized to fulfill
data warehouse design needs. However, two levels of parameterization keep it
relatively easy to tune. Finally, DWEB is implemented as a Java free software
that can be interfaced with most existing relational database management
systems. A sample usage of DWEB is also provided in this paper."
"In object-oriented or object-relational databases such as multimedia
databases or most XML databases, access patterns are not static, i.e.,
applications do not always access the same objects in the same order
repeatedly. However, this has been the way these databases and associated
optimisation techniques like clustering have been evaluated up to now. This
paper opens up research regarding this issue by proposing a dynamic object
evaluation framework (DOEF) that accomplishes access pattern change by defining
configurable styles of change. This preliminary prototype has been designed to
be open and fully extensible. To illustrate the capabilities of DOEF, we used
it to compare the performances of four state of the art dynamic clustering
algorithms. The results show that DOEF is indeed effective at determining the
adaptability of each dynamic clustering algorithm to changes in access pattern."
"Data mining is a useful decision support technique that can be used to
discover production rules in warehouses or corporate data. Data mining research
has made much effort to apply various mining algorithms efficiently on large
databases. However, a serious problem in their practical application is the
long processing time of such algorithms. Nowadays, one of the key challenges is
to integrate data mining methods within the framework of traditional database
systems. Indeed, such implementations can take advantage of the efficiency
provided by SQL engines. In this paper, we propose an integrating approach for
decision trees within a classical database system. In other words, we try to
discover knowledge from relational databases, in the form of production rules,
via a procedure embedding SQL queries. The obtained decision tree is defined by
successive, related relational views. Each view corresponds to a given
population in the underlying decision tree. We selected the classical Induction
Decision Tree (ID3) algorithm to build the decision tree. To prove that our
implementation of ID3 works properly, we successfully compared the output of
our procedure with the output of an existing and validated data mining
software, SIPINA. Furthermore, since our approach is tuneable, it can be
generalized to any other similar decision tree-based method."
"In a data warehousing process, mastering the data preparation phase allows
substantial gains in terms of time and performance when performing
multidimensional analysis or using data mining algorithms. Furthermore, a data
warehouse can require external data. The web is a prevalent data source in this
context. In this paper, we propose a modeling process for integrating diverse
and heterogeneous (so-called multiform) data into a unified format.
Furthermore, the very schema definition provides first-rate metadata in our
data warehousing context. At the conceptual level, a complex object is
represented in UML. Our logical model is an XML schema that can be described
with a DTD or the XML-Schema language. Eventually, we have designed a Java
prototype that transforms our multiform input data into XML documents
representing our physical model. Then, the XML documents we obtain are mapped
into a relational database we view as an ODS (Operational Data Storage), whose
content will have to be re-modeled in a multidimensional way to allow its
storage in a star schema-based warehouse and, later, its analysis."
"In a data warehousing process, the data preparation phase is crucial.
Mastering this phase allows substantial gains in terms of time and performance
when performing a multidimensional analysis or using data mining algorithms.
Furthermore, a data warehouse can require external data. The web is a prevalent
data source in this context, but the data broadcasted on this medium are very
heterogeneous. We propose in this paper a UML conceptual model for a complex
object representing a superclass of any useful data source (databases, plain
texts, HTML and XML documents, images, sounds, video clips...). The translation
into a logical model is achieved with XML, which helps integrating all these
diverse, heterogeneous data into a unified format, and whose schema definition
provides first-rate metadata in our data warehousing context. Moreover, we
benefit from XML's flexibility, extensibility and from the richness of the
semi-structured data model, but we are still able to later map XML documents
into a database if more structuring is needed."
"Recent work has shown the necessity of considering an attacker's background
knowledge when reasoning about privacy in data publishing. However, in
practice, the data publisher does not know what background knowledge the
attacker possesses. Thus, it is important to consider the worst-case. In this
paper, we initiate a formal study of worst-case background knowledge. We
propose a language that can express any background knowledge about the data. We
provide a polynomial time algorithm to measure the amount of disclosure of
sensitive information in the worst case, given that the attacker has at most a
specified number of pieces of information in this language. We also provide a
method to efficiently sanitize the data so that the amount of disclosure in the
worst case is less than a specified threshold."
"Uncertain information is commonplace in real-world data management scenarios.
The ability to represent large sets of possible instances (worlds) while
supporting efficient storage and processing is an important challenge in this
context. The recent formalism of world-set decompositions (WSDs) provides a
space-efficient representation for uncertain data that also supports scalable
processing. WSDs are complete for finite world-sets in that they can represent
any finite set of possible worlds. For possibly infinite world-sets, we show
that a natural generalization of WSDs precisely captures the expressive power
of c-tables. We then show that several important decision problems are
efficiently solvable on WSDs while they are NP-hard on c-tables. Finally, we
give a polynomial-time algorithm for factorizing WSDs, i.e. an efficient
algorithm for minimizing such representations."
"Information retrieval from distributed heterogeneous data sources remains a
challenging issue. As the number of data sources increases more intelligent
retrieval techniques, focusing on information content and semantics, are
required. Currently ontologies are being widely used for managing semantic
knowledge, especially in the field of bioinformatics. In this paper we describe
an ontology assisted system that allows users to query distributed
heterogeneous data sources by hiding details like location, information
structure, access pattern and semantic structure of the data. Our goal is to
provide an integrated view on biomedical information sources for the
Health-e-Child project with the aim to overcome the lack of sufficient
semantic-based reformulation techniques for querying distributed data sources.
In particular, this paper examines the problem of query reformulation across
biomedical data sources, based on merged ontologies and the underlying
heterogeneous descriptions of the respective data sources."
"Evidence-based medicine is critically dependent on three sources of
information: a medical knowledge base, the patients medical record and
knowledge of available resources, including where appropriate, clinical
protocols. Patient data is often scattered in a variety of databases and may,
in a distributed model, be held across several disparate repositories.
Consequently addressing the needs of an evidence-based medicine community
presents issues of biomedical data integration, clinical interpretation and
knowledge management. This paper outlines how the Health-e-Child project has
approached the challenge of requirements specification for (bio-) medical data
integration, from the level of cellular data, through disease to that of
patient and population. The approach is illuminated through the requirements
elicitation and analysis of Juvenile Idiopathic Arthritis (JIA), one of three
diseases being studied in the EC-funded Health-e-Child project."
"In decision-support systems, the visual component is important for On Line
Analysis Processing (OLAP). In this paper, we propose a new approach that faces
the visualization problem due to data sparsity. We use the results of a
Multiple Correspondence Analysis (MCA) to reduce the negative effect of
sparsity by organizing differently data cube cells. Our approach does not
reduce sparsity, however it tries to build relevant representation spaces where
facts are efficiently gathered. In order to evaluate our approach, we propose
an homogeneity criterion based on geometric neighborhood of cells. The obtained
experimental results have shown the efficiency of our method."
"XML data warehouses form an interesting basis for decision-support
applications that exploit heterogeneous data from multiple sources. However,
XML-native database systems currently bear limited performances and it is
necessary to research ways to optimize them. In this paper, we propose a new
index that is specifically adapted to the multidimensional architecture of XML
warehouses and eliminates join operations, while preserving the information
contained in the original warehouse. A theoretical study and experimental
results demonstrate the efficiency of our index, even when queries are complex."
"Indices and materialized views are physical structures that accelerate data
access in data warehouses. However, these data structures generate some
maintenance overhead. They also share the same storage space. The existing
studies about index and materialized view selection consider these structures
separately. In this paper, we adopt the opposite stance and couple index and
materialized view selection to take into account the interactions between them
and achieve an efficient storage space sharing. We develop cost models that
evaluate the respective benefit of indexing and view materialization. These
cost models are then exploited by a greedy algorithm to select a relevant
configuration of indices and materialized views. Experimental results show that
our strategy performs better than the independent selection of indices and
materialized views."
"Nowadays, many decision support applications need to exploit data that are
not only numerical or symbolic, but also multimedia, multistructure,
multisource, multimodal, and/or multiversion. We term such data complex data.
Managing and analyzing complex data involves a lot of different issues
regarding their structure, storage and processing, and metadata are a key
element in all these processes. Such problems have been addressed by classical
data warehousing (i.e., applied to ""simple"" data). However, data warehousing
approaches need to be adapted for complex data. In this paper, we first propose
a precise, though open, definition of complex data. Then we present a general
architecture framework for warehousing complex data. This architecture heavily
relies on metadata and domain-related knowledge, and rests on the XML language,
which helps storing data, metadata and domain-specific knowledge altogether,
and facilitates communication between the various warehousing processes."
"Materialized views and indexes are physical structures for accelerating data
access that are casually used in data warehouses. However, these data
structures generate some maintenance overhead. They also share the same storage
space. Most existing studies about materialized view and index selection
consider these structures separately. In this paper, we adopt the opposite
stance and couple materialized view and index selection to take view-index
interactions into account and achieve efficient storage space sharing.
Candidate materialized views and indexes are selected through a data mining
process. We also exploit cost models that evaluate the respective benefit of
indexing and view materialization, and help select a relevant configuration of
indexes and materialized views among the candidates. Experimental results show
that our strategy performs better than an independent selection of materialized
views and indexes."
"Data aggregation in Geographic Information Systems (GIS) is only marginally
present in commercial systems nowadays, mostly through ad-hoc solutions. In
this paper, we first present a formal model for representing spatial data. This
model integrates geographic data and information contained in data warehouses
external to the GIS. We define the notion of geometric aggregation, a general
framework for aggregate queries in a GIS setting. We also identify the class of
summable queries, which can be efficiently evaluated by precomputing the
overlay of two or more of the thematic layers involved in the query. We also
sketch a language, denoted GISOLAP-QL, for expressing queries that involve GIS
and OLAP features. In addition, we introduce Piet, an implementation of our
proposal, that makes use of overlay precomputation for answering spatial
queries (aggregate or not). Our experimental evaluation showed that for a
certain class of geometric queries with or without aggregation, overlay
precomputation outperforms R-tree-based techniques. Finally, as a particular
application of our proposal, we study topological queries."
"Relational data model defines a specification of a type ""relation"". However,
its simplicity does not mean that the system implementing this model must
operate with structures having the same simplicity. We consider two principles
allowing create a system which combines object-oriented paradigm (OOP) and
relational data model (RDM) in one framework. The first principle -- ""complex
data in encapsulated domains"" -- is well known from The Third Manifesto by Date
and Darwen. The second principle --""data complexity in names""-- is the basis
for a system where data are described as complex objects and uniquely
represented as a set of relations. Names of these relations and names of their
attributes are combinations of names entered in specifications of the complex
objects. Below, we consider the main properties of such a system."
"XML access control policies involving updates may contain security flaws,
here called inconsistencies, in which a forbidden operation may be simulated by
performing a sequence of allowed operations. This paper investigates the
problem of deciding whether a policy is consistent, and if not, how its
inconsistencies can be repaired. We consider policies expressed in terms of
annotated DTDs defining which operations are allowed or denied for the XML
trees that are instances of the DTD. We show that consistency is decidable in
PTIME for such policies and that consistent partial policies can be extended to
unique ""least-privilege"" consistent total policies. We also consider repair
problems based on deleting privileges to restore consistency, show that finding
minimal repairs is NP-complete, and give heuristics for finding repairs."
"We address aggregate queries over GIS data and moving object data, where
non-spatial data are stored in a data warehouse. We propose a formal data model
and query language to express complex aggregate queries. Next, we study the
compression of trajectory data, produced by moving objects, using the notions
of stops and moves. We show that stops and moves are expressible in our query
language and we consider a fragment of this language, consisting of regular
expressions to talk about temporally ordered sequences of stops and moves. This
fragment can be used to efficiently express data mining and pattern matching
tasks over trajectory data."
"Increasingly, business projects are ephemeral. New Business Intelligence
tools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds
are a popular community-driven visualization technique. Hence, we investigate
tag-cloud views with support for OLAP operations such as roll-ups, slices,
dices, clustering, and drill-downs. As a case study, we implemented an
application where users can upload data and immediately navigate through its ad
hoc dimensions. To support social networking, views can be easily shared and
embedded in other Web sites. Algorithmically, our tag-cloud views are
approximate range top-k queries over spontaneous data cubes. We present
experimental evidence that iceberg cuboids provide adequate online
approximations. We benchmark several browser-oblivious tag-cloud layout
optimizations."
"Current skyline evaluation techniques assume a fixed ordering on the
attributes. However, dynamic preferences on nominal attributes are more
realistic in known applications. In order to generate online response for any
such preference issued by a user, we propose two methods of different
characteristics. The first one is a semi-materialization method and the second
is an adaptive SFS method. Finally, we conduct experiments to show the
efficiency of our proposed algorithms."
"In this paper, we present a generalization of the relational data model based
on interval neutrosophic set. Our data model is capable of manipulating
incomplete as well as inconsistent information. Fuzzy relation or
intuitionistic fuzzy relation can only handle incomplete information.
Associated with each relation are two membership functions one is called
truth-membership function T which keeps track of the extent to which we believe
the tuple is in the relation, another is called falsity-membership function F
which keeps track of the extent to which we believe that it is not in the
relation. A neutrosophic relation is inconsistent if there exists one tuple a
such that T(a) + F(a) > 1 . In order to handle inconsistent situation, we
propose an operator called ""split"" to transform inconsistent neutrosophic
relations into pseudo-consistent neutrosophic relations and do the
set-theoretic and relation-theoretic operations on them and finally use another
operator called ""combine"" to transform the result back to neutrosophic
relation. For this data model, we define algebraic operators that are
generalizations of the usual operators such as intersection, union, selection,
join on fuzzy relations. Our data model can underlie any database and
knowledge-base management system that deals with incomplete and inconsistent
information."
"We introduce a controlled form of recursion in XQuery, inflationary fixed
points, familiar in the context of relational databases. This imposes
restrictions on the expressible types of recursion, but we show that
inflationary fixed points nevertheless are sufficiently versatile to capture a
wide range of interesting use cases, including the semantics of Regular XPath
and its core transitive closure construct.
  While the optimization of general user-defined recursive functions in XQuery
appears elusive, we will describe how inflationary fixed points can be
efficiently evaluated, provided that the recursive XQuery expressions exhibit a
distributivity property. We show how distributivity can be assessed both,
syntactically and algebraically, and provide experimental evidence that XQuery
processors can substantially benefit during inflationary fixed point
evaluation."
"In this paper we describe a new approach to data modelling called the
concept-oriented model (CoM). This model is based on the formalism of nested
ordered sets which uses inclusion relation to produce hierarchical structure of
sets and ordering relation to produce multi-dimensional structure among its
elements. Nested ordered set is defined as an ordered set where an each element
can be itself an ordered set. Ordering relation in CoM is used to define data
semantics and operations with data such as projection and de-projection. This
data model can be applied to very different problems and the paper describes
some its uses such grouping with aggregation and multi-dimensional analysis."
"In the paper a new approach to data representation and manipulation is
described, which is called the concept-oriented data model (CODM). It is
supposed that items represent data units, which are stored in concepts. A
concept is a combination of superconcepts, which determine the concept's
dimensionality or properties. An item is a combination of superitems taken by
one from all the superconcepts. An item stores a combination of references to
its superitems. The references implement inclusion relation or attribute-value
relation among items. A concept-oriented database is defined by its concept
structure called syntax or schema and its item structure called semantics. The
model defines formal transformations of syntax and semantics including the
canonical semantics where all concepts are merged and the data semantics is
represented by one set of items. The concept-oriented data model treats
relations as subconcepts where items are instances of the relations.
Multi-valued attributes are defined via subconcepts as a view on the database
semantics rather than as a built-in mechanism. The model includes
concept-oriented query language, which is based on collection manipulations. It
also has such mechanisms as aggregation and inference based on semantics
propagation through the database schema."
"We investigate a decentralised approach to committing transactions in a
replicated database, under partial replication. Previous protocols either
re-execute transactions entirely and/or compute a total order of transactions.
In contrast, ours applies update values, and orders only conflicting
transactions. It results that transactions execute faster, and distributed
databases commit in small committees. Both effects contribute to preserve
scalability as the number of databases and transactions increase. Our algorithm
ensures serializability, and is live and safe in spite of faults."
"Mining frequent itemsets is a popular method for finding associated items in
databases. For this method, support, the co-occurrence frequency of the items
which form an association, is used as the primary indicator of the
associations's significance. A single user-specified support threshold is used
to decided if associations should be further investigated. Support has some
known problems with rare items, favors shorter itemsets and sometimes produces
misleading associations.
  In this paper we develop a novel model-based frequency constraint as an
alternative to a single, user-specified minimum support. The constraint
utilizes knowledge of the process generating transaction data by applying a
simple stochastic mixture model (the NB model) which allows for transaction
data's typically highly skewed item frequency distribution. A user-specified
precision threshold is used together with the model to find local frequency
thresholds for groups of itemsets. Based on the constraint we develop the
notion of NB-frequent itemsets and adapt a mining algorithm to find all
NB-frequent itemsets in a database. In experiments with publicly available
transaction databases we show that the new constraint provides improvements
over a single minimum support threshold and that the precision threshold is
more robust and easier to set and interpret by the user."
"The Dynamic Time Warping (DTW) is a popular similarity measure between time
series. The DTW fails to satisfy the triangle inequality and its computation
requires quadratic time. Hence, to find closest neighbors quickly, we use
bounding techniques. We can avoid most DTW computations with an inexpensive
lower bound (LB_Keogh). We compare LB_Keogh with a tighter lower bound
(LB_Improved). We find that LB_Improved-based search is faster for sequential
search. As an example, our approach is 3 times faster over random-walk and
shape time series. We also review some of the mathematical properties of the
DTW. We derive a tight triangle inequality for the DTW. We show that the DTW
becomes the l_1 distance when time series are separated by a constant."
"This thesis introduces DescribeX, a powerful framework that is capable of
describing arbitrarily complex XML summaries of web collections, providing
support for more efficient evaluation of XPath workloads. DescribeX permits the
declarative description of document structure using all axes and language
constructs in XPath, and generalizes many of the XML indexing and summarization
approaches in the literature. DescribeX supports the construction of
heterogeneous summaries where different document elements sharing a common
structure can be declaratively defined and refined by means of path regular
expressions on axes, or axis path regular expression (AxPREs). DescribeX can
significantly help in the understanding of both the structure of complex,
heterogeneous XML collections and the behaviour of XPath queries evaluated on
them.
  Experimental results demonstrate the scalability of DescribeX summary
refinements and stabilizations (the key enablers for tailoring summaries) with
multi-gigabyte web collections. A comparative study suggests that using a
DescribeX summary created from a given workload can produce query evaluation
times orders of magnitude better than using existing summaries. DescribeX's
light-weight approach of combining summaries with a file-at-a-time XPath
processor can be a very competitive alternative, in terms of performance, to
conventional fully-fledged XML query engines that provide DB-like functionality
such as security, transaction processing, and native storage."
"Relational lattice is a formal mathematical model for Relational algebra. It
reduces the set of six classic relational algebra operators to two: natural
join and inner union. We continue to investigate Relational lattice properties
with emphasis onto axiomatic definition. New results include additional axioms,
equational definition for set difference (more generally anti-join), and case
study demonstrating application of the relational lattice theory for query
transformations."
"MEMS storage devices are new non-volatile secondary storages that have
outstanding advantages over magnetic disks. MEMS storage devices, however, are
much different from magnetic disks in the structure and access characteristics.
They have thousands of heads called probe tips and provide the following two
major access facilities: (1) flexibility: freely selecting a set of probe tips
for accessing data, (2) parallelism: simultaneously reading and writing data
with the set of probe tips selected. Due to these characteristics, it is
nontrivial to find data placements that fully utilize the capability of MEMS
storage devices. In this paper, we propose a simple logical model called the
Region-Sector (RS) model that abstracts major characteristics affecting data
retrieval performance, such as flexibility and parallelism, from the physical
MEMS storage model. We also suggest heuristic data placement strategies based
on the RS model and derive new data placements for relational data and
two-dimensional spatial data by using those strategies. Experimental results
show that the proposed data placements improve the data retrieval performance
by up to 4.0 times for relational data and by up to 4.8 times for
two-dimensional spatial data of approximately 320 Mbytes compared with those of
existing data placements. Further, these improvements are expected to be more
marked as the database size grows."
"Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination."
"The framework of consistent query answers and repairs has been introduced to
alleviate the impact of inconsistent data on the answers to a query. A repair
is a minimally different consistent instance and an answer is consistent if it
is present in every repair. In this article we study the complexity of
consistent query answers and repair checking in the presence of universal
constraints.
  We propose an extended version of the conflict hypergraph which allows to
capture all repairs w.r.t. a set of universal constraints. We show that repair
checking is in PTIME for the class of full tuple-generating dependencies and
denial constraints, and we present a polynomial repair algorithm. This
algorithm is sound, i.e. always produces a repair, but also complete, i.e.
every repair can be constructed. Next, we present a polynomial-time algorithm
computing consistent answers to ground quantifier-free queries in the presence
of denial constraints, join dependencies, and acyclic full-tuple generating
dependencies. Finally, we show that extending the class of constraints leads to
intractability. For arbitrary full tuple-generating dependencies consistent
query answering becomes coNP-complete. For arbitrary universal constraints
consistent query answering is \Pi_2^p-complete and repair checking
coNP-complete."
"XML data warehouses form an interesting basis for decision-support
applications that exploit complex data. However, native XML database management
systems currently bear limited performances and it is necessary to design
strategies to optimize them. In this paper, we propose an automatic strategy
for the selection of XML materialized views that exploits a data mining
technique, more precisely the clustering of the query workload. To validate our
strategy, we implemented an XML warehouse modeled along the XCube
specifications. We executed a workload of XQuery decision-support queries on
this warehouse, with and without using our strategy. Our experimental results
demonstrate its efficiency, even when queries are complex."
"Analytical queries defined on data warehouses are complex and use several
join operations that are very costly, especially when run on very large data
volumes. To improve response times, data warehouse administrators casually use
indexing techniques. This task is nevertheless complex and fastidious. In this
paper, we present an automatic, dynamic index selection method for data
warehouses that is based on incremental frequent itemset mining from a given
query workload. The main advantage of this approach is that it helps update the
set of selected indexes when workload evolves instead of recreating it from
scratch. Preliminary experimental results illustrate the efficiency of this
approach, both in terms of performance enhancement and overhead."
"With the ever-growing availability of so-called complex data, especially on
the Web, decision-support systems such as data warehouses must store and
process data that are not only numerical or symbolic. Warehousing and analyzing
such data requires the joint exploitation of metadata and domain-related
knowledge, which must thereby be integrated. In this paper, we survey the types
of knowledge and metadata that are needed for managing complex data, discuss
the issue of knowledge and metadata integration, and propose a CWM-compliant
integration solution that we incorporate into an XML complex data warehousing
framework we previously designed."
"XML data warehouses form an interesting basis for decision-support
applications that exploit complex data. However, native-XML database management
systems (DBMSs) currently bear limited performances and it is necessary to
research for ways to optimize them. In this paper, we propose a new join index
that is specifically adapted to the multidimensional architecture of XML
warehouses. It eliminates join operations while preserving the information
contained in the original warehouse. A theoretical study and experimental
results demonstrate the efficiency of our join index. They also show that
native XML DBMSs can compete with XML-compatible, relational DBMSs when
warehousing and analyzing XML data."
"In a data warehousing process, the phase of data integration is crucial. Many
methods for data integration have been published in the literature. However,
with the development of the Internet, the availability of various types of data
(images, texts, sounds, videos, databases...) has increased, and structuring
such data is a difficult task. We name these data, which may be structured or
unstructured, ""complex data"". In this paper, we propose a new approach for
complex data integration, based on a Multi-Agent System (MAS), in association
to a data warehousing approach. Our objective is to take advantage of the MAS
to perform the integration phase for complex data. We indeed consider the
different tasks of the data integration process as services offered by agents.
To validate this approach, we have actually developed an MAS for complex data
integration."
"With the wide development of databases in general and data warehouses in
particular, it is important to reduce the tasks that a database administrator
must perform manually. The aim of auto-administrative systems is to
administrate and adapt themselves automatically without loss (or even with a
gain) in performance. The idea of using data mining techniques to extract
useful knowledge for administration from the data themselves has existed for
some years. However, little research has been achieved. This idea nevertheless
remains a very promising approach, notably in the field of data warehousing,
where queries are very heterogeneous and cannot be interpreted easily. The aim
of this study is to search for a way of extracting useful knowledge from stored
data themselves to automatically apply performance optimization techniques, and
more particularly indexing techniques. We have designed a tool that extracts
frequent itemsets from a given workload to compute an index configuration that
helps optimizing data access time. The experiments we performed showed that the
index configurations generated by our tool allowed performance gains of 15% to
25% on a test database and a test data warehouse."
"With the growing use of new technologies, healthcare is nowadays undergoing
significant changes. Information-based medicine has to exploit medical
decision-support systems and requires the analysis of various, heterogeneous
data, such as patient records, medical images, biological analysis results,
etc. In this paper, we present the design of the complex data warehouse
relating to high-level athletes. It is original in two ways. First, it is aimed
at storing complex medical data. Second, it is designed to allow innovative and
quite different kinds of analyses to support: (1) personalized and anticipative
medicine (in opposition to curative medicine) for well-identified patients; (2)
broad-band statistical studies over a given population of patients.
Furthermore, the system includes data relating to several medical fields. It is
also designed to be evolutionary to take into account future advances in
medical research."
"With the rise of XML as a standard for representing business data, XML data
warehouses appear as suitable solutions for Web-based decision-support
applications. In this context, it is necessary to allow OLAP analyses over XML
data cubes (XOLAP). Thus, XQuery extensions are needed. To help define a formal
framework and allow much-needed performance optimizations on analytical queries
expressed in XQuery, having an algebra at one's disposal is desirable. However,
XOLAP approaches and algebras from the literature still largely rely on the
relational model and/or only feature a small number of OLAP operators. In
opposition, we propose in this paper to express a broad set of OLAP operators
with the TAX XML algebra."
"A purely relational account of the true XQuery semantics can turn any
relational database system into an XQuery processor. Compiling nested
expressions of the fully compositional XQuery language, however, yields odd
algebraic plan shapes featuring scattered distributions of join operators that
currently overwhelm commercial SQL query optimizers.
  This work rewrites such plans before submission to the relational database
back-end. Once cast into the shape of join graphs, we have found off-the-shelf
relational query optimizers--the B-tree indexing subsystem and join tree
planner, in particular--to cope and even be autonomously capable of
""reinventing"" advanced processing strategies that have originally been devised
specifically for the XQuery domain, e.g., XPath step reordering, axis reversal,
and path stitching. Performance assessments provide evidence that relational
query engines are among the most versatile and efficient XQuery processors
readily available today."
"With the multiplication of XML data sources, many XML data warehouse models
have been proposed to handle data heterogeneity and complexity in a way
relational data warehouses fail to achieve. However, XML-native database
systems currently suffer from limited performances, both in terms of manageable
data volume and response time. Fragmentation helps address both these issues.
Derived horizontal fragmentation is typically used in relational data
warehouses and can definitely be adapted to the XML context. However, the
number of fragments produced by classical algorithms is difficult to control.
In this paper, we propose the use of a k-means-based fragmentation approach
that allows to master the number of fragments through its $k$ parameter. We
experimentally compare its efficiency to classical derived horizontal
fragmentation algorithms adapted to XML data warehouses and show its
superiority."
"This paper addresses the problem of representing the set of repairs of a
possibly inconsistent database by means of a disjunctive database.
Specifically, the class of denial constraints is considered. We show that,
given a database and a set of denial constraints, there exists a (unique)
disjunctive database, called canonical, which represents the repairs of the
database w.r.t. the constraints and is contained in any other disjunctive
database with the same set of minimal models. We propose an algorithm for
computing the canonical disjunctive database. Finally, we study the size of the
canonical disjunctive database in the presence of functional dependencies for
both repairs and cardinality-based repairs."
"We study here fundamental issues involved in top-k query evaluation in
probabilistic databases. We consider simple probabilistic databases in which
probabilities are associated with individual tuples, and general probabilistic
databases in which, additionally, exclusivity relationships between tuples can
be represented. In contrast to other recent research in this area, we do not
limit ourselves to injective scoring functions. We formulate three intuitive
postulates that the semantics of top-k queries in probabilistic databases
should satisfy, and introduce a new semantics, Global-Topk, that satisfies
those postulates to a large degree. We also show how to evaluate queries under
the Global-Topk semantics. For simple databases we design dynamic-programming
based algorithms, and for general databases we show polynomial-time reductions
to the simple cases. For example, we demonstrate that for a fixed k the time
complexity of top-k query evaluation is as low as linear, under the assumption
that probabilistic databases are simple and scoring functions are injective."
"Classic algorithms for sequential pattern discovery, return all frequent
sequences present in a database, but, in general, only a few ones are
interesting for the user. Languages based on regular expressions (RE) have been
proposed to restrict frequent sequences to the ones that satisfy user-specified
constraints. Although the support of a sequence is computed as the number of
data-sequences satisfying a pattern with respect to the total number of
data-sequences in the database, once regular expressions come into play, new
approaches to the concept of support are needed. For example, users may be
interested in computing the support of the RE as a whole, in addition to the
one of a particular pattern. Also, when the items are frequently updated, the
traditional way of counting support in sequential pattern mining may lead to
incorrect (or, at least incomplete), conclusions. The problem gets more
involved if we are interested in categorical sequential patterns. In light of
the above, in this paper we propose to revise the classic notion of support in
sequential pattern mining, introducing the concept of temporal support of
regular expressions, intuitively defined as the number of sequences satisfying
a target pattern, out of the total number of sequences that could have possibly
matched such pattern, where the pattern is defined as a RE over complex items
(i.e., not only item identifiers, but also attributes and functions)."
"We describe a new approach to data modeling, called the concept-oriented
model (COM), and a novel concept-oriented query language (COQL). The model is
based on three principles: duality principle postulates that any element is a
couple consisting of one identity and one entity, inclusion principle
postulates that any element has a super-element, and order principle assumes
that any element has a number of greater elements within a partially ordered
set. Concept-oriented query language is based on a new data modeling construct,
called concept, inclusion relation between concepts, and concept partial
ordering in which greater concepts are represented by their field types. It is
demonstrated how COM and COQL can be used to solve three general data modeling
tasks: logical navigation, multidimensional analysis and inference. Logical
navigation is based on two operations of projection and de-projection.
Multidimensional analysis uses product operation for producing a cube from
level concepts chosen along the chosen dimension paths. Inference is defined as
a two-step procedure where input constraints are first propagated downwards
using de-projection and then the constrained result is propagated upwards using
projection."
"Bitmap indexes must be compressed to reduce input/output costs and minimize
CPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use
techniques based on run-length encoding (RLE), such as Word-Aligned Hybrid
(WAH) compression. These techniques are sensitive to the order of the rows: a
simple lexicographical sort can divide the index size by 9 and make indexes
several times faster. We investigate row-reordering heuristics. Simply
permuting the columns of the table can increase the sorting efficiency by 40%.
Secondary contributions include efficient algorithms to construct and aggregate
bitmaps. The effect of word length is also reviewed by constructing 16-bit,
32-bit and 64-bit indexes. Using 64-bit CPUs, we find that 64-bit indexes are
slightly faster than 32-bit indexes despite being nearly twice as large."
"The chase procedure, an algorithm proposed 25+ years ago to fix constraint
violations in database instances, has been successfully applied in a variety of
contexts, such as query optimization, data exchange, and data integration. Its
practicability, however, is limited by the fact that - for an arbitrary set of
constraints - it might not terminate; even worse, chase termination is an
undecidable problem in general. In response, the database community has
proposed sufficient restrictions on top of the constraints that guarantee chase
termination on any database instance. In this paper, we propose a novel
sufficient termination condition, called inductive restriction, which strictly
generalizes previous conditions, but can be checked as efficiently.
Furthermore, we motivate and study the problem of data-dependent chase
termination and, as a key result, present sufficient termination conditions
w.r.t. fixed instances. They are strictly more general than inductive
restriction and might guarantee termination although the chase does not
terminate in the general case."
"This thesis presents practical suggestions towards the implementation of the
hyperset approach to semi-structured databases and the associated query
language Delta. This work can be characterised as part of a top-down approach
to semi-structured databases, from theory to practice. The main original part
of this work consisted in implementation of the hyperset Delta query language
to semi-structured databases, including worked example queries. In fact, the
goal was to demonstrate the practical details of this approach and language.
The required development of an extended, practical version of the language
based on the existing theoretical version, and the corresponding operational
semantics. Here we present detailed description of the most essential steps of
the implementation. Another crucial problem for this approach was to
demonstrate how to deal in reality with the concept of the equality relation
between (hyper)sets, which is computationally realised by the bisimulation
relation. In fact, this expensive procedure, especially in the case of
distributed semi-structured data, required some additional theoretical
considerations and practical suggestions for efficient implementation. To this
end the 'local/global' strategy for computing the bisimulation relation over
distributed semi-structured data was developed and its efficiency was
experimentally confirmed."
"Understanding the importance that the electronic medical health records
system has, with its various structural types and grades, has led to the
elaboration of a series of standards and quality control methods, meant to
control its functioning. In time, the electronic health records system has
evolved along with the medical data change of structure. Romania has not yet
managed to fully clarify this concept, various definitions still being
encountered, such as ""Patient's electronic chart"", ""Electronic health file"". A
slow change from functional interoperability (OSI level 6) to semantic
interoperability (level 7) is being aimed at the moment. This current article
will try to present the main electronic files models, from a functional
interoperability system's possibility to be created perspective."
"Column-oriented indexes-such as projection or bitmap indexes-are compressed
by run-length encoding to reduce storage and increase speed. Sorting the tables
improves compression. On realistic data sets, permuting the columns in the
right order before sorting can reduce the number of runs by a factor of two or
more. Unfortunately, determining the best column order is NP-hard. For many
cases, we prove that the number of runs in table columns is minimized if we
sort columns by increasing cardinality. Experimentally, sorting based on
Hilbert space-filling curves is poor at minimizing the number of runs."
"Workloads that comb through vast amounts of data are gaining importance in
the sciences. These workloads consist of ""needle in a haystack"" queries that
are long running and data intensive so that query throughput limits
performance. To maximize throughput for data-intensive queries, we put forth
LifeRaft: a query processing system that batches queries with overlapping data
requirements. Rather than scheduling queries in arrival order, LifeRaft
executes queries concurrently against an ordering of the data that maximizes
data sharing among queries. This decreases I/O and increases cache utility.
However, such batch processing can increase query response time by starving
interactive workloads. LifeRaft addresses starvation using techniques inspired
by head scheduling in disk drives. Depending upon the workload saturation and
queuing times, the system adaptively and incrementally trades-off processing
queries in arrival order and data-driven batch processing. Evaluating LifeRaft
in the SkyQuery federation of astronomy databases reveals a two-fold
improvement in query throughput."
"R is a numerical computing environment that is widely popular for statistical
data analysis. Like many such environments, R performs poorly for large
datasets whose sizes exceed that of physical memory. We present our vision of
RIOT (R with I/O Transparency), a system that makes R programs I/O-efficient in
a way transparent to the users. We describe our experience with RIOT-DB, an
initial prototype that uses a relational database system as a backend. Despite
the overhead and inadequacy of generic database systems in handling array data
and numerical computation, RIOT-DB significantly outperforms R in many
large-data scenarios, thanks to a suite of high-level, inter-operation
optimizations that integrate seamlessly into R. While many techniques in RIOT
are inspired by databases (and, for RIOT-DB, realized by a database system),
RIOT users are insulated from anything database related. Compared with previous
approaches that require users to learn new languages and rewrite their programs
to interface with a database, RIOT will, we believe, be easier to adopt by the
majority of the R users."
"Database management systems (DBMSs) have largely ignored the task of managing
the energy consumed during query processing. Both economical and environmental
factors now require that DBMSs pay close attention to energy consumption. In
this paper we approach this issue by considering energy consumption as a
first-class performance goal for query processing in a DBMS. We present two
concrete techniques that can be used by a DBMS to directly manage the energy
consumption. Both techniques trade energy consumption for performance. The
first technique, called PVC, leverages the ability of modern processors to
execute at lower processor voltage and frequency. The second technique, called
QED, uses query aggregation to leverage common components of queries in a
workload. Using experiments run on a commercial DBMS and MySQL, we show that
PVC can reduce the processor energy consumption by 49% of the original
consumption while increasing the response time by only 3%. On MySQL, PVC can
reduce energy consumption by 20% with a response time penalty of only 6%. For
simple selection queries with no predicate overlap, we show that QED can be
used to gracefully trade response time for energy, reducing energy consumption
by 54% for a 43% increase in average response time. In this paper we also
highlight some research issues in the emerging area of energy-efficient data
processing."
"To date, the principal use case for schema matching research has been as a
precursor for code generation, i.e., constructing mappings between schema
elements with the end goal of data transfer. In this paper, we argue that
schema matching plays valuable roles independent of mapping construction,
especially as schemata grow to industrial scales. Specifically, in large
enterprises human decision makers and planners are often the immediate consumer
of information derived from schema matchers, instead of schema mapping tools.
We list a set of real application areas illustrating this role for schema
matching, and then present our experiences tackling a customer problem in one
of these areas. We describe the matcher used, where the tool was effective,
where it fell short, and our lessons learned about how well current schema
matching technology is suited for use in large enterprises. Finally, we suggest
a new agenda for schema matching research based on these experiences."
"Analytical processing on XML repositories is usually enabled by designing
complex data transformations that shred the documents into a common data
warehousing schema. This can be very time-consuming and costly, especially if
the underlying XML data has a lot of variety in structure, and only a subset of
attributes constitutes meaningful dimensions and facts. Today, there is no tool
to explore an XML data set, discover interesting attributes, dimensions and
facts, and rapidly prototype an OLAP solution.
  In this paper, we propose a system, called SEDA that enables users to start
with simple keyword-style querying, and interactively refine the query based on
result summaries. SEDA then maps query results onto a set of known, or newly
created, facts and dimensions, and derives a star schema and its instantiation
to be fed into an off-the-shelf OLAP tool, for further analysis."
"We present the design and development of a data stream system that captures
data uncertainty from data collection to query processing to final result
generation. Our system focuses on data that is naturally modeled as continuous
random variables. For such data, our system employs an approach grounded in
probability and statistical theory to capture data uncertainty and integrates
this approach into high-volume stream processing. The first component of our
system captures uncertainty of raw data streams from sensing devices. Since
such raw streams can be highly noisy and may not carry sufficient information
for query processing, our system employs probabilistic models of the data
generation process and stream-speed inference to transform raw data into a
desired format with an uncertainty metric. The second component captures
uncertainty as data propagates through query operators. To efficiently quantify
result uncertainty of a query operator, we explore a variety of techniques
based on probability and statistical theory to compute the result distribution
at stream speed. We are currently working with a group of scientists to
evaluate our system using traces collected from the domains of (and eventually
in the real systems for) hazardous weather monitoring and object tracking and
monitoring."
"Over the past 40 years, database management systems (DBMSs) have evolved to
provide a sophisticated variety of data management capabilities. At the same
time, tools for managing queries over the data have remained relatively
primitive. One reason for this is that queries are typically issued through
applications. They are thus debugged once and re-used repeatedly. This mode of
interaction, however, is changing. As scientists (and others) store and share
increasingly large volumes of data in data centers, they need the ability to
analyze the data by issuing exploratory queries. In this paper, we argue that,
in these new settings, data management systems must provide powerful query
management capabilities, from query browsing to automatic query
recommendations. We first discuss the requirements for a collaborative query
management system. We outline an early system architecture and discuss the many
research challenges associated with building such an engine."
"Recent excitement in the database community surrounding new
applications?analytic, scientific, graph, geospatial, etc.?has led to an
explosion in research on database storage systems. New storage systems are
vital to the database community, as they are at the heart of making database
systems perform well in new application domains. Unfortunately, each such
system also represents a substantial engineering effort including a great deal
of duplication of mechanisms for features such as transactions and caching. In
this paper, we make the case for RodentStore, an adaptive and declarative
storage system providing a high-level interface for describing the physical
representation of data. Specifically, RodentStore uses a declarative storage
algebra whereby administrators (or database design tools) specify how a logical
schema should be grouped into collections of rows, columns, and/or arrays, and
the order in which those groups should be laid out on disk. We describe the key
operators and types of our algebra, outline the general architecture of
RodentStore, which interprets algebraic expressions to generate a physical
representation of the data, and describe the interface between RodentStore and
other parts of a database system, such as the query optimizer and executor. We
provide a case study of the potential use of RodentStore in representing dense
geospatial data collected from a mobile sensor network, showing the ease with
which different storage layouts can be expressed using some of our algebraic
constructs and the potential performance gains that a RodentStore-built storage
system can offer."
"Data consistency is very desirable because strong semantic properties make it
easier to write correct programs that perform as users expect. However, there
are good reasons why consistency may have to be weakened to achieve other
business goals. In this CIDR 2009 Perspectives paper, we present real-world
reasons inconsistency may be necessary, offer principles for managing
inconsistency coherently, and describe implementation approaches we are
investigating for sustainably scalable systems that offer comprehensible user
experiences despite inconsistency."
"Over the past few years, we have built a system that has exposed large
volumes of Deep-Web content to Google.com users. The content that our system
exposes contributes to more than 1000 search queries per-second and spans over
50 languages and hundreds of domains. The Deep Web has long been acknowledged
to be a major source of structured data on the web, and hence accessing
Deep-Web content has long been a problem of interest in the data management
community. In this paper, we report on where we believe the Deep Web provides
value and where it does not. We contrast two very different approaches to
exposing Deep-Web content -- the surfacing approach that we used, and the
virtual integration approach that has often been pursued in the data management
literature. We emphasize where the values of each of the two approaches lie and
caution against potential pitfalls. We outline important areas of future
research and, in particular, emphasize the value that can be derived from
analyzing large collections of potentially disparate structured data on the
web."
"High-volume, high-speed data streams may overwhelm the capabilities of stream
processing systems; techniques such as data prioritization, avoidance of
unnecessary processing and on-demand result production may be necessary to
reduce processing requirements. However, the dynamic nature of data streams, in
terms of both rate and content, makes the application of such techniques
challenging. Such techniques have been addressed in the context of static and
centralized query optimization; however, they have not been fully addressed for
data stream management systems. In this work, we present a comprehensive
framework that supports prioritization, avoidance of unnecessary work, and
on-demand result production over distributed, unreliable, bursty, disordered
data sources, typical of many data streams. We propose a form of inter-operator
feedback, which flows against the stream direction, to communicate the
information needed to enable execution of these techniques. This feedback
leverages punctuations to describe the subsets of interest. We identify
potential sources of feedback information, characterize new types of
punctuation to support feedback, and describe the roles of producers,
exploiters, and relayers of feedback that query operators may implement. We
present initial experimental observations using the NiagaraST data-stream
system."
"A major problem of unstructured P2P systems is their heavy network traffic.
This is caused mainly by high numbers of query answers, many of which are
irrelevant for users. One solution to this problem is to use Top-k queries
whereby the user can specify a limited number (k) of the most relevant answers.
In this paper, we present FD, a (Fully Distributed) framework for executing
Top-k queries in unstructured P2P systems, with the objective of reducing
network traffic. FD consists of a family of algorithms that are simple but
effec-tive. FD is completely distributed, does not depend on the existence of
certain peers, and addresses the volatility of peers during query execution. We
vali-dated FD through implementation over a 64-node cluster and simulation
using the BRITE topology generator and SimJava. Our performance evaluation
shows that FD can achieve major performance gains in terms of communication and
response time."
"One of the most important aspects of security organization is to establish a
framework to identify security significant points where policies and procedures
are declared. The (information) security infrastructure comprises entities,
processes, and technology. All are participants in handling information, which
is the item that needs to be protected. Privacy and security information
technology is a critical and unmet need in the management of personal
information. This paper proposes concepts and technologies for management of
personal information. Two different types of information can be distinguished:
personal information and nonpersonal information. Personal information can be
either personal identifiable information (PII), or nonidentifiable information
(NII). Security, policy, and technical requirements can be based on this
distinction. At the conceptual level, PII is defined and formalized by
propositions over infons (discrete pieces of information) that specify
transformations in PII and NII. PII is categorized into simple infons that
reflect the proprietor s aspects, relationships with objects, and relationships
with other proprietors. The proprietor is the identified person about whom the
information is communicated. The paper proposes a database organization that
focuses on the PII spheres of proprietors. At the design level, the paper
describes databases of personal identifiable information built exclusively for
this type of information, with their own conceptual scheme, system management,
and physical structure."
"Clustering large spatial databases is an important problem, which tries to
find the densely populated regions in a spatial area to be used in data mining,
knowledge discovery, or efficient information retrieval. However most
algorithms have ignored the fact that physical obstacles such as rivers, lakes,
and highways exist in the real world and could thus affect the result of the
clustering. In this paper, we propose CPO, an efficient clustering technique to
solve the problem of clustering in the presence of obstacles. The proposed
algorithm divides the spatial area into rectangular cells. Each cell is
associated with statistical information used to label the cell as dense or
non-dense. It also labels each cell as obstructed (i.e. intersects any
obstacle) or nonobstructed. For each obstructed cell, the algorithm finds a
number of non-obstructed sub-cells. Then it finds the dense regions of
non-obstructed cells or sub-cells by a breadthfirst search as the required
clusters with a center to each region."
"In this paper, we propose an efficient clustering technique to solve the
problem of clustering in the presence of obstacles. The proposed algorithm
divides the spatial area into rectangular cells. Each cell is associated with
statistical information that enables us to label the cell as dense or
non-dense. We also label each cell as obstructed (i.e. intersects any obstacle)
or non-obstructed. Then the algorithm finds the regions (clusters) of
connected, dense, non-obstructed cells. Finally, the algorithm finds a center
for each such region and returns those centers as centers of the relatively
dense regions (clusters) in the spatial area."
"Privacy preserving data publishing has attracted considerable research
interest in recent years. Among the existing solutions, {\em
$\epsilon$-differential privacy} provides one of the strongest privacy
guarantees. Existing data publishing methods that achieve
$\epsilon$-differential privacy, however, offer little data utility. In
particular, if the output dataset is used to answer count queries, the noise in
the query answers can be proportional to the number of tuples in the data,
which renders the results useless.
  In this paper, we develop a data publishing technique that ensures
$\epsilon$-differential privacy while providing accurate answers for {\em
range-count queries}, i.e., count queries where the predicate on each attribute
is a range. The core of our solution is a framework that applies {\em wavelet
transforms} on the data before adding noise to it. We present instantiations of
the proposed framework for both ordinal and nominal data, and we provide a
theoretical analysis on their privacy and utility guarantees. In an extensive
experimental study on both real and synthetic data, we show the effectiveness
and efficiency of our solution."
"Many database applications perform complex data retrieval and update tasks.
Nested queries, and queries that invoke user-defined functions, which are
written using a mix of procedural and SQL constructs, are often used in such
applications. A straight-forward evaluation of such queries involves repeated
execution of parameterized sub-queries or blocks containing queries and
procedural code.
  An important problem that arises while optimizing nested queries as well as
queries with joins, aggregates and set operations is the problem of finding an
optimal sort order from a factorial number of possible sort orders. We show
that even a special case of this problem is NP-Hard, and present practical
heuristics that are effective and easy to incorporate in existing query
optimizers.
  We also consider iterative execution of queries and updates inside complex
procedural blocks such as user-defined functions and stored procedures.
Parameter batching is an important means of improving performance as it enables
set-orientated processing. The key challenge to parameter batching lies in
rewriting a given procedure/function to process a batch of parameter values. We
propose a solution, based on program analysis and rewrite rules, to automate
the generation of batched forms of procedures and replace iterative database
calls within imperative loops with a single call to the batched form.
  We present experimental results for the proposed techniques, and the results
show significant gains in performance."
"This paper presents a novel approach for the integration of a set of XML
Schemas. The proposed approach is specialized for XML, is almost automatic,
semantic and ""light"". As a further, original, peculiarity, it is parametric
w.r.t. a ""severity"" level against which the integration task is performed. The
paper describes the approach in all details, illustrates various theoretical
results, presents the experiments we have performed for testing it and,
finally, compares it with various related approaches already proposed in the
literature."
"XML keyword search is a user-friendly way to query XML data using only
keywords. In XML keyword search, to achieve high precision without sacrificing
recall, it is important to remove spurious results not intended by the user.
Efforts to eliminate spurious results have enjoyed some success by using the
concepts of LCA or its variants, SLCA and MLCA. However, existing methods still
could find many spurious results. The fundamental cause for the occurrence of
spurious results is that the existing methods try to eliminate spurious results
locally without global examination of all the query results and, accordingly,
some spurious results are not consistently eliminated. In this paper, we
propose a novel keyword search method that removes spurious results
consistently by exploiting the new concept of structural consistency."
"In this paper, we describe a multidatabase system as 4tiered Client-Server
DBMS architectures. We discuss their functional components and provide an
overview of their performance characteristics. The first component of this
proposed system is a web based interface or Graphical User Interface, which
resides on top of the Client Application Program, the second component of the
system is a client Application program running in an application server, which
resides on top of the Global Database Management System, the third component of
the system is a Global Database Management System and global schema of the
multidatabase system server, which resides on top of the distributed
heterogeneous local component database system servers, and the fourth component
is remote heterogeneous local component database system servers. Transaction
submitted from client interface to a multidatabase system server through an
application server will be decomposed into a set of sub queries and will be
executed at various remote heterogeneous local component database servers and
also in case of information retrieval all sub queries will be composed and will
get back results to the end users."
"One of the challenging problems in the multidatabase systems is to find the
most viable solution to the problem of interoperability of distributed
heterogeneous autonomous local component databases. This has resulted in the
creation of a global schema over set of these local component database schemas
to provide a uniform representation of local schemas. The aim of this paper is
to use object oriented approach to integrate schemas of distributed
heterogeneous autonomous local component database schemas into a global schema.
The resulting global schema provides a uniform interface and high level of
location transparency for retrieval of data from the local component databases.
A set of integration operators are defined to integrate local schemas based on
the semantic relevance of their classes and to provide a model independent
representation of virtual classes of the global schema. The schematic
representation and heterogeneity is also taken into account in the integration
process. Justifications about Object Oriented Modal are also discussed. Bottom
up local schema modifications propagation in Global schema is also considered
to maintain Global schema as local schemas are autonomous and evolve over time.
An example illustrates the applicability of the integration operator defined."
"The technique of database refactoring is all about applying disciplined and
controlled techniques to change an existing database schema. The problem is to
successfully create a Database Refactoring Framework for databases. This paper
concentrates on the feasibility of adapting this concept to work as a generic
template. To retain the constraints regardless of the modifications to the
metadata, the paper proposes a MetaData Manipulation Tool to facilitate change.
The tool adopts a Template Design Pattern to make it database independent. The
paper presents a drawback of using java for constraint extraction and proposes
an alternative."
"As XML becomes ubiquitous and XML storage and processing becomes more
efficient, the range of use cases for these technologies widens daily. One
promising area is the integration of XML and data warehouses, where an
XML-native database stores multidimensional data and processes OLAP queries
written in the XQuery interrogation language. This paper explores issues
arising in the implementation of such a data warehouse. We first compare
approaches for multidimensional data modelling in XML, then describe how
typical OLAP queries on these models can be expressed in XQuery. We then show
how, regardless of the model, the grouping features of XQuery 1.1 improve
performance and readability of these queries. Finally, we evaluate the
performance of query evaluation in each modelling choice using the eXist
database, which we extended with a grouping clause implementation."
"Educational data mining (EDM) is a new growing research area and the essence
of data mining concepts are used in the educational field for the purpose of
extracting useful information on the behaviors of students in the learning
process. In this EDM, feature selection is to be made for the generation of
subset of candidate variables. As the feature selection influences the
predictive accuracy of any performance model, it is essential to study
elaborately the effectiveness of student performance model in connection with
feature selection techniques. In this connection, the present study is devoted
not only to investigate the most relevant subset features with minimum
cardinality for achieving high predictive performance by adopting various
filtered feature selection techniques in data mining but also to evaluate the
goodness of subsets with different cardinalities and the quality of six
filtered feature selection algorithms in terms of F-measure value and Receiver
Operating Characteristics (ROC) value, generated by the NaiveBayes algorithm as
base-line classifier method. The comparative study carried out by us on six
filter feature section algorithms reveals the best method, as well as optimal
dimensionality of the feature subset. Benchmarking of filter feature selection
method is subsequently carried out by deploying different classifier models.
The result of the present study effectively supports the well known fact of
increase in the predictive accuracy with the existence of minimum number of
features. The expected outcomes show a reduction in computational time and
constructional cost in both training and classification phases of the student
performance model."
"The existing solutions to privacy preserving publication can be classified
into the theoretical and heuristic categories. The former guarantees provably
low information loss, whereas the latter incurs gigantic loss in the worst
case, but is shown empirically to perform well on many real inputs. While
numerous heuristic algorithms have been developed to satisfy advanced privacy
principles such as l-diversity, t-closeness, etc., the theoretical category is
currently limited to k-anonymity which is the earliest principle known to have
severe vulnerability to privacy attacks. Motivated by this, we present the
first theoretical study on l-diversity, a popular principle that is widely
adopted in the literature. First, we show that optimal l-diverse generalization
is NP-hard even when there are only 3 distinct sensitive values in the
microdata. Then, an (l*d)-approximation algorithm is developed, where d is the
dimensionality of the underlying dataset. This is the first known algorithm
with a non-trivial bound on information loss. Extensive experiments with real
datasets validate the effectiveness and efficiency of proposed solution."
"This paper studies the problem of identification and extraction of flat and
nested data records from a given web page. With the explosive growth of
information sources available on the World Wide Web, it has become increasingly
difficult to identify the relevant pieces of information, since web pages are
often cluttered with irrelevant content like advertisements, navigation-panels,
copyright notices etc., surrounding the main content of the web page. Hence, it
is useful to mine such data regions and data records in order to extract
information from such web pages to provide value-added services. Currently
available automatic techniques to mine data regions and data records from web
pages are still unsatisfactory because of their poor performance. In this paper
a novel method to identify and extract the flat and nested data records from
the web pages automatically is proposed. It comprises of two steps : (1)
Identification and Extraction of the data regions based on visual clues
information. (2) Identification and extraction of flat and nested data records
from the data region of a web page automatically. For step1, a novel and more
effective method is proposed, which finds the data regions formed by all types
of tags using visual clues. For step2, a more effective and efficient method
namely, Visual Clue based Extraction of web Data (VCED), is proposed, which
extracts each record from the data region and identifies it whether it is a
flat or nested data record based on visual clue information the area covered by
and the number of data items present in each record. Our experimental results
show that the proposed technique is effective and better than existing
techniques."
"In this paper we present the state of advancement of the French ANR WebStand
project. The objective of this project is to construct a customizable XML based
warehouse platform to acquire, transform, analyze, store, query and export data
from the web, in particular mailing lists, with the final intension of using
this data to perform sociological studies focused on social groups of World
Wide Web, with a specific emphasis on the temporal aspects of this data. We are
currently using this system to analyze the standardization process of the W3C,
through its social network of standard setters."
"Time is one of the most difficult aspects to handle in real world
applications such as database systems. Relational database management systems
proposed by Codd offer very little built-in query language support for temporal
data management. The model itself incorporates neither the concept of time nor
any theory of temporal semantics. Many temporal extensions of the relational
model have been proposed and some of them are also implemented. This paper
offers a brief introduction to temporal database research. We propose a
conceptual model for handling time varying attributes in the relational
database model with minimal temporal attributes."
"Data mining is the task of discovering interesting patterns from large
amounts of data. There are many data mining tasks, such as classification,
clustering, association rule mining, and sequential pattern mining. Sequential
pattern mining finds sets of data items that occur together frequently in some
sequences. Sequential pattern mining, which extracts frequent subsequences from
a sequence database, has attracted a great deal of interest during the recent
data mining research because it is the basis of many applications, such as: web
user analysis, stock trend prediction, DNA sequence analysis, finding language
or linguistic patterns from natural language texts, and using the history of
symptoms to predict certain kind of disease. The diversity of the applications
may not be possible to apply a single sequential pattern model to all these
problems. Each application may require a unique model and solution. A number of
research projects were established in recent years to develop meaningful
sequential pattern models and efficient algorithms for mining these patterns.
In this paper, we theoretically provided a brief overview three types of
sequential patterns model."
"The importance of finding the characteristics leading to either a success or
a failure is one of the driving forces of data mining. The various application
areas of finding success/failure factors cover vast variety of areas such as
credit risk evaluation and granting loans, micro array analysis, health factors
and health risk factors, and parameter combination leading to a product
success. This paper presents a new approach for making inferences about
dichotomous data. The objective is to determine rules that lead to a certain
result. The method consists of four phases: in the first phase, the data is
processed into a binary format of a truth table, in the second phase; rules are
found by utilizing an algorithm that minimizes Boolean functions. In the third
phase the rules are checked and filtered. In the fourth phase, simple rules
that involve one to two features are revealed."
"There is a considerable body of work on sequence mining of Web Log Data. We
are using One Pass frequent Episode discovery (or FED) algorithm, takes a
different approach than the traditional apriori class of pattern detection
algorithms. In this approach significant intervals for each Website are
computed first (independently) and these interval used for detecting frequent
patterns/Episode and then the Analysis is performed on Significant Intervals
and frequent patterns That can be used to forecast the user's behavior using
previous trends and this can be also used for advertising purpose. This type of
applications predicts the Website interest. In this approach, time-series data
are folded over a periodicity (day, week, etc.) Which are used to form the
Interval? Significant intervals are discovered from these time points that
satisfy the criteria of minimum confidence and maximum interval length
specified by the user."
"Given the vast reservoirs of data stored worldwide, efficient mining of data
from a large information store has emerged as a great challenge. Many databases
like that of intrusion detection systems, web-click records, player statistics,
texts, proteins etc., store strings or sequences. Searching for an unusual
pattern within such long strings of data has emerged as a requirement for
diverse applications. Given a string, the problem then is to identify the
substrings that differs the most from the expected or normal behavior, i.e.,
the substrings that are statistically significant. In other words, these
substrings are less likely to occur due to chance alone and may point to some
interesting information or phenomenon that warrants further exploration. To
this end, we use the chi-square measure. We propose two heuristics for
retrieving the top-k substrings with the largest chi-square measure. We show
that the algorithms outperform other competing algorithms in the runtime, while
maintaining a high approximation ratio of more than 0.96."
"In data management, and in particular in data integration, data exchange,
query optimization, and data privacy, the notion of view plays a central role.
In several contexts, such as data integration, data mashups, and data
warehousing, the need arises of designing views starting from a set of known
correspondences between queries over different schemas. In this paper we deal
with the issue of automating such a design process. We call this novel problem
""view synthesis from schema mappings"": given a set of schema mappings, each
relating a query over a source schema to a query over a target schema,
automatically synthesize for each source a view over the target schema in such
a way that for each mapping, the query over the source is a rewriting of the
query over the target wrt the synthesized views. We study view synthesis from
schema mappings both in the relational setting, where queries and views are
(unions of) conjunctive queries, and in the semistructured data setting, where
queries and views are (two-way) regular path queries, as well as unions of
conjunctions thereof. We provide techniques and complexity upper bounds for
each of these cases."
"Several multi-pass algorithms have been proposed for Association Rule Mining
from static repositories. However, such algorithms are incapable of online
processing of transaction streams. In this paper we introduce an efficient
single-pass algorithm for mining association rules, given a hierarchical
classification amongest items. Processing efficiency is achieved by utilizing
two optimizations, hierarchy aware counting and transaction reduction, which
become possible in the context of hierarchical classification. This paper
considers the problem of integrating constraints that are Boolean expression
over the presence or absence of items into the association discovery algorithm.
This paper present three integrated algorithms for mining association rules
with item constraints and discuss their tradeoffs. It is concluded that the
variation of complexity depends on the measure of DIT (Depth of Inheritance
Tree) and NOC (Number of Children) in the context of Hierarchical
Classification."
"Data Mining deals extracting hidden knowledge, unexpected pattern and new
rules from large database. Various customized data mining tools have been
developed for domain specific applications such as Biomedicine, DNA analysis
and telecommunication. Trends in data mining include further efforts towards
the exploration of new application areas and methods for handling complex data
types, algorithm scalability, constraint based data mining and visualization
methods. In this paper we will present domain specific Secure Multiparty
computation technique and applications. Data mining has matured as a field of
basic and applied research in computer science in general. In this paper, we
survey some of the recent approaches and architectures where data mining has
been applied in the fields of e-payment systems. In this paper we limit our
discussion to data mining in the context of e-payment systems. We also mention
a few directions for further work in this domain, based on the survey."
"The HL7 standard is widely used to exchange medical information
electronically. As a part of the standard, HL7 defines scalar communication
data types like physical quantity, point in time and concept descriptor but
also complex types such as interval types, collection types and probabilistic
types. Typical HL7 applications will store their communications in a database,
resulting in a translation from HL7 concepts and types into database types.
Since the data types were not designed to be implemented in a relational
database server, this transition is cumbersome and fraught with programmer
error. The purpose of this paper is two fold. First we analyze the HL7 version
3 data type definitions and define a number of conditions that must be met, for
the data type to be suitable for implementation in a relational database. As a
result of this analysis we describe a number of possible improvements in the
HL7 specification. Second we describe an implementation in the PostgreSQL
database server and show that the database server can effectively execute
scientific calculations with units of measure, supports a large number of
operations on time points and intervals, and can perform operations that are
akin to a medical terminology server. Experiments on synthetic data show that
the user defined types perform better than an implementation that uses only
standard data types from the database server."
"Finding multilevel association rules in transaction databases is most
commonly seen in is widely used in data mining. In this paper, we present a
model of mining multilevel association rules which satisfies the different
minimum support at each level, we have employed fuzzy set concepts, multi-level
taxonomy and different minimum supports to find fuzzy multilevel association
rules in a given transaction data set. Apriori property is used in model to
prune the item sets. The proposed model adopts a topdown progressively
deepening approach to derive large itemsets. This approach incorporates fuzzy
boundaries instead of sharp boundary intervals. An example is also given to
demonstrate and support that the proposed mining algorithm can derive the
multiple-level association rules under different supports in a simple and
effective manner."
"Data mining has been widely recognized as a powerful tool to explore added
value from large-scale databases. Finding frequent item sets in databases is a
crucial in data mining process of extracting association rules. Many algorithms
were developed to find the frequent item sets. This paper presents a summary
and a comparative study of the available FP-growth algorithm variations
produced for mining frequent item sets showing their capabilities and
efficiency in terms of time and memory consumption on association rule mining
by taking application of specific information into account. It proposes pattern
growth mining paradigm based FP-tree growth algorithm, which employs a tree
structure to compress the database. The performance study shows that the anti-
FP-growth method is efficient and scalable for mining both long and short
frequent patterns and is about an order of magnitude faster than the Apriority
algorithm and also faster than some recently reported new frequent-pattern
mining."
"Previous work reports about SXSI, a fast XPath engine which executes tree
automata over compressed XML indexes. Here, reasons are investigated why SXSI
is so fast. It is shown that tree automata can be used as a general framework
for fine grained XML query optimization. We define the ""relevant nodes"" of a
query as those nodes that a minimal automaton must touch in order to answer the
query. This notion allows to skip many subtrees during execution, and, with the
help of particular tree indexes, even allows to skip internal nodes of the
tree. We efficiently approximate runs over relevant nodes by means of
on-the-fly removal of alternation and non-determinism of (alternating) tree
automata. We also introduce many implementation techniques which allows us to
efficiently evaluate tree automata, even in the absence of special indexes.
Through extensive experiments, we demonstrate the impact of the different
optimization techniques."
"Commutativity has the same inherent limitations as compatibility. Then, it is
worth conceiving simple concurrency control techniques. We propose a restricted
form of commutativity which increases parallelism without incurring a higher
overhead than compatibility. Advantages of our proposition are: (1)
commutativity of operations is determined at compile-time, (2) run-time
checking is as efficient as for compatibility, (3) neither commutativity
relations, (4) nor inverse operations, need to be specified, and (5) log space
utilization is reduced."
"In this paper, we try to focus the reader's interest on the problems that
transactional systems have to resolve for taking advantage of commutativity in
a serializable and recoverable way. Our framework is, (as others), based on the
use of conditional commutativity on abstract date types. We present new
features that have not been found in the literature hitherto, that both
increase concurrency and simplify recovery."
"We present some formal properties of (symmetrical) commutativity, the major
criterion used in transactional systems, which allow us to fully understand its
advantages and disadvantages. The main result is that commutativity is subject
to the same limitation as compatibility for arbitrary objects. However,
commutativity has also a number of attracting properties, one of which is
related to recovery and, to our knowledge, has not been exploited in the
literature. Advantages and disadvantages are illustrated on abstract data types
of interest. We also show how limits of commutativity have been circumvented,
which gives guidelines for doing so (or not!)."
"Several propositions were done to provide adapted concurrency control to
object-oriented databases. However, most of these proposals miss the fact that
considering solely read and write access modes on instances may lead to less
parallelism than in relational databases! This paper cope with that issue, and
advantages are numerous: (1) commutativity of methods is determined a priori
and automatically by the compiler, without measurable overhead, (2) run-time
checking of commutativity is as efficient as for compatibility, (3) inverse
operations need not be specified for recovery, (4) this scheme does not
preclude more sophisticated approaches, and, last but not least, (5) relational
and object-oriented concurrency control schemes with read and write access
modes are subsumed under this proposition."
"In various approaches, data cubes are pre-computed in order to answer
efficiently OLAP queries. The notion of data cube has been declined in various
ways: iceberg cubes, range cubes or differential cubes. In this paper, we
introduce the concept of convex cube which captures all the tuples of a
datacube satisfying a constraint combination. It can be represented in a very
compact way in order to optimize both computation time and required storage
space. The convex cube is not an additional structure appended to the list of
cube variants but we propose it as a unifying structure that we use to
characterize, in a simple, sound and homogeneous way, the other quoted types of
cubes. Finally, we introduce the concept of emerging cube which captures the
significant trend inversions. characterizations."
"Numerous generalization techniques have been proposed for privacy preserving
data publishing. Most existing techniques, however, implicitly assume that the
adversary knows little about the anonymization algorithm adopted by the data
publisher. Consequently, they cannot guard against privacy attacks that exploit
various characteristics of the anonymization mechanism. This paper provides a
practical solution to the above problem. First, we propose an analytical model
for evaluating disclosure risks, when an adversary knows everything in the
anonymization process, except the sensitive values. Based on this model, we
develop a privacy principle, transparent l-diversity, which ensures privacy
protection against such powerful adversaries. We identify three algorithms that
achieve transparent l-diversity, and verify their effectiveness and efficiency
through extensive experiments with real data."
"This paper deals with personalization of annotated OLAP systems. Data
constellation is extended to support annotations and user preferences.
Annotations reflect the decision-maker experience whereas user preferences
enable users to focus on the most interesting data. User preferences allow
annotated contextual recommendations helping the decision-maker during his/her
multidimensional navigations."
"This paper deals with decision support systems resting on multidimensional
modelling of data. Moreover, we intend to offer a set of concepts and
mechanisms for personalized multidimensional database specifications. This
personalization consists in associating weights to different components of a
multidimensional schema. Personalization specifications are specified through
the use of a language based on the principle of Event Condition Action. This
personalisation determines multidimensional data display as well as their
analyses (with the use of drilling or rotating operations)."
"Nowadays, decisional systems have became a significant research topic in
databases. Data warehouses and data marts are the main elements of such
systems. This paper presents our decisional support system. We present
graphical interfaces which help the administrator to build data warehouses and
data marts. We present a data warehouse building interface based on an
object-oriented conceptual model. This model allows the warehouse data
historisation at three levels: attribute, class and environment. Also, we
present a data mart building interface which allows warehouse data to be
reorganised through a multidimensional object-oriented model."
"This article deals with OLAP systems based on multidimensional model. The
conceptual model we provide, represents data through a constellation
(multi-facts) composed of several multi-hierarchy dimensions. In this model,
data are displayed through multidimensional tables. We define a query algebra
handling these tables. This user oriented algebra is composed of a closure core
of OLAP operators as soon as advanced operators dedicated to complex analysis.
Finally, we specify a graphical OLAP language based on this algebra. This
language facilitates analyses of decision makers."
"This paper describes an object-oriented model for designing complex and
time-variant data warehouse data. The main contribution is the warehouse class
concept, which extends the class concept by temporal and archive filters as
well as a mapping function. Filters allow the keeping of relevant data changes
whereas the mapping function defines the warehouse class schema from a global
data source schema. The approach take into account static properties as well as
dynamic properties. The behaviour extraction is based on the use-matrix
concept."
"Decisional systems are based on multidimensional databases improving OLAP
analyses. The paper describes a new OLAP operator named ""BLEND"" to perform
multigradual analyses. The operation transforms multidimensional structures
during querying in order to analyse measures according to various granularity
levels, which are reorganised into a single parameter. We study valid
combinations of the operation in the context of strict hierarchies. First
experimentations implement the operation in an R-OLAP framework showing the
slight cost of this operation."
"This paper defines a constraint-based model dedicated to multidimensional
databases. The model we define represents data through a constellation of facts
(subjects of analyse) associated to dimensions (axis of analyse), which are
possibly shared. Each dimension is organised according to several hierarchies
(views of analyse) integrating several levels of data granularity. In order to
insure data consistency, we introduce 5 semantic constraints (exclusion,
inclusion, partition, simultaneity, totality) which can be intra-dimension or
inter-dimensions; the intra-dimension constraints allow the expression of
constraints between hierarchies within a same dimension whereas the
inter-dimensions constraints focus on hierarchies of distinct dimensions. We
also study repercussions of these constraints on multidimensional manipulations
and we provide extensions of the multidimensional operators."
"This paper deals with temporal and archive object-oriented data warehouse
modelling and querying. In a first step, we define a data model describing
warehouses as central repositories of complex and temporal data extracted from
one information source. The model is based on the concepts of warehouse object
and environment. A warehouse object is composed of one current state, several
past states (modelling value changes) and several archive states (summarising
some value changes). An environment defines temporal parts in a warehouse
schema according to a relevant granularity (attribute, class or graph). In a
second step, we provide a query algebra dedicated to data warehouses. This
algebra, which is based on common object algebras, integrates temporal
operators and operators for querying object states. An other important
contribution concerns dedicated operators allowing users to transform warehouse
objects in temporal series as well as operators facilitating analytical
treatments."
"In this paper, we study the data warehouse modelling used in decision support
systems. We provide an object-oriented data warehouse model allowing data
warehouse description as a central repository of relevant, complex and temporal
data. Our model integrates three concepts such as warehouse object, environment
and warehouse class. Each warehouse object is composed of one current state,
several past states (modelling its detailed evolutions) and several archive
states (modelling its evolutions within a summarised form). The environment
concept defines temporal parts in the data warehouse schema with significant
granularities (attribute, class, graph). Finally, we provide five functions
aiming at defining the data warehouse structures and two functions allowing the
warehouse class inheritance hierarchy organisation."
"Multidimensional databases support efficiently on-line analytical processing
(OLAP). In this paper, we depict a model dedicated to multidimensional
databases. The approach we present designs decisional information through a
constellation of facts and dimensions. Each dimension is possibly shared
between several facts and it is organised according to multiple hierarchies. In
addition, we define a comprehensive query algebra regrouping the more popular
multidimensional operations in current commercial systems and research
approaches. We introduce new operators dedicated to a constellation. Finally,
we describe a prototype that allows managers to query constellations of facts,
dimensions and multiple hierarchies."
"Time Series Data Server (TSDS) is a software package for implementing a
server that provides fast super-setting, sub-setting, filtering, and uniform
gridding of time series-like data. TSDS was developed to respond quickly to
requests for long time spans of data. Data may be served from a fast database,
typically created by aggregating granules (e.g., data files) from a remote data
source and storing them in a local cache that is optimized for serving time
series. The system was designed specifically for time series data, and is
optimized for requests where the longest dimension of the requested data
structure is time. Scalar, vector, and spectrogram time series types are
supported. The user can interact with the server by requesting a time series, a
date range, and an optional filter to apply to the data. Available filters
include strides, block average/minimum/maximum, exclude, and inequality.
Constraint expressions are supported, which allow such operations as a request
for data from one time series when a different time series satisfied a
specified relationship. TSDS builds upon DAP (Data Access Protocol), NcML
(netCDF Mark-up language) and related software libraries. In this work, we
describe the current design of this server, as well as planned features and
potential implementation strategies."
"Performance tuning of Database Management Systems(DBMS) is both complex and
challenging as it involves identifying and altering several key performance
tuning parameters. The quality of tuning and the extent of performance
enhancement achieved greatly depends on the skill and experience of the
Database Administrator (DBA). As neural networks have the ability to adapt to
dynamically changing inputs and also their ability to learn makes them ideal
candidates for employing them for tuning purpose. In this paper, a novel tuning
algorithm based on neural network estimated tuning parameters is presented. The
key performance indicators are proactively monitored and fed as input to the
Neural Network and the trained network estimates the suitable size of the
buffer cache, shared pool and redo log buffer size. The tuner alters these
tuning parameters using the estimated values using a rate change computing
algorithm. The preliminary results show that the proposed method is effective
in improving the query response time for a variety of workload types. ."
"Functional dependencies -- traditional, approximate and conditional are of
critical importance in relational databases, as they inform us about the
relationships between attributes. They are useful in schema normalization, data
rectification and source selection. Most of these were however developed in the
context of deterministic data. Although uncertain databases have started
receiving attention, these dependencies have not been defined for them, nor are
fast algorithms available to evaluate their confidences. This paper defines the
logical extensions of various forms of functional dependencies for
probabilistic databases and explores the connections between them. We propose a
pruning-based exact algorithm to evaluate the confidence of functional
dependencies, a Monte-Carlo based algorithm to evaluate the confidence of
approximate functional dependencies and algorithms for their conditional
counterparts in probabilistic databases. Experiments are performed on both
synthetic and real data evaluating the performance of these algorithms in
assessing the confidence of dependencies and mining them from data. We believe
that having these dependencies and algorithms available for probabilistic
databases will drive adoption of probabilistic data storage in the industry."
"This paper will propose a novel star schema attribute induction as a new
attribute induction paradigm and as improving from current attribute oriented
induction. A novel star schema attribute induction will be examined with
current attribute oriented induction based on characteristic rule and using non
rule based concept hierarchy by implementing both of approaches. In novel star
schema attribute induction some improvements have been implemented like
elimination threshold number as maximum tuples control for generalization
result, there is no ANY as the most general concept, replacement the role
concept hierarchy with concept tree, simplification for the generalization
strategy steps and elimination attribute oriented induction algorithm. Novel
star schema attribute induction is more powerful than the current attribute
oriented induction since can produce small number final generalization tuples
and there is no ANY in the results."
"Nowadays, Data Warehouse (DW) plays a crucial role in the process of decision
making. However, their design remains a very delicate and difficult task either
for expert or users. The goal of this paper is to propose a new approach based
on the clover model, destined to assist users to design a DW. The proposed
approach is based on two main steps. The first one aims to guide users in their
choice of DW schema model. The second one aims to finalize the chosen model by
offering to the designer views related to former successful DW design
experiences."
"There have been many recent studies on sequential pattern mining. The
sequential pattern mining on progressive databases is relatively very new, in
which we progressively discover the sequential patterns in period of interest.
Period of interest is a sliding window continuously advancing as the time goes
by. As the focus of sliding window changes, the new items are added to the
dataset of interest and obsolete items are removed from it and become up to
date. In general, the existing proposals do not fully explore the real world
scenario, such as items associated with support in data stream applications
such as market basket analysis. Thus mining important knowledge from supported
frequent items becomes a non trivial research issue. Our proposed novel
approach efficiently mines frequent sequential pattern coupled with support
using progressive mining tree."
"Sharing musical files via the Internet was the essential motivation of early
P2P systems. Despite of the great success of the P2P file sharing systems,
these systems support only ""simple"" queries. The focus in such systems is how
to carry out an efficient query routing in order to find the nodes storing a
desired file. Recently, several research works have been made to extend P2P
systems to be able to share data having a fine granularity (i.e. atomic
attribute) and to process queries written with a highly expressive language
(i.e. SQL). These works have led to the emergence of P2P data sharing systems
that represent a new generation of P2P systems and, on the other hand, a next
stage in a long period of the database research area. ? The characteristics of
P2P systems (e.g. large-scale, node autonomy and instability) make impractical
to have a global catalog that represents often an essential component in
traditional database systems. Usually, such a catalog stores information about
data, schemas and data sources. Query routing and processing are two problems
affected by the absence of a global catalog. Locating relevant data sources and
generating a close to optimal execution plan become more difficult. In this
paper, we concentrate our study on proposed solutions for the both problems.
Furthermore, selected case studies of main P2P data sharing systems are
analyzed and compared."
"In this paper we deal with the notion of semantic loss in Peer Data
Management Systems (PDMS) queries. We define such a notion and we give a
mechanism that discovers semantic loss in a PDMS network. Next, we propose an
algorithm that addresses the problem of restoring such a loss. Further
evaluation of our proposed algorithm is an ongoing work"
"Different types of data skew can result in load imbalance in the context of
parallel joins under the shared nothing architecture. We study one important
type of skew, join product skew (JPS). A static approach based on frequency
classes is proposed which takes for granted the data distribution of join
attribute values. It comes from the observation that the join selectivity can
be expressed as a sum of products of frequencies of the join attribute values.
As a consequence, an appropriate assignment of join sub-tasks, that takes into
consideration the magnitude of the frequency products can alleviate the join
product skew. Motivated by the aforementioned remark, we propose an algorithm,
called Handling Join Product Skew (HJPS), to handle join product skew."
"In this demonstration, we propose a model for the management of XML time
series (TS), using the new XQuery 1.1 window operator. We argue that
centralized computation is slow, and demonstrate XQ2P, our prototype of
efficient XQuery P2P TS computation in the context of financial analysis of
large data sets (>1M values)."
"In this paper, we propose a simple generic model to manage time series. A
time series is composed of a calendar with a typed value for each calendar
entry. Although the model could support any kind of XML typed values, in this
paper we focus on real numbers, which are the usual application. We define
basic vector space operations (plus, minus, scale), and also relational-like
and application oriented operators to manage time series. We show the interest
of this generic model on two applications: (i) a stock investment helper; (ii)
an ecological transport management system. Stock investment requires
window-based operations while trip management requires complex queries. The
model has been implemented and tested in PHP, Java, and XQuery. We show
benchmark results illustrating that the computing of 5000 series of over
100.000 entries in length - common requirements for both applications - is
difficult on classical centralized PCs. In order to serve a community of users
sharing time series, we propose a P2P implementation of time series by dividing
them in segments and providing optimized algorithms for operator expression
computation."
"The amounts of data available to decision makers are increasingly important,
given the network availability, low cost storage and diversity of applications.
To maximize the potential of these data within the National Social Security
Fund (NSSF) in Tunisia, we have built a data warehouse as a multidimensional
database, cleaned, homogenized, historicized and consolidated. We used Oracle
Warehouse Builder to extract, transform and load the source data into the Data
Warehouse, by applying the KDD process. We have implemented the Data Warehouse
as an Oracle OLAP. The knowledge extraction has been performed using the Oracle
Discoverer tool. This allowed users to take maximum advantage of knowledge as a
regular report or as ad hoc queries. We started by implementing the main topic
for this public institution, accounting for the movements of insured persons.
The great success that has followed the completion of this work has encouraged
the NSSF to complete the achievement of other topics of interest within the
NSSF. We suggest in the near future to use Multidimensional Data Mining to
extract hidden knowledge and that are not predictable by the OLAP."
"This paper describes our experience with using Grid files as the main storage
organization for a relational database management system. We primarily focus on
the following two aspects. (i) Strategies for implementing grid files
efficiently. (ii) Methods for efficiency evaluating queries posed to a database
organized using grid files."
"The emergence of new higher education institutions has created the
competition in higher education market, and data warehouse can be used as an
effective technology tools for increasing competitiveness in the higher
education market. Data warehouse produce reliable reports for the institution's
high-level management in short time for faster and better decision making, not
only on increasing the admission number of students, but also on the
possibility to find extraordinary, unconventional funds for the institution.
Efficiency comparison was based on length and amount of processed records,
total processed byte, amount of processed tables, time to run query and
produced record on OLTP database and data warehouse. Efficiency percentages was
measured by the formula for percentage increasing and the average efficiency
percentage of 461.801,04% shows that using data warehouse is more powerful and
efficient rather than using OLTP database. Data warehouse was modeled based on
hypercube which is created by limited high demand reports which usually used by
high level management. In every table of fact and dimension fields will be
inserted which represent the loading constructive merge where the ETL
(Extraction, Transformation and Loading) process is run based on the old and
new files."
"Searching learning or rules in relational database for data mining purposes
with characteristic or classification/discriminant rule in attribute oriented
induction technique can be quicker, easy, and simple with simple SQL statement.
With just only one simple SQL statement, characteristic and classification rule
can be created simultaneously. Collaboration SQL statement with any other
application software will increase the ability for creating t-weight as
measurement the typicality of each record in the characteristic rule and
d-weight as measurement the discriminating behavior of the learned
classification/discriminant rule, particularly for further generalization in
characteristic rule. Handling concept hierarchy into tables based on concept
tree will influence for the successful simple SQL statement and by knowing the
right standard knowledge to transform each of concept tree in concept hierarchy
into one table as transforming concept hierarchy into table, the simple SQL
statement can be run properly."
"Multidimensional in data warehouse is a compulsion and become the most
important for information delivery, without multidimensional Multidimensional
in data warehouse is a compulsion and become the most important for information
delivery, without multidimensional datawarehouse is incomplete.
Multidimensional give ability to analyze business measurement in many different
ways. Multidimensional is also synonymous with online analytical processing
(OLAP). By using some concepts in datawarehouse like slice-dice,drill down and
roll up will increase the ability of multidimensional datawarehouse. The
research question and the discussing for this paper are how much deepest the
multidimensional ability from each fact table in datawarehouse. By using the
statistic combination formula we try to explore the combination that can be
yielded from each dimension in hypercubes, the entire of dimensi combination,
minimum combination and maximum combination."
"Multidimensional in data warehouse is a compulsion and become the most
important for information delivery, without multidimensional data warehouse is
incomplete. Multidimensional give the able to analyze business measurement in
many different ways. Multidimensional is also synonymous with online analytical
processing (OLAP)."
"A simple sql statement can be used to search learning or rule in relational
database for data mining purposes particularly for classification rule. With
just only one simple sql statement, characteristic and classification rule can
be created simultaneously. Collaboration sql statement with any other
application software will increase the ability for creating t-weight as
measurement the typicality of each record in the characteristic rule and
d-weight as measurement the discriminating behavior of the learned
classification/discriminant rule, specifically for further generalization in
characteristic rule. Handling concept hierarchy into tables based on concept
tree will influence for the successful simple sql statement and by knowing the
right standard knowledge to transform each of concept tree in concept hierarchy
into one table as to transform concept hierarchy into table, the simple sql
statement can be run properly."
"In OLAP, analysts often select an interesting sample of the data. For
example, an analyst might focus on products bringing revenues of at least 100
000 dollars, or on shops having sales greater than 400 000 dollars. However,
current systems do not allow the application of both of these thresholds
simultaneously, selecting products and shops satisfying both thresholds. For
such purposes, we introduce the diamond cube operator, filling a gap among
existing data warehouse operations.
  Because of the interaction between dimensions the computation of diamond
cubes is challenging. We compare and test various algorithms on large data sets
of more than 100 million facts. We find that while it is possible to implement
diamonds in SQL, it is inefficient. Indeed, our custom implementation can be a
hundred times faster than popular database engines (including a row-store and a
column-store)."
"We present a generic API suitable for provision of highly generic storage
facilities that can be tailored to produce various individually customised
storage infrastructures. The paper identifies a candidate set of minimal
storage system building blocks, which are sufficiently simple to avoid
encapsulating policy where it cannot be customised by applications, and
composable to build highly flexible storage architectures. Four main generic
components are defined: the store, the namer, the caster and the interpreter.
It is hypothesised that these are sufficiently general that they could act as
building blocks for any information storage and retrieval system. The essential
characteristics of each are defined by an interface, which may be implemented
by multiple implementing classes."
"Each time-series has its own linear trend, the directionality of a
timeseries, and removing the linear trend is crucial to get the more intuitive
matching results. Supporting the linear detrending in subsequence matching is a
challenging problem due to a huge number of possible subsequences. In this
paper we define this problem the linear detrending subsequence matching and
propose its efficient index-based solution. To this end, we first present a
notion of LD-windows (LD means linear detrending), which is obtained as
follows: we eliminate the linear trend from a subsequence rather than each
window itself and obtain LD-windows by dividing the subsequence into windows.
Using the LD-windows we then present a lower bounding theorem for the
index-based matching solution and formally prove its correctness. Based on the
lower bounding theorem, we next propose the index building and subsequence
matching algorithms for linear detrending subsequence matching.We finally show
the superiority of our index-based solution through extensive experiments."
"This project addressed the conceptual fundamentals of data storage,
investigating techniques for provision of highly generic storage facilities
that can be tailored to produce various individually customised storage
infrastructures, compliant to the needs of particular applications. This
requires the separation of mechanism and policy wherever possible. Aspirations
include: actors, whether users or individual processes, should be able to bind
to, update and manipulate data and programs transparently with respect to their
respective locations; programs should be expressed independently of the storage
and network technology involved in their execution; storage facilities should
be structure-neutral so that actors can impose multiple interpretations over
information, simultaneously and safely; information should not be discarded so
that arbitrary historical views are supported; raw stored information should be
open to all; where security restrictions on its use are required this should be
achieved using cryptographic techniques. The key advances of the research were:
1) the identification of a candidate set of minimal storage system building
blocks, which are sufficiently simple to avoid encapsulating policy where it
cannot be customised by applications, and composable to build highly flexible
storage architectures 2) insight into the nature of append-only storage
components, and the issues arising from their application to common storage
use-cases."
"Data warehouses are overwhelmingly built through a bottom-up process, which
starts with the identification of sources, continues with the extraction and
transformation of data from these sources, and then loads the data into a set
of data marts according to desired multidimensional relational schemas. End
user business intelligence tools are added on top of the materialized
multidimensional schemas to drive decision making in an organization.
Unfortunately, this bottom-up approach is costly both in terms of the skilled
users needed and the sheer size of the warehouses. This paper proposes a
top-down framework in which data warehousing is driven by a conceptual model.
The framework offers both design time and run time environments. At design
time, a business user first uses the conceptual modeling language as a
multidimensional object model to specify what business information is needed;
then she maps the conceptual model to a pre-existing logical multidimensional
representation. At run time, a system will transform the user conceptual model
together with the mappings into views over the logical multidimensional
representation. We focus on how the user can conceptually abstract from an
existing data warehouse, and on how this conceptual model can be mapped to the
logical multidimensional representation. We also give an indication of what
query language is used over the conceptual model. Finally, we argue that our
framework is a step along the way to allowing automatic generation of the data
warehouse."
"Most of the organizations put information on the web because they want it to
be seen by the world. Their goal is to have visitors come to the site, feel
comfortable and stay a while and try to know completely about the running
organization. As educational system increasingly requires data mining, the
opportunity arises to mine the resulting large amounts of student information
for hidden useful information (patterns like rule, clustering, and
classification, etc). The education domain offers ground for many interesting
and challenging data mining applications like astronomy, chemistry,
engineering, climate studies, geology, oceanography, ecology, physics, biology,
health sciences and computer science. Collecting the interesting patterns using
the required interestingness measures, which help us in discovering the
sophisticated patterns that are ultimately used for developing the site. We
study the application of data mining to educational log data collected from
Guru Nanak Institute of Technology, Ibrahimpatnam, India. We have proposed a
custom-built apriori algorithm to find the effective pattern analysis. Finally,
analyzing web logs for usage and access trends can not only provide important
information to web site developers and administrators, but also help in
creating adaptive web sites."
"Problem statement: Clustering has a number of techniques that have been
developed in statistics, pattern recognition, data mining, and other fields.
Subspace clustering enumerates clusters of objects in all subspaces of a
dataset. It tends to produce many over lapping clusters. Approach: Subspace
clustering and projected clustering are research areas for clustering in high
dimensional spaces. In this research we experiment three clustering oriented
algorithms, PROCLUS, P3C and STATPC. Results: In general, PROCLUS performs
better in terms of time of calculation and produced the least number of
un-clustered data while STATPC outperforms PROCLUS and P3C in the accuracy of
both cluster points and relevant attributes found. Conclusions/Recommendations:
In this study, we analyze in detail the properties of different data clustering
method."
"To analyze complex phenomena which involve moving objects, Trajectory Data
Warehouse (TDW) seems to be an answer for many recent decision problems related
to various professions (physicians, commercial representatives, transporters,
ecologists ...) concerned with mobility. This work aims to make trajectories as
a first class concept in the trajectory data conceptual model and to design a
TDW, in which data resulting from mobile information collectors' trajectory are
gathered. These data will be analyzed, according to trajectory characteristics,
for decision making purposes, such as new products commercialization, new
commerce implementation, etc."
"In this paper a novel fragile watermarking scheme is proposed to detect,
localize and recover malicious modifications in relational databases. In the
proposed scheme, all tuples in the database are first securely divided into
groups. Then watermarks are embedded and verified group-by-group independently.
By using the embedded watermark, we are able to detect and localize the
modification made to the database and even we recover the true data from the
database modified locations. Our experimental results show that this scheme is
so qualified; i.e. distortion detection and true data recovery both are
performed successfully."
"A target-oriented sequential pattern is a sequential pattern with a concerned
itemset in the end of pattern. A time-interval sequential pattern is a
sequential pattern with time-intervals between every pair of successive
itemsets. In this paper we present an algorithm to discover target-oriented
sequential pattern with time-intervals. To this end, the original sequences are
reversed so that the last itemsets can be arranged in front of the sequences.
The contrasts between reversed sequences and the concerned itemset are then
used to exclude the irrelevant sequences. Clustering analysis is used with
typical sequential pattern mining algorithm to extract the sequential patterns
with time-intervals between successive itemsets. Finally, the discovered
time-interval sequential patterns are reversed again to the original order for
searching the target patterns."
"A Transaction database contains a set of transactions along with items and
their associated timestamps. Transitional patterns are the patterns which
specify the dynamic behavior of frequent patterns in a transaction database. To
discover transitional patterns and their significant milestones, first we have
to extract all frequent patterns and their supports using any frequent pattern
generation algorithm. These frequent patterns are used in the generation of
transitional patterns. The existing algorithm (TP-Mine) generates frequent
patterns, some of which cannot be used in generation of transitional patterns.
In this paper, we propose a modification to the existing algorithm, which
prunes the candidate items to be used in the generation of frequent patterns.
This method drastically reduces the number of frequent patterns which are used
in discovering transitional patterns. Extensive simulation test is done to
evaluate the proposed method."
"We study active integrity constraints and revision programming, two
formalisms designed to describe integrity constraints on databases and to
specify policies on preferred ways to enforce them. Unlike other more commonly
accepted approaches, these two formalisms attempt to provide a declarative
solution to the problem. However, the original semantics of founded repairs for
active integrity constraints and justified revisions for revision programs
differ. Our main goal is to establish a comprehensive framework of semantics
for active integrity constraints, to find a parallel framework for revision
programs, and to relate the two. By doing so, we demonstrate that the two
formalisms proposed independently of each other and based on different
intuitions when viewed within a broader semantic framework turn out to be
notational variants of each other. That lends support to the adequacy of the
semantics we develop for each of the formalisms as the foundation for a
declarative approach to the problem of database update and repair. In the paper
we also study computational properties of the semantics we consider and
establish results concerned with the concept of the minimality of change and
the invariance under the shifting transformation."
"A Blink Tree latch method and protocol supports synchronous node deletion in
a high concurrency environment. Full source code is available."
"Recently, the cyclic association rules have been introduced in order to
discover rules from items characterized by their regular variation over time.
In real life situations, temporal databases are often appended or updated.
Rescanning the whole database every time is highly expensive while existing
incremental mining techniques can efficiently solve such a problem. In this
paper, we propose an incremental algorithm for cyclic association rules
maintenance. The carried out experiments of our proposal stress on its
efficiency and performance."
"Ontologies such as taxonomies, product catalogs or web directories are
heavily used and hence evolve frequently to meet new requirements or to better
reflect the current instance data of a domain. To effectively manage the
evolution of ontologies it is essential to identify the difference (Diff)
between two ontology versions. We propose a novel approach to determine an
expressive and invertible diff evolution mapping between given versions of an
ontology. Our approach utilizes the result of a match operation to determine an
evolution mapping consisting of a set of basic change operations
(insert/update/delete). To semantically enrich the evolution mapping we adopt a
rule-based approach to transform the basic change operations into a smaller set
of more complex change operations, such as merge, split, or changes of entire
subgraphs. The proposed algorithm is customizable in different ways to meet the
requirements of diverse ontologies and application scenarios. We evaluate the
proposed approach by determining and analyzing evolution mappings for
real-world life science ontologies and web directories."
"A large amount of transaction data containing associations between
individuals and sensitive information flows everyday into data stores. Examples
include web queries, credit card transactions, medical exam records, transit
database records. The serial release of these data to partner institutions or
data analysis centers is a common situation. In this paper we show that, in
most domains, correlations among sensitive values associated to the same
individuals in different releases can be easily mined, and used to violate
users' privacy by adversaries observing multiple data releases. We provide a
formal model for privacy attacks based on this sequential background knowledge,
as well as on background knowledge on the probability distribution of sensitive
values over different individuals. We show how sequential background knowledge
can be actually obtained by an adversary, and used to identify with high
confidence the sensitive values associated with an individual. A defense
algorithm based on Jensen-Shannon divergence is proposed, and extensive
experiments show the superiority of the proposed technique with respect to
other applicable solutions. To the best of our knowledge, this is the first
work that systematically investigates the role of sequential background
knowledge in serial release of transaction data."
"The electronic marketplace offers great potential for the recommendation of
supplies. In the so called recommender systems, it is crucial to apply
matchmaking strategies that faithfully satisfy the predicates specified in the
demand, and take into account as much as possible the user preferences. We
focus on real-life ontology-driven matchmaking scenarios and identify a number
of challenges, being inspired by such scenarios. A key challenge is that of
presenting the results to the users in an understandable and clear-cut fashion
in order to facilitate the analysis of the results. Indeed, such scenarios
evoke the opportunity to rank and group the results according to specific
criteria. A further challenge consists of presenting the results to the user in
an asynchronous fashion, i.e. the 'push' mode, along with the 'pull' mode, in
which the user explicitly issues a query, and displays the results. Moreover,
an important issue to consider in real-life cases is the possibility of
submitting a query to multiple providers, and collecting the various results.
We have designed and implemented an ontology-based matchmaking system that
suitably addresses the above challenges. We have conducted a comprehensive
experimental study, in order to investigate the usability of the system, the
performance and the effectiveness of the matchmaking strategies with real
ontological datasets."
"Commutative Replicated Data-Type (CRDT) is a new class of algorithms that
ensures scalable consistency of replicated data. It has been successfully
applied to collaborative editing of texts without complex concurrency control.
In this paper, we present a CRDT to edit XML data. Compared to existing
approaches for XML collaborative editing, our approach is more scalable and
handles all the XML editing aspects : elements, contents, attributes and undo.
Indeed, undo is recognized as an important feature for collaborative editing
that allows to overcome system complexity through error recovery or
collaborative conflict resolution."
"The skyline concept has been introduced in order to exhibit the best objects
according to all the criterion combinations and makes it possible to analyse
the relationships between skyline objects. Like the data cube, the skycube is
so voluminous that reduction approaches are really necessary. In this paper, we
define an approach which partially materializes the skycube. The underlying
idea is to discard from the representation the skycuboids which can be computed
again the most easily. To meet this reduction objective, we characterize a
formal framework: the agree concept lattice. From this structure, we derive the
skyline concept lattice which is one of its constrained instances. The strong
points of our approach are: (i) it is attribute oriented; (ii) it provides a
boundary for the number of lattice nodes; (iii) it facilitates the navigation
within the Skycuboids."
"We study in this paper provenance information for queries with aggregation.
Provenance information was studied in the context of various query languages
that do not allow for aggregation, and recent work has suggested to capture
provenance by annotating the different database tuples with elements of a
commutative semiring and propagating the annotations through query evaluation.
We show that aggregate queries pose novel challenges rendering this approach
inapplicable. Consequently, we propose a new approach, where we annotate with
provenance information not just tuples but also the individual values within
tuples, using provenance to describe the values computation. We realize this
approach in a concrete construction, first for ""simple"" queries where the
aggregation operator is the last one applied, and then for arbitrary (positive)
relational algebra queries with aggregation; the latter queries are shown to be
more challenging in this context. Finally, we use aggregation to encode queries
with difference, and study the semantics obtained for such queries on
provenance annotated databases."
"In this paper, we propose a novel, effective and efficient probabilistic
pruning criterion for probabilistic similarity queries on uncertain data. Our
approach supports a general uncertainty model using continuous probabilistic
density functions to describe the (possibly correlated) uncertain attributes of
objects. In a nutshell, the problem to be solved is to compute the PDF of the
random variable denoted by the probabilistic domination count: Given an
uncertain database object B, an uncertain reference object R and a set D of
uncertain database objects in a multi-dimensional space, the probabilistic
domination count denotes the number of uncertain objects in D that are closer
to R than B. This domination count can be used to answer a wide range of
probabilistic similarity queries. Specifically, we propose a novel geometric
pruning filter and introduce an iterative filter-refinement strategy for
conservatively and progressively estimating the probabilistic domination count
in an efficient way while keeping correctness according to the possible world
semantics. In an experimental evaluation, we show that our proposed technique
allows to acquire tight probability bounds for the probabilistic domination
count quickly, even for large uncertain databases."
"There are many clustering methods, such as hierarchical clustering method.
Most of the approaches to the clustering of variables encountered in the
literature are of hierarchical type. The great majority of hierarchical
approaches to the clustering of variables are of agglomerative nature. The
agglomerative hierarchical approach to clustering starts with each observation
as its own cluster and then continually groups the observations into
increasingly larger groups. Higher Learning Institution (HLI) provides training
to introduce final-year students to the real working environment. In this
research will use Euclidean single linkage and complete linkage. MATLAB and HCE
3.5 software will used to train data and cluster course implemented during
industrial training. This study indicates that different method will create a
different number of clusters."
"Log files contain information about User Name, IP Address, Time Stamp, Access
Request, number of Bytes Transferred, Result Status, URL that Referred and User
Agent. The log files are maintained by the web servers. By analysing these log
files gives a neat idea about the user. This paper gives a detailed discussion
about these log files, their formats, their creation, access procedures, their
uses, various algorithms used and the additional parameters that can be used in
the log files which in turn gives way to an effective mining. It also provides
the idea of creating an extended log file and learning the user behaviour."
"Consider the situation where a query is to be answered using Web sources that
restrict the accesses that can be made on backend relational data by requiring
some attributes to be given as input of the service. The accesses provide
lookups on the collection of attributes values that match the binding. They can
differ in whether or not they require arguments to be generated from prior
accesses. Prior work has focused on the question of whether a query can be
answered using a set of data sources, and in developing static access plans
(e.g., Datalog programs) that implement query answering. We are interested in
dynamic aspects of the query answering problem: given partial information about
the data, which accesses could provide relevant data for answering a given
query? We consider immediate and long-term notions of ""relevant accesses"", and
ascertain the complexity of query relevance, for both conjunctive queries and
arbitrary positive queries. In the process, we relate dynamic relevance of an
access to query containment under access limitations and characterize the
complexity of this problem; we produce several complexity results about
containment that are of interest by themselves."
"Querying is one of the basic functionality expected from a database system.
Query efficiency is adversely affected by increase in the number of
participating tables. Also, querying based on syntax largely limits the gamut
of queries a database system can process. Syntactic queries rely on the
database table structure, which is a cause of concern for large organisations
due to incompatibility between heterogeneous systems that store data
distributed across geographic locations. Solution to these problems is answered
to some extent by moving towards semantic technology by making data and the
database meaningful. In doing so, relationship between sets of entity sets will
not be limited only to syntactic constraints but would also permit semantic
connections nonetheless such relationships may be tacit, intangible and
invisible. The goal of this work is to extract such hidden relationships
between unrelated sets of entity sets and store them in a tangible form. A few
sample cases are provided to vindicate that the proposed work improves querying
significantly."
"XML data projection (or pruning) is a natural optimization for main memory
query engines: given a query Q over a document D, the subtrees of D that are
not necessary to evaluate Q are pruned, thus producing a smaller document D';
the query Q is then executed on D', hence avoiding to allocate and process
nodes that will never be reached by Q. In this article, we propose a new
approach, based on types, that greatly improves current solutions. Besides
providing comparable or greater precision and far lesser pruning overhead, our
solution ---unlike current approaches--- takes into account backward axes,
predicates, and can be applied to multiple queries rather than just to single
ones. A side contribution is a new type system for XPath able to handle
backward axes. The soundness of our approach is formally proved. Furthermore,
we prove that the approach is also complete (i.e., yields the best possible
type-driven pruning) for a relevant class of queries and Schemas. We further
validate our approach using the XMark and XPathMark benchmarks and show that
pruning not only improves the main memory query engine's performances (as
expected) but also those of state of the art native XML databases."
"Web log data is usually diverse and voluminous. This data must be assembled
into a consistent, integrated and comprehensive view, in order to be used for
pattern discovery. Without properly cleaning, transforming and structuring the
data prior to the analysis, one cannot expect to find meaningful patterns. As
in most data mining applications, data preprocessing involves removing and
filtering redundant and irrelevant data, removing noise, transforming and
resolving any inconsistencies. In this paper, a complete preprocessing
methodology having merging, data cleaning, user/session identification and data
formatting and summarization activities to improve the quality of data by
reducing the quantity of data has been proposed. To validate the efficiency of
the proposed preprocessing methodology, several experiments are conducted and
the results show that the proposed methodology reduces the size of Web access
log files down to 73-82% of the initial size and offers richer logs that are
structured for further stages of Web Usage Mining (WUM). So preprocessing of
raw data in this WUM process is the central theme of this paper."
"The appearance of wireless communication is dramatically changing our life.
Mobile telecommunications emerged as a technological marvel allowing for access
to personal and other services, devices, computation and communication, in any
place and at any time through effortless plug and play. Setting up wireless
mobile networks often requires: Frequency Assignment, Communication Protocol
selection, Routing schemes selection, and cells towers distributions. This
research aims to optimize the cells towers distribution by using spatial mining
with Geographic Information System (GIS) as a tool. The distribution
optimization could be done by applying the Digital Elevation Model (DEM) on the
image of the area which must be covered with two levels of hierarchy. The
research will apply the spatial association rules technique on the second level
to select the best square in the cell for placing the antenna. From that the
proposal will try to minimize the number of installed towers, makes tower's
location feasible, and provides full area coverage."
"We present a novel approach that protects trajectory privacy of users who
access location-based services through a moving k nearest neighbor (MkNN)
query. An MkNN query continuously returns the k nearest data objects for a
moving user (query point). Simply updating a user's imprecise location such as
a region instead of the exact position to a location-based service provider
(LSP) cannot ensure privacy of the user for an MkNN query: continuous
disclosure of regions enables the LSP to follow a user's trajectory. We
identify the problem of trajectory privacy that arises from the overlap of
consecutive regions while requesting an MkNN query and provide the first
solution to this problem. Our approach allows a user to specify the confidence
level that represents a bound of how much more the user may need to travel than
the actual kth nearest data object. By hiding a user's required confidence
level and the required number of nearest data objects from an LSP, we develop a
technique to prevent the LSP from tracking the user's trajectory for MkNN
queries. We propose an efficient algorithm for the LSP to find k nearest data
objects for a region with a user's specified confidence level, which is an
essential component to evaluate an MkNN query in a privacy preserving manner;
this algorithm is at least two times faster than the state-of-the-art
algorithm. Extensive experimental studies validate the effectiveness of our
trajectory privacy protection technique and the efficiency of our algorithm."
"Index tuning, i.e., selecting the indexes appropriate for a workload, is a
crucial problem in database system tuning. In this paper, we solve index tuning
for large problem instances that are common in practice, e.g., thousands of
queries in the workload, thousands of candidate indexes and several hard and
soft constraints. Our work is the first to reveal that the index tuning problem
has a well structured space of solutions, and this space can be explored
efficiently with well known techniques from linear optimization. Experimental
results demonstrate that our approach outperforms state-of-the-art commercial
and research techniques by a significant margin (up to an order of magnitude)."
"Markov Logic Networks (MLNs) have emerged as a powerful framework that
combines statistical and logical reasoning; they have been applied to many data
intensive problems including information extraction, entity resolution, and
text mining. Current implementations of MLNs do not scale to large real-world
data sets, which is preventing their wide-spread adoption. We present Tuffy
that achieves scalability via three novel contributions: (1) a bottom-up
approach to grounding that allows us to leverage the full power of the
relational optimizer, (2) a novel hybrid architecture that allows us to perform
AI-style local search efficiently using an RDBMS, and (3) a theoretical insight
that shows when one can (exponentially) improve the efficiency of stochastic
local search. We leverage (3) to build novel partitioning, loading, and
parallel algorithms. We show that our approach outperforms state-of-the-art
implementations in both quality and speed on several publicly available
datasets."
"In recent years, there has been a lot of interest in the field of keyword
querying relational databases. A variety of systems such as DBXplorer [ACD02],
Discover [HP02] and ObjectRank [BHP04] have been proposed. Another such system
is BANKS, which enables data and schema browsing together with keyword-based
search for relational databases. It models tuples as nodes in a graph,
connected by links induced by foreign key and other relationships. The size of
the database graph that BANKS uses is proportional to the sum of the number of
nodes and edges in the graph. Systems such as SPIN, which search on Personal
Information Networks and use BANKS as the backend, maintain a lot of
information about the users' data. Since these systems run on the user
workstation which have other demands of memory, such a heavy use of memory is
unreasonable and if possible, should be avoided. In order to alleviate this
problem, we introduce EMBANKS (acronym for External Memory BANKS), a framework
for an optimized disk-based BANKS system. The complexity of this framework
poses many questions, some of which we try to answer in this thesis. We
demonstrate that the cluster representation proposed in EMBANKS enables
in-memory processing of very large database graphs. We also present detailed
experiments that show that EMBANKS can significantly reduce database load time
and query execution times when compared to the original BANKS algorithms."
"In this paper we will present the two basic operations for database schemas
used in database mapping systems (separation and Data Federation), and we will
explain why the functorial semantics for database mappings needed a new base
category instead of usual Set category. Successively, it is presented a
definition of the graph G for a schema database mapping system, and the
definition of its sketch category Sch(G). Based on this framework we presented
functorial semantics for database mapping systems with the new base category
DB."
"With the advance of Web Services technologies and the emergence of Web
Services into the information space, tremendous opportunities for empowering
users and organizations appear in various application domains including
electronic commerce, travel, intelligence information gathering and analysis,
health care, digital government, etc. In fact, Web services appear to be s
solution for integrating distributed, autonomous and heterogeneous information
sources. However, as Web services evolve in a dynamic environment which is the
Internet many changes can occur and affect them. A Web service is affected when
one or more of its associated information sources is affected by schema
changes. Changes can alter the information sources contents but also their
schemas which may render Web services partially or totally undefined. In this
paper, we propose a solution for integrating information sources into Web
services. Then we tackle the Web service synchronization problem by
substituting the affected information sources. Our work is illustrated with a
healthcare case study."
"Obtaining an implementation of a data warehouse is a complex task that forces
designers to acquire wide knowledge of the domain, thus requiring a high level
of expertise and becoming it a prone-to-fail task. Based on our experience, we
have detected a set of situations we have faced up with in real-world projects
in which we believe that the use of ontologies will improve several aspects of
the design of data warehouses. The aim of this article is to describe several
shortcomings of current data warehouse design approaches and discuss the
benefit of using ontologies to overcome them. This work is a starting point for
discussing the convenience of using ontologies in data warehouse design."
"Consistent query answering is an inconsistency tolerant approach to obtaining
semantically correct answers from a database that may be inconsistent with
respect to its integrity constraints. In this work we formalize the notion of
consistent query answer for spatial databases and spatial semantic integrity
constraints. In order to do this, we first characterize conflicting spatial
data, and next, we define admissible instances that restore consistency while
staying close to the original instance. In this way we obtain a repair
semantics, which is used as an instrumental concept to define and possibly
derive consistent query answers. We then concentrate on a class of spatial
denial constraints and spatial queries for which there exists an efficient
strategy to compute consistent query answers. This study applies inconsistency
tolerance in spatial databases, rising research issues that shift the goal from
the consistency of a spatial database to the consistency of query answering."
"Multi-criteria decision making has been made possible with the advent of
skyline queries. However, processing such queries for high dimensional datasets
remains a time consuming task. Real-time applications are thus infeasible,
especially for non-indexed skyline techniques where the datasets arrive online.
In this paper, we propose a caching mechanism that uses the semantics of
previous skyline queries to improve the processing time of a new query. In
addition to exact queries, utilizing such special semantics allow accelerating
related queries. We achieve this by generating partial result sets guaranteed
to be in the skyline sets. We also propose an index structure for efficient
organization of the cached queries. Experiments on synthetic and real datasets
show the effectiveness and scalability of our proposed methods."
"It is known that the composition of schema mappings, each specified by
source-to-target tgds (st-tgds), can be specified by a second-order tgd (SO
tgd). We consider the question of what happens when target constraints are
allowed. Specifically, we consider the question of specifying the composition
of standard schema mappings (those specified by st-tgds, target egds, and a
weakly acyclic set of target tgds). We show that SO tgds, even with the
assistance of arbitrary source constraints and target constraints, cannot
specify in general the composition of two standard schema mappings. Therefore,
we introduce source-to-target second-order dependencies (st-SO dependencies),
which are similar to SO tgds, but allow equations in the conclusion. We show
that st-SO dependencies (along with target egds and target tgds) are sufficient
to express the composition of every finite sequence of standard schema
mappings, and further, every st-SO dependency specifies such a composition. In
addition to this expressive power, we show that st-SO dependencies enjoy other
desirable properties. In particular, they have a polynomial-time chase that
generates a universal solution. This universal solution can be used to find the
certain answers to unions of conjunctive queries in polynomial time. It is easy
to show that the composition of an arbitrary number of standard schema mappings
is equivalent to the composition of only two standard schema mappings. We show
that surprisingly, the analogous result holds also for schema mappings
specified by just st-tgds (no target constraints). This is proven by showing
that every SO tgd is equivalent to an unnested SO tgd (one where there is no
nesting of function symbols). Similarly, we prove unnesting results for st-SO
dependencies, with the same types of consequences."
"A large spectrum of applications such as location based services and
environmental monitoring demand efficient query processing on uncertain
databases. In this paper, we propose the probabilistic Voronoi diagram (PVD)
for processing moving nearest neighbor queries on uncertain data, namely the
probabilistic moving nearest neighbor (PMNN) queries. A PMNN query finds the
most probable nearest neighbor of a moving query point continuously. To process
PMNN queries efficiently, we provide two techniques: a pre-computation approach
and an incremental approach. In the pre-computation approach, we develop an
algorithm to efficiently evaluate PMNN queries based on the pre-computed PVD
for the entire data set. In the incremental approach, we propose an incremental
probabilistic safe region based technique that does not require to pre-compute
the whole PVD to answer the PMNN query. In this incremental approach, we
exploit the knowledge for a known region to compute the lower bound of the
probability of an object being the nearest neighbor. Experimental results show
that our approaches significantly outperform a sampling based approach by
orders of magnitude in terms of I/O, query processing time, and communication
overheads."
"Web search logs contain extremely sensitive data, as evidenced by the recent
AOL incident. However, storing and analyzing search logs can be very useful for
many purposes (i.e. investigating human behavior). Thus, an important research
question is how to privately sanitize search logs. Several search log
anonymization techniques have been proposed with concrete privacy models.
However, in all of these solutions, the output utility of the techniques is
only evaluated rather than being maximized in any fashion. Indeed, for
effective search log anonymization, it is desirable to derive the optimal
(maximum utility) output while meeting the privacy standard. In this paper, we
propose utility-maximizing sanitization based on the rigorous privacy standard
of differential privacy, in the context of search logs. Specifically, we
utilize optimization models to maximize the output utility of the sanitization
for different applications, while ensuring that the production process
satisfies differential privacy. An added benefit is that our novel
randomization strategy ensures that the schema of the output is identical to
that of the input. A comprehensive evaluation on real search logs validates the
approach and demonstrates its robustness and scalability."
"Often corporations need tools to improve their decision making in a
competitive market. In general, these tools are based on data warehouse
platforms to mange and analyze large amounts of data. However, several of these
corporations do not have enough resources to buy such platforms because of the
high cost. This work is dedicated to a feasibility study of a low cost platform
to data warehouse. We consider as a low cost platform the use of open source
software like the PostgreSQL database system and the GNU/Linux operational
system. We verify the feasibility of this platform by executing two benchmarks
that simulate a data warehouse workload. The workload reproduces a multi-user
environment with the execution of complex queries, which executes:
aggregations, nested sub queries, multi joins, in-line views and more.
Considering the results we were able to highlight some problems on the
PostgreSQL database system, and discuss improvements in the context of data
warehouse."
"The integration of Geographic Information Systems (GIS) and On-Line
Analytical Processing (OLAP), denoted SOLAP, is aimed at exploring and
analyzing spatial data. In real-world SOLAP applications, spatial and
non-spatial data are subject to changes. In this paper we present a temporal
query language for SOLAP, called TPiet-QL, supporting so-called discrete
changes (for example, in land use or cadastral applications there are
situations where parcels are merged or split). TPiet-QL allows expressing
integrated GIS-OLAP queries in an scenario where spatial objects change across
time."
"Mapping complex metadata structures is crucial in a number of domains such as
data integration, ontology alignment or model management. To speed up that
process automatic matching systems were developed to compute mapping
suggestions that can be corrected by a user. However, constructing and tuning
match strategies still requires a high manual effort by matching experts as
well as correct mappings to evaluate generated mappings. We therefore propose a
self-configuring schema matching system that is able to automatically adapt to
the given mapping problem at hand. Our approach is based on analyzing the input
schemas as well as intermediate matching results. A variety of matching rules
use the analysis results to automatically construct and adapt an underlying
matching process for a given match task. We comprehensively evaluate our
approach on different mapping problems from the schema, ontology and model
management domains. The evaluation shows that our system is able to robustly
return good quality mappings across different mapping problems and domains."
"In this paper, we present the guidelines for an XML-based approach for the
sociological study of Web data such as the analysis of mailing lists or
databases available online. The use of an XML warehouse is a flexible solution
for storing and processing this kind of data. We propose an implemented
solution and show possible applications with our case study of profiles of
experts involved in W3C standard-setting activity. We illustrate the
sociological use of semi-structured databases by presenting our XML Schema for
mailing-list warehousing. An XML Schema allows many adjunctions or crossings of
data sources, without modifying existing data sets, while allowing possible
structural evolution. We also show that the existence of hidden data implies
increased complexity for traditional SQL users. XML content warehousing allows
altogether exhaustive warehousing and recursive queries through contents, with
far less dependence on the initial storage. We finally present the possibility
of exporting the data stored in the warehouse to commonly-used advanced
software devoted to sociological analysis."
"There are many algorithms developed for improvement the time of mining
frequent itemsets (FI) or frequent closed itemsets (FCI). However, the
algorithms which deal with the time of generating association rules were not
put in deep research. In reality, in case of a database containing many FI/FCI
(from ten thousands up to millions), the time of generating association rules
is much larger than that of mining FI/FCI. Therefore, this paper presents an
application of frequent closed itemsets lattice (FCIL) for mining minimal
non-redundant association rules (MNAR) to reduce a lot of time for generating
rules. Firstly, we use CHARM-L for building FCIL. After that, based on FCIL, an
algorithm for fast generating MNAR will be proposed. Experimental results show
that the proposed algorithm is much faster than frequent itemsets lattice-based
algorithm in the mining time."
"Within the research area of deductive databases three different database
tasks have been deeply investigated: query evaluation, update propagation and
view updating. Over the last thirty years various inference mechanisms have
been proposed for realizing these main functionalities of a rule-based system.
However, these inference mechanisms have been rarely used in commercial DB
systems until now. One important reason for this is the lack of a uniform
approach well-suited for implementation in an SQL-based system. In this paper,
we present such a uniform approach in form of a new version of the soft
consequence operator. Additionally, we present improved transformation-based
approaches to query optimization and update propagation and view updating which
are all using this operator as underlying evaluation mechanism."
"The growing volume of data usually creates an interesting challenge for the
need of data analysis tools that discover regularities in these data. Data
mining has emerged as disciplines that contribute tools for data analysis,
discovery of hidden knowledge, and autonomous decision making in many
application domains. The purpose of this study is to compare the performance of
two data mining techniques viz., factor analysis and multiple linear regression
for different sample sizes on three unique sets of data. The performance of the
two data mining techniques is compared on following parameters like mean square
error (MSE), R-square, R-Square adjusted, condition number, root mean square
error(RMSE), number of variables included in the prediction model, modified
coefficient of efficiency, F-value, and test of normality. These parameters
have been computed using various data mining tools like SPSS, XLstat, Stata,
and MS-Excel. It is seen that for all the given dataset, factor analysis
outperform multiple linear regression. But the absolute value of prediction
accuracy varied between the three datasets indicating that the data
distribution and data characteristics play a major role in choosing the correct
prediction technique."
"This document is part of original research work by the authors in a bid to
explore new fields for applying Data Mining Techniques. The sample data is part
of a large data set from University of Maryland (UMD) and outlines how more
meaningful patterns can be discovered by preprocessing the data in the form of
OLAP cubes."
"Query execution over the Web of Linked Data has attracted much attention
recently. A particularly interesting approach is link traversal based query
execution which proposes to integrate the traversal of data links into the
construction of query results. Hence -in contrast to traditional query
execution paradigms- this approach does not assume a fixed set of relevant data
sources beforehand; instead, it discovers data on the fly and, thus, enables
applications to tap the full potential of the Web.
  While several authors study possibilities to implement the idea of link
traversal based query execution and to optimize query execution in this
context, no work exists that discusses the theoretical foundations of the
approach in general. Our paper fills this gap.
  We introduce a well-defined semantics for queries that may be executed using
the link traversal based approach. Based on this semantics we formally analyze
properties of such queries. In particular, we study the computability of
queries as well as the implications of querying a potentially infinite Web of
Linked Data. Our results show that query computation in general is not
guaranteed to terminate and that for any given query it is undecidable whether
the execution terminates. Furthermore, we define an abstract execution model
that captures the integration of link traversal into the query execution
process. Based on this model we prove the soundness and completeness of link
traversal based query execution and analyze an existing implementation
approach.."
"The paper ""Stack-based Algorithms for Pattern Matching on DAGs"" generalizes
the classical holistic twig join algorithms and proposes PathStackD, TwigStackD
and DagStackD to respectively evaluate path, twig and DAG pattern queries on
directed acyclic graphs. In this paper, we investigate the major results of
that paper, pointing out several discrepancies and proposing solutions to
resolving them. We show that the original algorithms do not find particular
types of query solutions that are common in practice. We also analyze the
effect of an underlying assumption on the correctness of the algorithms and
discuss the pre-filtering process that the original work proposes to prune
redundant nodes. Our experimental study on both real and synthetic data
substantiates our conclusions."
"Aside from crawling, indexing, and querying RDF data centrally, Linked Data
principles allow for processing SPARQL queries on-the-fly by dereferencing
URIs. Proposed link-traversal query approaches for Linked Data have the
benefits of up-to-date results and decentralised (i.e., client-side) execution,
but operate on incomplete knowledge available in dereferenced documents, thus
affecting recall. In this paper, we investigate how implicit knowledge -
specifically that found through owl:sameAs and RDFS reasoning - can improve the
recall in this setting. We start with an empirical analysis of a large crawl
featuring 4 m Linked Data sources and 1.1 g quadruples: we (1) measure expected
recall by only considering dereferenceable information, (2) measure the
improvement in recall given by considering rdfs:seeAlso links as previous
proposals did. We further propose and measure the impact of additionally
considering (3) owl:sameAs links, and (4) applying lightweight RDFS reasoning
(specifically {\rho}DF) for finding more results, relying on static schema
information. We evaluate our methods for live queries over our crawl."
"Wireless sensor networks become integral part of our life. These networks can
be used for monitoring the data in various domain due to their flexibility and
functionality. Query processing and optimization in the WSN is a very
challenging task because of their energy and memory constraint. In this paper,
first our focus is to review the different approaches that have significant
impacts on the development of query processing techniques for WSN. Finally, we
aim to illustrate the existing approach in popular query processing engines
with future research challenges in query optimization."
"Modern RDBMSs support the ability to compress data using methods such as null
suppression and dictionary encoding. Data compression offers the promise of
significantly reducing storage requirements and improving I/O performance for
decision support queries. However, compression can also slow down update and
query performance due to the CPU costs of compression and decompression. In
this paper, we study how data compression affects choice of appropriate
physical database design, such as indexes, for a given workload. We observe
that approaches that decouple the decision of whether or not to choose an index
from whether or not to compress the index can result in poor solutions. Thus,
we focus on the novel problem of integrating compression into physical database
design in a scalable manner. We have implemented our techniques by modifying
Microsoft SQL Server and the Database Engine Tuning Advisor (DTA) physical
design tool. Our techniques are general and are potentially applicable to DBMSs
that support other compression methods. Our experimental results on real world
as well as TPC-H benchmark workloads demonstrate the effectiveness of our
techniques."
"Today in every organization financial analysis provides the basis for
understanding and evaluating the results of business operations and delivering
how well a business is doing. This means that the organizations can control the
operational activities primarily related to corporate finance. One way that
doing this is by analysis of bankruptcy prediction. This paper develops an
ontological model from financial information of an organization by analyzing
the Semantics of the financial statement of a business. One of the best
bankruptcy prediction models is Altman Z-score model. Altman Z-score method
uses financial rations to predict bankruptcy. From the financial ontological
model the relation between financial data is discovered by using data mining
algorithm. By combining financial domain ontological model with association
rule mining algorithm and Zscore model a new business intelligence model is
developed to predict the bankruptcy."
"Spatial co-location patterns are the subsets of Boolean spatial features
whose instances are often located in close geographic proximity. Co-location
rules can be identified by spatial statistics or data mining approaches. In
data mining method, Association rule-based approaches can be used which are
further divided into transaction-based approaches and distance-based
approaches. Transaction-based approaches focus on defining transactions over
space so that an Apriori algorithm can be used. The natural notion of
transactions is absent in spatial data sets which are embedded in continuous
geographic space. A new distance -based approach is developed to mine
co-location patterns from spatial data by using the concept of proximity
neighborhood. A new interest measure, a participation index, is used for
spatial co-location patterns as it possesses an anti-monotone property. An
algorithm to discover co-location patterns are designed which generates
candidate locations and their table instances. Finally the co-location rules
are generated to identify the patterns."
"Various studies on consumer purchasing behaviors have been presented and used
in real problems. Data mining techniques are expected to be a more effective
tool for analyzing consumer behaviors. However, the data mining method has
disadvantages as well as advantages. Therefore, it is important to select
appropriate techniques to mine databases. The objective of this paper is to
know consumer behavior, his psychological condition at the time of purchase and
how suitable data mining method apply to improve conventional method. Moreover,
in an experiment, association rule is employed to mine rules for trusted
customers using sales data in a super market industry"
"This paper presents a new solution, which adds a new database server on an
existing Oracle Multimaster Data replication system with Online Instantiation
method. During this time the system is down, because we cannot execute DML
statements on replication objects but we can only make queries. The time for
adding the new database server depends on the number of objects, on the
replication group and on the network conditions. We propose to add a new layer
between replication objects and the database sessions, which contain DML
statements. The layer eliminates the system down time exploiting our developed
packages. The packages will be active only during the addition of a new site
process and will modify all DML statements and queries based on replication
objects."
"ID is derived from the word identity, derived from the first two characters
in the word. ID is used to distinguish between an entity to another entity.
Student ID (SID) is the key differentiator between a student with other
students. On the concept of database, the differentiator is unique. SID can be
numbers, letters, or a combination of both (alphanumeric). Viewed from the
daily context, it is not important to determine which a SID belongs to the type
of data. However, when reviewed on database design, determining the type of
data, including SID in this case, is important. Problems arise because there is
a contradiction between the data type viewed from the data characteristic and
practical needs. Type of data for SID is a string, if it is evaluated from the
basic concepts and its characteristic. It is acceptable because SID consists of
a set of numbers which will not be meaningful if applied arithmetic operations
like addition, subtraction, multiplication and division. But in terms of
computer organization, data representation type will determine how much data
space requirements, speed of access, and speed of operation. By considering the
constraints of space and speed on the experiments conducted, SID is better
expressed as an integer rather than a set of characters.
  KEYWORDS aphanumeric,representation, string, integer, space, speed"
"Finding frequent itemsets in a data source is a fundamental operation behind
Association Rule Mining. Generally, many algorithms use either the bottom-up or
top-down approaches for finding these frequent itemsets. When the length of
frequent itemsets to be found is large, the traditional algorithms find all the
frequent itemsets from 1-length to n-length, which is a difficult process. This
problem can be solved by mining only the Maximal Frequent Itemsets (MFS).
Maximal Frequent Itemsets are frequent itemsets which have no proper frequent
superset. Thus, the generation of only maximal frequent itemsets reduces the
number of itemsets and also time needed for the generation of all frequent
itemsets as each maximal itemset of length m implies the presence of 2m-2
frequent itemsets. Furthermore, mining only maximal frequent itemset is
sufficient in many data mining applications like minimal key discovery and
theory extraction. In this paper, we suggest a novel method for finding the
maximal frequent itemset from huge data sources using the concept of
segmentation of data source and prioritization of segments. Empirical
evaluation shows that this method outperforms various other known methods."
"As data are increasingly modeled as graphs for expressing complex
relationships, the tree pattern query on graph-structured data becomes an
important type of queries in real-world applications. Most practical query
languages, such as XQuery and SPARQL, support logical expressions using
logical-AND/OR/NOT operators to define structural constraints of tree patterns.
In this paper, (1) we propose generalized tree pattern queries (GTPQs) over
graph-structured data, which fully support propositional logic of structural
constraints. (2) We make a thorough study of fundamental problems including
satisfiability, containment and minimization, and analyze the computational
complexity and the decision procedures of these problems. (3) We propose a
compact graph representation of intermediate results and a pruning approach to
reduce the size of intermediate results and the number of join operations --
two factors that often impair the efficiency of traditional algorithms for
evaluating tree pattern queries. (4) We present an efficient algorithm for
evaluating GTPQs using 3-hop as the underlying reachability index. (5)
Experiments on both real-life and synthetic data sets demonstrate the
effectiveness and efficiency of our algorithm, from several times to orders of
magnitude faster than state-of-the-art algorithms in terms of evaluation time,
even for traditional tree pattern queries with only conjunctive operations."
"We present sensitivity analysis for results of query executions in a
relational model of data extended by ordinal ranks. The underlying model of
data results from the ordinary Codd's model of data in which we consider
ordinal ranks of tuples in data tables expressing degrees to which tuples match
queries. In this setting, we show that ranks assigned to tuples are insensitive
to small changes, i.e., small changes in the input data do not yield large
changes in the results of queries."
"To comply with emerging privacy laws and regulations, it has become common
for applications like electronic health records systems (EHRs) to collect
access logs, which record each time a user (e.g., a hospital employee) accesses
a piece of sensitive data (e.g., a patient record). Using the access log, it is
easy to answer simple queries (e.g., Who accessed Alice's medical record?), but
this often does not provide enough information. In addition to learning who
accessed their medical records, patients will likely want to understand why
each access occurred. In this paper, we introduce the problem of generating
explanations for individual records in an access log. The problem is motivated
by user-centric auditing applications, and it also provides a novel approach to
misuse detection. We develop a framework for modeling explanations which is
based on a fundamental observation: For certain classes of databases, including
EHRs, the reason for most data accesses can be inferred from data stored
elsewhere in the database. For example, if Alice has an appointment with Dr.
Dave, this information is stored in the database, and it explains why Dr. Dave
looked at Alice's record. Large numbers of data accesses can be explained using
general forms called explanation templates. Rather than requiring an
administrator to manually specify explanation templates, we propose a set of
algorithms for automatically discovering frequent templates from the database
(i.e., those that explain a large number of accesses). We also propose
techniques for inferring collaborative user groups, which can be used to
enhance the quality of the discovered explanations. Finally, we have evaluated
our proposed techniques using an access log and data from the University of
Michigan Health System. Our results demonstrate that in practice we can provide
explanations for over 94% of data accesses in the log."
"Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible
to task people with small jobs, such as labeling images or looking up phone
numbers, via a programmatic interface. MTurk tasks for processing datasets with
humans are currently designed with significant reimplementation of common
workflows and ad-hoc selection of parameters such as price to pay per task. We
describe how we have integrated crowds into a declarative workflow engine
called Qurk to reduce the burden on workflow designers. In this paper, we focus
on how to use humans to compare items for sorting and joining data, two of the
most common operations in DBMSs. We describe our basic query interface and the
user interface of the tasks we post to MTurk. We also propose a number of
optimizations, including task batching, replacing pairwise comparisons with
numerical ratings, and pre-filtering tables before joining them, which
dramatically reduce the overall cost of running sorts and joins on the crowd.
In an experiment joining two sets of images, we reduce the overall cost from
$67 in a naive implementation to about $3, without substantially affecting
accuracy or latency. In an end-to-end experiment, we reduced cost by a factor
of 14.5."
"When computation is outsourced, the data owner would like to be assured that
the desired computation has been performed correctly by the service provider.
In theory, proof systems can give the necessary assurance, but prior work is
not sufficiently scalable or practical. In this paper, we develop new proof
protocols for verifying computations which are streaming in nature: the
verifier (data owner) needs only logarithmic space and a single pass over the
input, and after observing the input follows a simple protocol with a prover
(service provider) that takes logarithmic communication spread over a
logarithmic number of rounds. These ensure that the computation is performed
correctly: that the service provider has not made any errors or missed out some
data. The guarantee is very strong: even if the service provider deliberately
tries to cheat, there is only vanishingly small probability of doing so
undetected, while a correct computation is always accepted. We first observe
that some theoretical results can be modified to work with streaming verifiers,
showing that there are efficient protocols for problems in the complexity
classes NP and NC. Our main results then seek to bridge the gap between theory
and practice by developing usable protocols for a variety of problems of
central importance in streaming and database processing. All these problems
require linear space in the traditional streaming model, and therefore our
protocols demonstrate that adding a prover can exponentially reduce the effort
needed by the verifier. Our experimental results show that our protocols are
practical and scalable."
"With the growing use of location-based services, location privacy attracts
increasing attention from users, industry, and the research community. While
considerable effort has been devoted to inventing techniques that prevent
service providers from knowing a user's exact location, relatively little
attention has been paid to enabling so-called peer-wise privacy--the protection
of a user's location from unauthorized peer users. This paper identifies an
important efficiency problem in existing peer-privacy approaches that simply
apply a filtering step to identify users that are located in a query range, but
that do not want to disclose their location to the querying peer. To solve this
problem, we propose a novel, privacy-policy enabled index called the PEB-tree
that seamlessly integrates location proximity and policy compatibility. We
propose efficient algorithms that use the PEB-tree for processing privacy-aware
range and kNN queries. Extensive experiments suggest that the PEB-tree enables
efficient query processing."
"The suffix tree is a data structure for indexing strings. It is used in a
variety of applications such as bioinformatics, time series analysis,
clustering, text editing and data compression. However, when the string and the
resulting suffix tree are too large to fit into the main memory, most existing
construction algorithms become very inefficient. This paper presents a
disk-based suffix tree construction method, called Elastic Range (ERa), which
works efficiently with very long strings that are much larger than the
available memory. ERa partitions the tree construction process horizontally and
vertically and minimizes I/Os by dynamically adjusting the horizontal
partitions independently for each vertical partition, based on the evolving
shape of the tree and the available memory. Where appropriate, ERa also groups
vertical partitions together to amortize the I/O cost. We developed a serial
version; a parallel version for shared-memory and shared-disk multi-core
systems; and a parallel version for shared-nothing architectures. ERa indexes
the entire human genome in 19 minutes on an ordinary desktop computer. For
comparison, the fastest existing method needs 15 minutes using 1024 CPUs on an
IBM BlueGene supercomputer."
"Read-optimized columnar databases use differential updates to handle writes
by maintaining a separate write-optimized delta partition which is periodically
merged with the read-optimized and compressed main partition. This merge
process introduces significant overheads and unacceptable downtimes in update
intensive systems, aspiring to combine transactional and analytical workloads
into one system. In the first part of the paper, we report data analyses of 12
SAP Business Suite customer systems. In the second half, we present an
optimized merge process reducing the merge overhead of current systems by a
factor of 30. Our linear-time merge algorithm exploits the underlying high
compute and bandwidth resources of modern multi-core CPUs with
architecture-aware optimizations and efficient parallelization. This enables
compressed in-memory column stores to handle the transactional update rate
required by enterprise applications, while keeping properties of read-optimized
databases for analytic-style queries."
"Influence maximization is the problem of finding a set of users in a social
network, such that by targeting this set, one maximizes the expected spread of
influence in the network. Most of the literature on this topic has focused
exclusively on the social graph, overlooking historical data, i.e., traces of
past action propagations. In this paper, we study influence maximization from a
novel data-based perspective. In particular, we introduce a new model, which we
call credit distribution, that directly leverages available propagation traces
to learn how influence flows in the network and uses this to estimate expected
influence spread. Our approach also learns the different levels of
influenceability of users, and it is time-aware in the sense that it takes the
temporal nature of influence into account. We show that influence maximization
under the credit distribution model is NP-hard and that the function that
defines expected spread under our model is submodular. Based on these, we
develop an approximation algorithm for solving the influence maximization
problem that at once enjoys high accuracy compared to the standard approach,
while being several orders of magnitude faster and more scalable."
"XML Schema is the language used to define the structure of messages exchanged
between OGC-based web service clients and providers. The size of these schemas
has been growing with time, reaching a state that makes its understanding and
effective application a hard task. A first step to cope with this situation is
to provide different ways to measure the complexity of the schemas. In this
regard, we present in this paper an analysis of the complexity of XML schemas
in OGC web services. We use a group of metrics found in the literature and
introduce new metrics to measure size and/or complexity of these schemas. The
use of adequate metrics allows us to quantify the complexity, quality and other
properties of the schemas, which can be very useful in different scenarios."
"Although the adoption of OGC Web Services for server, desktop and web
applications has been successful, its penetration in mobile devices has been
slow. One of the main reasons is the performance problems associated with XML
processing as it consumes a lot of memory and processing time, which are scarce
resources in a mobile device. In this paper we propose an algorithm to generate
efficient code for XML data binding for mobile SOS-based applications. The
algorithm take advantage of the fact that individual implementations use only
some portions of the standards' schemas, which allows the simplification of
large XML schema sets in an application-specific manner by using a subset of
XML instance files conforming to these schemas."
"In many modern applications, data are received as infinite, rapid,
unpredictable and time- variant data elements that are known as data streams.
Systems which are able to process data streams with such properties are called
Data Stream Management Systems (DSMS). Due to the unpredictable and time-
variant properties of data streams as well as system, adaptivity of the DSMS is
a major requirement for each DSMS. Accordingly, determining parameters which
are effective on the most important performance metric of a DSMS (i.e.,
response time) and analysing them will affect on designing an adaptive DSMS. In
this paper, effective parameters on response time of DSMS are studied and
analysed and a solution is proposed for DSMSs' adaptivity. The proposed
adaptive DSMS architecture includes a learning unit that frequently evaluates
system to adjust the optimal value for each of tuneable effective. Learning
Automata is used as the learning mechanism of the learning unit to adjust the
value of tuneable effective parameters. So, when system faces some changes, the
learning unit increases performance by tuning each of tuneable effective
parameters to its optimum value. Evaluation results illustrate that after a
while, parameters reach their optimum value and then DSMS's adaptivity will be
improved considerably."
"This paper outlines certain scenarios from the fields of astrophysics and
fluid dynamics simulations which require high performance data warehouses that
support array data type. A common feature of all these use cases is that
subsetting and preprocessing the data on the server side (as far as possible
inside the database server process) is necessary to avoid the client-server
overhead and to minimize IO utilization. Analyzing and summarizing the
requirements of the various fields help software engineers to come up with a
comprehensive design of an array extension to relational database systems that
covers a wide range of scientific applications. We also present a working
implementation of an array data type for Microsoft SQL Server 2008 to support
large-scale scientific applications. We introduce the design of the array type,
results from a performance evaluation, and discuss the lessons learned from
this implementation. The library can be downloaded from our website at
http://voservices.net/sqlarray/"
"The need to increase accuracy in detecting sophisticated cyber attacks poses
a great challenge not only to the research community but also to corporations.
So far, many approaches have been proposed to cope with this threat. Among
them, data mining has brought on remarkable contributions to the intrusion
detection problem. However, the generalization ability of data mining-based
methods remains limited, and hence detecting sophisticated attacks remains a
tough task. In this thread, we present a novel method based on both clustering
and classification for developing an efficient intrusion detection system
(IDS). The key idea is to take useful information exploited from fuzzy
clustering into account for the process of building an IDS. To this aim, we
first present cornerstones to construct additional cluster features for a
training set. Then, we come up with an algorithm to generate an IDS based on
such cluster features and the original input features. Finally, we
experimentally prove that our method outperforms several well-known methods."
"As probabilistic data management is becoming one of the main research focuses
and keyword search is turning into a more popular query means, it is natural to
think how to support keyword queries on probabilistic XML data. With regards to
keyword query on deterministic XML documents, ELCA (Exclusive Lowest Common
Ancestor) semantics allows more relevant fragments rooted at the ELCAs to
appear as results and is more popular compared with other keyword query result
semantics (such as SLCAs).
  In this paper, we investigate how to evaluate ELCA results for keyword
queries on probabilistic XML documents. After defining probabilistic ELCA
semantics in terms of possible world semantics, we propose an approach to
compute ELCA probabilities without generating possible worlds. Then we develop
an efficient stack-based algorithm that can find all probabilistic ELCA results
and their ELCA probabilities for a given keyword query on a probabilistic XML
document. Finally, we experimentally evaluate the proposed ELCA algorithm and
compare it with its SLCA counterpart in aspects of result effectiveness, time
and space efficiency, and scalability."
"Many applications log a large amount of events continuously. Extracting
interesting knowledge from logged events is an emerging active research area in
data mining. In this context, we propose an approach for mining frequent events
and association rules from logged events in XML format. This approach is
composed of two-main phases: I) constructing a novel tree structure called
Frequency XML-based Tree (FXT), which contains the frequency of events to be
mined; II) querying the constructed FXT using XQuery to discover frequent
itemsets and association rules. The FXT is constructed with a single-pass over
logged data. We implement the proposed algorithm and study various performance
issues. The performance study shows that the algorithm is efficient, for both
constructing the FXT and discovering association rules."
"The current data tends to be more complex than conventional data and need
dimension reduction. Dimension reduction is important in cluster analysis and
creates a smaller data in volume and has the same analytical results as the
original representation. A clustering process needs data reduction to obtain an
efficient processing time while clustering and mitigate curse of
dimensionality. This paper proposes a model for extracting multidimensional
data clustering of health database. We implemented four dimension reduction
techniques such as Singular Value Decomposition (SVD), Principal Component
Analysis (PCA), Self Organizing Map (SOM) and FastICA. The results show that
dimension reductions significantly reduce dimension and shorten processing time
and also increased performance of cluster in several health datasets."
"The mining of frequent subgraphs from labeled graph data has been studied
extensively. Furthermore, much attention has recently been paid to frequent
pattern mining from graph sequences. A method, called GTRACE, has been proposed
to mine frequent patterns from graph sequences under the assumption that
changes in graphs are gradual. Although GTRACE mines the frequent patterns
efficiently, it still needs substantial computation time to mine the patterns
from graph sequences containing large graphs and long sequences. In this paper,
we propose a new version of GTRACE that enables efficient mining of frequent
patterns based on the principle of a reverse search. The underlying concept of
the reverse search is a general scheme for designing efficient algorithms for
hard enumeration problems. Our performance study shows that the proposed method
is efficient and scalable for mining both long and large graph sequence
patterns and is several orders of magnitude faster than the original GTRACE."
"A new emerging class of parallel database management systems (DBMS) is
designed to take advantage of the partitionable workloads of on-line
transaction processing (OLTP) applications. Transactions in these systems are
optimized to execute to completion on a single node in a shared-nothing cluster
without needing to coordinate with other nodes or use expensive concurrency
control measures. But some OLTP applications cannot be partitioned such that
all of their transactions execute within a single-partition in this manner.
These distributed transactions access data not stored within their local
partitions and subsequently require more heavy-weight concurrency control
protocols. Further difficulties arise when the transaction's execution
properties, such as the number of partitions it may need to access or whether
it will abort, are not known beforehand. The DBMS could mitigate these
performance issues if it is provided with additional information about
transactions. Thus, in this paper we present a Markov model-based approach for
automatically selecting which optimizations a DBMS could use, namely (1) more
efficient concurrency control schemes, (2) intelligent scheduling, (3) reduced
undo logging, and (4) speculative execution. To evaluate our techniques, we
implemented our models and integrated them into a parallel, main-memory OLTP
DBMS to show that we can improve the performance of applications with diverse
workloads."
"We consider the setting of a Semantic Web database, containing both explicit
data encoded in RDF triples, and implicit data, implied by the RDF semantics.
Based on a query workload, we address the problem of selecting a set of views
to be materialized in the database, minimizing a combination of query
processing, view storage, and view maintenance costs. Starting from an existing
relational view selection method, we devise new algorithms for recommending
view sets, and show that they scale significantly beyond the existing
relational ones when adapted to the RDF context. To account for implicit
triples in query answers, we propose a novel RDF query reformulation algorithm
and an innovative way of incorporating it into view selection in order to avoid
a combinatorial explosion in the complexity of the selection process. The
interest of our techniques is demonstrated through a set of experiments."
"MapReduce is becoming the de facto framework for storing and processing
massive data, due to its excellent scalability, reliability, and elasticity. In
many MapReduce applications, obtaining a compact accurate summary of data is
essential. Among various data summarization tools, histograms have proven to be
particularly important and useful for summarizing data, and the wavelet
histogram is one of the most widely used histograms. In this paper, we
investigate the problem of building wavelet histograms efficiently on large
datasets in MapReduce. We measure the efficiency of the algorithms by both
end-to-end running time and communication cost. We demonstrate straightforward
adaptations of existing exact and approximate methods for building wavelet
histograms to MapReduce clusters are highly inefficient. To that end, we design
new algorithms for computing exact and approximate wavelet histograms and
discuss their implementation in MapReduce. We illustrate our techniques in
Hadoop, and compare to baseline solutions with extensive experiments performed
in a heterogeneous Hadoop cluster of 16 nodes, using large real and synthetic
datasets, up to hundreds of gigabytes. The results suggest significant (often
orders of magnitude) performance improvement achieved by our new algorithms."
"Density-based cluster mining is known to serve a broad range of applications
ranging from stock trade analysis to moving object monitoring. Although methods
for efficient extraction of density-based clusters have been studied in the
literature, the problem of summarizing and matching of such clusters with
arbitrary shapes and complex cluster structures remains unsolved. Therefore,
the goal of our work is to extend the state-of-art of density-based cluster
mining in streams from cluster extraction only to now also support analysis and
management of the extracted clusters. Our work solves three major technical
challenges. First, we propose a novel multi-resolution cluster summarization
method, called Skeletal Grid Summarization (SGS), which captures the key
features of density-based clusters, covering both their external shape and
internal cluster structures. Second, in order to summarize the extracted
clusters in real-time, we present an integrated computation strategy C-SGS,
which piggybacks the generation of cluster summarizations within the online
clustering process. Lastly, we design a mechanism to efficiently execute
cluster matching queries, which identify similar clusters for given cluster of
analyst's interest from clusters extracted earlier in the stream history. Our
experimental study using real streaming data shows the clear superiority of our
proposed methods in both efficiency and effectiveness for cluster summarization
and cluster matching queries to other potential alternatives."
"Recent research has taken advantage of Wikipedia's multilingualism as a
resource for cross-language information retrieval and machine translation, as
well as proposed techniques for enriching its cross-language structure. The
availability of documents in multiple languages also opens up new opportunities
for querying structured Wikipedia content, and in particular, to enable answers
that straddle different languages. As a step towards supporting such queries,
in this paper, we propose a method for identifying mappings between attributes
from infoboxes that come from pages in different languages. Our approach finds
mappings in a completely automated fashion. Because it does not require
training data, it is scalable: not only can it be used to find mappings between
many language pairs, but it is also effective for languages that are
under-represented and lack sufficient training samples. Another important
benefit of our approach is that it does not depend on syntactic similarity
between attribute names, and thus, it can be applied to language pairs that
have distinct morphologies. We have performed an extensive experimental
evaluation using a corpus consisting of pages in Portuguese, Vietnamese, and
English. The results show that not only does our approach obtain high precision
and recall, but it also outperforms state-of-the-art techniques. We also
present a case study which demonstrates that the multilingual mappings we
derive lead to substantial improvements in answer quality and coverage for
structured queries over Wikipedia content."
"Association rule mining is an important problem in the data mining area. It
enumerates and tests a large number of rules on a dataset and outputs rules
that satisfy user-specified constraints. Due to the large number of rules being
tested, rules that do not represent real systematic effect in the data can
satisfy the given constraints purely by random chance. Hence association rule
mining often suffers from a high risk of false positive errors. There is a lack
of comprehensive study on controlling false positives in association rule
mining. In this paper, we adopt three multiple testing correction
approaches---the direct adjustment approach, the permutation-based approach and
the holdout approach---to control false positives in association rule mining,
and conduct extensive experiments to study their performance. Our results show
that (1) Numerous spurious rules are generated if no correction is made. (2)
The three approaches can control false positives effectively. Among the three
approaches, the permutation-based approach has the highest power of detecting
real association rules, but it is very computationally expensive. We employ
several techniques to reduce its cost effectively."
"Graphs have been commonly used to model many applications. A natural problem
which abstracts applications such as itinerary planning, playlist
recommendation, and flow analysis in information networks is that of finding
the heaviest path(s) in a graph. More precisely, we can model these
applications as a graph with non-negative edge weights, along with a monotone
function such as sum, which aggregates edge weights into a path weight,
capturing some notion of quality. We are then interested in finding the top-k
heaviest simple paths, i.e., the $k$ simple (cycle-free) paths with the
greatest weight, whose length equals a given parameter $\ell$. We call this the
\emph{Heavy Path Problem} (HPP). It is easy to show that the problem is
NP-Hard.
  In this work, we develop a practical approach to solve the Heavy Path problem
by leveraging a strong connection with the well-known Rank Join paradigm. We
first present an algorithm by adapting the Rank Join algorithm. We identify its
limitations and develop a new exact algorithm called HeavyPath and a scalable
heuristic algorithm. We conduct a comprehensive set of experiments on three
real data sets and show that HeavyPath outperforms the baseline algorithms
significantly, with respect to both $\ell$ and $k$. Further, our heuristic
algorithm scales to longer lengths, finding paths that are empirically within
50% of the optimum solution or better under various settings, and takes only a
fraction of the running time compared to the exact algorithm."
"Electronic health records (EHR's) are only a first step in capturing and
utilizing health-related data - the problem is turning that data into useful
information. Models produced via data mining and predictive analysis profile
inherited risks and environmental/behavioral factors associated with patient
disorders, which can be utilized to generate predictions about treatment
outcomes. This can form the backbone of clinical decision support systems
driven by live data based on the actual population. The advantage of such an
approach based on the actual population is that it is ""adaptive"". Here, we
evaluate the predictive capacity of a clinical EHR of a large mental healthcare
provider (~75,000 distinct clients a year) to provide decision support
information in a real-world clinical setting. Initial research has achieved a
70% success rate in predicting treatment outcomes using these methods."
"With the increasing prevalence of location-aware devices, trajectory data has
been generated and collected in various application domains. Trajectory data
carries rich information that is useful for many data analysis tasks. Yet,
improper publishing and use of trajectory data could jeopardize individual
privacy. However, it has been shown that existing privacy-preserving trajectory
data publishing methods derived from partition-based privacy models, for
example k-anonymity, are unable to provide sufficient privacy protection.
  In this paper, motivated by the data publishing scenario at the Societe de
transport de Montreal (STM), the public transit agency in Montreal area, we
study the problem of publishing trajectory data under the rigorous differential
privacy model. We propose an efficient data-dependent yet differentially
private sanitization algorithm, which is applicable to different types of
trajectory data. The efficiency of our approach comes from adaptively narrowing
down the output domain by building a noisy prefix tree based on the underlying
data. Moreover, as a post-processing step, we make use of the inherent
constraints of a prefix tree to conduct constrained inferences, which lead to
better utility. This is the first paper to introduce a practical solution for
publishing large volume of trajectory data under differential privacy. We
examine the utility of sanitized data in terms of count queries and frequent
sequential pattern mining. Extensive experiments on real-life trajectory data
from the STM demonstrate that our approach maintains high utility and is
scalable to large trajectory datasets."
"Weighted association rule mining reflects semantic significance of item by
considering its weight. Classification constructs the classifier and predicts
the new data instance. This paper proposes compact weighted class association
rule mining method, which applies weighted association rule mining in the
classification and constructs an efficient weighted associative classifier.
This proposed associative classification algorithm chooses one non class
informative attribute from dataset and all the weighted class association rules
are generated based on that attribute. The weight of the item is considered as
one of the parameter in generating the weighted class association rules. This
proposed algorithm calculates the weight using the HITS model. Experimental
results show that the proposed system generates less number of high quality
rules which improves the classification accuracy."
"Disconnection of mobile clients from server, in an unclear time and for an
unknown duration, due to mobility of mobile clients, is the most important
challenges for concurrency control in mobile database with client-server model.
Applying pessimistic common classic methods of concurrency control (like 2pl)
in mobile database leads to long duration blocking and increasing waiting time
of transactions. Because of high rate of aborting transactions, optimistic
methods aren`t appropriate in mobile database. In this article, OPCOT
concurrency control algorithm is introduced based on optimistic concurrency
control method. Reducing communications between mobile client and server,
decreasing blocking rate and deadlock of transactions, and increasing
concurrency degree are the most important motivation of using optimistic method
as the basis method of OPCOT algorithm. To reduce abortion rate of
transactions, in execution time of transactions` operators a timestamp is
assigned to them. In other to checking commitment ordering property of
scheduler, the assigned timestamp is used in server on time of commitment. In
this article, serializability of OPCOT algorithm scheduler has been proved by
using serializability graph. Results of evaluating simulation show that OPCOT
algorithm decreases abortion rate and waiting time of transactions in compare
to 2pl and optimistic algorithms."
"User preference queries are very important in spatial databases. With the
help of these queries, one can found best location among points saved in
database. In many situation users evaluate quality of a location with its
distance from its nearest neighbor among a special set of points. There has
been less attention about evaluating a location with its distance to nearest
neighbors in spatial user preference queries. This problem has application in
many domains such as service recommendation systems and investment planning.
Related works in this field are based on top-k queries. The problem with top-k
queries is that user must set weights for attributes and a function for
aggregating them. This is hard for him in most cases. In this paper a new type
of user preference queries called spatial nearest neighbor skyline queries will
be introduced in which user has some sets of points as query parameters. For
each point in database attributes are its distances to the nearest neighbors
from each set of query points. By separating this query as a subset of dynamic
skyline queries N2S2 algorithm is provided for computing it. This algorithm has
good performance compared with the general branch and bound algorithm for
skyline queries."
"The real-time database service selection depends typically to the system
stability in order to handle the time-constrained transactions within their
deadline. However, applying the real-time database system in the mobile ad hoc
networks requires considering the mobile nodes limited capacities. In this
paper, we propose cross-layer service selection which combines performance
metrics measured in the real-time database system to those used by the routing
protocol in order to make the best selection decision. It ensures both
timeliness and energy efficiency by avoiding low-power and busy service
provider node. A multicast packet is used in order to reduce the transmission
cost and network load when sending the same packet to multiple service
providers. In this paper, we evaluate the performance of our proposed protocol.
Simulation results, using the Network Simulator NS2, improve that the protocol
decreases the deadline miss ratio of packets, increases the service
availability and reduces the service response time."
"The growing volumes of XML data sources on the Web or produced by
enterprises, organizations etc. raise many performance challenges for data
management applications. In this work, we are concerned with the distributed,
peer-to-peer management of large corpora of XML documents, based on distributed
hash table (or DHT, in short) overlay networks. We present ViP2P (standing for
Views in Peer-to-Peer), a distributed platform for sharing XML documents based
on a structured P2P network infrastructure (DHT). At the core of ViP2P stand
distributed materialized XML views, defined by arbitrary XML queries, filled in
with data published anywhere in the network, and exploited to efficiently
answer queries issued by any network peer. ViP2P allows user queries to be
evaluated over XML documents published by peers in two modes. First, a
long-running subscription mode, when a query can be registered in the system
and receive answers incrementally when and if published data matches the query.
Second, queries can also be asked in an ad-hoc, snapshot mode, where results
are required immediately and must be computed based on the results of other
long-running, subscription queries. ViP2P innovates over other similar
DHT-based XML sharing platforms by using a very expressive structured XML query
language. This expressivity leads to a very flexible distribution of XML
content in the ViP2P network, and to efficient snapshot query execution. ViP2P
has been tested in real deployments of hundreds of computers. We present the
platform architecture, its internal algorithms, and demonstrate its efficiency
and scalability through a set of experiments. Our experimental results outgrow
by orders of magnitude similar competitor systems in terms of data volumes,
network size and data dissemination throughput."
"Classification and patterns extraction from customer data is very important
for business support and decision making. Timely identification of newly
emerging trends is very important in business process. Large companies are
having huge volume of data but starving for knowledge. To overcome the
organization current issue, the new breed of technique is required that has
intelligence and capability to solve the knowledge scarcity and the technique
is called Data mining. The objectives of this paper are to identify the
high-profit, high-value and low-risk customers by one of the data mining
technique - customer clustering. In the first phase, cleansing the data and
developed the patterns via demographic clustering algorithm using IBM I-Miner.
In the second phase, profiling the data, develop the clusters and identify the
high-value low-risk customers. This cluster typically represents the 10-20
percent of customers which yields 80% of the revenue."
"Customer Relationship Management (CRM) systems are very common in large
companies. However, CRM systems are not very common in Small and Medium
Enterprises (SMEs). Most SMEs do not implement CRM systems due to several
reasons, such as lack of knowledge about CRM or lack of financial resources to
implement CRM systems. SMEs have to start implementing Information Systems (IS)
technology into their business operations in order to improve business values
and gain more competitive advantage over rivals. CRM system has the potential
to help improve the business value and competitive capabilities of SMEs. Given
the high fixed costs of normal activity of companies, we intend to promote free
and viable solutions for small and medium businesses. In this paper, we explain
the reasons why SMEs do not implement CRM system and the benefits of using open
source CRM system in SMEs. We also describe the functionalities of top open
source CRM systems, examining the applicability of these tools in fitting the
needs of SMEs."
"Different ways of entering data into databases result in duplicate records
that cause increasing of databases' size. This is a fact that we cannot ignore
it easily. There are several methods that are used for this purpose. In this
paper, we have tried to increase the accuracy of operations by using cluster
similarity instead of direct similarity of fields. So that clustering is done
on fields of database and according to accomplished clustering on fields,
similarity degree of records is obtained. In this method by using present
information in database, more logical similarity is obtained for deficient
information that in general, the method of cluster similarity could improve
operations 24% compared with previous methods."
"The online analytical processing (OLAP) does not provide any explanation of
correlations discovered between data. Thus, the coupling of OLAP and data
mining, especially association rules, is considered as an efficient solution to
this problem. In this context, we mainly focus on a particular class of
association rules which is the cyclic association rules. These rules aimed to
discover patterns that display regular variation over user-defined intervals.
Generally,the generated patterns do not take an advantage from the
specificities of the multidimensional context namely, the consideration of the
measures and their aggregations. In this paper, we introduce a novel method for
extracting cyclic association rules from measures, and we redefine the
evaluation metrics of association rules quality inspired of the temporal
summarizability of measures concept through the integration of appropriate
aggregation functions. To prove the usefulness of our approach, we conduct an
empirical study on a real data warehouse."
"Data center operators face a bewildering set of choices when considering how
to provision resources on machines with complex I/O subsystems. Modern I/O
subsystems often have a rich mix of fast, high performing, but expensive SSDs
sitting alongside with cheaper but relatively slower (for random accesses)
traditional hard disk drives. The data center operators need to determine how
to provision the I/O resources for specific workloads so as to abide by
existing Service Level Agreements (SLAs), while minimizing the total operating
cost (TOC) of running the workload, where the TOC includes the amortized
hardware costs and the run time energy costs. The focus of this paper is on
introducing this new problem of TOC-based storage allocation, cast in a
framework that is compatible with traditional DBMS query optimization and query
processing architecture. We also present a heuristic-based solution to this
problem, called DOT. We have implemented DOT in PostgreSQL, and experiments
using TPC-H and TPC-C demonstrate significant TOC reduction by DOT in various
settings."
"Previous research addressed the potential problems of the hard-disk oriented
design of DBMSs of flashSSDs. In this paper, we focus on exploiting potential
benefits of flashSSDs. First, we examine the internal parallelism issues of
flashSSDs by conducting benchmarks to various flashSSDs. Then, we suggest
algorithm-design principles in order to best benefit from the internal
parallelism. We present a new I/O request concept, called psync I/O that can
exploit the internal parallelism of flashSSDs in a single process. Based on
these ideas, we introduce B+-tree optimization methods in order to utilize
internal parallelism. By integrating the results of these methods, we present a
B+-tree variant, PIO B-tree. We confirmed that each optimization method
substantially enhances the index performance. Consequently, PIO B-tree enhanced
B+-tree's insert performance by a factor of up to 16.3, while improving
point-search performance by a factor of 1.2. The range search of PIO B-tree was
up to 5 times faster than that of the B+-tree. Moreover, PIO B-tree
outperformed other flash-aware indexes in various synthetic workloads. We also
confirmed that PIO B-tree outperforms B+-tree in index traces collected inside
the Postgresql DBMS with TPC-C benchmark."
"A database system optimized for in-memory storage can support much higher
transaction rates than current systems. However, standard concurrency control
methods used today do not scale to the high transaction rates achievable by
such systems. In this paper we introduce two efficient concurrency control
methods specifically designed for main-memory databases. Both use
multiversioning to isolate read-only transactions from updates but differ in
how atomicity is ensured: one is optimistic and one is pessimistic. To avoid
expensive context switching, transactions never block during normal processing
but they may have to wait before commit to ensure correct serialization
ordering. We also implemented a main-memory optimized version of single-version
locking. Experimental results show that while single-version locking works well
when transactions are short and contention is low performance degrades under
more demanding conditions. The multiversion schemes have higher overhead but
are much less sensitive to hotspots and the presence of long-running
transactions."
"Graph pattern matching is often defined in terms of subgraph isomorphism, an
NP-complete problem. To lower its complexity, various extensions of graph
simulation have been considered instead. These extensions allow pattern
matching to be conducted in cubic-time. However, they fall short of capturing
the topology of data graphs, i.e., graphs may have a structure drastically
different from pattern graphs they match, and the matches found are often too
large to understand and analyze. To rectify these problems, this paper proposes
a notion of strong simulation, a revision of graph simulation, for graph
pattern matching. (1) We identify a set of criteria for preserving the topology
of graphs matched. We show that strong simulation preserves the topology of
data graphs and finds a bounded number of matches. (2) We show that strong
simulation retains the same complexity as earlier extensions of simulation, by
providing a cubic-time algorithm for computing strong simulation. (3) We
present the locality property of strong simulation, which allows us to
effectively conduct pattern matching on distributed graphs. (4) We
experimentally verify the effectiveness and efficiency of these algorithms,
using real-life data and synthetic data."
"We consider the classical tree edit distance between ordered labeled trees,
which is defined as the minimum-cost sequence of node edit operations that
transform one tree into another. The state-of-the-art solutions for the tree
edit distance are not satisfactory. The main competitors in the field either
have optimal worst-case complexity, but the worst case happens frequently, or
they are very efficient for some tree shapes, but degenerate for others. This
leads to unpredictable and often infeasible runtimes. There is no obvious way
to choose between the algorithms. In this paper we present RTED, a robust tree
edit distance algorithm. The asymptotic complexity of RTED is smaller or equal
to the complexity of the best competitors for any input instance, i.e., RTED is
both efficient and worst-case optimal. We introduce the class of LRH
(Left-Right-Heavy) algorithms, which includes RTED and the fastest tree edit
distance algorithms presented in literature. We prove that RTED outperforms all
previously proposed LRH algorithms in terms of runtime complexity. In our
experiments on synthetic and real world data we empirically evaluate our
solution and compare it to the state-of-the-art."
"Workflow provenance typically assumes that each module is a ""black-box"", so
that each output depends on all inputs (coarse-grained dependencies).
Furthermore, it does not model the internal state of a module, which can change
between repeated executions. In practice, however, an output may depend on only
a small subset of the inputs (fine-grained dependencies) as well as on the
internal state of the module. We present a novel provenance framework that
marries database-style and workflow-style provenance, by using Pig Latin to
expose the functionality of modules, thus capturing internal state and
fine-grained dependencies. A critical ingredient in our solution is the use of
a novel form of provenance graph that models module invocations and yields a
compact representation of fine-grained workflow provenance. It also enables a
number of novel graph transformation operations, allowing to choose the desired
level of granularity in provenance querying (ZoomIn and ZoomOut), and
supporting ""what-if"" workflow analytic queries. We implemented our approach in
the Lipstick system and developed a benchmark in support of a systematic
performance evaluation. Our results demonstrate the feasibility of tracking and
querying fine-grained workflow provenance."
"With the rapid growth of large graphs, we cannot assume that graphs can still
be fully loaded into memory, thus the disk-based graph operation is inevitable.
In this paper, we take the shortest path discovery as an example to investigate
the technique issues when leveraging existing infrastructure of relational
database (RDB) in the graph data management. Based on the observation that a
variety of graph search queries can be implemented by iterative operations
including selecting frontier nodes from visited nodes, making expansion from
the selected frontier nodes, and merging the expanded nodes into the visited
ones, we introduce a relational FEM framework with three corresponding
operators to implement graph search tasks in the RDB context. We show new
features such as window function and merge statement introduced by recent SQL
standards can not only simplify the expression but also improve the performance
of the FEM framework. In addition, we propose two optimization strategies
specific to shortest path discovery inside the FEM framework. First, we take a
bi-directional set Dijkstra's algorithm in the path finding. The bi-directional
strategy can reduce the search space, and set Dijkstra's algorithm finds the
shortest path in a set-at-a-time fashion. Second, we introduce an index named
SegTable to preserve the local shortest segments, and exploit SegTable to
further improve the performance. The final extensive experimental results
illustrate our relational approach with the optimization strategies achieves
high scalability and performance."
"In this paper we introduce a new type of pattern -- a flipping correlation
pattern. The flipping patterns are obtained from contrasting the correlations
between items at different levels of abstraction. They represent surprising
correlations, both positive and negative, which are specific for a given
abstraction level, and which ""flip"" from positive to negative and vice versa
when items are generalized to a higher level of abstraction. We design an
efficient algorithm for finding flipping correlations, the Flipper algorithm,
which outperforms naive pattern mining methods by several orders of magnitude.
We apply Flipper to real-life datasets and show that the discovered patterns
are non-redundant, surprising and actionable. Flipper finds strong contrasting
correlations in itemsets with low-to-medium support, while existing techniques
cannot handle the pattern discovery in this frequency range."
"The need for accurate SQL progress estimation in the context of decision
support administration has led to a number of techniques proposed for this
task. Unfortunately, no single one of these progress estimators behaves
robustly across the variety of SQL queries encountered in practice, meaning
that each technique performs poorly for a significant fraction of queries. This
paper proposes a novel estimator selection framework that uses a statistical
model to characterize the sets of conditions under which certain estimators
outperform others, leading to a significant increase in estimation robustness.
The generality of this framework also enables us to add a number of novel
""special purpose"" estimators which increase accuracy further. Most importantly,
the resulting model generalizes well to queries very different from the ones
used to train it. We validate our findings using a large number of industrial
real-life and benchmark workloads."
"We focus on range query processing on large-scale, typically distributed
infrastructures, such as clouds of thousands of nodes of shared-datacenters, of
p2p distributed overlays, etc. In such distributed environments, efficient
range query processing is the key for managing the distributed data sets per
se, and for monitoring the infrastructure's resources. We wish to develop an
architecture that can support range queries in such large-scale decentralized
environments and can scale in terms of the number of nodes as well as in terms
of the data items stored. Of course, in the last few years there have been a
number of solutions (mostly from researchers in the p2p domain) for designing
such large-scale systems. However, these are inadequate for our purposes, since
at the envisaged scales the classic logarithmic complexity (for point queries)
is still too expensive while for range queries it is even more disappointing.
In this paper we go one step further and achieve a sub-logarithmic complexity.
We contribute the ART, which outperforms the most popular decentralized
structures, including Chord (and some of its successors), BATON (and its
successor) and Skip-Graphs. We contribute theoretical analysis, backed up by
detailed experimental results, showing that the communication cost of query and
update operations is $O(\log_{b}^2 \log N)$ hops, where the base $b$ is a
double-exponentially power of two and $N$ is the total number of nodes.
Moreover, ART is a fully dynamic and fault-tolerant structure, which supports
the join/leave node operations in $O(\log \log N)$ expected w.h.p number of
hops. Our experimental performance studies include a detailed performance
comparison which showcases the improved performance, scalability, and
robustness of ART."
"We study a problem of detecting priming events based on a time series index
and an evolving document stream. We define a priming event as an event which
triggers abnormal movements of the time series index, i.e., the Iraq war with
respect to the president approval index of President Bush. Existing solutions
either focus on organizing coherent keywords from a document stream into events
or identifying correlated movements between keyword frequency trajectories and
the time series index. In this paper, we tackle the problem in two major steps.
(1) We identify the elements that form a priming event. The element identified
is called influential topic which consists of a set of coherent keywords. And
we extract them by looking at the correlation between keyword trajectories and
the interested time series index at a global level. (2) We extract priming
events by detecting and organizing the bursty influential topics at a micro
level. We evaluate our algorithms on a real-world dataset and the result
confirms that our method is able to discover the priming events effectively."
"In the present paper, we propose a Neuroelectromagnetic Ontology Framework
(NOF) for mining Event-related Potentials (ERP) patterns as well as the
process. The aim for this research is to develop an infrastructure for mining,
analysis and sharing the ERP domain ontologies. The outcome of this research is
a Neuroelectromagnetic knowledge-based system. The framework has 5 stages: 1)
Data pre-processing and preparation; 2) Data mining application; 3) Rule
Comparison and Evaluation; 4) Association rules Post-processing 5) Domain
Ontologies. In 5th stage a new set of hidden rules can be discovered base on
comparing association rules by domain ontologies and expert rules."
"With the rapid development of online social media, online shopping sites and
cyber-physical systems, heterogeneous information networks have become
increasingly popular and content-rich over time. In many cases, such networks
contain multiple types of objects and links, as well as different kinds of
attributes. The clustering of these objects can provide useful insights in many
applications. However, the clustering of such networks can be challenging since
(a) the attribute values of objects are often incomplete, which implies that an
object may carry only partial attributes or even no attributes to correctly
label itself; and (b) the links of different types may carry different kinds of
semantic meanings, and it is a difficult task to determine the nature of their
relative importance in helping the clustering for a given purpose. In this
paper, we address these challenges by proposing a model-based clustering
algorithm. We design a probabilistic model which clusters the objects of
different types into a common hidden space, by using a user-specified set of
attributes, as well as the links from different relations. The strengths of
different types of links are automatically learned, and are determined by the
given purpose of clustering. An iterative algorithm is designed for solving the
clustering problem, in which the strengths of different types of links and the
quality of clustering results mutually enhance each other. Our experimental
results on real and synthetic data sets demonstrate the effectiveness and
efficiency of the algorithm."
"Computing the shortest path between two given locations in a road network is
an important problem that finds applications in various map services and
commercial navigation products. The state-of-the-art solutions for the problem
can be divided into two categories: spatial-coherence-based methods and
vertex-importance-based approaches. The two categories of techniques, however,
have not been compared systematically under the same experimental framework, as
they were developed from two independent lines of research that do not refer to
each other. This renders it difficult for a practitioner to decide which
technique should be adopted for a specific application. Furthermore, the
experimental evaluation of the existing techniques, as presented in previous
work, falls short in several aspects. Some methods were tested only on small
road networks with up to one hundred thousand vertices; some approaches were
evaluated using distance queries (instead of shortest path queries), namely,
queries that ask only for the length of the shortest path; a state-of-the-art
technique was examined based on a faulty implementation that led to incorrect
query results. To address the above issues, this paper presents a comprehensive
comparison of the most advanced spatial-coherence-based and
vertex-importance-based approaches. Using a variety of real road networks with
up to twenty million vertices, we evaluated each technique in terms of its
preprocessing time, space consumption, and query efficiency (for both shortest
path and distance queries). Our experimental results reveal the characteristics
of different techniques, based on which we provide guidelines on selecting
appropriate methods for various scenarios."
"In many information networks, data items -- such as updates in social
networks, news flowing through interconnected RSS feeds and blogs, measurements
in sensor networks, route updates in ad-hoc networks -- propagate in an
uncoordinated manner: nodes often relay information they receive to neighbors,
independent of whether or not these neighbors received the same information
from other sources. This uncoordinated data dissemination may result in
significant, yet unnecessary communication and processing overheads, ultimately
reducing the utility of information networks. To alleviate the negative impacts
of this information multiplicity phenomenon, we propose that a subset of nodes
(selected at key positions in the network) carry out additional information
filtering functionality. Thus, nodes are responsible for the removal (or
significant reduction) of the redundant data items relayed through them. We
refer to such nodes as filters. We formally define the Filter Placement problem
as a combinatorial optimization problem, and study its computational complexity
for different types of graphs. We also present polynomial-time approximation
algorithms and scalable heuristics for the problem. Our experimental results,
which we obtained through extensive simulations on synthetic and real-world
information flow networks, suggest that in many settings a relatively small
number of filters are fairly effective in removing a large fraction of
redundant information."
"Graphs are fundamental data structures and have been employed for centuries
to model real-world systems and phenomena. Random walk with restart (RWR)
provides a good proximity score between two nodes in a graph, and it has been
successfully used in many applications such as automatic image captioning,
recommender systems, and link prediction. The goal of this work is to find
nodes that have top-k highest proximities for a given node. Previous approaches
to this problem find nodes efficiently at the expense of exactness. The main
motivation of this paper is to answer, in the affirmative, the question, `Is it
possible to improve the search time without sacrificing the exactness?'. Our
solution, {it K-dash}, is based on two ideas: (1) It computes the proximity of
a selected node efficiently by sparse matrices, and (2) It skips unnecessary
proximity computations when searching for the top-k nodes. Theoretical analyses
show that K-dash guarantees result exactness. We perform comprehensive
experiments to verify the efficiency of K-dash. The results show that K-dash
can find top-k nodes significantly faster than the previous approaches while it
guarantees exactness."
"The problem of finding locally dense components of a graph is an important
primitive in data analysis, with wide-ranging applications from community
mining to spam detection and the discovery of biological network modules. In
this paper we present new algorithms for finding the densest subgraph in the
streaming model. For any epsilon>0, our algorithms make O((log n)/log
(1+epsilon)) passes over the input and find a subgraph whose density is
guaranteed to be within a factor 2(1+epsilon) of the optimum. Our algorithms
are also easily parallelizable and we illustrate this by realizing them in the
MapReduce model. In addition we perform extensive experimental evaluation on
massive real-world graphs showing the performance and scalability of our
algorithms in practice."
"In this work, we study the correlation between attribute sets and the
occurrence of dense subgraphs in large attributed graphs, a task we call
structural correlation pattern mining. A structural correlation pattern is a
dense subgraph induced by a particular attribute set. Existing methods are not
able to extract relevant knowledge regarding how vertex attributes interact
with dense subgraphs. Structural correlation pattern mining combines aspects of
frequent itemset and quasi-clique mining problems. We propose statistical
significance measures that compare the structural correlation of attribute sets
against their expected values using null models. Moreover, we evaluate the
interestingness of structural correlation patterns in terms of size and
density. An efficient algorithm that combines search and pruning strategies in
the identification of the most relevant structural correlation patterns is
presented. We apply our method for the analysis of three real-world attributed
graphs: a collaboration, a music, and a citation network, verifying that it
provides valuable knowledge in a feasible time."
"This paper presents a query evaluation technique for positive relational
algebra queries with aggregates on a representation system for probabilistic
data based on the algebraic structures of semiring and semimodule. The core of
our evaluation technique is a procedure that compiles semimodule and semiring
expressions into so-called decomposition trees, for which the computation of
the probability distribution can be done in time linear in the product of the
sizes of the probability distributions represented by its nodes. We give
syntactic characterisations of tractable queries with aggregates by exploiting
the connection between query tractability and polynomial-time decomposition
trees. A prototype of the technique is incorporated in the probabilistic
database engine SPROUT. We report on performance experiments with custom
datasets and TPC-H data."
"Data-centric dynamic systems are systems where both the process controlling
the dynamics and the manipulation of data are equally central. In this paper we
study verification of (first-order) mu-calculus variants over relational
data-centric dynamic systems, where data are represented by a full-fledged
relational database, and the process is described in terms of atomic actions
that evolve the database. The execution of such actions may involve calls to
external services, providing fresh data inserted into the system. As a result
such systems are typically infinite-state. We show that verification is
undecidable in general, and we isolate notable cases, where decidability is
achieved. Specifically we start by considering service calls that return values
deterministically (depending only on passed parameters). We show that in a
mu-calculus variant that preserves knowledge of objects appeared along a run we
get decidability under the assumption that the fresh data introduced along a
run are bounded, though they might not be bounded in the overall system. In
fact we tie such a result to a notion related to weak acyclicity studied in
data exchange. Then, we move to nondeterministic services where the assumption
of data bounded run would result in a bound on the service calls that can be
invoked during the execution and hence would be too restrictive. So we
investigate decidability under the assumption that knowledge of objects is
preserved only if they are continuously present. We show that if infinitely
many values occur in a run but do not accumulate in the same state, then we get
again decidability. We give syntactic conditions to avoid this accumulation
through the novel notion of ""generate-recall acyclicity"", which takes into
consideration that every service call activation generates new values that
cannot be accumulated indefinitely."
"Modern business applications and scientific databases call for inherently
dynamic data storage environments. Such environments are characterized by two
challenging features: (a) they have little idle system time to devote on
physical design; and (b) there is little, if any, a priori workload knowledge,
while the query and data workload keeps changing dynamically. In such
environments, traditional approaches to index building and maintenance cannot
apply. Database cracking has been proposed as a solution that allows on-the-fly
physical data reorganization, as a collateral effect of query processing.
Cracking aims to continuously and automatically adapt indexes to the workload
at hand, without human intervention. Indexes are built incrementally,
adaptively, and on demand. Nevertheless, as we show, existing adaptive indexing
methods fail to deliver workload-robustness; they perform much better with
random workloads than with others. This frailty derives from the inelasticity
with which these approaches interpret each query as a hint on how data should
be stored. Current cracking schemes blindly reorganize the data within each
query's range, even if that results into successive expensive operations with
minimal indexing benefit. In this paper, we introduce stochastic cracking, a
significantly more resilient approach to adaptive indexing. Stochastic cracking
also uses each query as a hint on how to reorganize data, but not blindly so;
it gains resilience and avoids performance bottlenecks by deliberately applying
certain arbitrary choices in its decision-making. Thereby, we bring adaptive
indexing forward to a mature formulation that confers the workload-robustness
previous approaches lacked. Our extensive experimental study verifies that
stochastic cracking maintains the desired properties of original database
cracking while at the same time it performs well with diverse realistic
workloads."
"Traditional database systems are built around the query-at-a-time model. This
approach tries to optimize performance in a best-effort way. Unfortunately,
best effort is not good enough for many modern applications. These applications
require response time guarantees in high load situations. This paper describes
the design of a new database architecture that is based on batching queries and
shared computation across possibly hundreds of concurrent queries and updates.
Performance experiments with the TPC-W benchmark show that the performance of
our implementation, SharedDB, is indeed robust across a wide range of dynamic
workloads."
"By incorporating human workers into the query execution process crowd-enabled
databases facilitate intelligent, social capabilities like completing missing
data at query time or performing cognitive operators. But despite all their
flexibility, crowd-enabled databases still maintain rigid schemas. In this
paper, we extend crowd-enabled databases by flexible query-driven schema
expansion, allowing the addition of new attributes to the database at query
time. However, the number of crowd-sourced mini-tasks to fill in missing values
may often be prohibitively large and the resulting data quality is doubtful.
Instead of simple crowd-sourcing to obtain all values individually, we leverage
the user-generated data found in the Social Web: By exploiting user ratings we
build perceptual spaces, i.e., highly-compressed representations of opinions,
impressions, and perceptions of large numbers of users. Using few training
samples obtained by expert crowd sourcing, we then can extract all missing data
automatically from the perceptual space with high quality and at low costs.
Extensive experiments show that our approach can boost both performance and
quality of crowd-enabled databases, while also providing the flexibility to
expand schemas in a query-driven fashion."
"Data-management-as-a-service systems are increasingly being used in
collaborative settings, where multiple users access common datasets. Cloud
providers have the choice to implement various optimizations, such as indexing
or materialized views, to accelerate queries over these datasets. Each
optimization carries a cost and may benefit multiple users. This creates a
major challenge: how to select which optimizations to perform and how to share
their cost among users. The problem is especially challenging when users are
selfish and will only report their true values for different optimizations if
doing so maximizes their utility. In this paper, we present a new approach for
selecting and pricing shared optimizations by using Mechanism Design. We first
show how to apply the Shapley Value Mechanism to the simple case of selecting
and pricing additive optimizations, assuming an offline game where all users
access the service for the same time-period. Second, we extend the approach to
online scenarios where users come and go. Finally, we consider the case of
substitutive optimizations. We show analytically that our mechanisms induce
truth- fulness and recover the optimization costs. We also show experimentally
that our mechanisms yield higher utility than the state-of-the-art approach
based on regret accumulation."
"Recent years have witnessed an unprecedented proliferation of social media.
People around the globe author, every day, millions of blog posts, social
network status updates, etc. This rich stream of information can be used to
identify, on an ongoing basis, emerging stories, and events that capture
popular attention. Stories can be identified via groups of tightly-coupled
real-world entities, namely the people, locations, products, etc., that are
involved in the story. The sheer scale, and rapid evolution of the data
involved necessitate highly efficient techniques for identifying important
stories at every point of time. The main challenge in real-time story
identification is the maintenance of dense subgraphs (corresponding to groups
of tightly-coupled entities) under streaming edge weight updates (resulting
from a stream of user-generated content). This is the first work to study the
efficient maintenance of dense subgraphs under such streaming edge weight
updates. For a wide range of definitions of density, we derive theoretical
results regarding the magnitude of change that a single edge weight update can
cause. Based on these, we propose a novel algorithm, DYNDENS, which outperforms
adaptations of existing techniques to this setting, and yields meaningful
results. Our approach is validated by a thorough experimental evaluation on
large-scale real and synthetic datasets."
"Analyzing large scale data has emerged as an important activity for many
organizations in the past few years. This large scale data analysis is
facilitated by the MapReduce programming and execution model and its
implementations, most notably Hadoop. Users of MapReduce often have analysis
tasks that are too complex to express as individual MapReduce jobs. Instead,
they use high-level query languages such as Pig, Hive, or Jaql to express their
complex tasks. The compilers of these languages translate queries into
workflows of MapReduce jobs. Each job in these workflows reads its input from
the distributed file system used by the MapReduce system and produces output
that is stored in this distributed file system and read as input by the next
job in the workflow. The current practice is to delete these intermediate
results from the distributed file system at the end of executing the workflow.
One way to improve the performance of workflows of MapReduce jobs is to keep
these intermediate results and reuse them for future workflows submitted to the
system. In this paper, we present ReStore, a system that manages the storage
and reuse of such intermediate results. ReStore can reuse the output of whole
MapReduce jobs that are part of a workflow, and it can also create additional
reuse opportunities by materializing and storing the output of query execution
operators that are executed within a MapReduce job. We have implemented ReStore
as an extension to the Pig dataflow system on top of Hadoop, and we
experimentally demonstrate significant speedups on queries from the PigMix
benchmark."
"A well-established and fundamental insight in database theory is that
negation (also known as complementation) tends to make queries difficult to
process and difficult to reason about. Many basic problems are decidable and
admit practical algorithms in the case of unions of conjunctive queries, but
become difficult or even undecidable when queries are allowed to contain
negation. Inspired by recent results in finite model theory, we consider a
restricted form of negation, guarded negation. We introduce a fragment of SQL,
called GN-SQL, as well as a fragment of Datalog with stratified negation,
called GN-Datalog, that allow only guarded negation, and we show that these
query languages are computationally well behaved, in terms of testing query
containment, query evaluation, open-world query answering, and boundedness.
GN-SQL and GN-Datalog subsume a number of well known query languages and
constraint languages, such as unions of conjunctive queries, monadic Datalog,
and frontier-guarded tgds. In addition, an analysis of standard benchmark
workloads shows that most usage of negation in SQL in practice is guarded
negation."
"Bayesian inference is an important technique throughout statistics. The
essence of Beyesian inference is to derive the posterior belief updated from
prior belief by the learned information, which is a set of differentially
private answers under differential privacy. Although Bayesian inference can be
used in a variety of applications, it becomes theoretically hard to solve when
the number of differentially private answers is large. To facilitate Bayesian
inference under differential privacy, this paper proposes a systematic
mechanism. The key step of the mechanism is the implementation of Bayesian
updating with the best linear unbiased estimator derived by Gauss-Markov
theorem. In addition, we also apply the proposed inference mechanism into an
online queryanswering system, the novelty of which is that the utility for
users is guaranteed by Bayesian inference in the form of credible interval and
confidence level. Theoretical and experimental analysis are shown to
demonstrate the efficiency and effectiveness of both inference mechanism and
online query-answering system."
"The World Wide Web currently evolves into a Web of Linked Data where content
providers publish and link data as they have done with hypertext for the last
20 years. While the declarative query language SPARQL is the de facto for
querying a-priory defined sets of data from the Web, no language exists for
querying the Web of Linked Data itself. However, it seems natural to ask
whether SPARQL is also suitable for such a purpose.
  In this paper we formally investigate the applicability of SPARQL as a query
language for Linked Data on the Web. In particular, we study two query models:
1) a full-Web semantics where the scope of a query is the complete set of
Linked Data on the Web and 2) a family of reachability-based semantics which
restrict the scope to data that is reachable by traversing certain data links.
For both models we discuss properties such as monotonicity and computability as
well as the implications of querying a Web that is infinitely large due to data
generating servers."
"Extract, Transform, Load (ETL) is an integral part of Data Warehousing (DW)
implementation. The commercial tools that are used for this purpose captures
lot of execution trace in form of various log files with plethora of
information. However there has been hardly any initiative where any proactive
analyses have been done on the ETL logs to improve their efficiency. In this
paper we utilize outlier detection technique to find the processes varying most
from the group in terms of execution trace. As our experiment was carried on
actual production processes, any outlier we would consider as a signal rather
than a noise. To identify the input parameters for the outlier detection
algorithm we employ a survey among developer community with varied mix of
experience and expertise. We use simple text parsing to extract these features
from the logs, as shortlisted from the survey. Subsequently we applied outlier
detection technique (Clustering based) on the logs. By this process we reduced
our domain of detailed analysis from 500 logs to 44 logs (8 Percentage). Among
the 5 outlier cluster, 2 of them are genuine concern, while the other 3 figure
out because of the huge number of rows involved."
"The increasing use of statistical data analysis in enterprise applications
has created an arms race among database vendors to offer ever more
sophisticated in-database analytics. One challenge in this race is that each
new statistical technique must be implemented from scratch in the RDBMS, which
leads to a lengthy and complex development process. We argue that the root
cause for this overhead is the lack of a unified architecture for in-database
analytics. Our main contribution in this work is to take a step towards such a
unified architecture. A key benefit of our unified architecture is that
performance optimizations for analytics techniques can be studied generically
instead of an ad hoc, per-technique fashion. In particular, our technical
contributions are theoretical and empirical studies of two key factors that we
found impact performance: the order data is stored, and parallelization of
computations on a single-node multicore RDBMS. We demonstrate the feasibility
of our architecture by integrating several popular analytics techniques into
two commercial and one open-source RDBMS. Our architecture requires changes to
only a few dozen lines of code to integrate a new statistical technique. We
then compare our approach with the native analytics tools offered by the
commercial RDBMSes on various analytics tasks, and validate that our approach
achieves competitive or higher performance, while still achieving the same
quality."
"Clustering is one of the main tasks in exploratory data analysis and
descriptive statistics where the main objective is partitioning observations in
groups. Clustering has a broad range of application in varied domains like
climate, business, information retrieval, biology, psychology, to name a few. A
variety of methods and algorithms have been developed for clustering tasks in
the last few decades. We observe that most of these algorithms define a cluster
in terms of value of the attributes, density, distance etc. However these
definitions fail to attach a clear meaning/semantics to the generated clusters.
We argue that clusters having understandable and distinct semantics defined in
terms of quartiles/halves are more appealing to business analysts than the
clusters defined by data boundaries or prototypes. On the samepremise, we
propose our new algorithm named as quartile clustering technique. Through a
series of experiments we establish efficacy of this algorithm. We demonstrate
that the quartile clustering technique adds clear meaning to each of the
clusters compared to K-means. We use DB Index to measure goodness of the
clusters and show our method is comparable to EM (Expectation Maximization),
PAM (Partition around Medoid) and K Means. We have explored its capability in
detecting outlier and the benefit of added semantics. We discuss some of the
limitations in its present form and also provide a rough direction in
addressing the issue of merging the generated clusters."
"RDBMS is the heart for both OLTP and OLAP types of applications. For both
types of applications thousands of queries expressed in terms of SQL are
executed on daily basis. All the commercial DBMS engines capture various
attributes in system tables about these executed queries. These queries need to
conform to best practices and need to be tuned to ensure optimal performance.
While we use checklists, often tools to enforce the same, a black box technique
on the queries for profiling, outlier detection is not employed for a summary
level understanding. This is the motivation of the paper, as this not only
points out to inefficiencies built in the system, but also has the potential to
point evolving best practices and inappropriate usage. Certainly this can
reduce latency in information flow and optimal utilization of hardware and
software capacity. In this paper we start with formulating the problem. We
explore four outlier detection techniques. We apply these techniques over rich
corpora of production queries and analyze the results. We also explore benefit
of an ensemble approach. We conclude with future courses of action. The same
philosophy we have used for optimization of extraction, transform, load (ETL)
jobs in one of our previous work. We give a brief introduction of the same in
section four."
"Computing frequent itemsets in transactional databases is a vital but
computationally expensive task. Measuring the difference of two datasets is
often done by computing their respective frequent itemsets despite high
computational cost. This paper proposes a linear programming-based approach to
this problem and shows that there exists a distance measure for transactional
database that relies on closed frequent itemsets but does not require their
generation."
"In this extended abstract we provide a unifying framework that can be used to
characterize and compare the expressive power of query languages for different
data base models. The framework is based upon the new idea of valid partition,
that is a partition of the elements of a given data base, where each class of
the partition is composed by elements that cannot be separated (distinguished)
according to some level of information contained in the data base. We describe
two applications of this new framework, first by deriving a new syntactic
characterization of the expressive power of relational algebra which is
equivalent to the one given by Paredaens, and subsequently by studying the
expressive power of a simple graph-based data model."
"While users today have access to many tools that assist in performing large
scale data analysis tasks, understanding the performance characteristics of
their parallel computations, such as MapReduce jobs, remains difficult. We
present PerfXplain, a system that enables users to ask questions about the
relative performances (i.e., runtimes) of pairs of MapReduce jobs. PerfXplain
provides a new query language for articulating performance queries and an
algorithm for generating explanations from a log of past MapReduce job
executions. We formally define the notion of an explanation together with three
metrics, relevance, precision, and generality, that measure explanation
quality. We present the explanation-generation algorithm based on techniques
related to decision-tree building. We evaluate the approach on a log of past
executions on Amazon EC2, and show that our approach can generate quality
explanations, outperforming two naive explanation-generation methods."
"Clustering uncertain data has emerged as a challenging task in uncertain data
management and mining. Thanks to a computational complexity advantage over
other clustering paradigms, partitional clustering has been particularly
studied and a number of algorithms have been developed. While existing
proposals differ mainly in the notions of cluster centroid and clustering
objective function, little attention has been given to an analysis of their
characteristics and limits. In this work, we theoretically investigate major
existing methods of partitional clustering, and alternatively propose a
well-founded approach to clustering uncertain data based on a novel notion of
cluster centroid. A cluster centroid is seen as an uncertain object defined in
terms of a random variable whose realizations are derived based on all
deterministic representations of the objects to be clustered. As demonstrated
theoretically and experimentally, this allows for better representing a cluster
of uncertain objects, thus supporting a consistently improved clustering
performance while maintaining comparable efficiency with existing partitional
clustering algorithms."
"Over half a century old and showing no signs of aging, k-means remains one of
the most popular data processing algorithms. As is well-known, a proper
initialization of k-means is crucial for obtaining a good final solution. The
recently proposed k-means++ initialization algorithm achieves this, obtaining
an initial set of centers that is provably close to the optimum solution. A
major downside of the k-means++ is its inherent sequential nature, which limits
its applicability to massive data: one must make k passes over the data to find
a good initial set of centers. In this work we show how to drastically reduce
the number of passes needed to obtain, in parallel, a good initialization. This
is unlike prevailing efforts on parallelizing k-means that have mostly focused
on the post-initialization phases of k-means. We prove that our proposed
initialization algorithm k-means|| obtains a nearly optimal solution after a
logarithmic number of passes, and then show that in practice a constant number
of passes suffices. Experimental evaluation on real-world large-scale data
demonstrates that k-means|| outperforms k-means++ in both sequential and
parallel settings."
"We study verification of systems whose transitions consist of accesses to a
Web-based data-source. An access is a lookup on a relation within a relational
database, fixing values for a set of positions in the relation. For example, a
transition can represent access to a Web form, where the user is restricted to
filling in values for a particular set of fields. We look at verifying
properties of a schema describing the possible accesses of such a system. We
present a language where one can describe the properties of an access path, and
also specify additional restrictions on accesses that are enforced by the
schema. Our main property language, AccLTL, is based on a first-order extension
of linear-time temporal logic, interpreting access paths as sequences of
relational structures. We also present a lower-level automaton model,
Aautomata, which AccLTL specifications can compile into. We show that AccLTL
and A-automata can express static analysis problems related to ""querying with
limited access patterns"" that have been studied in the database literature in
the past, such as whether an access is relevant to answering a query, and
whether two queries are equivalent in the accessible data they can return. We
prove decidability and complexity results for several restrictions and variants
of AccLTL, and explain which properties of paths can be expressed in each
restriction."
"The three traditional failure classes are system, media, and transaction
failures. Sometimes, however, modern storage exhibits failures that differ from
all of those. In order to capture and describe such cases, single-page failures
are introduced as a fourth failure class. This class encompasses all failures
to read a data page correctly and with plausible contents despite all
correction attempts in lower system levels. Efficient recovery seems to require
a new data structure called the page recovery index. Its transactional
maintenance can be accomplished writing the same number of log records as
today's efficient implementations of logging and recovery. Detection and
recovery of a single-page failure can be sufficiently fast that the affected
data access is merely delayed, without the need to abort the transaction."
"Adaptive indexing initializes and optimizes indexes incrementally, as a side
effect of query processing. The goal is to achieve the benefits of indexes
while hiding or minimizing the costs of index creation. However,
index-optimizing side effects seem to turn read-only queries into update
transactions that might, for example, create lock contention. This paper
studies concurrency control in the context of adaptive indexing. We show that
the design and implementation of adaptive indexing rigorously separates index
structures from index contents; this relaxes the constraints and requirements
during adaptive indexing compared to those of traditional index updates. Our
design adapts to the fact that an adaptive index is refined continuously, and
exploits any concurrency opportunities in a dynamic way. A detailed
experimental analysis demonstrates that (a) adaptive indexing maintains its
adaptive properties even when running concurrent queries, (b) adaptive indexing
can exploit the opportunity for parallelism due to concurrent queries, (c) the
number of concurrency conflicts and any concurrency administration overheads
follow an adaptive behavior, decreasing as the workload evolves and adapting to
the workload needs."
"In this paper, we analyze the nature and distribution of structured data on
the Web. Web-scale information extraction, or the problem of creating
structured tables using extraction from the entire web, is gathering lots of
research interest. We perform a study to understand and quantify the value of
Web-scale extraction, and how structured information is distributed amongst top
aggregator websites and tail sites for various interesting domains. We believe
this is the first study of its kind, and gives us new insights for information
extraction over the Web."
"Storing XML documents in a relational database is a promising solution
because relational databases are mature and scale very well and they have the
advantages that in a relational database XML data and structured data can
coexist making it possible to build application that involve both kinds of data
with little extra effort . In this paper, we propose an algorithm schema named
XRecursive that translates XML documents to relational database according to
the proposed storing structure. The steps and algorithm are given in details to
describe how to use the storing structure to storage and query XML documents in
relational database. Then we report our experimental results on a real database
to show the performance of our method in some features."
"A new approach, to measure normalization completeness for conceptual model,
is introduced using quantitative fuzzy functionality in this paper. We measure
the normalization completeness of the conceptual model in two steps. In the
first step, different normalization techniques are analyzed up to Boyce Codd
Normal Form (BCNF) to find the current normal form of the relation. In the
second step, fuzzy membership values are used to scale the normal form between
0 and 1. Case studies to explain schema transformation rules and measurements.
Normalization completeness is measured by considering completeness attributes,
preventing attributes of the functional dependencies and total number of
attributes such as if the functional dependency is non-preventing then the
attributes of that functional dependency are completeness attributes. The
attributes of functional dependency which prevent to go to the next normal form
are called preventing attributes."
"Recent improvements in positioning technology has led to a much wider
availability of massive moving object data. A crucial task is to find the
moving objects that travel together. Usually, these object sets are called
spatio-temporal patterns. Due to the emergence of many different kinds of
spatio-temporal patterns in recent years, different approaches have been
proposed to extract them. However, each approach only focuses on mining a
specific kind of pattern. In addition to being a painstaking task due to the
large number of algorithms used to mine and manage patterns, it is also time
consuming. Moreover, we have to execute these algorithms again whenever new
data are added to the existing database. To address these issues, we first
redefine spatio-temporal patterns in the itemset context. Secondly, we propose
a unifying approach, named GeT_Move, which uses a frequent closed itemset-based
spatio-temporal pattern-mining algorithm to mine and manage different
spatio-temporal patterns. GeT_Move is implemented in two versions which are
GeT_Move and Incremental GeT_Move. To optimize the efficiency and to free the
parameters setting, we also propose a Parameter Free Incremental GeT_Move
algorithm. Comprehensive experiments are performed on real datasets as well as
large synthetic datasets to demonstrate the effectiveness and efficiency of our
approaches."
"Mappings between related ontologies are increasingly used to support data
integration and analysis tasks. Changes in the ontologies also require the
adaptation of ontology mappings. So far the evolution of ontology mappings has
received little attention albeit ontologies change continuously especially in
the life sciences. We therefore analyze how mappings between popular life
science ontologies evolve for different match algorithms. We also evaluate
which semantic ontology changes primarily affect the mappings. We further
investigate alternatives to predict or estimate the degree of future mapping
changes based on previous ontology and mapping transitions."
"Database flexible querying is an alternative to the classic one for users.
The use of Formal Concepts Analysis (FCA) makes it possible to make approximate
answers that those turned over by a classic DataBase Management System (DBMS).
Some applications do not need exact answers. However, flexible querying can be
expensive in response time. This time is more significant when the flexible
querying require the calculation of aggregate functions (""Sum"", ""Avg"", ""Count"",
""Var"" etc.). In this paper, we propose an approach which tries to solve this
problem by using Approximate Query Processing (AQP)."
"We solve a problem, stated in [CGP10], showing that Sticky Datalog, defined
in the cited paper as an element of the Datalog\pm project, has the finite
controllability property. In order to do that, we develop a technique, which we
believe can have further applications, of approximating Chase(D, T), for a
database instance D and some sets of tuple generating dependencies T, by an
infinite sequence of finite structures, all of them being models of T."
"We study three different kinds of embeddings of tree patterns:
weakly-injective, ancestor-preserving, and lca-preserving. While each of them
is often referred to as injective embedding, they form a proper hierarchy and
their computational properties vary (from P to NP-complete). We present a
thorough study of the complexity of the model checking problem i.e., is there
an embedding of a given tree pattern in a given tree, and we investigate the
impact of various restrictions imposed on the tree pattern: bound on the degree
of a node, bound on the height, and type of allowed labels and edges."
"Most works on Web services has focused on discovery, composition and
selection processes of these kinds of services. Other few works were interested
in how to represent Web services search queries. However, these queries cannot
be processed by ensuring a high level of performance without being adequately
represented first. To this end, different query languages were designed. Even
so, in the absence of a standard, these languages are quite various. Their
diversity makes it difficult choosing the most suitable language. In fact, this
language should be able to cover all types of preferences or requirements of
clients such as their functional, nonfunctional,temporal or even specific
constraints as is the case of geographical or spatial constraints and meet
their needs and preferences helping to provide them the best answer. It must
also be mutually simple and imposes no restrictions or at least not too many
constraints in terms of prior knowledge to use and also provide a formal or
semi-formal queries presentation to support their automatic post-processing. A
comparative study is eventually established to allow to reveal the advantages
and limitations of various existing languages in this context. It is a
synthesis of this category of languages discussing their performance level and
their capability to respond to various needs related to the Web services
research and discovery case. The criterions identified at this stage may, in
our opinion, constitute then the main pre-requisite that a language should
satisfy to be called perfect or to be a future standard."
"Shortest path computation is one of the most common queries in location-based
services (LBSs). Although particularly useful, such queries raise serious
privacy concerns. Exposing to a (potentially untrusted) LBS the client's
position and her destination may reveal personal information, such as social
habits, health condition, shopping preferences, lifestyle choices, etc. The
only existing method for privacy-preserving shortest path computation follows
the obfuscation paradigm; it prevents the LBS from inferring the source and
destination of the query with a probability higher than a threshold. This
implies, however, that the LBS still deduces some information (albeit not
exact) about the client's location and her destination. In this paper we aim at
strong privacy, where the adversary learns nothing about the shortest path
query. We achieve this via established private information retrieval
techniques, which we treat as black-box building blocks. Experiments on real,
large-scale road networks assess the practicality of our schemes."
"This work proposes V-SMART-Join, a scalable MapReduce-based framework for
discovering all pairs of similar entities. The V-SMART-Join framework is
applicable to sets, multisets, and vectors. V-SMART-Join is motivated by the
observed skew in the underlying distributions of Internet traffic, and is a
family of 2-stage algorithms, where the first stage computes and joins the
partial results, and the second stage computes the similarity exactly for all
candidate pairs. The V-SMART-Join algorithms are very efficient and scalable in
the number of entities, as well as their cardinalities. They were up to 30
times faster than the state of the art algorithm, VCL, when compared on a real
dataset of a small size. We also established the scalability of the proposed
algorithms by running them on a dataset of a realistic size, on which VCL never
succeeded to finish. Experiments were run using real datasets of IPs and
cookies, where each IP is represented as a multiset of cookies, and the goal is
to discover similar IPs to identify Internet proxies."
"We address the problem of performing semantic transformations on strings,
which may represent a variety of data types (or their combination) such as a
column in a relational table, time, date, currency, etc. Unlike syntactic
transformations, which are based on regular expressions and which interpret a
string as a sequence of characters, semantic transformations additionally
require exploiting the semantics of the data type represented by the string,
which may be encoded as a database of relational tables. Manually performing
such transformations on a large collection of strings is error prone and
cumbersome, while programmatic solutions are beyond the skill-set of end-users.
We present a programming by example technology that allows end-users to
automate such repetitive tasks. We describe an expressive transformation
language for semantic manipulation that combines table lookup operations and
syntactic manipulations. We then present a synthesis algorithm that can learn
all transformations in the language that are consistent with the user-provided
set of input-output examples. We have implemented this technology as an add-in
for the Microsoft Excel Spreadsheet system and have evaluated it successfully
over several benchmarks picked from various Excel help-forums."
"This paper presents Cologne, a declarative optimization platform that enables
constraint optimization problems (COPs) to be declaratively specified and
incrementally executed in distributed systems. Cologne integrates a declarative
networking engine with an off-the-shelf constraint solver. We have developed
the Colog language that combines distributed Datalog used in declarative
networking with language constructs for specifying goals and constraints used
in COPs. Cologne uses novel query processing strategies for processing Colog
programs, by combining the use of bottom-up distributed Datalog evaluation with
top-down goal-oriented constraint solving. Using case studies based on cloud
and wireless network optimizations, we demonstrate that Cologne (1) can
flexibly support a wide range of policy-based optimizations in distributed
systems, (2) results in orders of magnitude less code compared to imperative
implementations, and (3) is highly efficient with low overhead and fast
convergence times."
"Big array analytics is becoming indispensable in answering important
scientific and business questions. Most analysis tasks consist of multiple
steps, each making one or multiple passes over the arrays to be analyzed and
generating intermediate results. In the big data setting, I/O optimization is a
key to efficient analytics. In this paper, we develop a framework and
techniques for capturing a broad range of analysis tasks expressible in
nested-loop forms, representing them in a declarative way, and optimizing their
I/O by identifying sharing opportunities. Experiment results show that our
optimizer is capable of finding execution plans that exploit nontrivial I/O
sharing opportunities with significant savings."
"Purpose: This goal of this study was to evaluate the effects of a data-driven
clinical productivity system that leverages Electronic Health Record (EHR) data
to provide productivity decision support functionality in a real-world clinical
setting. The system was implemented for a large behavioral health care provider
seeing over 75,000 distinct clients a year. Design/methodology/approach: The
key metric in this system is a ""VPU"", which simultaneously optimizes multiple
aspects of clinical care. The resulting mathematical value of clinical
productivity was hypothesized to tightly link the organization's performance to
its expectations and, through transparency and decision support tools at the
clinician level, affect significant changes in productivity, quality, and
consistency relative to traditional models of clinical productivity. Findings:
In only 3 months, every single variable integrated into the VPU system showed
significant improvement, including a 30% rise in revenue, 10% rise in clinical
percentage, a 25% rise in treatment plan completion, a 20% rise in case rate
eligibility, along with similar improvements in compliance/audit issues,
outcomes collection, access, etc. Practical implications: A data-driven
clinical productivity system employing decision support functionality is
effective because of the impact on clinician behavior relative to traditional
clinical productivity systems. Critically, the model is also extensible to
integration with outcomes-based productivity. Originality/Value: EHR's are only
a first step - the problem is turning that data into useful information.
Technology can leverage the data in order to produce actionable information
that can inform clinical practice and decision-making. Without additional
technology, EHR's are essentially just copies of paper-based records stored in
electronic form."
"Clustering is one of the major tasks in data mining. In the last few years,
Clustering of spatial data has received a lot of research attention. Spatial
databases are components of many advanced information systems like geographic
information systems VLSI design systems. In this thesis, we introduce several
efficient algorithms for clustering spatial data. First, we present a
grid-based clustering algorithm that has several advantages and comparable
performance to the well known efficient clustering algorithm. The algorithm has
several advantages. The algorithm does not require many input parameters. It
requires only three parameters, the number of the points in the data space, the
number of the cells in the grid and a percentage. The number of the cells in
the grid reflects the accuracy that should be achieved by the algorithm. The
algorithm is capable of discovering clusters of arbitrary shapes. The
computational complexity of the algorithm is comparable to the complexity of
the most efficient clustering algorithm. The algorithm has been implemented and
tested against different ranges of database sizes. The performance results show
that the running time of the algorithm is superior to the most well known
algorithms (CLARANS [23]). The results show also that the performance of the
algorithm do not degrade as the number of the data points increases."
"Mining frequent itemsets through static Databases has been extensively
studied and used and is always considered a highly challenging task. For this
reason it is interesting to extend it to data streams field. In the streaming
case, the frequent patterns' mining has much more information to track and much
greater complexity to manage. Infrequent items can become frequent later on and
hence cannot be ignored. The output structure needs to be dynamically
incremented to reflect the evolution of itemset frequencies over time. In this
paper, we study this problem and specifically the methodology of mining
time-sensitive data streams. We tried to improve an existing algorithm by
increasing the temporal accuracy and discarding the out-of-date data by adding
a new concept called the ""Shaking Point"". We presented as well some experiments
illustrating the time and space required."
"The multi-criteria decision making, which is possible with the advent of
skyline queries, has been applied in many areas. Though most of the existing
research is concerned with only a single relation, several real world
applications require finding the skyline set of records over multiple
relations. Consequently, the join operation over skylines where the preferences
are local to each relation, has been proposed. In many of those cases, however,
the join often involves performing aggregate operations among some of the
attributes from the different relations. In this paper, we introduce such
queries as ""aggregate skyline join queries"". Since the naive algorithm is
impractical, we propose three algorithms to efficiently process such queries.
The algorithms utilize certain properties of skyline sets, and processes the
skylines as much as possible locally before computing the join. Experiments
with real and synthetic datasets exhibit the practicality and scalability of
the algorithms with respect to the cardinality and dimensionality of the
relations."
"This paper investigates the MaxRS problem in spatial databases. Given a set O
of weighted points and a rectangular region r of a given size, the goal of the
MaxRS problem is to find a location of r such that the sum of the weights of
all the points covered by r is maximized. This problem is useful in many
location-based applications such as finding the best place for a new franchise
store with a limited delivery range and finding the most attractive place for a
tourist with a limited reachable range. However, the problem has been studied
mainly in theory, particularly, in computational geometry. The existing
algorithms from the computational geometry community are in-memory algorithms
which do not guarantee the scalability. In this paper, we propose a scalable
external-memory algorithm (ExactMaxRS) for the MaxRS problem, which is optimal
in terms of the I/O complexity. Furthermore, we propose an approximation
algorithm (ApproxMaxCRS) for the MaxCRS problem that is a circle version of the
MaxRS problem. We prove the correctness and optimality of the ExactMaxRS
algorithm along with the approximation bound of the ApproxMaxCRS algorithm.
From extensive experimental results, we show that the ExactMaxRS algorithm is
two orders of magnitude faster than methods adapted from existing algorithms,
and the approximation bound in practice is much better than the theoretical
bound of the ApproxMaxCRS algorithm."
"The widespread use of location-aware devices has led to countless
location-based services in which a user query can be arbitrarily complex, i.e.,
one that embeds multiple spatial selection and join predicates. Amongst these
predicates, the k-Nearest-Neighbor (kNN) predicate stands as one of the most
important and widely used predicates. Unlike related research, this paper goes
beyond the optimization of queries with single kNN predicates, and shows how
queries with two kNN predicates can be optimized. In particular, the paper
addresses the optimization of queries with: (i) two kNN-select predicates, (ii)
two kNN-join predicates, and (iii) one kNN-join predicate and one kNN-select
predicate. For each type of queries, conceptually correct query evaluation
plans (QEPs) and new algorithms that optimize the query execution time are
presented. Experimental results demonstrate that the proposed algorithms
outperform the conceptually correct QEPs by orders of magnitude."
"A hidden database refers to a dataset that an organization makes accessible
on the web by allowing users to issue queries through a search interface. In
other words, data acquisition from such a source is not by following static
hyper-links. Instead, data are obtained by querying the interface, and reading
the result page dynamically generated. This, with other facts such as the
interface may answer a query only partially, has prevented hidden databases
from being crawled effectively by existing search engines. This paper remedies
the problem by giving algorithms to extract all the tuples from a hidden
database. Our algorithms are provably efficient, namely, they accomplish the
task by performing only a small number of queries, even in the worst case. We
also establish theoretical results indicating that these algorithms are
asymptotically optimal -- i.e., it is impossible to improve their efficiency by
more than a constant factor. The derivation of our upper and lower bound
results reveals significant insight into the characteristics of the underlying
problem. Extensive experiments confirm the proposed techniques work very well
on all the real datasets examined."
"Top-k query processing finds a list of k results that have largest scores
w.r.t the user given query, with the assumption that all the k results are
independent to each other. In practice, some of the top-k results returned can
be very similar to each other. As a result some of the top-k results returned
are redundant. In the literature, diversified top-k search has been studied to
return k results that take both score and diversity into consideration. Most
existing solutions on diversified top-k search assume that scores of all the
search results are given, and some works solve the diversity problem on a
specific problem and can hardly be extended to general cases. In this paper, we
study the diversified top-k search problem. We define a general diversified
top-k search problem that only considers the similarity of the search results
themselves. We propose a framework, such that most existing solutions for top-k
query processing can be extended easily to handle diversified top-k search, by
simply applying three new functions, a sufficient stop condition sufficient(),
a necessary stop condition necessary(), and an algorithm for diversified top-k
search on the current set of generated results, div-search-current(). We
propose three new algorithms, namely, div-astar, div-dp, and div-cut to solve
the div-search-current() problem. div-astar is an A* based algorithm, div-dp is
an algorithm that decomposes the results into components which are searched
using div-astar independently and combined using dynamic programming. div-cut
further decomposes the current set of generated results using cut points and
combines the results using sophisticated operations. We conducted extensive
performance studies using two real datasets, enwiki and reuters. Our div-cut
algorithm finds the optimal solution for diversified top-k search problem in
seconds even for k as large as 2,000."
"Identifying a preferable route is an important problem that finds
applications in map services. When a user plans a trip within a city, the user
may want to find ""a most popular route such that it passes by shopping mall,
restaurant, and pub, and the travel time to and from his hotel is within 4
hours."" However, none of the algorithms in the existing work on route planning
can be used to answer such queries. Motivated by this, we define the problem of
keyword-aware optimal route query, denoted by KOR, which is to find an optimal
route such that it covers a set of user-specified keywords, a specified budget
constraint is satisfied, and an objective score of the route is optimal. The
problem of answering KOR queries is NP-hard. We devise an approximation
algorithm OSScaling with provable approximation bounds. Based on this
algorithm, another more efficient approximation algorithm BucketBound is
proposed. We also design a greedy approximation algorithm. Results of empirical
studies show that all the proposed algorithms are capable of answering KOR
queries efficiently, while the BucketBound and Greedy algorithms run faster.
The empirical studies also offer insight into the accuracy of the proposed
algorithms."
"We study the complexity of query answering using views in a probabilistic XML
setting, identifying large classes of XPath queries -- with child and
descendant navigation and predicates -- for which there are efficient (PTime)
algorithms. We consider this problem under the two possible semantics for XML
query results: with persistent node identifiers and in their absence.
Accordingly, we consider rewritings that can exploit a single view, by means of
compensation, and rewritings that can use multiple views, by means of
intersection. Since in a probabilistic setting queries return answers with
probabilities, the problem of rewriting goes beyond the classic one of
retrieving XML answers from views. For both semantics of XML queries, we show
that, even when XML answers can be retrieved from views, their probabilities
may not be computable. For rewritings that use only compensation, we describe a
PTime decision procedure, based on easily verifiable criteria that distinguish
between the feasible cases -- when probabilistic XML results are computable --
and the unfeasible ones. For rewritings that can use multiple views, with
compensation and intersection, we identify the most permissive conditions that
make probabilistic rewriting feasible, and we describe an algorithm that is
sound in general, and becomes complete under fairly permissive restrictions,
running in PTime modulo worst-case exponential time equivalence tests. This is
the best we can hope for since intersection makes query equivalence intractable
already over deterministic data. Our algorithm runs in PTime whenever
deterministic rewritings can be found in PTime."
"Most of the work on query evaluation in probabilistic databases has focused
on the simple tuple-independent data model, where tuples are independent random
events. Several efficient query evaluation techniques exists in this setting,
such as safe plans, algorithms based on OBDDs, tree-decomposition and a variety
of approximation algorithms. However, complex data analytics tasks often
require complex correlations, and query evaluation then is significantly more
expensive, or more restrictive. In this paper, we propose MVDB as a framework
both for representing complex correlations and for efficient query evaluation.
An MVDB specifies correlations by views, called MarkoViews, on the
probabilistic relations and declaring the weights of the view's outputs. An
MVDB is a (very large) Markov Logic Network. We make two sets of contributions.
First, we show that query evaluation on an MVDB is equivalent to evaluating a
Union of Conjunctive Query(UCQ) over a tuple-independent database. The
translation is exact (thus allowing the techniques developed for tuple
independent databases to be carried over to MVDB), yet it is novel and quite
non-obvious (some resulting probabilities may be negative!). This translation
in itself though may not lead to much gain since the translated query gets
complicated as we try to capture more correlations. Our second contribution is
to propose a new query evaluation strategy that exploits offline compilation to
speed up online query evaluation. Here we utilize and extend our prior work on
compilation of UCQ. We validate experimentally our techniques on a large
probabilistic database with MarkoViews inferred from the DBLP data."
"Coordination is a challenging everyday task; just think of the last time you
organized a party or a meeting involving several people. As a growing part of
our social and professional life goes online, an opportunity for an improved
coordination process arises. Recently, Gupta et al. proposed entangled queries
as a declarative abstraction for data-driven coordination, where the difficulty
of the coordination task is shifted from the user to the database.
Unfortunately, evaluating entangled queries is very hard, and thus previous
work considered only a restricted class of queries that satisfy safety (the
coordination partners are fixed) and uniqueness (all queries need to be
satisfied). In this paper we significantly extend the class of feasible
entangled queries beyond uniqueness and safety. First, we show that we can
simply drop uniqueness and still efficiently evaluate a set of safe entangled
queries. Second, we show that as long as all users coordinate on the same set
of attributes, we can give an efficient algorithm for coordination even if the
set of queries does not satisfy safety. In an experimental evaluation we show
that our algorithms are feasible for a wide spectrum of coordination scenarios."
"Multi-way Theta-join queries are powerful in describing complex relations and
therefore widely employed in real practices. However, existing solutions from
traditional distributed and parallel databases for multi-way Theta-join queries
cannot be easily extended to fit a shared-nothing distributed computing
paradigm, which is proven to be able to support OLAP applications over immense
data volumes. In this work, we study the problem of efficient processing of
multi-way Theta-join queries using MapReduce from a cost-effective perspective.
Although there have been some works using the (key,value) pair-based
programming model to support join operations, efficient processing of multi-way
Theta-join queries has never been fully explored. The substantial challenge
lies in, given a number of processing units (that can run Map or Reduce tasks),
mapping a multi-way Theta-join query to a number of MapReduce jobs and having
them executed in a well scheduled sequence, such that the total processing time
span is minimized. Our solution mainly includes two parts: 1) cost metrics for
both single MapReduce job and a number of MapReduce jobs executed in a certain
order; 2) the efficient execution of a chain-typed Theta-join with only one
MapReduce job. Comparing with the query evaluation strategy proposed in [23]
and the widely adopted Pig Latin and Hive SQL solutions, our method achieves
significant improvement of the join processing efficiency."
"There is a growing trend of performing analysis on large datasets using
workflows composed of MapReduce jobs connected through producer-consumer
relationships based on data. This trend has spurred the development of a number
of interfaces--ranging from program-based to query-based interfaces--for
generating MapReduce workflows. Studies have shown that the gap in performance
can be quite large between optimized and unoptimized workflows. However,
automatic cost-based optimization of MapReduce workflows remains a challenge
due to the multitude of interfaces, large size of the execution plan space, and
the frequent unavailability of all types of information needed for
optimization. We introduce a comprehensive plan space for MapReduce workflows
generated by popular workflow generators. We then propose Stubby, a cost-based
optimizer that searches selectively through the subspace of the full plan space
that can be enumerated correctly and costed based on the information available
in any given setting. Stubby enumerates the plan space based on plan-to-plan
transformations and an efficient search algorithm. Stubby is designed to be
extensible to new interfaces and new types of optimizations, which is a
desirable feature given how rapidly MapReduce systems are evolving. Stubby's
efficiency and effectiveness have been evaluated using representative workflows
from many domains."
"This paper considers the problem of efficiently answering reachability
queries over views of provenance graphs, derived from executions of workflows
that may include recursion. Such views include composite modules and model
fine-grained dependencies between module inputs and outputs. A novel
view-adaptive dynamic labeling scheme is developed for efficient query
evaluation, in which view specifications are labeled statically (i.e. as they
are created) and data items are labeled dynamically as they are produced during
a workflow execution. Although the combination of fine-grained dependencies and
recursive workflows entail, in general, long (linear-size) data labels, we show
that for a large natural class of workflows and views, labels are compact
(logarithmic-size) and reachability queries can be evaluated in constant time.
Experimental results demonstrate the benefit of this approach over the
state-of-the-art technique when applied for labeling multiple views."
"Dependencies have played a significant role in database design for many
years. They have also been shown to be useful in query optimization. In this
paper, we discuss dependencies between lexicographically ordered sets of
tuples. We introduce formally the concept of order dependency and present a set
of axioms (inference rules) for them. We show how query rewrites based on these
axioms can be used for query optimization. We present several interesting
theorems that can be derived using the inference rules. We prove that
functional dependencies are subsumed by order dependencies and that our set of
axioms for order dependencies is sound and complete."
"Analytic functions represent the state-of-the-art way of performing complex
data analysis within a single SQL statement. In particular, an important class
of analytic functions that has been frequently used in commercial systems to
support OLAP and decision support applications is the class of window
functions. A window function returns for each input tuple a value derived from
applying a function over a window of neighboring tuples. However, existing
window function evaluation approaches are based on a naive sorting scheme. In
this paper, we study the problem of optimizing the evaluation of window
functions. We propose several efficient techniques, and identify optimization
opportunities that allow us to optimize the evaluation of a set of window
functions. We have integrated our scheme into PostgreSQL. Our comprehensive
experimental study on the TPC-DS datasets as well as synthetic datasets and
queries demonstrate significant speedup over existing approaches."
"Many systems for big data analytics employ a data flow abstraction to define
parallel data processing tasks. In this setting, custom operations expressed as
user-defined functions are very common. We address the problem of performing
data flow optimization at this level of abstraction, where the semantics of
operators are not known. Traditionally, query optimization is applied to
queries with known algebraic semantics. In this work, we find that a handful of
properties, rather than a full algebraic specification, suffice to establish
reordering conditions for data processing operators. We show that these
properties can be accurately estimated for black box operators by statically
analyzing the general-purpose code of their user-defined functions. We design
and implement an optimizer for parallel data flows that does not assume
knowledge of semantics or algebraic properties of operators. Our evaluation
confirms that the optimizer can apply common rewritings such as selection
reordering, bushy join-order enumeration, and limited forms of aggregation
push-down, hence yielding similar rewriting power as modern relational DBMS
optimizers. Moreover, it can optimize the operator order of non-relational data
flows, a unique feature among today's systems."
"Parallel dataflow systems are a central part of most analytic pipelines for
big data. The iterative nature of many analysis and machine learning
algorithms, however, is still a challenge for current systems. While certain
types of bulk iterative algorithms are supported by novel dataflow frameworks,
these systems cannot exploit computational dependencies present in many
algorithms, such as graph algorithms. As a result, these algorithms are
inefficiently executed and have led to specialized systems based on other
paradigms, such as message passing or shared memory. We propose a method to
integrate incremental iterations, a form of workset iterations, with parallel
dataflows. After showing how to integrate bulk iterations into a dataflow
system and its optimizer, we present an extension to the programming model for
incremental iterations. The extension alleviates for the lack of mutable state
in dataflows and allows for exploiting the sparse computational dependencies
inherent in many iterative algorithms. The evaluation of a prototypical
implementation shows that those aspects lead to up to two orders of magnitude
speedup in algorithm runtime, when exploited. In our experiments, the improved
dataflow system is highly competitive with specialized systems while
maintaining a transparent and unified dataflow abstraction."
"In today's Web and social network environments, query workloads include ad
hoc and OLAP queries, as well as iterative algorithms that analyze data
relationships (e.g., link analysis, clustering, learning). Modern DBMSs support
ad hoc and OLAP queries, but most are not robust enough to scale to large
clusters. Conversely, ""cloud"" platforms like MapReduce execute chains of batch
tasks across clusters in a fault tolerant way, but have too much overhead to
support ad hoc queries.
  Moreover, both classes of platform incur significant overhead in executing
iterative data analysis algorithms. Most such iterative algorithms repeatedly
refine portions of their answers, until some convergence criterion is reached.
However, general cloud platforms typically must reprocess all data in each
step. DBMSs that support recursive SQL are more efficient in that they
propagate only the changes in each step -- but they still accumulate each
iteration's state, even if it is no longer useful. User-defined functions are
also typically harder to write for DBMSs than for cloud platforms.
  We seek to unify the strengths of both styles of platforms, with a focus on
supporting iterative computations in which changes, in the form of deltas, are
propagated from iteration to iteration, and state is efficiently updated in an
extensible way. We present a programming model oriented around deltas, describe
how we execute and optimize such programs in our REX runtime system, and
validate that our platform also handles failures gracefully. We experimentally
validate our techniques, and show speedups over the competing methods ranging
from 2.5 to nearly 100 times."
"We study the problem of answering k-hop reachability queries in a directed
graph, i.e., whether there exists a directed path of length k, from a source
query vertex to a target query vertex in the input graph. The problem of k-hop
reachability is a general problem of the classic reachability (where
k=infinity). Existing indexes for processing classic reachability queries, as
well as for processing shortest path queries, are not applicable or not
efficient for processing k-hop reachability queries. We propose an index for
processing k-hop reachability queries, which is simple in design and efficient
to construct. Our experimental results on a wide range of real datasets show
that our index is more efficient than the state-of-the-art indexes even for
processing classic reachability queries, for which these indexes are primarily
designed. We also show that our index is efficient in answering k-hop
reachability queries."
"In the real world a graph is often fragmented and distributed across
different sites. This highlights the need for evaluating queries on distributed
graphs. This paper proposes distributed evaluation algorithms for three classes
of queries: reachability for determining whether one node can reach another,
bounded reachability for deciding whether there exists a path of a bounded
length between a pair of nodes, and regular reachability for checking whether
there exists a path connecting two nodes such that the node labels on the path
form a string in a given regular expression. We develop these algorithms based
on partial evaluation, to explore parallel computation. When evaluating a query
Q on a distributed graph G, we show that these algorithms possess the following
performance guarantees, no matter how G is fragmented and distributed: (1) each
site is visited only once; (2) the total network traffic is determined by the
size of Q and the fragmentation of G, independent of the size of G; and (3) the
response time is decided by the largest fragment of G rather than the entire G.
In addition, we show that these algorithms can be readily implemented in the
MapReduce framework. Using synthetic and real-life data, we experimentally
verify that these algorithms are scalable on large graphs, regardless of how
the graphs are distributed."
"Natural language text corpora are often available as sets of syntactically
parsed trees. A wide range of expressive tree queries are possible over such
parsed trees that open a new avenue in searching over natural language text.
They not only allow for querying roles and relationships within sentences, but
also improve search effectiveness compared to flat keyword queries. One major
drawback of current systems supporting querying over parsed text is the
performance of evaluating queries over large data. In this paper we propose a
novel indexing scheme over unique subtrees as index keys. We also propose a
novel root-split coding scheme that stores subtree structural information only
partially, thus reducing index size and improving querying performance. Our
extensive set of experiments show that root-split coding reduces the index size
of any interval coding which stores individual node numbers by a factor of 50%
to 80%, depending on the sizes of subtrees indexed. Moreover, We show that our
index using root-split coding, outperforms previous approaches by at least an
order of magnitude in terms of the response time of queries."
"The discovery of frequent itemsets can serve valuable economic and research
purposes. Releasing discovered frequent itemsets, however, presents privacy
challenges. In this paper, we study the problem of how to perform frequent
itemset mining on transaction databases while satisfying differential privacy.
We propose an approach, called PrivBasis, which leverages a novel notion called
basis sets. A theta-basis set has the property that any itemset with frequency
higher than theta is a subset of some basis. We introduce algorithms for
privately constructing a basis set and then using it to find the most frequent
itemsets. Experiments show that our approach greatly outperforms the current
state of the art."
"Differential privacy is a promising privacy-preserving paradigm for
statistical query processing over sensitive data. It works by injecting random
noise into each query result, such that it is provably hard for the adversary
to infer the presence or absence of any individual record from the published
noisy results. The main objective in differentially private query processing is
to maximize the accuracy of the query results, while satisfying the privacy
guarantees. Previous work, notably the matrix mechanism, has suggested that
processing a batch of correlated queries as a whole can potentially achieve
considerable accuracy gains, compared to answering them individually. However,
as we point out in this paper, the matrix mechanism is mainly of theoretical
interest; in particular, several inherent problems in its design limit its
accuracy in practice, which almost never exceeds that of naive methods. In
fact, we are not aware of any existing solution that can effectively optimize a
query batch under differential privacy. Motivated by this, we propose the
Low-Rank Mechanism (LRM), the first practical differentially private technique
for answering batch queries with high accuracy, based on a low rank
approximation of the workload matrix. We prove that the accuracy provided by
LRM is close to the theoretical lower bound for any mechanism to answer a batch
of queries under differential privacy. Extensive experiments using real data
demonstrate that LRM consistently outperforms state-of-the-art query processing
solutions under differential privacy, by large margins."
"Data warehouse store and provide access to large volume of historical data
supporting the strategic decisions of organisations. Data warehouse is based on
a multidimensional model which allow to express user's needs for supporting the
decision making process. Since it is estimated that 80% of data used for
decision making has a spatial or location component [1, 2], spatial data have
been widely integrated in Data Warehouses and in OLAP systems. Extending a
multidimensional data model by the inclusion of spatial data provides a concise
and organised spatial datawarehouse representation. This paper aims to provide
a comprehensive review of litterature on developed and suggested spatial and
spatio-temporel multidimensional models. A benchmarking study of the proposed
models is presented. Several evaluation criterias are used to identify the
existence of trends as well as potential needs for further investigations."
"Multidimensional databases are a great asset for decision making. Their users
express complex OLAP (On-Line Analytical Processing) queries, often returning
huge volumes of facts, sometimes providing little or no information.
Furthermore, due to the huge volume of historical data stored in DWs, the OLAP
applications may return a big amount of irrelevant information that could make
the data exploration process not efficient and tardy. OLAP personalization
systems play a major role in reducing the effort of decision-makers to find the
most interesting information. Several works dealing with OLAP personalization
were presented in the last few years. This paper aims to provide a
comprehensive review of literature on OLAP personalization approaches. A
benchmarking study of OLAP personalization methods is proposed. Several
evaluation criteria are used to identify the existence of trends as well as
potential needs for further investigations."
"\epsilon-differential privacy is the state-of-the-art model for releasing
sensitive information while protecting privacy. Numerous methods have been
proposed to enforce epsilon-differential privacy in various analytical tasks,
e.g., regression analysis. Existing solutions for regression analysis, however,
are either limited to non-standard types of regression or unable to produce
accurate regression results. Motivated by this, we propose the Functional
Mechanism, a differentially private method designed for a large class of
optimization-based analyses. The main idea is to enforce epsilon-differential
privacy by perturbing the objective function of the optimization problem,
rather than its results. As case studies, we apply the functional mechanism to
address two most widely used regression models, namely, linear regression and
logistic regression. Both theoretical analysis and thorough experimental
evaluations show that the functional mechanism is highly effective and
efficient, and it significantly outperforms existing solutions."
"Today, the publication of microdata poses a privacy threat. Vast research has
striven to define the privacy condition that microdata should satisfy before it
is released, and devise algorithms to anonymize the data so as to achieve this
condition. Yet, no method proposed to date explicitly bounds the percentage of
information an adversary gains after seeing the published data for each
sensitive value therein. This paper introduces beta-likeness, an appropriately
robust privacy model for microdata anonymization, along with two anonymization
schemes designed therefor, the one based on generalization, and the other based
on perturbation. Our model postulates that an adversary's confidence on the
likelihood of a certain sensitive-attribute (SA) value should not increase, in
relative difference terms, by more than a predefined threshold. Our techniques
aim to satisfy a given beta threshold with little information loss. We
experimentally demonstrate that (i) our model provides an effective privacy
guarantee in a way that predecessor models cannot, (ii) our generalization
scheme is more effective and efficient in its task than methods adapting
algorithms for the k-anonymity model, and (iii) our perturbation method
outperforms a baseline approach. Moreover, we discuss in detail the resistance
of our model and methods to attacks proposed in previous research."
"Real-life graphs usually have various kinds of events happening on them,
e.g., product purchases in online social networks and intrusion alerts in
computer networks. The occurrences of events on the same graph could be
correlated, exhibiting either attraction or repulsion. Such structural
correlations can reveal important relationships between different events.
Unfortunately, correlation relationships on graph structures are not well
studied and cannot be captured by traditional measures. In this work, we design
a novel measure for assessing two-event structural correlations on graphs.
Given the occurrences of two events, we choose uniformly a sample of ""reference
nodes"" from the vicinity of all event nodes and employ the Kendall's tau rank
correlation measure to compute the average concordance of event density
changes. Significance can be efficiently assessed by tau's nice property of
being asymptotically normal under the null hypothesis. In order to compute the
measure in large scale networks, we develop a scalable framework using
different sampling strategies. The complexity of these strategies is analyzed.
Experiments on real graph datasets with both synthetic and real events
demonstrate that the proposed framework is not only efficacious, but also
efficient and scalable."
"Ranking temporal data has not been studied until recently, even though
ranking is an important operator (being promoted as a firstclass citizen) in
database systems. However, only the instant top-k queries on temporal data were
studied in, where objects with the k highest scores at a query time instance t
are to be retrieved. The instant top-k definition clearly comes with
limitations (sensitive to outliers, difficult to choose a meaningful query time
t). A more flexible and general ranking operation is to rank objects based on
the aggregation of their scores in a query interval, which we dub the aggregate
top-k query on temporal data. For example, return the top-10 weather stations
having the highest average temperature from 10/01/2010 to 10/07/2010; find the
top-20 stocks having the largest total transaction volumes from 02/05/2011 to
02/07/2011. This work presents a comprehensive study to this problem by
designing both exact and approximate methods (with approximation quality
guarantees). We also provide theoretical analysis on the construction cost, the
index size, the update and the query costs of each approach. Extensive
experiments on large real datasets clearly demonstrate the efficiency, the
effectiveness, and the scalability of our methods compared to the baseline
methods."
"Growing main memory sizes have facilitated database management systems that
keep the entire database in main memory. The drastic performance improvements
that came along with these in-memory systems have made it possible to reunite
the two areas of online transaction processing (OLTP) and online analytical
processing (OLAP): An emerging class of hybrid OLTP and OLAP database systems
allows to process analytical queries directly on the transactional data. By
offering arbitrarily current snapshots of the transactional data for OLAP,
these systems enable real-time business intelligence. Despite memory sizes of
several Terabytes in a single commodity server, RAM is still a precious
resource: Since free memory can be used for intermediate results in query
processing, the amount of memory determines query performance to a large
extent. Consequently, we propose the compaction of memory-resident databases.
Compaction consists of two tasks: First, separating the mutable working set
from the immutable ""frozen"" data. Second, compressing the immutable data and
optimizing it for efficient, memory-consumption-friendly snapshotting. Our
approach reorganizes and compresses transactional data online and yet hardly
affects the mission-critical OLTP throughput. This is achieved by unburdening
the OLTP threads from all additional processing and performing these tasks
asynchronously."
"Column-oriented database systems have been a real game changer for the
industry in recent years. Highly tuned and performant systems have evolved that
provide users with the possibility of answering ad hoc queries over large
datasets in an interactive manner. In this paper we present the column-oriented
datastore developed as one of the central components of PowerDrill. It combines
the advantages of columnar data layout with other known techniques (such as
using composite range partitions) and extensive algorithmic engineering on key
data structures. The main goal of the latter being to reduce the main memory
footprint and to increase the efficiency in processing typical user queries. In
this combination we achieve large speed-ups. These enable a highly interactive
Web UI where it is common that a single mouse click leads to processing a
trillion values in the underlying dataset."
"Modern hardware is abundantly parallel and increasingly heterogeneous. The
numerous processing cores have non-uniform access latencies to the main memory
and to the processor caches, which causes variability in the communication
costs. Unfortunately, database systems mostly assume that all processing cores
are the same and that microarchitecture differences are not significant enough
to appear in critical database execution paths. As we demonstrate in this
paper, however, hardware heterogeneity does appear in the critical path and
conventional database architectures achieve suboptimal and even worse,
unpredictable performance. We perform a detailed performance analysis of OLTP
deployments in servers with multiple cores per CPU (multicore) and multiple
CPUs per server (multisocket). We compare different database deployment
strategies where we vary the number and size of independent database instances
running on a single server, from a single shared-everything instance to
fine-grained shared-nothing configurations. We quantify the impact of
non-uniform hardware on various deployments by (a) examining how efficiently
each deployment uses the available hardware resources and (b) measuring the
impact of distributed transactions and skewed requests on different workloads.
Finally, we argue in favor of shared-nothing deployments that are topology- and
workload-aware and take advantage of fast on-chip communication between islands
of cores on the same socket."
"We present a framework for concurrency control and availability in
multi-datacenter datastores. While we consider Google's Megastore as our
motivating example, we define general abstractions for key components, making
our solution extensible to any system that satisfies the abstraction
properties. We first develop and analyze a transaction management and
replication protocol based on a straightforward implementation of the Paxos
algorithm. Our investigation reveals that this protocol acts as a concurrency
prevention mechanism rather than a concurrency control mechanism. We then
propose an enhanced protocol called Paxos with Combination and Promotion
(Paxos-CP) that provides true transaction concurrency while requiring the same
per instance message complexity as the basic Paxos protocol. Finally, we
compare the performance of Paxos and Paxos-CP in a multi-datacenter
experimental study, and we demonstrate that Paxos-CP results in significantly
fewer aborted transactions than basic Paxos."
"Database-backed applications are nearly ubiquitous in our daily lives.
Applications that make many small accesses to the database create two
challenges for developers: increased latency and wasted resources from numerous
network round trips. A well-known technique to improve transactional database
application performance is to convert part of the application into stored
procedures that are executed on the database server. Unfortunately, this
conversion is often difficult. In this paper we describe Pyxis, a system that
takes database-backed applications and automatically partitions their code into
two pieces, one of which is executed on the application server and the other on
the database server. Pyxis profiles the application and server loads,
statically analyzes the code's dependencies, and produces a partitioning that
minimizes the number of control transfers as well as the amount of data sent
during each transfer. Our experiments using TPC-C and TPC-W show that Pyxis is
able to generate partitions with up to 3x reduction in latency and 1.7x
improvement in throughput when compared to a traditional non-partitioned
implementation and has comparable performance to that of a custom stored
procedure implementation."
"It is universal to see people obtain knowledge on micro-blog services by
asking others decision making questions. In this paper, we study the Jury
Selection Problem(JSP) by utilizing crowdsourcing for decision making tasks on
micro-blog services. Specifically, the problem is to enroll a subset of crowd
under a limited budget, whose aggregated wisdom via Majority Voting scheme has
the lowest probability of drawing a wrong answer(Jury Error Rate-JER). Due to
various individual error-rates of the crowd, the calculation of JER is
non-trivial. Firstly, we explicitly state that JER is the probability when the
number of wrong jurors is larger than half of the size of a jury. To avoid the
exponentially increasing calculation of JER, we propose two efficient
algorithms and an effective bounding technique. Furthermore, we study the Jury
Selection Problem on two crowdsourcing models, one is for altruistic
users(AltrM) and the other is for incentive-requiring users(PayM) who require
extra payment when enrolled into a task. For the AltrM model, we prove the
monotonicity of JER on individual error rate and propose an efficient exact
algorithm for JSP. For the PayM model, we prove the NP-hardness of JSP on PayM
and propose an efficient greedy-based heuristic algorithm. Finally, we conduct
a series of experiments to investigate the traits of JSP, and validate the
efficiency and effectiveness of our proposed algorithms on both synthetic and
real micro-blog data."
"We study the problem of local alignment, which is finding pairs of similar
subsequences with gaps. The problem exists in biosequence databases. BLAST is a
typical software for finding local alignment based on heuristic, but could miss
results. Using the Smith-Waterman algorithm, we can find all local alignments
in O(mn) time, where m and n are lengths of a query and a text, respectively. A
recent exact approach BWT-SW improves the complexity of the Smith-Waterman
algorithm under constraints, but still much slower than BLAST. This paper takes
on the challenge of designing an accurate and efficient algorithm for
evaluating local-alignment searches, especially for long queries. In this
paper, we propose an efficient software called ALAE to speed up BWT-SW using a
compressed suffix array. ALAE utilizes a family of filtering techniques to
prune meaningless calculations and an algorithm for reusing score calculations.
We also give a mathematical analysis and show that the upper bound of the total
number of calculated entries using ALAE could vary from 4.50mn0.520 to
9.05mn0.896 for random DNA sequences and vary from 8.28mn0.364 to 7.49mn0.723
for random protein sequences. We demonstrate the significant performance
improvement of ALAE on BWT-SW using a thorough experimental study on real
biosequences. ALAE guarantees correctness and accelerates BLAST for most of
parameters."
"Many applications generate and consume temporal data and retrieval of time
series is a key processing step in many application domains. Dynamic time
warping (DTW) distance between time series of size N and M is computed relying
on a dynamic programming approach which creates and fills an NxM grid to search
for an optimal warp path. Since this can be costly, various heuristics have
been proposed to cut away the potentially unproductive portions of the DTW
grid. In this paper, we argue that time series often carry structural features
that can be used for identifying locally relevant constraints to eliminate
redundant work. Relying on this observation, we propose salient feature based
sDTW algorithms which first identify robust salient features in the given time
series and then find a consistent alignment of these to establish the
boundaries for the warp path search. More specifically, we propose alternative
fixed core&adaptive width, adaptive core&fixed width, and adaptive
core&adaptive width strategies which enforce different constraints reflecting
the high level structural characteristics of the series in the data set.
Experiment results show that the proposed sDTW algorithms help achieve much
higher accuracy in DTWcomputation and time series retrieval than fixed core &
fixed width algorithms that do not leverage local features of the given time
series."
"Today's scientists are quickly moving from in vitro to in silico
experimentation: they no longer analyze natural phenomena in a petri dish, but
instead they build models and simulate them. Managing and analyzing the massive
amounts of data involved in simulations is a major task. Yet, they lack the
tools to efficiently work with data of this size. One problem many scientists
share is the analysis of the massive spatial models they build. For several
types of analysis they need to interactively follow the structures in the
spatial model, e.g., the arterial tree, neuron fibers, etc., and issue range
queries along the way. Each query takes long to execute, and the total time for
executing a sequence of queries significantly delays data analysis. Prefetching
the spatial data reduces the response time considerably, but known approaches
do not prefetch with high accuracy. We develop SCOUT, a structure-aware method
for prefetching data along interactive spatial query sequences. SCOUT uses an
approximate graph model of the structures involved in past queries and attempts
to identify what particular structure the user follows. Our experiments with
neuroscience data show that SCOUT prefetches with an accuracy from 71% to 92%,
which translates to a speedup of 4x-15x. SCOUT also improves the prefetching
accuracy on datasets from other scientific domains, such as medicine and
biology."
"As an important application of spatial databases in pathology imaging
analysis, cross-comparing the spatial boundaries of a huge amount of segmented
micro-anatomic objects demands extremely data- and compute-intensive
operations, requiring high throughput at an affordable cost. However, the
performance of spatial database systems has not been satisfactory since their
implementations of spatial operations cannot fully utilize the power of modern
parallel hardware. In this paper, we provide a customized software solution
that exploits GPUs and multi-core CPUs to accelerate spatial cross-comparison
in a cost-effective way. Our solution consists of an efficient GPU algorithm
and a pipelined system framework with task migration support. Extensive
experiments with real-world data sets demonstrate the effectiveness of our
solution, which improves the performance of spatial cross-comparison by over 18
times compared with a parallelized spatial database approach."
"The ability to estimate resource consumption of SQL queries is crucial for a
number of tasks in a database system such as admission control, query
scheduling and costing during query optimization. Recent work has explored the
use of statistical techniques for resource estimation in place of the manually
constructed cost models used in query optimization. Such techniques, which
require as training data examples of resource usage in queries, offer the
promise of superior estimation accuracy since they can account for factors such
as hardware characteristics of the system or bias in cardinality estimates.
However, the proposed approaches lack robustness in that they do not generalize
well to queries that are different from the training examples, resulting in
significant estimation errors. Our approach aims to address this problem by
combining knowledge of database query processing with statistical models. We
model resource-usage at the level of individual operators, with different
models and features for each operator type, and explicitly model the asymptotic
behavior of each operator. This results in significantly better estimation
accuracy and the ability to estimate resource usage of arbitrary plans, even
when they are very different from the training instances. We validate our
approach using various large scale real-life and benchmark workloads on
Microsoft SQL Server."
"The rise of Web 2.0 is signaled by sites such as Flickr, del.icio.us, and
YouTube, and social tagging is essential to their success. A typical tagging
action involves three components, user, item (e.g., photos in Flickr), and tags
(i.e., words or phrases). Analyzing how tags are assigned by certain users to
certain items has important implications in helping users search for desired
information. In this paper, we explore common analysis tasks and propose a dual
mining framework for social tagging behavior mining. This framework is centered
around two opposing measures, similarity and diversity, being applied to one or
more tagging components, and therefore enables a wide range of analysis
scenarios such as characterizing similar users tagging diverse items with
similar tags, or diverse users tagging similar items with diverse tags, etc. By
adopting different concrete measures for similarity and diversity in the
framework, we show that a wide range of concrete analysis problems can be
defined and they are NP-Complete in general. We design efficient algorithms for
solving many of those problems and demonstrate, through comprehensive
experiments over real data, that our algorithms significantly out-perform the
exact brute-force approach without compromising analysis result quality."
"This paper proposes a general framework for matching similar subsequences in
both time series and string databases. The matching results are pairs of query
subsequences and database subsequences. The framework finds all possible pairs
of similar subsequences if the distance measure satisfies the ""consistency""
property, which is a property introduced in this paper. We show that most
popular distance functions, such as the Euclidean distance, DTW, ERP, the
Frechet distance for time series, and the Hamming distance and Levenshtein
distance for strings, are all ""consistent"". We also propose a generic index
structure for metric spaces named ""reference net"". The reference net occupies
O(n) space, where n is the size of the dataset and is optimized to work well
with our framework. The experiments demonstrate the ability of our method to
improve retrieval performance when combined with diverse distance measures. The
experiments also illustrate that the reference net scales well in terms of
space overhead and query time."
"Yellow elephants are slow. A major reason is that they consume their inputs
entirely before responding to an elephant rider's orders. Some clever riders
have trained their yellow elephants to only consume parts of the inputs before
responding. However, the teaching time to make an elephant do that is high. So
high that the teaching lessons often do not pay off. We take a different
approach. We make elephants aggressive; only this will make them very fast. We
propose HAIL (Hadoop Aggressive Indexing Library), an enhancement of HDFS and
Hadoop MapReduce that dramatically improves runtimes of several classes of
MapReduce jobs. HAIL changes the upload pipeline of HDFS in order to create
different clustered indexes on each data block replica. An interesting feature
of HAIL is that we typically create a win-win situation: we improve both data
upload to HDFS and the runtime of the actual Hadoop MapReduce job. In terms of
data upload, HAIL improves over HDFS by up to 60% with the default replication
factor of three. In terms of query execution, we demonstrate that HAIL runs up
to 68x faster than Hadoop. In our experiments, we use six clusters including
physical and EC2 clusters of up to 100 nodes. A series of scalability
experiments also demonstrates the superiority of HAIL."
"Users' locations are important for many applications such as personalized
search and localized content delivery. In this paper, we study the problem of
profiling Twitter users' locations with their following network and tweets. We
propose a multiple location profiling model (MLP), which has three key
features: 1) it formally models how likely a user follows another user given
their locations and how likely a user tweets a venue given his location, 2) it
fundamentally captures that a user has multiple locations and his following
relationships and tweeted venues can be related to any of his locations, and
some of them are even noisy, and 3) it novelly utilizes the home locations of
some users as partial supervision. As a result, MLP not only discovers users'
locations accurately and completely, but also ""explains"" each following
relationship by revealing users' true locations in the relationship.
Experiments on a large-scale data set demonstrate those advantages.
Particularly, 1) for predicting users' home locations, MLP successfully places
62% users and outperforms two state-of-the-art methods by 10% in accuracy, 2)
for discovering users' multiple locations, MLP improves the baseline methods by
14% in recall, and 3) for explaining following relationships, MLP achieves 57%
accuracy."
"Considering the current price gap between disk and flash memory drives, for
applications dealing with large scale data, it will be economically more
sensible to use flash memory drives to supplement disk drives rather than to
replace them. This paper presents FaCE, which is a new low-overhead caching
strategy that uses flash memory as an extension to the DRAM buffer. FaCE aims
at improving the transaction throughput as well as shortening the recovery time
from a system failure. To achieve the goals, we propose two novel algorithms
for flash cache management, namely, Multi-Version FIFO replacement and Group
Second Chance. One striking result from FaCE is that using a small flash memory
drive as a caching device could deliver even higher throughput than using a
large flash memory drive to store the entire database tables. This was possible
due to flash write optimization as well as disk access reduction obtained by
the FaCE caching methods. In addition, FaCE takes advantage of the
non-volatility of flash memory to fully support database recovery by extending
the scope of a persistent database to include the data pages stored in the
flash cache. We have implemented FaCE in the PostgreSQL open source database
server and demonstrated its effectiveness for TPC-C benchmarks."
"This paper presents new alternatives to the well-known Bloom filter data
structure. The Bloom filter, a compact data structure supporting set insertion
and membership queries, has found wide application in databases, storage
systems, and networks. Because the Bloom filter performs frequent random reads
and writes, it is used almost exclusively in RAM, limiting the size of the sets
it can represent. This paper first describes the quotient filter, which
supports the basic operations of the Bloom filter, achieving roughly comparable
performance in terms of space and time, but with better data locality.
Operations on the quotient filter require only a small number of contiguous
accesses. The quotient filter has other advantages over the Bloom filter: it
supports deletions, it can be dynamically resized, and two quotient filters can
be efficiently merged. The paper then gives two data structures, the buffered
quotient filter and the cascade filter, which exploit the quotient filter
advantages and thus serve as SSD-optimized alternatives to the Bloom filter.
The cascade filter has better asymptotic I/O performance than the buffered
quotient filter, but the buffered quotient filter outperforms the cascade
filter on small to medium data sets. Both data structures significantly
outperform recently-proposed SSD-optimized Bloom filter variants, such as the
elevator Bloom filter, buffered Bloom filter, and forest-structured Bloom
filter. In experiments, the cascade filter and buffered quotient filter
performed insertions 8.6-11 times faster than the fastest Bloom filter variant
and performed lookups 0.94-2.56 times faster."
"A central problem in data integration and data cleansing is to find entities
in different data sources that describe the same real-world object. Many
existing methods for identifying such entities rely on explicit linkage rules
which specify the conditions that entities must fulfill in order to be
considered to describe the same real-world object. In this paper, we present
the GenLink algorithm for learning expressive linkage rules from a set of
existing reference links using genetic programming. The algorithm is capable of
generating linkage rules which select discriminative properties for comparison,
apply chains of data transformations to normalize property values, choose
appropriate distance measures and thresholds and combine the results of
multiple comparisons using non-linear aggregation functions. Our experiments
show that the GenLink algorithm outperforms the state-of-the-art genetic
programming approach to learning linkage rules recently presented by Carvalho
et. al. and is capable of learning linkage rules which achieve a similar
accuracy as human written rules for the same problem."
"In recent years, due to the wide applications of uncertain data, mining
frequent itemsets over uncertain databases has attracted much attention. In
uncertain databases, the support of an itemset is a random variable instead of
a fixed occurrence counting of this itemset. Thus, unlike the corresponding
problem in deterministic databases where the frequent itemset has a unique
definition, the frequent itemset under uncertain environments has two different
definitions so far. The first definition, referred as the expected
support-based frequent itemset, employs the expectation of the support of an
itemset to measure whether this itemset is frequent. The second definition,
referred as the probabilistic frequent itemset, uses the probability of the
support of an itemset to measure its frequency. Thus, existing work on mining
frequent itemsets over uncertain databases is divided into two different groups
and no study is conducted to comprehensively compare the two different
definitions. In addition, since no uniform experimental platform exists,
current solutions for the same definition even generate inconsistent results.
In this paper, we firstly aim to clarify the relationship between the two
different definitions. Through extensive experiments, we verify that the two
definitions have a tight connection and can be unified together when the size
of data is large enough. Secondly, we provide baseline implementations of eight
existing representative algorithms and test their performances with uniform
measures fairly. Finally, according to the fair tests over many different
benchmark data sets, we clarify several existing inconsistent conclusions and
discuss some new findings."
"In recent years, the management and processing of data streams has become a
topic of active research in several fields of computer science such as,
distributed systems, database systems, and data mining. A data stream can be
thought of as a transient, continuously increasing sequence of data. In data
streams' applications, because of online monitoring, answering to the user's
queries should be time and space efficient. In this paper, we consider the
special requirements of indexing to determine the performance of different
techniques in data stream processing environments. Stream indexing has main
differences with approaches in traditional databases. Also, we compare data
stream indexing models analytically that can provide a suitable method for
stream indexing."
"Halls of Fame are fascinating constructs. They represent the elite of an
often very large amount of entities---persons, companies, products, countries
etc. Beyond their practical use as static rankings, changes to them are
particularly interesting---for decision making processes, as input to common
media or novel narrative science applications, or simply consumed by users. In
this work, we aim at detecting events that can be characterized by changes to a
Hall of Fame ranking in an automated way. We describe how the schema and data
of a database can be used to generate Halls of Fame. In this database scenario,
by Hall of Fame we refer to distinguished tuples; entities, whose
characteristics set them apart from the majority. We define every Hall of Fame
as one specific instance of an SQL query, such that a change in its result is
considered a noteworthy event. Identified changes (i.e., events) are ranked
using lexicographic tradeoffs over event and query properties and presented to
users or fed in higher-level applications. We have implemented a full-fledged
prototype system that uses either database triggers or a Java based middleware
for event identification. We report on an experimental evaluation using a
real-world dataset of basketball statistics."
"Entity resolution is central to data integration and data cleaning.
Algorithmic approaches have been improving in quality, but remain far from
perfect. Crowdsourcing platforms offer a more accurate but expensive (and slow)
way to bring human insight into the process. Previous work has proposed
batching verification tasks for presentation to human workers but even with
batching, a human-only approach is infeasible for data sets of even moderate
size, due to the large numbers of matches to be tested. Instead, we propose a
hybrid human-machine approach in which machines are used to do an initial,
coarse pass over all the data, and people are used to verify only the most
likely matching pairs. We show that for such a hybrid system, generating the
minimum number of verification tasks of a given size is NP-Hard, but we develop
a novel two-tiered heuristic approach for creating batched tasks. We describe
this method, and present the results of extensive experiments on real data sets
using a popular crowdsourcing platform. The experiments show that our hybrid
approach achieves both good efficiency and high accuracy compared to
machine-only or human-only alternatives."
"In the last years there has been a considerable increase in the availability
of continuous sensor measurements in a wide range of application domains, such
as Location-Based Services (LBS), medical monitoring systems, manufacturing
plants and engineering facilities to ensure efficiency, product quality and
safety, hydrologic and geologic observing systems, pollution management, and
others. Due to the inherent imprecision of sensor observations, many
investigations have recently turned into querying, mining and storing uncertain
data. Uncertainty can also be due to data aggregation, privacy-preserving
transforms, and error-prone mining algorithms. In this study, we survey the
techniques that have been proposed specifically for modeling and processing
uncertain time series, an important model for temporal data. We provide an
analytical evaluation of the alternatives that have been proposed in the
literature, highlighting the advantages and disadvantages of each approach, and
further compare these alternatives with two additional techniques that were
carefully studied before. We conduct an extensive experimental evaluation with
17 real datasets, and discuss some surprising results, which suggest that a
fruitful research direction is to take into account the temporal correlations
in the time series. Based on our evaluations, we also provide guidelines useful
for the practitioners in the field."
"We introduce the notion of statistical distortion as an essential metric for
measuring the effectiveness of data cleaning strategies. We use this metric to
propose a widely applicable yet scalable experimental framework for evaluating
data cleaning strategies along three dimensions: glitch improvement,
statistical distortion and cost-related criteria. Existing metrics focus on
glitch improvement and cost, but not on the statistical impact of data cleaning
strategies. We illustrate our framework on real world data, with a
comprehensive suite of experiments and analyses."
"Energy is a growing component of the operational cost for many ""big data""
deployments, and hence has become increasingly important for practitioners of
large-scale data analysis who require scale-out clusters or parallel DBMS
appliances. Although a number of recent studies have investigated the energy
efficiency of DBMSs, none of these studies have looked at the architectural
design space of energy-efficient parallel DBMS clusters. There are many
challenges to increasing the energy efficiency of a DBMS cluster, including
dealing with the inherent scaling inefficiency of parallel data processing, and
choosing the appropriate energy-efficient hardware. In this paper, we
experimentally examine and analyze a number of key parameters related to these
challenges for designing energy-efficient database clusters. We explore the
cluster design space using empirical results and propose a model that considers
the key bottlenecks to energy efficiency in a parallel DBMS. This paper
represents a key first step in designing energy-efficient database clusters,
which is increasingly important given the trend toward parallel database
appliances."
"In keyword search, when user cannot get what she wants, query refinement is
needed and reason can be various. We first give a thorough categorization of
the reason, then focus on solving one category of query refinement problem in
the context of XML keyword search, where what user searches for does not exist
in the data. We refer to it as the MisMatch problem in this paper. Then we
propose a practical way to detect the MisMatch problem and generate helpful
suggestions to users. Our approach can be viewed as a post-processing job of
query evaluation, and has three main features: (1) it adopts both the suggested
queries and their sample results as the output to user, helping user judge
whether the MisMatch problem is solved without consuming all query results; (2)
it is portable in the sense that it can work with any LCA-based matching
semantics and orthogonal to the choice of result retrieval method adopted; (3)
it is lightweight in the way that it occupies a very small proportion of the
whole query evaluation time. Extensive experiments on three real datasets
verify the effectiveness, efficiency and scalability of our approach. An online
XML keyword search engine called XClear that embeds the MisMatch problem
detector and suggester has been built."
"Record linkage has been extensively used in various data mining applications
involving sharing data. While the amount of available data is growing, the
concern of disclosing sensitive information poses the problem of utility vs
privacy. In this paper, we study the problem of private record linkage via
secure data transformations. In contrast to the existing techniques in this
area, we propose a novel approach that provides strong privacy guarantees under
the formal framework of differential privacy. We develop an embedding strategy
based on frequent variable length grams mined in a private way from the
original data. We also introduce personalized threshold for matching individual
records in the embedded space which achieves better linkage accuracy than the
existing global threshold approach. Compared with the state-of-the-art secure
matching schema, our approach provides formal, provable privacy guarantees and
achieves better scalability while providing comparable utility."
"A problem of impedance mismatch between applications written in OO languages
and relational DB is not a problem of discrepancy between object-oriented and
relational approaches themselves. Its real causes can be found in usual
implementation of the OO approach. Direct comparison of the two approaches
cannot be used as a base for the conclusion that they are discrepant or
mismatched. Experimental proof of absence of contradiction between
object-oriented paradigm and relational data model is also presented"
"Recently, result diversification has attracted a lot of attention as a means
to improve the quality of results retrieved by user queries. In this paper, we
propose a new, intuitive definition of diversity called DisC diversity. A DisC
diverse subset of a query result contains objects such that each object in the
result is represented by a similar object in the diverse subset and the objects
in the diverse subset are dissimilar to each other. We show that locating a
minimum DisC diverse subset is an NP-hard problem and provide heuristics for
its approximation. We also propose adapting DisC diverse subsets to a different
degree of diversification. We call this operation zooming. We present efficient
implementations of our algorithms based on the M-tree, a spatial index
structure, and experimentally evaluate their performance."
"Knowledge of the association information between the attributes in a data set
provides insight into the underlying structure of the data and explains the
relationships (independence, synergy, redundancy) between the attributes and
class (if present). Complex models learnt computationally from the data are
more interpretable to a human analyst when such interdependencies are known. In
this paper, we focus on mining two types of association information among the
attributes - correlation information and interaction information for both
supervised (class attribute present) and unsupervised analysis (class attribute
absent). Identifying the statistically significant attribute associations is a
computationally challenging task - the number of possible associations
increases exponentially and many associations contain redundant information
when a number of correlated attributes are present. In this paper, we explore
efficient data mining methods to discover non-redundant attribute sets that
contain significant association information indicating the presence of
informative patterns in the data."
"A large number of web databases are only accessible through proprietary
form-like interfaces which require users to query the system by entering
desired values for a few attributes. A key restriction enforced by such an
interface is the top-k output constraint - i.e., when there are a large number
of matching tuples, only a few (top-k) of them are preferentially selected and
returned by the website, often according to a proprietary ranking function.
Since most web database owners set k to be a small value, the top-k output
constraint prevents many interesting third-party (e.g., mashup) services from
being developed over real-world web databases. In this paper we consider the
novel problem of ""digging deeper"" into such web databases. Our main
contribution is the meta-algorithm GetNext that can retrieve the next ranked
tuple from the hidden web database using only the restrictive interface of a
web database without any prior knowledge of its ranking function. This
algorithm can then be called iteratively to retrieve as many top ranked tuples
as necessary. We develop principled and efficient algorithms that are based on
generating and executing multiple reformulated queries and inferring the next
ranked tuple from their returned results. We provide theoretical analysis of
our algorithms, as well as extensive experimental results over synthetic and
real-world databases that illustrate the effectiveness of our techniques."
"Database systems have to cater to the growing demands of the information age.
The growth of the new age information retrieval powerhouses like search engines
has thrown a challenge to the data management community to come up with novel
mechanisms for feeding information to end users. The burgeoning use of natural
language query interfaces compels system designers to present meaningful and
customised information. Conventional query languages like SQL do not cater to
these requirements due to syntax oriented design. Providing a semantic cover
over these systems was the aim of latent table discovery focusing on
semantically connecting unrelated tables that were not syntactically related by
design and document the discovered knowledge. This paper throws a new direction
towards improving the semantic capabilities of database systems by introducing
a concept driven framework over the latent table discovery method."
"Data collected nowadays by social-networking applications create fascinating
opportunities for building novel services, as well as expanding our
understanding about social structures and their dynamics. Unfortunately,
publishing social-network graphs is considered an ill-advised practice due to
privacy concerns. To alleviate this problem, several anonymization methods have
been proposed, aiming at reducing the risk of a privacy breach on the published
data, while still allowing to analyze them and draw relevant conclusions. In
this paper we introduce a new anonymization approach that is based on injecting
uncertainty in social graphs and publishing the resulting uncertain graphs.
While existing approaches obfuscate graph data by adding or removing edges
entirely, we propose using a finer-grained perturbation that adds or removes
edges partially: this way we can achieve the same desired level of obfuscation
with smaller changes in the data, thus maintaining higher utility. Our
experiments on real-world networks confirm that at the same level of identity
obfuscation our method provides higher usefulness than existing randomized
methods that publish standard graphs."
"MADlib is a free, open source library of in-database analytic methods. It
provides an evolving suite of SQL-based algorithms for machine learning, data
mining and statistics that run at scale within a database engine, with no need
for data import/export to other tools. The goal is for MADlib to eventually
serve a role for scalable database systems that is similar to the CRAN library
for R: a community repository of statistical methods, this time written with
scale and parallelism in mind. In this paper we introduce the MADlib project,
including the background that led to its beginnings, and the motivation for its
open source nature. We provide an overview of the library's architecture and
design patterns, and provide a description of various statistical methods in
that context. We include performance and speedup results of a core design
pattern from one of those methods over the Greenplum parallel DBMS on a
modest-sized test cluster. We then report on two initial efforts at
incorporating academic research into MADlib, which is one of the project's
goals. MADlib is freely available at http://madlib.net, and the project is open
for contributions of both new methods, and ports to additional database
platforms."
"In this new era of ""big data"", traditional DBMSs are under attack from two
sides. At one end of the spectrum, the use of document store NoSQL systems
(e.g. MongoDB) threatens to move modern Web 2.0 applications away from
traditional RDBMSs. At the other end of the spectrum, big data DSS analytics
that used to be the domain of parallel RDBMSs is now under attack by another
class of NoSQL data analytics systems, such as Hive on Hadoop. So, are the
traditional RDBMSs, aka ""big elephants"", doomed as they are challenged from
both ends of this ""big data"" spectrum? In this paper, we compare one
representative NoSQL system from each end of this spectrum with SQL Server, and
analyze the performance and scalability aspects of each of these approaches
(NoSQL vs. SQL) on two workloads (decision support analysis and interactive
data-serving) that represent the two ends of the application spectrum. We
present insights from this evaluation and speculate on potential trends for the
future."
"As the complexity of enterprise systems increases, the need for monitoring
and analyzing such systems also grows. A number of companies have built
sophisticated monitoring tools that go far beyond simple resource utilization
reports. For example, based on instrumentation and specialized APIs, it is now
possible to monitor single method invocations and trace individual transactions
across geographically distributed systems. This high-level of detail enables
more precise forms of analysis and prediction but comes at the price of high
data rates (i.e., big data). To maximize the benefit of data monitoring, the
data has to be stored for an extended period of time for ulterior analysis.
This new wave of big data analytics imposes new challenges especially for the
application performance monitoring systems. The monitoring data has to be
stored in a system that can sustain the high data rates and at the same time
enable an up-to-date view of the underlying infrastructure. With the advent of
modern key-value stores, a variety of data storage systems have emerged that
are built with a focus on scalability and high data rates as predominant in
this monitoring use case. In this work, we present our experience and a
comprehensive performance evaluation of six modern (open-source) data stores in
the context of application performance monitoring as part of CA Technologies
initiative. We evaluated these systems with data and workloads that can be
found in application performance monitoring, as well as, on-line advertisement,
power monitoring, and many other use cases. We present our insights not only as
performance results but also as lessons learned and our experience relating to
the setup and configuration complexity of these data stores in an industry
setting."
"Main Memory Map Reduce (M3R) is a new implementation of the Hadoop Map Reduce
(HMR) API targeted at online analytics on high mean-time-to-failure clusters.
It does not support resilience, and supports only those workloads which can fit
into cluster memory. In return, it can run HMR jobs unchanged -- including jobs
produced by compilers for higher-level languages such as Pig, Jaql, and
SystemML and interactive front-ends like IBM BigSheets -- while providing
significantly better performance than the Hadoop engine on several workloads
(e.g. 45x on some input sizes for sparse matrix vector multiply). M3R also
supports extensions to the HMR API which can enable Map Reduce jobs to run
faster on the M3R engine, while not affecting their performance under the
Hadoop engine."
"With the SAP HANA database, SAP offers a high-performance in-memory
hybrid-store database. Hybrid-store databases---that is, databases supporting
row- and column-oriented data management---are getting more and more prominent.
While the columnar management offers high-performance capabilities for
analyzing large quantities of data, the row-oriented store can handle
transactional point queries as well as inserts and updates more efficiently. To
effectively take advantage of both stores at the same time the novel question
whether to store the given data row- or column-oriented arises. We tackle this
problem with a storage advisor tool that supports database administrators at
this decision. Our proposed storage advisor recommends the optimal store based
on data and query characteristics; its core is a cost model to estimate and
compare query execution times for the different stores. Besides a per-table
decision, our tool also considers to horizontally and vertically partition the
data and manage the partitions on different stores. We evaluated the storage
advisor for the use in the SAP HANA database; we show the recommendation
quality as well as the benefit of having the data in the optimal store with
respect to increased query performance."
"In analytical applications, database systems often need to sustain workloads
with multiple concurrent scans hitting the same table. The Cooperative Scans
(CScans) framework, which introduces an Active Buffer Manager (ABM) component
into the database architecture, has been the most effective and elaborate
response to this problem, and was initially developed in the X100 research
prototype. We now report on the the experiences of integrating Cooperative
Scans into its industrial-strength successor, the Vectorwise database product.
During this implementation we invented a simpler optimization of concurrent
scan buffer management, called Predictive Buffer Management (PBM). PBM is based
on the observation that in a workload with long-running scans, the buffer
manager has quite a bit of information on the workload in the immediate future,
such that an approximation of the ideal OPT algorithm becomes feasible. In the
evaluation on both synthetic benchmarks as well as a TPC-H throughput run we
compare the benefits of naive buffer management (LRU) versus CScans, PBM and
OPT; showing that PBM achieves benefits close to Cooperative Scans, while
incurring much lower architectural impact."
"In recent years, there has been a substantial amount of work on large-scale
data analytics using Hadoop-based platforms running on large clusters of
commodity machines. A less-explored topic is how those data, dominated by
application logs, are collected and structured to begin with. In this paper, we
present Twitter's production logging infrastructure and its evolution from
application-specific logging to a unified ""client events"" log format, where
messages are captured in common, well-formatted, flexible Thrift messages.
Since most analytics tasks consider the user session as the basic unit of
analysis, we pre-materialize ""session sequences"", which are compact summaries
that can answer a large class of common queries quickly. The development of
this infrastructure has streamlined log collection and data analysis, thereby
improving our ability to rapidly experiment and iterate on various aspects of
the service."
"Database backups have traditionally been used as the primary mechanism to
recover from hardware and user errors. High availability solutions maintain
redundant copies of data that can be used to recover from most failures except
user or application errors. Database backups are neither space nor time
efficient for recovering from user errors which typically occur in the recent
past and affect a small portion of the database. Moreover periodic full backups
impact user workload and increase storage costs. In this paper we present a
scheme that can be used for both user and application error recovery starting
from the current state and rewinding the database back in time using the
transaction log. While we provide a consistent view of the entire database as
of a point in time in the past, the actual prior versions are produced only for
data that is accessed. We make the as of data accessible to arbitrary point in
time queries by integrating with the database snapshot feature in Microsoft SQL
Server."
"This paper describes the system architecture of the Vertica Analytic Database
(Vertica), a commercialization of the design of the C-Store research prototype.
Vertica demonstrates a modern commercial RDBMS system that presents a classical
relational interface while at the same time achieving the high performance
expected from modern ""web scale"" analytic systems by making appropriate
architectural choices. Vertica is also an instructive lesson in how academic
systems research can be directly commercialized into a successful product."
"Within the past few years, organizations in diverse industries have adopted
MapReduce-based systems for large-scale data processing. Along with these new
users, important new workloads have emerged which feature many small, short,
and increasingly interactive jobs in addition to the large, long-running batch
jobs for which MapReduce was originally designed. As interactive, large-scale
query processing is a strength of the RDBMS community, it is important that
lessons from that field be carried over and applied where possible in this new
domain. However, these new workloads have not yet been described in the
literature. We fill this gap with an empirical analysis of MapReduce traces
from six separate business-critical deployments inside Facebook and at Cloudera
customers in e-commerce, telecommunications, media, and retail. Our key
contribution is a characterization of new MapReduce workloads which are driven
in part by interactive analysis, and which make heavy use of query-like
programming frameworks on top of MapReduce. These workloads display diverse
behaviors which invalidate prior assumptions about MapReduce such as uniform
data access, regular diurnal patterns, and prevalence of large jobs. A
secondary contribution is a first step towards creating a TPC-like data
processing benchmark for MapReduce."
"MapReduce has emerged as a popular method to process big data. In the past
few years, however, not just big data, but fast data has also exploded in
volume and availability. Examples of such data include sensor data streams, the
Twitter Firehose, and Facebook updates. Numerous applications must process fast
data. Can we provide a MapReduce-style framework so that developers can quickly
write such applications and execute them over a cluster of machines, to achieve
low latency and high scalability? In this paper we report on our investigation
of this question, as carried out at Kosmix and WalmartLabs. We describe
MapUpdate, a framework like MapReduce, but specifically developed for fast
data. We describe Muppet, our implementation of MapUpdate. Throughout the
description we highlight the key challenges, argue why MapReduce is not well
suited to address them, and briefly describe our current solutions. Finally, we
describe our experience and lessons learned with Muppet, which has been used
extensively at Kosmix and WalmartLabs to power a broad range of applications in
social media and e-commerce."
"Stream processing applications are deployed as continuous queries that run
from the time of their submission until their cancellation. This deployment
mode limits developers who need their applications to perform runtime
adaptation, such as algorithmic adjustments, incremental job deployment, and
application-specific failure recovery. Currently, developers do runtime
adaptation by using external scripts and/or by inserting operators into the
stream processing graph that are unrelated to the data processing logic. In
this paper, we describe a component called orchestrator that allows users to
write routines for automatically adapting the application to runtime
conditions. Developers build an orchestrator by registering and handling events
as well as specifying actuations. Events can be generated due to changes in the
system state (e.g., application component failures), built-in system metrics
(e.g., throughput of a connection), or custom application metrics (e.g.,
quality score). Once the orchestrator receives an event, users can take
adaptation actions by using the orchestrator actuation APIs. We demonstrate the
use of the orchestrator in IBM's System S in the context of three different
applications, illustrating application adaptation to changes on the incoming
data distribution, to application failures, and on-demand dynamic composition."
"Location-Based Service (LBS) is rapidly becoming the next ubiquitous
technology for a wide range of mobile applications. To support applications
that demand nearest-neighbor and history queries, an LBS spatial indexer must
be able to efficiently update, query, archive and mine location records, which
can be in contention with each other. In this work, we propose MOIST, whose
baseline is a recursive spatial partitioning indexer built upon BigTable. To
reduce update and query contention, MOIST groups nearby objects of similar
trajectory into the same school, and keeps track of only the history of school
leaders. This dynamic clustering scheme can eliminate redundant updates and
hence reduce update latency. To improve history query processing, MOIST keeps
some history data in memory, while it flushes aged data onto parallel disks in
a locality-preserving way. Through experimental studies, we show that MOIST can
support highly efficient nearest-neighbor and history queries and can scale
well with an increasing number of users and update frequency."
"This paper describes our experience implementing PostgreSQL's new
serializable isolation level. It is based on the recently-developed
Serializable Snapshot Isolation (SSI) technique. This is the first
implementation of SSI in a production database release as well as the first in
a database that did not previously have a lock-based serializable isolation
level. We reflect on our experience and describe how we overcame some of the
resulting challenges, including the implementation of a new lock manager, a
technique for ensuring memory usage is bounded, and integration with other
PostgreSQL features. We also introduce an extension to SSI that improves
performance for read-only transactions. We evaluate PostgreSQL's serializable
isolation level using several benchmarks and show that it achieves performance
only slightly below that of snapshot isolation, and significantly outperforms
the traditional two-phase locking approach on read-intensive workloads."
"Recently, parallel search engines have been implemented based on scalable
distributed file systems such as Google File System. However, we claim that
building a massively-parallel search engine using a parallel DBMS can be an
attractive alternative since it supports a higher-level (i.e., SQL-level)
interface than that of a distributed file system for easy and less error-prone
application development while providing scalability. In this paper, we propose
a new approach of building a massively-parallel search engine using a DB-IR
tightly-integrated parallel DBMS and demonstrate its commercial-level
scalability and performance. In addition, we present a hybrid (i.e., analytic
and experimental) performance model for the parallel search engine. We have
built a five-node parallel search engine according to the proposed architecture
using a DB-IR tightly-integrated DBMS. Through extensive experiments, we show
the correctness of the model by comparing the projected output with the
experimental results of the five-node engine. Our model demonstrates that ODYS
is capable of handling 1 billion queries per day (81 queries/sec) for 30
billion web pages by using only 43,472 nodes with an average query response
time of 211 ms, which is equivalent to or better than those of commercial
search engines. We also show that, by using twice as many (86,944) nodes, ODYS
can provide an average query response time of 162 ms, which is significantly
lower than those of commercial search engines."
"The proportionate increase in the size of the data with increase in space
implies that clustering a very large data set becomes difficult and is a time
consuming process.Sampling is one important technique to scale down the size of
dataset and to improve the efficiency of clustering. After sampling allocating
unlabeled objects into proper clusters is impossible in the categorical
domain.To address the problem, Chen employed a method called MAximal
Representative Data Labeling to allocate each unlabeled data point to the
appropriate cluster based on Node Importance Representative and N-Node
Importance Representative algorithms. This paper took off from Chen s
investigation and analyzed and compared the results of NIR and NNIR leading to
the conclusion that the two processes contradict each other when it comes to
finding the resemblance between an unlabeled data point and a cluster.A new and
better way of solving the problem was arrived at that finds resemblance between
unlabeled data point within all clusters, while also providing maximal
resemblance for allocation of data in the required cluster."
"Statistical privacy views privacy definitions as contracts that guide the
behavior of algorithms that take in sensitive data and produce sanitized data.
For most existing privacy definitions, it is not clear what they actually
guarantee.
  In this paper, we propose the first (to the best of our knowledge) framework
for extracting semantic guarantees from privacy definitions. That is, instead
of answering narrow questions such as ""does privacy definition Y protect X?""
the goal is to answer the more general question ""what does privacy definition Y
protect?""
  The privacy guarantees we can extract are Bayesian in nature and deal with
changes in an attacker's beliefs. The key to our framework is an object we call
the row cone. Every privacy definition has a row cone, which is a convex set
that describes all the ways an attacker's prior beliefs can be turned into
posterior beliefs after observing an output of an algorithm satisfying that
privacy definition.
  The framework can be applied to privacy definitions or even to individual
algorithms to identify the types of inferences they defend against. We
illustrate the use of our framework with analyses of several definitions and
algorithms for which we can derive previously unknown semantics. These include
randomized response, FRAPP, and several algorithms that add integer-valued
noise to their inputs."
"As the information available to lay users through autonomous data sources
continues to increase, mediators become important to ensure that the wealth of
information available is tapped effectively. A key challenge that these
information mediators need to handle is the varying levels of incompleteness in
the underlying databases in terms of missing attribute values. Existing
approaches such as QPIAD aim to mine and use Approximate Functional
Dependencies (AFDs) to predict and retrieve relevant incomplete tuples. These
approaches make independence assumptions about missing values---which
critically hobbles their performance when there are tuples containing missing
values for multiple correlated attributes. In this paper, we present a
principled probabilistic alternative that views an incomplete tuple as defining
a distribution over the complete tuples that it stands for. We learn this
distribution in terms of Bayes networks. Our approach involves
mining/""learning"" Bayes networks from a sample of the database, and using it to
do both imputation (predict a missing value) and query rewriting (retrieve
relevant results with incompleteness on the query-constrained attributes, when
the data sources are autonomous). We present empirical studies to demonstrate
that (i) at higher levels of incompleteness, when multiple attribute values are
missing, Bayes networks do provide a significantly higher classification
accuracy and (ii) the relevant possible answers retrieved by the queries
reformulated using Bayes networks provide higher precision and recall than AFDs
while keeping query processing costs manageable."
"In this paper we introduce and experimentally compare alternative algorithms
to join uncertain relations. Different algorithms are based on specific
principles, e.g., sorting, indexing, or building intermediate relational tables
to apply traditional approaches. As a consequence their performance is affected
by different features of the input data, and each algorithm is shown to be more
efficient than the others in specific cases. In this way statistics explicitly
representing the amount and kind of uncertainty in the input uncertain
relations can be used to choose the most efficient algorithm."
"Views on RDF datasets have been discussed in several works, nevertheless
there is no consensus on their definition nor the requirements they should
fulfill. In traditional data management systems, views have proved to be useful
in different application scenarios such as data integration, query answering,
data security, and query modularization.
  In this work we have reviewed existent work on views over RDF datasets, and
discussed the application of existent view definition mechanisms to four
scenarios in which views have proved to be useful in traditional (relational)
data management systems. To give a framework for the discussion we provided a
definition of views over RDF datasets, an issue over which there is no
consensus so far. We finally chose the three proposals closer to this
definition, and analyzed them with respect to four selected goals."
"Nowadays, information management systems deal with data originating from
different sources including relational databases, NoSQL data stores, and Web
data formats, varying not only in terms of data formats, but also in the
underlying data model. Integrating data from heterogeneous data sources is a
time-consuming and error-prone engineering task; part of this process requires
that the data has to be transformed from its original form to other forms,
repeating all along the life cycle. With this report we provide a principled
overview on the fundamental data shapes tabular, tree, and graph as well as
transformations between them, in order to gain a better understanding for
performing said transformations more efficiently and effectively."
"In today's world, healthcare is the most important factor affecting human
life. Due to heavy work load it is not possible for personal healthcare. The
proposed system acts as a preventive measure for determining whether a person
is fit or unfit based on person's historical and real time data by applying
clustering algorithms like K-means and D-stream. The Density-based clustering
algorithm i.e. the D-stream algorithm overcomes drawbacks of K-Means algorithm.
By calculating their performance measures we finally find out effectiveness and
efficiency of both the algorithms. Both clustering algorithms are applied on
patient's bio-medical historical database. To check the correctness of both the
algorithms, we apply them on patient's current bio-medical data."
"We study the problem of computing shortest path or distance between two query
vertices in a graph, which has numerous important applications. Quite a number
of indexes have been proposed to answer such distance queries. However, all of
these indexes can only process graphs of size barely up to 1 million vertices,
which is rather small in view of many of the fast-growing real-world graphs
today such as social networks and Web graphs. We propose an efficient index,
which is a novel labeling scheme based on the independent set of a graph. We
show that our method can handle graphs of size three orders of magnitude larger
than those existing indexes."
"In this paper, we revisit the view update problem in a relational setting and
propose a framework based on the notion of determinacy under constraints.
Within such a framework, we characterise when a view mapping is invertible,
establishing that this is the case precisely when each database symbol has an
exact rewriting in terms of the view symbols under the given constraints, and
we provide a general effective criterion to understand whether the changes
introduced by a view update can be propagated to the underlying database
relations in a unique and unambiguous way.
  Afterwards, we show how determinacy under constraints can be checked, and
rewritings effectively found, in three different relevant scenarios in the
absence of view constraints. First, we settle the long-standing open issue of
how to solve the view update problem in a multi-relational database with views
that are projections of joins of relations, and we do so in a more general
setting where views are defined by arbitrary conjunctive queries and database
constraints are stratified embedded dependencies. Next, we study a setting
based on horizontal decompositions of a single database relation, where views
are defined by selections on possibly interpreted attributes (e.g., arithmetic
comparisons) in the presence of domain constraints over the database schema.
Lastly, we look into another multi-relational database setting, where views are
defined in an expressive ""Type"" Relational Algebra based on the n-ary
Description Logic DLR and database constraints are inclusions of expressions in
that algebra."
"The multi relational data mining approach has developed as an alternative way
for handling the structured data such that RDBMS. This will provides the mining
in multiple tables directly. In MRDM the patterns are available in multiple
tables (relations) from a relational database. As the data are available over
the many tables which will affect the many problems in the practice of the data
mining. To deal with this problem, one either constructs a single table by
Propositionalisation, or uses a Multi-Relational Data Mining algorithm. MRDM
approaches have been successfully applied in the area of bioinformatics. Three
popular pattern finding techniques classification, clustering and association
are frequently used in MRDM. Multi relational approach has developed as an
alternative for analyzing the structured data such as relational database. MRDM
allowing applying directly in the data mining in multiple tables. To avoid the
expensive joining operations and semantic losses we used the MRDM technique.
This paper focuses some of the application areas of MRDM and feature directions
as well as the comparison of ILP, GM, SSDM and MRDM"
"This paper presents architecture for health care data warehouse specific to
cancer diseases which could be used by executive managers, doctors, physicians
and other health professionals to support the healthcare process. The data
today existing in multi-sources with different formats makes it necessary to
have some techniques for data integration. Executive managers need access to
Information so that decision makers can react in real time to changing needs.
Information is one of the most factors to an organization success that
executive managers or physicians would need to base their decisions on, during
decision making. A health care data warehouse is therefore necessary to
integrate the different data sources into a central data repository and
analysis this data."
"Provenance refers to the documentation of an object's lifecycle. This
documentation (often represented as a graph) should include all the information
necessary to reproduce a certain piece of data or the process that led to it.
In a dynamic world, as data changes, it is important to be able to get a piece
of data as it was, and its provenance graph, at a certain point in time.
Supporting time-aware provenance querying is challenging and requires: (i)
explicitly representing the time information in the provenance graphs, and (ii)
providing abstractions and efficient mechanisms for time-aware querying of
provenance graphs over an ever growing volume of data. The existing provenance
models treat time as a second class citizen (i.e. as an optional annotation).
This makes time-aware querying of provenance data inefficient and sometimes
inaccessible. We introduce an extended provenance graph model to explicitly
represent time as an additional dimension of provenance data. We also provide a
query language, novel abstractions and efficient mechanisms to query and
analyze timed provenance graphs. The main contributions of the paper include:
(i) proposing a Temporal Provenance Model (TPM) as a timed provenance model;
and (ii) introducing two concepts of timed folder, as a container of related
set of objects and their provenance relationship over time, and timed paths, to
represent the evolution of objects tracing information over time, for analyzing
and querying TPM graphs. We have implemented the approach on top of FPSPARQL, a
query engine for large graphs, and have evaluated for querying TPM models. The
evaluation shows the viability and efficiency of our approach."
"The popularity of the Mobile Database is increasing day by day as people need
information even on the move in the fast changing world. This database
technology permits employees using mobile devices to connect to their corporate
networks, hoard the needed data, work in the disconnected mode and reconnect to
the network to synchronize with the corporate database. In this scenario, the
data is being moved closer to the applications in order to improve the
performance and autonomy. This leads to many interesting problems in mobile
database research and Mobile Database has become a fertile land for many
researchers. In this paper a survey is presented on data and Transaction
management in Mobile Databases from the year 2000 onwards. The survey focuses
on the complete study on the various types of Architectures used in Mobile
databases and Mobile Transaction Models. It also addresses the data management
issues namely Replication and Caching strategies and the transaction management
functionalities such as Concurrency Control and Commit protocols,
Synchronization, Query Processing, Recovery and Security. It also provides
Research Directions in Mobile databases."
"This paper presents the method of mining the data and which contains the
information about the large information about the PR (Panchayat Raj
Department)of Orissa.We have focused some of the techniques,approaches and
different methodologies of the demand forecasting. Every organizations are
operated in different places of the country. Each place of operation may
generate a huge amount of data. In an organization, worker prediction is the
difficult task of the manager. It is the complex process not only because its
nature of feature prediction but also various approaches methodologies always
makes user confused. This paper aims to deal with the problem selection
process. In this paper we have used some of the approaches from literature are
been introduced and analyzed to find its suitable organization and situation.
Based on this we have designed with automatic selection function to help users
make a prejudgment. This information about each approach will be showed to
users with examples to help understanding. This system also provides
calculation function to help users work out a predication result. Generally the
new developed system has a more comprehensive functions compared with existing
ones. It aims to improve the accuracy of demand forecasting by implementing the
forecasting algorithm. While it is still a decision support system with no
ability of make the final judgment.This type of huge amount of data are are
available in the form of different ways which has drastically changed in the
areas of science and engineering.To analyze, manage and make a decision of such
type of huge amount of data we need techniques called the data mining which
will transforming in many fields. We have implemented the algorithms in JAVA
technology. This paper provides the prediction algorithm Linear Regression,
result which will helpful in the further research."
"The ability to efficiently find relevant subgraphs and paths in a large graph
to a given query is important in many applications including scientific data
analysis, social networks, and business intelligence. Currently, there is
little support and no efficient approaches for expressing and executing such
queries. This paper proposes a data model and a query language to address this
problem. The contributions include supporting the construction and selection
of: (i) folder nodes, representing a set of related entities, and (ii) path
nodes, representing a set of paths in which a path is the transitive
relationship of two or more entities in the graph. Folders and paths can be
stored and used for future queries. We introduce FPSPARQL which is an extension
of the SPARQL supporting folder and path nodes. We have implemented a query
engine that supports FPSPARQL and the evaluation results shows its viability
and efficiency for querying large graph datasets."
"Shark is a new data analysis system that marries query processing with
complex analytics on large clusters. It leverages a novel distributed memory
abstraction to provide a unified engine that can run SQL queries and
sophisticated analytics functions (e.g., iterative machine learning) at scale,
and efficiently recovers from failures mid-query. This allows Shark to run SQL
queries up to 100x faster than Apache Hive, and machine learning programs up to
100x faster than Hadoop. Unlike previous systems, Shark shows that it is
possible to achieve these speedups while retaining a MapReduce-like execution
engine, and the fine-grained fault tolerance properties that such engines
provide. It extends such an engine in several ways, including column-oriented
in-memory storage and dynamic mid-query replanning, to effectively execute SQL.
The result is a system that matches the speedups reported for MPP analytic
databases over MapReduce, while offering fault tolerance properties and complex
analytics capabilities that they lack."
"Data integration is one of the main problems in distributed data sources. An
approach is to provide an integrated mediated schema for various data sources.
This research work aims at developing a framework for defining an integrated
schema and querying on it. The basic idea is to employ recent standard
languages and tools to provide a unified data integration framework. RDF is
used for integrated schema descriptions as well as providing a unified view of
data. RDQL is used for query reformulation. Furthermore, description logic
inference services provide necessary means for satisfiability checking of
concepts in integrated schema. The framework has tools to display integrated
schema, query on it, and provides enough flexibilities to be used in different
application domains."
"Digital world is growing very fast and become more complex in the volume
(terabyte to petabyte), variety (structured and un-structured and hybrid),
velocity (high speed in growth) in nature. This refers to as Big Data that is a
global phenomenon. This is typically considered to be a data collection that
has grown so large it can not be effectively managed or exploited using
conventional data management tools: e.g., classic relational database
management systems (RDBMS) or conventional search engines. To handle this
problem, traditional RDBMS are complemented by specifically designed a rich set
of alternative DBMS; such as - NoSQL, NewSQL and Search-based systems. This
paper motivation is to provide - classification, characteristics and evaluation
of NoSQL databases in Big Data Analytics. This report is intended to help
users, especially to the organizations to obtain an independent understanding
of the strengths and weaknesses of various NoSQL database approaches to
supporting applications that process huge volumes of data."
"As of 2005, sampling has been incorporated in all major database systems.
While efficient sampling techniques are realizable, determining the accuracy of
an estimate obtained from the sample is still an unresolved problem. In this
paper, we present a theoretical framework that allows an elegant treatment of
the problem. We base our work on generalized uniform sampling (GUS), a class of
sampling methods that subsumes a wide variety of sampling techniques. We
introduce a key notion of equivalence that allows GUS sampling operators to
commute with selection and join, and derivation of confidence intervals. We
illustrate the theory through extensive examples and give indications on how to
use it to provide meaningful estimations in database systems."
"Existence of incomplete and imprecise data has moved the database paradigm
from deterministic to proba- babilistic information. Probabilistic databases
contain tuples that may or may not exist with some probability. As a result,
the number of possible deterministic database instances that can be observed
from a probabilistic database grows exponentially with the number of
probabilistic tuples. In this paper, we consider the problem of answering both
aggregate and non-aggregate queries on massive probabilistic databases. We
adopt the tuple independence model, in which each tuple is assigned a
probability value. We develop a method that exploits Probability Generating
Functions (PGF) to answer such queries efficiently. Our method maintains a
polynomial for each tuple. It incrementally builds a master polynomial that
expresses the distribution of the possible result values precisely. We also
develop an approximation method that finds the distribution of the result value
with negligible errors. Our experiments suggest that our methods are orders of
magnitude faster than the most recent systems that answer such queries,
including MayBMS and SPROUT. In our experiments, we were able to scale up to
several terabytes of data on TPC- H queries, while existing methods could only
run for a few gigabytes of data on the same queries."
"This paper introduces a new method for the session construction problem,
which is the first main step of the web usage mining process. Through
experiments, it is shown that when our new technique is used, it outperforms
previous approaches in web usage mining applications such as next-page
prediction."
"With the development of decision systems and specially data warehouses, the
visibility of the data warehouse design before its creation has become
essential, and that because of data warehouse importance as considered as the
unique data source giving meaning to the decision. In a decision system the
proper functioning of a data warehouse resides in the smooth running of the
middleware tools ETC step one hand, and the restitution step through the data
mining, reporting solutions, dashboards... etc other. The large volume of data
that passes through these stages require an optimal design for a highly
efficient decision system, without disregarding the choice of technologies that
are introduced for the data warehouse implementation such as: database
management system, the type of server operating systems, physical server
architecture (64-bit, for example) that can be a benefit performance of this
system. The designer of the data warehouse should consider the effectiveness of
data query, this depends on the selection of relevant indexes and their
combination with the materialized views, note that the index selection is a
NPcomplete problem, because the number of indexes is exponential in the total
number of attributes in the database, So, it is necessary to provide, while the
data warehouse design, the suitable type of index for this data warehouse. This
paper presents a comparative study between the index B-tree type and type
Bitmap, their advantages and disadvantages, with a real experiment showing that
its index of type Bitmap more advantageous than the index B-tree type."
"The World Wide Web infrastructure together with its more than 2 billion users
enables to store information at a rate that has never been achieved before.
This is mainly due to the will of storing almost all end-user interactions
performed on some web applications. In order to reply to scalability and
availability constraints, many web companies involved in this process recently
started to design their own data management systems. Many of them are referred
to as NOSQL databases, standing for 'Not only SQL'. With their wide adoption
emerges new needs and data integration is one of them. In this paper, we
consider that an ontology-based representation of the information stored in a
set of NOSQL sources is highly needed. The main motivation of this approach is
the ability to reason on elements of the ontology and to retrieve information
in an efficient and distributed manner. Our contributions are the following:
(1) we analyze a set of schemaless NOSQL databases to generate local
ontologies, (2) we generate a global ontology based on the discovery of
correspondences between the local ontologies and finally (3) we propose a query
translation solution from SPARQL to query languages of the sources. We are
currently implementing our data integration solution on two popular NOSQL
databases: MongoDB as a document database and Cassandra as a column family
store."
"In recent years, due to the wide applications of uncertain data (e.g., noisy
data), uncertain frequent itemsets (UFI) mining over uncertain databases has
attracted much attention, which differs from the corresponding deterministic
problem from the generalized definition and resolutions. As the most costly
task in association rule mining process, it has been shown that outsourcing
this task to a service provider (e.g.,the third cloud party) brings several
benefits to the data owner such as cost relief and a less commitment to storage
and computational resources. However, the correctness integrity of mining
results can be corrupted if the service provider is with random fault or not
honest (e.g., lazy, malicious, etc). Therefore, in this paper, we focus on the
integrity and verification issue in UFI mining problem during outsourcing
process, i.e., how the data owner verifies the mining results. Specifically, we
explore and extend the existing work on deterministic FI outsourcing
verification to uncertain scenario. For this purpose, We extend the existing
outsourcing FI mining work to uncertain area w.r.t. the two popular UFI
definition criteria and the approximate UFI mining methods. Specifically, We
construct and improve the basic/enhanced verification scheme with such
different UFI definition respectively. After that, we further discuss the
scenario of existing approximation UFP mining, where we can see that our
technique can provide good probabilistic guarantees about the correctness of
the verification. Finally, we present the comparisons and analysis on the
schemes proposed in this paper."
"In the last decade, Moving Object Databases (MODs) have attracted a lot of
attention from researchers. Several research works were conducted to extend
traditional database techniques to accommodate the new requirements imposed by
the continuous change in location information of moving objects. Managing,
querying, storing, and mining moving objects were the key research directions.
This extensive interest in moving objects is a natural consequence of the
recent ubiquitous location-aware devices, such as PDAs, mobile phones, etc., as
well as the variety of information that can be extracted from such new
databases. In this paper we propose a Spatio-Temporal data warehousing (STDW)
for efficiently querying location information of moving objects. The proposed
schema introduces new measures like direction majority and other
direction-based measures that enhance the decision making based on location
information."
"This paper describes the technology of data warehouse in healthcare
decision-making and tools for support of these technologies, which is used to
cancer diseases. The healthcare executive managers and doctors needs
information about and insight into the existing health data, so as to make
decision more efficiently without interrupting the daily work of an On-Line
Transaction Processing (OLTP) system. This is a complex problem during the
healthcare decision-making process. To solve this problem, the building a
healthcare data warehouse seems to be efficient. First in this paper we explain
the concepts of the data warehouse, On-Line Analysis Processing (OLAP).
Changing the data in the data warehouse into a multidimensional data cube is
then shown. Finally, an application example is given to illustrate the use of
the healthcare data warehouse specific to cancer diseases developed in this
study. The executive managers and doctors can view data from more than one
perspective with reduced query time, thus making decisions faster and more
comprehensive."
"Although the intention of RDF is to provide an open, minimally constraining
way for representing information, there exists an increasing number of
applications for which guarantees on the structure and values of an RDF data
set become desirable if not essential. What is missing in this respect are
mechanisms to tie RDF data to quality guarantees akin to schemata of relational
databases, or DTDs in XML, in particular when translating legacy data coming
with a rich set of integrity constraints - like keys or cardinality
restrictions - into RDF. Addressing this shortcoming, we present the RDF Data
Description language (RDD), which makes it possible to specify instance-level
data constraints over RDF. Making such constraints explicit does not only help
in asserting and maintaining data quality, but also opens up new optimization
opportunities for query engines and, most importantly, makes query formulation
a lot easier for users and system developers. We present design goals, syntax,
and a formal, First-order logics based semantics of RDDs and discuss the impact
on consuming Linked Data."
"This paper presents the evaluation of the architecture of healthcare data
warehouse specific to cancer diseases. This data warehouse containing relevant
cancer medical information and patient data. The data warehouse provides the
source for all current and historical health data to help executive manager and
doctors to improve the decision making process for cancer patients. The
evaluation model based on Bill Inmon's definition of data warehouse is proposed
to evaluate the Cancer data warehouse."
"A novel approach for creating ER conceptual models and an algorithm for
transforming them to the relational model has been developed by modifying and
extending the existing methods. A part of the new algorithm has previously been
presented. This paper presents the rest of the algorithm. One of the objectives
of this paper is to use it as a supportive document for ongoing empirical
evaluations of the new approach being conducted using the cognitive engagement
method and with the participation of different segments of the field as
respondents."
"Data mining environment produces a large amount of data, that need to be
analyzed, patterns have to be extracted from that to gain knowledge. In this
new era with boom of data both structured and unstructured, in the field of
genomics, meteorology, biology, environmental research and many others, it has
become difficult to process, manage and analyze patterns using traditional
databases and architectures. So, a proper architecture should be understood to
gain knowledge about the Big Data. This paper presents a review of various
algorithms from 1994-2013 necessary for handling such large data set. These
algorithms define various structures and methods implemented to handle Big
Data, also in the paper are listed various tool that were developed for
analyzing them."
"We consider unordered XML, where the relative order among siblings is
ignored, and we investigate the problem of learning schemas from examples given
by the user. We focus on the schema formalisms proposed in [10]: disjunctive
multiplicity schemas (DMS) and its restriction, disjunction-free multiplicity
schemas (MS). A learning algorithm takes as input a set of XML documents which
must satisfy the schema (i.e., positive examples) and a set of XML documents
which must not satisfy the schema (i.e., negative examples), and returns a
schema consistent with the examples. We investigate a learning framework
inspired by Gold [18], where a learning algorithm should be sound i.e., always
return a schema consistent with the examples given by the user, and complete
i.e., able to produce every schema with a sufficiently rich set of examples.
Additionally, the algorithm should be efficient i.e., polynomial in the size of
the input. We prove that the DMS are learnable from positive examples only, but
they are not learnable when we also allow negative examples. Moreover, we show
that the MS are learnable in the presence of positive examples only, and also
in the presence of both positive and negative examples. Furthermore, for the
learnable cases, the proposed learning algorithms return minimal schemas
consistent with the examples."
"With the proliferation of the data warehouses as supportive decision making
tools, organizations are increasingly looking forward for a complete data
warehouse success model that would manage the enormous amounts of growing data.
It is therefore important to measure the success of these massive projects.
While general IS success models have received great deals of attention, few
research has been conducted to assess the success of data warehouses for
strategic business intelligence purposes. The framework developed in this study
consists of the following nine measures: Vendors and Consultants, Management
Actions, System Quality, Information Quality, Data Warehouse Usage, Perceived
utility, Individual Decision Making Impact, Organizational Decision Making
Impact, and Corporate Strategic Goals Attainment."
"With the emergence of graph databases, the task of frequent subgraph
discovery has been extensively addressed. Although the proposed approaches in
the literature have made this task feasible, the number of discovered frequent
subgraphs is still very high to be efficiently used in any further exploration.
Feature selection for graph data is a way to reduce the high number of frequent
subgraphs based on exact or approximate structural similarity. However, current
structural similarity strategies are not efficient enough in many real-world
applications, besides, the combinatorial nature of graphs makes it
computationally very costly. In order to select a smaller yet structurally
irredundant set of subgraphs, we propose a novel approach that mines the top-k
topological representative subgraphs among the frequent ones. Our approach
allows detecting hidden structural similarities that existing approaches are
unable to detect such as the density or the diameter of the subgraph. In
addition, it can be easily extended using any user defined structural or
topological attributes depending on the sought properties. Empirical studies on
real and synthetic graph datasets show that our approach is fast and scalable."
"We survey recent work on the specification of an access control mechanism in
a collaborative environment. The work is presented in the context of the
WebdamLog language, an extension of datalog to a distributed context. We
discuss a fine-grained access control mechanism for intentional data based on
provenance as well as a control mechanism for delegation, i.e., for deploying
rules at remote peers."
"This paper introduces ENFrame, a unified data processing platform for
querying and mining probabilistic data. Using ENFrame, users can write programs
in a fragment of Python with constructs such as bounded-range loops, list
comprehension, aggregate operations on lists, and calls to external database
engines. The program is then interpreted probabilistically by ENFrame.
  The realisation of ENFrame required novel contributions along several
directions. We propose an event language that is expressive enough to
succinctly encode arbitrary correlations, trace the computation of user
programs, and allow for computation of discrete probability distributions of
program variables. We exemplify ENFrame on three clustering algorithms:
k-means, k-medoids, and Markov Clustering. We introduce sequential and
distributed algorithms for computing the probability of interconnected events
exactly or approximately with error guarantees. Experiments with k-medoids
clustering of sensor readings from energy networks show orders-of-magnitude
improvements of exact clustering using ENFrame over na\""ive clustering in each
possible world, of approximate over exact, and of distributed over sequential
algorithms."
"A common approach to scaling transactional databases in practice is
horizontal partitioning, which increases system scalability, high availability
and self-manageability. Usu- ally it is very challenging to choose or design an
optimal partitioning scheme for a given workload and database. In this
technical report, we propose a fine-grained hyper-graph based database
partitioning system for transactional work- loads. The partitioning system
takes a database, a workload, a node cluster and partitioning constraints as
input and out- puts a lookup-table encoding the final database partitioning
decision. The database partitioning problem is modeled as a multi-constraints
hyper-graph partitioning problem. By deriving a min-cut of the hyper-graph, our
system can min- imize the total number of distributed transactions in the
workload, balance the sizes and workload accesses of the partitions and satisfy
all the partition constraints imposed. Our system is highly interactive as it
allows users to im- pose partition constraints, watch visualized partitioning
ef- fects, and provide feedback based on human expertise and indirect domain
knowledge for generating better partition- ing schemes."
"Multilevel association rules explore the concept hierarchy at multiple levels
which provides more specific information. Apriori algorithm explores the single
level association rules. Many implementations are available of Apriori
algorithm. Fast Apriori implementation is modified to develop new algorithm for
finding multilevel association rules. In this study the performance of this new
algorithm is analyzed in terms of running time in seconds."
"Time series analysis is the process of building a model using statistical
techniques to represent characteristics of time series data. Processing and
forecasting huge time series data is a challenging task. This paper presents
Approximation and Prediction of Stock Time-series data (APST), which is a two
step approach to predict the direction of change of stock price indices. First,
performs data approximation by using the technique called Multilevel Segment
Mean (MSM). In second phase, prediction is performed for the approximated data
using Euclidian distance and Nearest-Neighbour technique. The computational
cost of data approximation is O(n ni) and computational cost of prediction task
is O(m |NN|). Thus, the accuracy and the time required for prediction in the
proposed method is comparatively efficient than the existing Label Based
Forecasting (LBF) method [1]."
"The number of accidents and health diseases which are increasing at an
alarming rate are resulting in a huge increase in the demand for blood. There
is a necessity for the organized analysis of the blood donor database or blood
banks repositories. Clustering analysis is one of the data mining applications
and K-means clustering algorithm is the fundamental algorithm for modern
clustering techniques. K-means clustering algorithm is traditional approach and
iterative algorithm. At every iteration, it attempts to find the distance from
the centroid of each cluster to each and every data point. This paper gives the
improvement to the original k-means algorithm by improving the initial
centroids with distribution of data. Results and discussions show that improved
K-means algorithm produces accurate clusters in less computation time to find
the donors information."
"CrowdPlanner -- a novel crowd-based route recommendation system has been
developed, which requests human workers to evaluate candidates routes
recommended by different sources and methods, and determine the best route
based on the feedbacks of these workers. Our system addresses two critical
issues in its core components: a) task generation component generates a series
of informative and concise questions with optimized ordering for a given
candidate route set so that workers feel comfortable and easy to answer; and b)
worker selection component utilizes a set of selection criteria and an
efficient algorithm to find the most eligible workers to answer the questions
with high accuracy."
"Differential dependencies (DDs) capture the relationships between data
columns of relations. They are more general than functional dependencies (FDs)
and and the difference is that DDs are defined on the distances between values
of two tuples, not directly on the values. Because of this difference, the
algorithms for discovering FDs from data find only special DDs, not all DDs and
therefore are not applicable to DD discovery. In this paper, we propose an
algorithm to discover DDs from data following the way of fixing the left hand
side of a candidate DD to determine the right hand side. We also show some
properties of DDs and conduct a comprehensive analysis on how sampling affects
the DDs discovered from data."
"The term big data has become ubiquitous. Owing to a shared origin between
academia, industry and the media there is no single unified definition, and
various stakeholders provide diverse and often contradictory definitions. The
lack of a consistent definition introduces ambiguity and hampers discourse
relating to big data. This short paper attempts to collate the various
definitions which have gained some degree of traction and to furnish a clear
and concise definition of an otherwise ambiguous term."
"Access limitations are restrictions in the way in which the tuples of a
relation can be accessed. Under access limitations, query answering becomes
more complex than in the traditional case, with no guarantee that the answer
tuples that can be extracted (aka maximal answer) are all those that would be
found without access limitations (aka complete answer). The field of query
answering under access limitations has been broadly investigated in the past.
Attention has been devoted to the problem of determining relations that are
relevant for a query, i.e., those (possibly off-query) relations that might
need to be accessed in order to find all tuples in the maximal answer. In this
short paper, we show that relevance is undecidable for Datalog queries."
"Data is often partially known, vague or ambiguous in many real world
applications. To deal with such imprecise information, fuzziness is introduced
in the classical model. SQLf is one of the practical language to deal with
flexible fuzzy querying in Fuzzy DataBases (FDB). However, with a huge amount
of fuzzy data, the necessity to work with synthetic views became a challenge
for many DB community researchers. The present work deals with Flexible SQLf
query based on fuzzy linguistic summaries. We use the fuzzy summaries produced
by our Fuzzy-SaintEtiq approach. It provides a description of objects depending
on the fuzzy linguistic labels specified as selection criteria."
"Most pattern mining methods output a very large number of frequent patterns
and isolating a small but relevant subset is a challenging problem of current
interest in frequent pattern mining. In this paper we consider discovery of a
small set of relevant frequent episodes from data sequences. We make use of the
Minimum Description Length principle to formulate the problem of selecting a
subset of episodes. Using an interesting class of serial episodes with
inter-event constraints and a novel encoding scheme for data using such
episodes, we present algorithms for discovering small set of episodes that
achieve good data compression. Using an example of the data streams obtained
from distributed sensors in a composable coupled conveyor system, we show that
our method is very effective in unearthing highly relevant episodes and that
our scheme also achieves good data compression."
"The curse of dimensionality has remained a challenge for a wide variety of
algorithms in data mining, clustering, classification and privacy. Recently, it
was shown that an increasing dimensionality makes the data resistant to
effective privacy. The theoretical results seem to suggest that the
dimensionality curse is a fundamental barrier to privacy preservation. However,
in practice, we show that some of the common properties of real data can be
leveraged in order to greatly ameliorate the negative effects of the curse of
dimensionality. In real data sets, many dimensions contain high levels of
inter-attribute correlations. Such correlations enable the use of a process
known as vertical fragmentation in order to decompose the data into vertical
subsets of smaller dimensionality. An information-theoretic criterion of mutual
information is used in the vertical decomposition process. This allows the use
of an anonymization process, which is based on combining results from multiple
independent fragments. We present a general approach which can be applied to
the k-anonymity, l-diversity, and t-closeness models. In the presence of
inter-attribute correlations, such an approach continues to be much more robust
in higher dimensionality, without losing accuracy. We present experimental
results illustrating the effectiveness of the approach. This approach is
resilient enough to prevent identity, attribute, and membership disclosure
attack."
"As architecture, systems, and data management communities pay greater
attention to innovative big data systems and architectures, the pressure of
benchmarking and evaluating these systems rises. Considering the broad use of
big data systems, big data benchmarks must include diversity of data and
workloads. Most of the state-of-the-art big data benchmarking efforts target
evaluating specific types of applications or system software stacks, and hence
they are not qualified for serving the purposes mentioned above. This paper
presents our joint research efforts on this issue with several industrial
partners. Our big data benchmark suite BigDataBench not only covers broad
application scenarios, but also includes diverse and representative data sets.
BigDataBench is publicly available from http://prof.ict.ac.cn/BigDataBench .
Also, we comprehensively characterize 19 big data workloads included in
BigDataBench with varying data inputs. On a typical state-of-practice
processor, Intel Xeon E5645, we have the following observations: First, in
comparison with the traditional benchmarks: including PARSEC, HPCC, and
SPECCPU, big data applications have very low operation intensity; Second, the
volume of data input has non-negligible impact on micro-architecture
characteristics, which may impose challenges for simulation-based big data
architecture research; Last but not least, corroborating the observations in
CloudSuite and DCBench (which use smaller data inputs), we find that the
numbers of L1 instruction cache misses per 1000 instructions of the big data
applications are higher than in the traditional benchmarks; also, we find that
L3 caches are effective for the big data applications, corroborating the
observation in DCBench."
"In this document, I present the main notions of NoSQL databases and compare
four selected products (Riak, MongoDB, Cassandra, Neo4J) according to their
capabilities with respect to consistency, availability, and partition
tolerance, as well as performance. I also propose a few criteria for selecting
the right tool for the right situation."
"Fast and efficient data management is one of the demanding technologies of
todays aspect. This paper proposes a system which makes the working procedures
of present manual system of storing and retrieving huge citizens information of
Bangladesh automated and increases its effectiveness. The implemented search
methodology is user friendly and efficient enough for high speed data retrieval
ignoring spelling error in the input keywords used for searching a particular
citizen. The main concern in this research is minimizing the total searching
time for a given keyword. This can be done if we can pre-establish the idea of
getting the data belonging to the searching keyword. The primary and secondary
key-code generated by the Double Metaphone Algorithm for each word is used to
establish that idea about the word. This algorithm is used for creating the map
of the original database, through which the keyword is matched against the
data."
"Computing shortest distances is one of the fundamental problems on graphs,
and remains a {\em challenging} task today. {\em Distance} {\em landmarks} have
been recently studied for shortest distance queries with an auxiliary data
structure, referred to as {\em landmark} {\em covers}. This paper studies how
to apply distance landmarks for fast {\em exact} shortest distance query
answering on large road graphs. However, the {\em direct} application of
distance landmarks is {\em impractical} due to the high space and time cost. To
rectify this problem, we investigate novel techniques that can be seamlessly
combined with distance landmarks. We first propose a notion of {\em hybrid
landmark covers}, a revision of landmark covers. Second, we propose a notion of
{\em agents}, each of which represents a small subgraph and holds good
properties for fast distance query answering. We also show that agents can be
computed in {\em linear time}. Third, we introduce graph partitions to deal
with the remaining subgraph that cannot be captured by agents. Fourth, we
develop a unified framework that seamlessly integrates our proposed techniques
and existing optimization techniques, for fast shortest distance query
answering. Finally, we experimentally verify that our techniques significantly
improve the efficiency of shortest distance queries, using real-life road
graphs."
"Front end of data collection and loading into database manually may cause
potential errors in data sets and a very time consuming process. Scanning of a
data document in the form of an image and recognition of corresponding
information in that image can be considered as a possible solution of this
challenge. This paper presents an automated solution for the problem of data
cleansing and recognition of user written data to transform into standard
printed format with the help of artificial neural networks. Three different
neural models namely direct, correlation based and hierarchical have been
developed to handle this issue. In a very hostile input environment, the
solution is developed to justify the proposed logic."
"We show that all--instances termination of chase is undecidable. More
precisely, there is no algorithm deciding, for a given set $\cal T$ consisting
of Tuple Generating Dependencies (a.k.a. Datalog$^\exists$ program), whether
the $\cal T$-chase on $D$ will terminate for every finite database instance
$D$. Our method applies to Oblivious Chase, Semi-Oblivious Chase and -- after a
slight modification -- also for Standard Chase. This means that we give a
(negative) solution to the all--instances termination problem for all version
of chase that are usually considered.
  The arity we need for our undecidability proof is three. We also show that
the problem is EXPSPACE-hard for binary signatures, but decidability for this
case is left open.
  Both the proofs -- for ternary and binary signatures -- are easy. Once you
know them."
"In this paper, we present a novel approach -- called WaterFowl -- for the
storage of RDF triples that addresses some key issues in the contexts of big
data and the Semantic Web. The architecture of our prototype, largely based on
the use of succinct data structures, enables the representation of triples in a
self-indexed, compact manner without requiring decompression at query answering
time. Moreover, it is adapted to efficiently support RDF and RDFS entailment
regimes thanks to an optimized encoding of ontology concepts and properties
that does not require a complete inference materialization or extensive query
rewriting algorithms. This approach implies to make a distinction between the
terminological and the assertional components of the knowledge base early in
the process of data preparation, i.e., preprocessing the data before storing it
in our structures. The paper describes the complete architecture of this system
and presents some preliminary results obtained from evaluations conducted on
our first prototype."
"Data generation is a key issue in big data benchmarking that aims to generate
application-specific data sets to meet the 4V requirements of big data.
Specifically, big data generators need to generate scalable data (Volume) of
different types (Variety) under controllable generation rates (Velocity) while
keeping the important characteristics of raw data (Veracity). This gives rise
to various new challenges about how we design generators efficiently and
successfully. To date, most existing techniques can only generate limited types
of data and support specific big data systems such as Hadoop. Hence we develop
a tool, called Big Data Generator Suite (BDGS), to efficiently generate
scalable big data while employing data models derived from real data to
preserve data veracity. The effectiveness of BDGS is demonstrated by developing
six data generators covering three representative data types (structured,
semi-structured and unstructured) and three data sources (text, graph, and
table data)."
"Solid State Drives (SSDs) are a moving target for system designers: they are
black boxes, their internals are undocumented, and their performance
characteristics vary across models. There is no appropriate analytical model
and experimenting with commercial SSDs is cumbersome, as it requires a careful
experimental methodology to ensure repeatability. Worse, performance results
obtained on a given SSD cannot be generalized. Overall, it is impossible to
explore how a given algorithm, say a hash join or LSM-tree insertions,
leverages the intrinsic parallelism of a modern SSD, or how a slight change in
the internals of an SSD would impact its overall performance. In this paper, we
propose a new SSD simulation framework, named EagleTree, which addresses these
problems, and enables a principled study of SSD-Based algorithms. The
demonstration scenario illustrates the design space for algorithms based on an
SSD-based IO stack, and shows how researchers and practitioners can use
EagleTree to perform tractable explorations of this complex design space."
"Summarized data analysis of graphs using OLAP (Online Analytical Processing)
is very popular these days. However due to high dimensionality and large size,
it is not easy to decide which data should be aggregated for OLAP analysis.
Though iceberg cubing is useful, but it is unaware of the significance of
dimensional values with respect to the structure of the graph. In this paper,
we propose a Structural Significance, SS, measure to identify the structurally
significant dimensional values in each dimension. This leads to structure aware
pruning. We then propose an algorithm, iGraphCubing, to compute the graph cube
to analyze the structurally significant data using the proposed measure. We
evaluated the proposed ideas on real and synthetic data sets and observed very
encouraging results."
"Spreadsheets are end-user programs and domain models that are heavily
employed in administration, financial forecasting, education, and science
because of their intuitive, flexible, and direct approach to computation. As a
result, institutions are swamped by millions of spreadsheets that are becoming
increasingly difficult to manage, access, and control.
  This note presents the XLSearch system, a novel search engine for
spreadsheets. It indexes spreadsheet formulae and efficiently answers formula
queries via unification (a complex query language that allows metavariables in
both the query as well as the index). But a web-based search engine is only one
application of the underlying technology: Spreadsheet formula export to web
standards like MathML combined with formula indexing can be used to find
similar spreadsheets or common formula errors."
"Security features must be addressed when escalating a distributed database.
The choice between the object oriented and the relational data model, several
factors should be considered. The most important of these factors are single
and multilevel access controls (MAC), protection and integrity maintenance.
While determining which distributed database replica will be more secure for a
particular function, the choice should not be made exclusively on the basis of
available security features. One should also query the effectiveness and
efficiency of the delivery of these characteristics. In this paper, the
security strengths and weaknesses of both database models and the thorough
problems initiate in the distributed environment are conversed."
"Monotonicity is a simple yet significant qualitative characteristic. We
consider the problem of segmenting a sequence in up to K segments. We want
segments to be as monotonic as possible and to alternate signs. We propose a
quality metric for this problem using the l_inf norm, and we present an optimal
linear time algorithm based on novel formalism. Moreover, given a
precomputation in time O(n log n) consisting of a labeling of all extrema, we
compute any optimal segmentation in constant time. We compare experimentally
its performance to two piecewise linear segmentation heuristics (top-down and
bottom-up). We show that our algorithm is faster and more accurate.
Applications include pattern recognition and qualitative modeling."
"Through purposeful introduction of malicious transactions (tracking
transactions) into randomly select nodes of a (database) graph, soiled and
clean segments are identified. Soiled and clean measures corresponding those
segments are then computed. These measures are used to repose the problem of
critical database elements detection as an optimization problem over the graph.
This method is universally applicable over a large class of graphs (including
directed, weighted, disconnected, cyclic) that occur in several contexts of
databases. A generalization argument is presented which extends the critical
data problem to abstract settings."
"Bitmap indexes are frequently used to index multidimensional data. They rely
mostly on sequential input/output. Bitmaps can be compressed to reduce
input/output costs and minimize CPU usage. The most efficient compression
techniques are based on run-length encoding (RLE), such as Word-Aligned Hybrid
(WAH) compression. This type of compression accelerates logical operations
(AND, OR) over the bitmaps. However, run-length encoding is sensitive to the
order of the facts. Thus, we propose to sort the fact tables. We review
lexicographic, Gray-code, and block-wise sorting. We found that a lexicographic
sort improves compression--sometimes generating indexes twice as small--and
make indexes several times faster. While sorting takes time, this is partially
offset by the fact that it is faster to index a sorted table. Column order is
significant: it is generally preferable to put the columns having more distinct
values at the beginning. A block-wise sort is much less efficient than a full
sort. Moreover, we found that Gray-code sorting is not better than
lexicographic sorting when using word-aligned compression."
"This paper presents an extensive experimental study of the state-of-the-art
of XML compression tools. The study reports the behavior of nine XML
compressors using a large corpus of XML documents which covers the different
natures and scales of XML documents. In addition to assessing and comparing the
performance characteristics of the evaluated XML compression tools, the study
tries to assess the effectiveness and practicality of using these tools in the
real world. Finally, we provide some guidelines and recommen- dations which are
useful for helping developers and users for making an effective decision for
selecting the most suitable XML compression tool for their needs."
"There is a growing realization that uncertain information is a first-class
citizen in modern database management. As such, we need techniques to correctly
and efficiently process uncertain data in database systems. In particular, data
reduction techniques that can produce concise, accurate synopses of large
probabilistic relations are crucial. Similar to their deterministic relation
counterparts, such compact probabilistic data synopses can form the foundation
for human understanding and interactive data exploration, probabilistic query
planning and optimization, and fast approximate query processing in
probabilistic database systems.
  In this paper, we introduce definitions and algorithms for building
histogram- and wavelet-based synopses on probabilistic data. The core problem
is to choose a set of histogram bucket boundaries or wavelet coefficients to
optimize the accuracy of the approximate representation of a collection of
probabilistic tuples under a given error metric. For a variety of different
error metrics, we devise efficient algorithms that construct optimal or near
optimal B-term histogram and wavelet synopses. This requires careful analysis
of the structure of the probability distributions, and novel extensions of
known dynamic-programming-based techniques for the deterministic domain. Our
experiments show that this approach clearly outperforms simple ideas, such as
building summaries for samples drawn from the data distribution, while taking
equal or less time."
"This report details the generation and use of tree node ordering keys in a
single relational database table. The keys for each node are calculated from
the keys of its parent, in such a way that the sort order places every node in
the tree before all of its descendants and after all siblings having a lower
index. The calculation from parent keys to child keys is simple, and reversible
in the sense that the keys of every ancestor of a node can be calculated from
that node's keys without having to consult the database.
  Proofs of the above properties of the key encoding process and of its
correspondence to a finite continued fraction form are provided."
"Most existing anonymization work has been done on static datasets, which have
no update and need only one-time publication. Recent studies consider
anonymizing dynamic datasets with external updates: the datasets are updated
with record insertions and/or deletions. This paper addresses a new problem:
anonymous re-publication of datasets with internal updates, where the attribute
values of each record are dynamically updated. This is an important and
challenging problem for attribute values of records are updating frequently in
practice and existing methods are unable to deal with such a situation.
  We initiate a formal study of anonymous re-publication of dynamic datasets
with internal updates, and show the invalidation of existing methods. We
introduce theoretical definition and analysis of dynamic datasets, and present
a general privacy disclosure framework that is applicable to all anonymous
re-publication problems. We propose a new counterfeited generalization
principle alled m-Distinct to effectively anonymize datasets with both external
updates and internal updates. We also develop an algorithm to generalize
datasets to meet m-Distinct. The experiments conducted on real-world data
demonstrate the effectiveness of the proposed solution."
"In this paper we present a new approach to data modelling, called the
concept-oriented model (CoM), and describe its main features and
characteristics including data semantics and operations. The distinguishing
feature of this model is that it is based on the formalism of nested ordered
sets where any element participates in two structures simultaneously:
hierarchical (nested) and multi-dimensional (ordered). An element of the model
is postulated to consist of two parts, called identity and entity, and the
whole approach can be naturally broken into two branches: identity modelling
and entity modelling. We also propose a new query language with the main
construct, called concept, defined as a pair of two classes: identity class and
entity class. We describe how its operations of projection, de-projection and
product can be used to solve typical data modelling tasks."
"XML has emerged as the leading language for representing and exchanging data
not only on the Web, but also in general in the enterprise. XQuery is emerging
as the standard query language for XML. Thus, tools are required to mediate
between XML queries and heterogeneous data sources to integrate data in XML.
This paper presents the XMedia mediator, a unique tool for integrating and
querying disparate heterogeneous information as unified XML views. It describes
the mediator architecture and focuses on the unique distributed query
processing technology implemented in this component. Query evaluation is based
on an original XML algebra simply extending classical operators to process
tuples of tree elements. Further, we present a set of performance evaluation on
a relational benchmark, which leads to discuss possible performance
enhancements."
"Group based anonymization is the most widely studied approach for privacy
preserving data publishing. This includes k-anonymity, l-diversity, and
t-closeness, to name a few. The goal of this paper is to raise a fundamental
issue on the privacy exposure of the current group based approach. This has
been overlooked in the past. The group based anonymization approach basically
hides each individual record behind a group to preserve data privacy. If not
properly anonymized, patterns can actually be derived from the published data
and be used by the adversary to breach individual privacy. For example, from
the medical records released, if patterns such as people from certain countries
rarely suffer from some disease can be derived, then the information can be
used to imply linkage of other people in an anonymized group with this disease
with higher likelihood. We call the derived patterns from the published data
the foreground knowledge. This is in contrast to the background knowledge that
the adversary may obtain from other channels as studied in some previous work.
Finally, we show by experiments that the attack is realistic in the privacy
benchmark dataset under the traditional group based anonymization approach."
"Increasingly, business projects are ephemeral. New Business Intelligence
tools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds
are a popular community-driven visualization technique. Hence, we investigate
tag-cloud views with support for OLAP operations such as roll-ups, slices,
dices, clustering, and drill-downs. As a case study, we implemented an
application where users can upload data and immediately navigate through its ad
hoc dimensions. To support social networking, views can be easily shared and
embedded in other Web sites. Algorithmically, our tag-cloud views are
approximate range top-k queries over spontaneous data cubes. We present
experimental evidence that iceberg cuboids provide adequate online
approximations. We benchmark several browser-oblivious tag-cloud layout
optimizations."
"The goal of the presented work is to illustrate a method by which the data
exchange between a standalone computer software and a shared database server
can be protected of unauthorized interceptation of the traffic in Internet
network, a transport network for data managed by those two systems,
interceptation by which an attacker could gain illegetimate access to the
database, threatening this way the data integrity and compromising the
database."
"The eXtensible Markup Language (XML) provides a powerful and flexible means
of encoding and exchanging data. As it turns out, its main advantage as an
encoding format (namely, its requirement that all open and close markup tags
are present and properly balanced) yield also one of its main disadvantages:
verbosity. XML-conscious compression techniques seek to overcome this drawback.
Many of these techniques first separate XML structure from the document
content, and then compress each independently. Further compression gains can be
realized by identifying and compressing together document content that is
highly similar, thereby amortizing the storage costs of auxiliary information
required by the chosen compression algorithm. Additionally, the proper choice
of compression algorithm is an important factor not only for the achievable
compression gain, but also for access performance. Hence, choosing a
compression configuration that optimizes compression gain requires one to
determine (1) a partitioning strategy for document content, and (2) the best
available compression algorithm to apply to each set within this partition. In
this paper, we show that finding an optimal compression configuration with
respect to compression gain is an NP-hard optimization problem. This problem
remains intractable even if one considers a single compression algorithm for
all content. We also describe an approximation algorithm for selecting a
partitioning strategy for document content based on the branch-and-bound
paradigm."
"In this paper, we study the problem of processing continuous range queries in
a hierarchical wireless sensor network. Contrasted with the traditional
approach of building networks in a ""flat"" structure using sensor devices of the
same capability, the hierarchical approach deploys devices of higher capability
in a higher tier, i.e., a tier closer to the server. While query processing in
flat sensor networks has been widely studied, the study on query processing in
hierarchical sensor networks has been inadequate. In wireless sensor networks,
the main costs that should be considered are the energy for sending data and
the storage for storing queries. There is a trade-off between these two costs.
Based on this, we first propose a progressive processing method that
effectively processes a large number of continuous range queries in
hierarchical sensor networks. The proposed method uses the query merging
technique proposed by Xiang et al. as the basis and additionally considers the
trade-off between the two costs. More specifically, it works toward reducing
the storage cost at lower-tier nodes by merging more queries, and toward
reducing the energy cost at higher-tier nodes by merging fewer queries (thereby
reducing ""false alarms""). We then present how to build a hierarchical sensor
network that is optimal with respect to the weighted sum of the two costs. It
allows for a cost-based systematic control of the trade-off based on the
relative importance between the storage and energy in a given network
environment and application. Experimental results show that the proposed method
achieves a near-optimal control between the storage and energy and reduces the
cost by 0.989~84.995 times compared with the cost achieved using the flat
(i.e., non-hierarchical) setup as in the work by Xiang et al."
"Now a days, data mining and knowledge discovery methods are applied to a
variety of enterprise and engineering disciplines to uncover interesting
patterns from databases. The study of Sequential patterns is an important data
mining problem due to its wide applications to real world time dependent
databases. Sequential patterns are inter-event patterns ordered over a
time-period associated with specific objects under study. Analysis and
discovery of frequent sequential patterns over a predetermined time-period are
interesting data mining results, and can aid in decision support in many
enterprise applications. The problem of sequential pattern mining poses
computational challenges as a long frequent sequence contains enormous number
of frequent subsequences. Also useful results depend on the right choice of
event window. In this paper, we have studied the problem of sequential pattern
mining through two perspectives, one the computational aspect of the problem
and the other is incorporation and adjustability of time constraint. We have
used Indiscernibility relation from theory of rough sets to partition the
search space of sequential patterns and have proposed a novel algorithm that
allows previsualization of patterns and allows adjustment of time constraint
prior to execution of mining task. The algorithm Rough Set Partitioning is at
least ten times faster than the naive time constraint based sequential pattern
mining algorithm GSP. Besides this an additional knowledge of time interval of
sequential patterns is also determined with the method."
"The probabilistic top-k queries based on the interplay of score and
probability, under the possible worlds semantic, become an important research
issue that considers both score and uncertainty on the same basis. In the
literature, many different probabilistic top-k queries are proposed. Almost all
of them need to compute the probability of a tuple t_i to be ranked at the j-th
position across the entire set of possible worlds. The cost of such computing
is the dominant cost and is known as O(kn^2), where n is the size of dataset.
In this paper, we propose a new novel algorithm that computes such probability
in O(kn)."
"A consistent query answer in an inconsistent database is an answer obtained
in every (minimal) repair. The repairs are obtained by resolving all conflicts
in all possible ways. Often, however, the user is able to provide a preference
on how conflicts should be resolved. We investigate here the framework of
preferred consistent query answers, in which user preferences are used to
narrow down the set of repairs to a set of preferred repairs. We axiomatize
desirable properties of preferred repairs. We present three different families
of preferred repairs and study their mutual relationships. Finally, we
investigate the complexity of preferred repairing and computing preferred
consistent query answers."
"XML data warehouses form an interesting basis for decision-support
applications that exploit heterogeneous data from multiple sources. However,
XML-native database systems currently suffer from limited performances in terms
of manageable data volume and response time for complex analytical queries.
Fragmenting and distributing XML data warehouses (e.g., on data grids) allow to
address both these issues. In this paper, we work on XML warehouse
fragmentation. In relational data warehouses, several studies recommend the use
of derived horizontal fragmentation. Hence, we propose to adapt it to the XML
context. We particularly focus on the initial horizontal fragmentation of
dimensions' XML documents and exploit two alternative algorithms. We
experimentally validate our proposal and compare these alternatives with
respect to a unified XML warehouse model we advocate for."
"Date and Darwen have proposed a theory of types, the latter forms the basis
of a detailed presentation of a panoply of simple and complex types. However,
this proposal has not been structured in a formal system. Specifically, Date
and Darwen haven't indicated the formalism of the type system that corresponds
to the type theory established. In this paper, we propose a pseudo-algorithmic
and grammatical description of a system of types for Date and Darwen's model.
Our type system is supposed take into account null values; for such intention,
we introduce a particular type noted #, which expresses one or more occurrences
of incomplete information in a database. Our algebraic grammar describes in
detail the complete specification of an inheritance model and the subryping
relation induced, thus the different definitions of related concepts."
"In this work we present in-network techniques to improve the efficiency of
spatial aggregate queries. Such queries are very common in a sensornet setting,
demanding more targeted techniques for their handling. Our approach constructs
and maintains multi-resolution cube hierarchies inside the network, which can
be constructed in a distributed fashion. In case of failures, recovery can also
be performed with in-network decisions. In this paper we demonstrate how
in-network cube hierarchies can be used to summarize sensor data, and how they
can be exploited to improve the efficiency of spatial aggregate queries. We
show that query plans over our cube summaries can be computed in polynomial
time, and we present a PTIME algorithm that selects the minimum number of data
requests that can compute the answer to a spatial query. We further extend our
algorithm to handle optimization over multiple queries, which can also be done
in polynomial time. We discuss enriching cube hierarchies with extra summary
information, and present an algorithm for distributed cube construction.
Finally we investigate node and area failures, and algorithms to recover query
results."
"We propose to demonstrate LiquidXML, a platform for managing large corpora of
XML documents in large-scale P2P networks. All LiquidXML peers may publish XML
documents to be shared with all the network peers. The challenge then is to
efficiently (re-)distribute the published content in the network, possibly in
overlapping, redundant fragments, to support efficient processing of queries at
each peer. The novelty of LiquidXML relies in its adaptive method of choosing
which data fragments are stored where, to improve performance. The ""liquid""
aspect of XML management is twofold: XML data flows from many sources towards
many consumers, and its distribution in the network continuously adapts to
improve query performance."
"This research paper briefly describes the industrial contributions of Product
Data Management in any organization's technical and managerial data management.
Then focusing on some current major PDM based problems i.e. Static and
Unintelligent Search, Platform Independent System and Successful PDM System
Implementation, briefly presents a semantic based solution i.e. I-SOAS. Majorly
this research paper is about to present and discuss the contributions of I-SOAS
in any organization's technical and system data management."
"This research is about an online forum designed and developed to improve the
communication process between alumni, new, old and upcoming students. In this
research paper we present targeted problems, designed architecture, used
technologies in development and final end product in detail."
"Frequent itemset mining in uncertain transaction databases semantically and
computationally differs from traditional techniques applied on standard
(certain) transaction databases. Uncertain transaction databases consist of
sets of existentially uncertain items. The uncertainty of items in transactions
makes traditional techniques inapplicable. In this paper, we tackle the problem
of finding probabilistic frequent itemsets based on possible world semantics.
In this context, an itemset X is called frequent if the probability that X
occurs in at least minSup transactions is above a given threshold. We make the
following contributions: We propose the first probabilistic FP-Growth algorithm
(ProFP-Growth) and associated probabilistic FP-Tree (ProFP-Tree), which we use
to mine all probabilistic frequent itemsets in uncertain transaction databases
without candidate generation. In addition, we propose an efficient technique to
compute the support probability distribution of an itemset in linear time using
the concept of generating functions. An extensive experimental section
evaluates the our proposed techniques and shows that our ProFP-Growth approach
is significantly faster than the current state-of-the-art algorithm."
"Matching dependencies were recently introduced as declarative rules for data
cleaning and entity resolution. Enforcing a matching dependency on a database
instance identifies the values of some attributes for two tuples, provided that
the values of some other attributes are sufficiently similar. Assuming the
existence of matching functions for making two attributes values equal, we
formally introduce the process of cleaning an instance using matching
dependencies, as a chase-like procedure. We show that matching functions
naturally introduce a lattice structure on attribute domains, and a partial
order of semantic domination between instances. Using the latter, we define the
semantics of clean query answering in terms of certain/possible answers as the
greatest lower bound/least upper bound of all possible answers obtained from
the clean instances. We show that clean query answering is intractable in some
cases. Then we study queries that behave monotonically wrt semantic domination
order, and show that we can provide an under/over approximation for clean
answers to monotone queries. Moreover, non-monotone positive queries can be
relaxed into monotone queries."
"Over the last couple of years, ""Cloud Computing"" or ""Elastic Computing"" has
emerged as a compelling and successful paradigm for internet scale computing.
One of the major contributing factors to this success is the elasticity of
resources. In spite of the elasticity provided by the infrastructure and the
scalable design of the applications, the elephant (or the underlying database),
which drives most of these web-based applications, is not very elastic and
scalable, and hence limits scalability. In this paper, we propose ElasTraS
which addresses this issue of scalability and elasticity of the data store in a
cloud computing environment to leverage from the elastic nature of the
underlying infrastructure, while providing scalable transactional data access.
This paper aims at providing the design of a system in progress, highlighting
the major design choices, analyzing the different guarantees provided by the
system, and identifying several important challenges for the research community
striving for computing in the cloud."
"Matching dependencies (MDs) were introduced to specify the identification or
matching of certain attribute values in pairs of database tuples when some
similarity conditions are satisfied. Their enforcement can be seen as a natural
generalization of entity resolution. In what we call the ""pure case"" of MDs,
any value from the underlying data domain can be used for the value in common
that does the matching. We investigate the semantics and properties of data
cleaning through the enforcement of matching dependencies for the pure case. We
characterize the intended clean instances and also the ""clean answers"" to
queries as those that are invariant under the cleaning process. The complexity
of computing clean instances and clean answers to queries is investigated.
Tractable and intractable cases depending on the MDs and queries are
identified. Finally, we establish connections with database ""repairs"" under
integrity constraints."
"Regular tree grammars and regular path expressions constitute core constructs
widely used in programming languages and type systems. Nevertheless, there has
been little research so far on frameworks for reasoning about path expressions
where node cardinality constraints occur along a path in a tree. We present a
logic capable of expressing deep counting along paths which may include
arbitrary recursive forward and backward navigation. The counting extensions
can be seen as a generalization of graded modalities that count immediate
successor nodes. While the combination of graded modalities, nominals, and
inverse modalities yields undecidable logics over graphs, we show that these
features can be combined in a decidable tree logic whose main features can be
decided in exponential time. Our logic being closed under negation, it may be
used to decide typical problems on XPath queries such as satisfiability, type
checking with relation to regular types, containment, or equivalence."
"Preference queries incorporate the notion of binary preference relation into
relational database querying. Instead of returning all the answers, such
queries return only the best answers, according to a given preference relation.
Preference queries are a fast growing area of database research. Skyline
queries constitute one of the most thoroughly studied classes of preference
queries. A well known limitation of skyline queries is that skyline preference
relations assign the same importance to all attributes. In this work, we study
p-skyline queries that generalize skyline queries by allowing varying attribute
importance in preference relations. We perform an in-depth study of the
properties of p-skyline preference relations. In particular,we study the
problems of containment and minimal extension. We apply the obtained results to
the central problem of the paper: eliciting relative importance of attributes.
Relative importance is implicit in the constructed p-skyline preference
relation. The elicitation is based on user-selected sets of superior (positive)
and inferior (negative) examples. We show that the computational complexity of
elicitation depends on whether inferior examples are involved. If they are not,
elicitation can be achieved in polynomial time. Otherwise, it is NP-complete.
Our experiments show that the proposed elicitation algorithm has high accuracy
and good scalability"
"In general frequent itemsets are generated from large data sets by applying
association rule mining algorithms like Apriori, Partition, Pincer-Search,
Incremental, Border algorithm etc., which take too much computer time to
compute all the frequent itemsets. By using Genetic Algorithm (GA) we can
improve the scenario. The major advantage of using GA in the discovery of
frequent itemsets is that they perform global search and its time complexity is
less compared to other algorithms as the genetic algorithm is based on the
greedy approach. The main aim of this paper is to find all the frequent
itemsets from given data sets using genetic algorithm."
"Over the last decade there have been great strides made in developing
techniques to compute functions privately. In particular, Differential Privacy
gives strong promises about conclusions that can be drawn about an individual.
In contrast, various syntactic methods for providing privacy (criteria such as
kanonymity and l-diversity) have been criticized for still allowing private
information of an individual to be inferred. In this report, we consider the
ability of an attacker to use data meeting privacy definitions to build an
accurate classifier. We demonstrate that even under Differential Privacy, such
classifiers can be used to accurately infer ""private"" attributes in realistic
data. We compare this to similar approaches for inferencebased attacks on other
forms of anonymized data. We place these attacks on the same scale, and observe
that the accuracy of inference of private attributes for Differentially Private
data and l-diverse data can be quite similar."
"Data warehouses are the core of decision support sys- tems, which nowadays
are used by all kind of enter- prises in the entire world. Although many
studies have been conducted on the need of decision support systems (DSSs) for
small businesses, most of them adopt ex- isting solutions and approaches, which
are appropriate for large-scaled enterprises, but are inadequate for small and
middle-sized enterprises. Small enterprises require cheap, lightweight
architec- tures and tools (hardware and software) providing on- line data
analysis. In order to ensure these features, we review web-based business
intelligence approaches. For real-time analysis, the traditional OLAP
architecture is cumbersome and storage-costly; therefore, we also re- view
in-memory processing. Consequently, this paper discusses the existing approa-
ches and tools working in main memory and/or with web interfaces (including
freeware tools), relevant for small and middle-sized enterprises in decision
making."
"With the emergence of XML as a standard for representing business data, new
decision support applications are being developed. These XML data warehouses
aim at supporting On-Line Analytical Processing (OLAP) operations that
manipulate irregular XML data. To ensure feasibility of these new tools,
important performance issues must be addressed. Performance is customarily
assessed with the help of benchmarks. However, decision support benchmarks do
not currently support XML features. In this paper, we introduce the XML
Warehouse Benchmark (XWeB), which aims at filling this gap. XWeB derives from
the relational decision support benchmark TPC-H. It is mainly composed of a
test data warehouse that is based on a unified reference model for XML
warehouses and that features XML-specific structures, and its associate XQuery
decision support workload. XWeB's usage is illustrated by experiments on
several XML database management systems."
"With the rise of XML as a standard for representing business data, XML data
warehousing appears as a suitable solution for decision-support applications.
In this context, it is necessary to allow OLAP analyses on XML data cubes.
Thus, XQuery extensions are needed. To define a formal framework and allow
much-needed performance optimizations on analytical queries expressed in
XQuery, defining an algebra is desirable. However, XML-OLAP (XOLAP) algebras
from the literature still largely rely on the relational model. Hence, we
propose in this paper a rollup operator based on a pattern tree in order to
handle multidimensional XML data expressed within complex hierarchies."
"A large amount of data resulting from trajectories of moving objects
activities are collected thanks to localization based services and some
associated automated processes. Trajectories data can be used either for
transactional and analysis purposes in various domains (heath care, commerce,
environment, etc.). For this reason, modeling trajectory data at the conceptual
level is an important stair leading to global vision and successful
implementations. However, current modeling tools fail to fulfill specific
moving objects activities requirements. In this paper, we propose a new profile
based on UML in order to enhance the conceptual modeling of trajectory data
related to mobile objects by new stereotypes and icons. As illustration, we
present a mobile hospital use case."
"Distributed systems can be very large and complex. The various considerations
that influence their design can result in a substantial specification, which
requires a structured framework that has to be managed successfully. The
purpose of the RMODP is to define such a framework. The Reference Model for
Open Distributed Processing (RM-ODP) provides a framework within which support
of distribution, inter-working and portability can be integrated. It defines:
an object model, architectural concepts and architecture for the development of
ODP systems in terms of five viewpoints. Which include an information
viewpoint. Since the usage of Data bases management systems (DBMS) in complex
networks is increasing considerably, we are interested, in our work, in giving
DBMS specifications through the use of the three schemas (static, dynamic,
invariant). The present paper is organized as follows. After a literature
review, we will describe then the subset of concepts considered in this work
named the database management system (DBMS) object model. In the third section,
we will be interested in the engineering language and DMBS structure by
describing essentially DBMS objects. Finally, we will present DBMS engineering
specifications and makes the connection between models and their instances.
This introduces the basic form of the semantic approach we have described here."
"The data warehousing is becoming increasingly important in terms of strategic
decision making through their capacity to integrate heterogeneous data from
multiple information sources in a common storage space, for querying and
analysis. So it can evolve into a multi-tier structure where parts of the
organization take information from the main data warehouse into their own
systems. These may include analysis databases or dependent data marts. As the
data warehouse evolves and the organization gets better at capturing
information on all interactions with the customer. Data warehouse can track
customer interactions over the whole of the customer's lifetime."
"In today's competitive scenario in corporate world, ""Customer Retention""
strategy in Customer Relationship Management (CRM) is an increasingly pressed
issue. Data mining techniques play a vital role in better CRM. This paper
attempts to bring a new perspective by focusing the issue of data mining
applications, opportunities and challenges in CRM. It covers the topic such as
customer retention, customer services, risk assessment, fraud detection and
some of the data mining tools which are widely used in CRM."
"Association rule mining (ARM) is the process of generating rules based on the
correlation between the set of items that the customers purchase.Of late, data
mining researchers have improved upon the quality of association rule mining
for business development by incorporating factors like value (utility),
quantity of items sold (weight) and profit. The rules mined without considering
utility values (profit margin) will lead to a probable loss of profitable
rules. The advantage of wealth of the customers' needs information and rules
aids the retailer in designing his store layout[9]. An algorithm CSHURI,
Customer Segmentation using HURI, is proposed, a modified version of HURI [6],
finds customers who purchase high profitable rare items and accordingly
classify the customers based on some criteria; for example, a retail business
may need to identify valuable customers who are major contributors to a
company's overall profit. For a potential customer arriving in the store, which
customer group one should belong to according to customer needs, what are the
preferred functional features or products that the customer focuses on and what
kind of offers will satisfy the customer, etc., finds the key in targeting
customers to improve sales [9], which forms the base for customer utility
mining."
"In this paper, a general moving object trajectories framework is put forward
to allow independent applications processing trajectories data benefit from a
high level of interoperability, information sharing as well as an efficient
answer for a wide range of complex trajectory queries. Our proposed meta-model
is based on ontology and event approach, incorporates existing presentations of
trajectory and integrates new patterns like space-time path to describe
activities in geographical space-time. We introduce recursive Region of
Interest concepts and deal mobile objects trajectories with diverse
spatio-temporal sampling protocols and different sensors available that
traditional data model alone are incapable for this purpose."
"Breast cancer is one of the leading cancers for women in developed countries
including India. It is the second most common cause of cancer death in women.
The high incidence of breast cancer in women has increased significantly in the
last years. In this paper we have discussed various data mining approaches that
have been utilized for breast cancer diagnosis and prognosis. Breast Cancer
Diagnosis is distinguishing of benign from malignant breast lumps and Breast
Cancer Prognosis predicts when Breast Cancer is to recur in patients that have
had their cancers excised. This study paper summarizes various review and
technical articles on breast cancer diagnosis and prognosis also we focus on
current research being carried out using the data mining techniques to enhance
the breast cancer diagnosis and prognosis."
"This paper presents our Linked Open Data (LOD) infrastructures for genomic
and experimental data related to microRNA biomolecules. Legacy data from two
well-known microRNA databases with experimental data and observations, as well
as change and version information about microRNA entities, are fused and
exported as LOD. Our LOD server assists biologists to explore biological
entities and their evolution, and provides a SPARQL endpoint for applications
and services to query historical miRNA data and track changes, their causes and
effects."
"Open data platforms such as data.gov or opendata.socrata. com provide a huge
amount of valuable information. Their free-for-all nature, the lack of
publishing standards and the multitude of domains and authors represented on
these platforms lead to new integration and standardization problems. At the
same time, crowd-based data integration techniques are emerging as new way of
dealing with these problems. However, these methods still require input in form
of specific questions or tasks that can be passed to the crowd. This paper
discusses integration problems on Open Data Platforms, and proposes a method
for identifying and ranking integration hypotheses in this context. We will
evaluate our findings by conducting a comprehensive evaluation using on one of
the largest Open Data platforms."
"With today's public data sets containing billions of data items, more and
more companies are looking to integrate external data with their traditional
enterprise data to improve business intelligence analysis. These distributed
data sources however exhibit heterogeneous data formats and terminologies and
may contain noisy data. In this paper, we present a novel framework that
enables business users to semi-automatically perform data integration on
potentially noisy tabular data. This framework offers an extension to Google
Refine with novel schema matching algorithms leveraging Freebase rich types.
First experiments show that using Linked Data to map cell values with instances
and column headers with types improves significantly the quality of the
matching results and therefore should lead to more informed decisions."
"OpenData movement around the globe is demanding more access to information
which lies locked in public or private servers. As recently reported by a
McKinsey publication, this data has significant economic value, yet its release
has potential to blatantly conflict with people privacy. Recent UK government
inquires have shown concern from various parties about publication of
anonymized databases, as there is concrete possibility of user identification
by means of linkage attacks. Differential privacy stands out as a model that
provides strong formal guarantees about the anonymity of the participants in a
sanitized database. Only recent results demonstrated its applicability on
real-life datasets, though. This paper covers such breakthrough discoveries, by
reviewing applications of differential privacy for non-interactive publication
of anonymized real-life datasets. Theory, utility and a data-aware comparison
are discussed on a variety of principles and concrete applications."
"An increasing amount of trajectory data is being annotated with text
descriptions to better capture the semantics associated with locations. The
fusion of spatial locations and text descriptions in trajectories engenders a
new type of top-$k$ queries that take into account both aspects. Each
trajectory in consideration consists of a sequence of geo-spatial locations
associated with text descriptions. Given a user location $\lambda$ and a
keyword set $\psi$, a top-$k$ query returns $k$ trajectories whose text
descriptions cover the keywords $\psi$ and that have the shortest match
distance. To the best of our knowledge, previous research on querying
trajectory databases has focused on trajectory data without any text
description, and no existing work has studied such kind of top-$k$ queries on
trajectories. This paper proposes one novel method for efficiently computing
top-$k$ trajectories. The method is developed based on a new hybrid index,
cell-keyword conscious B$^+$-tree, denoted by \cellbtree, which enables us to
exploit both text relevance and location proximity to facilitate efficient and
effective query processing. The results of our extensive empirical studies with
an implementation of the proposed algorithms on BerkeleyDB demonstrate that our
proposed methods are capable of achieving excellent performance and good
scalability."
"The exponential increase of availability of digital data and the necessity to
process it in business and scientific fields has literally forced upon us the
need to analyze and mine useful knowledge from it. Traditionally data mining
has used a data warehousing model of gathering all data into a central site,
and then running an algorithm upon that data. Such a centralized approach is
fundamentally inappropriate due to many reasons like huge amount of data,
infeasibility to centralize data stored at multiple sites, bandwidth limitation
and privacy concerns. To solve these problems, Distributed Data Mining (DDM)
has emerged as a hot research area. Careful attention in the usage of
distributed resources of data, computing, communication, and human factors in a
near optimal fashion are paid by distributed data mining. DDM is gaining
attention in peer-to-peer (P2P) systems which are emerging as a choice of
solution for applications such as file sharing, collaborative movie and song
scoring, electronic commerce, and surveillance using sensor networks. The main
intension of this draft paper is to provide an overview of DDM and P2P Data
Mining. The paper discusses the need for DDM, taxonomy of DDM architectures,
various DDM approaches, DDM related works in P2P systems and issues and
challenges in P2P data mining."
"Evaluating conjunctive queries and solving constraint satisfaction problems
are fundamental problems in database theory and artificial intelligence,
respectively. These problems are NP-hard, so that several research efforts have
been made in the literature for identifying tractable classes, known as islands
of tractability, as well as for devising clever heuristics for solving
efficiently real-world instances. Many heuristic approaches are based on
enforcing on the given instance a property called local consistency, where (in
database terms) each tuple in every query atom matches at least one tuple in
every other query atom. Interestingly, it turns out that, for many well-known
classes of queries, such as for the acyclic queries, enforcing local
consistency is even sufficient to solve the given instance correctly. However,
the precise power of such a procedure was unclear, but for some very restricted
cases. The paper provides full answers to the long-standing questions about the
precise power of algorithms based on enforcing local consistency. The classes
of instances where enforcing local consistency turns out to be a correct
query-answering procedure are however not efficiently recognizable. In fact,
the paper finally focuses on certain subclasses defined in terms of the novel
notion of greedy tree projections. These latter classes are shown to be
efficiently recognizable and strictly larger than most islands of tractability
known so far, both in the general case of tree projections and for specific
structural decomposition methods."
"This paper presents a Framework for converting a class diagram into an XML
structure and shows how to use Web files for the design of data warehouses
based on the classification UML. Extensible Markup Language (XML) has become a
standard for representing data over the Internet. We use XSD schema for define
the structure of XML documents and validate XML documents.
  A prototype has been developed, which migrates successfully UML Class into
XML document based on the formulation mathematics model. The experimental
results were very encouraging, demonstrating that the proposed approach is
feasible efficient and correct."
"This paper proposes a solution for migrating an RDB into Web semantic. The
solution takes an existing RDB as input, and extracts its metadata
representation (MTRDB). Based on the MTRDB, a Canonical Data Model (CDM) is
generated. Finally, the structure of the classification scheme in the CDM model
is converted into OWL ontology and the recordsets of database are stored in owl
document. A prototype has been implemented, which migrates a RDB into OWL
structure, for demonstrate the practical applicability of our approach by
showing how the results of reasoning of this technique can help improve the Web
systems."
"The ability to handle large scale graph data is crucial to an increasing
number of applications. Much work has been dedicated to supporting basic graph
operations such as subgraph matching, reachability, regular expression
matching, etc. In many cases, graph indices are employed to speed up query
processing. Typically, most indices require either super-linear indexing time
or super-linear indexing space. Unfortunately, for very large graphs,
super-linear approaches are almost always infeasible. In this paper, we study
the problem of subgraph matching on billion-node graphs. We present a novel
algorithm that supports efficient subgraph matching for graphs deployed on a
distributed memory store. Instead of relying on super-linear indices, we use
efficient graph exploration and massive parallel computing for query
processing. Our experimental results demonstrate the feasibility of performing
subgraph matching on web-scale graph data."
"Many studies have been conducted on seeking the efficient solution for
subgraph similarity search over certain (deterministic) graphs due to its wide
application in many fields, including bioinformatics, social network analysis,
and Resource Description Framework (RDF) data management. All these works
assume that the underlying data are certain. However, in reality, graphs are
often noisy and uncertain due to various factors, such as errors in data
extraction, inconsistencies in data integration, and privacy preserving
purposes. Therefore, in this paper, we study subgraph similarity search on
large probabilistic graph databases. Different from previous works assuming
that edges in an uncertain graph are independent of each other, we study the
uncertain graphs where edges' occurrences are correlated. We formally prove
that subgraph similarity search over probabilistic graphs is #P-complete, thus,
we employ a filter-and-verify framework to speed up the search. In the
filtering phase,we develop tight lower and upper bounds of subgraph similarity
probability based on a probabilistic matrix index, PMI. PMI is composed of
discriminative subgraph features associated with tight lower and upper bounds
of subgraph isomorphism probability. Based on PMI, we can sort out a large
number of probabilistic graphs and maximize the pruning capability. During the
verification phase, we develop an efficient sampling algorithm to validate the
remaining candidates. The efficiency of our proposed solutions has been
verified through extensive experiments."
"The k-truss is a type of cohesive subgraphs proposed recently for the study
of networks. While the problem of computing most cohesive subgraphs is NP-hard,
there exists a polynomial time algorithm for computing k-truss. Compared with
k-core which is also efficient to compute, k-truss represents the ""core"" of a
k-core that keeps the key information of, while filtering out less important
information from, the k-core. However, existing algorithms for computing
k-truss are inefficient for handling today's massive networks. We first improve
the existing in-memory algorithm for computing k-truss in networks of moderate
size. Then, we propose two I/O-efficient algorithms to handle massive networks
that cannot fit in main memory. Our experiments on real datasets verify the
efficiency of our algorithms and the value of k-truss."
"Location-based services (LBS) have become more and more ubiquitous recently.
Existing methods focus on finding relevant points-of-interest (POIs) based on
users' locations and query keywords. Nowadays, modern LBS applications generate
a new kind of spatio-textual data, regions-of-interest (ROIs), containing
region-based spatial information and textual description, e.g., mobile user
profiles with active regions and interest tags. To satisfy search requirements
on ROIs, we study a new research problem, called spatio-textual similarity
search: Given a set of ROIs and a query ROI, we find the similar ROIs by
considering spatial overlap and textual similarity. Spatio-textual similarity
search has many important applications, e.g., social marketing in
location-aware social networks. It calls for an efficient search method to
support large scales of spatio-textual data in LBS systems. To this end, we
introduce a filter-and-verification framework to compute the answers. In the
filter step, we generate signatures for the ROIs and the query, and utilize the
signatures to generate candidates whose signatures are similar to that of the
query. In the verification step, we verify the candidates and identify the
final answers. To achieve high performance, we generate effective high-quality
signatures, and devise efficient filtering algorithms as well as pruning
techniques. Experimental results on real and synthetic datasets show that our
method achieves high performance."
"Thousands of documents are made available to the users via the web on a daily
basis. One of the most extensively studied problems in the context of such
document streams is burst identification. Given a term t, a burst is generally
exhibited when an unusually high frequency is observed for t. While spatial and
temporal burstiness have been studied individually in the past, our work is the
first to simultaneously track and measure spatiotemporal term burstiness. In
addition, we use the mined burstiness information toward an efficient
document-search engine: given a user's query of terms, our engine returns a
ranked list of documents discussing influential events with a strong
spatiotemporal impact. We demonstrate the efficiency of our methods with an
extensive experimental evaluation on real and synthetic datasets."
"With the advent of reliable positioning technologies and prevalence of
location-based services, it is now feasible to accurately study the propagation
of items such as infectious viruses, sensitive information pieces, and malwares
through a population of moving objects, e.g., individuals, mobile devices, and
vehicles. In such application scenarios, an item passes between two objects
when the objects are sufficiently close (i.e., when they are, so-called, in
contact), and hence once an item is initiated, it can penetrate the object
population through the evolving network of contacts among objects, termed
contact network. In this paper, for the first time we define and study
reachability queries in large (i.e., disk-resident) contact datasets which
record the movement of a (potentially large) set of objects moving in a spatial
environment over an extended time period. A reachability query verifies whether
two objects are ""reachable"" through the evolving contact network represented by
such contact datasets. We propose two contact-dataset indexes that enable
efficient evaluation of such queries despite the potentially humongous size of
the contact datasets. With the first index, termed ReachGrid, at the query time
only a small necessary portion of the contact network which is required for
reachability evaluation is constructed and traversed. With the second approach,
termed ReachGraph, we precompute reachability at different scales and leverage
these precalculations at the query time for efficient query processing. We
optimize the placement of both indexes on disk to enable efficient index
traversal during query processing. We study the pros and cons of our proposed
approaches by performing extensive experiments with both real and synthetic
data. Based on our experimental results, our proposed approaches outperform
existing reachability query processing techniques in contact n...[truncated]."
"There have been intense research interests in moving object indexing in the
past decade. However, existing work did not exploit the important property of
skewed velocity distributions. In many real world scenarios, objects travel
predominantly along only a few directions. Examples include vehicles on road
networks, flights, people walking on the streets, etc. The search space for a
query is heavily dependent on the velocity distribution of the objects grouped
in the nodes of an index tree. Motivated by this observation, we propose the
velocity partitioning (VP) technique, which exploits the skew in velocity
distribution to speed up query processing using moving object indexes. The VP
technique first identifies the ""dominant velocity axes (DVAs)"" using a
combination of principal components analysis (PCA) and k-means clustering.
Then, a moving object index (e.g., a TPR-tree) is created based on each DVA,
using the DVA as an axis of the underlying coordinate system. An object is
maintained in the index whose DVA is closest to the object's current moving
direction. Thus, all the objects in an index are moving in a near 1-dimensional
space instead of a 2-dimensional space. As a result, the expansion of the
search space with time is greatly reduced, from a quadratic function of the
maximum speed (of the objects in the search range) to a near linear function of
the maximum speed. The VP technique can be applied to a wide range of moving
object index structures. We have implemented the VP technique on two
representative ones, the TPR*-tree and the Bx-tree. Extensive experiments
validate that the VP technique consistently improves the performance of those
index structures."
"This paper presents a novel static analysis technique to detect XML
query-update independence, in the presence of a schema. Rather than types, our
system infers chains of types. Each chain represents a path that can be
traversed on a valid document during query/update evaluation. The resulting
independence analysis is precise, although it raises a challenging issue:
recursive schemas may lead to infer infinitely many chains. A sound and
complete approximation technique ensuring a finite analysis in any case is
presented, together with an efficient implementation performing the chain-based
analysis in polynomial space and time."
"Data management systems have traditionally been designed to support either
long-running analytics queries or short-lived transactions, but an increasing
number of applications need both. For example, online games, socio-mobile apps,
and e-commerce sites need to not only maintain operational state, but also
analyze that data quickly to make predictions and recommendations that improve
user experience. In this paper, we present Minuet, a distributed, main-memory
B-tree that supports both transactions and copy-on-write snapshots for in-situ
analytics. Minuet uses main-memory storage to enable low-latency transactional
operations as well as analytics queries without compromising transaction
performance. In addition to supporting read-only analytics queries on
snapshots, Minuet supports writable clones, so that users can create branching
versions of the data. This feature can be quite useful, e.g. to support complex
""what-if"" analysis or to facilitate wide-area replication. Our experiments show
that Minuet outperforms a commercial main-memory database in many ways. It
scales to hundreds of cores and TBs of memory, and can process hundreds of
thousands of B-tree operations per second while executing long-running scans."
"The success of ""infinite-inventory"" retailers such as Amazon.com and Netflix
has been largely attributed to a ""long tail"" phenomenon. Although the majority
of their inventory is not in high demand, these niche products, unavailable at
limited-inventory competitors, generate a significant fraction of total revenue
in aggregate. In addition, tail product availability can boost head sales by
offering consumers the convenience of ""one-stop shopping"" for both their
mainstream and niche tastes. However, most of existing recommender systems,
especially collaborative filter based methods, can not recommend tail products
due to the data sparsity issue. It has been widely acknowledged that to
recommend popular products is easier yet more trivial while to recommend long
tail products adds more novelty yet it is also a more challenging task. In this
paper, we propose a novel suite of graph-based algorithms for the long tail
recommendation. We first represent user-item information with undirected
edge-weighted graph and investigate the theoretical foundation of applying
Hitting Time algorithm for long tail item recommendation. To improve
recommendation diversity and accuracy, we extend Hitting Time and propose
efficient Absorbing Time algorithm to help users find their favorite long tail
items. Finally, we refine the Absorbing Time algorithm and propose two
entropy-biased Absorbing Cost algorithms to distinguish the variation on
different user-item rating pairs, which further enhances the effectiveness of
long tail recommendation. Empirical experiments on two real life datasets show
that our proposed algorithms are effective to recommend long tail items and
outperform state-of-the-art recommendation techniques."
"This paper presents a family of algorithms for fast subset filtering within
ordered sets of integers representing composite keys. Applications include
significant acceleration of (ad-hoc) analytic queries against a data warehouse
without any additional indexing. The algorithms work for point, range and set
restrictions on multiple attributes, in any combination, and are inherently
multidimensional. The main idea consists in intelligent combination of
sequential crawling with jumps over large portions of irrelevant keys. The way
to combine them is adaptive to characteristics of the underlying data store."
"We consider online mining of correlated heavy-hitters from a data stream.
Given a stream of two-dimensional data, a correlated aggregate query first
extracts a substream by applying a predicate along a primary dimension, and
then computes an aggregate along a secondary dimension. Prior work on
identifying heavy-hitters in streams has almost exclusively focused on
identifying heavy-hitters on a single dimensional stream, and these yield
little insight into the properties of heavy-hitters along other dimensions. In
typical applications however, an analyst is interested not only in identifying
heavy-hitters, but also in understanding further properties such as: what other
items appear frequently along with a heavy-hitter, or what is the frequency
distribution of items that appear along with the heavy-hitters. We consider
queries of the following form: In a stream S of (x, y) tuples, on the substream
H of all x values that are heavy-hitters, maintain those y values that occur
frequently with the x values in H. We call this problem as Correlated
Heavy-Hitters (CHH). We formulate an approximate formulation of CHH
identification, and present an algorithm for tracking CHHs on a data stream.
The algorithm is easy to implement and uses workspace which is orders of
magnitude smaller than the stream itself. We present provable guarantees on the
maximum error, as well as detailed experimental results that demonstrate the
space-accuracy trade-off."
"Considerable Progress has been made in the last few years in improving the
performance of the distributed database systems. The development of Fragment
allocation models in Distributed database is becoming difficult due to the
complexity of huge number of sites and their communication considerations.
Under such conditions, simulation of clustering and data allocation is adequate
tools for understanding and evaluating the performance of data allocation in
Distributed databases. Clustering sites and fragment allocation are key
challenges in Distributed database performance, and are considered to be
efficient methods that have a major role in reducing transferred and accessed
data during the execution of applications. In this paper a review on Fragment
allocation by using Clustering technique is given in Distributed Database
System."
"This paper describes practical observations during the Database system Lab.
Oracle 10g DBMS is used in the data base system lab and performed SQL queries
based many concepts like Data Definition Language Commands (DDL), Data
Modification Language Commands ((DML), Views, Integrity Constraints, Aggregate
functions, Joins and Abstract type . While performing practical during the lab
session, many problems occurred, in order to solve them many text books and
websites referred but could not obtain expected help from them. Even though by
spending much time in the database labs with Oracle 10g, tried in numerous
ways, as a final point expected output is achieved. This paper describes
annotations which were experimentally proved in the Database lab."
"The development of novel platforms and techniques for emerging ""Big Data""
applications requires the availability of real-life datasets for data-driven
experiments, which are however out of reach for academic research in most cases
as they are typically proprietary. A possible solution is to use synthesized
datasets that reflect patterns of real ones in order to ensure high quality
experimental findings. A first step in this direction is to use inverse mining
techniques such as inverse frequent itemset mining (IFM) that consists of
generating a transactional database satisfying given support constraints on the
itemsets in an input set, that are typically the frequent ones. This paper
introduces an extension of IFM, called many-sorted IFM, where the schemes for
the datasets to be generated are those typical of Big Tables as required in
emerging big data applications, e.g., social network analytics."
"Data Warehouse (DW) is an essential part of Business Intelligence. DW emerged
as a fast growing reporting and analysis technique in early 1980s. Today, it
has almost replaced relational databases. However, with passage of time, static
and historic data of DWs could not produce Real Time reporting and analysis,
thus giving a way to emerge the Idea of Real Time Data Warehouse (RTDW).
Although, there are problems with RTDWs, but with advancement in technology and
researchers focus, RTDWs will be able to generate real time reports, analysis
and forecasting."
"Data warehouses are nowadays an important component in every competitive
system, it's one of the main components on which business intelligence is
based. We can even say that many companies are climbing to the next level and
use a set of Data warehouses to provide the complete information or it's
generally due to fusion of two or many companies. these Data warehouses can be
heterogeneous and geographically separated, this structure is what we call
federation, and even if the components are physically separated, they are
logically seen as a single component. generally, these items are heterogeneous
which make it difficult to create the logical federation schema,and the
execution of user queries a complicated mission. In this paper, we will fill
this gap by proposing an extension of an existent algorithm in order to treat
different schema types (star, snow flack) including the treatment of
hierarchies dimension using ontology"
"The conventional clustering algorithms mine static databases and generate a
set of patterns in the form of clusters. Many real life databases keep growing
incrementally. For such dynamic databases, the patterns extracted from the
original database become obsolete. Thus the conventional clustering algorithms
are not suitable for incremental databases due to lack of capability to modify
the clustering results in accordance with recent updates. In this paper, the
author proposes a new incremental clustering algorithm called CFICA(Cluster
Feature-Based Incremental Clustering Approach for numerical data) to handle
numerical data and suggests a new proximity metric called Inverse Proximity
Estimate (IPE) which considers the proximity of a data point to a cluster
representative as well as its proximity to a farthest point in its vicinity.
CFICA makes use of the proposed proximity metric to determine the membership of
a data point into a cluster."
"Recent advances in 3D modeling provide us with real 3D datasets to answer
queries, such as ""What is the best position for a new billboard?"" and ""Which
hotel room has the best view?"" in the presence of obstacles. These applications
require measuring and differentiating the visibility of an object (target) from
different viewpoints in a dataspace, e.g., a billboard may be seen from two
viewpoints but is readable only from the viewpoint closer to the target. In
this paper, we formulate the above problem of quantifying the visibility of
(from) a target object from (of) the surrounding area with a visibility color
map (VCM). A VCM is essentially defined as a surface color map of the space,
where each viewpoint of the space is assigned a color value that denotes the
visibility measure of the target from that viewpoint. Measuring the visibility
of a target even from a single viewpoint is an expensive operation, as we need
to consider factors such as distance, angle, and obstacles between the
viewpoint and the target. Hence, a straightforward approach to construct the
VCM that requires visibility computation for every viewpoint of the surrounding
space of the target, is prohibitively expensive in terms of both I/Os and
computation, especially for a real dataset comprising of thousands of
obstacles. We propose an efficient approach to compute the VCM based on a key
property of the human vision that eliminates the necessity of computing the
visibility for a large number of viewpoints of the space. To further reduce the
computational overhead, we propose two approximations; namely, minimum bounding
rectangle and tangential approaches with guaranteed error bounds. Our extensive
experiments demonstrate the effectiveness and efficiency of our solutions to
construct the VCM for real 2D and 3D datasets."
"The goal of multi-objective query optimization (MOQO) is to find query plans
that realize a good compromise between conflicting objectives such as
minimizing execution time and minimizing monetary fees in a Cloud scenario. A
previously proposed exhaustive MOQO algorithm needs hours to optimize even
simple TPC-H queries. This is why we propose several approximation schemes for
MOQO that generate guaranteed near-optimal plans in seconds where exhaustive
optimization takes hours.
  We integrated all MOQO algorithms into the Postgres optimizer and present
experimental results for TPC-H queries; we extended the Postgres cost model and
optimize for up to nine conflicting objectives in our experiments. The proposed
algorithms are based on a formal analysis of typical cost functions that occur
in the context of MOQO. We identify properties that hold for a broad range of
objectives and can be exploited for the design of future MOQO algorithms."
"In recent IoT (Internet of Things) and Web 2.0 technologies, a critical
problem arises with respect to storing and processing the large amount of
collected data. In this paper we develop and evaluate distributed
infrastructures for storing and processing large amount of such data. We
present a distributed framework that supports customized deployment of a
variety of indexing engines over million-node overlays. The proposed framework
provides the appropriate integrated set of tools that allows applications
processing large amount of data, to evaluate and test the performance of
various application protocols for very large scale deployments (multi million
nodes - billions of keys). The key aim is to provide the appropriate
environment that contributes in taking decisions regarding the choice of the
protocol in storage P2P systems for a variety of big data applications. Using
lightweight and efficient collection mechanisms, our system enables real-time
registration of multiple measures, integrating support for real-life parameters
such as node failure models and recovery strategies. Experiments have been
performed at the PlanetLab network and at a typical research laboratory in
order to verify scalability and show maximum re-usability of our setup.
D-P2P-Sim+ framework is publicly available at
http://code.google.com/p/d-p2p-sim/downloads/list."
"We propose shape expression schema (ShEx), a novel schema formalism for
describing the topology of an RDF graph that uses regular bag expressions
(RBEs) to define constraints on the admissible neighborhood for the nodes of a
given type. We provide two alternative semantics, multi- and single-type,
depending on whether or not a node may have more than one type. We study the
expressive power of ShEx and study the complexity of the validation problem. We
show that the single-type semantics is strictly more expressive than the
multi-type semantics, single-type validation is generally intractable and
multi-type validation is feasible for a small class of RBEs. To further curb
the high computational complexity of validation, we propose a natural notion of
determinism and show that multi-type validation for the class of deterministic
schemas using single-occurrence regular bag expressions (SORBEs) is tractable.
Finally, we consider the problem of validating only a fragment of a graph with
preassigned types for some of its nodes, and argue that for deterministic ShEx
using SORBEs, multi-type validation can be performed efficiently and
single-type validation can be performed with a single pass over the graph."
"Adaptive indexing is a concept that considers index creation in databases as
a by-product of query processing; as opposed to traditional full index creation
where the indexing effort is performed up front before answering any queries.
Adaptive indexing has received a considerable amount of attention, and several
algorithms have been proposed over the past few years; including a recent
experimental study comparing a large number of existing methods. Until now,
however, most adaptive indexing algorithms have been designed single-threaded,
yet with multi-core systems already well established, the idea of designing
parallel algorithms for adaptive indexing is very natural. In this regard only
one parallel algorithm for adaptive indexing has recently appeared in the
literature: The parallel version of standard cracking. In this paper we
describe three alternative parallel algorithms for adaptive indexing, including
a second variant of a parallel standard cracking algorithm. Additionally, we
describe a hybrid parallel sorting algorithm, and a NUMA-aware method based on
sorting. We then thoroughly compare all these algorithms experimentally; along
a variant of a recently published parallel version of radix sort. Parallel
sorting algorithms serve as a realistic baseline for multi-threaded adaptive
indexing techniques. In total we experimentally compare seven parallel
algorithms. Additionally, we extensively profile all considered algorithms. The
initial set of experiments considered in this paper indicates that our parallel
algorithms significantly improve over previously known ones. Our results
suggest that, although adaptive indexing algorithms are a good design choice in
single-threaded environments, the rules change considerably in the parallel
case. That is, in future highly-parallel environments, sorting algorithms could
be serious alternatives to adaptive indexing."
"In-memory computing has changed the landscape of database technology. Within
the database and technology field, advancements occur over the course of time
that has had the capacity to transform some fundamental tenants of the
technology and how it is applied. The concept of Database Management Systems
(DBMS) was realized in industry during the 1960s, allowing users and developers
to use a navigational model to access the data stored by the computers of that
day as they grew in speed and capability. This manuscript is specifically
examines the SAPHigh Performance Analytics Appliance(HANA) approach, which is
one of the commonly used technologies today. Additionally, this manuscript
provides the analysis of the first two of the four common main usecases to
utilize SAP HANA's in-memory computing database technology. The performance
benefits are important factors for DB calculations.Some of the benefits are
quantified and the demonstrated by the defined sets of data."
"In this paper, we propose a 2D based partition method for solving the problem
of Ranking under Team Context(RTC) on datasets without a priori. We first map
the data into 2D space using its minimum and maximum value among all
dimensions. Then we construct window queries with consideration of current team
context. Besides, during the query mapping procedure, we can pre-prune some
tuples which are not top ranked ones. This pre-classified step will defer
processing those tuples and can save cost while providing solutions for the
problem. Experiments show that our algorithm performs well especially on large
datasets with correctness."
"Since about 10 years ago, University of Applied Science and Technology (UAST)
in Iran has admitted students in discontinuous associate degree by modular
method, so that almost 100,000 students are accepted every year. Although the
first aim of holding such courses was to improve scientific and skill level of
employees, over time a considerable group of unemployed people have been
interested to participate in these courses. According to this fact, in this
paper, we mine and analyze a sample data of accepted candidates in modular 2008
and 2009 courses by using unsupervised and supervised learning paradigms. In
the first step, by using unsupervised paradigm, we grouped (clustered) set of
modular accepted candidates based on their student status and labeled data sets
by three classes so that each class somehow shows educational and student
status of modular accepted candidates. In the second step, by using supervised
and unsupervised algorithms, we generated predicting models in 2008 data sets.
Then, by making a comparison between performances of generated models, we
selected predicting model of association rules through which some rules were
extracted. Finally, this model is executed for Test set which includes accepted
candidates of next course then by evaluation of results, the percentage of
correctness and confidentiality of obtained results can be viewed."
"Functional dependencies are an integral part of database design. However,
they are only defined when we exclude null markers. Yet we commonly use null
markers in practice. To bridge this gap between theory and practice,
researchers have proposed definitions of functional dependencies over relations
with null markers. Though sound, these definitions lack some qualities that we
find desirable. For example, some fail to satisfy Armstrong's axioms---while
these axioms are part of the foundation of common database methodologies. We
propose a set of properties that any extension of functional dependencies over
relations with null markers should possess. We then propose two new extensions
having these properties. These extensions attempt to allow null markers where
they make sense to practitioners.
  They both support Armstrong's axioms and provide realizable null markers: at
any time, some or all of the null markers can be replaced by actual values
without causing an anomaly. Our proposals may improve database designs."
"In this work, we present EAGr, a system for supporting large numbers of
continuous neighborhood-based (""ego-centric"") aggregate queries over large,
highly dynamic, and rapidly evolving graphs. Examples of such queries include
computation of personalized, tailored trends in social networks, anomaly/event
detection in financial transaction networks, local search and alerts in
spatio-temporal networks, to name a few. Key challenges in supporting such
continuous queries include high update rates typically seen in these
situations, large numbers of queries that need to be executed simultaneously,
and stringent low latency requirements. We propose a flexible, general, and
extensible in-memory framework for executing different types of ego-centric
aggregate queries over large dynamic graphs with low latencies. Our framework
is built around the notion of an aggregation overlay graph, a pre-compiled data
structure that encodes the computations to be performed when an update/query is
received. The overlay graph enables sharing of partial aggregates across
multiple ego-centric queries (corresponding to the nodes in the graph), and
also allows partial pre-computation of the aggregates to minimize the query
latencies. We present several highly scalable techniques for constructing an
overlay graph given an aggregation function, and also design incremental
algorithms for handling structural changes to the underlying graph. We also
present an optimal, polynomial-time algorithm for making the pre-computation
decisions given an overlay graph, and evaluate an approach to incrementally
adapt those decisions as the workload changes. Although our approach is
naturally parallelizable, we focus on a single-machine deployment and show that
our techniques can easily handle graphs of size up to 320 million nodes and
edges, and achieve update/query throughputs of over 500K/s using a single,
powerful machine."
"This document is an white paper about how to connect reverse engineering and
programing skills to extract data from a proprietary implementation of a
database system to build EML-Tools for data format conversion into raw data.
This article shows how to access data of a source software system without any
interface for data conversion. We discuss how raw data can be transfered into
structural format by using XML or any other custom designed software solution.
For demonstration purposes only, we will use a CRM system called Harmony(r) by
Harmony(r) Software AG, the programing language Python and methods of computer
security, which are used to get quick access to the raw data.
  All trademarks are property of their owners, as Harmony(r) is of Harmony
Software AG."
"In this work we establish and investigate the connections between causality
for query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new problems in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes and the
other way around. The vast body of research on database repairs can be applied
to the newer problem of determining actual causes for query answers. By
formulating a causality problem as a diagnosis problem, we manage to
characterize causes in terms of a system's diagnoses."
"Tracking and approximating data matrices in streaming fashion is a
fundamental challenge. The problem requires more care and attention when data
comes from multiple distributed sites, each receiving a stream of data. This
paper considers the problem of ""tracking approximations to a matrix"" in the
distributed streaming model. In this model, there are m distributed sites each
observing a distinct stream of data (where each element is a row of a
distributed matrix) and has a communication channel with a coordinator, and the
goal is to track an eps-approximation to the norm of the matrix along any
direction. To that end, we present novel algorithms to address the matrix
approximation problem. Our algorithms maintain a smaller matrix B, as an
approximation to a distributed streaming matrix A, such that for any unit
vector x: | ||A x||^2 - ||B x||^2 | <= eps ||A||_F^2. Our algorithms work in
streaming fashion and incur small communication, which is critical for
distributed computation. Our best method is deterministic and uses only
O((m/eps) log(beta N)) communication, where N is the size of stream (at the
time of the query) and beta is an upper-bound on the squared norm of any row of
the matrix. In addition to proving all algorithmic properties theoretically,
extensive experiments with real large datasets demonstrate the efficiency of
these protocols."
"We consider the problem of similarity search within a set of top-k lists
under the Kendall's Tau distance function. This distance describes how related
two rankings are in terms of concordantly and discordantly ordered items. As
top-k lists are usually very short compared to the global domain of possible
items to be ranked, creating an inverted index to look up overlapping lists is
possible but does not capture tight enough the similarity measure. In this
work, we investigate locality sensitive hashing schemes for the Kendall's Tau
distance and evaluate the proposed methods using two real-world datasets."
"Relational databases have limited support for data collaboration, where teams
collaboratively curate and analyze large datasets. Inspired by software version
control systems like git, we propose (a) a dataset version control system,
giving users the ability to create, branch, merge, difference and search large,
divergent collections of datasets, and (b) a platform, DataHub, that gives
users the ability to perform collaborative data analysis building on this
version control system. We outline the challenges in providing dataset version
control at scale."
"We present a complete logic for reasoning with functional dependencies (FDs)
with semantics defined over classes of commutative integral partially ordered
monoids and complete residuated lattices. The dependencies allow us to express
stronger relationships between attribute values than the ordinary FDs. In our
setting, the dependencies not only express that certain values are determined
by others but also express that similar values of attributes imply similar
values of other attributes. We show complete axiomatization using a system of
Armstrong-like rules, comment on related computational issues, and the
relational vs. propositional semantics of the dependencies."
"Mining labeled subgraph is a popular research task in data mining because of
its potential application in many different scientific domains. All the
existing methods for this task explicitly or implicitly solve the subgraph
isomorphism task which is computationally expensive, so they suffer from the
lack of scalability problem when the graphs in the input database are large. In
this work, we propose FS^3, which is a sampling based method. It mines a small
collection of subgraphs that are most frequent in the probabilistic sense. FS^3
performs a Markov Chain Monte Carlo (MCMC) sampling over the space of a
fixed-size subgraphs such that the potentially frequent subgraphs are sampled
more often. Besides, FS^3 is equipped with an innovative queue manager. It
stores the sampled subgraph in a finite queue over the course of mining in such
a manner that the top-k positions in the queue contain the most frequent
subgraphs. Our experiments on database of large graphs show that FS^3 is
efficient, and it obtains subgraphs that are the most frequent amongst the
subgraphs of a given size."
"We aim to provide table answers to keyword queries against knowledge bases.
For queries referring to multiple entities, like ""Washington cities population""
and ""Mel Gibson movies"", it is better to represent each relevant answer as a
table which aggregates a set of entities or entity-joins within the same table
scheme or pattern. In this paper, we study how to find highly relevant patterns
in a knowledge base for user-given keyword queries to compose table answers. A
knowledge base can be modeled as a directed graph called knowledge graph, where
nodes represent entities in the knowledge base and edges represent the
relationships among them. Each node/edge is labeled with type and text. A
pattern is an aggregation of subtrees which contain all keywords in the texts
and have the same structure and types on node/edges. We propose efficient
algorithms to find patterns that are relevant to the query for a class of
scoring functions. We show the hardness of the problem in theory, and propose
path-based indexes that are affordable in memory. Two query-processing
algorithms are proposed: one is fast in practice for small queries (with small
patterns as answers) by utilizing the indexes; and the other one is better in
theory, with running time linear in the sizes of indexes and answers, which can
handle large queries better. We also conduct extensive experimental study to
compare our approaches with a naive adaption of known techniques."
"In data warehousing, Extract-Transform-Load (ETL) extracts the data from data
sources into a central data warehouse regularly for the support of business
decision-makings. The data from transaction processing systems are featured
with the high frequent changes of insertion, update, and deletion. It is
challenging for ETL to propagate the changes to the data warehouse, and
maintain the change history. Moreover, ETL jobs typically run in a sequential
order when processing the data with dependencies, which is not optimal, \eg,
when processing early-arriving data. In this paper, we propose a two-level data
staging ETL for handling transaction data. The proposed method detects the
changes of the data from transactional processing systems, identifies the
corresponding operation codes for the changes, and uses two staging databases
to facilitate the data processing in an ETL process. The proposed ETL provides
the ""one-stop"" method for fast-changing, slowly-changing and early-arriving
data processing."
"Extract-Transform-Load (ETL) handles large amount of data and manages
workload through dataflows. ETL dataflows are widely regarded as complex and
expensive operations in terms of time and system resources. In order to
minimize the time and the resources required by ETL dataflows, this paper
presents a framework to optimize dataflows using shared cache and
parallelization techniques. The framework classifies the components in an ETL
dataflow into different categories based on their data operation properties.
The framework then partitions the dataflow based on the classification at
different granularities. Furthermore, the framework applies optimization
techniques such as cache re-using, pipelining and multi-threading to the
already-partitioned dataflows. The proposed techniques reduce system memory
footprint and the frequency of copying data between different components, and
also take full advantage of the computing power of multi-core processors. The
experimental results show that the proposed optimization framework is 4.7 times
faster than the ordinary ETL dataflows (without using the proposed optimization
techniques), and outperforms the similar tool (Kettle)."
"Database analytics algorithms leverage quantifiable structural properties of
the data to predict interesting concepts and relationships. The same
information, however, can be represented using many different structures and
the structural properties observed over particular representations do not
necessarily hold for alternative structures. Thus, there is no guarantee that
current database analytics algorithms will still provide the correct insights,
no matter what structures are chosen to organize the database. Because these
algorithms tend to be highly effective over some choices of structure, such as
that of the databases used to validate them, but not so effective with others,
database analytics has largely remained the province of experts who can find
the desired forms for these algorithms. We argue that in order to make database
analytics usable, we should use or develop algorithms that are effective over a
wide range of choices of structural organizations. We introduce the notion of
representation independence, study its fundamental properties for a wide range
of data analytics algorithms, and empirically analyze the amount of
representation independence of some popular database analytics algorithms. Our
results indicate that most algorithms are not generally representation
independent and find the characteristics of more representation independent
heuristics under certain representational shifts."
"Directions and paths, as commonly provided by navigation systems, are usually
derived considering absolute metrics, e.g., finding the shortest path within an
underlying road network. With the aid of crowdsourced geospatial data we aim at
obtaining paths that do not only minimize distance but also lead through more
popular areas using knowledge generated by users. We extract spatial relations
such as ""nearby"" or ""next to"" from travel blogs, that define closeness between
pairs of points of interest (PoIs) and quantify each of these relations using a
probabilistic model. Subsequently, we create a relationship graph where each
node corresponds to a PoI and each edge describes the spatial connection
between the respective PoIs. Using Bayesian inference we obtain a probabilistic
measure of spatial closeness according to the crowd. Applying this measure to
the corresponding road network, we obtain an altered cost function which does
not exclusively rely on distance, and enriches an actual road networks taking
crowdsourced spatial relations into account. Finally, we propose two routing
algorithms on the enriched road networks. To evaluate our approach, we use
Flickr photo data as a ground truth for popularity. Our experimental results --
based on real world datasets -- show that the paths computed w.r.t.\ our
alternative cost function yield competitive solutions in terms of path length
while also providing more ""popular"" paths, making routing easier and more
informative for the user."
"Spatial data is playing an emerging role in new technologies such as web and
mobile mapping and Geographic Information Systems (GIS). Important decisions in
political, social and many other aspects of modern human life are being made
using location data. Decision makers in many countries are exploiting spatial
databases for collecting information, analyzing them and planning for the
future. In fact, not every spatial database is suitable for this type of
application. Inaccuracy, imprecision and other deficiencies are present in
location data just as any other type of data and may have a negative impact on
credibility of any action taken based on unrefined information. So we need a
method for evaluating the quality of spatial data and separating usable data
from misleading data which leads to weak decisions. On the other hand, spatial
databases are usually huge in size and therefore working with this type of data
has a negative impact on efficiency. To improve the efficiency of working with
spatial big data, we need a method for shrinking the volume of data. Sampling
is one of these methods, but its negative effects on the quality of data are
inevitable. In this paper we are trying to show and assess this change in
quality of spatial data that is a consequence of sampling. We used this
approach for evaluating the quality of sampled spatial data related to mobile
user trajectories in China which are available in a well-known spatial
database. The results show that sample-based control of data quality will
increase the query performance significantly, without losing too much accuracy.
Based on this results some future improvements are pointed out which will help
to process location-based queries faster than before and to make more accurate
location-based decisions in limited times."
"Both the notion of Property Graphs (PG) and the Resource Description
Framework (RDF) are commonly used models for representing graph-shaped data.
While there exist some system-specific solutions to convert data from one model
to the other, these solutions are not entirely compatible with one another and
none of them appears to be based on a formal foundation. In fact, for the PG
model, there does not even exist a commonly agreed-upon formal definition.
  The aim of this document is to reconcile both models formally. To this end,
the document proposes a formalization of the PG model and introduces
well-defined transformations between PGs and RDF. As a result, the document
provides a basis for the following two innovations: On one hand, by
implementing the RDF-to-PG transformations defined in this document, PG-based
systems can enable their users to load RDF data and make it accessible in a
compatible, system-independent manner using, e.g., the graph traversal language
Gremlin or the declarative graph query language Cypher. On the other hand, the
PG-to-RDF transformation in this document enables RDF data management systems
to support compatible, system-independent queries over the content of Property
Graphs by using the standard RDF query language SPARQL. Additionally, this
document represents a foundation for systematic research on relationships
between the two models and between their query languages."
"In spite of its fundamental importance, inference has not been an inherent
function of multidimensional models and analytical applications. These models
are mainly aimed at numeric (quantitative) analysis where the notions of
inference and semantics are not well defined. In this paper we argue that
inference can be and should be integral part of multidimensional data models
and analytical applications. It is demonstrated how inference can be defined
using only multidimensional terms like axes and coordinates as opposed to using
logic-based approaches. We propose a novel approach to inference in
multidimensional space based on the concept-oriented model of data and
introduce elementary operations which are then used to define constraint
propagation and inference procedures. We describe a query language with
inference operator and demonstrate its usefulness in solving complex analytical
tasks."
"We present a series of novel techniques and algorithms for transaction
commit, logging, recovery, and propagation control. In combination, they
provide a recovery component that maintains the persistent state of the
database (both log and data pages) always in a committed state. Recovery from
system and media failures only requires only REDO operations, which can happen
concurrently with the processing of new transactions. The mechanism supports
fine-granularity locking, partial rollbacks, and snapshot isolation for reader
transactions. Our design does not assume a specific hardware configuration such
as non-volatile RAM or flash---it is designed for traditional disk
environments. Nevertheless, it can exploit modern I/O devices for higher
transaction throughput and reduced recovery time with a high degree of
flexibility."
"To support complex data-intensive applications such as personalized
recommendations, targeted advertising, and intelligent services, the data
management community has focused heavily on the design of systems to support
training complex models on large datasets. Unfortunately, the design of these
systems largely ignores a critical component of the overall analytics process:
the deployment and serving of models at scale. In this work, we present Velox,
a new component of the Berkeley Data Analytics Stack. Velox is a data
management system for facilitating the next steps in real-world, large-scale
analytics pipelines: online model management, maintenance, and serving. Velox
provides end-user applications and services with a low-latency, intuitive
interface to models, transforming the raw statistical models currently trained
using existing offline large-scale compute frameworks into full-blown,
end-to-end data products capable of recommending products, targeting
advertisements, and personalizing web content. To provide up-to-date results
for these complex models, Velox also facilitates lightweight online model
maintenance and selection (i.e., dynamic weighting). In this paper, we describe
the challenges and architectural considerations required to achieve this
functionality, including the abilities to span online and offline systems, to
adaptively adjust model materialization strategies, and to exploit inherent
statistical properties such as model error tolerance, all while operating at
""Big Data"" scale."
"Efficient management of RDF data plays an important role in successfully
understanding and fast querying data. Although the current approaches of
indexing in RDF Triples such as property tables and vertically partitioned
solved many issues; however, they still suffer from the performance in the
complex self-join queries and insert data in the same table. As an improvement
in this paper, we propose an alternative solution to facilitate flexibility and
efficiency in that queries and try to reach to the optimal solution to decrease
the self-joins as much as possible, this solution based on the idea of
""Recursive Mapping of Twin Tables"". Our main goal of Recursive Mapping of Twin
Tables (RMTT) approach is divided the main RDF Triple into two tables which
have the same structure of RDF Triple and insert the RDF data recursively. Our
experimental results compared the performance of join queries in vertically
partitioned approach and the RMTT approach using very large RDF data, like DBLP
and DBpedia datasets. Our experimental results with a number of complex
submitted queries shows that our approach is highly scalable compared with
RDF-3X approach and RMTT reduces the number of self-joins especially in complex
queries 3-4 times than RDF-3X approach"
"As declarative query processing techniques expand in scope --- to the Web,
data streams, network routers, and cloud platforms --- there is an increasing
need for adaptive query processing techniques that can re-plan in the presence
of failures or unanticipated performance changes. A status update on the data
distributions or the compute nodes may have significant repercussions on the
choice of which query plan should be running. Ideally, new system architectures
would be able to make cost-based decisions about reallocating work, migrating
data, etc., and react quickly as real-time status information becomes
available. Existing cost-based query optimizers are not incremental in nature,
and must be run ""from scratch"" upon each status or cost update. Hence, they
generally result in adaptive schemes that can only react slowly to updates.
  An open question has been whether it is possible to build a cost-based
re-optimization architecture for adaptive query processing in a streaming or
repeated query execution environment, e.g., by incrementally updating optimizer
state given new cost information. We show that this can be achieved
beneficially, especially for stream processing workloads. Our techniques build
upon the recently proposed approach of formulating query plan enumeration as a
set of recursive datalog queries; we develop a variety of novel optimization
approaches to ensure effective pruning in both static and incremental cases. We
implement our solution within an existing research query processing system, and
show that it effectively supports cost-based initial optimization as well as
frequent adaptivity."
"A fundamental problem in data fusion is to determine the veracity of
multi-source data in order to resolve conflicts. While previous work in truth
discovery has proved to be useful in practice for specific settings, sources'
behavior or data set characteristics, there has been limited systematic
comparison of the competing methods in terms of efficiency, usability, and
repeatability. We remedy this deficit by providing a comprehensive review of 12
state-of-the art algorithms for truth discovery. We provide reference
implementations and an in-depth evaluation of the methods based on extensive
experiments on synthetic and real-world data. We analyze aspects of the problem
that have not been explicitly studied before, such as the impact of
initialization and parameter setting, convergence, and scalability. We provide
an experimental framework for extensively comparing the methods in a wide range
of truth discovery scenarios where source coverage, numbers and distributions
of conflicts, and true positive claims can be controlled and used to evaluate
the quality and performance of the algorithms. Finally, we report comprehensive
findings obtained from the experiments and provide new insights for future
research."
"Clustering has become an increasingly important task in analysing huge
amounts of data. Traditional applications require that all data has to be
located at the site where it is scrutinized. Nowadays, large amounts of
heterogeneous, complex data reside on different, independently working
computers which are connected to each other via local or wide area networks. In
this paper, we propose a scalable density-based distributed clustering
algorithm which allows a user-defined trade-off between clustering quality and
the number of transmitted objects from the different local sites to a global
server site. Our approach consists of the following steps: First, we order all
objects located at a local site according to a quality criterion reflecting
their suitability to serve as local representatives. Then we send the best of
these representatives to a server site where they are clustered with a slightly
enhanced density-based clustering algorithm. This approach is very efficient,
because the local detemination of suitable representatives can be carried out
quickly and independently from each other. Furthermore, based on the scalable
number of the most suitable local representatives, the global clustering can be
done very effectively and efficiently. In our experimental evaluation, we will
show that our new scalable density-based distributed clustering approach
results in high quality clusterings with scalable transmission cost."
"This paper presents Clustering based on Near Neighbor Influence (CNNI), a new
clustering algorithm which is inspired by the idea of near neighbor and the
superposition principle of influence. In order to clearly describe this
algorithm, it introduces some important concepts, such as near neighbor point
set, near neighbor influence, and similarity measure. By simulated experiments
of some artificial data sets and seven real data sets, we observe that this
algorithm can often get good clustering quality when making proper value of
some parameters. At last, it gives some research expectations to popularize
this algorithm."
"We present a browser application for estimating the number of frequent
patterns, in particular itemsets, as well as the pattern frequency spectrum.
The pattern frequency spectrum is defined as the function that shows for every
value of the frequency threshold $\sigma$ the number of patterns that are
frequent in a given dataset. Our demo implements a recent algorithm proposed by
the authors for finding the spectrum. The demo is 100% JavaScript, and runs in
all modern browsers. We observe that modern JavaScript engines can deliver
performance that makes it viable to run non-trivial data analysis algorithms in
browser applications."
"In the SIGMOD 2013 conference, we published a paper extending our earlier
work on crowdsourced entity resolution to improve crowdsourced join processing
by exploiting transitive relationships [Wang et al. 2013]. The VLDB 2014
conference has a paper that follows up on our previous work [Vesdapunt et al.,
2014], which points out and corrects a mistake we made in our SIGMOD paper.
Specifically, in Section 4.2 of our SIGMOD paper, we defined the ""Expected
Optimal Labeling Order"" (EOLO) problem, and proposed an algorithm for solving
it. We incorrectly claimed that our algorithm is optimal. In their paper,
Vesdapunt et al. show that the problem is actually NP-Hard, and based on that
observation, propose a new algorithm to solve it. In this note, we would like
to put the Vesdapunt et al. results in context, something we believe that their
paper does not adequately do."
"Bitmap indexes must be compressed to reduce input/output costs and minimize
CPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use
techniques based on run-length encoding (RLE), such as Word-Aligned Hybrid
(WAH) compression. These techniques are sensitive to the order of the rows: a
simple lexicographical sort can divide the index size by 9 and make indexes
several times faster. We investigate reordering heuristics based on computed
attribute-value histograms. Simply permuting the columns of the table based on
these histograms can increase the sorting efficiency by 40%."
"We address the problem of finding a ""best"" deterministic query answer to a
query over a probabilistic database. For this purpose, we propose the notion of
a consensus world (or a consensus answer) which is a deterministic world
(answer) that minimizes the expected distance to the possible worlds (answers).
This problem can be seen as a generalization of the well-studied inconsistent
information aggregation problems (e.g. rank aggregation) to probabilistic
databases. We consider this problem for various types of queries including SPJ
queries, \Topk queries, group-by aggregate queries, and clustering. For
different distance metrics, we obtain polynomial time optimal or approximation
algorithms for computing the consensus answers (or prove NP-hardness). Most of
our results are for a general probabilistic database model, called {\em and/xor
tree model}, which significantly generalizes previous probabilistic database
models like x-tuples and block-independent disjoint models, and is of
independent interest."
"We consider the problem of finding equivalent minimal-size reformulations of
SQL queries in presence of embedded dependencies [1]. Our focus is on
select-project-join (SPJ) queries with equality comparisons, also known as safe
conjunctive (CQ) queries, possibly with grouping and aggregation. For SPJ
queries, the semantics of the SQL standard treat query answers as multisets
(a.k.a. bags), whereas the stored relations may be treated either as sets,
which is called bag-set semantics for query evaluation, or as bags, which is
called bag semantics. (Under set semantics, both query answers and stored
relations are treated as sets.)
  In the context of the above Query-Reformulation Problem, we develop a
comprehensive framework for equivalence of CQ queries under bag and bag-set
semantics in presence of embedded dependencies, and make a number of conceptual
and technical contributions. Specifically, we develop equivalence tests for CQ
queries in presence of arbitrary sets of embedded dependencies under bag and
bag-set semantics, under the condition that chase [9] under set semantics
(set-chase) on the inputs terminates. We also present equivalence tests for
aggregate CQ queries in presence of embedded dependencies. We use our
equivalence tests to develop sound and complete (whenever set-chase on the
inputs terminates) algorithms for solving instances of the Query-Reformulation
Problem with CQ queries under each of bag and bag-set semantics, as well as for
instances of the problem with aggregate queries."
"There has been much research activity in recent times about providing the
data infrastructures needed for the provision of personalised healthcare. In
particular the requirement of integrating multiple, potentially distributed,
heterogeneous data sources in the medical domain for the use of clinicians has
set challenging goals for the healthgrid community. The approach advocated in
this paper surrounds the provision of an Integrated Data Model plus links
to/from ontologies to homogenize biomedical (from genomic, through cellular,
disease, patient and population-related) data in the context of the EC
Framework 6 Health-e-Child project. Clinical requirements are identified, the
design approach in constructing the model is detailed and the integrated model
described in the context of examples taken from that project. Pointers are
given to future work relating the model to medical ontologies and challenges to
the use of fully integrated models and ontologies are identified."
"End users of recent biomedical information systems are often unaware of the
storage structure and access mechanisms of the underlying data sources and can
require simplified mechanisms for writing domain specific complex queries. This
research aims to assist users and their applications in formulating queries
without requiring complete knowledge of the information structure of underlying
data sources. To achieve this, query reformulation techniques and algorithms
have been developed that can interpret ontology-based search criteria and
associated domain knowledge in order to reformulate a relational query. These
query reformulation algorithms exploit the semantic relationships and assertion
capabilities of OWL-DL based domain ontologies for query reformulation. In this
paper, this approach is applied to the integrated database schema of the EU
funded Health-e-Child (HeC) project with the aim of providing ontology assisted
query reformulation techniques to simplify the global access that is needed to
millions of medical records across the UK and Europe."
"In an economic environment more and more competitive, the effective
management of information and knowledge is a strategic issue for industrial
enterprises. In the global marketplace, companies must use reactive strategies
and reduce their products development cycle. In this context, the PLM (Product
Lifecycle Management) is considered as a key component of the information
system. The aim of this paper is to present an approach to integrate Business
Processes in a PLM system. This approach is implemented in automotive sector
with second-tier subcontractor"
"The SPARQL query language is a recent W3C standard for processing RDF data, a
format that has been developed to encode information in a machine-readable way.
We investigate the foundations of SPARQL query optimization and (a) provide
novel complexity results for the SPARQL evaluation problem, showing that the
main source of complexity is operator OPTIONAL alone; (b) propose a
comprehensive set of algebraic query rewriting rules; (c) present a framework
for constraint-based SPARQL optimization based upon the well-known chase
procedure for Conjunctive Query minimization. In this line, we develop two
novel termination conditions for the chase. They subsume the strongest
conditions known so far and do not increase the complexity of the recognition
problem, thus making a larger class of both Conjunctive and SPARQL queries
amenable to constraint-based optimization. Our results are of immediate
practical interest and might empower any SPARQL query optimizer."
"This is a proposal of an algebra which aims at distributed array processing.
The focus lies on re-arranging and distributing array data, which may be
multi-dimensional. The context of the work is scientific processing; thus, the
core science operations are assumed to be taken care of in external libraries
or languages. A main design driver is the desire to carry over some of the
strategies of the relational algebra into the array domain."
"This paper presents the use of a computer application based on a MySQL
database, managed by PHP programs, allowing the selection of a heating device
using coefficient-based calculus."
"We present a new method for computing core universal solutions in data
exchange settings specified by source-to-target dependencies, by means of SQL
queries. Unlike previously known algorithms, which are recursive in nature, our
method can be implemented directly on top of any DBMS. Our method is based on
the new notion of a laconic schema mapping. A laconic schema mapping is a
schema mapping for which the canonical universal solution is the core universal
solution. We give a procedure by which every schema mapping specified by FO s-t
tgds can be turned into a laconic schema mapping specified by FO s-t tgds that
may refer to a linear order on the domain of the source instance. We show that
our results are optimal, in the sense that the linear order is necessary and
the method cannot be extended to schema mapping involving target constraints."
"The concept of matching dependencies (mds) is recently pro- posed for
specifying matching rules for object identification. Similar to the functional
dependencies (with conditions), mds can also be applied to various data quality
applications such as violation detection. In this paper, we study the problem
of discovering matching dependencies from a given database instance. First, we
formally define the measures, support and confidence, for evaluating utility of
mds in the given database instance. Then, we study the discovery of mds with
certain utility requirements of support and confidence. Exact algorithms are
developed, together with pruning strategies to improve the time performance.
Since the exact algorithm has to traverse all the data during the computation,
we propose an approximate solution which only use some of the data. A bound of
relative errors introduced by the approximation is also developed. Finally, our
experimental evaluation demonstrates the efficiency of the proposed methods."
"The objective of this paper is to show how the interrogation processor
responds to SQL interrogation. The interrogation processor is split into two
parts. The first, called the interrogation compiler translates an SQL query
into a plan of physical execution. The second, called evaluation query runs the
execution plan."
"Youtopia is a platform for collaborative management and integration of
relational data. At the heart of Youtopia is an update exchange abstraction:
changes to the data propagate through the system to satisfy user-specified
mappings. We present a novel change propagation model that combines a
deterministic chase with human intervention. The process is fundamentally
cooperative and gives users significant control over how mappings are repaired.
An additional advantage of our model is that mapping cycles can be permitted
without compromising correctness.
  We investigate potential harmful interference between updates in our model;
we introduce two appropriate notions of serializability that avoid such
interference if enforced. The first is very general and related to classical
final-state serializability; the second is more restrictive but highly
practical and related to conflict-serializability. We present an algorithm to
enforce the latter notion. Our algorithm is an optimistic one, and as such may
sometimes require updates to be aborted. We develop techniques for reducing the
number of aborts and we test these experimentally."
"Many enterprise environments have databases running on network-attached
server-storage infrastructure (referred to as Storage Area Networks or SANs).
Both the database and the SAN are complex systems that need their own separate
administrative teams. This paper puts forth the vision of an innovative
management framework to simplify administrative tasks that require an in-depth
understanding of both the database and the SAN. As a concrete instance, we
consider the task of diagnosing the slowdown in performance of a database query
that is executed multiple times (e.g., in a periodic report-generation
setting). This task is very challenging because the space of possible causes
includes problems specific to the database, problems specific to the SAN, and
problems that arise due to interactions between the two systems. In addition,
the monitoring data available from these systems can be noisy.
  We describe the design of DIADS which is an integrated diagnosis tool for
database and SAN administrators. DIADS generates and uses a powerful
abstraction called Annotated Plan Graphs (APGs) that ties together the
execution path of queries in the database and the SAN. Using an innovative
workflow that combines domain-specific knowledge with machine-learning
techniques, DIADS was applied successfully to diagnose query slowdowns caused
by complex combinations of events across a PostgreSQL database and a production
SAN."
"In the recent years, a lot of attention has been paid to the development of
solid foundations for the composition and inversion of schema mappings. In this
paper, we review the proposals for the semantics of these crucial operators.
For each of these proposals, we concentrate on the three following problems:
the definition of the semantics of the operator, the language needed to express
the operator, and the algorithmic issues associated to the problem of computing
the operator. It should be pointed out that we primarily consider the
formalization of schema mappings introduced in the work on data exchange. In
particular, when studying the problem of computing the composition and inverse
of a schema mapping, we will be mostly interested in computing these operators
for mappings specified by source-to-target tuple-generating dependencies."
"Real-time databases deal with time-constrained data and time-constrained
transactions. The design of this kind of databases requires the introduction of
new concepts to support both data structures and the dynamic behaviour of the
database. In this paper, we give an overview about different aspects of
real-time databases and we clarify requirements of their modelling. Then, we
present a framework for real-time database design and describe its fundamental
operations. A case study demonstrates the validity of the structural model and
illustrates SQL queries and Java code generated from the classes of the model"
"Certainly, nowadays knowledge discovery or extracting knowledge from large
amount of data is a desirable task in competitive businesses. Data mining is a
main step in knowledge discovery process. Meanwhile frequent patterns play
central role in data mining tasks such as clustering, classification, and
association analysis. Identifying all frequent patterns is the most time
consuming process due to a massive number of candidate patterns. For the past
decade there have been an increasing number of efficient algorithms to mine the
frequent patterns. However reducing the number of candidate patterns and
comparisons for support counting are still two problems in this field which
have made the frequent pattern mining one of the active research themes in data
mining. A reasonable solution is identifying a small candidate pattern set from
which can generate all frequent patterns. In this paper, a method is proposed
based on a new candidate set called candidate head set or H which forms a small
set of candidate patterns. The experimental results verify the accuracy of the
proposed method and reduction of the number of candidate patterns and
comparisons."
"With the growing focus on semantic searches and interpretations, an
increasing number of standardized vocabularies and ontologies are being
designed and used to describe data. We investigate the querying of objects
described by a tree-structured ontology. Specifically, we consider the case of
finding the top-k best pairs of objects that have been annotated with terms
from such an ontology when the object descriptions are available only at
runtime. We consider three distance measures. The first one defines the object
distance as the minimum pairwise distance between the sets of terms describing
them, and the second one defines the distance as the average pairwise term
distance. The third and most useful distance measure, earth mover's distance,
finds the best way of matching the terms and computes the distance
corresponding to this best matching. We develop lower bounds that can be
aggregated progressively and utilize them to speed up the search for top-k
object pairs when the earth mover's distance is used. For the minimum pairwise
distance, we devise an algorithm that runs in O(D + Tk log k) time, where D is
the total information size and T is the total number of terms in the ontology.
We also develop a novel best-first search strategy for the average pairwise
distance that utilizes lower bounds generated in an ordered manner. Experiments
on real and synthetic datasets demonstrate the practicality and scalability of
our algorithms."
"The problem of developing models and algorithms for multilevel association
mining pose for new challenges for mathematics and computer science. These
problems become more challenging, when some form of uncertainty like fuzziness
is present in data or relationships in data. This paper proposes a multilevel
fuzzy association rule mining models for extracting knowledge implicit in
transactions database with different support at each level. The proposed
algorithm adopts a top-down progressively deepening approach to derive large
itemsets. This approach incorporates fuzzy boundaries instead of sharp boundary
intervals. An example is also given to demonstrate that the proposed mining
algorithm can derive the multiple-level association rules under different
supports in a simple and effective manner."
"This paper proposes a multi agent system by compiling two technologies, query
processing optimization and agents which contains features of personalized
queries and adaption with changing of requirements. This system uses a new
algorithm based on modeling of users' long-term requirements and also GA to
gather users' query data. Experimented Result shows more adaption capability
for presented algorithm in comparison with classic algorithms."
"Flash memory is widely used as the secondary storage in lightweight computing
devices due to its outstanding advantages over magnetic disks. Flash memory has
many access characteristics different from those of magnetic disks, and how to
take advantage of them is becoming an important research issue. There are two
existing approaches to storing data into flash memory: page-based and
log-based. The former has good performance for read operations, but poor
performance for write operations. In contrast, the latter has good performance
for write operations when updates are light, but poor performance for read
operations. In this paper, we propose a new method of storing data, called
page-differential logging, for flash-based storage systems that solves the
drawbacks of the two methods. The primary characteristics of our method are:
(1) writing only the difference (which we define as the page-differential)
between the original page in flash memory and the up-to-date page in memory;
(2) computing and writing the page-differential only once at the time the page
needs to be reflected into flash memory. The former contrasts with existing
page-based methods that write the whole page including both changed and
unchanged parts of data or from log-based ones that keep track of the history
of all the changes in a page. Our method allows existing disk-based DBMSs to be
reused as flash-based DBMSs just by modifying the flash memory driver, i.e., it
is DBMS-independent. Experimental results show that the proposed method
improves the I/O performance by 1.2 ~ 6.1 times over existing methods for the
TPC-C data of approximately 1 Gbytes."
"In this article, we describe the XML storage system used in the WebContent
project. We begin by advocating the use of an XML database in order to store
WebContent documents, and we present two different ways of storing and querying
these documents : the use of a centralized XML database and the use of a P2P
XML database."
"The increasing popularity of social networks has initiated a fertile research
area in information extraction and data mining. Anonymization of these social
graphs is important to facilitate publishing these data sets for analysis by
external entities. Prior work has concentrated mostly on node identity
anonymization and structural anonymization. But with the growing interest in
analyzing social networks as a weighted network, edge weight anonymization is
also gaining importance. We present An\'onimos, a Linear Programming based
technique for anonymization of edge weights that preserves linear properties of
graphs. Such properties form the foundation of many important graph-theoretic
algorithms such as shortest paths problem, k-nearest neighbors, minimum cost
spanning tree, and maximizing information spread. As a proof of concept, we
apply An\'onimos to the shortest paths problem and its extensions, prove the
correctness, analyze complexity, and experimentally evaluate it using real
social network data sets. Our experiments demonstrate that An\'onimos
anonymizes the weights, improves k-anonymity of the weights, and also scrambles
the relative ordering of the edges sorted by weights, thereby providing robust
and effective anonymization of the sensitive edge-weights. Additionally, we
demonstrate the composability of different models generated using An\'onimos, a
property that allows a single anonymized graph to preserve multiple linear
properties."
"To obtain good system performance, a DBA must choose a set of indices that is
appropriate for the workload. The system can aid in this challenging task by
providing recommendations for the index configuration. We propose a new index
recommendation technique, termed semi-automatic tuning, that keeps the DBA ""in
the loop"" by generating recommendations that use feedback about the DBA's
preferences. The technique also works online, which avoids the limitations of
commercial tools that require the workload to be known in advance. The
foundation of our approach is the Work Function Algorithm, which can solve a
wide variety of online optimization problems with strong competitive
guarantees. We present an experimental analysis that validates the benefits of
semi-automatic tuning in a wide variety of conditions."
"Complex information extraction (IE) pipelines assembled by plumbing together
off-the-shelf operators, specially customized operators, and operators re-used
from other text processing pipelines are becoming an integral component of most
text processing frameworks. A critical task faced by the IE pipeline user is to
run a post-mortem analysis on the output. Due to the diverse nature of
extraction operators (often implemented by independent groups), it is time
consuming and error-prone to describe operator semantics formally or
operationally to a provenance system. We introduce the first system that helps
IE users analyze pipeline semantics and infer provenance interactively while
debugging. This allows the effort to be proportional to the need, and to focus
on the portions of the pipeline under the greatest suspicion. We present a
generic debugger for running post-execution analysis of any IE pipeline
consisting of arbitrary types of operators. We propose an effective provenance
model for IE pipelines which captures a variety of operator types, ranging from
those for which full or no specifications are available. We present a suite of
algorithms to effectively build provenance and facilitate debugging. Finally,
we present an extensive experimental study on large-scale real-world
extractions from an index of ~500 million Web documents."
"Association rule mining is an active data mining research area and most ARM
algorithms cater to a centralized environment. Centralized data mining to
discover useful patterns in distributed databases isn't always feasible because
merging data sets from different sites incurs huge network communication costs.
In this paper, an Improved algorithm based on good performance level for data
mining is being proposed. In local sites, it runs the application based on the
improved LMatrix algorithm, which is used to calculate local support counts.
Local Site also finds a centre site to manage every message exchanged to obtain
all globally frequent item sets. It also reduces the time of scan of partition
database by using LMatrix which increases the performance of the algorithm.
Therefore, the research is to develop a distributed algorithm for
geographically distributed data sets that reduces communication costs, superior
running efficiency, and stronger scalability than direct application of a
sequential algorithm in distributed databases."
"The rapidly expanding technology of mobile communication will give mobile
users capability of accessing information from anywhere and any time. The
wireless technology has made it possible to achieve continuous connectivity in
mobile environment. When the query is specified as continuous, the requesting
mobile user can obtain continuously changing result. In order to provide
accurate and timely outcome to requesting mobile user, the locations of moving
object has to be closely monitored. The objective of paper is to discuss the
problem related to the role of personal and terminal mobility and query
processing in the mobile environment."
"Maintaining a legacy database is a difficult task especially when system
documentation is poor written or even missing. Database reverse engineering is
an attempt to recover high-level conceptual design from the existing database
instances. In this paper, we propose a technique to discover conceptual schema
using the association mining technique. The discovered schema corresponds to
the normalization at the third normal form, which is a common practice in many
business organizations. Our algorithm also includes the rule filtering
heuristic to solve the problem of exponential growth of discovered rules
inherited with the association mining technique."
"Association rule mining aims to explore large transaction databases for
association rules. Classical Association Rule Mining (ARM) model assumes that
all items have the same significance without taking their weight into account.
It also ignores the difference between the transactions and importance of each
and every itemsets. But, the Weighted Association Rule Mining (WARM) does not
work on databases with only binary attributes. It makes use of the importance
of each itemset and transaction. WARM requires each item to be given weight to
reflect their importance to the user. The weights may correspond to special
promotions on some products, or the profitability of different items. This
research work first focused on a weight assignment based on a directed graph
where nodes denote items and links represent association rules. A generalized
version of HITS is applied to the graph to rank the items, where all nodes and
links are allowed to have weights. This research then uses enhanced HITS
algorithm by developing an online eigenvector calculation method that can
compute the results of mutual reinforcement voting in case of frequent updates.
For Example in Share Market Shares price may go down or up. So we need to
carefully watch the market and our association rule mining has to produce the
items that have undergone frequent changes. These are done by estimating the
upper bound of perturbation and postponing of the updates whenever possible.
Next we prove that enhanced algorithm is more efficient than the original HITS
under the context of dynamic data."
"The importance of security in database research has greatly increased over
the years as most of critical functionality of the business and military
enterprises became digitized. Database is an integral part of any information
system and they often hold sensitive data. The security of the data depends on
physical security, OS security and DBMS security. Database security can be
compromised by obtaining sensitive data, changing data or degrading
availability of the database. Over the last 30 years the information technology
environment have gone through many changes of evolution and the database
research community have tried to stay a step ahead of the upcoming threats to
the database security. The database research community has thoughts about these
issues long before they were address by the implementations. This paper will
examine the different topics pertaining to database security and see the
adaption of the research to the changing environment. Some short term database
research trends will be ascertained at the conclusion."
"In this paper, we emphasize the need for data cleansing when clustering
large-scale transaction databases and propose a new data cleansing method that
improves clustering quality and performance. We evaluate our data cleansing
method through a series of experiments. As a result, the clustering quality and
performance were significantly improved by up to 165% and 330%, respectively."
"A boolean expression is in read-once form if each of its variables appears
exactly once. When the variables denote independent events in a probability
space, the probability of the event denoted by the whole expression in
read-once form can be computed in polynomial time (whereas the general problem
for arbitrary expressions is #P-complete). Known approaches to checking
read-once property seem to require putting these expressions in disjunctive
normal form. In this paper, we tell a better story for a large subclass of
boolean event expressions: those that are generated by conjunctive queries
without self-joins and on tuple-independent probabilistic databases. We first
show that given a tuple-independent representation and the provenance graph of
an SPJ query plan without self-joins, we can, without using the DNF of a result
event expression, efficiently compute its co-occurrence graph. From this, the
read-once form can already, if it exists, be computed efficiently using
existing techniques. Our second and key contribution is a complete, efficient,
and simple to implement algorithm for computing the read-once forms (whenever
they exist) directly, using a new concept, that of co-table graph, which can be
significantly smaller than the co-occurrence graph."
"The data warehouse (DW) technology was developed to integrate heterogeneous
information sources for analysis purposes. Information sources are more and
more autonomous and they often change their content due to perpetual
transactions (data changes) and may change their structure due to continual
users' requirements evolving (schema changes). Handling properly all type of
changes is a must. In fact, the DW which is considered as the core component of
the modern decision support systems has to be update according to different
type of evolution of information sources to reflect the real world subject to
analysis. The goal of this paper is to propose an overview and a comparative
study between different works related to the DW evolution problem."
"A key goal of bioinformatics is to create database systems and software
platforms capable of storing and analysing large sets of biological data.
Hundreds of biological databases are now available and provide access to huge
amount of biological data. SGD, Yeastract, CYGD-MIPS, BioGrid and PhosphoGrid
are five of the most visited databases by the yeast community. These sources
provide complementary data on biological entities. Biologists are brought
systematically to query these data sources in order to analyse the results of
their experiments. Because of the heterogeneity of these sources, querying them
separately and then manually combining the returned result is a complex and
laborious task. To provide transparent and simultaneous access to these
sources, we have developed a mediator-based system called YeastMed. In this
paper, we present YeastMed focusing on its architecture."
"We have compared the performance of five non-commercial triple stores,
Virtuoso-open source, Jena SDB, Jena TDB, SWIFT-OWLIM and 4Store. We examined
three performance aspects: the query execution time, scalability and run-to-run
reproducibility. The queries we chose addressed different ontological or
biological topics, and we obtained evidence that individual store performance
was quite query specific. We identified three groups of queries displaying
similar behavior across the different stores: 1) relatively short response
time, 2) moderate response time and 3) relatively long response time. OWLIM
proved to be a winner in the first group, 4Store in the second and Virtuoso in
the third. Our benchmarking showed Virtuoso to be a very balanced performer -
its response time was better than average for all the 24 queries; it showed a
very good scalability and a reasonable run-to-run reproducibility."
"Our Chemical e-Science Information Cloud (ChemCloud) - a Semantic Web based
eScience infrastructure - integrates and automates a multitude of databases,
tools and services in the domain of chemistry, pharmacy and bio-chemistry
available at the Fachinformationszentrum Chemie (FIZ Chemie), at the Freie
Universitaet Berlin (FUB), and on the public Web. Based on the approach of the
W3C Linked Open Data initiative and the W3C Semantic Web technologies for
ontologies and rules it semantically links and integrates knowledge from our
W3C HCLS knowledge base hosted at the FUB, our multi-domain knowledge base
DBpedia (Deutschland) implemented at FUB, which is extracted from Wikipedia
(De) providing a public semantic resource for chemistry, and our
well-established databases at FIZ Chemie such as ChemInform for organic
reaction data, InfoTherm the leading source for thermophysical data, Chemisches
Zentralblatt, the complete chemistry knowledge from 1830 to 1969, and
ChemgaPedia the largest and most frequented e-Learning platform for Chemistry
and related sciences in German language."
"The primary mission of UniProt is to support biological research by
maintaining a stable, comprehensive, fully classified, richly and accurately
annotated protein sequence knowledgebase, with extensive cross-references to
external resources, that is freely available to the scientific community. To
enable users of the knowledgebase to accurately assess the reliability of the
information contained in this resource, the evidence for and provenance of the
information must be recorded. This paper discusses the user requirements for
this kind of metadata and the manner in which UniProtKB records it."
"The Zebrafish Model Organism Database (ZFIN) provides a Web resource of
zebrafish genomic, genetic, developmental, and phenotypic data. Four different
ontologies are currently used to annotate data to the most specific term
available facilitating a better comparison between inter-species data. In
addition, ontologies are used to help users find and cluster data more quickly
without the need of knowing the exact technical name for a term."
"Motivated by a recent conjecture concerning the expressiveness of declarative
networking, we propose a formal computation model for ""eventually consistent""
distributed querying, based on relational transducers. A tight link has been
conjectured between coordination-freeness of computations, and monotonicity of
the queries expressed by such computations. Indeed, we propose a formal
definition of coordination-freeness and confirm that the class of monotone
queries is captured by coordination-free transducer networks.
Coordination-freeness is a semantic property, but the syntactic class that we
define of ""oblivious"" transducers also captures the same class of monotone
queries. Transducer networks that are not coordination-free are much more
powerful."
"Since the release of human genome sequences, one of the most important
research issues is about indexing the genome sequences, and the suffix tree is
most widely adopted for that purpose. The traditional suffix tree construction
algorithms have severe performance degradation due to the memory bottleneck
problem. The recent disk-based algorithms also have limited performance
improvement due to random disk accesses. Moreover, they do not fully utilize
the recent CPUs with multiple cores. In this paper, we propose a fast algorithm
based on 'divide-and-conquer' strategy for indexing the human genome sequences.
Our algorithm almost eliminates random disk accesses by accessing the disk in
the unit of contiguous chunks. In addition, our algorithm fully utilizes the
multi-core CPUs by dividing the genome sequences into multiple partitions and
then assigning each partition to a different core for parallel processing.
Experimental results show that our algorithm outperforms the previous fastest
DIGEST algorithm by up to 3.5 times."
"The proliferation of ontologies and taxonomies in many domains increasingly
demands the integration of multiple such ontologies. The goal of ontology
integration is to merge two or more given ontologies in order to provide a
unified view on the input ontologies while maintaining all information coming
from them. We propose a new taxonomy merging algorithm that, given as input two
taxonomies and an equivalence matching between them, can generate an integrated
taxonomy in a fully automatic manner. The approach is target-driven, i.e. we
merge a source taxonomy into the target taxonomy and preserve the structure of
the target ontology as much as possible. We also discuss how to extend the
merge algorithm providing auxiliary information, like additional relationships
between source and target concepts, in order to semantically improve the final
result. The algorithm was implemented in a working prototype and evaluated
using synthetic and real-world scenarios."
"XML document markup is highly repetitive and therefore well compressible
using dictionary-based methods such as DAGs or grammars. In the context of
selectivity estimation, grammar-compressed trees were used before as synopsis
for structural XPath queries. Here a fully-fledged index over such grammars is
presented. The index allows to execute arbitrary tree algorithms with a
slow-down that is comparable to the space improvement. More interestingly,
certain algorithms execute much faster over the index (because no decompression
occurs). E.g., for structural XPath count queries, evaluating over the index is
faster than previous XPath implementations, often by two orders of magnitude.
The index also allows to serialize XML results (including texts) faster than
previous systems, by a factor of ca. 2-3. This is due to efficient copy
handling of grammar repetitions, and because materialization is totally
avoided. In order to compare with twig join implementations, we implemented a
materializer which writes out pre-order numbers of result nodes, and show its
competitiveness."
"Clustering real world data often faced with curse of dimensionality, where
real world data often consist of many dimensions. Multidimensional data
clustering evaluation can be done through a density-based approach. Density
approaches based on the paradigm introduced by DBSCAN clustering. In this
approach, density of each object neighbours with MinPoints will be calculated.
Cluster change will occur in accordance with changes in density of each object
neighbours. The neighbours of each object typically determined using a distance
function, for example the Euclidean distance. In this paper SUBCLU, FIRES and
INSCY methods will be applied to clustering 6x1595 dimension synthetic
datasets. IO Entropy, F1 Measure, coverage, accurate and time consumption used
as evaluation performance parameters. Evaluation results showed SUBCLU method
requires considerable time to process subspace clustering; however, its value
coverage is better. Meanwhile INSCY method is better for accuracy comparing
with two other methods, although consequence time calculation was longer."
"To optimize telecom service management, it is necessary that information
about telecom services is highly related to the most popular telecom service.
To this end, we propose an algorithm for mining target-oriented fuzzy
correlation rules. In this paper, we show that by using the fuzzy statistics
analysis and the data mining technology, the target-oriented fuzzy correlation
rules can be obtained from a given database. We conduct an experiment by using
a sample database from a telecom service provider in Taiwan. Our work can be
used to assist the telecom service provider in providing the appropriate
services to the customers for better customer relationship management."
"Traditional spatial queries return, for a given query object $q$, all
database objects that satisfy a given predicate, such as epsilon range and
$k$-nearest neighbors. This paper defines and studies {\em inverse} spatial
queries, which, given a subset of database objects $Q$ and a query predicate,
return all objects which, if used as query objects with the predicate, contain
$Q$ in their result. We first show a straightforward solution for answering
inverse spatial queries for any query predicate. Then, we propose a
filter-and-refinement framework that can be used to improve efficiency. We show
how to apply this framework on a variety of inverse queries, using appropriate
space pruning strategies. In particular, we propose solutions for inverse
epsilon range queries, inverse $k$-nearest neighbor queries, and inverse
skyline queries. Our experiments show that our framework is significantly more
efficient than naive approaches."
"In this paper a tool called RDBNorma is proposed, that uses a novel approach
to represent a relational database schema and its functional dependencies in
computer memory using only one linked list and used for semi-automating the
process of relational database schema normalization up to third normal form.
This paper addresses all the issues of representing a relational schema along
with its functional dependencies using one linked list along with the
algorithms to convert a relation into second and third normal form by using
above representation. We have compared performance of RDBNorma with existing
tool called Micro using standard relational schemas collected from various
resources. It is observed that proposed tool is at least 2.89 times faster than
the Micro and requires around half of the space than Micro to represent a
relation. Comparison is done by entering all the attributes and functional
dependencies holds on a relation in the same order and implementing both the
tools in same language and on same machine."
"Many works have focused, for over twenty five years, on the integration of
the time dimension in databases (DB). However, the standard SQL3 does not yet
allow easy definition, manipulation and querying of temporal DBs. In this
paper, we study how we can simplify querying and manipulating temporal facts in
SQL3, using a model that integrates time in a native manner. To do this, we
propose new keywords and syntax to define different temporal versions for many
relational operators and functions used in SQL. It then becomes possible to
perform various queries and updates appropriate to temporal facts. We
illustrate the use of these proposals on many examples from a real application."
"The problem of privately releasing data is to provide a version of a dataset
without revealing sensitive information about the individuals who contribute to
the data. The model of differential privacy allows such private release while
providing strong guarantees on the output. A basic mechanism achieves
differential privacy by adding noise to the frequency counts in the contingency
tables (or, a subset of the count data cube) derived from the dataset. However,
when the dataset is sparse in its underlying space, as is the case for most
multi-attribute relations, then the effect of adding noise is to vastly
increase the size of the published data: it implicitly creates a huge number of
dummy data points to mask the true data, making it almost impossible to work
with.
  We present techniques to overcome this roadblock and allow efficient private
release of sparse data, while maintaining the guarantees of differential
privacy. Our approach is to release a compact summary of the noisy data.
Generating the noisy data and then summarizing it would still be very costly,
so we show how to shortcut this step, and instead directly generate the summary
from the input data, without materializing the vast intermediate noisy data. We
instantiate this outline for a variety of sampling and filtering methods, and
show how to use the resulting summary for approximate, private, query
answering. Our experimental study shows that this is an effective, practical
solution, with comparable and occasionally improved utility over the costly
materialization approach."
"With the advance of Web Services technologies and the emergence of Web
Services into the information space, tremendous opportunities for empowering
users and organizations appear in various application domains including
electronic commerce, travel, intelligence information gathering and analysis,
health care, digital government, etc. However, the technology to organize,
search, integrate these Web Services has not kept pace with the rapid growth of
the available information space. The number of Web Services to be integrated
may be large and continuously changing. To ease and improve the process of Web
services discovery in an open environment like the Internet, it is suggested to
gather similar Web services into groups known as communities. Although Web
services are intensively investigated, the community management issues have not
been addressed yet In this paper we draw an overview of several Web services
Communities' management approaches based on some currently existing communities
platforms and frameworks. We also discuss different approaches for querying and
selecting Web services under the umbrella of Web services communities'. We
compare the current approaches among each others with respect to some key
requirements."
"We describe a generic framework for representing and reasoning with annotated
Semantic Web data, a task becoming more important with the recent increased
amount of inconsistent and non-reliable meta-data on the web. We formalise the
annotated language, the corresponding deductive system and address the query
answering problem. Previous contributions on specific RDF annotation domains
are encompassed by our unified reasoning formalism as we show by instantiating
it on (i) temporal, (ii) fuzzy, and (iii) provenance annotations. Moreover, we
provide a generic method for combining multiple annotation domains allowing to
represent, e.g. temporally-annotated fuzzy RDF. Furthermore, we address the
development of a query language -- AnQL -- that is inspired by SPARQL,
including several features of SPARQL 1.1 (subqueries, aggregates, assignment,
solution modifiers) along with the formal definitions of their semantics."
"Differential privacy is a rigorous privacy condition achieved by randomizing
query answers. This paper develops efficient algorithms for answering multiple
queries under differential privacy with low error. We pursue this goal by
advancing a recent approach called the matrix mechanism, which generalizes
standard differentially private mechanisms. This new mechanism works by first
answering a different set of queries (a strategy) and then inferring the
answers to the desired workload of queries. Although a few strategies are known
to work well on specific workloads, finding the strategy which minimizes error
on an arbitrary workload is intractable. We prove a new lower bound on the
optimal error of this mechanism, and we propose an efficient algorithm that
approaches this bound for a wide range of workloads."
"We present a generic framework to make wrapper induction algorithms tolerant
to noise in the training data. This enables us to learn wrappers in a
completely unsupervised manner from automatically and cheaply obtained noisy
training data, e.g., using dictionaries and regular expressions. By removing
the site-level supervision that wrapper-based techniques require, we are able
to perform information extraction at web-scale, with accuracy unattained with
existing unsupervised extraction techniques. Our system is used in production
at Yahoo! and powers live applications."
"There have been several recent advancements in Machine Learning community on
the Entity Matching (EM) problem. However, their lack of scalability has
prevented them from being applied in practical settings on large real-life
datasets. Towards this end, we propose a principled framework to scale any
generic EM algorithm. Our technique consists of running multiple instances of
the EM algorithm on small neighborhoods of the data and passing messages across
neighborhoods to construct a global solution. We prove formal properties of our
framework and experimentally demonstrate the effectiveness of our approach in
scaling EM algorithms."
"In this paper we present GDR, a Guided Data Repair framework that
incorporates user feedback in the cleaning process to enhance and accelerate
existing automatic repair techniques while minimizing user involvement. GDR
consults the user on the updates that are most likely to be beneficial in
improving data quality. GDR also uses machine learning methods to identify and
apply the correct updates directly to the database without the actual
involvement of the user on these specific updates. To rank potential updates
for consultation by the user, we first group these repairs and quantify the
utility of each group using the decision-theory concept of value of information
(VOI). We then apply active learning to order updates within a group based on
their ability to improve the learned model. User feedback is used to repair the
database and to adaptively refine the training set for the model. We
empirically evaluate GDR on a real-world dataset and show significant
improvement in data quality using our user guided repairing process. We also,
assess the trade-off between the user efforts and the resulting data quality."
"The proliferation of imprecise data has motivated both researchers and the
database industry to push statistical techniques into relational database
management systems (RDBMSs). We study algorithms to maintain model-based views
for a popular statistical technique, classification, inside an RDBMS in the
presence of updates to the training examples. We make three technical
contributions: (1) An algorithm that incrementally maintains classification
inside an RDBMS. (2) An analysis of the above algorithm that shows that our
algorithm is optimal among all deterministic algorithms (and asymptotically
within a factor of 2 of a nondeterministic optimal). (3) An index structure
based on the technical ideas that underlie the above algorithm which allows us
to store only a fraction of the entities in memory. We apply our techniques to
text processing, and we demonstrate that our algorithms provide several orders
of magnitude improvement over non-incremental approaches to classification on a
variety of data sets: such as the Cora, UCI Machine Learning Repository data
sets, Citeseer, and DBLife."
"It is commonly accepted in the practice of on-line analytical processing of
databases that the multidimensional database organization is less scalable than
the relational one. It is easy to see that the size of the multidimensional
organization may increase very quickly. For example, if we introduce one
additional dimension, then the total number of possible cells will be at least
doubled. However, this reasoning does not takethe fact into account that the
multidimensional organization can be compressed. There are compression
techniques, which can remove all or at least a part of the empty cells from the
multidimensional organization, while maintaining a good retrieval performance.
Relational databases often use B-tree indices to speed up the access to given
rows of tables. It can be proven, under some reasonable assumptions, that the
total size of the table and the B-tree index is bigger than a compressed
multidimensional representation. This implies that the compressed array results
in a smaller database and faster access at the same time. This paper compares
several compression techniques and shows when we should and should not apply
compressed arrays instead of relational tables."
"The multidimensional databases often use compression techniques in order to
decrease the size of the database. This paper introduces a new method called
difference sequence compression. Under some conditions, this new technique is
able to create a smaller size multidimensional database than others like single
count header compression, logical position compression or base-offset
compression. Keywords: compression, multidimensional database, On-line
Analytical Processing, OLAP."
"In the past few years, the number of OLAP applications increased quickly.
These applications use two significantly different DB structures:
multidimensional (MD) and table-based. One can show that the traditional model
of relational databases cannot make difference between these two structures.
Another model is necessary to make the differences visible. One of these is the
speed of the system. It can be proven that the multidimensional DB organization
results in shorter response times. And it is crucial, since a manager may
become impatient, if he or she has to wait say more than 20 seconds for the
next screen. On the other hand, we have to pay for the speed with a bigger DB
size. Why does the size of MD databases grow so quickly? The reason is the
sparsity of data: The MD matrix contains many empty cells. Efficient handling
of sparse matrices is indispensable in an OLAP application. One way to handle
sparsity is to take the structure closer to the table-based one. Thus the DB
size decreases, while the application gets slower. Therefore, other methods are
needed. This paper deals with the comparison of the two DB structures and the
limits of their usage. The new results of the paper: (1) It gives a
constructive proof that all relations can be represented in MD arrays. (2) It
also shows when the MD array representation is quicker than the table-based
one. (3) The MD representation results in smaller DB size under some
conditions. One such sufficient condition is proved in the paper. (4) A
variation of the single count header compression scheme is described with an
algorithm, which creates the compressed array from the ordered table without
materializing the uncompressed array. (5) The speed of the two different
database organizations is tested with experiments, as well. The tests are done
on benchmark as well as real life data. The experiments support the theoretical
results."
"One utilisation of multidimensional databases is the field of On-line
Analytical Processing (OLAP). The applications in this area are designed to
make the analysis of shared multidimensional information fast [9]. On one hand,
speed can be achieved by specially devised data structures and algorithms. On
the other hand, the analytical process is cyclic. In other words, the user of
the OLAP application runs his or her queries one after the other. The output of
the last query may be there (at least partly) in one of the previous results.
Therefore caching also plays an important role in the operation of these
systems. However, caching itself may not be enough to ensure acceptable
performance. Size does matter: The more memory is available, the more we gain
by loading and keeping information in there. Oftentimes, the cache size is
fixed. This limits the performance of the multidimensional database, as well,
unless we compress the data in order to move a greater proportion of them into
the memory. Caching combined with proper compression methods promise further
performance improvements. In this paper, we investigate how caching influences
the speed of OLAP systems. Different physical representations (multidimensional
and table) are evaluated. For the thorough comparison, models are proposed. We
draw conclusions based on these models, and the conclusions are verified with
empirical data."
"A new compression method called difference-Huffman coding (DHC) is introduced
in this paper. It is verified empirically that DHC results in a smaller
multidimensional physical representation than those for other previously
published techniques (single count header compression, logical position
compression, base-offset compression and difference sequence compression). The
article examines how caching influences the expected retrieval time of the
multidimensional and table representations of relations. A model is proposed
for this, which is then verified with empirical data. Conclusions are drawn,
based on the model and the experiment, about when one physical representation
outperforms another in terms of retrieval time. Over the tested range of
available memory, the performance for the multidimensional representation was
always much quicker than for the table representation."
"In this paper, we present the design of a scalable, distributed stream
processing system for RFID tracking and monitoring. Since RFID data lacks
containment and location information that is key to query processing, we
propose to combine location and containment inference with stream query
processing in a single architecture, with inference as an enabling mechanism
for high-level query processing. We further consider challenges in
instantiating such a system in large distributed settings and design techniques
for distributed inference and query processing. Our experimental results, using
both real-world data and large synthetic traces, demonstrate the accuracy,
efficiency, and scalability of our proposed techniques."
"The Change detection based on analysis and samples are analyzed. Land
use/cover change detection based on SDM is discussed."
"This write-up is the suggested lecture notes for a second level course on
advanced topics in database systems for master's students of Computer Science
with a theoretical focus. A prerequisite in algorithms and an exposure to
database systems are required. Additional reading may require exposure to
mathematical logic. The starting point for these notes are from M.Y.Vardi's
survey listed herein as a reference - some of the proofs are presented as such
. This select rewrite on functional dependency is intended to provide a few
clarifications even though radically new design approaches are now being
proposed."
"Differential privacy has recently emerged as the de facto standard for
private data release. This makes it possible to provide strong theoretical
guarantees on the privacy and utility of released data. While it is well-known
how to release data based on counts and simple functions under this guarantee,
it remains to provide general purpose techniques to release different kinds of
data. In this paper, we focus on spatial data such as locations and more
generally any data that can be indexed by a tree structure. Directly applying
existing differential privacy methods to this type of data simply generates
noise. Instead, we introduce a new class of ""private spatial decompositions"":
these adapt standard spatial indexing methods such as quadtrees and kd-trees to
provide a private description of the data distribution. Equipping such
structures with differential privacy requires several steps to ensure that they
provide meaningful privacy guarantees. Various primitives, such as choosing
splitting points and describing the distribution of points within a region,
must be done privately, and the guarantees of the different building blocks
composed to provide an overall guarantee. Consequently, we expose the design
space for private spatial decompositions, and analyze some key examples. Our
experimental study demonstrates that it is possible to build such
decompositions efficiently, and use them to answer a variety of queries
privately with high accuracy."
"In this work we are analyzing scalability of the heuristic algorithm we used
in the past to discover knowledge from multi-valued symbolic attributes in
fuzzy databases. The non-atomic descriptors, characterizing a single attribute
of a database record, are commonly used in fuzzy databases to reflect
uncertainty about the recorded observation. In this paper, we present
implementation details and scalability tests of the algorithm, which we
developed to precisely interpret such non-atomic values and to transfer (i.e.
defuzzify) the fuzzy tuples to the forms acceptable for many regular (i.e.
atomic values based) data mining algorithms. Important advantages of our
approach are: (1) its linear scalability, and (2) its unique capability of
incorporating background knowledge, implicitly stored in the fuzzy database
models in the form of fuzzy similarity hierarchy, into the
interpretation/defuzzification process."
"Data Security is a major issue in any web-based application. There have been
approaches to handle intruders in any system, however, these approaches are not
fully trustable; evidently data is not totally protected. Real world databases
have information that needs to be securely stored. The approach of generating
negative database could help solve such problem. A Negative Database can be
defined as a database that contains huge amount of data consisting of
counterfeit data along with the real data. Intruders may be able to get access
to such databases, but, as they try to extract information, they will retrieve
data sets that would include both the actual and the negative data. In this
paper we present our approach towards implementing the concept of negative
database to help prevent data theft from malicious users and provide efficient
data retrieval for all valid users."
"Web log data is usually diverse and voluminous. This data must be assembled
into a consistent, integrated and comprehensive view, in order to be used for
pattern discovery. Without properly cleaning, transforming and structuring the
data prior to the analysis, one cannot expect to find meaningful patterns. As
in most data mining applications, data preprocessing involves removing and
filtering redundant and irrelevant data, removing noise, transforming and
resolving any inconsistencies. In this paper, a complete preprocessing
methodology having merging, data cleaning, user/session identification and data
formatting and summarization activities to improve the quality of data by
reducing the quantity of data has been proposed. To validate the efficiency of
the proposed preprocessing methodology, several experiments are conducted and
the results show that the proposed methodology reduces the size of Web access
log files down to 73-82% of the initial size and offers richer logs that are
structured for further stages of Web Usage Mining (WUM). So preprocessing of
raw data in this WUM process is the central theme of this paper."
"The database community is exploring more and more multidisciplinary avenues:
Data semantics overlaps with ontology management; reasoning tasks venture into
the domain of artificial intelligence; and data stream management and
information retrieval shake hands, e.g., when processing Web click-streams.
These new research avenues become evident, for example, in the topics that
doctoral students choose for their dissertations. This paper surveys the
emerging multidisciplinary research by doctoral students in database systems
and related areas. It is based on the PIKM 2010, which is the 3rd Ph.D.
workshop at the International Conference on Information and Knowledge
Management (CIKM). The topics addressed include ontology development, data
streams, natural language processing, medical databases, green energy, cloud
computing, and exploratory search. In addition to core ideas from the workshop,
we list some open research questions in these multidisciplinary areas."
"A growing number of applications that generate massive streams of data need
intelligent data processing and online analysis. Real-time surveillance
systems, telecommunication systems, sensor networks and other dynamic
environments are such examples. The imminent need for turning such data into
useful information and knowledge augments the development of systems,
algorithms and frameworks that address streaming challenges. The storage,
querying and mining of such data sets are highly computationally challenging
tasks. Mining data streams is concerned with extracting knowledge structures
represented in models and patterns in non stopping streams of information.
Generally, two main challenges are designing fast mining methods for data
streams and need to promptly detect changing concepts and data distribution
because of highly dynamic nature of data streams. The goal of this article is
to analyze and classify the application of diverse data mining techniques in
different challenges of data stream mining. In this paper, we present the
theoretical foundations of data stream analysis and propose an analytical
framework for data stream mining techniques."
"The annotation of the results of database transformations was shown to be
very effective for various applications. Until recently, most works in this
context focused on positive query languages. The provenance semirings is a
particular approach that was proven effective for these languages, and it was
shown that when propagating provenance with semirings, the expected equivalence
axioms of the corresponding query languages are satisfied. There have been
several attempts to extend the framework to account for relational algebra
queries with difference. We show here that these suggestions fail to satisfy
some expected equivalence axioms (that in particular hold for queries on
""standard"" set and bag databases). Interestingly, we show that this is not a
pitfall of these particular attempts, but rather every such attempt is bound to
fail in satisfying these axioms, for some semirings. Finally, we show
particular semirings for which an extension for supporting difference is
(im)possible."
"A high-quality, comprehensive product catalog is essential to the success of
Product Search engines and shopping sites such as Yahoo! Shopping, Google
Product Search or Bing Shopping. But keeping catalogs up-to-date becomes a
challenging task, calling for the need of automated techniques. In this paper,
we introduce the problem of product synthesis, a key component of catalog
creation and maintenance. Given a set of offers advertised by merchants, the
goal is to identify new products and add them to the catalog together with
their (structured) attributes. A fundamental challenge is the scale of the
problem: a Product Search engine receives data from thousands of merchants and
millions of products; the product taxonomy contains thousands of categories,
where each category comes in a different schema; and merchants use
representations for products that are different from the ones used in the
catalog of the Product Search engine.
  We propose a system that provides an end-to-end solution to the product
synthesis problem, and includes components for extraction, and addresses issues
involved in data extraction from offers, schema reconciliation, and data
fusion. We developed a novel and scalable technique for schema matching which
leverages knowledge about previously-known instance-level associations between
offers and products; and it is trained using automatically created training
sets (no manually-labeled data is needed). We present an experimental
evaluation of our system using data from Bing Shopping for more than 800K
offers, a thousand merchants, and 400 categories. The evaluation confirms that
our approach is able to automatically generate a large number of accurate
product specifications, and that our schema reconciliation component
outperforms state-of-the-art schema matching techniques in terms of precision
and recall."
"New hardware platforms, e.g. cloud, multi-core, etc., have led to a
reconsideration of database system architecture. Our Deuteronomy project
separates transactional functionality from data management functionality,
enabling a flexible response to exploiting new platforms. This separation
requires, however, that recovery is described logically. In this paper, we
extend current recovery methods to work in this logical setting. While this is
straightforward in principle, performance is an issue. We show how ARIES style
recovery optimizations can work for logical recovery where page information is
not captured on the log. In side-by-side performance experiments using a common
log, we compare logical recovery with a state-of-the art ARIES style recovery
implementation and show that logical redo performance can be competitive."
"We show that the default-all propagation scheme for database annotations is
dangerous. Dangerous here means that it can propagate annotations to the query
output which are semantically irrelevant to the query the user asked. This is
the result of considering all relationally equivalent queries and returning the
union of their where-provenance in an attempt to define a propagation scheme
that is insensitive to query rewriting. We propose an alternative
query-rewrite-insensitive (QRI) where-provenance called minimum propagation. It
is analogous to the minimum witness basis for why-provenance, straight-forward
to evaluate, and returns all relevant and only relevant annotations."
"Atomic commit protocols are used where data integrity is more important than
data availability. Two-Phase commit (2PC) is a standard commit protocol for
commercial database management systems. To reduce certain drawbacks in 2PC
protocol people have suggested different variance of this protocol.
Short-Commit protocol is developed with an objective to achieve low cost
transaction commitment cost with non-blocking capability. In this paper we have
briefly explained short-commit protocol executing pattern. Experimental
analysis and results are presented to support the claim that short-commit can
work efficiently in extreme database environment."
"Good database design is crucial to obtain a sound, consistent database, and -
in turn - good database design methodologies are the best way to achieve the
right design. These methodologies are taught to most Computer Science
undergraduates, as part of any Introduction to Database class. They can be
considered part of the ""canon"", and indeed, the overall approach to database
design has been unchanged for years. Moreover, none of the major database
research assessments identify database design as a strategic research
direction.
  Should we conclude that database design is a solved problem?
  Our thesis is that database design remains a critical unsolved problem.
Hence, it should be the subject of more research. Our starting point is the
observation that traditional database design is not used in practice - and if
it were used it would result in designs that are not well adapted to current
environments. In short, database design has failed to keep up with the times.
In this paper, we put forth arguments to support our viewpoint, analyze the
root causes of this situation and suggest some avenues of research."
"The interlinking of datasets published in the Linked Data Cloud is a
challenging problem and a key factor for the success of the Semantic Web.
Manual rule-based methods are the most effective solution for the problem, but
they require skilled human data publishers going through a laborious, error
prone and time-consuming process for manually describing rules mapping
instances between two datasets. Thus, an automatic approach for solving this
problem is more than welcome. In this paper, we propose a novel interlinking
method, SERIMI, for solving this problem automatically. SERIMI matches
instances between a source and a target datasets, without prior knowledge of
the data, domain or schema of these datasets. Experiments conducted with
benchmark collections demonstrate that our approach considerably outperforms
state-of-the-art automatic approaches for solving the interlinking problem on
the Linked Data Cloud."
"Providing a customized support for the OLAP brings tremendous challenges to
the OLAP technology. Standing at the crossroads of the preferences and the data
warehouse, two emerging trends are pointed out; namely: (i) the personalization
and (ii) the recommendation. Although the panoply of the proposed approaches,
the user-centric data warehouse community issues have not been addressed yet.
In this paper we draw an overview of several user centric data warehouse
proposals. We also discuss the two promising concepts in this issue, namely,
the personalization and the recommendation of the data warehouses. We compare
the current approaches among each others with respect to some criteria."
"Query workloads and database schemas in OLAP applications are becoming
increasingly complex. Moreover, the queries and the schemas have to continually
\textit{evolve} to address business requirements. During such repetitive
transitions, the \textit{order} of index deployment has to be considered while
designing the physical schemas such as indexes and MVs.
  An effective index deployment ordering can produce (1) a prompt query runtime
improvement and (2) a reduced total deployment time. Both of these are
essential qualities of design tools for quickly evolving databases, but
optimizing the problem is challenging because of complex index interactions and
a factorial number of possible solutions.
  We formulate the problem in a mathematical model and study several techniques
for solving the index ordering problem. We demonstrate that Constraint
Programming (CP) is a more flexible and efficient platform to solve the problem
than other methods such as mixed integer programming and A* search. In addition
to exact search techniques, we also studied local search algorithms to find
near optimal solution very quickly.
  Our empirical analysis on the TPC-H dataset shows that our pruning techniques
can reduce the size of the search space by tens of orders of magnitude. Using
the TPC-DS dataset, we verify that our local search algorithm is a highly
scalable and stable method for quickly finding a near-optimal solution."
"Skyline queries have been widely used as a practical tool for multi-criteria
decision analysis and for applications involving preference queries. For
example, in a typical online retail application, skyline queries can help
customers select the most interesting, among a pool of available, products.
Recently, reverse skyline queries have been proposed, highlighting the
manufacturer's perspective, i.e. how to determine the expected buyers of a
given product. In this work we develop novel algorithms for two important
classes of queries involving customer preferences. We first propose a novel
algorithm, termed as RSA, for answering reverse skyline queries. We then
introduce a new type of queries, namely the k-Most Attractive Candidates k-MAC
query. In this type of queries, given a set of existing product specifications
P, a set of customer preferences C and a set of new candidate products Q, the
k-MAC query returns the set of k candidate products from Q that jointly
maximizes the total number of expected buyers, measured as the cardinality of
the union of individual reverse skyline sets (i.e., influence sets). Applying
existing approaches to solve this problem would require calculating the reverse
skyline set for each candidate, which is prohibitively expensive for large data
sets. We, thus, propose a batched algorithm for this problem and compare its
performance against a branch-and-bound variant that we devise. Both of these
algorithms use in their core variants of our RSA algorithm. Our experimental
study using both synthetic and real data sets demonstrates that our proposed
algorithms outperform existing, or naive solutions to our studied classes of
queries."
"With the rapid growth of internet technologies, Web has become a huge
repository of information and keeps growing exponentially under no editorial
control. However the human capability to read, access and understand Web
content remains constant. This motivated researchers to provide Web
personalized online services such as Web recommendations to alleviate the
information overload problem and provide tailored Web experiences to the Web
users. Recent studies show that Web usage mining has emerged as a popular
approach in providing Web personalization. However conventional Web usage based
recommender systems are limited in their ability to use the domain knowledge of
the Web application. The focus is only on Web usage data. As a consequence the
quality of the discovered patterns is low. In this paper, we propose a novel
framework integrating semantic information in the Web usage mining process.
Sequential Pattern Mining technique is applied over the semantic space to
discover the frequent sequential patterns. The frequent navigational patterns
are extracted in the form of Ontology instances instead of Web page views and
the resultant semantic patterns are used for generating Web page
recommendations to the user. Experimental results shown are promising and
proved that incorporating semantic information into Web usage mining process
can provide us with more interesting patterns which consequently make the
recommendation system more functional, smarter and comprehensive."
"The one of the most time consuming steps for association rule mining is the
computation of the frequency of the occurrences of itemsets in the database.
The hash table index approach converts a transaction database to an hash index
tree by scanning the transaction database only once. Whenever user requests for
any Uniform Resource Locator (URL), the request entry is stored in the Log File
of the server. This paper presents the hash index table structure, a general
and dense structure which provides web page set extraction from Log File of
server. This hash table provides information about the original database. Web
Page set mining (WPs-Mine) provides a complete representation of the original
database. This approach works well for both sparse and dense data
distributions. Web page set mining supported by hash table index shows the
performance always comparable with and often better than algorithms accessing
data on flat files. Incremental update is feasible without reaccessing the
original transactional database."
"With the advents of high-speed networks, fast commodity hardware, and the
web, distributed data sources have become ubiquitous. The third edition of the
\""Ozsu-Valduriez textbook Principles of Distributed Database Systems [10]
reflects the evolution of distributed data management and distributed database
systems. In this new edition, the fundamental principles of distributed data
management could be still presented based on the three dimensions of earlier
editions: distribution, heterogeneity and autonomy of the data sources. In
retrospect, the focus on fundamental principles and generic techniques has been
useful not only to understand and teach the material, but also to enable an
infinite number of variations. The primary application of these generic
techniques has been obviously for distributed and parallel DBMS versions.
Today, to support the requirements of important data-intensive applications
(e.g. social networks, web data analytics, scientific applications, etc.), new
distributed data management techniques and systems (e.g. MapReduce, Hadoop,
SciDB, Peanut, Pig latin, etc.) are emerging and receiving much attention from
the research community. Although they do well in terms of
consistency/flexibility/performance trade-offs for specific applications, they
seem to be ad-hoc and might hurt data interoperability. The key questions I
discuss are: What are the fundamental principles behind the emerging solutions?
Is there any generic architectural model, to explain those principles? Do we
need new foundations to look at data distribution?"
"In this paper we present the functionality of a currently under development
database programming methodology called ODRA (Object Database for Rapid
Application development) which works fully on the object oriented principles.
The database programming language is called SBQL (Stack based query language).
We discuss some concepts in ODRA for e.g. the working of ODRA, how ODRA runtime
environment operates, the interoperability of ODRA with .net and java .A view
of ODRA's working with web services and xml. Currently the stages under
development in ODRA are query optimization. So we present the prior work that
is done in ODRA related to Query optimization and we also present a new fusion
algorithm of how ODRA can deal with joins based on collections like set, lists,
and arrays for query optimization."
"De-duplication---identification of distinct records referring to the same
real-world entity---is a well-known challenge in data integration. Since very
large datasets prohibit the comparison of every pair of records, {\em blocking}
has been identified as a technique of dividing the dataset for pairwise
comparisons, thereby trading off {\em recall} of identified duplicates for {\em
efficiency}. Traditional de-duplication tasks, while challenging, typically
involved a fixed schema such as Census data or medical records. However, with
the presence of large, diverse sets of structured data on the web and the need
to organize it effectively on content portals, de-duplication systems need to
scale in a new dimension to handle a large number of schemas, tasks and data
sets, while handling ever larger problem sizes. In addition, when working in a
map-reduce framework it is important that canopy formation be implemented as a
{\em hash function}, making the canopy design problem more challenging. We
present CBLOCK, a system that addresses these challenges. CBLOCK learns hash
functions automatically from attribute domains and a labeled dataset consisting
of duplicates. Subsequently, CBLOCK expresses blocking functions using a
hierarchical tree structure composed of atomic hash functions. The application
may guide the automated blocking process based on architectural constraints,
such as by specifying a maximum size of each block (based on memory
requirements), impose disjointness of blocks (in a grid environment), or
specify a particular objective function trading off recall for efficiency. As a
post-processing step to automatically generated blocks, CBLOCK {\em rolls-up}
smaller blocks to increase recall. We present experimental results on two
large-scale de-duplication datasets at Yahoo!---consisting of over 140K movies
and 40K restaurants respectively---and demonstrate the utility of CBLOCK."
"In this paper, we proposed a new technique for backing up and restoring
different Database Management Systems (DBMS). The technique is enabling to
backup and restore a part of or the whole database using a unified interface
using ASP.NET and XML technologies. It presents a Web Solution allowing the
administrators to do their jobs from everywhere, locally or remotely. To show
the importance of our solution, we have taken two case studies, oracle 11g and
SQL Server 2008."
"Coron is a domain and platform independent, multi-purposed data mining
toolkit, which incorporates not only a rich collection of data mining
algorithms, but also allows a number of auxiliary operations. To the best of
our knowledge, a data mining toolkit designed specifically for itemset
extraction and association rule generation like Coron does not exist elsewhere.
Coron also provides support for preparing and filtering data, and for
interpreting the extracted units of knowledge."
"Coron is a domain and platform independent, multi-purposed data mining
toolkit, which incorporates not only a rich collection of data mining
algorithms, but also allows a number of auxiliary operations. To the best of
our knowledge, a data mining toolkit designed specifically for itemset
extraction and association rule generation like Coron does not exist elsewhere.
Coron also provides support for preparing and filtering data, and for
interpreting the extracted units of knowledge."
"Correlated rare pattern mining is an interesting issue in Data mining. In
this respect, the set of correlated rare patterns w.r.t. to the bond
correlation measure was studied in a recent work, in which the RCPR concise
exact representation of the set of correlated rare patterns was proposed.
However, none algorithm was proposed in order to mine this representation and
none experiment was carried out to evaluate it. In this paper, we introduce the
new RcprMiner algorithm allowing an efficient extraction of RCPR. We also
present the IsRCP algorithm allowing the query of the RCPR representation in
addition to the RCPRegeneration algorithm allowing the regeneration of the
whole set RCP of rare correlated patterns starting from this representation.
The carried out experiments highlight interesting compactness rates offered by
RCPR. The effectiveness of the proposed classification method, based on generic
rare correlated association rules derived from RCPR, has also been proved in
the context of intrusion detection."
"One of the main challenges that the Semantic Web faces is the integration of
a growing number of independently designed ontologies. In this work, we present
PARIS, an approach for the automatic alignment of ontologies. PARIS aligns not
only instances, but also relations and classes. Alignments at the instance
level cross-fertilize with alignments at the schema level. Thereby, our system
provides a truly holistic solution to the problem of ontology alignment. The
heart of the approach is probabilistic, i.e., we measure degrees of matchings
based on probability estimates. This allows PARIS to run without any parameter
tuning. We demonstrate the efficiency of the algorithm and its precision
through extensive experiments. In particular, we obtain a precision of around
90% in experiments with some of the world's largest ontologies."
"In this paper, we formulate a top-k query that compares objects in a database
to a user-provided query object on a novel scoring function. The proposed
scoring function combines the idea of attractive and repulsive dimensions into
a general framework to overcome the weakness of traditional distance or
similarity measures. We study the properties of the proposed class of scoring
functions and develop efficient and scalable index structures that index the
isolines of the function. We demonstrate various scenarios where the query
finds application. Empirical evaluation demonstrates a performance gain of one
to two orders of magnitude on querying time over existing state-of-the-art
top-k techniques. Further, a qualitative analysis is performed on a real
dataset to highlight the potential of the proposed query in discovering hidden
data characteristics."
"Newly-released web applications often succumb to a ""Success Disaster,"" where
overloaded database machines and resulting high response times destroy a
previously good user experience. Unfortunately, the data independence provided
by a traditional relational database system, while useful for agile
development, only exacerbates the problem by hiding potentially expensive
queries under simple declarative expressions. As a result, developers of these
applications are increasingly abandoning relational databases in favor of
imperative code written against distributed key/value stores, losing the many
benefits of data independence in the process. Instead, we propose PIQL, a
declarative language that also provides scale independence by calculating an
upper bound on the number of key/value store operations that will be performed
for any query. Coupled with a service level objective (SLO) compliance
prediction model and PIQL's scalable database architecture, these bounds make
it easy for developers to write success-tolerant applications that support an
arbitrarily large number of users while still providing acceptable performance.
In this paper, we present the PIQL query processing system and evaluate its
scale independence on hundreds of machines using two benchmarks, TPC-W and
SCADr."
"Many dynamic applications are built upon large network infrastructures, such
as social networks, communication networks, biological networks and the Web.
Such applications create data that can be naturally modeled as graph streams,
in which edges of the underlying graph are received and updated sequentially in
a form of a stream. It is often necessary and important to summarize the
behavior of graph streams in order to enable effective query processing.
However, the sheer size and dynamic nature of graph streams present an enormous
challenge to existing graph management techniques. In this paper, we propose a
new graph sketch method, gSketch, which combines well studied synopses for
traditional data streams with a sketch partitioning technique, to estimate and
optimize the responses to basic queries on graph streams. We consider two
different scenarios for query estimation: (1) A graph stream sample is
available; (2) Both a graph stream sample and a query workload sample are
available. Algorithms for different scenarios are designed respectively by
partitioning a global sketch to a group of localized sketches in order to
optimize the query estimation accuracy. We perform extensive experimental
studies on both real and synthetic data sets and demonstrate the power and
robustness of gSketch in comparison with the state-of-the-art global sketch
method."
"Querying uncertain data sets (represented as probability distributions)
presents many challenges due to the large amount of data involved and the
difficulties comparing uncertainty between distributions. The Earth Mover's
Distance (EMD) has increasingly been employed to compare uncertain data due to
its ability to effectively capture the differences between two distributions.
Computing the EMD entails finding a solution to the transportation problem,
which is computationally intensive. In this paper, we propose a new lower bound
to the EMD and an index structure to significantly improve the performance of
EMD based K-nearest neighbor (K-NN) queries on uncertain databases. We propose
a new lower bound to the EMD that approximates the EMD on a projection vector.
Each distribution is projected onto a vector and approximated by a normal
distribution, as well as an accompanying error term. We then represent each
normal as a point in a Hough transformed space. We then use the concept of
stochastic dominance to implement an efficient index structure in the
transformed space. We show that our method significantly decreases K-NN query
time on uncertain databases. The index structure also scales well with database
cardinality. It is well suited for heterogeneous data sets, helping to keep EMD
based queries tractable as uncertain data sets become larger and more complex."
"A previously proposed keyword search paradigm produces, as a query result, a
ranked list of Object Summaries (OSs). An OS is a tree structure of related
tuples that summarizes all data held in a relational database about a
particular Data Subject (DS). However, some of these OSs are very large in size
and therefore unfriendly to users that initially prefer synoptic information
before proceeding to more comprehensive information about a particular DS. In
this paper, we investigate the effective and efficient retrieval of concise and
informative OSs. We argue that a good size-l OS should be a stand-alone and
meaningful synopsis of the most important information about the particular DS.
More precisely, we define a size-l OS as a partial OS composed of l important
tuples. We propose three algorithms for the efficient generation of size-l OSs
(in addition to the optimal approach which requires exponential time).
Experimental evaluation on DBLP and TPC-H databases verifies the effectiveness
and efficiency of our approach."
"Knowledge bases of entities and relations (either constructed manually or
automatically) are behind many real world search engines, including those at
Yahoo!, Microsoft, and Google. Those knowledge bases can be viewed as graphs
with nodes representing entities and edges representing (primary)
relationships, and various studies have been conducted on how to leverage them
to answer entity seeking queries. Meanwhile, in a complementary direction,
analyses over the query logs have enabled researchers to identify entity pairs
that are statistically correlated. Such entity relationships are then presented
to search users through the ""related searches"" feature in modern search
engines. However, entity relationships thus discovered can often be ""puzzling""
to the users because why the entities are connected is often indescribable. In
this paper, we propose a novel problem called ""entity relationship
explanation"", which seeks to explain why a pair of entities are connected, and
solve this challenging problem by integrating the above two complementary
approaches, i.e., we leverage the knowledge base to ""explain"" the connections
discovered between entity pairs. More specifically, we present REX, a system
that takes a pair of entities in a given knowledge base as input and
efficiently identifies a ranked list of relationship explanations. We formally
define relationship explanations and analyze their desirable properties.
Furthermore, we design and implement algorithms to efficiently enumerate and
rank all relationship explanations based on multiple measures of
""interestingness."" We perform extensive experiments over real web-scale data
gathered from DBpedia and a commercial search engine, demonstrating the
efficiency and scalability of REX. We also perform user studies to corroborate
the effectiveness of explanations generated by REX."
"As an essential operation in data cleaning, the similarity join has attracted
considerable attention from the database community. In this paper, we study
string similarity joins with edit-distance constraints, which find similar
string pairs from two large sets of strings whose edit distance is within a
given threshold. Existing algorithms are efficient either for short strings or
for long strings, and there is no algorithm that can efficiently and adaptively
support both short strings and long strings. To address this problem, we
propose a partition-based method called Pass-Join. Pass-Join partitions a
string into a set of segments and creates inverted indices for the segments.
Then for each string, Pass-Join selects some of its substrings and uses the
selected substrings to find candidate pairs using the inverted indices. We
devise efficient techniques to select the substrings and prove that our method
can minimize the number of selected substrings. We develop novel pruning
techniques to efficiently verify the candidate pairs. Experimental results show
that our algorithms are efficient for both short strings and long strings, and
outperform state-of-the-art methods on real datasets."
"Taking advantage of the Web, many advertisements (ads for short) websites,
which aspire to increase client's transactions and thus profits, offer
searching tools which allow users to (i) post keyword queries to capture their
information needs or (ii) invoke form-based interfaces to create queries by
selecting search options, such as a price range, filled-in entries, check
boxes, or drop-down menus. These search mechanisms, however, are inadequate,
since they cannot be used to specify a natural-language query with rich
syntactic and semantic content, which can only be handled by a question
answering (QA) system. Furthermore, existing ads websites are incapable of
evaluating arbitrary Boolean queries or retrieving partiallymatched answers
that might be of interest to the user whenever a user's search yields only a
few or no results at all. In solving these problems, we present a QA system for
ads, called CQAds, which (i) allows users to post a natural-language question Q
for retrieving relevant ads, if they exist, (ii) identifies ads as answers that
partially-match the requested information expressed in Q, if insufficient or no
answers to Q can be retrieved, which are ordered using a similarity-ranking
approach, and (iii) analyzes incomplete or ambiguous questions to perform the
""best guess"" in retrieving answers that ""best match"" the selection criteria
specified in Q. CQAds is also equipped with a Boolean model to evaluate Boolean
operators that are either explicitly or implicitly specified in Q, i.e., with
or without Boolean operators specified by the users, respectively. CQAds is
easy to use, scalable to all ads domains, and more powerful than search tools
provided by existing ads websites, since its query-processing strategy
retrieves relevant ads of higher quality and quantity. We have verified the
accuracy of CQAds in retrieving ads on eight ads domains and compared
it...[truncated]."
"Hybrid human/computer systems promise to greatly expand the usefulness of
query processing by incorporating the crowd for data gathering and other tasks.
Such systems raise many database system implementation questions. Perhaps most
fundamental is that the closed world assumption underlying relational query
semantics does not hold in such systems. As a consequence the meaning of even
simple queries can be called into question. Furthermore query progress
monitoring becomes difficult due to non-uniformities in the arrival of
crowdsourced data and peculiarities of how people work in crowdsourcing
systems. To address these issues, we develop statistical tools that enable
users and systems developers to reason about tradeoffs between time/cost and
completeness. These tools can also help drive query execution and crowdsourcing
strategies. We evaluate our techniques using experiments on a popular
crowdsourcing platform."
"We present a novel technique to identify calendar-based (annual, monthly and
daily) periodicities of an interval-based temporal pattern. An interval-based
temporal pattern is a pattern that occurs across a time-interval, then
disappears for some time, again recurs across another time-interval and so on
and so forth. Given the sequence of time-intervals in which an interval-based
temporal pattern has occurred, we propose a method for identifying the extent
to which the pattern is periodic with respect to a calendar cycle. In
comparison to previous work, our method is asymptotically faster. We also show
an interesting relationship between periodicities across different levels of
any hierarchical timestamp (year/month/day, hour/minute/second etc.)."
"With the randomization approach, sensitive data items of records are
randomized to protect privacy of individuals while allowing the distribution
information to be reconstructed for data analysis. In this paper, we
distinguish between reconstruction that has potential privacy risk, called
micro reconstruction, and reconstruction that does not, called aggregate
reconstruction. We show that the former could disclose sensitive information
about a target individual, whereas the latter is more useful for data analysis
than for privacy breaches. To limit the privacy risk of micro reconstruction,
we propose a privacy definition, called (epsilon,delta)-reconstruction-privacy.
Intuitively, this privacy notion requires that micro reconstruction has a large
error with a large probability. The promise of this approach is that micro
reconstruction is more sensitive to the number of independent trials in the
randomization process than aggregate reconstruction is; therefore, reducing the
number of independent trials helps achieve
(epsilon,delta)-reconstruction-privacy while preserving the accuracy of
aggregate reconstruction. We present an algorithm based on this idea and
evaluate the effectiveness of this approach using real life data sets."
"Data quality on categorical attribute is a difficult problem that has not
received as much attention as numerical counterpart. Our basic idea is to
employ association rule for the purpose of data quality measurement. Strong
rule generation is an important area of data mining. Association rule mining
problems can be considered as a multi objective problem rather than as a single
objective one. The main area of concentration was the rules generated by
association rule mining using genetic algorithm. The advantage of using genetic
algorithm is to discover high level prediction rules is that they perform a
global search and cope better with attribute interaction than the greedy rule
induction algorithm often used in data mining. Genetic algorithm based approach
utilizes the linkage between association rule and feature selection. In this
paper, we put forward a Multi objective genetic algorithm approach for data
quality on categorical attributes. The result shows that our approach is
outperformed by the objectives like accuracy, completeness, comprehensibility
and interestingness."
"While the introduction of differential privacy has been a major breakthrough
in the study of privacy preserving data publication, some recent work has
pointed out a number of cases where it is not possible to limit inference about
individuals. The dilemma that is intrinsic in the problem is the simultaneous
requirement of data utility in the published data. Differential privacy does
not aim to protect information about an individual that can be uncovered even
without the participation of the individual. However, this lack of coverage may
violate the principle of individual privacy. Here we propose a solution by
providing protection to sensitive information, by which we refer to the answers
for aggregate queries with small counts. Previous works based on
$\ell$-diversity can be seen as providing a special form of this kind of
protection. Our method is developed with another goal which is to provide
differential privacy guarantee, and for that we introduce a more refined form
of differential privacy to deal with certain practical issues. Our empirical
studies show that our method can preserve better utilities than a number of
state-of-the-art methods although these methods do not provide the protections
that we provide."
"Sharing real-time aggregate statistics of private data is of great value to
the public to perform data mining for understanding important phenomena, such
as Influenza outbreaks and traffic congestion. However, releasing time-series
data with standard differential privacy mechanism has limited utility due to
high correlation between data values. We propose FAST, a novel framework to
release real-time aggregate statistics under differential privacy based on
filtering and adaptive sampling. To minimize the overall privacy cost, FAST
adaptively samples long time-series according to the detected data dynamics. To
improve the accuracy of data release per time stamp, FAST predicts data values
at non-sampling points and corrects noisy observations at sampling points. Our
experiments with real-world as well as synthetic data sets confirm that FAST
improves the accuracy of released aggregates even under small privacy cost and
can be used to enable a wide range of monitoring applications."
"Mapping relational databases to RDF is a fundamental problem for the
development of the Semantic Web. We present a solution, inspired by draft
methods defined by the W3C where relational databases are directly mapped to
RDF and OWL. Given a relational database schema and its integrity constraints,
this direct mapping produces an OWL ontology, which, provides the basis for
generating RDF instances. The semantics of this mapping is defined using
Datalog. Two fundamental properties are information preservation and query
preservation. We prove that our mapping satisfies both conditions, even for
relational databases that contain null values. We also consider two desirable
properties: monotonicity and semantics preservation. We prove that our mapping
is monotone and also prove that no monotone mapping, including ours, is
semantic preserving. We realize that monotonicity is an obstacle for semantic
preservation and thus present a non-monotone direct mapping that is semantics
preserving."
"So far, privacy models follow two paradigms. The first paradigm, termed
inferential privacy in this paper, focuses on the risk due to statistical
inference of sensitive information about a target record from other records in
the database. The second paradigm, known as differential privacy, focuses on
the risk to an individual when included in, versus when not included in, the
database. The contribution of this paper consists of two parts. The first part
presents a critical analysis on differential privacy with two results: (i) the
differential privacy mechanism does not provide inferential privacy, (ii) the
impossibility result about achieving Dalenius's privacy goal [5] is based on an
adversary simulated by a Turing machine, but a human adversary may behave
differently; consequently, the practical implication of the impossibility
result remains unclear. The second part of this work is devoted to a solution
addressing three major drawbacks in previous approaches to inferential privacy:
lack of flexibility for handling variable sensitivity, poor utility, and
vulnerability to auxiliary information."
"We propose a novel mechanism for answering sets of count- ing queries under
differential privacy. Given a workload of counting queries, the mechanism
automatically selects a different set of ""strategy"" queries to answer
privately, using those answers to derive answers to the workload. The main
algorithm proposed in this paper approximates the optimal strategy for any
workload of linear counting queries. With no cost to the privacy guarantee, the
mechanism improves significantly on prior approaches and achieves near-optimal
error for many workloads, when applied under (\epsilon, \delta)-differential
privacy. The result is an adaptive mechanism which can help users achieve good
utility without requiring that they reason carefully about the best formulation
of their task."
"In this paper a new mining algorithm is defined based on frequent item set.
Apriori Algorithm scans the database every time when it finds the frequent item
set so it is very time consuming and at each step it generates candidate item
set. So for large databases it takes lots of space to store candidate item set
.In undirected item set graph, it is improvement on apriori but it takes time
and space for tree generation. The defined algorithm scans the database at the
start only once and then from that scanned data base it generates the Trade
List. It contains the information of whole database. By considering minimum
support it finds the frequent item set and by considering the minimum
confidence it generates the association rule. If database and minimum support
is changed, the new algorithm finds the new frequent items by scanning Trade
List. That is why it's executing efficiency is improved distinctly compared to
traditional algorithm."
"Differential privacy is a strong notion for protecting individual privacy in
privacy preserving data analysis or publishing. In this paper, we study the
problem of differentially private histogram release for random workloads. We
study two multidimensional partitioning strategies including: 1) a baseline
cell-based partitioning strategy for releasing an equi-width cell histogram,
and 2) an innovative 2-phase kd-tree based partitioning strategy for releasing
a v-optimal histogram. We formally analyze the utility of the released
histograms and quantify the errors for answering linear queries such as
counting queries. We formally characterize the property of the input data that
will guarantee the optimality of the algorithm. Finally, we implement and
experimentally evaluate several applications using the released histograms,
including counting queries, classification, and blocking for record linkage and
show the benefit of our approach."
"We consider Location-based Service (LBS) settings, where a LBS provider logs
the requests sent by mobile device users over a period of time and later wants
to publish/share these logs. Log sharing can be extremely valuable for
advertising, data mining research and network management, but it poses a
serious threat to the privacy of LBS users. Sender anonymity solutions prevent
a malicious attacker from inferring the interests of LBS users by associating
them with their service requests after gaining access to the anonymized logs.
With the fast-increasing adoption of smartphones and the concern that historic
user trajectories are becoming more accessible, it becomes necessary for any
sender anonymity solution to protect against attackers that are
trajectory-aware (i.e. have access to historic user trajectories) as well as
policy-aware (i.e they know the log anonymization policy). We call such
attackers TP-aware.
  This paper introduces a first privacy guarantee against TP-aware attackers,
called TP-aware sender k-anonymity. It turns out that there are many possible
TP-aware anonymizations for the same LBS log, each with a different utility to
the consumer of the anonymized log. The problem of finding the optimal TP-aware
anonymization is investigated. We show that trajectory-awareness renders the
problem computationally harder than the trajectory-unaware variants found in
the literature (NP-complete in the size of the log, versus PTIME). We describe
a PTIME l-approximation algorithm for trajectories of length l and empirically
show that it scales to large LBS logs (up to 2 million users)."
"We present the design of a structured search engine which returns a
multi-column table in response to a query consisting of keywords describing
each of its columns. We answer such queries by exploiting the millions of
tables on the Web because these are much richer sources of structured knowledge
than free-format text. However, a corpus of tables harvested from arbitrary
HTML web pages presents huge challenges of diversity and redundancy not seen in
centrally edited knowledge bases. We concentrate on one concrete task in this
paper. Given a set of Web tables T1, . . ., Tn, and a query Q with q sets of
keywords Q1, . . ., Qq, decide for each Ti if it is relevant to Q and if so,
identify the mapping between the columns of Ti and query columns. We represent
this task as a graphical model that jointly maps all tables by incorporating
diverse sources of clues spanning matches in different parts of the table,
corpus-wide co-occurrence statistics, and content overlap across table columns.
We define a novel query segmentation model for matching keywords to table
columns, and a robust mechanism of exploiting content overlap across table
columns. We design efficient inference algorithms based on bipartite matching
and constrained graph cuts to solve the joint labeling task. Experiments on a
workload of 59 queries over a 25 million web table corpus shows significant
boost in accuracy over baseline IR methods."
"The purpose of data warehouses is to enable business analysts to make better
decisions. Over the years the technology has matured and data warehouses have
become extremely successful. As a consequence, more and more data has been
added to the data warehouses and their schemas have become increasingly
complex. These systems still work great in order to generate pre-canned
reports. However, with their current complexity, they tend to be a poor match
for non tech-savvy business analysts who need answers to ad-hoc queries that
were not anticipated. This paper describes the design, implementation, and
experience of the SODA system (Search over DAta Warehouse). SODA bridges the
gap between the business needs of analysts and the technical complexity of
current data warehouses. SODA enables a Google-like search experience for data
warehouses by taking keyword queries of business users and automatically
generating executable SQL. The key idea is to use a graph pattern matching
algorithm that uses the metadata model of the data warehouse. Our results with
real data from a global player in the financial services industry show that
SODA produces queries with high precision and recall, and makes it much easier
for business users to interactively explore highly-complex data warehouses."
"In this work, we focus on protection against identity disclosure in the
publication of sparse multidimensional data. Existing multidimensional
anonymization techniquesa) protect the privacy of users either by altering the
set of quasi-identifiers of the original data (e.g., by generalization or
suppression) or by adding noise (e.g., using differential privacy) and/or (b)
assume a clear distinction between sensitive and non-sensitive information and
sever the possible linkage. In many real world applications the above
techniques are not applicable. For instance, consider web search query logs.
Suppressing or generalizing anonymization methods would remove the most
valuable information in the dataset: the original query terms. Additionally,
web search query logs contain millions of query terms which cannot be
categorized as sensitive or non-sensitive since a term may be sensitive for a
user and non-sensitive for another. Motivated by this observation, we propose
an anonymization technique termed disassociation that preserves the original
terms but hides the fact that two or more different terms appear in the same
record. We protect the users' privacy by disassociating record terms that
participate in identifying combinations. This way the adversary cannot
associate with high probability a record with a rare combination of terms. To
the best of our knowledge, our proposal is the first to employ such a technique
to provide protection against identity disclosure. We propose an anonymization
algorithm based on our approach and evaluate its performance on real and
synthetic datasets, comparing it against other state-of-the-art methods based
on generalization and differential privacy."
"Recommender systems based on latent factor models have been effectively used
for understanding user interests and predicting future actions. Such models
work by projecting the users and items into a smaller dimensional space,
thereby clustering similar users and items together and subsequently compute
similarity between unknown user-item pairs. When user-item interactions are
sparse (sparsity problem) or when new items continuously appear (cold start
problem), these models perform poorly. In this paper, we exploit the
combination of taxonomies and latent factor models to mitigate these issues and
improve recommendation accuracy. We observe that taxonomies provide structure
similar to that of a latent factor model: namely, it imposes human-labeled
categories (clusters) over items. This leads to our proposed taxonomy-aware
latent factor model (TF) which combines taxonomies and latent factors using
additive models. We develop efficient algorithms to train the TF models, which
scales to large number of users/items and develop scalable
inference/recommendation algorithms by exploiting the structure of the
taxonomy. In addition, we extend the TF model to account for the temporal
dynamics of user interests using high-order Markov chains. To deal with
large-scale data, we develop a parallel multi-core implementation of our TF
model. We empirically evaluate the TF model for the task of predicting user
purchases using a real-world shopping dataset spanning more than a million
users and products. Our experiments demonstrate the benefits of using our TF
models over existing approaches, in terms of both prediction accuracy and
running time."
"Applications ranging from algorithmic trading to scientific data analysis
require realtime analytics based on views over databases that change at very
high rates. Such views have to be kept fresh at low maintenance cost and
latencies. At the same time, these views have to support classical SQL, rather
than window semantics, to enable applications that combine current with aged or
historical data. In this paper, we present viewlet transforms, a recursive
finite differencing technique applied to queries. The viewlet transform
materializes a query and a set of its higher-order deltas as views. These views
support each other's incremental maintenance, leading to a reduced overall view
maintenance cost. The viewlet transform of a query admits efficient evaluation,
the elimination of certain expensive query operations, and aggressive
parallelization. We develop viewlet transforms into a workable query execution
technique, present a heuristic and cost-based optimization framework, and
report on experiments with a prototype dynamic data management system that
combines viewlet transforms with an optimizing compilation technique. The
system supports tens of thousands of complete view refreshes a second for a
wide range of queries."
"While traditional data-management systems focus on evaluating single, ad-hoc
queries over static data sets in a centralized setting, several emerging
applications require (possibly, continuous) answers to queries on dynamic data
that is widely distributed and constantly updated. Furthermore, such query
answers often need to discount data that is ""stale"", and operate solely on a
sliding window of recent data arrivals (e.g., data updates occurring over the
last 24 hours). Such distributed data streaming applications mandate novel
algorithmic solutions that are both time- and space-efficient (to manage
high-speed data streams), and also communication-efficient (to deal with
physical data distribution). In this paper, we consider the problem of complex
query answering over distributed, high-dimensional data streams in the
sliding-window model. We introduce a novel sketching technique (termed
ECM-sketch) that allows effective summarization of streaming data over both
time-based and count-based sliding windows with probabilistic accuracy
guarantees. Our sketch structure enables point as well as inner-product
queries, and can be employed to address a broad range of problems, such as
maintaining frequency statistics, finding heavy hitters, and computing
quantiles in the sliding-window model. Focusing on distributed environments, we
demonstrate how ECM-sketches of individual, local streams can be composed to
generate a (low-error) ECM-sketch summary of the order-preserving aggregation
of all streams; furthermore, we show how ECM-sketches can be exploited for
continuous monitoring of sliding-window queries over distributed streams. Our
extensive experimental study with two real-life data sets validates our
theoretical claims and verifies the effectiveness of our techniques. To the
best of our knowledge, ours is the first work to address efficient,
guaranteed-error complex query answ...[truncated]."
"Numerous applications such as financial transactions (e.g., stock trading)
are write-heavy in nature. The shift from reads to writes in web applications
has also been accelerating in recent years. Write-ahead-logging is a common
approach for providing recovery capability while improving performance in most
storage systems. However, the separation of log and application data incurs
write overheads observed in write-heavy environments and hence adversely
affects the write throughput and recovery time in the system. In this paper, we
introduce LogBase - a scalable log-structured database system that adopts
log-only storage for removing the write bottleneck and supporting fast system
recovery. LogBase is designed to be dynamically deployed on commodity clusters
to take advantage of elastic scaling property of cloud environments. LogBase
provides in-memory multiversion indexes for supporting efficient access to data
maintained in the log. LogBase also supports transactions that bundle read and
write operations spanning across multiple records. We implemented the proposed
system and compared it with HBase and a disk-based log-structured
record-oriented system modeled after RAMCloud. The experimental results show
that LogBase is able to provide sustained write throughput, efficient data
access out of the cache, and effective system recovery."
"k nearest neighbor join (kNN join), designed to find k nearest neighbors from
a dataset S for every object in another dataset R, is a primitive operation
widely adopted by many data mining applications. As a combination of the k
nearest neighbor query and the join operation, kNN join is an expensive
operation. Given the increasing volume of data, it is difficult to perform a
kNN join on a centralized machine efficiently. In this paper, we investigate
how to perform kNN join using MapReduce which is a well-accepted framework for
data-intensive applications over clusters of computers. In brief, the mappers
cluster objects into groups; the reducers perform the kNN join on each group of
objects separately. We design an effective mapping mechanism that exploits
pruning rules for distance filtering, and hence reduces both the shuffling and
computational costs. To reduce the shuffling cost, we propose two approximate
algorithms to minimize the number of replicas. Extensive experiments on our
in-house cluster demonstrate that our proposed methods are efficient, robust
and scalable."
"Approximate results based on samples often provide the only way in which
advanced analytical applications on very massive data sets can satisfy their
time and resource constraints. Unfortunately, methods and tools for the
computation of accurate early results are currently not supported in
MapReduce-oriented systems although these are intended for `big data'.
Therefore, we proposed and implemented a non-parametric extension of Hadoop
which allows the incremental computation of early results for arbitrary
work-flows, along with reliable on-line estimates of the degree of accuracy
achieved so far in the computation. These estimates are based on a technique
called bootstrapping that has been widely employed in statistics and can be
applied to arbitrary functions and data distributions. In this paper, we
describe our Early Accurate Result Library (EARL) for Hadoop that was designed
to minimize the changes required to the MapReduce framework. Various tests of
EARL of Hadoop are presented to characterize the frequent situations where EARL
can provide major speed-ups over the current version of Hadoop."
"Some complex problems, such as image tagging and natural language processing,
are very challenging for computers, where even state-of-the-art technology is
yet able to provide satisfactory accuracy. Therefore, rather than relying
solely on developing new and better algorithms to handle such tasks, we look to
the crowdsourcing solution -- employing human participation -- to make good the
shortfall in current technology. Crowdsourcing is a good supplement to many
computer tasks. A complex job may be divided into computer-oriented tasks and
human-oriented tasks, which are then assigned to machines and humans
respectively. To leverage the power of crowdsourcing, we design and implement a
Crowdsourcing Data Analytics System, CDAS. CDAS is a framework designed to
support the deployment of various crowdsourcing applications. The core part of
CDAS is a quality-sensitive answering model, which guides the crowdsourcing
engine to process and monitor the human tasks. In this paper, we introduce the
principles of our quality-sensitive model. To satisfy user required accuracy,
the model guides the crowdsourcing query engine for the design and processing
of the corresponding crowdsourcing jobs. It provides an estimated accuracy for
each generated result based on the human workers' historical performances. When
verifying the quality of the result, the model employs an online strategy to
reduce waiting time. To show the effectiveness of the model, we implement and
deploy two analytics jobs on CDAS, a twitter sentiment analytics job and an
image tagging job. We use real Twitter and Flickr data as our queries
respectively. We compare our approaches with state-of-the-art classification
and image annotation techniques. The results show that the human-assisted
methods can indeed achieve a much higher accuracy. By embedding the
quality-sensitive model into crowdsourcing query engine, we
effectiv...[truncated]."
"The problem of identification of statistically significant patterns in a
sequence of data has been applied to many domains such as intrusion detection
systems, financial models, web-click records, automated monitoring systems,
computational biology, cryptology, and text analysis. An observed pattern of
events is deemed to be statistically significant if it is unlikely to have
occurred due to randomness or chance alone. We use the chi-square statistic as
a quantitative measure of statistical significance. Given a string of
characters generated from a memoryless Bernoulli model, the problem is to
identify the substring for which the empirical distribution of single letters
deviates the most from the distribution expected from the generative Bernoulli
model. This deviation is captured using the chi-square measure. The most
significant substring (MSS) of a string is thus defined as the substring having
the highest chi-square value. Till date, to the best of our knowledge, there
does not exist any algorithm to find the MSS in better than O(n^2) time, where
n denotes the length of the string. In this paper, we propose an algorithm to
find the most significant substring, whose running time is O(n^{3/2}) with high
probability. We also study some variants of this problem such as finding the
top-t set, finding all substrings having chi-square greater than a fixed
threshold and finding the MSS among substrings greater than a given length. We
experimentally demonstrate the asymptotic behavior of the MSS on varying the
string size and alphabet size. We also describe some applications of our
algorithm on cryptology and real world data from finance and sports. Finally,
we compare our technique with the existing heuristics for finding the MSS."
"Two emerging hardware trends will dominate the database system technology in
the near future: increasing main memory capacities of several TB per server and
massively parallel multi-core processing. Many algorithmic and control
techniques in current database technology were devised for disk-based systems
where I/O dominated the performance. In this work we take a new look at the
well-known sort-merge join which, so far, has not been in the focus of research
in scalable massively parallel multi-core data processing as it was deemed
inferior to hash joins. We devise a suite of new massively parallel sort-merge
(MPSM) join algorithms that are based on partial partition-based sorting.
Contrary to classical sort-merge joins, our MPSM algorithms do not rely on a
hard to parallelize final merge step to create one complete sort order. Rather
they work on the independently created runs in parallel. This way our MPSM
algorithms are NUMA-affine as all the sorting is carried out on local memory
partitions. An extensive experimental evaluation on a modern 32-core machine
with one TB of main memory proves the competitive performance of MPSM on large
main memory databases with billions of objects. It scales (almost) linearly in
the number of employed cores and clearly outperforms competing hash join
proposals - in particular it outperforms the ""cutting-edge"" Vectorwise parallel
query engine by a factor of four."
"As storage systems become increasingly heterogeneous and complex, it adds
burdens on DBAs, causing suboptimal performance even after a lot of human
efforts have been made. In addition, existing monitoring-based storage
management by access pattern detections has difficulties to handle workloads
that are highly dynamic and concurrent. To achieve high performance by best
utilizing heterogeneous storage devices, we have designed and implemented a
heterogeneity-aware software framework for DBMS storage management called
hStorage-DB, where semantic information that is critical for storage I/O is
identified and passed to the storage manager. According to the collected
semantic information, requests are classified into different types. Each type
is assigned a proper QoS policy supported by the underlying storage system, so
that every request will be served with a suitable storage device. With
hStorage-DB, we can well utilize semantic information that cannot be detected
through data access monitoring but is particularly important for a hybrid
storage system. To show the effectiveness of hStorage-DB, we have implemented a
system prototype that consists of an I/O request classification enabled DBMS,
and a hybrid storage system that is organized into a two-level caching
hierarchy. Our performance evaluation shows that hStorage-DB can automatically
make proper decisions for data allocation in different storage devices and make
substantial performance improvements in a cost-efficient way."
"Educational data mining (EDM) is defined as the area of scientific inquiry
centered around the development of methods for making discoveries within the
unique kinds of data that come from educational settings, and using those
methods to better understand students and the settings which they learn in.
Data mining enables organizations to use their current reporting capabilities
to uncover and understand hidden patterns in vast databases. As a result of
this insight, institutions are able to allocate resources and staff more
effectively. In this paper, we present a real-world experiment conducted in
Shree Rayeshwar Institute of Engineering and Information Technology (SRIEIT) in
Goa, India. Here we found the relevant subjects in an undergraduate syllabus
and the strength of their relationship. We have also focused on classification
of students into different categories such as good, average, poor depending on
their marks scored by them by obtaining a decision tree which will predict the
performance of the students and accordingly help the weaker section of students
to improve in their academics. We have also found clusters of students for
helping in analyzing student's performance and also improvising the subject
teaching in that particular subject."
"Sorting database tables before compressing them improves the compression
rate. Can we do better than the lexicographical order? For minimizing the
number of runs in a run-length encoding compression scheme, the best approaches
to row-ordering are derived from traveling salesman heuristics, although there
is a significant trade-off between running time and compression. A new
heuristic, Multiple Lists, which is a variant on Nearest Neighbor that trades
off compression for a major running-time speedup, is a good option for very
large tables. However, for some compression schemes, it is more important to
generate long runs rather than few runs. For this case, another novel
heuristic, Vortex, is promising. We find that we can improve run-length
encoding up to a factor of 3 whereas we can improve prefix coding by up to 80%:
these gains are on top of the gains due to lexicographically sorting the table.
We prove that the new row reordering is optimal (within 10%) at minimizing the
runs of identical values within columns, in a few cases."
"Pattern tree are based on integrated rules which are equal to a combination
of some points connected to each other in a hierarchical structure, called
Enquiry Hierarchical (EH). The main operation in pattern enquiry seeking is to
locate the steps that match the given EH in the dataset. A point of algorithms
has offered for EH matching; but the majority of this algorithms seeks all of
the enquiry steps to access all EHs in the dataset. A few algorithms such as
seek only steps that satisfy end points of EH. All of above algorithms are
trying to locate a way just for investigating direct testing of steps and to
locate the answer of enquiry, directly via these points. In this paper, we
describe a novel algorithm to locate the answer of enquiry without access to
real point of the dataset blindly. In this algorithm, first, the enquiry will
be executed on enquiry schema and this leads to a schema. Using this plan, it
will be clear how to seek end steps and how to achieve enquiry dataset, before
seeking of the dataset steps. Therefore, none of dataset steps will be seek
blindly."
"Itemset mining has been an active area of research due to its successful
application in various data mining scenarios including finding association
rules. Though most of the past work has been on finding frequent itemsets,
infrequent itemset mining has demonstrated its utility in web mining,
bioinformatics and other fields. In this paper, we propose a new algorithm
based on the pattern-growth paradigm to find minimally infrequent itemsets. A
minimally infrequent itemset has no subset which is also infrequent. We also
introduce the novel concept of residual trees. We further utilize the residual
trees to mine multiple level minimum support itemsets where different
thresholds are used for finding frequent itemsets for different lengths of the
itemset. Finally, we analyze the behavior of our algorithm with respect to
different parameters and show through experiments that it outperforms the
competing ones."
"Functional dependencies (FDs) specify the intended data semantics while
violations of FDs indicate deviation from these semantics. In this paper, we
study a data cleaning problem in which the FDs may not be completely correct,
e.g., due to data evolution or incomplete knowledge of the data semantics. We
argue that the notion of relative trust is a crucial aspect of this problem: if
the FDs are outdated, we should modify them to fit the data, but if we suspect
that there are problems with the data, we should modify the data to fit the
FDs. In practice, it is usually unclear how much to trust the data versus the
FDs. To address this problem, we propose an algorithm for generating
non-redundant solutions (i.e., simultaneous modifications of the data and the
FDs) corresponding to various levels of relative trust. This can help users
determine the best way to modify their data and/or FDs to achieve consistency."
"In order to generate synthetic basket data sets for better benchmark testing,
it is important to integrate characteristics from real-life databases into the
synthetic basket data sets. The characteristics that could be used for this
purpose include the frequent itemsets and association rules. The problem of
generating synthetic basket data sets from frequent itemsets is generally
referred to as inverse frequent itemset mining. In this paper, we show that the
problem of approximate inverse frequent itemset mining is {\bf NP}-complete.
Then we propose and analyze an approximate algorithm for approximate inverse
frequent itemset mining, and discuss privacy issues related to the synthetic
basket data set. In particular, we propose an approximate algorithm to
determine the privacy leakage in a synthetic basket data set."
"A central problem in releasing aggregate information about sensitive data is
to do so accurately while providing a privacy guarantee on the output. Recent
work focuses on the class of linear queries, which include basic counting
queries, data cubes, and contingency tables. The goal is to maximize the
utility of their output, while giving a rigorous privacy guarantee. Most
results follow a common template: pick a ""strategy"" set of linear queries to
apply to the data, then use the noisy answers to these queries to reconstruct
the queries of interest. This entails either picking a strategy set that is
hoped to be good for the queries, or performing a costly search over the space
of all possible strategies.
  In this paper, we propose a new approach that balances accuracy and
efficiency: we show how to improve the accuracy of a given query set by
answering some strategy queries more accurately than others. This leads to an
efficient optimal noise allocation for many popular strategies, including
wavelets, hierarchies, Fourier coefficients and more. For the important case of
marginal queries we show that this strictly improves on previous methods, both
analytically and empirically. Our results also extend to ensuring that the
returned query answers are consistent with an (unknown) data set at minimal
extra cost in terms of time and noise."
"Covering-based rough set theory is an extension to classical rough set. The
main purpose of this paper is to study covering rough sets from a topological
point of view. The relationship among upper approximations based on topological
spaces are explored."
"Determining trust of data available in the Semantic Web is fundamental for
applications and users, in particular for linked open data obtained from SPARQL
endpoints. There exist several proposals in the literature to annotate SPARQL
query results with values from abstract models, adapting the seminal works on
provenance for annotated relational databases. We provide an approach capable
of providing provenance information for a large and significant fragment of
SPARQL 1.1, including for the first time the major non-monotonic constructs
under multiset semantics. The approach is based on the translation of SPARQL
into relational queries over annotated relations with values of the most
general m-semiring, and in this way also refuting a claim in the literature
that the OPTIONAL construct of SPARQL cannot be captured appropriately with the
known abstract models."
"Extracting the relevant information by exploiting the spatial data warehouse
becomes increasingly hard. In fact, because of the enormous amount of data
stored in the spatial data warehouse, the user, usually, don't know what part
of the cube contain the relevant information and what the forthcoming query
should be. As a solution, we propose to study the similarity between the
behaviors of the users, in term of the spatial MDX queries launched on the
system, as a basis to recommend the next relevant MDX query to the current
user. This paper introduces a new similarity measure for comparing spatial MDX
queries. The proposed similarity measure could directly support the development
of spatial personalization approaches. The proposed similarity measure takes
into account the basic components of the similarity assessment models: the
topology, the direction and the distance."
"For data integration in information ecosystems, semantic heterogeneity is a
known difficulty. In this paper, we propose Shadow Theory as the philosophical
foundation to address this issue. It is based on the notion of shadows in
Plato's Allegory of the Cave. What we can observe are just shadows, and
meanings of shadows are mental entities that only exist in viewers' cognitive
structures. With enterprise customer data integration example, we proposed six
design principles and algebra to support required operations."
"In this paper we are presenting a new way to disable DDL statements on some
specific PL/SQL procedures to a dba user in the Oracle database. Nowadays dba
users have access to a lot of data and source code even if they do not have
legal permissions to see or modify them. With this method we can disable the
ability to execute DDL and DML statements on some specific pl/sql procedures
from every Oracle database user even if it has a dba role. Oracle gives to
developer the possibility to wrap the pl/sql procedures, functions and packages
but those wrapped scripts can be unwrapped by using third party tools. The
scripts that we have developed analyzes all database sessions, and if they
detect a DML or a DDL statement from an unauthorized user to procedure,
function or package which should be protected then the execution of the
statement is denied. Furthermore, these scripts do not allow a dba user to drop
or disable the scripts themselves. In other words by managing sessions prior to
the execution of an eventual statement from a dba user, we can prevent the
execution of eventual statements which target our scripts."
"We extend RDF with the ability to represent property values that exist, but
are unknown or partially known, using constraints. Following ideas from the
incomplete information literature, we develop a semantics for this extension of
RDF, called RDFi, and study SPARQL query evaluation in this framework."
"Generating a huge number of association rules reduces their utility in the
decision making process, done by domain experts. In this context, based on the
theory of Formal Concept Analysis, we propose to extend the notion of Formal
Concept through the generalization of the notion of itemset in order to
consider the itemset as an intent, its support as the cardinality of the extent
and its relevance which is related to the confidence of rule. Accordingly, we
propose a new approach to extract interesting itemsets through the concept
coverage. This approach uses a new quality-criteria of a rule: the relevance
bringing a semantic added value to formal concept analysis approach to discover
association rules."
"Several researchers have explored the temporal aspect of association rules
mining. In this paper, we focus on the cyclic association rules, in order to
discover correlations among items characterized by regular cyclic variation
overtime. The overview of the state of the art has revealed the drawbacks of
proposed algorithm literatures, namely the excessive number of generated rules
which are not meeting the expert's expectations. To overcome these
restrictions, we have introduced our approach dedicated to generate the cyclic
association rules under constraints through a new method called
Constraint-Based Cyclic Association Rules CBCAR. The carried out experiments
underline the usefulness and the performance of our new approach."
"Studying materials informatics from a data mining perspective can be
beneficial for manufacturing and other industrial engineering applications.
Predictive data mining technique and machine learning algorithm are combined to
design a knowledge discovery system for the selection of engineering materials
that meet the design specifications. Predictive method-Naive Bayesian
classifier and Machine learning Algorithm - Pearson correlation coefficient
method were implemented respectively for materials classification and
selection. The knowledge extracted from the engineering materials data sets is
proposed for effective decision making in advanced engineering materials design
applications."
"The World Wide Web (WWW) is the repository of large number of web pages which
can be accessed via Internet by multiple users at the same time and therefore
it is Ubiquitous in nature. The search engine is a key application used to
search the web pages from this huge repository, which uses the link analysis
for ranking the web pages without considering the facts provided by them. A new
algorithm called Probability of Correctness of Facts(PCF)-Engine is proposed to
find the accuracy of the facts provided by the web pages. It uses the
Probability based similarity function (SIM) which performs the string matching
between the true facts and the facts of web pages to find their probability of
correctness. The existing semantic search engines, may give the relevant result
to the user query but may not be 100% accurate. Our algorithm computes
trustworthiness of websites to rank the web pages. Simulation results show that
our approach is efficient when compared with existing Voting and Truthfinder[1]
algorithms with respect to the trustworthiness of the websites."
"Grid services are heavily used for handling large distributed computations.
They are also very useful to handle heavy data intensive applications where
data are distributed in different sites. Most of the data grid services used in
such situations are meant for homogeneous data source. In case of Heterogeneous
data sources, most of the grid services that are available are designed such a
way that they must be identical in schema definition for their smooth
operation. But there can be situations where the grid site databases are
heterogeneous and their schema definition is different from the central schema
definition. In this paper we propose a light weight coherent grid service for
heterogeneous data sources that is very easily install. It can map and convert
the central SQL schema into that of the grid members and send queries to get
according results from heterogeneous data sources."
"Relational association rules reveal patterns hide in multiple tables.
Existing rules are usually evaluated through two measures, namely support and
confidence. However, these two measures may not be enough to describe the
strength of a rule. In this paper, we introduce granular association rules with
four measures to reveal connections between granules in two universes, and
propose three algorithms for rule mining. An example of such a rule might be
""40% men like at least 30% kinds of alcohol; 45% customers are men and 6%
products are alcohol."" Here 45%, 6%, 40%, and 30% are the source coverage, the
target coverage, the source confidence, and the target confidence,
respectively. With these measures, our rules are semantically richer than
existing ones. Three subtypes of rules are obtained through considering special
requirements on the source/target confidence. Then we define a rule mining
problem, and design a sandwich algorithm with different rule checking
approaches for different subtypes. Experiments on a real world dataset show
that the approaches dedicated to three subtypes are 2-3 orders of magnitudes
faster than the one for the general case. A forward algorithm and a backward
algorithm for one particular subtype can speed up the mining process further.
This work opens a new research trend concerning relational association rule
mining, granular computing and rough sets."
"Criminal data comes in a variety of formats, mandated by state, federal, and
international standards. Specifying the data in a unified fashion is necessary
for any system that intends to integrate with state, federal, and international
law enforcement agencies. However, the contents, format, and structure of the
data is highly inconsistent across jurisdictions, and each datum requires
different ways of being printed, transmitted, and displayed. The goal was to
design a system that is unified in its approach to specify data, and is
amenable to future ""unknown unknowns"". We have developed a domain-specific
language in Common Lisp which allows the specification of complex data with
evolving formats and structure, and is inter-operable with the Common Lisp
language. The resultant system has enabled the easy handling of complex
evolving information in the general criminal data environment and has made it
possible to manage and extend the system in a high-paced market. The language
has allowed the principal product of Secure Outcomes Inc. to enjoy success with
over 50 users throughout the United States."
"Discovering frequent itemset is a key difficulty in significant data mining
applications, such as the discovery of association rules, strong rules,
episodes, and minimal keys. The problem of developing models and algorithms for
multilevel association mining poses for new challenges for mathematics and
computer science. In this paper, we present a model of mining multilevel
association rules which satisfies the different minimum support at each level,
we have employed princer search concepts, multilevel taxonomy and different
minimum supports to find multilevel association rules in a given transaction
data set. This search is used only for maintaining and updating a new data
structure. It is used to prune early candidates that would normally encounter
in the top-down search. A main characteristic of the algorithms is that it does
not require explicit examination of every frequent itemsets, an example is also
given to demonstrate and support that the proposed mining algorithm can derive
the multiple-level association rules under different supports in a simple and
effective manner"
"Scientific endeavors such as large astronomical surveys generate databases on
the terabyte scale. These, usually multidimensional databases must be
visualized and mined in order to find interesting objects or to extract
meaningful and qualitatively new relationships. Many statistical algorithms
required for these tasks run reasonably fast when operating on small sets of
in-memory data, but take noticeable performance hits when operating on large
databases that do not fit into memory. We utilize new software technologies to
develop and evaluate fast multidimensional indexing schemes that inherently
follow the underlying, highly non-uniform distribution of the data: they are
layered uniform grid indices, hierarchical binary space partitioning, and
sampled flat Voronoi tessellation of the data. Our working database is the
5-dimensional magnitude space of the Sloan Digital Sky Survey with more than
270 million data points, where we show that these techniques can dramatically
speed up data mining operations such as finding similar objects by example,
classifying objects or comparing extensive simulation sets with observations.
We are also developing tools to interact with the multidimensional database and
visualize the data at multiple resolutions in an adaptive manner."
"World wide technological advancement has brought in a widespread change in
adoption and utilization of open source tools. Since, most of the organizations
across the globe deal with a large amount of data to be updated online and
transactions are made every second, managing, mining and processing this
dynamic data is very complex. Successful implementation of the data mining
technique requires a careful assessment of the various tools and algorithms
available to mining experts. This paper provides a comparative study of open
source data mining tools available to the professionals. Parameters influencing
the choice of apt tools in addition to the real time challenges are discussed.
However, it is well proven that agents aid in improving the performance of data
mining tools. This paper provides information on an agent-based framework for
data preprocessing with implementation details for the development of better
tool in the market. An integration of open source data mining tools with agent
simulation enable one to implement an effective data pre processing
architecture thereby providing robust capabilities of the application which can
be upgraded using a minimum of pre planning requirement from the application
developer."
"Data mining is one of the most important steps of the knowledge discovery in
databases process and is considered as significant subfield in knowledge
management. Research in data mining continues growing in business and in
learning organization over coming decades. This review paper explores the
applications of data mining techniques which have been developed to support
knowledge management process. The journal articles indexed in ScienceDirect
Database from 2007 to 2012 are analyzed and classified. The discussion on the
findings is divided into 4 topics: (i) knowledge resource; (ii) knowledge types
and/or knowledge datasets; (iii) data mining tasks; and (iv) data mining
techniques and applications used in knowledge management. The article first
briefly describes the definition of data mining and data mining functionality.
Then the knowledge management rationale and major knowledge management tools
integrated in knowledge management cycle are described. Finally, the
applications of data mining techniques in the process of knowledge management
are summarized and discussed."
"There is a growing need to semantically process and integrate clinical data
from different sources for Clinical Data Management and Clinical Decision
Support in the healthcare IT industry. In the clinical practice domain, the
semantic gap between clinical information systems and domain ontologies is
quite often difficult to bridge in one step. In this paper, we report our
experience in using a two-step formalization approach to formalize clinical
data, i.e. from database schemas to local formalisms and from local formalisms
to domain (unifying) formalisms. We use N3 rules to explicitly and formally
state the mapping from local ontologies to domain ontologies. The resulting
data expressed in domain formalisms can be integrated and analyzed, though
originating from very distinct sources. Practices of applying the two-step
approach in the infectious disorders and cancer domains are introduced."
"We present an experimental study of large-scale RDF federations on top of the
Bio2RDF data sources, involving 29 data sets with more than four billion RDF
triples deployed in a local federation. Our federation is driven by FedX, a
highly optimized federation mediator for Linked Data. We discuss design
decisions, technical aspects, and experiences made in setting up and optimizing
the Bio2RDF federation, and present an exhaustive experimental evaluation of
the federation scenario. In addition to a controlled setting with local
federation members, we study implications arising in a hybrid setting, where
local federation members interact with remote federation members exhibiting
higher network latency. The outcome demonstrates the feasibility of federated
semantic data management in general and indicates remaining bottlenecks and
research opportunities that shall serve as a guideline for future work in the
area of federated semantic data processing."
"Forms are our gates to the web. They enable us to access the deep content of
web sites. Automatic form understanding provides applications, ranging from
crawlers over meta-search engines to service integrators, with a key to this
content. Yet, it has received little attention other than as component in
specific applications such as crawlers or meta-search engines. No comprehensive
approach to form understanding exists, let alone one that produces rich models
for semantic services or integration with linked open data.
  In this paper, we present OPAL, the first comprehensive approach to form
understanding and integration. We identify form labeling and form
interpretation as the two main tasks involved in form understanding. On both
problems OPAL pushes the state of the art: For form labeling, it combines
features from the text, structure, and visual rendering of a web page. In
extensive experiments on the ICQ and TEL-8 benchmarks and a set of 200 modern
web forms OPAL outperforms previous approaches for form labeling by a
significant margin. For form interpretation, OPAL uses a schema (or ontology)
of forms in a given domain. Thanks to this domain schema, it is able to produce
nearly perfect (more than 97 percent accuracy in the evaluation domains) form
interpretations. Yet, the effort to produce a domain schema is very low, as we
provide a Datalog-based template language that eases the specification of such
schemata and a methodology for deriving a domain schema largely automatically
from an existing domain ontology. We demonstrate the value of the form
interpretations in OPAL through a light-weight form integration system that
successfully translates and distributes master queries to hundreds of forms
with no error, yet is implemented with only a handful translation rules."
"The extraction of multi-attribute objects from the deep web is the bridge
between the unstructured web and structured data. Existing approaches either
induce wrappers from a set of human-annotated pages or leverage repeated
structures on the page without supervision. What the former lack in automation,
the latter lack in accuracy. Thus accurate, automatic multi-attribute object
extraction has remained an open challenge.
  AMBER overcomes both limitations through mutual supervision between the
repeated structure and automatically produced annotations. Previous approaches
based on automatic annotations have suffered from low quality due to the
inherent noise in the annotations and have attempted to compensate by exploring
multiple candidate wrappers. In contrast, AMBER compensates for this noise by
integrating repeated structure analysis with annotation-based induction: The
repeated structure limits the search space for wrapper induction, and
conversely, annotations allow the repeated structure analysis to distinguish
noise from relevant data. Both, low recall and low precision in the annotations
are mitigated to achieve almost human quality (more than 98 percent)
multi-attribute object extraction.
  To achieve this accuracy, AMBER needs to be trained once for an entire
domain. AMBER bootstraps its training from a small, possibly noisy set of
attribute instances and a few unannotated sites of the domain."
"Cooperative database systems support a database user by searching for answers
that are closely related to his query and hence are informative answers. Common
operators to relax the user query are Dropping Condition, Anti-Instantiation
and Goal Replacement. In this article, we provide an algebraic version of these
operators. Moreover we propose some heuristics to assign a degree of similarity
to each tuple of an answer table; this degree can help the user to determine
whether this answer is relevant for him or not."
"In this paper we tackle the fragmentation problem for highly distributed
databases. In such an environment, a suitable fragmentation strategy may
provide scalability and availability by minimizing distributed transactions. We
propose an approach for XML fragmentation that takes as input both the
application's expected workload and a storage threshold, and produces as output
an XML fragmentation schema. Our workload-aware method aims to minimize the
execution of distributed transactions by packing up related data in a small set
of fragments. We present experiments that compare alternative fragmentation
schemas, showing that the one produced by our technique provides a
finer-grained result and better system throughput."
"The advancement of mobile technologies and the proliferation of map-based
applications have enabled a user to access a wide variety of services that
range from information queries to navigation systems. Due to the popularity of
map-based applications among the users, the service provider often requires to
answer a large number of simultaneous queries. Thus, processing queries
efficiently on spatial networks (i.e., road networks) have become an important
research area in recent years. In this paper, we focus on path queries that
find the shortest path between a source and a destination of the user. In
particular, we address the problem of finding the shortest paths for a large
number of simultaneous path queries in road networks. Traditional systems that
consider one query at a time are not suitable for many applications due to high
computational and service costs. These systems cannot guarantee required
response time in high load conditions. We propose an efficient group based
approach that provides a practical solution with reduced cost. The key concept
for our approach is to group queries that share a common travel path and then
compute the shortest path for the group. Experimental results show that our
approach is on an average ten times faster than the traditional approach in
return of sacrificing the accuracy by 0.5% in the worst case, which is
acceptable for most of the users."
"Association rule has been an area of active research in the field of
knowledge discovery. Data mining researchers had improved upon the quality of
association rule mining for business development by incorporating influential
factors like value (utility), quantity of items sold (weight) and more for the
mining of association patterns. In this paper, we propose an efficient approach
to find maximal frequent itemset first. Most of the algorithms in literature
used to find minimal frequent item first, then with the help of minimal
frequent itemsets derive the maximal frequent itemsets. These methods consume
more time to find maximal frequent itemsets. To overcome this problem, we
propose a navel approach to find maximal frequent itemset directly using the
concepts of subsets. The proposed method is found to be efficient in finding
maximal frequent itemsets."
"This paper present several refinements of the Datalog +/- framework based on
resolution and Datalog-rewriting. We first present a resolution algorithm which
is complete for arbitrary sets of tgds and egds. We then show that a technique
of saturation can be used to achieve completeness with respect to First-Order
(FO) query rewriting. We then investigate the class of guarded tgds (with a
loose definition of guardedness), and show that every set of tgds in this class
can be rewritten into an equivalent set of standard Datalog rules. On the
negative side, this implies that Datalog +/- has (only) the same expressive
power as standard Datalog in terms of query answering. On the positive side
however, this mean that known results and existing optimization techniques
(such as Magic-Set) may be applied in the context of Datalog +/- despite its
richer syntax."
"Efficient discovery of frequent itemsets in large datasets is a crucial task
of data mining. In recent years, several approaches have been proposed for
generating high utility patterns, they arise the problems of producing a large
number of candidate itemsets for high utility itemsets and probably degrades
mining performance in terms of speed and space. Recently proposed compact tree
structure, viz., UP Tree, maintains the information of transactions and
itemsets, facilitate the mining performance and avoid scanning original
database repeatedly. In this paper, UP Tree (Utility Pattern Tree) is adopted,
which scans database only twice to obtain candidate items and manage them in an
efficient data structured way. Applying UP Tree to the UP Growth takes more
execution time for Phase II. Hence this paper presents modified algorithm
aiming to reduce the execution time by effectively identifying high utility
itemsets."
"In this paper, we propose the mqr-tree, a two-dimensional spatial access
method that organizes spatial objects in a two-dimensional node and based on
their spatial relationships. Previously proposed spatial access methods that
attempt to maintain spatial relationships between objects in their structures
are limited in their incorporation of existing one-dimensional spatial access
methods, or have lower space utilization in its nodes, and higher tree height,
overcoverage and overlap than is necessary. The mqr-tree utilizes a node
organization, set of spatial relationship rules and insertion strategy in order
to gain significant improvements in overlap and overcoverage. In addition,
other desirable properties are identified as a result of the chosen node
organization and insertion strategies. In particular, zero overlap is achieved
when the mqr-tree is used to index point data. A comparison of the mqr-tree
insertion strategy versus the R-tree shows significant improvements in overlap
and overcoverage, with comparable space utilization. In addition, a comparison
of region searching shows that the mqr-tree achieves a lower number of disk
accesses in many cases"
"Presently, large enterprises rely on database systems to manage their data
and information. These databases are useful for conducting daily business
transactions. However, the tight competition in the marketplace has led to the
concept of data mining in which data are analyzed to derive effective business
strategies and discover better ways in carrying out business. In order to
perform data mining, regular databases must be converted into what so called
informational databases also known as data warehouse. This paper presents a
design model for building data warehouse for a typical university information
system. It is based on transforming an operational database into an
informational warehouse useful for decision makers to conduct data analysis,
predication, and forecasting. The proposed model is based on four stages of
data migration: Data extraction, data cleansing, data transforming, and data
indexing and loading. The complete system is implemented under MS Access 2010
and is meant to serve as a repository of data for data mining operations."
"We study the problem of concealing functionality of a proprietary or private
module when provenance information is shown over repeated executions of a
workflow which contains both `public' and `private' modules. Our approach is to
use `provenance views' to hide carefully chosen subsets of data over all
executions of the workflow to ensure G-privacy: for each private module and
each input x, the module's output f(x) is indistinguishable from G -1 other
possible values given the visible data in the workflow executions. We show that
G-privacy cannot be achieved simply by combining solutions for individual
private modules; data hiding must also be `propagated' through public modules.
We then examine how much additional data must be hidden and when it is safe to
stop propagating data hiding. The answer depends strongly on the workflow
topology as well as the behavior of public modules on the visible data. In
particular, for a class of workflows (which include the common tree and chain
workflows), taking private solutions for each private module, augmented with a
`public closure' that is `upstream-downstream safe', ensures G-privacy. We
define these notions formally and show that the restrictions are necessary. We
also study the related optimization problems of minimizing the amount of hidden
data."
"Collaborative working is increasingly popular, but it presents challenges due
to the need for high responsiveness and disconnected work support. To address
these challenges the data is optimistically replicated at the edges of the
network, i.e. personal computers or mobile devices. This replication requires a
merge mechanism that preserves the consistency and structure of the shared data
subject to concurrent modifications. In this paper, we propose a generic design
to ensure eventual consistency (every replica will eventually view the same
data) and to maintain the specific constraints of the replicated data. Our
layered design provides to the application engineer the complete control over
system scalability and behavior of the replicated data in face of concurrent
modifications. We show that our design allows replication of complex data types
with acceptable performances."
"Active context-free games are two-player games on strings over finite
alphabets with one player trying to rewrite the input string to match a target
specification. These games have been investigated in the context of exchanging
Active XML (AXML) data. While it was known that the rewriting problem is
undecidable in general, it is shown here that it is EXPSPACE-complete to decide
for a given context-free game, whether all safely rewritable strings can be
safely rewritten in a left-to-right manner, a problem that was previously
considered by Abiteboul et al. Furthermore, it is shown that the corresponding
problem for games with finite replacement languages is EXPTIME-complete."
"The increasing interest in Semantic Web technologies has led not only to a
rapid growth of semantic data on the Web but also to an increasing number of
backend applications with already more than a trillion triples in some cases.
Confronted with such huge amounts of data and the future growth, existing
state-of-the-art systems for storing RDF and processing SPARQL queries are no
longer sufficient. In this paper, we introduce Partout, a distributed engine
for efficient RDF processing in a cluster of machines. We propose an effective
approach for fragmenting RDF data sets based on a query log, allocating the
fragments to nodes in a cluster, and finding the optimal configuration. Partout
can efficiently handle updates and its query optimizer produces efficient query
execution plans for ad-hoc SPARQL queries. Our experiments show the superiority
of our approach to state-of-the-art approaches for partitioning and distributed
SPARQL query processing."
"This article addresses the generation of the ETL
operators(Extract-Transform-Load) for supplying a Data Warehouse from a
relational data source. As a first step, we add new rules to those proposed by
the authors of [1], these rules deal with the combination of ETL operators. In
a second step, we propose an automatic approach based on model transformations
to generate the ETL operations needed for loading a data warehouse. This
approach offers the possibility to set some designer requirements for loading."
"More and more business activities are performed using information systems.
These systems produce such huge amounts of event data that existing systems are
unable to store and process them. Moreover, few processes are in steady-state
and due to changing circumstances processes evolve and systems need to adapt
continuously. Since conventional process discovery algorithms have been defined
for batch processing, it is difficult to apply them in such evolving
environments. Existing algorithms cannot cope with streaming event data and
tend to generate unreliable and obsolete results.
  In this paper, we discuss the peculiarities of dealing with streaming event
data in the context of process mining. Subsequently, we present a general
framework for defining process mining algorithms in settings where it is
impossible to store all events over an extended period or where processes
evolve while being analyzed. We show how the Heuristics Miner, one of the most
effective process discovery algorithms for practical applications, can be
modified using this framework. Different stream-aware versions of the
Heuristics Miner are defined and implemented in ProM. Moreover, experimental
results on artificial and real logs are reported."
"We study the problem of consistent query answering under primary key
violations. In this setting, the relations in a database violate the key
constraints and we are interested in maximal subsets of the database that
satisfy the constraints, which we call repairs. For a boolean query Q, the
problem CERTAINTY(Q) asks whether every such repair satisfies the query or not;
the problem is known to be always in coNP for conjunctive queries. However,
there are queries for which it can be solved in polynomial time. It has been
conjectured that there exists a dichotomy on the complexity of CERTAINTY(Q) for
conjunctive queries: it is either in PTIME or coNP-complete. In this paper, we
prove that the conjecture is indeed true for the case of conjunctive queries
without self-joins, where each atom has as a key either a single attribute
(simple key) or all attributes of the atom."
"In this short vision paper, we introduce a machine learning optimizer for
data management and describe its architecture and main functionality."
"Data warehousing is an essential element of decision support systems. It aims
at enabling the user knowledge to make better and faster daily business
decisions. To improve this decision support system and to give more and more
relevant information to the user, the need to integrate user's profiles into
the data warehouse process becomes crucial. In this paper, we propose to
exploit users' preferences as a basis for adapting OLAP (On-Line Analytical
Processing) queries to the user. For this, we present a user profile-driven
data warehouse approach that allows dening user's profile composed by his/her
identifier and a set of his/her preferences. Our approach is based on a general
data warehouse architecture and an adaptive OLAP analysis system. Our main idea
consists in creating a data warehouse materialized view for each user with
respect to his/her profile. This task is performed off-line when the user
defines his/her profile for the first time. Then, when a user query is
submitted to the data warehouse, the system deals with his/her data warehouse
materialized view instead of the whole data warehouse. In other words, the data
warehouse view summaries the data warehouse content for the user by taking into
account his/her preferences. Moreover, we are implementing our data warehouse
personalization approach under the SQL Server 2005 DBMS (DataBase Management
System)."
"The probabilistic threshold query is one of the most common queries in
uncertain databases, where a result satisfying the query must be also with
probability meeting the threshold requirement. In this paper, we investigate
probabilistic threshold keyword queries (PrTKQ) over XML data, which is not
studied before. We first introduce the notion of quasi-SLCA and use it to
represent results for a PrTKQ with the consideration of possible world
semantics. Then we design a probabilistic inverted (PI) index that can be used
to quickly return the qualified answers and filter out the unqualified ones
based on our proposed lower/upper bounds. After that, we propose two efficient
and comparable algorithms: Baseline Algorithm and PI index-based Algorithm. To
accelerate the performance of algorithms, we also utilize probability density
function. An empirical study using real and synthetic data sets has verified
the effectiveness and the efficiency of our approaches."
"While keyword query empowers ordinary users to search vast amount of data,
the ambiguity of keyword query makes it difficult to effectively answer keyword
queries, especially for short and vague keyword queries. To address this
challenging problem, in this paper we propose an approach that automatically
diversifies XML keyword search based on its different contexts in the XML data.
Given a short and vague keyword query and XML data to be searched, we firstly
derive keyword search candidates of the query by a classifical feature
selection model. And then, we design an effective XML keyword search
diversification model to measure the quality of each candidate. After that,
three efficient algorithms are proposed to evaluate the possible generated
query candidates representing the diversified search intentions, from which we
can find and return top-$k$ qualified query candidates that are most relevant
to the given keyword query while they can cover maximal number of distinct
results.At last, a comprehensive evaluation on real and synthetic datasets
demonstrates the effectiveness of our proposed diversification model and the
efficiency of our algorithms."
"In this paper we study how to efficiently compute \textit{frequent
co-occurring terms} (FCT) in the results of a keyword query in parallel using
the popular MapReduce framework. Taking as input a keyword query q and an
integer k, an FCT query reports the k terms that are not in q, but appear most
frequently in the results of the keyword query q over multiple joined
relations. The returned terms of FCT search can be used to do query expansion
and query refinement for traditional keyword search. Different from the method
of FCT search in a single platform, our proposed approach can efficiently
answer a FCT query using the MapReduce Paradigm without pre-computing the
results of the original keyword query, which is run in parallel platform. In
this work, we can output the final FCT search results by two MapReduce jobs:
the first is to extract the statistical information of the data; and the second
is to calculate the total frequency of each term based on the output of the
first job. At the two MapReduce jobs, we would guarantee the load balance of
mappers and the computational balance of reducers as much as possible.
Analytical and experimental evaluations demonstrate the efficiency and
scalability of our proposed approach using TPC-H benchmark datasets with
different sizes."
"Discovering frequent graph patterns in a graph database offers valuable
information in a variety of applications. However, if the graph dataset
contains sensitive data of individuals such as mobile phone-call graphs and
web-click graphs, releasing discovered frequent patterns may present a threat
to the privacy of individuals. {\em Differential privacy} has recently emerged
as the {\em de facto} standard for private data analysis due to its provable
privacy guarantee. In this paper we propose the first differentially private
algorithm for mining frequent graph patterns.
  We first show that previous techniques on differentially private discovery of
frequent {\em itemsets} cannot apply in mining frequent graph patterns due to
the inherent complexity of handling structural information in graphs. We then
address this challenge by proposing a Markov Chain Monte Carlo (MCMC) sampling
based algorithm. Unlike previous work on frequent itemset mining, our
techniques do not rely on the output of a non-private mining algorithm.
Instead, we observe that both frequent graph pattern mining and the guarantee
of differential privacy can be unified into an MCMC sampling framework. In
addition, we establish the privacy and utility guarantee of our algorithm and
propose an efficient neighboring pattern counting technique as well.
Experimental results show that the proposed algorithm is able to output
frequent patterns with good precision."
"Since scientific investigation is one of the most important providers of
massive amounts of ordered data, there is a renewed interest in array data
processing in the context of Big Data. To the best of our knowledge, a unified
resource that summarizes and analyzes array processing research over its long
existence is currently missing. In this survey, we provide a guide for past,
present, and future research in array processing. The survey is organized along
three main topics. Array storage discusses all the aspects related to array
partitioning into chunks. The identification of a reduced set of array
operators to form the foundation for an array query language is analyzed across
multiple such proposals. Lastly, we survey real systems for array processing.
The result is a thorough survey on array data storage and processing that
should be consulted by anyone interested in this research topic, independent of
experience level. The survey is not complete though. We greatly appreciate
pointers towards any work we might have forgotten to mention."
"To minimize network latency and remain online during server failures and
network partitions, many modern distributed data storage systems eschew
transactional functionality, which provides strong semantic guarantees for
groups of multiple operations over multiple data items. In this work, we
consider the problem of providing Highly Available Transactions (HATs):
transactional guarantees that do not suffer unavailability during system
partitions or incur high network latency. We introduce a taxonomy of highly
available systems and analyze existing ACID isolation and distributed data
consistency guarantees to identify which can and cannot be achieved in HAT
systems. This unifies the literature on weak transactional isolation, replica
consistency, and highly available systems. We analytically and experimentally
quantify the availability and performance benefits of HATs--often two to three
orders of magnitude over wide-area networks--and discuss their necessary
semantic compromises."
"The purpose of this research is to design database scheme of information
system at XYZ University. By using database design methods (conceptual scheme,
logical scheme, & physical scheme) the writer designs payroll information
system. The physical scheme is compatible with Borland Delphi Database Engine
Scheme to support the implementation of the I.S. After 3 (three) steps we get 7
(seven) tables, dan 6 (six) forms. By using this shemce, the system can produce
several reports quickly, accurately, efficiently, and effectively."
"OLAP systems operate on historical data and provide answers to analysts
queries. Recent in-memory implementations provide significant performance
improvement for real time ad-hoc analysis. Philosophy and techniques of what-if
analysis on data warehouse and in-memory data store based OLAP systems have
been covered in great detail before but exploration of new dimension value
(attribute) introduction has been limited in the context of what-if analysis.
We extend the approach of Andrey Balmin et al of using select modify operator
on data graph to introduce new values for dimensions and measures in a
read-only in-memory data store as scenarios. Our system constructs scenarios
without materializing the rows and stores the row information as queries. The
rows associated with the scenarios are constructed as and when required by an
ad-hoc query."
"We describe a new algorithm, Minesweeper, that is able to satisfy stronger
runtime guarantees than previous join algorithms (colloquially, `beyond
worst-case guarantees') for data in indexed search trees. Our first
contribution is developing a framework to measure this stronger notion of
complexity, which we call {\it certificate complexity}, that extends notions of
Barbay et al. and Demaine et al.; a certificate is a set of propositional
formulae that certifies that the output is correct. This notion captures a
natural class of join algorithms. In addition, the certificate allows us to
define a strictly stronger notion of runtime complexity than traditional
worst-case guarantees. Our second contribution is to develop a dichotomy
theorem for the certificate-based notion of complexity. Roughly, we show that
Minesweeper evaluates $\beta$-acyclic queries in time linear in the certificate
plus the output size, while for any $\beta$-cyclic query there is some instance
that takes superlinear time in the certificate (and for which the output is no
larger than the certificate size). We also extend our certificate-complexity
analysis to queries with bounded treewidth and the triangle query."
"Developing an application with some tables must concern the validation of
input (specially in Table Child). In order to maximize the accuracy and data
input validation. Its called lookup (took data from other dataset). There are
two ways to look up data from Table Parent: 1) Using Objects (DBLookupComboBox
and DBookupListBox), or 2) Arranging the properties of data types fields (shown
by using DBGrid). In this article is using Borland Delphi software (Inprise
product). The method is offered using 5 (five) practise steps: 1) Relational
Database Scheme, 2) Form Design, 3) Object DatabasesRelationships Scheme, 4)
Properties and Field Type Arrangement, and 5) Procedures. The result of this
paper are: 1) The relationship that using lookup objects are valid, and 2)
Delphi Lookup Objects can be used for 1-1, 1-N, and M-N relationship."
"Data mining is the practice to search large amount of data to discover data
patterns. Data mining uses mathematical algorithms to group the data and
evaluate the future events. Association rule is a research area in the field of
knowledge discovery. Many data mining researchers had improved upon the quality
of association rule for business development by incorporating influential
factors like utility, number of items sold and for the mining of association
data patterns. In this paper, we propose an efficient algorithm to find maximal
frequent itemset first. Most of the association rule algorithms used to find
minimal frequent item first, then with the help of minimal frequent itemsets
derive the maximal frequent itemsets, these methods consume more time to find
maximal frequent itemsets. To overcome this problem, we propose a new approach
to find maximal frequent itemset directly using the concepts of subsets. The
proposed method is found to be efficient in finding maximal frequent itemsets."
"View update is the problem of translating an update to a view to some updates
to the source data of the view. In this paper, we show the factors determining
XML view update translation, propose a translation procedure, and propose
translated updates to the source document for different types of views. We
further show that the translated updates are precise. The proposed solution
makes it possible for users who do not have access privileges to the source
data to update the source data via a view."
"In the last two decades, the continuous increase of computational power has
produced an overwhelming flow of data which has called for a paradigm shift in
the computing architecture and large scale data processing mechanisms.
MapReduce is a simple and powerful programming model that enables easy
development of scalable parallel applications to process vast amounts of data
on large clusters of commodity machines. It isolates the application from the
details of running a distributed program such as issues on data distribution,
scheduling and fault tolerance. However, the original implementation of the
MapReduce framework had some limitations that have been tackled by many
research efforts in several followup works after its introduction. This article
provides a comprehensive survey for a family of approaches and mechanisms of
large scale data processing mechanisms that have been implemented based on the
original idea of the MapReduce framework and are currently gaining a lot of
momentum in both research and industrial communities. We also cover a set of
introduced systems that have been implemented to provide declarative
programming interfaces on top of the MapReduce framework. In addition, we
review several large scale data processing systems that resemble some of the
ideas of the MapReduce framework for different purposes and application
scenarios. Finally, we discuss some of the future research directions for
implementing the next generation of MapReduce-like solutions."
"We present SPARSI, a theoretical framework for partitioning sensitive data
across multiple non-colluding adversaries. Most work in privacy-aware data
sharing has considered disclosing summaries where the aggregate information
about the data is preserved, but sensitive user information is protected.
Nonetheless, there are applications, including online advertising, cloud
computing and crowdsourcing markets, where detailed and fine-grained user-data
must be disclosed. We consider a new data sharing paradigm and introduce the
problem of privacy-aware data partitioning, where a sensitive dataset must be
partitioned among k untrusted parties (adversaries). The goal is to maximize
the utility derived by partitioning and distributing the dataset, while
minimizing the amount of sensitive information disclosed. The data should be
distributed so that an adversary, without colluding with other adversaries,
cannot draw additional inferences about the private information, by linking
together multiple pieces of information released to her. The assumption of no
collusion is both reasonable and necessary in the above application domains
that require release of private user information. SPARSI enables us to formally
define privacy-aware data partitioning using the notion of sensitive properties
for modeling private information and a hypergraph representation for describing
the interdependencies between data entries and private information. We show
that solving privacy-aware partitioning is, in general, NP-hard, but for
specific information disclosure functions, good approximate solutions can be
found using relaxation techniques. Finally, we present a local search algorithm
applicable to generic information disclosure functions. We apply SPARSI
together with the proposed algorithms on data from a real advertising scenario
and show that we can partition data with no disclosure to any single
advertiser."
"Satellite Tracking of People (STOP) tracks thousands of GPS-enabled devices
24 hours a day and 365 days a year. With locations captured for each device
every minute, STOP servers receive tens of millions of points each day. In
addition to cataloging these points in real-time, STOP must also respond to
questions from customers such as, ""What devices of mine were at this location
two months ago?"" They often then broaden their question to one such as, ""Which
of my devices have ever been at this location?"" The processing requirements
necessary to answer these questions while continuing to process inbound data in
real-time is non-trivial.
  To meet this demand, STOP developed Adaptive Partitioning to provide a
cost-effective and highly available hardware platform for the geographical and
time-spatial indexing capabilities necessary for responding to customer data
requests while continuing to catalog inbound data in real-time."
"Different spatial objects that vary in their characteristics, such as
molecular biology and geography, are presented in spatial areas. Methods to
organize, manage, and maintain those objects in a structured manner are
required. Data mining raised different techniques to overcome these
requirements. There are many major tasks of data mining, but the mostly used
task is clustering. Data set within the same cluster share common features that
give each cluster its characteristics. In this paper, an implementation of
Approximate kNN-based spatial clustering algorithm using the K-d tree is
proposed. The major contribution achieved by this research is the use of the
k-d tree data structure for spatial clustering, and comparing its performance
to the brute-force approach. The results of the work performed in this paper
revealed better performance using the k-d tree, compared to the traditional
brute-force approach."
"Finding a location for a new facility such that the facility attracts the
maximal number of customers is a challenging problem. Existing studies either
model customers as static sites and thus do not consider customer movement, or
they focus on theoretical aspects and do not provide solutions that are shown
empirically to be scalable. Given a road network, a set of existing facilities,
and a collection of customer route traversals, an optimal segment query returns
the optimal road network segment(s) for a new facility. We propose a practical
framework for computing this query, where each route traversal is assigned a
score that is distributed among the road segments covered by the route
according to a score distribution model. The query returns the road segment(s)
with the highest score. To achieve low latency, it is essential to prune the
very large search space. We propose two algorithms that adopt different
approaches to computing the query. Algorithm AUG uses graph augmentation, and
ITE uses iterative road-network partitioning. Empirical studies with real data
sets demonstrate that the algorithms are capable of offering high performance
in realistic settings."
"This article introduces a novel approach to spatial database design. Instead
of extending the canonical Solid-Face-Edge-Vertex schema by, say, ""hypersolids""
these classes are generalised to a common type SpatialEntity, and the
individual BoundedBy relations between two consecutive classes are generalised
to one BoundedBy relation on SpatialEntity instances. Then the pair
(SpatialEntity, BoundedBy) is a so-called incidence graph.
  The novelty about this approach uses the observation that an incidence graph
represents a topological space of SpatialEntity instances because the
BoundedBy-relation defines a so-called Alexandrov topology for them turning
them into a topological space. So spatial data becomes part of mathematical
topology and topology can be immediately applied to spatial data. For example,
continuous functions between two instances of spatial data allow the consistent
modelling of generalisation. Further, it is also possible to establish a formal
topological definition of spatial data dimension, and every topological data
model of arbitrary dimension gets a simple uniform data model. This model
covers space-time, and the version history of a spatial model can be
represented by an Alexandrov topology, too. By integrating space, time,
version, and scale into one single schema, topological queries across those
aspects are enabled through topological constructions. In fact, the topological
constructions cover a relationally complete query language for spaces and can
be redefined to operate accordingly on their graph representations.
  With these observations a relational database schema for a spatial data model
of dimension 6 and more is developed. The schema seamlessly integrates 4D
space-time, levels of detail and version history, and it can be easily expanded
to also contain non-spatial information or be linked to other data sources."
"We address the issue of incorporating a particular yet expressive form of
integrity constraints (namely, denial constraints) into probabilistic
databases. To this aim, we move away from the common way of giving semantics to
probabilistic databases, which relies on considering a unique interpretation of
the data, and address two fundamental problems: consistency checking and query
evaluation. The former consists in verifying whether there is an interpretation
which conforms to both the marginal probabilities of the tuples and the
integrity constraints. The latter is the problem of answering queries under a
""cautious"" paradigm, taking into account all interpretations of the data in
accordance with the constraints. In this setting, we investigate the complexity
of the above-mentioned problems, and identify several tractable cases of
practical relevance."
"We consider unordered XML, where the relative order among siblings is
ignored, and propose two simple yet practical schema formalisms: disjunctive
multiplicity schemas (DMS), and its restriction, disjunction-free multiplicity
schemas (MS). We investigate their computational properties and characterize
the complexity of the following static analysis problems: schema
satisfiability, membership of a tree to the language of a schema, schema
containment, twig query satisfiability, implication, and containment in the
presence of schema. Our research indicates that the proposed formalisms retain
much of the expressiveness of DTDs without an increase in computational
complexity."
"Applications such as Google Docs, Office 365, and Dropbox show a growing
trend towards incorporating multi-user live collaboration functionality into
web applications. These collaborative applications share a need to efficiently
express shared state, and a common strategy for doing so is a shared log
abstraction. Extensive research efforts on log abstractions by the database,
programming languages, and distributed systems communities have identified a
variety of optimization techniques based on the algebraic properties of updates
(i.e., pairwise commutativity, subsumption, and idempotence). Although these
techniques have been applied to specific applications and use-cases, to the
best of our knowledge, no attempt has been made to create a general framework
for such optimizations in the context of a non-trivial update language. In this
paper, we introduce mutation languages, a low-level framework for reasoning
about the algebraic properties of state updates, or mutations. We define BarQL,
a general purpose state-update language, and show how mutation languages allow
us to reason about the algebraic properties of updates expressed in BarQ L ."
"With the continuous increase of online services as well as energy costs,
energy consumption becomes a significant cost factor for the evaluation of data
center operations. A significant contributor to that is the performance of
database servers which are found to constitute the backbone of online services.
From a software approach, while a set of novel data management technologies
appear in the market e.g. key-value based or in-memory databases, classic
relational database management systems (RDBMS) are still widely used. In
addition from a hardware perspective, the majority of database servers is still
using standard magnetic hard drives (HDDs) instead of solid state drives (SSDs)
due to lower cost of storage per gigabyte, disregarding the performance boost
that might be given due to high cost.
  In this study we focus on a software based assessment of the energy
consumption of a database server by running three different and complete
database workloads namely TCP-H, Star Schema Benchmark -SSB as well a modified
benchmark we have derived for this study called W22. We profile the energy
distribution among the ost important server components and by using different
resource allocation we assess the energy consumption of a typical open source
RDBMS (PostgreSQL) on a standard server in relation with its performance
(measured by query time).
  Results confirm the well-known fact that even for complete workloads,
optimization of the RDBMS results to lower energy consumption."
"A lot of research activity has recently taken place around the chase
procedure, due to its usefulness in data integration, data exchange, query
optimization, peer data exchange and data correspondence, to mention a few. As
the chase has been investigated and further developed by a number of research
groups and authors, many variants of the chase have emerged and associated
results obtained. Due to the heterogeneous nature of the area it is frequently
difficult to verify the scope of each result. In this paper we take closer look
at recent developments, and provide additional results. Our analysis allows us
create a taxonomy of the chase variations and the properties they satisfy.
  Two of the most central problems regarding the chase is termination, and
discovery of restricted classes of sets of dependencies that guarantee
termination of the chase. The search for the restricted classes has been
motivated by a fairly recent result that shows that it is undecidable to
determine whether the chase with a given dependency set will terminate on a
given instance. There is a small dissonance here, since the quest has been for
classes of sets of dependencies guaranteeing termination of the chase on all
instances, even though the latter problem was not known to be undecidable. We
resolve the dissonance in this paper by showing that determining whether the
chase with a given set of dependencies terminates on all instances is
coRE-complete. Our reduction also gives us the aforementioned
instance-dependent RE-completeness result as a byproduct. For one of the
restricted classes, the stratified sets dependencies, we provide new complexity
results for the problem of testing whether a given set of dependencies belongs
to it. These results rectify some previous claims that have occurred in the
literature."
"The combination of the flexibility of RDF and the expressiveness of SPARQL
provides a powerful mechanism to model, integrate and query data. However,
these properties also mean that it is nontrivial to write performant SPARQL
queries. Indeed, it is quite easy to create queries that tax even the most
optimised triple stores. Currently, application developers have little concrete
guidance on how to write ""good"" queries. The goal of this paper is to begin to
bridge this gap. It describes 5 heuristics that can be applied to create
optimised queries. The heuristics are informed by formal results in the
literature on the semantics and complexity of evaluating SPARQL queries, which
ensures that queries following these rules can be optimised effectively by an
underlying RDF store. Moreover, we empirically verify the efficacy of the
heuristics using a set of openly available datasets and corresponding SPARQL
queries developed by a large pharmacology data integration project. The
experimental results show improvements in performance across 6 state-of-the-art
RDF stores."
"Due to the ever increasing importance of the internet, interoperability of
heterogeneous data sources is as well of ever increasing importance.
Interoperability can be achieved e.g. through data integration and data
exchange. Common to both approaches is the need for the DBMS to be able to
store and query incomplete databases. In this report we present PossDB, a DBMS
capable of storing and querying incomplete databases. The system is wrapper
over PostgreSQL, and the query language is an extension of a subset of standard
SQL. Our experimental results show that our system scales well, actually better
than comparable systems."
"Given a replicated database, a divergent design tunes the indexes in each
replica differently in order to specialize it for a specific subset of the
workload. This specialization brings significant performance gains compared to
the common practice of having the same indexes in all replicas, but requires
the development of new tuning tools for database administrators. In this paper
we introduce RITA (Replication-aware Index Tuning Advisor), a novel
divergent-tuning advisor that offers several essential features not found in
existing tools: it generates robust divergent designs that allow the system to
adapt gracefully to replica failures; it computes designs that spread the load
evenly among specialized replicas, both during normal operation and when
replicas fail; it monitors the workload online in order to detect changes that
require a recomputation of the divergent design; and, it offers suggestions to
elastically reconfigure the system (by adding/removing replicas or
adding/dropping indexes) to respond to workload changes. The key technical
innovation behind RITA is showing that the problem of selecting an optimal
design can be formulated as a Binary Integer Program (BIP). The BIP has a
relatively small number of variables, which makes it feasible to solve it
efficiently using any off-the-shelf linear-optimization software. Experimental
results demonstrate that RITA computes better divergent designs compared to
existing tools, offers more features, and has fast execution times."
"The paper introduces the principles of object-oriented translation for target
machine which provides executing the sequences of elementary operations on
persistent data presented as a set of relations (programmable relational
system). The language of this target machine bases on formal operations of
relational data model. An approach is given to convert both the description of
complex object-oriented data structures and operations on these data, into a
description of relational structures and operations on them. The proposed
approach makes possible to extend the target relational language with commands
allowing data be described as a set of complex persistent objects of different
classes. Object views are introduced which allow relational operations be
applied to the data of complex objects. It is shown that any operation and
method can be executed on any group of the objects without explicit and
implicit iterators. Binding of both attributes and methods with their
polymorphic implementations are discussed. Classes can be co-used with
relations as scalar domains, in referential integrity constraints and in data
query operations."
"Nested regular expressions (NREs) have been proposed as a powerful formalism
for querying RDFS graphs, but research in a more general graph database context
has been scarce, and static analysis results are currently lacking. In this
paper we investigate the problem of containment of NREs, and show that it can
be solved in PSPACE, i.e., the same complexity as the problem of containment of
regular expressions or regular path queries (RPQs)."
"The geospatial analyst is required to apply art, science, and technology to
measure relative positions of natural and man-made features above or beneath
the earths surface, and to present this information either graphically or
numerically. The reference positions for these measurements need to be well
archived and managed to effectively sustain the activities in the spatial
industry. The research herein described highlights the need for an information
system for the Land Surveyors Equipment Store. Such a system is a database
management system with a user friendly graphical interface. This paper
describes one such system that has been developed for the Equipment Store of
the Department of Geomatic Engineering, Kwame Nkrumah University of Science and
Technology, Ghana. The system facilitates efficient management and location of
instruments, as well as easy location of beacons together with their attribute
information, it provides multimedia information about instruments in an
Equipment Store. Digital camera was used capture the pictorial descriptions of
the beacons. Geographic Information System software was employed to visualize
the spatial location of beacons and to publish the various layers for the
Graphical User Interface. The aesthetics of the interface was developed with
user interface design tools and coded by programming. The developed Suite,
powered by a reliable and fully scalable database, provides an efficient way of
booking and analyzing transactions in an Equipment Store."
"Subspace clustering discovers the clusters embedded in multiple, overlapping
subspaces of high dimensional data. Many significant subspace clustering
algorithms exist, each having different characteristics caused by the use of
different techniques, assumptions, heuristics used etc. A comprehensive
classification scheme is essential which will consider all such characteristics
to divide subspace clustering approaches in various families. The algorithms
belonging to same family will satisfy common characteristics. Such a
categorization will help future developers to better understand the quality
criteria to be used and similar algorithms to be used to compare results with
their proposed clustering algorithms. In this paper, we first proposed the
concept of SCAF (Subspace Clustering Algorithms Family). Characteristics of
SCAF will be based on the classes such as cluster orientation, overlap of
dimensions etc. As an illustration, we further provided a comprehensive,
systematic description and comparison of few significant algorithms belonging
to 'Axis parallel, overlapping, density based' SCAF."
"Web sequential patterns are important for analyzing and understanding users
behaviour to improve the quality of service offered by the World Wide Web. Web
Prefetching is one such technique that utilizes prefetching rules derived
through Cyclic Model Analysis of the mined Web sequential patterns. The more
accurate the prediction and more satisfying the results of prefetching if we
use a highly efficient and scalable mining technique such as the Bidirectional
Growth based Directed Acyclic Graph. In this paper, we propose a novel
algorithm called Bidirectional Growth based mining Cyclic behavior Analysis of
web sequential Patterns (BGCAP) that effectively combines these strategies to
generate prefetching rules in the form of 2-sequence patterns with Periodicity
and threshold of Cyclic Behaviour that can be utilized to effectively prefetch
Web pages, thus reducing the users perceived latency. As BGCAP is based on
Bidirectional pattern growth, it performs only (log n+1) levels of recursion
for mining n Web sequential patterns. Our experimental results show that
prefetching rules generated using BGCAP is 5-10 percent faster for different
data sizes and 10-15% faster for a fixed data size than TD-Mine. In addition,
BGCAP generates about 5-15 percent more prefetching rules than TD-Mine."
"We study the use of WebdamLog, a declarative high-level lan- guage in the
style of datalog, to support the distribution of both data and knowledge (i.e.,
programs) over a network of au- tonomous peers. The main novelty of WebdamLog
compared to datalog is its use of delegation, that is, the ability for a peer
to communicate a program to another peer. We present results of a user study,
showing that users can write WebdamLog programs quickly and correctly, and with
a minimal amount of training. We present an implementation of the WebdamLog
inference engine relying on the Bud dat- alog engine. We describe an
experimental evaluation of the WebdamLog engine, demonstrating that WebdamLog
can be im- plemented efficiently. We conclude with a discussion of ongoing and
future work."
"To help a user specify and verify quantified queries --- a class of database
queries known to be very challenging for all but the most expert users --- one
can question the user on whether certain data objects are answers or
non-answers to her intended query. In this paper, we analyze the number of
questions needed to learn or verify qhorn queries, a special class of Boolean
quantified queries whose underlying form is conjunctions of quantified Horn
expressions. We provide optimal polynomial-question and polynomial-time
learning and verification algorithms for two subclasses of the class qhorn with
upper constant limits on a query's causal density."
"Existing studies on differential privacy mainly consider aggregation on data
sets where each entry corresponds to a particular participant to be protected.
In many situations, a user may pose a relational algebra query on a sensitive
database, and desires differentially private aggregation on the result of the
query. However, no known work is capable to release this kind of aggregation
when the query contains unrestricted join operations. This severely limits the
applications of existing differential privacy techniques because many data
analysis tasks require unrestricted joins. One example is subgraph counting on
a graph. Existing methods for differentially private subgraph counting address
only edge differential privacy and are subject to very simple subgraphs. Before
this work, whether any nontrivial graph statistics can be released with
reasonable accuracy under node differential privacy is still an open problem.
  In this paper, we propose a novel differentially private mechanism to release
an approximation to a linear statistic of the result of some positive
relational algebra calculation over a sensitive database. Unrestricted joins
are supported in our mechanism. The error bound of the approximate answer is
roughly proportional to the \emph{empirical sensitivity} of the query --- a new
notion that measures the maximum possible change to the query answer when a
participant withdraws its data from the sensitive database. For subgraph
counting, our mechanism provides the first solution to achieve node
differential privacy, for any kind of subgraphs."
"This paper addresses the problem of approximate processing for flexible
queries in the form SELECT-FROM-WHERE-GROUP BY with join condition. It offers a
flexible framework for online aggregation while promoting response time at the
expense of result accuracy."
"SPARQL basic graph pattern (BGP) (a.k.a. SQL inner-join) query optimization
is a well researched area. However, optimization of OPTIONAL pattern queries
(a.k.a. SQL left-outer-joins) poses additional challenges, due to the
restrictions on the \textit{reordering} of left-outer-joins. The occurrence of
such queries tends to be as high as 50% of the total queries (e.g., DBPedia
query logs).
  In this paper, we present \textit{Left Bit Right} (LBR), a technique for
\textit{well-designed} nested BGP and OPTIONAL pattern queries. Through LBR, we
propose a novel method to represent such queries using a graph of
\textit{supernodes}, which is used to aggressively prune the RDF triples, with
the help of compressed indexes. We also propose novel optimization strategies
-- first of a kind, to the best of our knowledge -- that combine together the
characteristics of \textit{acyclicity} of queries, \textit{minimality}, and
\textit{nullification}, \textit{best-match} operators. In this paper, we focus
on OPTIONAL patterns without UNIONs or FILTERs, but we also show how UNIONs and
FILTERs can be handled with our technique using a \textit{query rewrite}. Our
evaluation on RDF graphs of up to and over one billion triples, on a commodity
laptop with 8 GB memory, shows that LBR can process \textit{well-designed}
low-selectivity complex queries up to 11 times faster compared to the
state-of-the-art RDF column-stores as Virtuoso and MonetDB, and for highly
selective queries, LBR is at par with them."
"Matching Dependencies (MDs) are a relatively recent proposal for declarative
entity resolution. They are rules that specify, given the similarities
satisfied by values in a database, what values should be considered duplicates,
and have to be matched. On the basis of a chase-like procedure for MD
enforcement, we can obtain clean (duplicate-free) instances; actually possibly
several of them. The resolved answers to queries are those that are invariant
under the resulting class of resolved instances. In previous work we identified
some tractable cases (i.e. for certain classes of queries and MDs) of resolved
query answering. In this paper we further investigate the complexity of this
problem, identifying some intractable cases. For a special case we obtain a
dichotomy complexity result."
"A reachability oracle (or hop labeling) assigns each vertex v two sets of
vertices: Lout(v) and Lin(v), such that u reaches v iff Lout(u) \cap Lin(v)
\neq \emptyset. Despite their simplicity and elegance, reachability oracles
have failed to achieve efficiency in more than ten years since their
introduction: the main problem is high construction cost, which stems from a
set-cover framework and the need to materialize transitive closure. In this
paper, we present two simple and efficient labeling algorithms,
Hierarchical-Labeling and Distribution-Labeling, which can work onmassive
real-world graphs: their construction time is an order of magnitude faster than
the setcover based labeling approach, and transitive closure materialization is
not needed. On large graphs, their index sizes and their query performance can
now beat the state-of-the-art transitive closure compression and online search
approaches."
"Evaluating the performance of scientific data processing systems is a
difficult task considering the plethora of application-specific solutions
available in this landscape and the lack of a generally-accepted benchmark. The
dual structure of scientific data coupled with the complex nature of processing
complicate the evaluation procedure further. SS-DB is the first attempt to
define a general benchmark for complex scientific processing over raw and
derived data. It fails to draw sufficient attention though because of the
ambiguous plain language specification and the extraordinary SciDB results. In
this paper, we remedy the shortcomings of the original SS-DB specification by
providing a formal representation in terms of ArrayQL algebra operators and
ArrayQL/SciQL constructs. These are the first formal representations of the
SS-DB benchmark. Starting from the formal representation, we give a reference
implementation and present benchmark results in EXTASCID, a novel system for
scientific data processing. EXTASCID is complete in providing native support
both for array and relational data and extensible in executing any user code
inside the system by the means of a configurable metaoperator. These features
result in an order of magnitude improvement over SciDB at data loading,
extracting derived data, and operations over derived data."
"Variant Stochastic cracking is a significantly more resilient approach to
adaptive indexing. It showed [1]that Stochastic cracking uses each query as a
hint on how to reorganize data, but not blindly so; it gains resilience and
avoids performance bottlenecks by deliberately applying certain arbitrary
choices in its decision making. Therefore bring, adaptive indexing forward to a
mature formulation that confers the workload-robustness that previous
approaches lacked. Original cracking relies on the randomness of the workloads
to converge well. [2][3] However, where the workload is non-random, cracking
needs to introduce randomness on its own. Stochastic Cracking clearly improves
over original cracking by being robust in workload changes while maintaining
all original cracking features when it comes to adaptation. But looking at both
types of cracking, it conveyed an incomplete picture as at some point of time
it is must to know whether the workload is random or sequential. In this paper
our focus is on optimization of variant stochastic cracking, that could be
achieved in two ways either by reducing the initialization cost to make
stochastic cracking even more transparent to the user, especially for queries
that initiate a workload change and hence incur a higher cost or by combining
the strengths of the various stochastic cracking algorithms via a dynamic
component that decides which algorithm to choose for a query on the fly. The
efforts have been put in to make an algorithm that reduces the initialization
cost by using the main notion of both cracking, while considering the
requirements of adaptive indexing [2]."
"Spreadsheets are among the most commonly used applications for data
management and analysis. Perhaps they are even among the most widely used
computer applications of all kinds. They combine in a natural and intuitive way
data processing with very diverse supplementary features: statistical
functions, visualization tools, pivot tables, pivot charts, linear programming
solvers, Web queries periodically downloading data from external sources, etc.
However, the spreadsheet paradigm of computation still lacks sufficient
analysis.
  In this article we demonstrate that a spreadsheet can implement all data
transformations definable in SQL, without any use of macros or built-in
programming languages, merely by utilizing spreadsheet formulas. We provide a
query compiler, which translates any given SQL query into a worksheet of the
same semantics, including NULL values.
  Thereby database operations become available to the users who do not want to
migrate to a database. They can define their queries using a high-level
language and then get their execution plans in a plain vanilla spreadsheet. No
sophisticated database system, no spreadsheet plugins or macros are needed.
  The functions available in spreadsheets impose severe limitations on the
algorithms one can implement. In this paper we offer $O(n\log^2n)$ sorting
spreadsheet, but using a non-constant number of rows, improving on the
previously known $O(n^2)$ ones.
  It is therefore surprising, that a spreadsheet can implement, as we
demonstrate, Depth-First-Search and Breadth-First-Search on graphs, thereby
reaching beyond queries definable in SQL-92."
"Semantic web is a web of future. The Resource Description Framework (RDF) is
a language to represent resources in the World Wide Web. When these resources
are queried the problem of duplicate query results occurs. The present
techniques used hash index comparison to remove duplicate query results. The
major drawback of using the hash index to remove duplicate query results is
that, if there is a slight change in formatting or word order, then hash index
is changed and query results are no more considered as duplicate even though
they have same contents. We presented an algorithm for detection and
elimination of duplicate query results from semantic web using hash index and
page size comparisons. Experimental results showed that the proposed technique
removed duplicate query results from semantic web efficiently, solved the
problems of using hash index for duplicate handling and could be embedded in
existing SQL-Based query system for semantic web. Research could be carried out
for certain flexibilities in existing SQL-Based query system of semantic web to
accommodate other duplicate detection techniques as well."
"We present the WebdamLog system for managing distributed data on the Web in a
peer-to-peer manner. We demonstrate the main features of the system through an
application called Wepic for sharing pictures between attendees of the sigmod
conference. Using Wepic, the attendees will be able to share, download, rate
and annotate pictures in a highly decentralized manner. We show how WebdamLog
handles heterogeneity of the devices and services used to share data in such a
Web setting. We exhibit the simple rules that define the Wepic application and
show how to easily modify the Wepic application."
"Over the years, frequent subgraphs have been an important sort of targeted
patterns in the pattern mining literatures, where most works deal with
databases holding a number of graph transactions, e.g., chemical structures of
compounds. These methods rely heavily on the downward-closure property (DCP) of
the support measure to ensure an efficient pruning of the candidate patterns.
When switching to the emerging scenario of single-graph databases such as
Google Knowledge Graph and Facebook social graph, the traditional support
measure turns out to be trivial (either 0 or 1). However, to the best of our
knowledge, all attempts to redefine a single-graph support resulted in measures
that either lose DCP, or are no longer semantically intuitive.
  This paper targets mining patterns in the single-graph setting. We resolve
the ""DCP-intuitiveness"" dilemma by shifting the mining target from frequent
subgraphs to frequent neighborhoods. A neighborhood is a specific topological
pattern where a vertex is embedded, and the pattern is frequent if it is shared
by a large portion (above a given threshold) of vertices. We show that the new
patterns not only maintain DCP, but also have equally significant semantics as
subgraph patterns. Experiments on real-life datasets display the feasibility of
our algorithms on relatively large graphs, as well as the capability of mining
interesting knowledge that is not discovered in prior works."
"Nearest neighbor (NN) queries in trajectory databases have received
significant attention in the past, due to their application in spatio-temporal
data analysis. Recent work has considered the realistic case where the
trajectories are uncertain; however, only simple uncertainty models have been
proposed, which do not allow for accurate probabilistic search. In this paper,
we fill this gap by addressing probabilistic nearest neighbor queries in
databases with uncertain trajectories modeled by stochastic processes,
specifically the Markov chain model. We study three nearest neighbor query
semantics that take as input a query state or trajectory $q$ and a time
interval. For some queries, we show that no polynomial time solution can be
found. For problems that can be solved in PTIME, we present exact query
evaluation algorithms, while for the general case, we propose a sophisticated
sampling approach, which uses Bayesian inference to guarantee that sampled
trajectories conform to the observation data stored in the database. This
sampling approach can be used in Monte-Carlo based approximation solutions. We
include an extensive experimental study to support our theoretical results."
"We study the problem of searching a repository of complex hierarchical
workflows whose component modules, both composite and atomic, have been
annotated with keywords. Since keyword search does not use the graph structure
of a workflow, we develop a model of workflows using context-free bag grammars.
We then give efficient polynomial-time algorithms that, given a workflow and a
keyword query, determine whether some execution of the workflow matches the
query. Based on these algorithms we develop a search and ranking solution that
efficiently retrieves the top-k grammars from a repository. Finally, we propose
a novel result presentation method for grammars matching a keyword query, based
on representative parse-trees. The effectiveness of our approach is validated
through an extensive experimental evaluation."
"Geospatial extensions of SPARQL like GeoSPARQL and stSPARQL have recently
been defined and corresponding geospatial RDF stores have been implemented.
However, there is no widely used benchmark for evaluating geospatial RDF stores
which takes into account recent advances to the state of the art in this area.
In this paper, we develop a benchmark, called Geographica, which uses both
real-world and synthetic data to test the offered functionality and the
performance of some prominent geospatial RDF stores."
"The increasing growth of databases raises an urgent need for more accurate
methods to better understand the stored data. In this scope, association rules
were extensively used for the analysis and the comprehension of huge amounts of
data. However, the number of generated rules is too large to be efficiently
analyzed and explored in any further process. Association rules selection is a
classical topic to address this issue, yet, new innovated approaches are
required in order to provide help to decision makers. Hence, many interesting-
ness measures have been defined to statistically evaluate and filter the
association rules. However, these measures present two major problems. On the
one hand, they do not allow eliminating irrelevant rules, on the other hand,
their abun- dance leads to the heterogeneity of the evaluation results which
leads to confusion in decision making. In this paper, we propose a two-winged
approach to select statistically in- teresting and semantically incomparable
rules. Our statis- tical selection helps discovering interesting association
rules without favoring or excluding any measure. The semantic comparability
helps to decide if the considered association rules are semantically related
i.e comparable. The outcomes of our experiments on real datasets show promising
results in terms of reduction in the number of rules."
"In this short note I review and discuss fundamental options for physical and
logical data layouts as well as the impact of the choices on data processing. I
should say in advance that these notes offer no new insights, that is,
everything stated here has already been published elsewhere. In fact, it has
been published in so many different places, such as blog posts, in the
literature, etc. that the main contribution is to bring it all together in one
place."
"There is a growing need for methods which can capture uncertainties and
answer queries over graph-structured data. Two common types of uncertainty are
uncertainty over the attribute values of nodes and uncertainty over the
existence of edges. In this paper, we combine those with identity uncertainty.
Identity uncertainty represents uncertainty over the mapping from objects
mentioned in the data, or references, to the underlying real-world entities. We
propose the notion of a probabilistic entity graph (PEG), a probabilistic graph
model that defines a distribution over possible graphs at the entity level. The
model takes into account node attribute uncertainty, edge existence
uncertainty, and identity uncertainty, and thus enables us to systematically
reason about all three types of uncertainties in a uniform manner. We introduce
a general framework for constructing a PEG given uncertain data at the
reference level and develop highly efficient algorithms to answer subgraph
pattern matching queries in this setting. Our algorithms are based on two novel
ideas: context-aware path indexing and reduction by join-candidates, which
drastically reduce the query search space. A comprehensive experimental
evaluation shows that our approach outperforms baseline implementations by
orders of magnitude."
"Huge volume of data from domain specific applications such as medical,
financial, library, telephone, shopping records and individual are regularly
generated. Sharing of these data is proved to be beneficial for data mining
application. On one hand such data is an important asset to business decision
making by analyzing it. On the other hand data privacy concerns may prevent
data owners from sharing information for data analysis. In order to share data
while preserving privacy, data owner must come up with a solution which
achieves the dual goal of privacy preservation as well as an accuracy of data
mining task - clustering and classification. An efficient and effective
approach has been proposed that aims to protect privacy of sensitive
information and obtaining data clustering with minimum information loss."
"Data completeness is an essential aspect of data quality, and has in turn a
huge impact on the effective management of companies. For example, statistics
are computed and audits are conducted in companies by implicitly placing the
strong assumption that the analysed data are complete. In this work, we are
interested in studying the problem of completeness of data produced by business
processes, to the aim of automatically assessing whether a given database query
can be answered with complete information in a certain state of the process. We
formalize so-called quality-aware processes that create data in the real world
and store it in the company's information system possibly at a later point."
"Metadata represents the information about data to be stored in Data
Warehouses.It is a mandatory element of Data Warehouse to build an efficient
Data Warehouse.Metadata helps in data integration,lineage,data quality and
populating transformed data into data warehouse.Spatial data warehouses are
based on spatial data mostly collected from Geographical Information
Systems(GIS)and the transactional systems that are specific to an application
or enterprise.Metadata design and deployment is the most critical phase in
building of data warehouse where it is mandatory to bring the spatial
information and data modeling together.In this paper,we present a holistic
metadata framework that drives metadata creation for spatial data warehouse.
Theoretically, the proposed metadata framework improves the efficiency of
accessing of data in response to frequent queries on SDWs.In other words, the
proposed framework decreases the response time of the query and accurate
information is fetched from Data Warehouse including the spatial information."
"Acting on time-critical events by processing ever growing social media or
news streams is a major technical challenge. Many of these data sources can be
modeled as multi-relational graphs. Continuous queries or techniques to search
for rare events that typically arise in monitoring applications have been
studied extensively for relational databases. This work is dedicated to answer
the question that emerges naturally: how can we efficiently execute a
continuous query on a dynamic graph? This paper presents an exact subgraph
search algorithm that exploits the temporal characteristics of representative
queries for online news or social media monitoring. The algorithm is based on a
novel data structure called the Subgraph Join Tree (SJ-Tree) that leverages the
structural and semantic characteristics of the underlying multi-relational
graph. The paper concludes with extensive experimentation on several real-world
datasets that demonstrates the validity of this approach."
"Acting on time-critical events by processing ever growing social media, news
or cyber data streams is a major technical challenge. Many of these data
sources can be modeled as multi-relational graphs. Mining and searching for
subgraph patterns in a continuous setting requires an efficient approach to
incremental graph search. The goal of our work is to enable real-time search
capabilities for graph databases. This demonstration will present a dynamic
graph query system that leverages the structural and semantic characteristics
of the underlying multi-relational graph."
"The approximation introduced by finite-precision representation of continuous
data can induce arbitrarily large information leaks even when the computation
using exact semantics is secure. Such leakage can thus undermine design efforts
aimed at protecting sensitive information. We focus here on differential
privacy, an approach to privacy that emerged from the area of statistical
databases and is now widely applied also in other domains. In this approach,
privacy is protected by the addition of noise to a true (private) value. To
date, this approach to privacy has been proved correct only in the ideal case
in which computations are made using an idealized, infinite-precision
semantics. In this paper, we analyze the situation at the implementation level,
where the semantics is necessarily finite-precision, i.e. the representation of
real numbers and the operations on them, are rounded according to some level of
precision. We show that in general there are violations of the differential
privacy property, and we study the conditions under which we can still
guarantee a limited (but, arguably, totally acceptable) variant of the
property, under only a minor degradation of the privacy level. Finally, we
illustrate our results on two cases of noise-generating distributions: the
standard Laplacian mechanism commonly used in differential privacy, and a
bivariate version of the Laplacian recently introduced in the setting of
privacy-aware geolocation."
"The entity relationship modelling using the original ER notation has been
applauded providing a natural view of data in conceptual modelling of
information systems. However, the current ER to relational model transformation
algorithm is known to be insufficient in providing a complete and accurate
representation of the ER model undertaken for transformation. In an effort to
derive better transformations from ER models, we have understood that
modifications should be introduced to both of the existing transformation
algorithm as well as to the ER notation. Introducing some new concepts, we have
adapted the original ER notation and developed a new transformation algorithm
based on the existing one. This paper presents the modified ER notation with an
ER diagram drawn based on the new notation."
"We consider the problem of computing a relational query $q$ on a large input
database of size $n$, using a large number $p$ of servers. The computation is
performed in rounds, and each server can receive only $O(n/p^{1-\varepsilon})$
bits of data, where $\varepsilon \in [0,1]$ is a parameter that controls
replication. We examine how many global communication steps are needed to
compute $q$. We establish both lower and upper bounds, in two settings. For a
single round of communication, we give lower bounds in the strongest possible
model, where arbitrary bits may be exchanged; we show that any algorithm
requires $\varepsilon \geq 1-1/\tau^*$, where $\tau^*$ is the fractional vertex
cover of the hypergraph of $q$. We also give an algorithm that matches the
lower bound for a specific class of databases. For multiple rounds of
communication, we present lower bounds in a model where routing decisions for a
tuple are tuple-based. We show that for the class of tree-like queries there
exists a tradeoff between the number of rounds and the space exponent
$\varepsilon$. The lower bounds for multiple rounds are the first of their
kind. Our results also imply that transitive closure cannot be computed in O(1)
rounds of communication."
"The vision of the Semantic Web is becoming a reality with billions of RDF
triples being distributed over multiple queryable end-points (e.g. Linked
Data). Although there has been a body of work on RDF triples persistent
storage, it seems that, considering reasoning dependent queries, the problem of
providing an efficient, in terms of performance, scalability and data
redundancy, partitioning of the data is still open. In regards to recent data
partitioning studies, it seems reasonable to think that data partitioning
should be guided considering several directions (e.g. ontology, data,
application queries). This paper proposes several contributions: describe an
overview of what a road map for data partitioning for RDF data efficient and
persistent storage should contain, present some preliminary results and
analysis on the particular case of ontology-guided (property hierarchy)
partitioning and finally introduce a set of semantic query rewriting rules to
support querying RDF data needing OWL inferences"
"Conceptual modelling using the entity relationship (ER) model has been widely
used for database design for a long period of time. However, studies indicate
that creating a satisfactory relational model representation from an ER model
is uncertain due to the insufficiencies both in the transformation methods used
and in the relational model itself. In an effort to solve the issue the
original ER notation has been modified, and accordingly, a new transformation
algorithm has been developed. This paper presents the proposed transformation
algorithm. Using a real world example it shows how the algorithm can be applied
in practice. The paper also discusses how to validate the resulted database and
reclaim the information that it represents."
"This paper aims at finding a subclass of DTDs that covers many of the
real-world DTDs while offering a polynomial-time complexity for deciding the
XPath satisfiability problem. In our previous work, we proposed RW-DTDs, which
cover most of the real-world DTDs (26 out of 27 real-world DTDs and 1406 out of
1407 DTD rules). However, under RW-DTDs, XPath satisfiability with only child,
descendant-or-self, and sibling axes is tractable. In this paper, we propose
MRW-DTDs, which are slightly smaller than RW-DTDs but have tractability on
XPath satisfiability with parent axes or qualifiers. MRW-DTDs are a proper
superclass of duplicate-free DTDs proposed by Montazerian et al., and cover 24
out of the 27 real-world DTDs and 1403 out of the 1407 DTD rules. Under
MRW-DTDs, we show that XPath satisfiability problems with (1) child, parent,
and sibling axes, and (2) child and sibling axes and qualifiers are both
tractable, which are known to be intractable under RW-DTDs."
"Many fields of science rely on relational database management systems to
analyze, publish and share data. Since RDBMS are originally designed for, and
their development directions are primarily driven by, business use cases they
often lack features very important for scientific applications. Horizontal
scalability is probably the most important missing feature which makes it
challenging to adapt traditional relational database systems to the ever
growing data sizes. Due to the limited support of array data types and metadata
management, successful application of RDBMS in science usually requires the
development of custom extensions. While some of these extensions are specific
to the field of science, the majority of them could easily be generalized and
reused in other disciplines. With the Graywulf project we intend to target
several goals. We are building a generic platform that offers reusable
components for efficient storage, transformation, statistical analysis and
presentation of scientific data stored in Microsoft SQL Server. Graywulf also
addresses the distributed computational issues arising from current RDBMS
technologies. The current version supports load balancing of simple queries and
parallel execution of partitioned queries over a set of mirrored databases.
Uniform user access to the data is provided through a web based query interface
and a data surface for software clients. Queries are formulated in a slightly
modified syntax of SQL that offers a transparent view of the distributed data.
The software library consists of several components that can be reused to
develop complex scientific data warehouses: a system registry, administration
tools to manage entire database server clusters, a sophisticated workflow
execution framework, and a SQL parser library."
"The principles and strategies found in material management are comparable and
analogue with the data management. This paper concentrates on the conversion of
product inventory management principles into data inventory management
principles. Efforts were made to enumerate various impacting parameters that
would be appropriate to consider if any data inventory model could be plotted."
"In the data mining field, association rules are discovered having domain
knowledge specified as a minimum support threshold. The accuracy in setting up
this threshold directly influences the number and the quality of association
rules discovered. Typically, before association rules are mined, a user needs
to determine a support threshold in order to obtain only the frequent item
sets. Having users to determine a support threshold attracts a number of
issues. We propose an association rule mining framework that does not require a
per-set support threshold. Often, the number of association rules, even though
large in number, misses some interesting rules and the rules quality
necessitates further analysis. As a result, decision making using these rules
could lead to risky actions."
"One of the major challenges being faced by Database managers today is to
manage the performance of complex SQL queries which are dynamic in nature.
Since it is not possible to tune each and every query because of its dynamic
nature, there is a definite possibility that these queries may cause serious
database performance issues if left alone. Conventional indexes are useful only
for those queries which are frequently executed or those columns which are
frequently joined in SQL queries. This proposal is regarding a method, a query
optimizer for optimizing database queries in a database management system. Just
In Time(JIT) indexes are On Demand, temporary indexes created on the fly based
on current needs so that they would be able to satisfy any kind of queries. JIT
indexes are created only when the configured threshold values for resource
consumption are exceeded for a query. JIT indexes will be stored in a temporary
basis and will get replaced by new JIT indexes in course of time. The proposal
is substantiated with the help of experimental programs and with various test
cases. The idea of parallel programming is also brought into picture as it can
be effectively used in a multiprocessor system. Multiple threads are employed
while one set of threads proceed in the conventional way and the other set of
threads proceed with the proposed way. A live switch over is made when a
suitable stage is reached and from then onwards the proposed method will only
come into picture."
"In this paper, we focus on the problem of determining whether two conjunctive
(""CQ"") queries posed on relational data are combined-semantics equivalent [9].
We continue the tradition of [2,5,9] of studying this problem using the tool of
containment between queries. We introduce a syntactic necessary and sufficient
condition for equivalence of queries belonging to a large natural language of
""explicit-wave"" combined-semantics CQ queries; this language encompasses (but
is not limited to) all set, bag, and bag-set queries, and appears to cover all
combined-semantics CQ queries that are expressible in SQL. Our result solves in
the positive the decidability problem of determining combined-semantics
equivalence for pairs of explicit-wave CQ queries. That is, for an arbitrary
pair of combined-semantics CQ queries, it is decidable (i) to determine whether
each of the queries is explicit wave, and (ii) to determine, in case both
queries are explicit wave, whether or not they are combined-semantics
equivalent, by using our syntactic criterion. (The problem of determining
equivalence for general combined-semantics CQ queries remains open. Even so,
our syntactic sufficient containment condition could still be used to determine
that two general CQ queries are combined-semantics equivalent.) Our equivalence
test, as well as our general sufficient condition for containment of
combined-semantics CQ queries, reduce correctly to the special cases reported
in [2,5] for set, bag, and bag-set semantics. Our containment and equivalence
conditions also properly generalize the results of [9], provided that the
latter are restricted to the language of (combined-semantics) CQ queries."
"Spatial Online Analytical Processing System involves the non-categorical
attribute information also whereas standard Online Analytical Processing System
deals with only categorical attributes. Providing spatial information to the
data warehouse (DW); two major challenges faced are;1.Defining and Aggregation
of Spatial/Continues values and 2.Representation, indexing, updating and
efficient query processing. In this paper, we present GCUBE(Geographical Cube)
storage and indexing procedure to aggregate the spatial information/Continuous
values. We employed the proposed approach storing and indexing using synthetic
and real data sets and evaluated its build, update and Query time. It is
observed that the proposed procedure offers significant performance advantage."
"The standard approach for optimization of XPath queries by rewriting using
views techniques consists in navigating inside a view's output, thus allowing
the usage of only one view in the rewritten query. Algorithms for richer
classes of XPath rewritings, using intersection or joins on node identifiers,
have been proposed, but they either lack completeness guarantees, or require
additional information about the data. We identify the tightest restrictions
under which an XPath can be rewritten in polynomial time using an intersection
of views and propose an algorithm that works for any documents or type of
identifiers. As a side-effect, we analyze the complexity of the related problem
of deciding if an XPath with intersection can be equivalently rewritten as one
without intersection or union. We extend our formal study of the view-based
rewriting problem for XPath by describing also (i) algorithms for more complex
rewrite plans, with no limitations on the number of intersection and navigation
steps inside view outputs they employ, and (ii) adaptations of our techniques
to deal with XML documents without persistent node Ids, in the presence of XML
keys. Complementing our computational complexity study, we describe a
proof-of-concept implementation of our techniques and possible choices that may
speed up execution in practice, regarding how rewrite plans are built, tested
and executed. We also give a thorough experimental evaluation of these
techniques, focusing on scalability and the running time improvements achieved
by the execution of view-based plans."
"Although RDF graphs have schema information associated with them, in practice
it is very common to find cases in which data do not fully conform to their
schema. A prominent example of this is DBpedia, which is RDF data extracted
from Wikipedia, a publicly editable source of information. In such situations,
it becomes interesting to study the structural properties of the actual data,
because the schema gives an incomplete description of the organization of a
dataset. In this paper we have approached the study of the structuredness of an
RDF graph in a principled way: we propose a framework for specifying
structuredness functions, which gauge the degree to which an RDF graph conforms
to a schema. In particular, we first define a formal language for specifying
structuredness functions with expressions we call rules. This language allows a
user or a database administrator to state a rule to which an RDF graph may
fully or partially conform. Then we consider the issue of discovering a
refinement of a sort (type) by partitioning the dataset into subsets whose
structuredness is over a specified threshold. In particular, we prove that the
natural decision problem associated to this refinement problem is NP-complete,
and we provide a natural translation of this problem into Integer Linear
Programming (ILP). Finally, we test this ILP solution with two real world
datasets, DBpedia Persons and WordNet Nouns, and 4 different and intuitive
rules, which gauge the structuredness in different ways. The rules give
meaningful refinements of the datasets, showing that our language can be a
powerful tool for understanding the structure of RDF data."
"This paper presents a proposed model for database replication model in
private cloud availability regions, which is an enhancement of the SQL Server
AlwaysOn Layers of Protection Model presents by Microsoft in 2012. The
enhancement concentrates in the database replication for private cloud
availability regions through the use of primary and secondary servers. The
processes of proposed model during the client send Write/Read Request to the
server, in synchronous and semi synchronous replication level has been
described in details also the processes of proposed model when the client send
Write/Read Request to the Primary Server presented in details. All the types of
automatic failover situations are presented in this thesis. Using the proposed
models will increase the performance because each one of the secondary servers
will open for Read / Write and allow the clients to connect to the nearby
secondary and less loading on each server. Keywords: Availability Regions,
Cloud Computing, Database Replication, SQL Server AlwaysOn, Synchronization."
"The business intelligence and decision-support systems used in many
application domains casually rely on data warehouses, which are
decision-oriented data repositories modeled as multidimensional (MD)
structures. MD structures help navigate data through hierarchical levels of
detail. In many real-world situations, hierarchies in MD models are complex,
which causes data aggregation issues, collectively known as the summarizability
problem. This problem leads to incorrect analyses and critically affects
decision making. To enforce summarizability, existing approaches alter either
MD models or data, and must be applied a priori, on a case-by-case basis, by an
expert. To alter neither models nor data, a few query-time approaches have been
proposed recently, but they only detect summarizability issues without solving
them. Thus, we propose in this paper a novel approach that automatically
detects and processes summarizability issues at query time, without requiring
any particular expertise from the user. Moreover, while most existing
approaches are based on the relational model, our approach focus on an XML MD
model, since XML data is customarily used to represent business data and its
format better copes with complex hierarchies than the relational model.
Finally, our experiments show that our method is likely to scale better than a
reference approach for addressing the summarizability problem in the MD
context."
"Business Intelligence plays an important role in decision making. Based on
data warehouses and Online Analytical Processing, a business intelligence tool
can be used to analyze complex data. Still, summarizability issues in data
warehouses cause ineffective analyses that may become critical problems to
businesses. To settle this issue, many researchers have studied and proposed
various solutions, both in relational and XML data warehouses. However, they
find difficulty in evaluating the performance of their proposals since the
available benchmarks lack complex hierarchies. In order to contribute to
summarizability analysis, this paper proposes an extension to the XML warehouse
benchmark (XWeB) with complex hierarchies. The benchmark enables us to generate
XML data warehouses with scalable complex hierarchies as well as
summarizability processing. We experimentally demonstrated that complex
hierarchies can definitely be included into a benchmark dataset, and that our
benchmark is able to compare two alternative approaches dealing with
summarizability issues."
"Integrating data is a basic concern in many accredited laboratories that
perform a large variety of measurements. However, the present working style in
engineering faculties does not focus much on this aspect. To deal with this
challenge, we developed an educational platform that allows characterization of
acquisition ensembles, generation of Web pages for lessons, as well as
transformation of measured data and storage in a common format. As generally we
had to develop individual parsers for each instrument, we also added the
possibility to integrate the LabVIEW workbench, often used for rapid
development of applications in electrical engineering and automatic control.
This paper describes how we configure the platform for specific equipment, i.e.
how we model it, how we create the learning material and how we integrate the
results in a central database. It also introduces a case study for collecting
data from a thermocouple-based acquisition system based on LabVIEW, used by
students for a laboratory of measurement technologies and transducers."
"Aggregation has been an important operation since the early days of
relational databases. Today's Big Data applications bring further challenges
when processing aggregation queries, demanding adaptive aggregation algorithms
that can process large volumes of data relative to a potentially limited memory
budget (especially in multiuser settings). Despite its importance, the design
and evaluation of aggregation algorithms has not received the same attention
that other basic operators, such as joins, have received in the literature. As
a result, when considering which aggregation algorithm(s) to implement in a new
parallel Big Data processing platform (AsterixDB), we faced a lack of ""off the
shelf"" answers that we could simply read about and then implement based on
prior performance studies.
  In this paper we revisit the engineering of efficient local aggregation
algorithms for use in Big Data platforms. We discuss the salient implementation
details of several candidate algorithms and present an in-depth experimental
performance study to guide future Big Data engine developers. We show that the
efficient implementation of the aggregation operator for a Big Data platform is
non-trivial and that many factors, including memory usage, spilling strategy,
and I/O and CPU cost, should be considered. Further, we introduce precise cost
models that can help in choosing an appropriate algorithm based on input
parameters including memory budget, grouping key cardinality, and data skew."
"This paper studies the constrained-space probabilistic threshold range query
(CSPTRQ) for moving objects. We differentiate two kinds of CSPTRQs: implicit
and explicit ones. Specifically, for each moving object $o$, we assume $o$
cannot be located in some specific areas, we model its location as a closed
region, $u$, together with a probability density function, and model a query
range, $R$, as an arbitrary polygon. An implicit CSPTRQ can be reduced to a
search (over all the $u$) that returns a set of objects, which have
probabilities higher than a probability threshold $p_t$ to be located in $R$,
where $0\leq p_t\leq 1$. In contrast, an explicit CSPTRQ returns a set of
tuples in form of ($o$, $p$) such that $p\geq p_t$, where $p$ is the
probability of $o$ being located in $R$. A straightforward adaptation of
existing method is inefficient due to its weak pruning/validating capability.
In order to efficiently process such queries, we propose targeted solutions, in
which three main ideas are incorporated: (1) swapping the order of geometric
operations based on the computation duality; (2) pruning unrelated objects in
the early stages using the location unreachability; and (3) computing the
probability using the multi-step mechanism. Extensive experimental results
demonstrate the efficiency and effectiveness of the proposed algorithms."
"This paper presents and analysis the common existing sequential pattern
mining algorithms. It presents a classifying study of sequential pattern-mining
algorithms into five extensive classes. First, on the basis of Apriori-based
algorithm, second on Breadth First Search-based strategy, third on Depth First
Search strategy, fourth on sequential closed-pattern algorithm and five on the
basis of incremental pattern mining algorithms. At the end, a comparative
analysis is done on the basis of important key features supported by various
algorithms. This study gives an enhancement in the understanding of the
approaches of sequential pattern mining."
"Many automated systems need the capability of automatic change detection
without the given detection threshold. This paper presents an automated change
detection algorithm in streaming multivariate data. Two overlapping windows are
used to quantify the changes. While a window is used as the reference window
from which the clustering is created, the other called the current window
captures the newly incoming data points. A newly incoming data point can be
considered a change point if it is not a member of any cluster. As our
clustering-based change detector does not require detection threshold, it is an
automated detector. Based on this change detector, we propose a reactive
clustering algorithm for streaming data. Our empirical results show that, our
clustering-based change detector works well with multivariate streaming data.
The detection accuracy depends on the number of clusters in the reference
window, the window width."
"The Web of Data is an open environment consisting of a great number of large
inter-linked RDF datasets from various domains. In this environment,
organizations and companies adopt the Linked Data practices utilizing Semantic
Web (SW) technologies, in order to publish their data and offer SPARQL
endpoints (i.e., SPARQL-based search services). On the other hand, the dominant
standard for information exchange in the Web today is XML. The SW and XML
worlds and their developed infrastructures are based on different data models,
semantics and query languages. Thus, it is crucial to develop interoperability
mechanisms that allow the Web of Data users to access XML datasets, using
SPARQL, from their own working environments. It is unrealistic to expect that
all the existing legacy data (e.g., Relational, XML, etc.) will be transformed
into SW data. Therefore, publishing legacy data as Linked Data and providing
SPARQL endpoints over them has become a major research challenge. In this
direction, we introduce the SPARQL2XQuery Framework which creates an
interoperable environment, where SPARQL queries are automatically translated to
XQuery queries, in order to access XML data across the Web. The SPARQL2XQuery
Framework provides a mapping model for the expression of OWL-RDF/S to XML
Schema mappings as well as a method for SPARQL to XQuery translation. To this
end, our Framework supports both manual and automatic mapping specification
between ontologies and XML Schemas. In the automatic mapping specification
scenario, the SPARQL2XQuery exploits the XS2OWL component which transforms XML
Schemas into OWL ontologies. Finally, extensive experiments have been conducted
in order to evaluate the schema transformation, mapping generation, query
translation and query evaluation efficiency, using both real and synthetic
datasets."
"The well-known 3V architectural paradigm for Big Data introduced by Laney
(2011), provides a simplified framework for defining the architecture of a big
data platform to be deployed in various scenarios tackling processing of
massive datasets. While additional components such as Variability and Veracity
have been discussed as an extension to the 3V model, the basic components
(volume, variety, velocity) provide a quantitative framework while variability
and veracity target a more qualitative approach. In this paper we argue why the
basic 3V's are not equal due to the different requirements that need to be
covered in case higher demands for a particular ""V"". Similar to other
conjectures such as the CAP theorem 3V based architectures differ on their
implementation. We call this paradigm heterogeneity and we provide a taxonomy
of the existing tools (as of 2013) covering the Hadoop ecosystem from the
perspective of heterogeneity. This paper contributes on the understanding of
the Hadoop ecosystem from the perspective of different workloads and aims to
help researchers and practitioners on the design of scalable platforms
targeting different operational needs."
"Despite their relatively low sampling factor, the freely available, randomly
sampled status streams of Twitter are very useful sources of geographically
embedded social network data. To statistically analyze the information Twitter
provides via these streams, we have collected a year's worth of data and built
a multi-terabyte relational database from it. The database is designed for fast
data loading and to support a wide range of studies focusing on the statistics
and geographic features of social networks, as well as on the linguistic
analysis of tweets. In this paper we present the method of data collection, the
database design, the data loading procedure and special treatment of geo-tagged
and multi-lingual data. We also provide some SQL recipes for computing network
statistics."
"We witness an unprecedented proliferation of knowledge graphs that record
millions of entities and their relationships. While knowledge graphs are
structure-flexible and content rich, they are difficult to use. The challenge
lies in the gap between their overwhelming complexity and the limited database
knowledge of non-professional users. If writing structured queries over simple
tables is difficult, complex graphs are only harder to query. As an initial
step toward improving the usability of knowledge graphs, we propose to query
such data by example entity tuples, without requiring users to form complex
graph queries. Our system, GQBE (Graph Query By Example), automatically derives
a weighted hidden maximal query graph based on input query tuples, to capture a
user's query intent. It efficiently finds and ranks the top approximate answer
tuples. For fast query processing, GQBE only partially evaluates query graphs.
We conducted experiments and user studies on the large Freebase and DBpedia
datasets and observed appealing accuracy and efficiency. Our system provides a
complementary approach to the existing keyword-based methods, facilitating
user-friendly graph querying. To the best of our knowledge, there was no such
proposal in the past in the context of graphs."
"Graphs are becoming one of the most popular data modeling paradigms since
they are able to model complex relationships that cannot be easily captured
using traditional data models. One of the major tasks of graph management is
graph matching, which aims to find all of the subgraphs in a data graph that
match a query graph. In the literature, proposals in this context are
classified into two different categories: graph-at-a-time, which process the
whole query graph at the same time, and vertex-at-a-time, which process a
single vertex of the query graph at the same time. In this paper, we propose a
new vertex-at-a-time proposal that is based on graphlets, each of which
comprises a vertex of a graph, all of the immediate neighbors of that vertex,
and all of the edges that relate those neighbors. Furthermore, we also use the
concept of minimum hub covers, each of which comprises a subset of vertices in
the query graph that account for all of the edges in that graph. We present the
algorithms of our proposal and describe an implementation based on XQuery and
RDF. Our evaluation results show that our proposal is appealing to perform
graph matching."
"Datasets of different characteristics are needed by the research community
for experimental purposes. However, real data may be difficult to obtain due to
privacy concerns. Moreover, real data may not meet specific characteristics
which are needed to verify new approaches under certain conditions. Given these
limitations, the use of synthetic data is a viable alternative to complement
the real data. In this report, we describe the process followed to generate
synthetic data using Benerator, a publicly available tool. The results show
that the synthetic data preserves a high level of accuracy compared to the
original data. The generated datasets correspond to microdata containing
records with social, economic and demographic data which mimics the
distribution of aggregated statistics from the 2011 Irish Census data."
"SPARQL is the standard query language for RDF graphs. In its strict
instantiation, it only offers querying according to the RDF semantics and would
thus ignore the semantics of data expressed with respect to (RDF) schemas or
(OWL) ontologies. Several extensions to SPARQL have been proposed to query RDF
data modulo RDFS, i.e., interpreting the query with RDFS semantics and/or
considering external ontologies. We introduce a general framework which allows
for expressing query answering modulo a particular semantics in an homogeneous
way. In this paper, we discuss extensions of SPARQL that use regular
expressions to navigate RDF graphs and may be used to answer queries
considering RDFS semantics. We also consider their embedding as extensions of
SPARQL. These SPARQL extensions are interpreted within the proposed framework
and their drawbacks are presented. In particular, we show that the PSPARQL
query language, a strict extension of SPARQL offering transitive closure,
allows for answering SPARQL queries modulo RDFS graphs with the same complexity
as SPARQL through a simple transformation of the queries. We also consider
languages which, in addition to paths, provide constraints. In particular, we
present and compare nSPARQL and our proposal CPSPARQL. We show that CPSPARQL is
expressive enough to answer full SPARQL queries modulo RDFS. Finally, we
compare the expressiveness and complexity of both nSPARQL and the corresponding
fragment of CPSPARQL, that we call cpSPARQL. We show that both languages have
the same complexity through cpSPARQL, being a proper extension of SPARQL graph
patterns, is more expressive than nSPARQL."
"Data validation is becoming more and more important with the ever-growing
amount of data being consumed and transmitted by systems over the Internet. It
is important to ensure that the data being sent is valid as it may contain
entry errors, which may be consumed by different systems causing further
errors. XML has become the defacto standard for data transfer. The XML Schema
Definition language (XSD) was created to help XML structural validation and
provide a schema for data type restrictions, however it does not allow for more
complex situations. In this article we introduce a way to provide rule based
XML validation and correction through the extension and improvement of our SRML
metalanguage. We also explore the option of applying it in a database as a
trigger for CRUD operations allowing more granular dataset validation on an
atomic level allowing for more complex dataset record validation rules."
"Rough set theory is a new method that deals with vagueness and uncertainty
emphasized in decision making. Data mining is a discipline that has an
important contribution to data analysis, discovery of new meaningful knowledge,
and autonomous decision making. The rough set theory offers a viable approach
for decision rule extraction from data.This paper, introduces the fundamental
concepts of rough set theory and other aspects of data mining, a discussion of
data representation with rough set theory including pairs of attribute-value
blocks, information tables reducts, indiscernibility relation and decision
tables. Additionally, the rough set approach to lower and upper approximations
and certain possible rule sets concepts are introduced. Finally, some
description about applications of the data mining system with rough set theory
is included."
"We study the novel problem of finding new, prominent situational facts, which
are emerging statements about objects that stand out within certain contexts.
Many such facts are newsworthy---e.g., an athlete's outstanding performance in
a game, or a viral video's impressive popularity. Effective and efficient
identification of these facts assists journalists in reporting, one of the main
goals of computational journalism. Technically, we consider an ever-growing
table of objects with dimension and measure attributes. A situational fact is a
""contextual"" skyline tuple that stands out against historical tuples in a
context, specified by a conjunctive constraint involving dimension attributes,
when a set of measure attributes are compared. New tuples are constantly added
to the table, reflecting events happening in the real world. Our goal is to
discover constraint-measure pairs that qualify a new tuple as a contextual
skyline tuple, and discover them quickly before the event becomes yesterday's
news. A brute-force approach requires exhaustive comparison with every tuple,
under every constraint, and in every measure subspace. We design algorithms in
response to these challenges using three corresponding ideas---tuple reduction,
constraint pruning, and sharing computation across measure subspaces. We also
adopt a simple prominence measure to rank the discovered facts when they are
numerous. Experiments over two real datasets validate the effectiveness and
efficiency of our techniques."
"Scientific workflows are becoming increasingly popular for compute-intensive
and data-intensive scientific applications. The vision and promise of
scientific workflows includes rapid, easy workflow design, reuse, scalable
execution, and other advantages, e.g., to facilitate ""reproducible science""
through provenance (e.g., data lineage) support. However, as described in the
paper, important research challenges remain. While the database community has
studied (business) workflow technologies extensively in the past, most current
work in scientific workflows seems to be done outside of the database
community, e.g., by practitioners and researchers in the computational sciences
and eScience. We provide a brief introduction to scientific workflows and
provenance, and identify areas and problems that suggest new opportunities for
database research."
"Living in the era of data deluge, we have witnessed a web content explosion,
largely due to the massive availability of User-Generated Content (UGC). In
this work, we specifically consider the problem of geospatial information
extraction and representation, where one can exploit diverse sources of
information (such as image and audio data, text data, etc), going beyond
traditional volunteered geographic information. Our ambition is to include
available narrative information in an effort to better explain geospatial
relationships: with spatial reasoning being a basic form of human cognition,
narratives expressing such experiences typically contain qualitative spatial
data, i.e., spatial objects and spatial relationships.
  To this end, we formulate a quantitative approach for the representation of
qualitative spatial relations extracted from UGC in the form of texts. The
proposed method quantifies such relations based on multiple text observations.
Such observations provide distance and orientation features which are utilized
by a greedy Expectation Maximization-based (EM) algorithm to infer a
probability distribution over predefined spatial relationships; the latter
represent the quantified relationships under user-defined probabilistic
assumptions. We evaluate the applicability and quality of the proposed approach
using real UGC data originating from an actual travel blog text corpus. To
verify the quality of the result, we generate grid-based maps visualizing the
spatial extent of the various relations."
"Data cubes are widely used as a powerful tool to provide multidimensional
views in data warehousing and On-Line Analytical Processing (OLAP). However,
with increasing data sizes, it is becoming computationally expensive to perform
data cube analysis. The problem is exacerbated by the demand of supporting more
complicated aggregate functions (e.g. CORRELATION, Statistical Analysis) as
well as supporting frequent view updates in data cubes. This calls for new
scalable and efficient data cube analysis systems. In this paper, we introduce
HaCube, an extension of MapReduce, designed for efficient parallel data cube
analysis on large-scale data by taking advantages from both MapReduce (in terms
of scalability) and parallel DBMS (in terms of efficiency). We also provide a
general data cube materialization algorithm which is able to facilitate the
features in MapReduce-like systems towards an efficient data cube computation.
Furthermore, we demonstrate how HaCube supports view maintenance through either
incremental computation (e.g. used for SUM or COUNT) or recomputation (e.g.
used for MEDIAN or CORRELATION). We implement HaCube by extending Hadoop and
evaluate it based on the TPC-D benchmark over billions of tuples on a cluster
with over 320 cores. The experimental results demonstrate the efficiency,
scalability and practicality of HaCube for cube analysis over a large amount of
data in a distributed environment."
"Recent years have seen an increased interest in large-scale analytical
dataflows on non-relational data. These dataflows are compiled into execution
graphs scheduled on large compute clusters. In many novel application areas the
predominant building blocks of such dataflows are user-defined predicates or
functions (UDFs). However, the heavy use of UDFs is not well taken into account
for dataflow optimization in current systems.
  SOFA is a novel and extensible optimizer for UDF-heavy dataflows. It builds
on a concise set of properties for describing the semantics of Map/Reduce-style
UDFs and a small set of rewrite rules, which use these properties to find a
much larger number of semantically equivalent plan rewrites than possible with
traditional techniques. A salient feature of our approach is extensibility: We
arrange user-defined operators and their properties into a subsumption
hierarchy, which considerably eases integration and optimization of new
operators. We evaluate SOFA on a selection of UDF-heavy dataflows from
different domains and compare its performance to three other algorithms for
dataflow optimization. Our experiments reveal that SOFA finds efficient plans,
outperforming the best plans found by its competitors by a factor of up to 6."
"Streaming of XML transformations is a challenging task and only very few
systems support streaming. Research approaches generally define custom
fragments of XQuery and XPath that are amenable to streaming, and then design
custom algorithms for each fragment. These languages have several shortcomings.
Here we take a more principles approach to the problem of streaming
XQuery-based transformations. We start with an elegant transducer model for
which many static analysis problems are well-understood: the Macro Forest
Transducer (MFT). We show that a large fragment of XQuery can be translated
into MFTs --- indeed, a fragment of XQuery, that can express important features
that are missing from other XQuery stream engines, such as GCX: our fragment of
XQuery supports XPath predicates and let-statements. We then rely on a
streaming execution engine for MFTs, one which uses a well-founded set of
optimizations from functional programming, such as strictness analysis and
deforestation. Our prototype achieves time and memory efficiency comparable to
the fastest known engine for XQuery streaming, GCX. This is surprising because
our engine relies on the OCaml built in garbage collector and does not use any
specialized buffer management, while GCX's efficiency is due to clever and
explicit buffer management."
"In contrast to XML query languages as e.g. XPath which require knowledge on
the query language as well as on the document structure, keyword search is open
to anybody. As the size of XML sources grows rapidly, the need for efficient
search indices on XML data that support keyword search increases. In this
paper, we present an approach of XML keyword search which is based on the DAG
of the XML data, where repeated substructures are considered only once, and
therefore, have to be searched only once. As our performance evaluation shows,
this DAG-based extension of the set intersection search algorithm[1], [2], can
lead to search times that are on large documents more than twice as fast as the
search times of the XML-based approach. Additionally, we utilize a smaller
index, i.e., we consume less main memory to compute the results."
"Clustering is widely used in different field such as biology, psychology, and
economics. Most traditional clustering algorithms are limited to handling
datasets that contain either numeric or categorical attributes. However,
datasets with mixed types of attributes are common in real life data mining
applications. In this paper, we review partitioning based algorithm such as
K-prototype, Extension of K-prototype, K-histogram, Fuzzy approaches, genetic
approaches, etc. These algorithm works on both numerical and categorical data.
The approaches has been proposed to handle mixed data are based on four
different perceptive: i) split data set into two part such that each part
contain either numerical or categorical data, then apply separate clustering
algorithm on each data set, finally combined the result of both clustering
algorithm, ii) converting categorical attribute into numerical attribute and
apply numerical attribute clustering algorithm; iii) discrimination of
numerical attribute and apply categorical based clustering algorithm; iv)
Conversion of the categorical attributes into binary ones and apply any
numerical based clustering algorithm"
"We investigate schema languages for unordered XML having no relative order
among siblings. First, we propose unordered regular expressions (UREs),
essentially regular expressions with unordered concatenation instead of
standard concatenation, that define languages of unordered words to model the
allowed content of a node (i.e., collections of the labels of children).
However, unrestricted UREs are computationally too expensive as we show the
intractability of two fundamental decision problems for UREs: membership of an
unordered word to the language of a URE and containment of two UREs.
Consequently, we propose a practical and tractable restriction of UREs,
disjunctive interval multiplicity expressions (DIMEs).
  Next, we employ DIMEs to define languages of unordered trees and propose two
schema languages: disjunctive interval multiplicity schema (DIMS), and its
restriction, disjunction-free interval multiplicity schema (IMS). We study the
complexity of the following static analysis problems: schema satisfiability,
membership of a tree to the language of a schema, schema containment, as well
as twig query satisfiability, implication, and containment in the presence of
schema. Finally, we study the expressive power of the proposed schema languages
and compare them with yardstick languages of unordered trees (FO, MSO, and
Presburger constraints) and DTDs under commutative closure. Our results show
that the proposed schema languages are capable of expressing many practical
languages of unordered trees and enjoy desirable computational properties."
"In this paper a Set Theoretic approach has been reported for analyzing
inter-relationship between any numbers of RDF Graphs. An RDF Graph represents
triples in Resource Description Format of semantic web. So the identification
and characterization of criteria for inter-relationship of RDF Graphs shows a
new road in semantic search. Using set theoretic approach, a sound framing
criteria can be designed that examine whether two RDF Graphs are related and if
yes, how these relationships could be described with formal set theory. Along
with this, by introducing RDF Schema, the inter-relationship status is refined
into n-dimensional induced relationships."
"Recent technology breakthroughs have enabled data collection of unprecedented
scale, rate, variety and complexity that has led to an explosion in data
management requirements. Existing theories and techniques are not adequate to
fulfil these requirements. We endeavour to rethink the way data management
research is being conducted and we propose to work towards modular data
management that will allow for unification of the expression of data management
problems and systematization of their solution. The core of such an approach is
the novel notion of a datom, i.e. a data management atom, which encapsulates
generic data management provision. The datom is the foundation for comparison,
customization and re-usage of data management problems and solutions. The
proposed approach can signal a revolution in data management research and a
long anticipated evolution in data management engineering."
"With the widespread use of shared-nothing clusters of servers, there has been
a proliferation of distributed object stores that offer high availability,
reliability and enhanced performance for MapReduce-style workloads. However,
relational workloads cannot always be evaluated efficiently using MapReduce
without extensive data migrations, which cause network congestion and reduced
query throughput. We study the problem of computing data placement strategies
that minimize the data communication costs incurred by typical relational query
workloads in a distributed setting.
  Our main contribution is a reduction of the data placement problem to the
well-studied problem of {\sc Graph Partitioning}, which is NP-Hard but for
which efficient approximation algorithms exist. The novelty and significance of
this result lie in representing the communication cost exactly and using
standard graphs instead of hypergraphs, which were used in prior work on data
placement that optimized for different objectives (not communication cost).
  We study several practical extensions of the problem: with load balancing,
with replication, with materialized views, and with complex query plans
consisting of sequences of intermediate operations that may be computed on
different servers. We provide integer linear programs (IPs) that may be used
with any IP solver to find an optimal data placement. For the no-replication
case, we use publicly available graph partitioning libraries (e.g., METIS) to
efficiently compute nearly-optimal solutions. For the versions with
replication, we introduce two heuristics that utilize the {\sc Graph
Partitioning} solution of the no-replication case. Using the TPC-DS workload,
it may take an IP solver weeks to compute an optimal data placement, whereas
our reduction produces nearly-optimal solutions in seconds."
"ERP systems contain huge amounts of data related to the actual execution of
business processes. These systems have a particular way of recording activities
which results in an unclear display of business processes in event logs.
Several works have been conducted on ERP systems, most of them focusing on the
development of new algorithms for the automatic discovery of business
processes. We focused on addressing issues like, how can organizations with ERP
systems apply process mining for analyzing their business processes in order to
improve them. The data handling aspect of ERP systems contrasts with those of
BPMS or workflow based systems, whose systematical storage of events
facilitates the application of process mining techniques. CRISP-DM has emerged
as the de facto standard for developing data mining and knowledge discovery
projects. Successful data mining requires three families of analytical
capabilities namely reporting, classification and forecasting. A data miner
uses more than one analytical method to get the best results. The objective of
this paper is to improve the usability and understandability of process mining
techniques, by implementing CRISP-DM methodology for their application in ERP
contexts, detailed in terms of specific implementation tools and step by step
coordination. Our study confirms that data discovery from ERP system improves
strategic and operational decision making."
"Integrity checking is a crucial issue, as databases change their instance all
the time and therefore need to be checked continuously and rapidly. Decades of
research have produced a plethora of methods for checking integrity constraints
of a database in an incremental manner. However, not much has been said about
when to check integrity. In this paper, we study the differences and
similarities between checking integrity before an update (a.k.a. pre-test) or
after (a.k.a. post-test) in order to assess the respective convenience and
properties."
"Conceptual dependencies (CDs) are particular kinds of key dependencies (KDs)
and inclusion dependencies (IDs) that precisely characterize relational
schemata modeled according to the main features of the Entity-Relationship (ER)
model. An instance for such a schema may be inconsistent (data violate the
dependencies) and incomplete (data constitute a piece of correct information,
but not necessarily all the relevant information). While undecidable under
general KDs and IDs, query answering under incomplete data is known to be
decidable for CDs. The known techniques are based on the chase -- a special
instance, organized in levels of depth, that is a representative of all the
instances that satisfy the dependencies and that include the initial instance.
Although the chase generally has infinite size, query answering can be
addressed by posing the query (or a rewriting thereof) on a finite, initial
part of the chase. Contrary to previous claims, we show that the maximum level
of such an initial part cannot be bounded by a constant that does not depend on
the size of the initial instance."
"Unsupervised classification called clustering is a process of organizing
objects into groups whose members are similar in some way. Clustering of
uncertain data objects is a challenge in spatial data bases. In this paper we
use Probability Density Functions (PDF) to represent these uncertain data
objects, and apply Uncertain K-Means algorithm to generate the clusters. This
clustering algorithm uses the Expected Distance (ED) to compute the distance
between objects and cluster representatives. To further improve the performance
of UK-Means we propose a novel technique called Voronoi Diagrams from
Computational Geometry to prune the number of computations of ED. This
technique works efficiently but results pruning overheads. In order to reduce
these in pruning overhead we introduce R*-tree indexing over these uncertain
data objects, so that it reduces the computational cost and pruning overheads.
Our novel approach of integrating UK-Means with voronoi diagrams and R* Tree
applied over uncertain data objects generates imposing outcome when compared
with the accessible methods."
"Similarity matching and join of time series data streams has gained a lot of
relevance in today's world that has large streaming data. This process finds
wide scale application in the areas of location tracking, sensor networks,
object positioning and monitoring to name a few. However, as the size of the
data stream increases, the cost involved to retain all the data in order to aid
the process of similarity matching also increases. We develop a novel framework
to addresses the following objectives. Firstly, Dimension reduction is
performed in the preprocessing stage, where large stream data is segmented and
reduced into a compact representation such that it retains all the crucial
information by a technique called Multi-level Segment Means (MSM). This reduces
the space complexity associated with the storage of large time-series data
streams. Secondly, it incorporates effective Similarity Matching technique to
analyze if the new data objects are symmetric to the existing data stream. And
finally, the Pruning Technique that filters out the pseudo data object pairs
and join only the relevant pairs. The computational cost for MSM is O(l*ni) and
the cost for pruning is O(DRF*wsize*d), where DRF is the Dimension Reduction
Factor. We have performed exhaustive experimental trials to show that the
proposed framework is both efficient and competent in comparison with earlier
works."
"Clustering is an important data mining technique where we will be interested
in maximizing intracluster distance and also minimizing intercluster distance.
We have utilized clustering techniques for detecting deviation in product sales
and also to identify and compare sales over a particular period of time.
Clustering is suited to group items that seem to fall naturally together, when
there is no specified class for any new item. We have utilizedannual sales data
of a steel major to analyze Sales Volume & Value with respect to dependent
attributes like products, customers and quantities sold. The demand for steel
products is cyclical and depends on many factors like customer profile,
price,Discounts and tax issues. In this paper, we have analyzed sales data with
clustering algorithms like K-Means&EMwhichrevealed many interesting
patternsuseful for improving sales revenue and achieving higher sales volume.
Our study confirms that partition methods like K-Means & EM algorithms are
better suited to analyze our sales data in comparison to Density based methods
like DBSCAN & OPTICS or Hierarchical methods like COBWEB."
"In a recent paper by Hellerstein [15], a tight relationship was conjectured
between the number of strata of a Datalog${}^\neg$ program and the number of
""coordination stages"" required for its distributed computation. Indeed, Ameloot
et al. [9] showed that a query can be computed by a coordination-free
relational transducer network iff it is monotone, thus answering in the
affirmative a variant of Hellerstein's CALM conjecture, based on a particular
definition of coordination-free computation. In this paper, we present three
additional models for declarative networking. In these variants, relational
transducers have limited access to the way data is distributed. This variation
allows transducer networks to compute more queries in a coordination-free
manner: e.g., a transducer can check whether a ground atom $A$ over the input
schema is in the ""scope"" of the local node, and then send either $A$ or $\neg
A$ to other nodes.
  We show the surprising result that the query given by the well-founded
semantics of the unstratifiable win-move program is coordination-free in some
of the models we consider. We also show that the original transducer network
model [9] and our variants form a strict hierarchy of classes of
coordination-free queries. Finally, we identify different syntactic fragments
of Datalog${}^{\neg\neg}_{\forall}$, called semi-monotone programs, which can
be used as declarative network programming languages, whose distributed
computation is guaranteed to be eventually consistent and coordination-free."
"AI systems typically make decisions and find patterns in data based on the
computation of aggregate and specifically sum functions, expressed as queries,
on data's attributes. This computation can become costly or even inefficient
when these queries concern the whole or big parts of the data and especially
when we are dealing with big data. New types of intelligent analytics require
also the explanation of why something happened. In this paper we present a
randomised algorithm that constructs a small summary of the data, called
Aggregate Lineage, which can approximate well and explain all sums with large
values in time that depends only on its size. The size of Aggregate Lineage is
practically independent on the size of the original data. Our algorithm does
not assume any knowledge on the set of sum queries to be approximated."
"Privacy definitions provide ways for trading-off the privacy of individuals
in a statistical database for the utility of downstream analysis of the data.
In this paper, we present Blowfish, a class of privacy definitions inspired by
the Pufferfish framework, that provides a rich interface for this trade-off. In
particular, we allow data publishers to extend differential privacy using a
policy, which specifies (a) secrets, or information that must be kept secret,
and (b) constraints that may be known about the data. While the secret
specification allows increased utility by lessening protection for certain
individual properties, the constraint specification provides added protection
against an adversary who knows correlations in the data (arising from
constraints). We formalize policies and present novel algorithms that can
handle general specifications of sensitive information and certain count
constraints. We show that there are reasonable policies under which our privacy
mechanisms for k-means clustering, histograms and range queries introduce
significantly lesser noise than their differentially private counterparts. We
quantify the privacy-utility trade-offs for various policies analytically and
empirically on real datasets."
"Motivated by cloud security concerns, there is an increasing interest in
database systems that can store and support queries over encrypted data. A
common architecture for such systems is to use a trusted component such as a
cryptographic co-processor for query processing that is used to securely
decrypt data and perform computations in plaintext. The trusted component has
limited memory, so most of the (input and intermediate) data is kept encrypted
in an untrusted storage and moved to the trusted component on ``demand.''
  In this setting, even with strong encryption, the data access pattern from
untrusted storage has the potential to reveal sensitive information; indeed,
all existing systems that use a trusted component for query processing over
encrypted data have this vulnerability. In this paper, we undertake the first
formal study of secure query processing, where an adversary having full
knowledge of the query (text) and observing the query execution learns nothing
about the underlying database other than the result size of the query on the
database. We introduce a simpler notion, oblivious query processing, and show
formally that a query admits secure query processing iff it admits oblivious
query processing. We present oblivious query processing algorithms for a rich
class of database queries involving selections, joins, grouping and
aggregation. For queries not handled by our algorithms, we provide some initial
evidence that designing oblivious (and therefore secure) algorithms would be
hard via reductions from two simple, well-studied problems that are generally
believed to be hard. Our study of oblivious query processing also reveals
interesting connections to database join theory."
"Complex Event Processing (CEP) is a stream processing model that focuses on
detecting event patterns in continuous event streams. While the CEP model has
gained popularity in the research communities and commercial technologies, the
problem of gracefully degrading performance under heavy load in the presence of
resource constraints, or load shedding, has been largely overlooked. CEP is
similar to ""classical"" stream data management, but addresses a substantially
different class of queries. This unfortunately renders the load shedding
algorithms developed for stream data processing inapplicable. In this paper we
study CEP load shedding under various resource constraints. We formalize broad
classes of CEP load-shedding scenarios as different optimization problems. We
demonstrate an array of complexity results that reveal the hardness of these
problems and construct shedding algorithms with performance guarantees. Our
results shed some light on the difficulty of developing load-shedding
algorithms that maximize utility."
"Recent research on pattern discovery has progressed from mining frequent
patterns and sequences to mining structured patterns, such as trees and graphs.
Graphs as general data structure can model complex relations among data with
wide applications in web exploration and social networks. However, the process
of mining large graph patterns is a challenge due to the existence of large
number of subgraphs. In this paper, we aim to mine only frequent complete graph
patterns. A graph g in a database is complete if every pair of distinct
vertices is connected by a unique edge. Grid Complete Graph (GCG) is a mining
algorithm developed to explore interesting pruning techniques to extract
maximal complete graphs from large spatial dataset existing in Sloan Digital
Sky Survey (SDSS) data. Using a divide and conquer strategy, GCG shows high
efficiency especially in the presence of large number of patterns. In this
paper, we describe GCG that can mine not only simple co-location spatial
patterns but also complex ones. To the best of our knowledge, this is the first
algorithm used to exploit the extraction of maximal complete graphs in the
process of mining complex co-location patterns in large spatial dataset."
"The knowledge discovery algorithms have become ineffective at the abundance
of data and the need for fast algorithms or optimizing methods is required. To
address this limitation, the objective of this work is to adapt a new method
for optimizing the time of association rules extractions from large databases.
Indeed, given a relational database (one relation) represented as a set of
tuples, also called set of attributes, we transform the original database as a
binary table (Bitmap table) containing binary numbers. Then, we use this Bitmap
table to construct a data structure called Peano Tree stored as a binary file
on which we apply a new algorithm called BF-ARM (extension of the well known
Apriori algorithm). Since the database is loaded into a binary file, our
proposed algorithm will traverse this file, and the processes of association
rules extractions will be based on the file stored on disk. The BF-ARM
algorithm is implemented and compared with Apriori, Apriori+ and RS-Rules+
algorithms. The evaluation process is based on three benchmarks (Mushroom, Car
Evaluation and Adult). Our preliminary experimental results showed that our
algorithm produces association rules with a minimum time compared to other
algorithms."
"Context-aware database has drawn increasing attention from both industry and
academia recently by taking users' current situation and environment into
consideration. However, most of the literature focus on individual context,
overlooking the team users. In this paper, we investigate how to integrate team
context into database query process to help the users' get top-ranked database
tuples and make the team more competitive. We introduce naive and optimized
query algorithm to select the suitable records and show that they output the
same results while the latter is more computational efficient. Extensive
empirical studies are conducted to evaluate the query approaches and
demonstrate their effectiveness and efficiency."
"In data exchange, data are materialised from a source schema to a target
schema, according to suitable source-to-target constraints. Constraints are
also expressed on the target schema to represent the domain of interest. A
schema mapping is the union of the source-to-target and of the target
constraints.
  In this paper, we address the problem of containment of schema mappings for
data exchange, which has been recently proposed in this framework as a step
towards the optimization of data exchange settings. We refer to a natural
notion of containment that relies on the behaviour of schema mappings with
respect to conjunctive query answering, in the presence of so-called LAV TGDs
as target constraints. Our contribution is a practical technique for testing
the containment based on the existence of a homomorphism between special
""dummy"" instances, which can be easily built from schema mappings.
  We argue that containment of schema mappings is decidable for most practical
cases, and we set the basis for further investigations in the topic. This paper
extends our preliminary results."
"When data schemata are enriched with expressive constraints that aim at
representing the domain of interest, in order to answer queries one needs to
consider the logical theory consisting of both the data and the constraints.
Query answering in such a context is called ontological query answering.
Commonly adopted database constraints in this field are tuple-generating
dependencies (TGDs) and equality-generating dependencies (EGDs). It is well
known that their interaction leads to intractability or undecidability of query
answering even in the case of simple subclasses. Several conditions have been
found to guarantee separability, that is lack of interaction, between TGDs and
EGDs. Separability makes EGDs (mostly) irrelevant for query answering and
therefore often guarantees tractability, as long as the theory is satisfiable.
In this paper we review the two notions of separability found in the
literature, as well as several syntactic conditions that are sufficient to
prove them. We then shed light on the issue of satisfiability checking, showing
that under a sufficient condition called deep separability it can be done by
considering the TGDs only.
  We show that, fortunately, in the case of TGDs and EGDs, separability implies
deep separability. This result generalizes several analogous ones, proved ad
hoc for particular classes of constraints. Applications include the class of
sticky TGDs and EGDs, for which we provide a syntactic separability condition
which extends the analogous one for linear TGDs; preliminary experiments show
the feasibility of query answering in this case."
"Data quality and data cleaning are context dependent activities. Starting
from this observation, in previous work a context model for the assessment of
the quality of a database instance was proposed. In that framework, the context
takes the form of a possibly virtual database or data integration system into
which a database instance under quality assessment is mapped, for additional
analysis and processing, enabling quality assessment. In this work we extend
contexts with dimensions, and by doing so, we make possible a multidimensional
assessment of data quality assessment. Multidimensional contexts are
represented as ontologies written in Datalog+-. We use this language for
representing dimensional constraints, and dimensional rules, and also for doing
query answering based on dimensional navigation, which becomes an important
auxiliary activity in the assessment of data. We show ideas and mechanisms by
means of examples."
"The discovery, representation and reconstruction of Business Networks (BN)
from Network Mining (NM) raw data is a difficult problem for enterprises. This
is due to huge amounts of complex business processes within and across
enterprise boundaries, heterogeneous technology stacks, and fragmented data. To
remain competitive, visibility into the enterprise and partner networks on
different, interrelated abstraction levels is desirable. We present a novel
data discovery, mining and network inference system, called Business Network
System (BNS), that reconstructs the BN--integration and business process
networks--from raw data, hidden in the enterprises' landscapes. BNS provides a
new, declarative foundation for gathering information, defining a network
model, inferring the network and check its conformance to the real-world
""as-is"" network. The paper covers both the foundation and the key features of
BNS, including its underlying technologies, its overall system architecture,
and its most interesting capabilities."
"In recent years, stream data have become an immensely growing area of
research for the database, computer science and data mining communities. Stream
data is an ordered sequence of instances. In many applications of data stream
mining data can be read only once or a small number of times using limited
computing and storage capabilities. Some of the issues occurred in classifying
stream data that have significant impact in algorithm development are size of
database, online streaming, high dimensionality and concept drift. The concept
drift occurs when the properties of the historical data and target variable
change over time abruptly in such a case that the predictions will become
inaccurate as time passes. In this paper the framework of incremental
classification is proposed to solve the issues for the classification of stream
data. The Trie structure based incremental feature tree, Trie structure based
incremental FP (Frequent Pattern) growth tree and tree based incremental
classification algorithm are introduced in the proposed framework."
"In today world, organizations like Google, Yahoo, Amazon, Facebook etc. are
facing drastic increase in data. This leads to the problem of capturing,
storing, managing and analyzing terabytes or petabytes of data, stored in
multiple formats, from different internal and external sources. Moreover, new
applications scenarios like weather forecasting, trading, artificial
intelligence etc. need huge data processing in real time. These requirements
exceed the processing capacity of traditional on-disk database management
systems to manage this data and to give speedy real time results. Therefore,
data management needs new solutions for coping with the challenges of data
volumes and processing data in real-time. An in-memory database system (IMDS)
is a latest breed of database management system which is becoming answer to
above challenges with other supporting technologies. IMDS is capable to process
massive data distinctly faster. This paper explores IMDS approach and its
associated design issues and challenges. It also investigates some famous
commercial and open-source IMDS solutions available in the market."
"Spatial data mining or Knowledge discovery in spatial database is the
extraction of implicit knowledge, spatial relations and spatial patterns that
are not explicitly stored in databases. Co-location patterns discovery is the
process of finding the subsets of features that are frequently located together
in the same geographic area. In this paper, we discuss the different approaches
like Rule based approach, Join-less approach, Partial Join approach and
Constraint neighborhood based approach for finding co-location patterns."
"In the article, an experiment is aimed at clarifying the transfer efficiency
of the database in the cloud infrastructure. The system was added to the
control unit, which has guided the database search in the local part or in the
cloud. It is shown that the time data acquisition remains unchanged as a result
of modification. Suggestions have been made about the use of the theory of
dynamic systems to hybrid cloud database. The present work is aimed at
attracting the attention of spe-cialists in the field of cloud database to the
apparatus control theory. The experiment presented in this article allows the
use of the description of the known methods for solving important practical
problems."
"Location data becomes more and more important. In this paper, we focus on the
trajectory data, and propose a new framework, namely PRESS (Paralleled
Road-Network-Based Trajectory Compression), to effectively compress trajectory
data under road network constraints. Different from existing work, PRESS
proposes a novel representation for trajectories to separate the spatial
representation of a trajectory from the temporal representation, and proposes a
Hybrid Spatial Compression (HSC) algorithm and error Bounded Temporal
Compression (BTC) algorithm to compress the spatial and temporal information of
trajectories respectively. PRESS also supports common spatial-temporal queries
without fully decompressing the data. Through an extensive experimental study
on real trajectory dataset, PRESS significantly outperforms existing approaches
in terms of saving storage cost of trajectory data with bounded errors."
"As with the development of the IT technologies, the amount of accumulated
data is also increasing. Thus the role of data mining comes into picture.
Association rule mining becomes one of the significant responsibilities of
descriptive technique which can be defined as discovering meaningful patterns
from large collection of data. The frequent pattern mining algorithms determine
the frequent patterns from a database. Mining frequent itemset is very
fundamental part of association rule mining. Many algorithms have been proposed
from last many decades including majors are Apriori, Direct Hashing and
Pruning, FP-Growth, ECLAT etc. The aim of this study is to analyze the existing
techniques for mining frequent patterns and evaluate the performance of them by
comparing Apriori and DHP algorithms in terms of candidate generation, database
and transaction pruning. This creates a foundation to develop newer algorithm
for frequent pattern mining."
"Minimizing coordination, or blocking communication between concurrently
executing operations, is key to maximizing scalability, availability, and high
performance in database systems. However, uninhibited coordination-free
execution can compromise application correctness, or consistency. When is
coordination necessary for correctness? The classic use of serializable
transactions is sufficient to maintain correctness but is not necessary for all
applications, sacrificing potential scalability. In this paper, we develop a
formal framework, invariant confluence, that determines whether an application
requires coordination for correct execution. By operating on application-level
invariants over database states (e.g., integrity constraints), invariant
confluence analysis provides a necessary and sufficient condition for safe,
coordination-free execution. When programmers specify their application
invariants, this analysis allows databases to coordinate only when anomalies
that might violate invariants are possible. We analyze the invariant confluence
of common invariants and operations from real-world database systems (i.e.,
integrity constraints) and applications and show that many are invariant
confluent and therefore achievable without coordination. We apply these results
to a proof-of-concept coordination-avoiding database prototype and demonstrate
sizable performance gains compared to serializable execution, notably a 25-fold
improvement over prior TPC-C New-Order performance on a 200 server cluster."
"From social networks to language modeling, the growing scale and importance
of graph data has driven the development of numerous new graph-parallel systems
(e.g., Pregel, GraphLab). By restricting the computation that can be expressed
and introducing new techniques to partition and distribute the graph, these
systems can efficiently execute iterative graph algorithms orders of magnitude
faster than more general data-parallel systems. However, the same restrictions
that enable the performance gains also make it difficult to express many of the
important stages in a typical graph-analytics pipeline: constructing the graph,
modifying its structure, or expressing computation that spans multiple graphs.
As a consequence, existing graph analytics pipelines compose graph-parallel and
data-parallel systems using external storage systems, leading to extensive data
movement and complicated programming model.
  To address these challenges we introduce GraphX, a distributed graph
computation framework that unifies graph-parallel and data-parallel
computation. GraphX provides a small, core set of graph-parallel operators
expressive enough to implement the Pregel and PowerGraph abstractions, yet
simple enough to be cast in relational algebra. GraphX uses a collection of
query optimization techniques such as automatic join rewrites to efficiently
implement these graph-parallel operators. We evaluate GraphX on real-world
graphs and workloads and demonstrate that GraphX achieves comparable
performance as specialized graph computation systems, while outperforming them
in end-to-end graph pipelines. Moreover, GraphX achieves a balance between
expressiveness, performance, and ease of use."
"Materialized view is used in large data centric applications to expedite
query processing. The efficiency of materialized view depends on degree of
result found against the queries over the existing materialized views.
Materialized views are constructed following different methodologies. Thus the
efficacy of the materialized views depends on the methodology based on which
these are formed. Construction of materialized views are often time consuming
and moreover after a certain time the performance of the materialized views
degrade when the nature of queries change. In this situation either new
materialized views could be constructed from scratch or the existing views
could be upgraded. Fresh construction of materialized views has higher time
complexity hence the modification of the existing views is a better
solution.Modification process of materialized view is classified under
materialized view maintenance scheme. Materialized view maintenance is a
continuous process and the system could be tuned to ensure a constant rate of
performance. If a materialized view construction process is not supported by
materialized view maintenance scheme that system would suffer from performance
degradation. In this paper a new materialized view maintenance scheme is
proposed using markovs analysis to ensure consistent performance. Markovs
analysis is chosen here to predict steady state probability over initial
probability."
"The process of data mining produces various patterns from a given data
source. The most recognized data mining tasks are the process of discovering
frequent itemsets, frequent sequential patterns, frequent sequential rules and
frequent association rules. Numerous efficient algorithms have been proposed to
do the above processes. Frequent pattern mining has been a focused topic in
data mining research with a good number of references in literature and for
that reason an important progress has been made, varying from performant
algorithms for frequent itemset mining in transaction databases to complex
algorithms, such as sequential pattern mining, structured pattern mining,
correlation mining. Association Rule mining (ARM) is one of the utmost current
data mining techniques designed to group objects together from large databases
aiming to extract the interesting correlation and relation among huge amount of
data. In this article, we provide a brief review and analysis of the current
status of frequent pattern mining and discuss some promising research
directions. Additionally, this paper includes a comparative study between the
performance of the described approaches."
"Differential privacy is becoming a gold standard for privacy research; it
offers a guaranteed bound on loss of privacy due to release of query results,
even under worst-case assumptions. The theory of differential privacy is an
active research area, and there are now differentially private algorithms for a
wide range of interesting problems.
  However, the question of when differential privacy works in practice has
received relatively little attention. In particular, there is still no rigorous
method for choosing the key parameter $\epsilon$, which controls the crucial
tradeoff between the strength of the privacy guarantee and the accuracy of the
published results.
  In this paper, we examine the role that these parameters play in concrete
applications, identifying the key questions that must be addressed when
choosing specific values. This choice requires balancing the interests of two
different parties: the data analyst and the prospective participant, who must
decide whether to allow their data to be included in the analysis. We propose a
simple model that expresses this balance as formulas over a handful of
parameters, and we use our model to choose $\epsilon$ on a series of simple
statistical studies. We also explore a surprising insight: in some
circumstances, a differentially private study can be more accurate than a
non-private study for the same cost, under our model. Finally, we discuss the
simplifying assumptions in our model and outline a research agenda for possible
refinements."
"The performance of database/Web-service backed applications can be
significantly improved by asynchronous submission of queries/requests well
ahead of the point where the results are needed, so that results are likely to
have been fetched already when they are actually needed. However, manually
writing applications to exploit asynchronous query submission is tedious and
error-prone. In this paper we address the issue of automatically transforming a
program written assuming synchronous query submission, to one that exploits
asynchronous query submission. Our program transformation method is based on
data flow analysis and is framed as a set of transformation rules. Our rules
can handle query executions within loops, unlike some of the earlier work in
this area. We also present a novel approach that, at runtime, can combine
multiple asynchronous requests into batches, thereby achieving the benefits of
batching in addition to that of asynchronous submission. We have built a tool
that implements our transformation techniques on Java programs that use JDBC
calls; our tool can be extended to handle Web service calls. We have carried
out a detailed experimental study on several real-life applications, which
shows the effectiveness of the proposed rewrite techniques, both in terms of
their applicability and the performance gains achieved."
"Bitmap indexes are commonly used in databases and search engines. By
exploiting bit-level parallelism, they can significantly accelerate queries.
However, they can use much memory, and thus we might prefer compressed bitmap
indexes. Following Oracle's lead, bitmaps are often compressed using run-length
encoding (RLE). Building on prior work, we introduce the Roaring compressed
bitmap format: it uses packed arrays for compression instead of RLE. We compare
it to two high-performance RLE-based bitmap encoding techniques: WAH (Word
Aligned Hybrid compression scheme) and Concise (Compressed `n' Composable
Integer Set). On synthetic and real data, we find that Roaring bitmaps (1)
often compress significantly better (e.g., 2 times) and (2) are faster than the
compressed alternatives (up to 900 times faster for intersections). Our results
challenge the view that RLE-based bitmap compression is best."
"A $k$-nearest neighbor ($k$NN) query determines the $k$ nearest points, using
distance metrics, from a specific location. An all $k$-nearest neighbor
(A$k$NN) query constitutes a variation of a $k$NN query and retrieves the $k$
nearest points for each point inside a database. Their main usage resonates in
spatial databases and they consist the backbone of many location-based
applications and not only (i.e. $k$NN joins in databases, classification in
data mining). So, it is very crucial to develop methods that answer them
efficiently. In this work, we propose a novel method for classifying
multidimensional data using an A$k$NN algorithm in the MapReduce framework. Our
approach exploits space decomposition techniques for processing the
classification procedure in a parallel and distributed manner. To our
knowledge, we are the first to study the classification of multidimensional
objects under this perspective. Through an extensive experimental evaluation we
prove that our solution is efficient and scalable in processing the given
queries. We investigate many different perspectives that can affect the total
computational cost, such as different dataset distributions, number of
dimensions, growth of $k$ value and granularity of space decomposition and
prove that our system is efficient, robust and scalable."
"Privacy preservation emphasize on authorization of data, which signifies that
data should be accessed only by authorized users. Ensuring the privacy of data
is considered as one of the challenging task in data management. The
generalization of data with varying concept hierarchies seems to be interesting
solution. This paper proposes two stage prediction processes on privacy
preserved data. The privacy is preserved using generalization and betraying
other communicating parties by disguising generalized data which adds another
level of privacy. The generalization with betraying is performed in first stage
to define the knowledge or hypothesis and which is further optimized using
gradient descent method in second stage prediction for accurate prediction of
data. The experiment carried with both batch and stochastic gradient methods
and it is shown that bulk operation performed by batch takes long time and more
iterations than stochastic to give more accurate solution."
"In this paper we present a new family of Intensional RDBs (IRDBs) which
extends the traditional RDBs with the Big Data and flexible and 'Open schema'
features, able to preserve the user-defined relational database schemas and all
preexisting user's applications containing the SQL statements for a deployment
of such a relational data. The standard RDB data is parsed into an internal
vector key/value relation, so that we obtain a column representation of data
used in Big Data applications, covering the key/value and column-based Big Data
applications as well, into a unifying RDB framework. We define a query
rewriting algorithm, based on the GAV Data Integration methods, so that each
user-defined SQL query is rewritten into a SQL query over this vector relation,
and hence the user-defined standard RDB schema is maintained as an empty global
schema for the RDB schema modeling of data and as the SQL interface to stored
vector relation. Such an IRDB architecture is adequate for the massive
migrations from the existing slow RDBMSs into this new family of fast IRDBMSs
by offering a Big Data and new flexible schema features as well."
"We outline the approach being developed in the neuGRID project to use
provenance management techniques for the purposes of capturing and preserving
the provenance data that emerges in the specification and execution of
workflows in biomedical analyses. In the neuGRID project a provenance service
has been designed and implemented that is intended to capture, store, retrieve
and reconstruct the workflow information needed to facilitate users in
conducting user analyses. We describe the architecture of the neuGRID
provenance service and discuss how the CRISTAL system from CERN is being
adapted to address the requirements of the project and then consider how a
generalised approach for provenance management could emerge for more generic
application to the (Health)Grid community."
"We propose a new data structure, Parallel Adjacency Lists (PAL), for
efficiently managing graphs with billions of edges on disk. The PAL structure
is based on the graph storage model of GraphChi (Kyrola et. al., OSDI 2012),
but we extend it to enable online database features such as queries and fast
insertions. In addition, we extend the model with edge and vertex attributes.
Compared to previous data structures, PAL can store graphs more compactly while
allowing fast access to both the incoming and the outgoing edges of a vertex,
without duplicating data. Based on PAL, we design a graph database management
system, GraphChi-DB, which can also execute powerful analytical graph
computation.
  We evaluate our design experimentally and demonstrate that GraphChi-DB
achieves state-of-the-art performance on graphs that are much larger than the
available memory. GraphChi-DB enables anyone with just a laptop or a PC to work
with extremely large graphs."
"We study the problem of point-to-point distance querying for massive
scale-free graphs, which is important for numerous applications. Given a
directed or undirected graph, we propose to build an index for answering such
queries based on a hop-doubling labeling technique. We derive bounds on the
index size, the computation costs and I/O costs based on the properties of
unweighted scale-free graphs. We show that our method is much more efficient
compared to the state-of-the-art technique, in terms of both querying time and
indexing time. Our empirical study shows that our method can handle graphs that
are orders of magnitude larger than existing methods."
"Applications extracting data from crowdsourcing platforms must deal with the
uncertainty of crowd answers in two different ways: first, by deriving
estimates of the correct value from the answers; second, by choosing crowd
questions whose answers are expected to minimize this uncertainty relative to
the overall data collection goal. Such problems are already challenging when we
assume that questions are unrelated and answers are independent, but they are
even more complicated when we assume that the unknown values follow hard
structural constraints (such as monotonicity).
  In this vision paper, we examine how to formally address this issue with an
approach inspired by [Amsterdamer et al., 2013]. We describe a generalized
setting where we model constraints as linear inequalities, and use them to
guide the choice of crowd questions and the processing of answers. We present
the main challenges arising in this setting, and propose directions to solve
them."
"Datastores today rely on distribution and replication to achieve improved
performance and fault-tolerance. But correctness of many applications depends
on strong consistency properties - something that can impose substantial
overheads, since it requires coordinating the behavior of multiple nodes. This
paper describes a new approach to achieving strong consistency in distributed
systems while minimizing communication between nodes. The key insight is to
allow the state of the system to be inconsistent during execution, as long as
this inconsistency is bounded and does not affect transaction correctness. In
contrast to previous work, our approach uses program analysis to extract
semantic information about permissible levels of inconsistency and is fully
automated. We then employ a novel homeostasis protocol to allow sites to
operate independently, without communicating, as long as any inconsistency is
governed by appropriate treaties between the nodes. We discuss mechanisms for
optimizing treaties based on workload characteristics to minimize
communication, as well as a prototype implementation and experiments that
demonstrate the benefits of our approach on common transactional benchmarks."
"Many databases on the web are ""hidden"" behind (i.e., accessible only through)
their restrictive, form-like, search interfaces. Recent studies have shown that
it is possible to estimate aggregate query answers over such hidden web
databases by issuing a small number of carefully designed search queries
through the restrictive web interface. A problem with these existing work,
however, is that they all assume the underlying database to be static, while
most real-world web databases (e.g., Amazon, eBay) are frequently updated. In
this paper, we study the novel problem of estimating/tracking aggregates over
dynamic hidden web databases while adhering to the stringent query-cost
limitation they enforce (e.g., at most 1,000 search queries per day).
Theoretical analysis and extensive real-world experiments demonstrate the
effectiveness of our proposed algorithms and their superiority over baseline
solutions (e.g., the repeated execution of algorithms designed for static web
databases)."
"Fuzziness in databases is used to denote uncertain or incomplete data.
Relational Databases stress on the nature of the data to be certain. This
certainty based data is used as the basis of the normalization approach
designed for traditional relational databases. But real world data may not
always be certain, thereby making it necessary to design an approach for
normalization that deals with fuzzy data. This paper focuses on the approach
for designing the fifth normal form (5NF) based on join dependencies for fuzzy
data. The basis of join dependency for fuzzy relational databases is derived
from the basic relational database concepts. As join dependency implies an
multivalued dependency by symmetry the proof of join dependency based
normalization is stated from the perspective of multivalued dependency based
normalization on fuzzy relational databases."
"Moving Object Databases will have significant role in Geospatial Information
Systems as they allow users to model continuous movements of entities in the
databases and perform spatio-temporal analysis. For representing and querying
moving objects, and algebra with a comprehensive framework of User Defined
Types together with a set of functions on those types is needed. Moreover,
concerning real world applications, moving objects move along constrained
environments like transportation networks so that an extra algebra for modeling
networks is demanded, too. These algebras can be inserted in any data model if
their designs are based on available standards such as Open Geospatial
Consortium that provides a common model for existing DBMS's. In this paper, we
focus on extending a spatial data model for constrained moving objects. Static
and moving geometries in our model are based on Open Geospatial Consortium
standards. We also extend Structured Query Language for retrieving, querying,
and manipulating spatio-temporal data related to moving objects as a simple and
expressive query language. Finally as a proof of concept, we implement a
generator to generate data for moving objects constrained by a transportation
network. Such a generator primarily aims at traffic planning applications."
"There are several mining algorithms of association rules. One of the most
popular algorithms is Apriori that is used to extract frequent itemsets from
large database and getting the association rule for discovering the knowledge.
Based on this algorithm, this paper indicates the limitation of the original
Apriori algorithm of wasting time for scanning the whole database searching on
the frequent itemsets, and presents an improvement on Apriori by reducing that
wasted time depending on scanning only some transactions. The paper shows by
experimental results with several groups of transactions, and with several
values of minimum support that applied on the original Apriori and our
implemented improved Apriori that our improved Apriori reduces the time
consumed by 67.38% in comparison with the original Apriori, and makes the
Apriori algorithm more efficient and less time consuming."
"As computer clusters are found to be highly effective for handling massive
datasets, the design of efficient parallel algorithms for such a computing
model is of great interest. We consider ({\alpha}, k)-minimal algorithms for
such a purpose, where {\alpha} is the number of rounds in the algorithm, and k
is a bound on the deviation from perfect workload balance. We focus on new
({\alpha}, k)-minimal algorithms for sorting and skew equijoin operations for
computer clusters. To the best of our knowledge the proposed sorting and skew
join algorithms achieve the best workload balancing guarantee when compared to
previous works. Our empirical study shows that they are close to optimal in
workload balancing. In particular, our proposed sorting algorithm is around 25%
more efficient than the state-of-the-art Terasort algorithm and achieves
significantly more even workload distribution by over 50%."
"Transaction Repair is a method for lock-free, scalable transaction processing
that achieves full serializability. It demonstrates parallel speedup even in
inimical scenarios where all pairs of transactions have significant read-write
conflicts. In the transaction repair approach, each transaction runs in
complete isolation in a branch of the database; when conflicts occur, we detect
and repair them. These repairs are performed efficiently in parallel, and the
net effect is that of serial processing. Within transactions, we use no locks.
This frees users from the complications and performance hazards of locks, and
from the anomalies of sub-SERIALIZABLE isolation levels. Our approach builds on
an incrementalized variant of leapfrog triejoin, a worst-case optimal algorithm
for $\exists_1$ formulae, and on well-established techniques from programming
languages: declarative languages, purely functional data structures,
incremental computation, and fixpoint equations."
"Energy disaggregation is the process of estimating the energy consumed by
individual electrical appliances given only a time series of the whole-home
power demand. Energy disaggregation researchers require datasets of the power
demand from individual appliances and the whole-home power demand. Multiple
such datasets have been released over the last few years but provide metadata
in a disparate array of formats including CSV files and plain-text README
files. At best, the lack of a standard metadata schema makes it unnecessarily
time-consuming to write software to process multiple datasets and, at worse,
the lack of a standard means that crucial information is simply absent from
some datasets. We propose a metadata schema for representing appliances,
meters, buildings, datasets, prior knowledge about appliances and appliance
models. The schema is relational and provides a simple but powerful inheritance
mechanism."
"A new family of Intensional RDBs (IRDBs), introduced in [1], extends the
traditional RDBs with the Big Data and flexible and 'Open schema' features,
able to preserve the user-defined relational database schemas and all
preexisting user's applications containing the SQL statements for a deployment
of such a relational data. The standard RDB data is parsed into an internal
vector key/value relation, so that we obtain a column representation of data
used in Big Data applications, covering the key/value and column-based Big Data
applications as well, into a unifying RDB framework. Such an IRDB architecture
is adequate for the massive migrations from the existing slow RDBMSs into this
new family of fast IRDBMSs by offering a Big Data and new flexible schema
features as well. Here we present the interoperability features of the IRDBs by
permitting the queries also over the internal vector relations created by
parsing of each federated database in a given Multidatabase system. We show
that the SchemaLog with the second-order syntax and ad hoc Logic Programming
and its querying fragment can be embedded into the standard SQL IRDBMSs, so
that we obtain a full interoperabilty features of IRDBs by using only the
standard relational SQL for querying both data and meta-data."
"A novel fast algorithm for finding quasi identifiers in large datasets is
presented. Performance measurements on a broad range of datasets demonstrate
substantial reductions in run-time relative to the state of the art and the
scalability of the algorithm to realistically-sized datasets up to several
million records."
"Updates in RDF stores have recently been standardised in the SPARQL 1.1
Update specification. However, computing answers entailed by ontologies in
triple stores is usually treated orthogonal to updates. Even the W3C's recent
SPARQL 1.1 Update language and SPARQL 1.1 Entailment Regimes specifications
explicitly exclude a standard behaviour how SPARQL endpoints should treat
entailment regimes other than simple entailment in the context of updates. In
this paper, we take a first step to close this gap. We define a fragment of
SPARQL basic graph patterns corresponding to (the RDFS fragment of) DL-Lite and
the corresponding SPARQL update language, dealing with updates both of ABox and
of TBox statements. We discuss possible semantics along with potential
strategies for implementing them. We treat both, (i) materialised RDF stores,
which store all entailed triples explicitly, and (ii) reduced RDF Stores, that
is, redundancy-free RDF stores that do not store any RDF triples (corresponding
to DL-Lite ABox statements) entailed by others already."
"Scheduling query execution plans is a particularly complex problem in
shared-nothing parallel systems, where each site consists of a collection of
local time-shared (e.g., CPU(s) or disk(s)) and space-shared (e.g., memory)
resources and communicates with remote sites by message-passing. Earlier work
on parallel query scheduling employs either (a) one-dimensional models of
parallel task scheduling, effectively ignoring the potential benefits of
resource sharing, or (b) models of globally accessible resource units, which
are appropriate only for shared-memory architectures, since they cannot capture
the affinity of system resources to sites. In this paper, we develop a general
approach capturing the full complexity of scheduling distributed,
multi-dimensional resource units for all forms of parallelism within and across
queries and operators. We present a level-based list scheduling heuristic
algorithm for independent query tasks (i.e., physical operator pipelines) that
is provably near-optimal for given degrees of partitioned parallelism (with a
worst-case performance ratio that depends on the number of time-shared and
space-shared resources per site and the granularity of the clones). We also
propose extensions to handle blocking constraints in logical operator (e.g.,
hash-join) pipelines and bushy query plans as well as on-line task arrivals
(e.g., in a dynamic or multi-query execution environment). Experiments with our
scheduling algorithms implemented on top of a detailed simulation model verify
their effectiveness compared to existing approaches in a realistic setting.
Based on our analytical and experimental results, we revisit the open problem
of designing efficient cost models for parallel query optimization and propose
a solution that captures all the important parameters of parallel execution."
"We present a complex data handling system for the COMPASS tokamak, operated
by IPP ASCR Prague, Czech Republic [1]. The system, called CDB (Compass
DataBase), integrates different data sources as an assortment of data
acquisition hardware and software from different vendors is used. Based on
widely available open source technologies wherever possible, CDB is vendor and
platform independent and it can be easily scaled and distributed. The data is
directly stored and retrieved using a standard NAS (Network Attached Storage),
hence independent of the particular technology; the description of the data
(the metadata) is recorded in a relational database. Database structure is
general and enables the inclusion of multi-dimensional data signals in multiple
revisions (no data is overwritten). This design is inherently distributed as
the work is off-loaded to the clients. Both NAS and database can be implemented
and optimized for fast local access as well as secure remote access. CDB is
implemented in Python language; bindings for Java, C/C++, IDL and Matlab are
provided. Independent data acquisitions systems as well as nodes managed by
FireSignal [2] are all integrated using CDB. An automated data post-processing
server is a part of CDB. Based on dependency rules, the server executes, in
parallel if possible, prescribed post-processing tasks."
"Classical algorithms for query optimization presuppose the absence of
inconsistencies or uncertainties in the database and exploit only valid
semantic knowledge provided, e.g., by integrity constraints. Data inconsistency
or uncertainty, however, is a widespread critical issue in ordinary databases:
total integrity is often, in fact, an unrealistic assumption and violations to
integrity constraints may be introduced in several ways.
  In this report we present an approach for semantic query optimization that,
differently from the traditional ones, relies on not necessarily valid semantic
knowledge, e.g., provided by violated or soft integrity constraints, or induced
by applying data mining techniques. Query optimization that leverages invalid
semantic knowledge cannot guarantee the semantic equivalence between the
original user's query and its rewriting: thus a query optimized by our approach
yields approximate answers that can be provided to the users whenever fast but
possibly partial responses are required. Also, we evaluate the impact of use of
invalid semantic knowledge in the rewriting of a query by computing a measure
of the quality of the answer returned to the user, and we rely on the recent
theory of Belief Logic Programming to deal with the presence of possible
correlation in the semantic knowledge used in the rewriting."
"In the search for statistical dependency rules, a crucial task is to restrict
the search space by estimating upper bounds for the goodness of yet
undiscovered rules. In this paper, we show that all well-behaving goodness
measures achieve their maximal values in the same points. Therefore, the same
generic search strategy can be applied with any of these measures. The notion
of well-behaving measures is based on the classical axioms for any proper
goodness measures, and extended to negative dependencies, as well. As an
example, we show that several commonly used goodness measures are
well-behaving."
"An association rule is statistically significant, if it has a small
probability to occur by chance. It is well-known that the traditional
frequency-confidence framework does not produce statistically significant
rules. It can both accept spurious rules (type 1 error) and reject significant
rules (type 2 error). The same problem concerns other commonly used
interestingness measures and pruning heuristics.
  In this paper, we inspect the most common measure functions - frequency,
confidence, degree of dependence, $\chi^2$, correlation coefficient, and
$J$-measure - and redundancy reduction techniques. For each technique, we
analyze whether it can make type 1 or type 2 error and the conditions under
which the error occurs. In addition, we give new theoretical results which can
be use to guide the search for statistically significant association rules."
"In this paper we describe the support for data feed ingestion in AsterixDB,
an open-source Big Data Management System (BDMS) that provides a platform for
storage and analysis of large volumes of semi-structured data. Data feeds are a
mechanism for having continuous data arrive into a BDMS from external sources
and incrementally populate a persisted dataset and associated indexes. The need
to persist and index ""fast-flowing"" high-velocity data (and support ad hoc
analytical queries) is ubiquitous. However, the state of the art today involves
'gluing' together different systems. AsterixDB is different in being a unified
system with ""native support"" for data feed ingestion.
  We discuss the challenges and present the design and implementation of the
concepts involved in modeling and managing data feeds in AsterixDB. AsterixDB
allows the runtime behavior, allocation of resources and the offered degree of
robustness to be customized to suit the high-level application(s) that wish to
consume the ingested data. Initial experiments that evaluate scalability and
fault-tolerance of AsterixDB data feeds facility are reported."
"Privacy Preserving Data Mining(PPDM) is an ongoing research area aimed at
bridging the gap between the collaborative data mining and data confidentiality
There are many different approaches which have been adopted for PPDM, of them
the rule hiding approach is used in this article. This approach ensures output
privacy that prevent the mined patterns(itemsets) from malicious inference
problems. An efficient algorithm named as Pattern-based Maxcover Algorithm is
proposed with experimental results. This algorithm minimizes the dissimilarity
between the source and the released database; Moreover the patterns protected
cannot be retrieved from the released database by an adversary or counterpart
even with an arbitrarily low support threshold."
"Normalization is an important database design method, in the course of the
teaching of data modeling the understanding and applying of this method cause
problems for students the most. For improving the efficiency of learning
normalization we looked for alternative normalization methods and introduced
them into education. We made a survey among engineer students how efficient
could they execute the normalization with different methods. We executed
statistical and data mining examinations to decide whether any of the methods
resulted significantly better solutions."
"Ontological queries are evaluated against a knowledge base consisting of an
extensional database and an ontology (i.e., a set of logical assertions and
constraints which derive new intensional knowledge from the extensional
database), rather than directly on the extensional database. The evaluation and
optimization of such queries is an intriguing new problem for database
research. In this paper, we discuss two important aspects of this problem:
query rewriting and query optimization. Query rewriting consists of the
compilation of an ontological query into an equivalent first-order query
against the underlying extensional database. We present a novel query rewriting
algorithm for rather general types of ontological constraints which is
well-suited for practical implementations. In particular, we show how a
conjunctive query against a knowledge base, expressed using linear and sticky
existential rules, that is, members of the recently introduced Datalog+/-
family of ontology languages, can be compiled into a union of conjunctive
queries (UCQ) against the underlying database. Ontological query optimization,
in this context, attempts to improve this rewriting process so to produce
possibly small and cost-effective UCQ rewritings for an input query."
"NoSQL databases support semi-structured data, typically modeled as JSON. They
also provide limited (but expanding) query languages. Their idiomatic, non-SQL
language constructs, the many variations, and the lack of formal semantics
inhibit deep understanding of the query languages, and also impede progress
towards clean, powerful, declarative query languages.
  This paper specifies the syntax and semantics of SQL++, which is applicable
to both JSON native stores and SQL databases. The SQL++ semi-structured data
model is a superset of both JSON and the SQL data model. SQL++ offers powerful
computational capabilities for processing semi-structured data akin to prior
non-relational query languages, notably OQL and XQuery. Yet, SQL++ is SQL
backwards compatible and is generalized towards JSON by introducing only a
small number of query language extensions to SQL.
  Recognizing that a query language standard is probably premature for the fast
evolving area of NoSQL databases, SQL++ includes configuration options that
formally itemize the semantics variations that language designers may choose
from. The options often pertain to the treatment of semi-structuredness
(missing attributes, heterogeneous types, etc), where more than one sensible
approaches are possible.
  SQL++ is unifying: By appropriate choices of configuration options, the SQL++
semantics can morph into the semantics of existing semi-structured database
query languages. The extensive experimental validation shows how SQL and four
semi-structured database query languages (MongoDB, Cassandra CQL, Couchbase
N1QL and AsterixDB AQL) are formally described by appropriate settings of the
configuration options.
  Early adoption signs of SQL++ are positive: Version 4 of Couchbase's N1QL is
explained as syntactic sugar over SQL++. AsterixDB will soon support the full
SQL++ and Apache Drill is in the process of aligning with SQL++."
"In this work we establish and point out connections between the notion of
query-answer causality in databases and database repairs, model-based diagnosis
in its consistency-based and abductive versions, and database updates through
views. The mutual relationships among these areas of data management and
knowledge representation shed light on each of them and help to share notions
and results they have in common. In one way or another, these are all
approaches to uncertainty management, which becomes even more relevant in the
context of big data that have to be made sense of."
"Many repositories utilize the versatile RDF model to publish data.
Repositories are typically distributed and geographically remote, but data are
interconnected (e.g., the Semantic Web) and queried globally by a language such
as SPARQL. Due to the network cost and the nature of the queries, the execution
time can be prohibitively high. Current solutions attempt to minimize the
network cost by redistributing all data in a preprocessing phase, but here are
two drawbacks: (i) redistribution is based on heuristics that may not benefit
many of the future queries; and (ii) the preprocessing phase is very expensive
even for moderate size datasets. In this paper we propose PHD-Store, a SPARQL
engine for distributed RDF repositories. Our system does not assume any
particular initial data placement and does not require prepartitioning; hence,
it minimizes the startup cost. Initially, PHD-Store answers queries using a
potentially slow distributed semi-join algorithm, but adapts dynamically to the
query load by incrementally redistributing frequently accessed data.
Redistribution is done in a way that future queries can benefit from fast
hash-based parallel execution. Our experiments with synthetic and real data
verify that PHD-Store scales to very large datasets; many repositories;
converges to comparable or better quality of partitioning than existing
methods; and executes large query loads 1 to 2 orders of magnitude faster than
our competitors."
"The sheer scale of high-resolution raw data generated by simulation has
motivated non-conventional approaches for data exploration referred as
`immersive' and `in situ' query processing of the raw simulation data. Another
step towards supporting scientific progress is to enable data-driven hypothesis
management and predictive analytics out of simulation results. We present a
synthesis method and tool for encoding and managing competing hypotheses as
uncertain data in a probabilistic database that can be conditioned in the
presence of observations."
"In the recent years a lot of emphasis has been placed on two apparently
disjoined fields: data-parallel and eventually consistent distributed systems.
In this paper we propose a theoretical study over an eventually consistent
data-parallel computational model. The keystone is provided by the recent
finding that a class of programs exists which can be computed in an eventually
consistent, coordination-free way: monotonic programs. This principle is called
CALM and has been proven for distributed asynchronous settings. We make the
case that, using the techniques developed by Ameloot et al., CALM does not hold
in general for data-parallel systems, wherein computation usually proceeds
synchronously in rounds and where communication is reliable. We then show that
using novel techniques subsuming the one of Ameloot et al., the satisfiability
of the CALM principle is directly related with the assumptions imposed on the
behavior of the system."
"The relational DBMS (RDBMS) has been widely used since it supports various
high-level functionalities such as SQL, schemas, indexes, and transactions that
do not exist in the O/S file system. But, a recent advent of big data
technology facilitates development of new systems that sacrifice the DBMS
functionality in order to efficiently manage large-scale data. Those so-called
NoSQL systems use a distributed file system, which support scalability and
reliability. They support scalability of the system by storing data into a
large number of low-cost commodity hardware and support reliability by storing
the data in replica. However, they have a drawback that they do not adequately
support high-level DBMS functionality. In this paper, we propose an
architecture of a DBMS that uses the DFS as storage. With this novel
architecture, the DBMS is capable of supporting scalability and reliability of
the DFS as well as high-level functionality of DBMS. Thus, a DBMS can utilize a
virtually unlimited storage space provided by the DFS, rendering it to be
suitable for big data analytics. As part of the architecture of the DBMS, we
propose the notion of the meta DFS file, which allows the DBMS to use the DFS
as the storage, and an efficient transaction management method including
recovery and concurrency control. We implement this architecture in
Odysseus/DFS, an integration of the Odysseus relational DBMS, that has been
being developed at KAIST for over 24 years, with the DFS. Our experiments on
transaction processing show that, due to the high-level functionality of
Odysseus/DFS, it outperforms Hbase, which is a representative open-source NoSQL
system. We also show that, compared with an RDBMS with local storage, the
performance of Odysseus/DFS is comparable or marginally degraded, showing that
the overhead of Odysseus/DFS for supporting scalability by using the DFS as the
storage is not significant."
"One of the foundations of science is that researchers must publish the
methodology used to achieve their results so that others can attempt to
reproduce them. This has the added benefit of allowing methods to be adopted
and adapted for other purposes. In the field of e-Science, services -- often
choreographed through workflow, process data to generate results. The
reproduction of results is often not straightforward as the computational
objects may not be made available or may have been updated since the results
were generated. For example, services are often updated to fix bugs or improve
algorithms. This paper addresses these problems in three ways. Firstly, it
introduces a new framework to clarify the range of meanings of
""reproducibility"". Secondly, it describes a new algorithm, \PDIFF, that uses a
comparison of workflow provenance traces to determine whether an experiment has
been reproduced; the main innovation is that if this is not the case then the
specific point(s) of divergence are identified through graph analysis,
assisting any researcher wishing to understand those differences. One key
feature is support for user-defined, semantic data comparison operators.
Finally, the paper describes an implementation of \PDIFF that leverages the
power of the e-Science Central platform which enacts workflows in the cloud. As
well as automatically generating a provenance trace for consumption by \PDIFF,
the platform supports the storage and re-use of old versions of workflows, data
and services; the paper shows how this can be powerfully exploited in order to
achieve reproduction and re-use."
"This paper proposes a set of tools to help dealing with XML database
evolution. It aims at establishing a multi-system environment where a global
integrated system works in harmony with some local original ones, allowing data
translation in both directions and, thus, activities on both levels. To deal
with schemas, we propose an algorithm that computes a mapping capable of
obtaining a global schema which is a conservative extension of original local
schemas. The role of the obtained mapping is then twofold: it ensures schema
evolution, via composition and inversion, and it guides the construction of a
document translator, allowing automatic data adaptation w.r.t. type evolution.
This paper applies, extends and put together some of our previous
contributions."
"The management of technical documentation is an unavoidable activity
interesting for the enterprises. Indeed, the need to manage documents during
all the life cycle is an important issue. For that, the need to enhance the
ability of document management systems is an interesting challenge. Despite
existing systems on market (electronic document management systems), they are
considered as non-flexible systems which are based on data models preventing
any extension or improvement. In addition, those systems do not allow a slight
description of documents elements and propose an insufficient mechanisms for
both links and consistency management. LIRIS laboratory has developed research
in this area and proposed an active system, termed SAGED, whose objectives is
to manage link and consistency using active rules. However SAGED is based on an
approach that split rules (for consistency management) and documents
description. The main drawback is the rigidity of such approach which is
highlighted whenever documents are moved from one server to another or during
exchanges of documents. To contribute to solve this problem, we propose to
develop an approach aiming at improve the document management including
consistency. This approach is based on the introduction of rules with the XML
description of the documents [BoCP01]. In this context we proposed a
XML-oriented storage level allowing the storing of documents and rules
uniformly through a native XML database. We defined an intelligent system
termed SIGED according a client/server architecture built around an intelligent
component for active rules execution. These rules are extracted from XML
document, compiled and executed."
"Provenance metadata can be valuable in data sharing settings, where it can be
used to help data consumers form judgements regarding the reliability of the
data produced by third parties. However, some parts of provenance may be
sensitive, requiring access control, or they may need to be simplified for the
intended audience. Both these issues can be addressed by a single mechanism for
creating abstractions over provenance, coupled with a policy model to drive the
abstraction. Such mechanism, which we refer to as abstraction by grouping,
simultaneously achieves partial disclosure of provenance, and facilitates its
consumption. In this paper we introduce a formal foundation for this type of
abstraction, grounded in the W3C PROV model; describe the associated policy
model; and briefly present its implementation, the Provabs tool for interactive
experimentation with policies and abstractions."
"This paper introduces provGen, a generator aimed at producing large synthetic
provenance graphs with predictable properties and of arbitrary size. Synthetic
provenance graphs serve two main purposes. Firstly, they provide a variety of
controlled workloads that can be used to test storage and query capabilities of
provenance management systems at scale. Secondly, they provide challenging
testbeds for experimenting with graph algorithms for provenance analytics, an
area of increasing research interest. provGen produces PROV graphs and stores
them in a graph DBMS (Neo4J). A key feature is to let users control the
relationship makeup and topological features of the graph, by providing a seed
provenance pattern along with a set of constraints, expressed using a custom
Domain Specific Language. We also propose a simple method for evaluating the
quality of the generated graphs, by measuring how realistically they simulate
the structure of real-world patterns."
"This document defines extensions of the RDF data model and of the SPARQL
query language that capture an alternative approach to represent
statement-level metadata. While this alternative approach is backwards
compatible with RDF reification as defined by the RDF standard, the approach
aims to address usability and data management shortcomings of RDF reification.
One of the great advantages of the proposed approach is that it clarifies a
means to (i) understand sparse matrices, the property graph model, hypergraphs,
and other data structures with an emphasis on link attributes, (ii) map such
data onto RDF, and (iii) query such data using SPARQL. Further, the proposal
greatly expands both the freedom that database designers enjoy when creating
physical indexing schemes and query plans for graph data annotated with link
attributes and the interoperability of those database solutions."
"In this work, a new indexing technique of data streams called BSTree is
proposed. This technique uses the method of data discretization, SAX [4], to
reduce online the dimensionality of data streams. It draws on Btree to build
the index and finally uses an LRV (least Recently visited) pruning technique to
rid the index structure from data whose last visit time exceeds a threshold
value and thus minimizes response time for similarity search queries."
"Random samples are extensively used to summarize massive data sets and
facilitate scalable analytics. Coordinated sampling, where samples of different
data sets ""share"" the randomization, is a powerful method which facilitates
more accurate estimation of many aggregates and similarity measures. We
recently formulated a model of {\it Monotone Estimation Problems} (MEP), which
can be applied to coordinated sampling, projected on a single item. MEP
estimators can then be used to estimate sum aggregates, such as distances, over
coordinated samples. For MEP, we are interested in estimators that are unbiased
and nonnegative. We proposed {\it variance competitiveness} as a quality
measure of estimators: For each data vector, we consider the minimum variance
attainable on it by an unbiased and nonnegative estimator. We then define the
competitiveness of an estimator as the maximum ratio, over data, of the
expectation of the square to the minimum possible. We also presented a general
construction of the L$^*$ estimator, which is defined for any MEP for which a
nonnegative unbiased estimator exists, and is at most 4-competitive.
  Our aim here is to obtain tighter bounds on the {\em universal ratio}, which
we define to be the smallest competitive ratio that can be obtained for any
MEP. We obtain an upper bound of 3.375, improving over the bound of $4$ of the
L$^*$ estimator. We also establish a lower bound of 1.44. The lower bound is
obtained by constructing the {\it optimally competitive} estimator for
particular MEPs. The construction is of independent interest, as it facilitates
estimation with instance-optimal competitiveness."
"There is a fundamental discrepancy between the targeted and actual users of
current analytics frameworks. Most systems are designed for the data and
infrastructure of the Googles and Facebooks of the world---petabytes of data
distributed across large cloud deployments consisting of thousands of cheap
commodity machines. Yet, the vast majority of users operate clusters ranging
from a few to a few dozen nodes, analyze relatively small datasets of up to a
few terabytes, and perform primarily compute-intensive operations. Targeting
these users fundamentally changes the way we should build analytics systems.
  This paper describes the design of Tupleware, a new system specifically aimed
at the challenges faced by the typical user. Tupleware's architecture brings
together ideas from the database, compiler, and programming languages
communities to create a powerful end-to-end solution for data analysis. We
propose novel techniques that consider the data, computations, and hardware
together to achieve maximum performance on a case-by-case basis. Our
experimental evaluation quantifies the impact of our novel techniques and shows
orders of magnitude performance improvement over alternative systems."
"The weighted fuzzy c-mean clustering algorithm and weighted fuzzy
c-mean-adaptive cluster number are extension of traditional fuzzy c-mean
Algorithm to stream data clustering algorithm."
"Knowledge exploration from the large set of data,generated as a result of the
various data processing activities due to data mining only. Frequent Pattern
Mining is a very important undertaking in data mining. Apriori approach applied
to generate frequent item set generally espouse candidate generation and
pruning techniques for the satisfaction of the desired objective. This paper
shows how the different approaches achieve the objective of frequent mining
along with the complexities required to perform the job. This paper
demonstrates the use of WEKA tool for association rule mining using Apriori
algorithm."
"A data warehouse is a large data repository for the purpose of analysis and
decision making in organizations. To improve the query performance and to get
fast access to the data, data is stored as materialized views (MV) in the data
warehouse. When data at source gets updated, the materialized views also need
to be updated. In this paper, we focus on the problem of maintenance of these
materialized views and address the issue of finding such auxiliary views (AV)
that together with the materialized views make the data self-maintainable and
take minimal space. We propose an algorithm that uses key and referential
constraints which reduces the total number of tuples in auxiliary views and
uses idea of information sharing between these auxiliary views to further
reduce number of auxiliary views."
"AsterixDB is a new, full-function BDMS (Big Data Management System) with a
feature set that distinguishes it from other platforms in today's open source
Big Data ecosystem. Its features make it well-suited to applications like web
data warehousing, social data storage and analysis, and other use cases related
to Big Data. AsterixDB has a flexible NoSQL style data model; a query language
that supports a wide range of queries; a scalable runtime; partitioned,
LSM-based data storage and indexing (including B+-tree, R-tree, and text
indexes); support for external as well as natively stored data; a rich set of
built-in types; support for fuzzy, spatial, and temporal types and queries; a
built-in notion of data feeds for ingestion of data; and transaction support
akin to that of a NoSQL store.
  Development of AsterixDB began in 2009 and led to a mid-2013 initial open
source release. This paper is the first complete description of the resulting
open source AsterixDB system. Covered herein are the system's data model, its
query language, and its software architecture. Also included are a summary of
the current status of the project and a first glimpse into how AsterixDB
performs when compared to alternative technologies, including a parallel
relational DBMS, a popular NoSQL store, and a popular Hadoop-based SQL data
analytics platform, for things that both technologies can do. Also included is
a brief description of some initial trials that the system has undergone and
the lessons learned (and plans laid) based on those early ""customer""
engagements."
"There is a growing need for distributed graph processing systems that are
capable of gracefully scaling to very large graph datasets. Unfortunately, this
challenge has not been easily met due to the intense memory pressure imposed by
process-centric, message passing designs that many graph processing systems
follow. Pregelix is a new open source distributed graph processing system that
is based on an iterative dataflow design that is better tuned to handle both
in-memory and out-of-core workloads. As such, Pregelix offers improved
performance characteristics and scaling properties over current open source
systems (e.g., we have seen up to 15x speedup compared to Apache Giraph and up
to 35x speedup compared to distributed GraphLab), and makes more effective use
of available machine resources to support Big(ger) Graph Analytics."
"In an academic environment, student advising is considered a paramount
activity for both advisors and student to improve the academic performance of
students. In universities of large numbers of students, advising is a
time-consuming activity that may take a considerable effort of advisors and
university administration in guiding students to complete their registration
successfully and efficiently. Current systems are traditional and depend
greatly on the effort of the advisor to find the best selection of courses to
improve students performance. There is a need for a smart system that can
advise a large number of students every semester. In this paper, we propose a
smart system that uses association rule mining to help both students and
advisors in selecting and prioritizing courses. The system helps students to
improve their performance by suggesting courses that meet their current needs
and at the same time improve their academic performance. The system uses
association rule mining to find associations between courses that have been
registered by students in many previous semesters. The system successfully
generates a list of association rules that guide a particular student to select
courses registered by similar students."
"In this paper we take closer look at recent developments for the chase
procedure, and provide additional results. Our analysis allows us create a
taxonomy of the chase variations and the properties they satisfy. Two of the
most central problems regarding the chase is termination, and discovery of
restricted classes of sets of dependencies that guarantee termination of the
chase. The search for the restricted classes has been motivated by a fairly
recent result that shows that it is undecidable to determine whether the chase
with a given dependency set will terminate on a given instance. There is a
small dissonance here, since the quest has been for classes of sets of
dependencies guaranteeing termination of the chase on all instances, even
though the latter problem was not known to be undecidable. We resolve the
dissonance in this paper by showing that determining whether the chase with a
given set of dependencies terminates on all instances is coRE-complete. For the
hardness proof we use a reduction from word rewriting systems, thereby also
showing the close connection between the chase and word rewriting. The same
reduction also gives us the aforementioned instance-dependent RE-completeness
result as a byproduct. For one of the restricted classes guaranteeing
termination on all instances, the stratified sets dependencies, we provide new
complexity results for the problem of testing whether a given set of
dependencies belongs to it. These results rectify some previous claims that
have occurred in the literature."
"Data replication and deployment of local SPARQL endpoints improve scalability
and availability of public SPARQL endpoints, making the consumption of Linked
Data a reality. This solution requires synchronization and specific query
processing strategies to take advantage of replication. However, existing
replication aware techniques in federations of SPARQL endpoints do not consider
data dynamicity. We propose Fedra, an approach for querying federations of
endpoints that benefits from replication. Participants in Fedra federations can
copy fragments of data from several datasets, and describe them using
provenance and views. These descriptions enable Fedra to reduce the number of
selected endpoints while satisfying user divergence requirements. Experiments
on real-world datasets suggest savings of up to three orders of magnitude."
"Given a query graph that represents a pattern of interest, the emerging
pattern detection problem can be viewed as a continuous query problem on a
dynamic graph. We present an incremental algorithm for continuous query
processing on dynamic graphs. The algorithm is based on the concept of query
decomposition; we decompose a query graph into smaller subgraphs and assemble
the result of sub-queries to find complete matches with the specified query.
The novelty of our work lies in using the subgraph distributional statistics
collected from the dynamic graph to generate the decomposition. We introduce a
""Lazy Search"" algorithm where the search strategy is decided on a
vertex-to-vertex basis depending on the likelihood of a match in the vertex
neighborhood. We also propose a metric named ""Relative Selectivity"" that is
used to select between different query decomposition strategies. Our
experiments performed on real online news, network traffic stream and a
synthetic social network benchmark demonstrate 10-100x speedups over competing
approaches."
"Analyzing high dimensional data is a challenging task. For these data it is
known that traditional clustering algorithms fail to detect meaningful
patterns. As a solution, subspace clustering techniques have been introduced.
They analyze arbitrary subspace projections of the data to detect clustering
structures.
  In this paper, we present our subspace clustering extension for KDD
frameworks, termed KDD-SC. In contrast to existing subspace clustering
toolkits, our solution neither is a standalone product nor is it tightly
coupled to a specific KDD framework. Our extension is realized by a common
codebase and easy-to-use plugins for three of the most popular KDD frameworks,
namely KNIME, RapidMiner, and WEKA. KDD-SC extends these frameworks such that
they offer a wide range of different subspace clustering functionalities. It
provides a multitude of algorithms, data generators, evaluation measures, and
visualization techniques specifically designed for subspace clustering. These
functionalities integrate seamlessly with the frameworks' existing features
such that they can be flexibly combined. KDD-SC is publicly available on our
website."
"Streaming, big data applications face challenges in creating scalable data
flow pipelines, in which multiple data streams must be collected, stored,
queried, and analyzed. These data sources are characterized by their volume (in
terms of dataset size), velocity (in terms of data rates), and variety (in
terms of fields and types). For many applications, distributed NoSQL databases
are effective alternatives to traditional relational database management
systems. This paper considers a cyber situational awareness system that uses
the Apache Accumulo database to provide scalable data warehousing, real-time
data ingest, and responsive querying for human users and analytic algorithms.
We evaluate Accumulo's ingestion scalability as a function of number of client
processes and servers. We also describe a flexible data model with effective
techniques for query planning and query batching to deliver responsive results.
Query performance is evaluated in terms of latency of the client receiving
initial result sets. Accumulo performance is measured on a database of up to 8
nodes using real cyber data."
"An ERP is a kind of package which consist front end and backend as DBMS like
a collection of DBMSs. You can create DBMS to manage one aspect of your
business. For example, a publishing house has a database of books that keeps
information about books such as Author Name, Title, Translator Name, etc. But
this database app only helps enter books data and search them. It doesn't help
them, for example, sell books. They get or develop another DBMS database that
has all the Books data plus prices, discount formulas, names of common clients,
etc. Now they connect the Books database to Sales database and maybe also the
inventory database. Now its DBMS slowly turning into an ERP. They may add
payroll database and connect it to this ERP. They may develop sales staff and
commissions database and connect it to this ERP and so on. In the traditional
Database management system the different databases are used for the various
Campuses of the JSPM Group of Education like Wagholi Campus, Tathwade Campus,
Narhe Campus, Hadpsar Campuses, Bhavdhan Campus as well as Corporate office at
Katraj of same organization so it is not possible to keep different databases
for the same so in this paper proposed the use of Integrated Database for the
Entire organization using ERP system. The Proposed ERP system applied on the
existing Architecture of the JSPM Group; the marginal difference observed in
the Databases need to be accessed to generate the same number of Reports when
use the Traditional DBMS which end up with improvement in the Functional
efficiency of Organizational Architecture."
"This paper proposes a novel approach for efficiently evaluating regular path
queries over provenance graphs of workflows that may include recursion. The
approach assumes that an execution g of a workflow G is labeled with
query-agnostic reachability labels using an existing technique. At query time,
given g, G and a regular path query R, the approach decomposes R into a set of
subqueries R1, ..., Rk that are safe for G. For each safe subquery Ri, G is
rewritten so that, using the reachability labels of nodes in g, whether or not
there is a path which matches Ri between two nodes can be decided in constant
time. The results of each safe subquery are then composed, possibly with some
small unsafe remainder, to produce an answer to R. The approach results in an
algorithm that significantly reduces the number of subqueries k over existing
techniques by increasing their size and complexity, and that evaluates each
subquery in time bounded by its input and output size. Experimental results
demonstrate the benefit of this approach."
"While the Semantic Web currently can exhibit provenance information by using
the W3C PROV standards, there is a ""missing link"" in connecting PROV to storing
and querying for dynamic changes to RDF graphs using SPARQL. Solving this
problem would be required for such clear use-cases as the creation of version
control systems for RDF. While some provenance models and annotation techniques
for storing and querying provenance data originally developed with databases or
workflows in mind transfer readily to RDF and SPARQL, these techniques do not
readily adapt to describing changes in dynamic RDF datasets over time. In this
paper we explore how to adapt the dynamic copy-paste provenance model of
Buneman et al. [2] to RDF datasets that change over time in response to SPARQL
updates, how to represent the resulting provenance records themselves as RDF in
a manner compatible with W3C PROV, and how the provenance information can be
defined by reinterpreting SPARQL updates. The primary contribution of this
paper is a semantic framework that enables the semantics of SPARQL Update to be
used as the basis for a 'cut-and-paste' provenance model in a principled
manner."
"Bounded Derivation Depth property (BDD) and Finite Controllability (FC) are
two properties of sets of datalog rules and tuple generating dependencies
(known as Datalog +/- programs), which recently attracted some attention. We
conjecture that the first of these properties implies the second, and support
this conjecture by some evidence proving, among other results, that it holds
true for all theories over binary signature."
"The Web of Data encourages organizations and companies to publish their data
according to the Linked Data practices and offer SPARQL endpoints. On the other
hand, the dominant standard for information exchange is XML. The SPARQL2XQuery
Framework focuses on the automatic translation of SPARQL queries in XQuery
expressions in order to access XML data across the Web. In this paper, we
outline our ongoing work on supporting update queries in the RDF-XML
integration scenario."
"The purpose of data visualization is to offer intuitive ways for information
perception and manipulation, especially for non-expert users. The Web of Data
has realized the availability of a huge amount of datasets. However, the volume
and heterogeneity of available information make it difficult for humans to
manually explore and analyse large datasets. In this paper, we present
rdf:SynopsViz, a tool for hierarchical charting and visual exploration of
Linked Open Data (LOD). Hierarchical LOD exploration is based on the creation
of multiple levels of hierarchically related groups of resources based on the
values of one or more properties. The adopted hierarchical model provides
effective information abstraction and summarization. Also, it allows efficient
-on the fly- statistic computations, using aggregations over the hierarchy
levels."
"The challenges with respect to mining frequent items over data streaming
engaging variable window size and low memory space are addressed in this
research paper. To check the varying point of context change in streaming
transaction we have developed a window structure which will be in two levels
and supports in fixing the window size instantly and controls the
heterogeneities and assures homogeneities among transactions added to the
window. To minimize the memory utilization, computational cost and improve the
process scalability, this design will allow fixing the coverage or support at
window level. Here in this document, an incremental mining of frequent
item-sets from the window and a context variation analysis approach are being
introduced. The complete technology that we are presenting in this document is
named as Mining Frequent Item-sets using Variable Window Size fixed by Context
Variation Analysis (MFI-VWS-CVA). There are clear boundaries among frequent and
infrequent item-sets in specific item-sets. In this design we have used window
size change to represent the conceptual drift in an information stream. As it
were, whenever there is a problem in setting window size effectively the
item-set will be infrequent. The experiments that we have executed and
documented proved that the algorithm that we have designed is much efficient
than that of existing."
"How do RDF datasets currently get published on the Web? They are either
available as large RDF files, which need to be downloaded and processed
locally, or they exist behind complex SPARQL endpoints. By providing a RESTful
API that can access triple data, we allow users to query a dataset through a
simple interface based on just a couple of HTTP parameters. If RDF resources
were published this way we could quickly build applications that depend on
these datasets, without having to download and process them locally. This is
what Restpark is: a set of HTTP GET parameters that servers need to handle, and
respond with JSON-LD."
"The World Wide Web is the most wide known information source that is easily
available and searchable. It consists of billions of interconnected documents
Web pages are authored by millions of people. Accesses made by various users to
pages are recorded inside web logs. These log files exist in various formats.
Because of increase in usage of web, size of web log files is increasing at a
much faster rate. Web mining is application of data mining technique to these
log files. It can be of three types Web usage mining, Web structure mining and
Web content mining. Web Usage mining is mining of usage patterns of users which
can then be used to personalize web sites and create attractive web sites. It
consists of three main phases: Preprocessing, Pattern discovery and Pattern
analysis. In this paper we focus on Data cleaning and IP Address identification
stages of preprocessing. Methodology has been proposed for both the stages. At
the end conclusion is made about number of users left after IP address
identification."
"The ""crowd"" has become a very important geospatial data provider. Subsumed
under the term Volunteered Geographic Information (VGI), non-expert users have
been providing a wealth of quantitative geospatial data online. With spatial
reasoning being a basic form of human cognition, narratives expressing
geospatial experiences, e.g., travel blogs, would provide an even bigger source
of geospatial data. Textual narratives typically contain qualitative data in
the form of objects and spatial relationships. The scope of this work is (i) to
extract these relationships from user-generated texts, (ii) to quantify them
and (iii) to reason about object locations based only on this qualitative data.
We use information extraction methods to identify toponyms and spatial
relationships and to formulate a quantitative approach based on distance and
orientation features to represent the latter. Positional probability
distributions for spatial relationships are determined by means of a greedy
Expectation Maximization-based (EM) algorithm. These estimates are then used to
""triangulate"" the positions of unknown object locations. Experiments using a
text corpus harvested from travel blog sites establish the considerable
location estimation accuracy of the proposed approach."