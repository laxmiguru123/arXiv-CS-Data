summary
"This article is a sketch of ideas that were once intended to appear in the
author's famous series, ""The Art of Computer Programming"". He generalizes the
notion of a context-free language from a set to a multiset of words over an
alphabet. The idea is to keep track of the number of ways to parse a string.
For example, ""fruit flies like a banana"" can famously be parsed in two ways;
analogous examples in the setting of programming languages may yet be important
in the future.
  The treatment is informal but essentially rigorous."
"A perturbation technique can be used to simplify and sharpen A. C. Yao's
theorems about the behavior of shellsort with increments $(h,g,1)$. In
particular, when $h=\Theta(n^{7/15})$ and $g=\Theta(h^{1/5})$, the average
running time is $O(n^{23/15})$. The proof involves interesting properties of
the inversions in random permutations that have been $h$-sorted and $g$-sorted."
"Mallows and Riordan showed in 1968 that labeled trees with a small number of
inversions are related to labeled graphs that are connected and sparse. Wright
enumerated sparse connected graphs in 1977, and Kreweras related the inversions
of trees to the so-called ``parking problem'' in 1980. A~combination of these
three results leads to a surprisingly simple analysis of the behavior of
hashing by linear probing, including higher moments of the cost of successful
search."
"The classic all-terminal network reliability problem posits a graph, each of
whose edges fails independently with some given probability."
"We significantly improve known time bounds for solving the minimum cut
problem on undirected graphs. We use a ``semi-duality'' between minimum cuts
and maximum spanning tree packings combined with our previously developed
random sampling techniques. We give a randomized algorithm that finds a minimum
cut in an m-edge, n-vertex graph with high probability in O(m log^3 n) time. We
also give a simpler randomized algorithm that finds all minimum cuts with high
probability in O(n^2 log n) time. This variant has an optimal RNC
parallelization. Both variants improve on the previous best time bound of O(n^2
log^3 n). Other applications of the tree-packing approach are new, nearly tight
bounds on the number of near minimum cuts a graph may have and a new data
structure for representing them in a space-efficient manner."
"We consider the problem of coloring k-colorable graphs with the fewest
possible colors. We present a randomized polynomial time algorithm that colors
a 3-colorable graph on $n$ vertices with min O(Delta^{1/3} log^{1/2} Delta log
n), O(n^{1/4} log^{1/2} n) colors where Delta is the maximum degree of any
vertex. Besides giving the best known approximation ratio in terms of n, this
marks the first non-trivial approximation result as a function of the maximum
degree Delta. This result can be generalized to k-colorable graphs to obtain a
coloring using min O(Delta^{1-2/k} log^{1/2} Delta log n), O(n^{1-3/(k+1)}
log^{1/2} n) colors. Our results are inspired by the recent work of Goemans and
Williamson who used an algorithm for semidefinite optimization problems, which
generalize linear programs, to obtain improved approximations for the MAX CUT
and MAX 2-SAT problems. An intriguing outcome of our work is a duality
relationship established between the value of the optimum solution to our
semidefinite program and the Lovasz theta-function. We show lower bounds on the
gap between the optimum solution of our semidefinite program and the actual
chromatic number; by duality this also demonstrates interesting new facts about
the theta-function."
"We examine possibility to design an efficient solving algorithm for problems
of the class \np. It is introduced a classification of \np problems by the
property that a partial solution of size $k$ can be extended into a partial
solution of size $k+1$ in polynomial time. It is defined an unique class
problems to be worth to search an efficient solving algorithm. The problems,
which are outside of this class, are inherently exponential. We show that the
Hamiltonian cycle problem is inherently exponential."
"Tomography is the area of reconstructing objects from projections. Here we
wish to reconstruct a set of cells in a two dimensional grid, given the number
of cells in every row and column. The set is required to be an hv-convex
polyomino, that is all its cells must be connected and the cells in every row
and column must be consecutive. A simple, polynomial algorithm for
reconstructing hv-convex polyominoes is provided, which is several orders of
magnitudes faster than the best previously known algorithm from Barcucci et al.
In addition, the problem of reconstructing a special class of centered
hv-convex polyominoes is addressed. (An object is centered if it contains a row
whose length equals the total width of the object). It is shown that in this
case the reconstruction problem can be solved in linear time."
"We solve the subgraph isomorphism problem in planar graphs in linear time,
for any pattern of constant size. Our results are based on a technique of
partitioning the planar graph into pieces of small tree-width, and applying
dynamic programming within each piece. The same methods can be used to solve
other planar graph problems including connectivity, diameter, girth, induced
subgraph isomorphism, and shortest paths."
"We develop data structures for dynamic closest pair problems with arbitrary
distance functions, that do not necessarily come from any geometric structure
on the objects. Based on a technique previously used by the author for
Euclidean closest pairs, we show how to insert and delete objects from an
n-object set, maintaining the closest pair, in O(n log^2 n) time per update and
O(n) space. With quadratic space, we can instead use a quadtree-like structure
to achieve an optimal time bound, O(n) per update. We apply these data
structures to hierarchical clustering, greedy matching, and TSP heuristics, and
discuss other potential applications in machine learning, Groebner bases, and
local improvement algorithms for partition and placement problems. Experiments
show our new methods to be faster in practice than previously used heuristics."
"We discuss some aspects of approximating functions on high-dimensional data
sets with additive functions or ANOVA decompositions, that is, sums of
functions depending on fewer variables each. It is seen that under appropriate
smoothness conditions, the errors of the ANOVA decompositions are of order
$O(n^{m/2})$ for approximations using sums of functions of up to $m$ variables
under some mild restrictions on the (possibly dependent) predictor variables.
Several simulated examples illustrate this behaviour."
"We examine the Maximum Independent Set Problem in an undirected graph. The
main result is that this problem can be considered as the solving the same
problem in a subclass of the weighted normal twin-orthogonal graphs. The
problem is formulated which is dual to the problem above. It is shown that, for
trivial twin-orthogonal graphs, any of its maximal independent set is also
maximum one."
"We consider worst case time bounds for NP-complete problems including 3-SAT,
3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a
constraint satisfaction (CSP) formulation of these problems. 3-SAT is
equivalent to (2,3)-CSP while the other problems above are special cases of
(3,2)-CSP; there is also a natural duality transformation from (a,b)-CSP to
(b,a)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the
time bounds for solving the other problems listed above. Our techniques involve
a mixture of Davis-Putnam-style backtracking with more sophisticated matching
and network flow based ideas."
"We determine the asymptotical satisfiability probability of a random
at-most-k-Horn formula, via a probabilistic analysis of a simple version,
called PUR, of positive unit resolution. We show that for k=k(n)->oo the
problem can be ``reduced'' to the case k(n)=n, that was solved in
cs.DS/9912001. On the other hand, in the case k= a constant the behavior of PUR
is modeled by a simple queuing chain, leading to a closed-form solution when
k=2. Our analysis predicts an ``easy-hard-easy'' pattern in this latter case.
Under a rescaled parameter, the graphs of satisfaction probability
corresponding to finite values of k converge to the one for the uniform case, a
``dimension-dependent behavior'' similar to the one found experimentally by
Kirkpatrick and Selman (Science'94) for k-SAT. The phenomenon is qualitatively
explained by a threshold property for the number of iterations of PUR makes on
random satisfiable Horn formulas."
"In this paper we present a new data structure for double ended priority
queue, called min-max fine heap, which combines the techniques used in fine
heap and traditional min-max heap. The standard operations on this proposed
structure are also presented, and their analysis indicates that the new
structure outperforms the traditional one."
"We present two new algorithms for solving the {\em All Pairs Shortest Paths}
(APSP) problem for weighted directed graphs. Both algorithms use fast matrix
multiplication algorithms.
  The first algorithm solves the APSP problem for weighted directed graphs in
which the edge weights are integers of small absolute value in $\Ot(n^{2+\mu})$
time, where $\mu$ satisfies the equation $\omega(1,\mu,1)=1+2\mu$ and
$\omega(1,\mu,1)$ is the exponent of the multiplication of an $n\times n^\mu$
matrix by an $n^\mu \times n$ matrix. Currently, the best available bounds on
$\omega(1,\mu,1)$, obtained by Coppersmith, imply that $\mu<0.575$. The running
time of our algorithm is therefore $O(n^{2.575})$. Our algorithm improves on
the $\Ot(n^{(3+\omega)/2})$ time algorithm, where $\omega=\omega(1,1,1)<2.376$
is the usual exponent of matrix multiplication, obtained by Alon, Galil and
Margalit, whose running time is only known to be $O(n^{2.688})$.
  The second algorithm solves the APSP problem {\em almost} exactly for
directed graphs with {\em arbitrary} non-negative real weights. The algorithm
runs in $\Ot((n^\omega/\eps)\log (W/\eps))$ time, where $\eps>0$ is an error
parameter and W is the largest edge weight in the graph, after the edge weights
are scaled so that the smallest non-zero edge weight in the graph is 1. It
returns estimates of all the distances in the graph with a stretch of at most
$1+\eps$. Corresponding paths can also be found efficiently."
"We consider worst case time bounds for NP-complete problems including 3-SAT,
3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a
constraint satisfaction (CSP) formulation of these problems; 3-SAT is
equivalent to (2,3)-CSP while the other problems above are special cases of
(3,2)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the
time bounds for solving the other problems listed above. Our techniques involve
a mixture of Davis-Putnam-style backtracking with more sophisticated matching
and network flow based ideas."
"The author presents two tricks to accelerate depth-first search algorithms
for a class of combinatorial puzzle problems, such as tiling a tray by a fixed
set of polyominoes. The first trick is to implement each assumption of the
search with reversible local operations on doubly linked lists. By this trick,
every step of the search affects the data incrementally.
  The second trick is to add a ghost square that represents the identity of
each polyomino. Thus puts the rule that each polyomino be used once on the same
footing as the rule that each square be covered once. The coding simplifies to
a more abstract form which is equivalent to 0-1 integer programming. More
significantly for the total computation time, the search can naturally switch
between placing a fixed polyomino or covering a fixed square at different
stages, according to a combined heuristic.
  Finally the author reports excellent performance for his algorithm for some
familiar puzzles. These include tiling a hexagon by 19 hexiamonds and the N
queens problem for N up to 18."
"In this paper we present a random shuffling scheme to apply with adaptive
sorting algorithms. Adaptive sorting algorithms utilize the presortedness
present in a given sequence. We have probabilistically increased the amount of
presortedness present in a sequence by using a random shuffling technique that
requires little computation. Theoretical analysis suggests that the proposed
scheme can improve the performance of adaptive sorting. Experimental results
show that it significantly reduces the amount of disorder present in a given
sequence and improves the execution time of adaptive sorting algorithm as well."
"We introduce the smoothed analysis of algorithms, which is a hybrid of the
worst-case and average-case analysis of algorithms. In smoothed analysis, we
measure the maximum over inputs of the expected performance of an algorithm
under small random perturbations of that input. We measure this performance in
terms of both the input size and the magnitude of the perturbations. We show
that the simplex algorithm has polynomial smoothed complexity."
"In many applications, it is necessary to determine the string similarity.
Edit distance[WF74] approach is a classic method to determine Field Similarity.
A well known dynamic programming algorithm [GUS97] is used to calculate edit
distance with the time complexity O(nm). (for worst case, average case and even
best case) Instead of continuing with improving the edit distance approach,
[LL+99] adopted a brand new approach-token-based approach. Its new concept of
token-base-retain the original semantic information, good time complex-O(nm)
(for worst, average and best case) and good experimental performance make it a
milestone paper in this area. Further study indicates that there is still room
for improvement of its Field Similarity algorithm. Our paper is to introduce a
package of substring-based new algorithms to determine Field Similarity.
Combined together, our new algorithms not only achieve higher accuracy but also
gain the time complexity O(knm) (k<0.75) for worst case, O(*n) where <6 for
average case and O(1) for best case. Throughout the paper, we use the approach
of comparative examples to show higher accuracy of our algorithms compared to
the one proposed in [LL+99]. Theoretical analysis, concrete examples and
experimental result show that our algorithms can significantly improve the
accuracy and time complexity of the calculation of Field Similarity. [US97] D.
Guseld. Algorithms on Strings, Trees and Sequences, in Computer Science and
Computational Biology. [LL+99] Mong Li Lee, Cleansing data for mining and
warehousing, In Proceedings of the 10th International Conference on Database
and Expert Systems Applications (DEXA99), pages 751-760,August 1999. [WF74] R.
Wagner and M. Fisher, The String to String Correction Problem, JACM 21 pages
168-173, 1974."
"We study the problem of compressing massive tables within the
partition-training paradigm introduced by Buchsbaum et al. [SODA'00], in which
a table is partitioned by an off-line training procedure into disjoint
intervals of columns, each of which is compressed separately by a standard,
on-line compressor like gzip. We provide a new theory that unifies previous
experimental observations on partitioning and heuristic observations on column
permutation, all of which are used to improve compression rates. Based on the
theory, we devise the first on-line training algorithms for table compression,
which can be applied to individual files, not just continuously operating
sources; and also a new, off-line training algorithm, based on a link to the
asymmetric traveling salesman problem, which improves on prior work by
rearranging columns prior to partitioning. We demonstrate these results
experimentally. On various test files, the on-line algorithms provide 35-55%
improvement over gzip with negligible slowdown; the off-line reordering
provides up to 20% further improvement over partitioning alone. We also show
that a variation of the table compression problem is MAX-SNP hard."
"We show that several versions of Floyd and Rivest's algorithm Select for
finding the $k$th smallest of $n$ elements require at most
$n+\min\{k,n-k\}+o(n)$ comparisons on average and with high probability. This
rectifies the analysis of Floyd and Rivest, and extends it to the case of
nondistinct elements. Our computational results confirm that Select may be the
best algorithm in practice."
"Pattern-matching-based document-compression systems (e.g. for faxing) rely on
finding a small set of patterns that can be used to represent all of the ink in
the document. Finding an optimal set of patterns is NP-hard; previous
compression schemes have resorted to heuristics. This paper describes an
extension of the cross-entropy approach, used previously for measuring pattern
similarity, to this problem. This approach reduces the problem to a k-medians
problem, for which the paper gives a new algorithm with a provably good
performance guarantee. In comparison to previous heuristics (First Fit, with
and without generalized Lloyd's/k-means postprocessing steps), the new
algorithm generates a better codebook, resulting in an overall improvement in
compression performance of almost 17%."
"Mixed packing and covering problems are problems that can be formulated as
linear programs using only non-negative coefficients. Examples include
multicommodity network flow, the Held-Karp lower bound on TSP, fractional
relaxations of set cover, bin-packing, knapsack, scheduling problems,
minimum-weight triangulation, etc. This paper gives approximation algorithms
for the general class of problems. The sequential algorithm is a simple greedy
algorithm that can be implemented to find an epsilon-approximate solution in
O(epsilon^-2 log m) linear-time iterations. The parallel algorithm does
comparable work but finishes in polylogarithmic time.
  The results generalize previous work on pure packing and covering (the
special case when the constraints are all ""less-than"" or all ""greater-than"") by
Michael Luby and Noam Nisan (1993) and Naveen Garg and Jochen Konemann (1998)."
"We give a polynomial-time approximation scheme for the generalization of
Huffman Coding in which codeword letters have non-uniform costs (as in Morse
code, where the dash is twice as long as the dot). The algorithm computes a
(1+epsilon)-approximate solution in time O(n + f(epsilon) log^3 n), where n is
the input size."
"Describes a near-linear-time algorithm for a variant of Huffman coding, in
which the letters may have non-uniform lengths (as in Morse code), but with the
restriction that each word to be encoded has equal probability. [See also
``Huffman Coding with Unequal Letter Costs'' (2002).]"
"Falmagne recently introduced the concept of a medium, a combinatorial object
encompassing hyperplane arrangements, topological orderings, acyclic
orientations, and many other familiar structures. We find efficient solutions
for several algorithmic problems on media: finding short reset sequences,
shortest paths, testing whether a medium has a closed orientation, and listing
the states of a medium given a black-box description."
"We present algorithms that run in linear time on pointer machines for a
collection of problems, each of which either directly or indirectly requires
the evaluation of a function defined on paths in a tree. These problems
previously had linear-time algorithms but only for random-access machines
(RAMs); the best pointer-machine algorithms were super-linear by an
inverse-Ackermann-function factor. Our algorithms are also simpler, in some
cases substantially, than the previous linear-time RAM algorithms. Our
improvements come primarily from three new ideas: a refined analysis of path
compression that gives a linear bound if the compressions favor certain nodes,
a pointer-based radix sort as a replacement for table-based methods, and a more
careful partitioning of a tree into easily managed parts. Our algorithms
compute nearest common ancestors off-line, verify and construct minimum
spanning trees, do interval analysis on a flowgraph, find the dominators of a
flowgraph, and build the component tree of a weighted tree."
"Dealing with the NP-complete Dominating Set problem on undirected graphs, we
demonstrate the power of data reduction by preprocessing from a theoretical as
well as a practical side. In particular, we prove that Dominating Set
restricted to planar graphs has a so-called problem kernel of linear size,
achieved by two simple and easy to implement reduction rules. Moreover, having
implemented our reduction rules, first experiments indicate the impressive
practical potential of these rules. Thus, this work seems to open up a new and
prospective way how to cope with one of the most important problems in graph
theory and combinatorial optimization."
"We provide a data structure for maintaining an embedding of a graph on a
surface (represented combinatorially by a permutation of edges around each
vertex) and computing generators of the fundamental group of the surface, in
amortized time O(log n + log g(log log g)^3) per update on a surface of genus
g; we can also test orientability of the surface in the same time, and maintain
the minimum and maximum spanning tree of the graph in time O(log n + log^4 g)
per update. Our data structure allows edge insertion and deletion as well as
the dual operations; these operations may implicitly change the genus of the
embedding surface. We apply similar ideas to improve the constant factor in a
separator theorem for low-genus graphs, and to find in linear time a
tree-decomposition of low-genus low-diameter graphs."
"We study the problem of computing a preemptive schedule of equal-length jobs
with given release times, deadlines and weights. Our goal is to maximize the
weighted throughput, which is the total weight of completed jobs. In Graham's
notation this problem is described as (1 | r_j;p_j=p;pmtn | sum w_j U_j). We
provide an O(n^4)-time algorithm for this problem, improving the previous bound
of O(n^{10}) by Baptiste."
"We introduce exponential search trees as a novel technique for converting
static polynomial space search structures for ordered sets into fully-dynamic
linear space data structures.
  This leads to an optimal bound of O(sqrt(log n/loglog n)) for searching and
updating a dynamic set of n integer keys in linear space. Here searching an
integer y means finding the maximum key in the set which is smaller than or
equal to y. This problem is equivalent to the standard text book problem of
maintaining an ordered set (see, e.g., Cormen, Leiserson, Rivest, and Stein:
Introduction to Algorithms, 2nd ed., MIT Press, 2001).
  The best previous deterministic linear space bound was O(log n/loglog n) due
Fredman and Willard from STOC 1990. No better deterministic search bound was
known using polynomial space.
  We also get the following worst-case linear space trade-offs between the
number n, the word length w, and the maximal key U < 2^w: O(min{loglog n+log
n/log w, (loglog n)(loglog U)/(logloglog U)}). These trade-offs are, however,
not likely to be optimal.
  Our results are generalized to finger searching and string searching,
providing optimal results for both in terms of n."
"In this paper we present a theoretical analysis of the deterministic on-line
{\em Sum of Squares} algorithm ($SS$) for bin packing introduced and studied
experimentally in \cite{CJK99}, along with several new variants. $SS$ is
applicable to any instance of bin packing in which the bin capacity $B$ and
item sizes $s(a)$ are integral (or can be scaled to be so), and runs in time
$O(nB)$. It performs remarkably well from an average case point of view: For
any discrete distribution in which the optimal expected waste is sublinear,
$SS$ also has sublinear expected waste. For any discrete distribution where the
optimal expected waste is bounded, $SS$ has expected waste at most $O(\log n)$.
In addition, we discuss several interesting variants on $SS$, including a
randomized $O(nB\log B)$-time on-line algorithm $SS^*$, based on $SS$, whose
expected behavior is essentially optimal for all discrete distributions.
Algorithm $SS^*$ also depends on a new linear-programming-based
pseudopolynomial-time algorithm for solving the NP-hard problem of determining,
given a discrete distribution $F$, just what is the growth rate for the optimal
expected waste. This article is a greatly expanded version of the conference
paper \cite{sumsq2000}."
"This paper shows that a simple algorithm produces the {\em
all-prefixes-LCSs-graph} in $O(mn)$ time for two input sequences of size $m$
and $n$. Given any prefix $p$ of the first input sequence and any prefix $q$ of
the second input sequence, all longest common subsequences (LCSs) of $p$ and
$q$ can be generated in time proportional to the output size, once the
all-prefixes-LCSs-graph has been constructed. The problem can be solved in the
context of generating all the distinct character strings that represent an LCS
or in the context of generating all ways of embedding an LCS in the two input
strings."
"The number of the non-shared edges of two phylogenies is a basic measure of
the dissimilarity between the phylogenies. The non-shared edges are also the
building block for approximating a more sophisticated metric called the nearest
neighbor interchange (NNI) distance. In this paper, we give the first
subquadratic-time algorithm for finding the non-shared edges, which are then
used to speed up the existing approximating algorithm for the NNI distance from
$O(n^2)$ time to $O(n \log n)$ time. Another popular distance metric for
phylogenies is the subtree transfer (STT) distance. Previous work on computing
the STT distance considered degree-3 trees only. We give an approximation
algorithm for the STT distance for degree-$d$ trees with arbitrary $d$ and with
generalized STT operations."
"We consider the problem of laying out a tree with fixed parent/child
structure in hierarchical memory. The goal is to minimize the expected number
of block transfers performed during a search along a root-to-leaf path, subject
to a given probability distribution on the leaves. This problem was previously
considered by Gil and Itai, who developed optimal but slow algorithms when the
block-transfer size B is known. We present faster but approximate algorithms
for the same problem; the fastest such algorithm runs in linear time and
produces a solution that is within an additive constant of optimal.
  In addition, we show how to extend any approximately optimal algorithm to the
cache-oblivious setting in which the block-transfer size is unknown to the
algorithm. The query performance of the cache-oblivious layout is within a
constant factor of the query performance of the optimal known-block-size
layout. Computing the cache-oblivious layout requires only logarithmically many
calls to the layout algorithm for known block size; in particular, the
cache-oblivious layout can be computed in O(N lg N) time, where N is the number
of nodes.
  Finally, we analyze two greedy strategies, and show that they have a
performance ratio between Omega(lg B / lg lg B) and O(lg B) when compared to
the optimal layout."
"We suggest a variation of the Hellerstein--Koutsoupias--Papadimitriou
indexability model for datasets equipped with a similarity measure, with the
aim of better understanding the structure of indexing schemes for
similarity-based search and the geometry of similarity workloads. This in
particular provides a unified approach to a great variety of schemes used to
index into metric spaces and facilitates their transfer to more general
similarity measures such as quasi-metrics. We discuss links between performance
of indexing schemes and high-dimensional geometry. The concepts and results are
illustrated on a very large concrete dataset of peptide fragments equipped with
a biologically significant similarity measure."
"We consider geometric instances of the Maximum Weighted Matching Problem
(MWMP) and the Maximum Traveling Salesman Problem (MTSP) with up to 3,000,000
vertices. Making use of a geometric duality relationship between MWMP, MTSP,
and the Fermat-Weber-Problem (FWP), we develop a heuristic approach that yields
in near-linear time solutions as well as upper bounds. Using various
computational tools, we get solutions within considerably less than 1% of the
optimum.
  An interesting feature of our approach is that, even though an FWP is hard to
compute in theory and Edmonds' algorithm for maximum weighted matching yields a
polynomial solution for the MWMP, the practical behavior is just the opposite,
and we can solve the FWP with high accuracy in order to find a good heuristic
solution for the MWMP."
"We perform a smoothed analysis of the termination phase of an interior-point
method. By combining this analysis with the smoothed analysis of Renegar's
interior-point algorithm by Dunagan, Spielman and Teng, we show that the
smoothed complexity of an interior-point algorithm for linear programming is $O
(m^{3} \log (m/\sigma))$. In contrast, the best known bound on the worst-case
complexity of linear programming is $O (m^{3} L)$, where $L$ could be as large
as $m$. We include an introduction to smoothed analysis and a tutorial on proof
techniques that have been useful in smoothed analyses."
"In this paper we propose a simple and efficient data structure yielding a
perfect hashing of quite general arrays. The data structure is named phorma,
which is an acronym for perfectly hashable order restricted multidimensional
array.
  Keywords: Perfect hash function, Digraph, Implicit enumeration,
Nijenhuis-Wilf combinatorial family."
"We show how to find a Hamiltonian cycle in a graph of degree at most three
with n vertices, in time O(2^{n/3}) ~= 1.260^n and linear space. Our algorithm
can find the minimum weight Hamiltonian cycle (traveling salesman problem), in
the same time bound. We can also count or list all Hamiltonian cycles in a
degree three graph in time O(2^{3n/8}) ~= 1.297^n. We also solve the traveling
salesman problem in graphs of degree at most four, by randomized and
deterministic algorithms with runtime O((27/4)^{n/3}) ~= 1.890^n and
O((27/4+epsilon)^{n/3}) respectively. Our algorithms allow the input to specify
a set of forced edges which must be part of any generated cycle. Our cycle
listing algorithm shows that every degree three graph has O(2^{3n/8})
Hamiltonian cycles; we also exhibit a family of graphs with 2^{n/3} Hamiltonian
cycles per graph."
"We present the first explicit connection between quantum computation and
lattice problems. Namely, we show a solution to the Unique Shortest Vector
Problem (SVP) under the assumption that there exists an algorithm that solves
the hidden subgroup problem on the dihedral group by coset sampling. Moreover,
we solve the hidden subgroup problem on the dihedral group by using an average
case subset sum routine. By combining the two results, we get a quantum
reduction from $\Theta(n^{2.5})$-unique-SVP to the average case subset sum
problem."
"We propose a very simple randomised data structure that stores an
approximation from above of a lattice-valued function. Computing the function
value requires a constant number of steps, and the error probability can be
balanced with space usage, much like in Bloom filters. The structure is
particularly well suited for functions that are bottom on most of their domain.
We then show how to use our methods to store in a compact way the bad-character
shift function for variants of the Boyer-Moore text search algorithms. As a
result, we obtain practical implementations of these algorithms that can be
used with large alphabets, such as Unicode collation elements, with a small
setup time. The ideas described in this paper have been implemented as free
software under the GNU General Public License within the MG4J project
(http://mg4j.dsi.unimi.it/)."
"We show how to support efficient back traversal in a unidirectional list,
using small memory and with essentially no slowdown in forward steps. Using
$O(\log n)$ memory for a list of size $n$, the $i$'th back-step from the
farthest point reached so far takes $O(\log i)$ time in the worst case, while
the overhead per forward step is at most $\epsilon$ for arbitrary small
constant $\epsilon>0$. An arbitrary sequence of forward and back steps is
allowed. A full trade-off between memory usage and time per back-step is
presented: $k$ vs. $kn^{1/k}$ and vice versa. Our algorithms are based on a
novel pebbling technique which moves pebbles on a virtual binary, or $t$-ary,
tree that can only be traversed in a pre-order fashion. The compact data
structures used by the pebbling algorithms, called list traversal synopses,
extend to general directed graphs, and have other interesting applications,
including memory efficient hash-chain implementation. Perhaps the most
surprising application is in showing that for any program, arbitrary rollback
steps can be efficiently supported with small overhead in memory, and marginal
overhead in its ordinary execution. More concretely: Let $P$ be a program that
runs for at most $T$ steps, using memory of size $M$. Then, at the cost of
recording the input used by the program, and increasing the memory by a factor
of $O(\log T)$ to $O(M \log T)$, the program $P$ can be extended to support an
arbitrary sequence of forward execution and rollback steps: the $i$'th rollback
step takes $O(\log i)$ time in the worst case, while forward steps take O(1)
time in the worst case, and $1+\epsilon$ amortized time per step."
"A maximum weighted matching for bipartite graphs $G=(A \cup B,E)$ can be
found by using the algorithm of Edmonds and Karp with a Fibonacci Heap and a
modified Dijkstra in $O(nm + n^2 \log{n})$ time where n is the number of nodes
and m the number of edges. For the case that $|A|=|B|$ the number of edges is
$n^2$ and therefore the complexity is $O(n^3)$. In this paper we want to
present a simple heuristic method to reduce the number of edges of complete
bipartite graphs $G=(A \cup B,E)$ with $|A|=|B|$ such that $m = n\log{n}$ and
therefore the complexity of such that $m = n\log{n}$ and therefore the
complexity of $O(n^2 \log{n})$. The weights of all edges in G must be uniformly
distributed in [0,1]."
"We consider algorithms for preprocessing labelled lists and trees so that,
for any two nodes u and v we can answer queries of the form: What is the mode
or median label in the sequence of labels on the path from u to v."
"The Lovasz Local Lemma due to Erdos and Lovasz is a powerful tool in proving
the existence of rare events. We present an extension of this lemma, which
works well when the event to be shown to exist is a conjunction of individual
events, each of which asserts that a random variable does not deviate much from
its mean. As applications, we consider two classes of NP-hard integer programs:
minimax and covering integer programs. A key technique, randomized rounding of
linear relaxations, was developed by Raghavan and Thompson to derive good
approximation algorithms for such problems. We use our extension of the Local
Lemma to prove that randomized rounding produces, with non-zero probability,
much better feasible solutions than known before, if the constraint matrices of
these integer programs are column-sparse (e.g., routing using short paths,
problems on hypergraphs with small dimension/degree). This complements certain
well-known results from discrepancy theory. We also generalize the method of
pessimistic estimators due to Raghavan, to obtain constructive (algorithmic)
versions of our results for covering integer programs."
"In this paper we present a discrete data structure for reservations of
limited resources. A reservation is defined as a tuple consisting of the time
interval of when the resource should be reserved, $I_R$, and the amount of the
resource that is reserved, $B_R$, formally $R=\{I_R,B_R\}$.
  The data structure is similar to a segment tree. The maximum spanning
interval of the data structure is fixed and defined in advance. The granularity
and thereby the size of the intervals of the leaves is also defined in advance.
The data structure is built only once. Neither nodes nor leaves are ever
inserted, deleted or moved. Hence, the running time of the operations does not
depend on the number of reservations previously made. The running time does not
depend on the size of the interval of the reservation either. Let $n$ be the
number of leaves in the data structure. In the worst case, the number of
touched (i.e. traversed) nodes is in any operation $O(\log n)$, hence the
running time of any operation is also $O(\log n)$."
"We introduce a novel definition of approximate palindromes in strings, and
provide an algorithm to find all maximal approximate palindromes in a string
with up to $k$ errors. Our definition is based on the usual edit operations of
approximate pattern matching, and the algorithm we give, for a string of size
$n$ on a fixed alphabet, runs in $O(k^2 n)$ time. We also discuss two
implementation-related improvements to the algorithm, and demonstrate their
efficacy in practice by means of both experiments and an average-case analysis."
"We introduce top trees as a design of a new simpler interface for data
structures maintaining information in a fully-dynamic forest. We demonstrate
how easy and versatile they are to use on a host of different applications. For
example, we show how to maintain the diameter, center, and median of each tree
in the forest. The forest can be updated by insertion and deletion of edges and
by changes to vertex and edge weights. Each update is supported in O(log n)
time, where n is the size of the tree(s) involved in the update. Also, we show
how to support nearest common ancestor queries and level ancestor queries with
respect to arbitrary roots in O(log n) time. Finally, with marked and unmarked
vertices, we show how to compute distances to a nearest marked vertex. The
later has applications to approximate nearest marked vertex in general graphs,
and thereby to static optimization problems over shortest path metrics.
  Technically speaking, top trees are easily implemented either with
Frederickson's topology trees [Ambivalent Data Structures for Dynamic
2-Edge-Connectivity and k Smallest Spanning Trees, SIAM J. Comput. 26 (2) pp.
484-538, 1997] or with Sleator and Tarjan's dynamic trees [A Data Structure for
Dynamic Trees. J. Comput. Syst. Sc. 26 (3) pp. 362-391, 1983]. However, we
claim that the interface is simpler for many applications, and indeed our new
bounds are quadratic improvements over previous bounds where they exist."
"Wireless sensor networks (WSNs) are emerging as an effective means for
environment monitoring. This paper investigates a strategy for energy efficient
monitoring in WSNs that partitions the sensors into covers, and then activates
the covers iteratively in a round-robin fashion. This approach takes advantage
of the overlap created when many sensors monitor a single area. Our work builds
upon previous work in ""Power Efficient Organization of Wireless Sensor
Networks"" by Slijepcevic and Potkonjak, where the model is first formulated. We
have designed three approximation algorithms for a variation of the SET K-COVER
problem, where the objective is to partition the sensors into covers such that
the number of covers that include an area, summed over all areas, is maximized.
The first algorithm is randomized and partitions the sensors, in expectation,
within a fraction 1 - 1/e (~.63) of the optimum. We present two other
deterministic approximation algorithms. One is a distributed greedy algorithm
with a 1/2 approximation ratio and the other is a centralized greedy algorithm
with a 1 - 1/e approximation ratio. We show that it is NP-Complete to guarantee
better than 15/16 of the optimal coverage, indicating that all three algorithms
perform well with respect to the best approximation algorithm possible.
Simulations indicate that in practice, the deterministic algorithms perform far
above their worst case bounds, consistently covering more than 72% of what is
covered by an optimum solution. Simulations also indicate that the increase in
longevity is proportional to the amount of overlap amongst the sensors. The
algorithms are fast, easy to use, and according to simulations, significantly
increase the longevity of sensor networks. The randomized algorithm in
particular seems quite practical."
"We introduce several modifications of the partitioning schemes used in
Hoare's quicksort and quickselect algorithms, including ternary schemes which
identify keys less or greater than the pivot. We give estimates for the numbers
of swaps made by each scheme. Our computational experiments indicate that
ternary schemes allow quickselect to identify all keys equal to the selected
key at little additional cost."
"We show that several versions of Floyd and Rivest's algorithm Select for
finding the $k$th smallest of $n$ elements require at most
$n+\min\{k,n-k\}+o(n)$ comparisons on average and with high probability. This
rectifies the analysis of Floyd and Rivest, and extends it to the case of
nondistinct elements. Our computational results confirm that Select may be the
best algorithm in practice."
"We show that several versions of Floyd and Rivest's algorithm Select [Comm.\
ACM {\bf 18} (1975) 173] for finding the $k$th smallest of $n$ elements require
at most $n+\min\{k,n-k\}+o(n)$ comparisons on average, even when equal elements
occur. This parallels our recent analysis of another variant due to Floyd and
Rivest [Comm. ACM {\bf 18} (1975) 165--172]. Our computational results suggest
that both variants perform well in practice, and may compete with other
selection methods, such as Hoare's Find or quickselect with median-of-3 pivots."
"We show that several versions of Floyd and Rivest's improved algorithm Select
for finding the $k$th smallest of $n$ elements require at most
$n+\min\{k,n-k\}+O(n^{1/2}\ln^{1/2}n)$ comparisons on average and with high
probability. This rectifies the analysis of Floyd and Rivest, and extends it to
the case of nondistinct elements. Encouraging computational results on large
median-finding problems are reported."
"An optimization problem that naturally arises in the study of swarm robotics
is the Freeze-Tag Problem (FTP) of how to awaken a set of ``asleep'' robots, by
having an awakened robot move to their locations. Once a robot is awake, it can
assist in awakening other slumbering robots.The objective is to have all robots
awake as early as possible. While the FTP bears some resemblance to problems
from areas in combinatorial optimization such as routing, broadcasting,
scheduling, and covering, its algorithmic characteristics are surprisingly
different. We consider both scenarios on graphs and in geometric
environments.In graphs, robots sleep at vertices and there is a length function
on the edges. Awake robots travel along edges, with time depending on edge
length. For most scenarios, we consider the offline version of the problem, in
which each awake robot knows the position of all other robots. We prove that
the problem is NP-hard, even for the special case of star graphs. We also
establish hardness of approximation, showing that it is NP-hard to obtain an
approximation factor better than 5/3, even for graphs of bounded degree.These
lower bounds are complemented with several positive algorithmic results,
including: (1) We show that the natural greedy strategy on star graphs has a
tight worst-case performance of 7/3 and give a polynomial-time approximation
scheme (PTAS) for star graphs. (2) We give a simple O(log D)-competitive online
algorithm for graphs with maximum degree D and locally bounded edge weights.
(3) We give a PTAS, running in nearly linear time, for geometrically embedded
instances."
"We generalize univariate multipoint evaluation of polynomials of degree n at
sublinear amortized cost per point. More precisely, it is shown how to evaluate
a bivariate polynomial p of maximum degree less than n, specified by its n^2
coefficients, simultaneously at n^2 given points using a total of O(n^{2.667})
arithmetic operations. In terms of the input size N being quadratic in n, this
amounts to an amortized cost of O(N^{0.334}) per point."
"In this paper, we present a probabilistic self-balancing dictionary data
structure for massive data sets, and prove expected amortized I/O-optimal
bounds on the dictionary operations. We show how to use the structure as an
I/O-optimal priority queue. The data structure, which we call as the random
buffer tree, abstracts the properties of the random treap and the buffer tree
and has the same expected I/O-bounds as the buffer tree."
"We study an interesting family of cooperating coroutines, which is able to
generate all patterns of bits that satisfy certain fairly general ordering
constraints, changing only one bit at a time. (More precisely, the directed
graph of constraints is required to be cycle-free when it is regarded as an
undirected graph.) If the coroutines are implemented carefully, they yield an
algorithm that needs only a bounded amount of computation per bit change,
thereby solving an open problem in the field of combinatorial pattern
generation."
"The maximum intersection problem for a matroid and a greedoid, given by
polynomial-time oracles, is shown $NP$-hard by expressing the satisfiability of
boolean formulas in 3-conjunctive normal form as such an intersection. The
corresponding approximation problems are shown $NP$-hard for certain
approximation performance bounds. Moreover, some natural parameterized variants
of the problem are shown $W[P]$-hard. The results are in contrast with the
maximum matroid-matroid intersection which is solvable in polynomial time by an
old result of Edmonds. We also prove that it is $NP$-hard to approximate the
weighted greedoid maximization within $2^{n^{O(1)}}$ where $n$ is the size of
the domain of the greedoid.
  A preliminary version ``The Complexity of Maximum Matroid-Greedoid
Intersection'' appeared in Proc. FCT 2001, LNCS 2138, pp. 535--539,
Springer-Verlag 2001."
"A nearly logarithmic lower bound on the randomized competitive ratio for the
metrical task systems problem is presented. This implies a similar lower bound
for the extensively studied k-server problem. The proof is based on Ramsey-type
theorems for metric spaces, that state that every metric space contains a large
subspace which is approximately a hierarchically well-separated tree (and in
particular an ultrametric). These Ramsey-type theorems may be of independent
interest."
The paper referred to in the title is withdrawn.
"Unfair metrical task systems are a generalization of online metrical task
systems. In this paper we introduce new techniques to combine algorithms for
unfair metrical task systems and apply these techniques to obtain improved
randomized online algorithms for metrical task systems on arbitrary metric
spaces."
"This paper is concerned with online caching algorithms for the
(n,k)-companion cache, defined by Brehob et. al. In this model the cache is
composed of two components: a k-way set-associative cache and a companion
fully-associative cache of size n. We show that the deterministic competitive
ratio for this problem is (n+1)(k+1)-1, and the randomized competitive ratio is
O(\log n \log k) and \Omega(\log n +\log k)."
"We consider the problem of searching for an object on a line at an unknown
distance OPT from the original position of the searcher, in the presence of a
cost of d for each time the searcher changes direction. This is a
generalization of the well-studied linear-search problem. We describe a
strategy that is guaranteed to find the object at a cost of at most 9*OPT + 2d,
which has the optimal competitive ratio 9 with respect to OPT plus the minimum
corresponding additive term. Our argument for upper and lower bound uses an
infinite linear program, which we solve by experimental solution of an infinite
series of approximating finite linear programs, estimating the limits, and
solving the resulting recurrences. We feel that this technique is interesting
in its own right and should help solve other searching problems. In particular,
we consider the star search or cow-path problem with turn cost, where the
hidden object is placed on one of m rays emanating from the original position
of the searcher. For this problem we give a tight bound of
(1+(2(m^m)/((m-1)^(m-1))) OPT + m ((m/(m-1))^(m-1) - 1) d. We also discuss
tradeoff between the corresponding coefficients, and briefly consider
randomized strategies on the line."
"Traditional Insertion Sort runs in O(n^2) time because each insertion takes
O(n) time. When people run Insertion Sort in the physical world, they leave
gaps between items to accelerate insertions. Gaps help in computers as well.
This paper shows that Gapped Insertion Sort has insertion times of O(log n)
with high probability, yielding a total running time of O(n log n) with high
probability."
"The study of hashing is closely related to the analysis of balls and bins. It
is well-known that instead of using a single hash function if we randomly hash
a ball into two bins and place it in the smaller of the two, then this
dramatically lowers the maximum load on bins. This leads to the concept of
two-way hashing where the largest bucket contains $O(\log\log n)$ balls with
high probability. The hash look up will now search in both the buckets an item
hashes to. Since an item may be placed in one of two buckets, we could
potentially move an item after it has been initially placed to reduce maximum
load. with a maximum load of We show that by performing moves during inserts, a
maximum load of 2 can be maintained on-line, with high probability, while
supporting hash update operations. In fact, with $n$ buckets, even if the space
for two items are pre-allocated per bucket, as may be desirable in hardware
implementations, more than $n$ items can be stored giving a high memory
utilization. We also analyze the trade-off between the number of moves
performed during inserts and the maximum load on a bucket. By performing at
most $h$ moves, we can maintain a maximum load of $O(\frac{\log \log n}{h
\log(\log\log n/h)})$. So, even by performing one move, we achieve a better
bound than by performing no moves at all."
"We describe algorithms, based on Avis and Fukuda's reverse search paradigm,
for listing all maximal independent sets in a sparse graph in polynomial time
and delay per output. For bounded degree graphs, our algorithms take constant
time per set generated; for minor-closed graph families, the time is O(n) per
set, and for more general sparse graph families we achieve subquadratic time
per set. We also describe new data structures for maintaining a dynamic vertex
set S in a sparse or minor-closed graph family, and querying the number of
vertices not dominated by S; for minor-closed graph families the time per
update is constant, while it is sublinear for any sparse graph family. We can
also maintain a dynamic vertex set in an arbitrary m-edge graph and test the
independence of the maintained set in time O(sqrt m) per update. We use the
domination data structures as part of our enumeration algorithms."
"Metric embedding has become a common technique in the design of algorithms.
Its applicability is often dependent on how high the embedding's distortion is.
For example, embedding finite metric space into trees may require linear
distortion as a function of its size. Using probabilistic metric embeddings,
the bound on the distortion reduces to logarithmic in the size.
  We make a step in the direction of bypassing the lower bound on the
distortion in terms of the size of the metric. We define ""multi-embeddings"" of
metric spaces in which a point is mapped onto a set of points, while keeping
the target metric of polynomial size and preserving the distortion of paths.
The distortion obtained with such multi-embeddings into ultrametrics is at most
O(log Delta loglog Delta) where Delta is the aspect ratio of the metric. In
particular, for expander graphs, we are able to obtain constant distortion
embeddings into trees in contrast with the Omega(log n) lower bound for all
previous notions of embeddings.
  We demonstrate the algorithmic application of the new embeddings for two
optimization problems: group Steiner tree and metrical task systems."
"Sorting and hashing are two completely different concepts in computer
science, and appear mutually exclusive to one another. Hashing is a search
method using the data as a key to map to the location within memory, and is
used for rapid storage and retrieval. Sorting is a process of organizing data
from a random permutation into an ordered arrangement, and is a common activity
performed frequently in a variety of applications.
  Almost all conventional sorting algorithms work by comparison, and in doing
so have a linearithmic greatest lower bound on the algorithmic time complexity.
Any improvement in the theoretical time complexity of a sorting algorithm can
result in overall larger gains in implementation performance.. A gain in
algorithmic performance leads to much larger gains in speed for the application
that uses the sort algorithm. Such a sort algorithm needs to use an alternative
method for ordering the data than comparison, to exceed the linearithmic time
complexity boundary on algorithmic performance.
  The hash sort is a general purpose non-comparison based sorting algorithm by
hashing, which has some interesting features not found in conventional sorting
algorithms. The hash sort asymptotically outperforms the fastest traditional
sorting algorithm, the quick sort. The hash sort algorithm has a linear time
complexity factor -- even in the worst case. The hash sort opens an area for
further work and investigation into alternative means of sorting."
"We study the problem of scheduling equal-length jobs with release times and
deadlines, where the objective is to maximize the number of completed jobs.
Preemptions are not allowed. In Graham's notation, the problem is described as
1|r_j;p_j=p|\sum U_j. We give the following results: (1) We show that the often
cited algorithm by Carlier from 1981 is not correct. (2) We give an algorithm
for this problem with running time O(n^5)."
"Consider laying out a fixed-topology tree of N nodes into external memory
with block size B so as to minimize the worst-case number of block memory
transfers required to traverse a path from the root to a node of depth D. We
prove that the optimal number of memory transfers is $$ \cases{
  \displaystyle
  \Theta\left( {D \over \lg (1{+}B)} \right)
  & when $D = O(\lg N)$, \cr
  \displaystyle
  \Theta\left( {\lg N \over \lg \left(1{+}{B \lg N \over D}\right)} \right)
  & when $D = \Omega(\lg N)$ and $D = O(B \lg N)$, \cr
  \displaystyle
  \Theta\left( {D \over B} \right)
  & when $D = \Omega(B \lg N)$.
  } $$"
"Described are two algorithms to find long approximate palindromes in a
string, for example a DNA sequence. A simple algorithm requires O(n)-space and
almost always runs in $O(k.n)$-time where n is the length of the string and k
is the number of ``errors'' allowed in the palindrome. Its worst-case
time-complexity is $O(n^2)$ but this does not occur with real biological
sequences. A more complex algorithm guarantees $O(k.n)$ worst-case time
complexity."
"We present a new GCD algorithm of two integers or polynomials. The algorithm
is iterative and its time complexity is still $O(n \\log^2 n ~ log \\log n)$
for $n$-bit inputs."
"A concept of ""evolving categories"" is suggested to build a simple, scalable,
mathematically consistent framework for representing in uniform way both data
and algorithms. A state machine for executing algorithms becomes clear, rich
and powerful semantics, based on category theory, and still allows easy
implementation. Moreover, it gives an original insight into the nature and
semantics of algorithms."
"We study the problem of preemptive scheduling of n equal-length jobs with
given release times on m identical parallel machines. The objective is to
minimize the average flow time. Recently, Brucker and Kravchenko proved that
the optimal schedule can be computed in polynomial time by solving a linear
program with O(n^3) variables and constraints, followed by some substantial
post-processing (where n is the number of jobs.) In this note we describe a
simple linear program with only O(mn) variables and constraints. Our linear
program produces directly the optimal schedule and does not require any
post-processing."
"We give a formalization of the notion of test purpose based on (suitably
restricted) Message Sequence Charts. We define the validity of test cases with
respect to such a formal test purpose and provide a simple decision procedure
for validity."
"Histograms are used to summarize the contents of relations into a number of
buckets for the estimation of query result sizes. Several techniques (e.g.,
MaxDiff and V-Optimal) have been proposed in the past for determining bucket
boundaries which provide accurate estimations. However, while search strategies
for optimal bucket boundaries are rather sophisticated, no much attention has
been paid for estimating queries inside buckets and all of the above techniques
adopt naive methods for such an estimation. This paper focuses on the problem
of improving the estimation inside a bucket once its boundaries have been
fixed. The proposed technique is based on the addition, to each bucket, of
32-bit additional information (organized into a 4-level tree index), storing
approximate cumulative frequencies at 7 internal intervals of the bucket. Both
theoretical analysis and experimental results show that, among a number of
alternative ways to organize the additional information, the 4-level tree index
provides the best frequency estimation inside a bucket. The index is later
added to two well-known histograms, MaxDiff and V-Optimal, obtaining the
non-obvious result that despite the spatial cost of 4LT which reduces the
number of allowed buckets once the storage space has been fixed, the original
methods are strongly improved in terms of accuracy."
"We consider the problem of maintaining a dynamic set of integers and
answering queries of the form: report a point (equivalently, all points) in a
given interval. Range searching is a natural and fundamental variant of integer
search, and can be solved using predecessor search. However, for a RAM with
w-bit words, we show how to perform updates in O(lg w) time and answer queries
in O(lglg w) time. The update time is identical to the van Emde Boas structure,
but the query time is exponentially faster. Existing lower bounds show that
achieving our query time for predecessor search requires doubly-exponentially
slower updates. We present some arguments supporting the conjecture that our
solution is optimal.
  Our solution is based on a new and interesting recursion idea which is ""more
extreme"" that the van Emde Boas recursion. Whereas van Emde Boas uses a simple
recursion (repeated halving) on each path in a trie, we use a nontrivial, van
Emde Boas-like recursion on every such path. Despite this, our algorithm is
quite clean when seen from the right angle. To achieve linear space for our
data structure, we solve a problem which is of independent interest. We develop
the first scheme for dynamic perfect hashing requiring sublinear space. This
gives a dynamic Bloomier filter (an approximate storage scheme for sparse
vectors) which uses low space. We strengthen previous lower bounds to show that
these results are optimal."
"In this paper we address two optimization problems arising in the design of
genomic assays based on universal tag arrays. First, we address the universal
array tag set design problem. For this problem, we extend previous formulations
to incorporate antitag-to-antitag hybridization constraints in addition to
constraints on antitag-to-tag hybridization specificity, establish a
constructive upper bound on the maximum number of tags satisfying the extended
constraints, and propose a simple greedy tag selection algorithm. Second, we
give methods for improving the multiplexing rate in large-scale genomic assays
by combining primer selection with tag assignment. Experimental results on
simulated data show that this integrated optimization leads to reductions of up
to 50% in the number of required arrays."
"String barcoding is a recently introduced technique for genomic-based
identification of microorganisms. In this paper we describe the engineering of
highly scalable algorithms for robust string barcoding. Our methods enable
distinguisher selection based on whole genomic sequences of hundreds of
microorganisms of up to bacterial size on a well-equipped workstation, and can
be easily parallelized to further extend the applicability range to thousands
of bacterial size genomes. Experimental results on both randomly generated and
NCBI genomic data show that whole-genome based selection results in a number of
distinguishers nearly matching the information theoretic lower bounds for the
problem."
"We relate the Burrows-Wheeler transformation with a result in combinatorics
on words known as the Gessel-Reutenauer transformation."
"In this paper we propose new solution methods for designing tag sets for use
in universal DNA arrays. First, we give integer linear programming formulations
for two previous formalizations of the tag set design problem, and show that
these formulations can be solved to optimality for instance sizes of practical
interest by using general purpose optimization packages. Second, we note the
benefits of periodic tags, and establish an interesting connection between the
tag design problem and the problem of packing the maximum number of
vertex-disjoint directed cycles in a given graph. We show that combining a
simple greedy cycle packing algorithm with a previously proposed alphabetic
tree search strategy yields an increase of over 40% in the number of tags
compared to previous methods."
"We continue the investigation of problems concerning correlation clustering
or clustering with qualitative information, which is a clustering formulation
that has been studied recently. The basic setup here is that we are given as
input a complete graph on n nodes (which correspond to nodes to be clustered)
whose edges are labeled + (for similar pairs of items) and - (for dissimilar
pairs of items). Thus we have only as input qualitative information on
similarity and no quantitative distance measure between items. The quality of a
clustering is measured in terms of its number of agreements, which is simply
the number of edges it correctly classifies, that is the sum of number of -
edges whose endpoints it places in different clusters plus the number of +
edges both of whose endpoints it places within the same cluster.
  In this paper, we study the problem of finding clusterings that maximize the
number of agreements, and the complementary minimization version where we seek
clusterings that minimize the number of disagreements. We focus on the
situation when the number of clusters is stipulated to be a small constant k.
Our main result is that for every k, there is a polynomial time approximation
scheme for both maximizing agreements and minimizing disagreements. (The
problems are NP-hard for every k >= 2.) The main technical work is for the
minimization version, as the PTAS for maximizing agreements follows along the
lines of the property tester for Max k-CUT.
  In contrast, when the number of clusters is not specified, the problem of
minimizing disagreements was shown to be APX-hard, even though the maximization
version admits a PTAS."
"Following Mettu and Plaxton, we study online algorithms for the k-medians
problem. Such an algorithm must produce a nested sequence F_1\subseteq
F_2\subseteq...\subseteq F_n of sets of facilities. Mettu and Plaxton show that
online metric medians has a (roughly) 40-competitive deterministic
polynomial-time algorithm. We give improved algorithms, including a
(24+\epsilon)-competitive deterministic polynomial-time algorithm and a
5.44-competitive, randomized, non-polynomial-time algorithm.
  We also consider the competitive ratio with respect to size. An algorithm is
s-size-competitive if, for each k, the cost of F_k is at most the minimum cost
of any set of k facilities, while the size of F_k is at most s k. We present
optimally competitive algorithms for this problem.
  Our proofs reduce online medians to the following online bidding problem:
faced with some unknown threshold T>0, an algorithm must submit ``bids'' b>0
until it submits a bid as large as T. The algorithm pays the sum of its bids.
We describe optimally competitive algorithms for online bidding.
  Our results on cost-competitive online medians extend to approximately metric
distance functions, online fractional medians, and online bicriteria
approximation."
"The Reverse Greedy algorithm (RGreedy) for the k-median problem works as
follows. It starts by placing facilities on all nodes. At each step, it removes
a facility to minimize the resulting total distance from the customers to the
remaining facilities. It stops when k facilities remain. We prove that, if the
distance function is metric, then the approximation ratio of RGreedy is between
?(log n/ log log n) and O(log n)."
"We introduce a new class of non-standard variable-length codes, called
adaptive codes. This class of codes associates a variable-length codeword to
the symbol being encoded depending on the previous symbols in the input data
string. An efficient algorithm for constructing adaptive codes of order one is
presented. Then, we introduce a natural generalization of adaptive codes,
called GA codes."
"General wisdom is, mathematical operation is needed to generate number by
numbers. It is pointed out that without any mathematical operation true random
numbers can be generated by numbers through algorithmic process. It implies
that human brain itself is a living true random number generator. Human brain
can meet the enormous human demand of true random numbers."
"We study the multiple-precision addition of two positive floating-point
numbers in base 2, with exact rounding, as specified in the MPFR library, i.e.
where each number has its own precision. We show how the best possible
complexity (up to a constant factor that depends on the implementation) can be
obtain."
"We study practically efficient methods for performing combinatorial group
testing. We present efficient non-adaptive and two-stage combinatorial group
testing algorithms, which identify the at most d items out of a given set of n
items that are defective, using fewer tests for all practical set sizes. For
example, our two-stage algorithm matches the information theoretic lower bound
for the number of tests in a combinatorial group testing regimen."
"Adaptive variable-length codes associate a variable-length codeword to the
symbol being encoded depending on the previous symbols in the input string.
This class of codes has been recently presented in [Dragos Trinca,
arXiv:cs.DS/0505007] as a new class of non-standard variable-length codes. New
algorithms for data compression, based on adaptive variable-length codes of
order one and Huffman's algorithm, have been recently presented in [Dragos
Trinca, ITCC 2004]. In this paper, we extend the work done so far by the
following contributions: first, we propose an improved generalization of these
algorithms, called EAHn. Second, we compute the entropy bounds for EAHn, using
the well-known bounds for Huffman's algorithm. Third, we discuss implementation
details and give reports of experimental results obtained on some well-known
corpora. Finally, we describe a parallel version of EAHn using the PRAM model
of computation."
"In this paper, a sorting technique is presented that takes as input a data
set whose primary key domain is known to the sorting algorithm, and works with
an time efficiency of O(n+k), where k is the primary key domain. It is shown
that the algorithm has applicability over a wide range of data sets. Later, a
parallel formulation of the same is proposed and its effectiveness is argued.
Though this algorithm is applicable over a wide range of general data sets, it
finds special application (much superior to others) in places where sorting
information that arrives in parts and in cases where input data is huge in
size."
"A coloring of a tree is convex if the vertices that pertain to any color
induce a connected subtree; a partial coloring (which assigns colors to some of
the vertices) is convex if it can be completed to a convex (total) coloring.
Convex coloring of trees arise in areas such as phylogenetics, linguistics,
etc. eg, a perfect phylogenetic tree is one in which the states of each
character induce a convex coloring of the tree. Research on perfect phylogeny
is usually focused on finding a tree so that few predetermined partial
colorings of its vertices are convex.
  When a coloring of a tree is not convex, it is desirable to know ""how far"" it
is from a convex one. In [19], a natural measure for this distance, called the
recoloring distance was defined: the minimal number of color changes at the
vertices needed to make the coloring convex. This can be viewed as minimizing
the number of ""exceptional vertices"" w.r.t. to a closest convex coloring. The
problem was proved to be NP-hard even for colored string.
  In this paper we continue the work of [19], and present a 2-approximation
algorithm of convex recoloring of strings whose running time O(cn), where c is
the number of colors and n is the size of the input, and an O(cn^2)-time
3-approximation algorithm for convex recoloring of trees."
"We give the first sorting algorithm with bounds in terms of higher-order
entropies: let $S$ be a sequence of length $m$ containing $n$ distinct elements
and let (H_\ell (S)) be the $\ell$th-order empirical entropy of $S$, with
(n^{\ell + 1} \log n \in O (m)); our algorithm sorts $S$ using ((H_\ell (S) + O
(1)) m) comparisons."
"An explicit algorithm is presented for testing whether two non-directed
graphs are isomorphic or not. It is shown that for a graph of n vertices, the
number of n independent operations needed for the test is polynomial in n. A
proof that the algorithm actually performs the test is presented."
"In this paper we describe a new algorithm for buffered global routing
according to a prescribed buffer site map. Specifically, we describe a provably
good multi-commodity flow based algorithm that finds a global routing
minimizing buffer and wire congestion subject to given constraints on routing
area (wirelength and number of buffers) and sink delays. Our algorithm allows
computing the tradeoff curve between routing area and wire/buffer congestion
under any combination of delay and capacity constraints, and simultaneously
performs buffer/wire sizing, as well as layer and pin assignment. Experimental
results show that near-optimal results are obtained with a practical runtime."
"In 1994, Burrows and Wheeler developed a data compression algorithm which
performs significantly better than Lempel-Ziv based algorithms. Since then, a
lot of work has been done in order to improve their algorithm, which is based
on a reversible transformation of the input string, called BWT (the
Burrows-Wheeler transformation). In this paper, we propose a compression scheme
based on BWT, MTF (move-to-front coding), and a version of the algorithms
presented in [Dragos Trinca, ITCC-2004]."
"The well-known Eulerian path problem can be solved in polynomial time (more
exactly, there exists a linear time algorithm for this problem). In this paper,
we model the problem using a string matching framework, and then initiate an
algorithmic study on a variant of this problem, called the (2,1)-STRING-MATCH
problem (which is actually a generalization of the Eulerian path problem).
Then, we present a polynomial-time algorithm for the (2,1)-STRING-MATCH
problem, which is the most important result of this paper. Specifically, we get
a lower bound of Omega(n), and an upper bound of O(n^{2})."
"Adaptive codes associate variable-length codewords to symbols being encoded
depending on the previous symbols in the input data string. This class of codes
has been introduced in [Dragos Trinca, cs.DS/0505007] as a new class of
non-standard variable-length codes. New algorithms for data compression, based
on adaptive codes of order one, have been presented in [Dragos Trinca,
ITCC-2004], where we have behaviorally shown that for a large class of input
data strings, these algorithms substantially outperform the Lempel-Ziv
universal data compression algorithm. EAH has been introduced in [Dragos
Trinca, cs.DS/0505061], as an improved generalization of these algorithms. In
this paper, we present a translation of the EAH algorithm into the graph
theory."
"Adaptive codes have been introduced in [Dragos Trinca, cs.DS/0505007] as a
new class of non-standard variable-length codes. These codes associate
variable-length codewords to symbols being encoded depending on the previous
symbols in the input data string. A new data compression algorithm, called EAH,
has been introduced in [Dragos Trinca, cs.DS/0505061], where we have
behaviorally shown that for a large class of input data strings, this algorithm
substantially outperforms the well-known Lempel-Ziv universal data compression
algorithm. In this paper, we translate the EAH encoder into automata theory."
"This article introduces an adaptive sorting algorithm that can relocate
elements accurately by substituting their values into a function which we name
it the guessing function. We focus on building this function which is the
mapping relationship between record values and their corresponding sorted
locations essentially. The time complexity of this algorithm O(n),when records
distributed uniformly. Additionally, similar approach can be used in the
searching algorithm."
"Starting with a set of weighted items, we want to create a generic sample of
a certain size that we can later use to estimate the total weight of arbitrary
subsets. For this purpose, we propose priority sampling which tested on
Internet data performed better than previous methods by orders of magnitude.
  Priority sampling is simple to define and implement: we consider a steam of
items i=0,...,n-1 with weights w_i. For each item i, we generate a random
number r_i in (0,1) and create a priority q_i=w_i/r_i. The sample S consists of
the k highest priority items. Let t be the (k+1)th highest priority. Each
sampled item i in S gets a weight estimate W_i=max{w_i,t}, while non-sampled
items get weight estimate W_i=0.
  Magically, it turns out that the weight estimates are unbiased, that is,
E[W_i]=w_i, and by linearity of expectation, we get unbiased estimators over
any subset sum simply by adding the sampled weight estimates from the subset.
Also, we can estimate the variance of the estimates, and surpricingly, there is
no co-variance between different weight estimates W_i and W_j.
  We conjecture an extremely strong near-optimality; namely that for any weight
sequence, there exists no specialized scheme for sampling k items with unbiased
estimators that gets smaller total variance than priority sampling with k+1
items. Very recently Mario Szegedy has settled this conjecture."
"The Sum of Squares algorithm for bin packing was defined in [2] and studied
in great detail in [1], where it was proved that its worst case performance
ratio is at most 3. In this note, we improve the asymptotic worst case bound to
2.7777..."
"We here study Max Hamming XSAT, ie, the problem of finding two XSAT models at
maximum Hamming distance. By using a recent XSAT solver as an auxiliary
function, an O(1.911^n) time algorithm can be constructed, where n is the
number of variables. This upper time bound can be further improved to
O(1.8348^n) by introducing a new kind of branching, more directly suited for
finding models at maximum Hamming distance. The techniques presented here are
likely to be of practical use as well as of theoretical value, proving that
there are non-trivial algorithms for maximum Hamming distance problems."
"We study 4 problems in string matching, namely, regular expression matching,
approximate regular expression matching, string edit distance, and subsequence
indexing, on a standard word RAM model of computation that allows
logarithmic-sized words to be manipulated in constant time. We show how to
improve the space and/or remove a dependency on the alphabet size for each
problem using either an improved tabulation technique of an existing algorithm
or by combining known algorithms in a new way."
"In this paper we study the problem of finding the approximate nearest
neighbor of a query point in the high dimensional space, focusing on the
Euclidean space. The earlier approaches use locality-preserving hash functions
(that tend to map nearby points to the same value) to construct several hash
tables to ensure that the query point hashes to the same bucket as its nearest
neighbor in at least one table. Our approach is different -- we use one (or a
few) hash table and hash several randomly chosen points in the neighborhood of
the query point showing that at least one of them will hash to the bucket
containing its nearest neighbor. We show that the number of randomly chosen
points in the neighborhood of the query point $q$ required depends on the
entropy of the hash value $h(p)$ of a random point $p$ at the same distance
from $q$ at its nearest neighbor, given $q$ and the locality preserving hash
function $h$ chosen randomly from the hash family. Precisely, we show that if
the entropy $I(h(p)|q,h) = M$ and $g$ is a bound on the probability that two
far-off points will hash to the same bucket, then we can find the approximate
nearest neighbor in $O(n^\rho)$ time and near linear $\tilde O(n)$ space where
$\rho = M/\log(1/g)$. Alternatively we can build a data structure of size
$\tilde O(n^{1/(1-\rho)})$ to answer queries in $\tilde O(d)$ time. By applying
this analysis to the locality preserving hash functions in and adjusting the
parameters we show that the $c$ nearest neighbor can be computed in time
$\tilde O(n^\rho)$ and near linear space where $\rho \approx 2.06/c$ as $c$
becomes large."
"In this paper, we study the two choice balls and bins process when balls are
not allowed to choose any two random bins, but only bins that are connected by
an edge in an underlying graph. We show that for $n$ balls and $n$ bins, if the
graph is almost regular with degree $n^\epsilon$, where $\epsilon$ is not too
small, the previous bounds on the maximum load continue to hold. Precisely, the
maximum load is $\log \log n + O(1/\epsilon) + O(1)$. For general
$\Delta$-regular graphs, we show that the maximum load is $\log\log n +
O(\frac{\log n}{\log (\Delta/\log^4 n)}) + O(1)$ and also provide an almost
matching lower bound of $\log \log n + \frac{\log n}{\log (\Delta \log n)}$.
  V{\""o}cking [Voc99] showed that the maximum bin size with $d$ choice load
balancing can be further improved to $O(\log\log n /d)$ by breaking ties to the
left. This requires $d$ random bin choices. We show that such bounds can be
achieved by making only two random accesses and querying $d/2$ contiguous bins
in each access. By grouping a sequence of $n$ bins into $2n/d$ groups, each of
$d/2$ consecutive bins, if each ball chooses two groups at random and inserts
the new ball into the least-loaded bin in the lesser loaded group, then the
maximum load is $O(\log\log n/d)$ with high probability."
"Pbit, besides its simplicity, is definitely the fastest list sorting
algorithm. It considerably surpasses all already known methods. Among many
advantages, it is stable, linear and be made to run in place. I will compare
Pbit with algorithm described by Donald E. Knuth in the third volume of ''The
Art of Computer Programming'' and other (QuickerSort, MergeSort) list sorting
algorithms."
"The problem of clustering fingerprint vectors is an interesting problem in
Computational Biology that has been proposed in (Figureroa et al. 2004). In
this paper we show some improvements in closing the gaps between the known
lower bounds and upper bounds on the approximability of some variants of the
biological problem. Namely we are able to prove that the problem is APX-hard
even when each fingerprint contains only two unknown position. Moreover we have
studied some variants of the orginal problem, and we give two 2-approximation
algorithm for the IECMV and OECMV problems when the number of unknown entries
for each vector is at most a constant."
"This paper deals with the problem of finding, for a given graph and a given
natural number k, a subgraph of k nodes with a maximum number of edges. This
problem is known as the k-cluster problem and it is NP-hard on general graphs
as well as on chordal graphs. In this paper, it is shown that the k-cluster
problem is solvable in polynomial time on interval graphs. In particular, we
present two polynomial time algorithms for the class of proper interval graphs
and the class of general interval graphs, respectively. Both algorithms are
based on a matrix representation for interval graphs. In contrast to
representations used in most of the previous work, this matrix representation
does not make use of the maximal cliques in the investigated graph."
"Given two rooted, labeled trees $P$ and $T$ the tree path subsequence problem
is to determine which paths in $P$ are subsequences of which paths in $T$. Here
a path begins at the root and ends at a leaf. In this paper we propose this
problem as a useful query primitive for XML data, and provide new algorithms
improving the previously best known time and space bounds."
"We develop dynamic dictionaries on the word RAM that use asymptotically
optimal space, up to constant factors, subject to insertions and deletions, and
subject to supporting perfect-hashing queries and/or membership queries, each
operation in constant time with high probability. When supporting only
membership queries, we attain the optimal space bound of Theta(n lg(u/n)) bits,
where n and u are the sizes of the dictionary and the universe, respectively.
Previous dictionaries either did not achieve this space bound or had time
bounds that were only expected and amortized. When supporting perfect-hashing
queries, the optimal space bound depends on the range {1,2,...,n+t} of
hashcodes allowed as output. We prove that the optimal space bound is Theta(n
lglg(u/n) + n lg(n/(t+1))) bits when supporting only perfect-hashing queries,
and it is Theta(n lg(u/n) + n lg(n/(t+1))) bits when also supporting membership
queries. All upper bounds are new, as is the Omega(n lg(n/(t+1))) lower bound."
"We consider the problem of efficiently designing sets (codes) of equal-length
DNA strings (words) that satisfy certain combinatorial constraints. This
problem has numerous motivations including DNA computing and DNA self-assembly.
Previous work has extended results from coding theory to obtain bounds on code
size for new biologically motivated constraints and has applied heuristic local
search and genetic algorithm techniques for code design. This paper proposes a
natural optimization formulation of the DNA code design problem in which the
goal is to design n strings that satisfy a given set of constraints while
minimizing the length of the strings. For multiple sets of constraints, we
provide high-probability algorithms that run in time polynomial in n and any
given constraint parameters, and output strings of length within a constant
factor of the optimal. To the best of our knowledge, this work is the first to
consider this type of optimization problem in the context of DNA code design."
"This paper proposes a new algorithm for solving maximal cliques for simple
undirected graphs using the theory of prime numbers. A novel approach using
prime numbers is used to find cliques and ends with a discussion of the
algorithm."
"The competitive analysis fails to model locality of reference in the online
paging problem. To deal with it, Borodin et. al. introduced the access graph
model, which attempts to capture the locality of reference. However, the access
graph model has a number of troubling aspects. The access graph has to be known
in advance to the paging algorithm and the memory required to represent the
access graph itself may be very large.
  In this paper we present truly online strongly competitive paging algorithms
in the access graph model that do not have any prior information on the access
sequence. We present both deterministic and randomized algorithms. The
algorithms need only O(k log n) bits of memory, where k is the number of page
slots available and n is the size of the virtual address space. I.e.,
asymptotically no more memory than needed to store the virtual address
translation table.
  We also observe that our algorithms adapt themselves to temporal changes in
the locality of reference. We model temporal changes in the locality of
reference by extending the access graph model to the so called extended access
graph model, in which many vertices of the graph can correspond to the same
virtual page. We define a measure for the rate of change in the locality of
reference in G denoted by Delta(G). We then show our algorithms remain strongly
competitive as long as Delta(G) >= (1+ epsilon)k, and no truly online algorithm
can be strongly competitive on a class of extended access graphs that includes
all graphs G with Delta(G) >= k- o(k)."
"A particle-swarm is a set of indivisible processing elements that traverse a
network in order to perform a distributed function. This paper will describe a
particular implementation of a particle-swarm that can simulate the behavior of
the popular PageRank algorithm in both its {\it global-rank} and {\it
relative-rank} incarnations. PageRank is compared against the particle-swarm
method on artificially generated scale-free networks of 1,000 nodes constructed
using a common gamma value, $\gamma = 2.5$. The running time of the
particle-swarm algorithm is $O(|P|+|P|t)$ where $|P|$ is the size of the
particle population and $t$ is the number of particle propagation iterations.
The particle-swarm method is shown to be useful due to its ease of extension
and running time."
"Tree decompositions were developed by Robertson and Seymour. Since then
algorithms have been developed to solve intractable problems efficiently for
graphs of bounded treewidth. In this paper we extend tree decompositions to
allow cycles to exist in the decomposition graph; we call these new
decompositions plane decompositions because we require that the decomposition
graph be planar. First, we give some background material about tree
decompositions and an overview of algorithms both for decompositions and for
approximations of planar graphs. Then, we give our plane decomposition
definition and an algorithm that uses this decomposition to approximate the
size of the maximum independent set of the underlying graph in polynomial time."
"We present a simple algorithm which maintains the topological order of a
directed acyclic graph with n nodes under an online edge insertion sequence in
O(n^{2.75}) time, independent of the number of edges m inserted. For dense
DAGs, this is an improvement over the previous best result of O(min(m^{3/2}
log(n), m^{3/2} + n^2 log(n)) by Katriel and Bodlaender. We also provide an
empirical comparison of our algorithm with other algorithms for online
topological sorting. Our implementation outperforms them on certain hard
instances while it is still competitive on random edge insertion sequences
leading to complete DAGs."
"In this paper, a new general decomposition theory inspired from modular graph
decomposition is presented. Our main result shows that, within this general
theory, most of the nice algorithmic tools developed for modular decomposition
are still efficient. This theory not only unifies the usual modular
decomposition generalisations such as modular decomposition of directed graphs
or decomposition of 2-structures, but also star cutsets and bimodular
decomposition. Our general framework provides a decomposition algorithm which
improves the best known algorithms for bimodular decomposition."
"In a previous paper we generalized the Knuth-Morris-Pratt (KMP) pattern
matching algorithm and defined a non-conventional kind of RAM, the MP--RAMs
(RAMS equipped with extra operations), and designed an O(n) on-line algorithm
for solving the serial episode matching problem on MP--RAMs when there is only
one single episode. We here give two extensions of this algorithm to the case
when we search for several patterns simultaneously and compare them. More
preciseley, given $q+1$ strings (a text $t$ of length $n$ and $q$ patterns
$m\_1,...,m\_q$) and a natural number $w$, the {\em multiple serial episode
matching problem} consists in finding the number of size $w$ windows of text
$t$ which contain patterns $m\_1,...,m\_q$ as subsequences, i.e. for each
$m\_i$, if $m\_i=p\_1,..., p\_k$, the letters $p\_1,..., p\_k$ occur in the
window, in the same order as in $m\_i$, but not necessarily consecutively (they
may be interleaved with other letters).} The main contribution is an algorithm
solving this problem on-line in time $O(nq)$."
"In [11] we defined Inf-Datalog and characterized the fragments of Monadic
inf-Datalog that have the same expressive power as Modal Logic (resp. $CTL$,
alternation-free Modal $\mu$-calculus and Modal $\mu$-calculus). We study here
the time and space complexity of evaluation of Monadic inf-Datalog programs on
finite models. We deduce a new unified proof that model checking has 1. linear
data and program complexities (both in time and space) for $CTL$ and
alternation-free Modal $\mu$-calculus, and 2. linear-space (data and program)
complexities, linear-time program complexity and polynomial-time data
complexity for $L\mu\_k$ (Modal $\mu$-calculus with fixed alternation-depth at
most $k$).}"
"The {\em edit distance} between two ordered trees with vertex labels is the
minimum cost of transforming one tree into the other by a sequence of
elementary operations consisting of deleting and relabeling existing nodes, as
well as inserting new nodes. In this paper, we present a worst-case
$O(n^3)$-time algorithm for this problem, improving the previous best
$O(n^3\log n)$-time algorithm~\cite{Klein}. Our result requires a novel
adaptive strategy for deciding how a dynamic program divides into subproblems
(which is interesting in its own right), together with a deeper understanding
of the previous algorithms for the problem. We also prove the optimality of our
algorithm among the family of \emph{decomposition strategy} algorithms--which
also includes the previous fastest algorithms--by tightening the known lower
bound of $\Omega(n^2\log^2 n)$~\cite{Touzet} to $\Omega(n^3)$, matching our
algorithm's running time. Furthermore, we obtain matching upper and lower
bounds of $\Theta(n m^2 (1 + \log \frac{n}{m}))$ when the two trees have
different sizes $m$ and~$n$, where $m < n$."
"Higher-dimensional orthogonal packing problems have a wide range of practical
applications, including packing, cutting, and scheduling. Combining the use of
our data structure for characterizing feasible packings with our new classes of
lower bounds, and other heuristics, we develop a two-level tree search
algorithm for solving higher-dimensional packing problems to optimality.
Computational results are reported, including optimal solutions for all
two--dimensional test problems from recent literature.
  This is the third in a series of articles describing new approaches to
higher-dimensional packing; see cs.DS/0310032 and cs.DS/0402044."
"We introduces the umodules, a generalisation of the notion of graph module.
The theory we develop captures among others undirected graphs, tournaments,
digraphs, and $2-$structures. We show that, under some axioms, a unique
decomposition tree exists for umodules. Polynomial-time algorithms are provided
for: non-trivial umodule test, maximal umodule computation, and decomposition
tree computation when the tree exists. Our results unify many known
decomposition like modular and bi-join decomposition of graphs, and a new
decomposition of tournaments."
"This paper addresses the problem of finding a B-term wavelet representation
of a given discrete function $f \in \real^n$ whose distance from f is
minimized. The problem is well understood when we seek to minimize the
Euclidean distance between f and its representation. The first known algorithms
for finding provably approximate representations minimizing general $\ell_p$
distances (including $\ell_\infty$) under a wide variety of compactly supported
wavelet bases are presented in this paper. For the Haar basis, a polynomial
time approximation scheme is demonstrated. These algorithms are applicable in
the one-pass sublinear-space data stream model of computation. They generalize
naturally to multiple dimensions and weighted norms. A universal representation
that provides a provable approximation guarantee under all p-norms
simultaneously; and the first approximation algorithms for bit-budget versions
of the problem, known as adaptive quantization, are also presented. Further, it
is shown that the algorithms presented here can be used to select a basis from
a tree-structured dictionary of bases and find a B-term representation of the
given function that provably approximates its best dictionary-basis
representation."
"We study the problem of preemptive scheduling n jobs with given release times
on m identical parallel machines. The objective is to minimize the average flow
time. We show that when all jobs have equal processing times then the problem
can be solved in polynomial time using linear programming. Our algorithm can
also be applied to the open-shop problem with release times and unit processing
times. For the general case (when processing times are arbitrary), we show that
the problem is unary NP-hard."
"We consider offline scheduling algorithms that incorporate speed scaling to
address the bicriteria problem of minimizing energy consumption and a
scheduling metric. For makespan, we give linear-time algorithms to compute all
non-dominated solutions for the general uniprocessor problem and for the
multiprocessor problem when every job requires the same amount of work. We also
show that the multiprocessor problem becomes NP-hard when jobs can require
different amounts of work.
  For total flow, we show that the optimal flow corresponding to a particular
energy budget cannot be exactly computed on a machine supporting arithmetic and
the extraction of roots. This hardness result holds even when scheduling
equal-work jobs on a uniprocessor. We do, however, extend previous work by
Pruhs et al. to give an arbitrarily-good approximation for scheduling
equal-work jobs on a multiprocessor."
"This paper presents a new functionality of the Automatic Differentiation (AD)
tool Tapenade. Tapenade generates adjoint codes which are widely used for
optimization or inverse problems. Unfortunately, for large applications the
adjoint code demands a great deal of memory, because it needs to store a large
set of intermediates values. To cope with that problem, Tapenade implements a
sub-optimal version of a technique called checkpointing, which is a trade-off
between storage and recomputation. Our long-term goal is to provide an optimal
checkpointing strategy for every code, not yet achieved by any AD tool. Towards
that goal, we first introduce modifications in Tapenade in order to give the
user the choice to select the checkpointing strategy most suitable for their
code. Second, we conduct experiments in real-size scientific codes in order to
gather hints that help us to deduce an optimal checkpointing strategy. Some of
the experimental results show memory savings up to 35% and execution time up to
90%."
"This paper presents scheduling algorithms for procrastinators, where the
speed that a procrastinator executes a job increases as the due date
approaches. We give optimal off-line scheduling policies for linearly
increasing speed functions. We then explain the computational/numerical issues
involved in implementing this policy. We next explore the online setting,
showing that there exist adversaries that force any online scheduling policy to
miss due dates. This impossibility result motivates the problem of minimizing
the maximum interval stretch of any job; the interval stretch of a job is the
job's flow time divided by the job's due date minus release time. We show that
several common scheduling strategies, including the ""hit-the-highest-nail""
strategy beloved by procrastinators, have arbitrarily large maximum interval
stretch. Then we give the ""thrashing"" scheduling policy and show that it is a
\Theta(1) approximation algorithm for the maximum interval stretch."
"Let (X,d_X) be an n-point metric space. We show that there exists a
distribution D over non-contractive embeddings into trees f:X-->T such that for
every x in X, the expectation with respect to D of the maximum over y in X of
the ratio d_T(f(x),f(y)) / d_X(x,y) is at most C (log n)^2, where C is a
universal constant. Conversely we show that the above quadratic dependence on
log n cannot be improved in general. Such embeddings, which we call maximum
gradient embeddings, yield a framework for the design of approximation
algorithms for a wide range of clustering problems with monotone costs,
including fault-tolerant versions of k-median and facility location."
"In this paper we revisit the classical regular expression matching problem,
namely, given a regular expression $R$ and a string $Q$, decide if $Q$ matches
one of the strings specified by $R$. Let $m$ and $n$ be the length of $R$ and
$Q$, respectively. On a standard unit-cost RAM with word length $w \geq \log
n$, we show that the problem can be solved in $O(m)$ space with the following
running times: \begin{equation*} \begin{cases}
  O(n\frac{m \log w}{w} + m \log w) & \text{if $m > w$} \\
  O(n\log m + m\log m) & \text{if $\sqrt{w} < m \leq w$} \\
  O(\min(n+ m^2, n\log m + m\log m)) & \text{if $m \leq \sqrt{w}$.} \end{cases}
\end{equation*} This improves the best known time bound among algorithms using
$O(m)$ space. Whenever $w \geq \log^2 n$ it improves all known time bounds
regardless of how much space is used."
"In some applications of matching, the structural or hierarchical properties
of the two graphs being aligned must be maintained. The hierarchical properties
are induced by the direction of the edges in the two directed graphs. These
structural relationships defined by the hierarchy in the graphs act as a
constraint on the alignment. In this paper, we formalize the above problem as
the weighted alignment between two directed acyclic graphs. We prove that this
problem is NP-complete, show several upper bounds for approximating the
solution, and finally introduce polynomial time algorithms for sub-classes of
directed acyclic graphs."
"In this paper, we study online multidimensional bin packing problem when all
items are hypercubes.
  Based on the techniques in one dimensional bin packing algorithm Super
Harmonic by Seiden, we give a framework for online hypercube packing problem
and obtain new upper bounds of asymptotic competitive ratios.
  For square packing, we get an upper bound of 2.1439, which is better than
2.24437.
  For cube packing, we also give a new upper bound 2.6852 which is better than
2.9421 by Epstein and van Stee."
"In this paper we establish a general algorithmic framework between bin
packing and strip packing, with which we achieve the same asymptotic bounds by
applying bin packing algorithms to strip packing. More precisely we obtain the
following results: (1) Any offline bin packing algorithm can be applied to
strip packing maintaining the same asymptotic worst-case ratio. Thus using FFD
(MFFD) as a subroutine, we get a practical (simple and fast) algorithm for
strip packing with an upper bound 11/9 (71/60). A simple AFPTAS for strip
packing immediately follows. (2) A class of Harmonic-based algorithms for bin
packing can be applied to online strip packing maintaining the same asymptotic
competitive ratio. It implies online strip packing admits an upper bound of
1.58889 on the asymptotic competitive ratio, which is very close to the lower
bound 1.5401 and significantly improves the previously best bound of 1.6910 and
affirmatively answers an open question posed by Csirik et. al."
"In this paper, we study the 3D strip packing problem in which we are given a
list of 3-dimensional boxes and required to pack all of them into a
3-dimensional strip with length 1 and width 1 and unlimited height to minimize
the height used. Our results are below: i) we give an approximation algorithm
with asymptotic worst-case ratio 1.69103, which improves the previous best
bound of $2+\epsilon$ by Jansen and Solis-Oba of SODA 2006; ii) we also present
an asymptotic PTAS for the case in which all items have {\em square} bases."
"This paper develops a new method for recovering m-sparse signals that is
simultaneously uniform and quick. We present a reconstruction algorithm whose
run time, O(m log^2(m) log^2(d)), is sublinear in the length d of the signal.
The reconstruction error is within a logarithmic factor (in m) of the optimal
m-term approximation error in l_1. In particular, the algorithm recovers
m-sparse signals perfectly and noisy signals are recovered with polylogarithmic
distortion. Our algorithm makes O(m log^2 (d)) measurements, which is within a
logarithmic factor of optimal. We also present a small-space implementation of
the algorithm. These sketching techniques and the corresponding reconstruction
algorithms provide an algorithmic dimension reduction in the l_1 norm. In
particular, vectors of support m in dimension d can be linearly embedded into
O(m log^2 d) dimensions with polylogarithmic distortion. We can reconstruct a
vector from its low-dimensional sketch in time O(m log^2(m) log^2(d)).
Furthermore, this reconstruction is stable and robust under small
perturbations."
"Given two rooted, ordered, and labeled trees $P$ and $T$ the tree inclusion
problem is to determine if $P$ can be obtained from $T$ by deleting nodes in
$T$. This problem has recently been recognized as an important query primitive
in XML databases. Kilpel\""ainen and Mannila [\emph{SIAM J. Comput. 1995}]
presented the first polynomial time algorithm using quadratic time and space.
Since then several improved results have been obtained for special cases when
$P$ and $T$ have a small number of leaves or small depth. However, in the worst
case these algorithms still use quadratic time and space. Let $n_S$, $l_S$, and
$d_S$ denote the number of nodes, the number of leaves, and the %maximum depth
of a tree $S \in \{P, T\}$. In this paper we show that the tree inclusion
problem can be solved in space $O(n_T)$ and time: O(\min(l_Pn_T, l_Pl_T\log
\log n_T + n_T, \frac{n_Pn_T}{\log n_T} + n_{T}\log n_{T})). This improves or
matches the best known time complexities while using only linear space instead
of quadratic. This is particularly important in practical applications, such as
XML databases, where the space is likely to be a bottleneck."
"We present the \crprecis structure, that is a general-purpose, deterministic
and sub-linear data structure for summarizing \emph{update} data streams. The
\crprecis structure yields the \emph{first deterministic sub-linear space/time
algorithms for update streams} for answering a variety of fundamental stream
queries, such as, (a) point queries, (b) range queries, (c) finding approximate
frequent items, (d) finding approximate quantiles, (e) finding approximate
hierarchical heavy hitters, (f) estimating inner-products, (g) near-optimal
$B$-bucket histograms, etc.."
"We study the approximate string matching and regular expression matching
problem for the case when the text to be searched is compressed with the
Ziv-Lempel adaptive dictionary compression schemes. We present a time-space
trade-off that leads to algorithms improving the previously known complexities
for both problems. In particular, we significantly improve the space bounds,
which in practical applications are likely to be a bottleneck."
"Rank/Select dictionaries are data structures for an ordered set $S \subset
\{0,1,...,n-1\}$ to compute $\rank(x,S)$ (the number of elements in $S$ which
are no greater than $x$), and $\select(i,S)$ (the $i$-th smallest element in
$S$), which are the fundamental components of \emph{succinct data structures}
of strings, trees, graphs, etc. In those data structures, however, only
asymptotic behavior has been considered and their performance for real data is
not satisfactory. In this paper, we propose novel four Rank/Select
dictionaries, esp, recrank, vcode and sdarray, each of which is small if the
number of elements in $S$ is small, and indeed close to $nH_0(S)$ ($H_0(S) \leq
1$ is the zero-th order \textit{empirical entropy} of $S$) in practice, and its
query time is superior to the previous ones. Experimental results reveal the
characteristics of our data structures and also show that these data structures
are superior to existing implementations in both size and query time."
"The running maximum-minimum (max-min) filter computes the maxima and minima
over running windows of size w. This filter has numerous applications in signal
processing and time series analysis. We present an easy-to-implement online
algorithm requiring no more than 3 comparisons per element, in the worst case.
Comparatively, no algorithm is known to compute the running maximum (or
minimum) filter in 1.5 comparisons per element, in the worst case. Our
algorithm has reduced latency and memory usage."
"Lagrangian relaxation and approximate optimization algorithms have received
much attention in the last two decades. Typically, the running time of these
methods to obtain a $\epsilon$ approximate solution is proportional to
$\frac{1}{\epsilon^2}$. Recently, Bienstock and Iyengar, following Nesterov,
gave an algorithm for fractional packing linear programs which runs in
$\frac{1}{\epsilon}$ iterations. The latter algorithm requires to solve a
convex quadratic program every iteration - an optimization subroutine which
dominates the theoretical running time.
  We give an algorithm for convex programs with strictly convex constraints
which runs in time proportional to $\frac{1}{\epsilon}$. The algorithm does NOT
require to solve any quadratic program, but uses gradient steps and elementary
operations only. Problems which have strictly convex constraints include
maximum entropy frequency estimation, portfolio optimization with loss risk
constraints, and various computational problems in signal processing.
  As a side product, we also obtain a simpler version of Bienstock and
Iyengar's result for general linear programming, with similar running time.
  We derive these algorithms using a new framework for deriving convex
optimization algorithms from online game playing algorithms, which may be of
independent interest."
"In this paper we devise an extremely efficient fully dynamic distributed
algorithm for maintaining sparse spanners. Our resuls also include the first
fully dynamic centralized algorithm for the problem with non-trivial bounds for
both incremental and decremental update. Finally, we devise a very efficient
streaming algorithm for the problem."
"A new general decomposition theory inspired from modular graph decomposition
is presented. This helps unifying modular decomposition on different
structures, including (but not restricted to) graphs. Moreover, even in the
case of graphs, the terminology ``module'' not only captures the classical
graph modules but also allows to handle 2-connected components, star-cutsets,
and other vertex subsets. The main result is that most of the nice algorithmic
tools developed for modular decomposition of graphs still apply efficiently on
our generalisation of modules. Besides, when an essential axiom is satisfied,
almost all the important properties can be retrieved. For this case, an
algorithm given by Ehrenfeucht, Gabow, McConnell and Sullivan 1994 is
generalised and yields a very efficient solution to the associated
decomposition problem."
"Given an undirected graph $G=(V,E)$ on $n$ vertices, $m$ edges, and an
integer $t\ge 1$, a subgraph $(V,E_S)$, $E_S\subseteq E$ is called a
$t$-spanner if for any pair of vertices $u,v \in V$, the distance between them
in the subgraph is at most $t$ times the actual distance. We present streaming
algorithms for computing a $t$-spanner of essentially optimal size-stretch
trade offs for any undirected graph.
  Our first algorithm is for the classical streaming model and works for
unweighted graphs only. The algorithm performs a single pass on the stream of
edges and requires $O(m)$ time to process the entire stream of edges. This
drastically improves the previous best single pass streaming algorithm for
computing a $t$-spanner which requires $\theta(mn^{\frac{2}{t}})$ time to
process the stream and computes spanner with size slightly larger than the
optimal.
  Our second algorithm is for {\em StreamSort} model introduced by Aggarwal et
al. [FOCS 2004], which is the streaming model augmented with a sorting
primitive. The {\em StreamSort} model has been shown to be a more powerful and
still very realistic model than the streaming model for massive data sets
applications. Our algorithm, which works of weighted graphs as well, performs
$O(t)$ passes using $O(\log n)$ bits of working memory only.
  Our both the algorithms require elementary data structures."
"Since 1969 \cite{C-MST69,S-SMJ77}, we know that any Presburger-definable set
\cite{P-PCM29} (a set of integer vectors satisfying a formula in the
first-order additive theory of the integers) can be represented by a
state-based symmbolic representation, called in this paper Finite Digit Vector
Automata (FDVA). Efficient algorithms for manipulating these sets have been
recently developed. However, the problem of deciding if a FDVA represents such
a set, is a well-known hard problem first solved by Muchnik in 1991 with a
quadruply-exponential time algorithm. In this paper, we show how to determine
in polynomial time whether a FDVA represents a Presburger-definable set, and we
provide in this positive case a polynomial time algorithm that constructs a
Presburger-formula that defines the same set."
"We consider a memory allocation problem that can be modeled as a version of
bin packing where items may be split, but each bin may contain at most two
(parts of) items. A 3/2-approximation algorithm and an NP-hardness proof for
this problem was given by Chung et al. We give a simpler 3/2-approximation
algorithm for it which is in fact an online algorithm. This algorithm also has
good performance for the more general case where each bin may contain at most k
parts of items. We show that this general case is also strongly NP-hard.
Additionally, we give an efficient 7/5-approximation algorithm."
The paper has been withdrawn due to an error in Lemma 1.
"We introduce the concept of knowledge states; many well-known algorithms can
be viewed as knowledge state algorithms. The knowledge state approach can be
used to to construct competitive randomized online algorithms and study the
tradeoff between competitiveness and memory. A knowledge state simply states
conditional obligations of an adversary, by fixing a work function, and gives a
distribution for the algorithm. When a knowledge state algorithm receives a
request, it then calculates one or more ""subsequent"" knowledge states, together
with a probability of transition to each. The algorithm then uses randomization
to select one of those subsequents to be the new knowledge state. We apply the
method to the paging problem. We present optimally competitive algorithm for
paging for the cases where the cache sizes are k=2 and k=3. These algorithms
use only a very limited number of bookmarks."
"For high volume data streams and large data warehouses, sampling is used for
efficient approximate answers to aggregate queries over selected subsets.
Mathematically, we are dealing with a set of weighted items and want to support
queries to arbitrary subset sums. With unit weights, we can compute subset
sizes which together with the previous sums provide the subset averages. The
question addressed here is which sampling scheme we should use to get the most
accurate subset sum estimates.
  We present a simple theorem on the variance of subset sum estimation and use
it to prove variance optimality and near-optimality of subset sum estimation
with different known sampling schemes. This variance is measured as the average
over all subsets of any given size. By optimal we mean there is no set of input
weights for which any sampling scheme can have a better average variance. Such
powerful results can never be established experimentally. The results of this
paper are derived mathematically. For example, we show that appropriately
weighted systematic sampling is simultaneously optimal for all subset sizes.
More standard schemes such as uniform sampling and
probability-proportional-to-size sampling with replacement can be arbitrarily
bad.
  Knowing the variance optimality of different sampling schemes can help
deciding which sampling scheme to apply in a given context."
"We consider two optimization problems related to finding dense subgraphs. The
densest at-least-k-subgraph problem (DalkS) is to find an induced subgraph of
highest average degree among all subgraphs with at least k vertices, and the
densest at-most-k-subgraph problem (DamkS) is defined similarly. These problems
are related to the well-known densest k-subgraph problem (DkS), which is to
find the densest subgraph on exactly k vertices. We show that DalkS can be
approximated efficiently, while DamkS is nearly as hard to approximate as the
densest k-subgraph problem."
"The problem of computing the chromatic number of a $P_5$-free graph is known
to be NP-hard. In contrast to this negative result, we show that determining
whether or not a $P_5$-free graph admits a $k$-colouring, for each fixed number
of colours $k$, can be done in polynomial time. If such a colouring exists, our
algorithm produces it."
"A {\em leader election} algorithm is an elimination process that divides
recursively into tow subgroups an initial group of n items, eliminates one
subgroup and continues the procedure until a subgroup is of size 1. In this
paper the biased case is analyzed. We are interested in the {\em cost} of the
algorithm, i.e. the number of operations needed until the algorithm stops.
Using a probabilistic approach, the asymptotic behavior of the algorithm is
shown to be related to the behavior of a hitting time of two random sequences
on [0,1]."
"Let $v$ be a vertex of a graph $G$. By the local complementation of $G$ at
$v$ we mean to complement the subgraph induced by the neighbors of $v$. This
operator can be generalized as follows. Assume that, each edge of $G$ has a
label in the finite field $\mathbf{F}_q$. Let $(g_{ij})$ be set of labels
($g_{ij}$ is the label of edge $ij$). We define two types of operators. For the
first one, let $v$ be a vertex of $G$ and $a\in \mathbf{F}_q$, and obtain the
graph with labels $g'_{ij}=g_{ij}+ag_{vi}g_{vj}$. For the second, if $0\neq
b\in \mathbf{F}_q$ the resulted graph is a graph with labels $g''_{vi}=bg_{vi}$
and $g''_{ij}=g_{ij}$, for $i,j$ unequal to $v$. It is clear that if the field
is binary, the operators are just local complementations that we described.
  The problem of whether two graphs are equivalent under local complementations
has been studied, \cite{bouchalg}. Here we consider the general case and
assuming that $q$ is odd, present the first known efficient algorithm to verify
whether two graphs are locally equivalent or not."
"A streaming model is one where data items arrive over long period of time,
either one item at a time or in bursts. Typical tasks include computing various
statistics over a sliding window of some fixed time-horizon. What makes the
streaming model interesting is that as the time progresses, old items expire
and new ones arrive. One of the simplest and central tasks in this model is
sampling. That is, the task of maintaining up to $k$ uniformly distributed
items from a current time-window as old items expire and new ones arrive. We
call sampling algorithms {\bf succinct} if they use provably optimal (up to
constant factors) {\bf worst-case} memory to maintain $k$ items (either with or
without replacement). We stress that in many applications structures that have
{\em expected} succinct representation as the time progresses are not
sufficient, as small probability events eventually happen with probability 1.
Thus, in this paper we ask the following question: are Succinct Sampling on
Streams (or $S^3$-algorithms)possible, and if so for what models? Perhaps
somewhat surprisingly, we show that $S^3$-algorithms are possible for {\em all}
variants of the problem mentioned above, i.e. both with and without replacement
and both for one-at-a-time and bursty arrival models. Finally, we use $S^3$
algorithms to solve various problems in sliding windows model, including
frequency moments, counting triangles, entropy and density estimations. For
these problems we present \emph{first} solutions with provable worst-case
memory guarantees."
"The DIMACS 32-bit parity problem is a satisfiability (SAT) problem hard to
solve. So far, EqSatz by Li is the only solver which can solve this problem.
However, This solver is very slow. It is reported that it spent 11855 seconds
to solve a par32-5 instance on a Maxintosh G3 300 MHz. The paper introduces a
new solver, XORSAT, which splits the original problem into two parts:
structured part and random part, and then solves separately them with WalkSAT
and an XOR equation solver. Based our empirical observation, XORSAT is
surprisingly fast, which is approximately 1000 times faster than EqSatz. For a
par32-5 instance, XORSAT took 2.9 seconds, while EqSatz took 2844 seconds on
Intel Pentium IV 2.66GHz CPU. We believe that this method significantly
different from traditional methods is also useful beyond this domain."
"We obtain a 1.5-approximation algorithm for the metric uncapacitated facility
location problem (UFL), which improves on the previously best known
1.52-approximation algorithm by Mahdian, Ye and Zhang. Note, that the
approximability lower bound by Guha and Khuller is 1.463.
  An algorithm is a {\em ($\lambda_f$,$\lambda_c$)-approximation algorithm} if
the solution it produces has total cost at most $\lambda_f \cdot F^* +
\lambda_c \cdot C^*$, where $F^*$ and $C^*$ are the facility and the connection
cost of an optimal solution. Our new algorithm, which is a modification of the
$(1+2/e)$-approximation algorithm of Chudak and Shmoys, is a
(1.6774,1.3738)-approximation algorithm for the UFL problem and is the first
one that touches the approximability limit curve $(\gamma_f, 1+2e^{-\gamma_f})$
established by Jain, Mahdian and Saberi. As a consequence, we obtain the first
optimal approximation algorithm for instances dominated by connection costs.
When combined with a (1.11,1.7764)-approximation algorithm proposed by Jain et
al., and later analyzed by Mahdian et al., we obtain the overall approximation
guarantee of 1.5 for the metric UFL problem. We also describe how to use our
algorithm to improve the approximation ratio for the 3-level version of UFL."
"NLC-width is a variant of clique-width with many application in graph
algorithmic. This paper is devoted to graphs of NLC-width two. After giving new
structural properties of the class, we propose a $O(n^2 m)$-time algorithm,
improving Johansson's algorithm \cite{Johansson00}. Moreover, our alogrithm is
simple to understand. The above properties and algorithm allow us to propose a
robust $O(n^2 m)$-time isomorphism algorithm for NLC-2 graphs. As far as we
know, it is the first polynomial-time algorithm."
"Tag clouds provide an aggregate of tag-usage statistics. They are typically
sent as in-line HTML to browsers. However, display mechanisms suited for
ordinary text are not ideal for tags, because font sizes may vary widely on a
line. As well, the typical layout does not account for relationships that may
be known between tags. This paper presents models and algorithms to improve the
display of tag clouds that consist of in-line HTML, as well as algorithms that
use nested tables to achieve a more general 2-dimensional layout in which tag
relationships are considered. The first algorithms leverage prior work in
typesetting and rectangle packing, whereas the second group of algorithms
leverage prior work in Electronic Design Automation. Experiments show our
algorithms can be efficiently implemented and perform well."
"In this paper, we introduce the on-line Viterbi algorithm for decoding hidden
Markov models (HMMs) in much smaller than linear space. Our analysis on
two-state HMMs suggests that the expected maximum memory used to decode
sequence of length $n$ with $m$-state HMM can be as low as $\Theta(m\log n)$,
without a significant slow-down compared to the classical Viterbi algorithm.
Classical Viterbi algorithm requires $O(mn)$ space, which is impractical for
analysis of long DNA sequences (such as complete human genome chromosomes) and
for continuous data streams. We also experimentally demonstrate the performance
of the on-line Viterbi algorithm on a simple HMM for gene finding on both
simulated and real DNA sequences."
"A new incremental algorithm for data compression is presented. For a sequence
of input symbols algorithm incrementally constructs a p-adic integer number as
an output. Decoding process starts with less significant part of a p-adic
integer and incrementally reconstructs a sequence of input symbols. Algorithm
is based on certain features of p-adic numbers and p-adic norm. p-adic coding
algorithm may be considered as of generalization a popular compression
technique - arithmetic coding algorithms. It is shown that for p = 2 the
algorithm works as integer variant of arithmetic coding; for a special class of
models it gives exactly the same codes as Huffman's algorithm, for another
special model and a specific alphabet it gives Golomb-Rice codes."
"We introduce the straggler identification problem, in which an algorithm must
determine the identities of the remaining members of a set after it has had a
large number of insertion and deletion operations performed on it, and now has
relatively few remaining members. The goal is to do this in o(n) space, where n
is the total number of identities. The straggler identification problem has
applications, for example, in determining the set of unacknowledged packets in
a high-bandwidth multicast data stream. We provide a deterministic solution to
the straggler identification problem that uses only O(d log n) bits and is
based on a novel application of Newton's identities for symmetric polynomials.
This solution can identify any subset of d stragglers from a set of n O(log
n)-bit identifiers, assuming that there are no false deletions of identities
not already in the set. Indeed, we give a lower bound argument that shows that
any small-space deterministic solution to the straggler identification problem
cannot be guaranteed to handle false deletions. Nevertheless, we show that
there is a simple randomized solution using O(d log n log(1/epsilon)) bits that
can maintain a multiset and solve the straggler identification problem,
tolerating false deletions, where epsilon>0 is a user-defined parameter
bounding the probability of an incorrect response. This randomized solution is
based on a new type of Bloom filter, which we call the invertible Bloom filter."
"This paper describes an approach for obtaining direct access to the attacked
squares of sliding pieces without resorting to rotated bitboards. The technique
involves creating four hash tables using the built in hash arrays from an
interpreted, high level language. The rank, file, and diagonal occupancy are
first isolated by masking the desired portion of the board. The attacked
squares are then directly retrieved from the hash tables. Maintaining
incrementally updated rotated bitboards becomes unnecessary as does all the
updating, mapping and shifting required to access the attacked squares.
Finally, rotated bitboard move generation speed is compared with that of the
direct hash table lookup method."
"This paper presents a hybrid approach to spatial indexing of two dimensional
data. It sheds new light on the age old problem by thinking of the traditional
algorithms as working with images. Inspiration is drawn from an analogous
situation that is found in machine and human vision. Image processing
techniques are used to assist in the spatial indexing of the data. A fixed grid
approach is used and bins with too many records are sub-divided hierarchically.
Search queries are pre-computed for bins that do not contain any data records.
This has the effect of dividing the search space up into non rectangular
regions which are based on the spatial properties of the data. The bucketing
quad tree can be considered as an image with a resolution of two by two for
each layer. The results show that this method performs better than the quad
tree if there are more divisions per layer. This confirms our suspicions that
the algorithm works better if it gets to look at the data with higher
resolution images. An elegant class structure is developed where the
implementation of concrete spatial indexes for a particular data type merely
relies on rendering the data onto an image."
"We show how to test whether a graph with n vertices and m edges is a partial
cube, and if so how to find a distance-preserving embedding of the graph into a
hypercube, in the near-optimal time bound O(n^2), improving previous O(nm)-time
solutions."
"In this paper we consider module-composed graphs, i.e. graphs which can be
defined by a sequence of one-vertex insertions v_1,...,v_n, such that the
neighbourhood of vertex v_i, 2<= i<= n, forms a module (a homogeneous set) of
the graph defined by vertices v_1,..., v_{i-1}.
  We show that module-composed graphs are HHDS-free and thus homogeneously
orderable, weakly chordal, and perfect. Every bipartite distance hereditary
graph, every (co-2C_4,P_4)-free graph and thus every trivially perfect graph is
module-composed. We give an O(|V_G|(|V_G|+|E_G|)) time algorithm to decide
whether a given graph G is module-composed and construct a corresponding
module-sequence.
  For the case of bipartite graphs, module-composed graphs are exactly distance
hereditary graphs, which implies simple linear time algorithms for their
recognition and construction of a corresponding module-sequence."
"Setcover greedy algorithm is a natural approximation algorithm for test set
problem. This paper gives a precise and tighter analysis of performance
guarantee of this algorithm. The author improves the performance guarantee
$2\ln n$ which derives from set cover problem to $1.1354\ln n$ by applying the
potential function technique. In addition, the author gives a nontrivial lower
bound $1.0004609\ln n$ of performance guarantee of this algorithm. This lower
bound, together with the matching bound of information content heuristic,
confirms the fact information content heuristic is slightly better than
setcover greedy algorithm in worst case."
"We consider the well known \emph{Least Recently Used} (LRU) replacement
algorithm and analyze it under the independent reference model and generalized
power-law demand. For this extensive family of demand distributions we derive a
closed-form expression for the per object steady-state hit ratio. To the best
of our knowledge, this is the first analytic derivation of the per object hit
ratio of LRU that can be obtained in constant time without requiring laborious
numeric computations or simulation. Since most applications of replacement
algorithms include (at least) some scenarios under i.i.d. requests, our method
has substantial practical value, especially when having to analyze multiple
caches, where existing numeric methods and simulation become too time
consuming."
"We show that the absolute worst case time complexity for Hopcroft's
minimization algorithm applied to unary languages is reached only for de Bruijn
words. A previous paper by Berstel and Carton gave the example of de Bruijn
words as a language that requires O(n log n) steps by carefully choosing the
splitting sets and processing these sets in a FIFO mode. We refine the previous
result by showing that the Berstel/Carton example is actually the absolute
worst case time complexity in the case of unary languages. We also show that a
LIFO implementation will not achieve the same worst time complexity for the
case of unary languages. Lastly, we show that the same result is valid also for
the cover automata and a modification of the Hopcroft's algorithm, modification
used in minimization of cover automata."
"A quantum algorithm is a set of instructions for a quantum computer, however,
unlike algorithms in classical computer science their results cannot be
guaranteed. A quantum system can undergo two types of operation, measurement
and quantum state transformation, operations themselves must be unitary
(reversible). Most quantum algorithms involve a series of quantum state
transformations followed by a measurement. Currently very few quantum
algorithms are known and no general design methodology exists for their
construction."
"In this paper, we propose a useful replacement for quicksort-style utility
functions. The replacement is called Symmetry Partition Sort, which has
essentially the same principle as Proportion Extend Sort. The maximal
difference between them is that the new algorithm always places already
partially sorted inputs (used as a basis for the proportional extension) on
both ends when entering the partition routine. This is advantageous to speeding
up the partition routine. The library function based on the new algorithm is
more attractive than Psort which is a library function introduced in 2004. Its
implementation mechanism is simple. The source code is clearer. The speed is
faster, with O(n log n) performance guarantee. Both the robustness and
adaptivity are better. As a library function, it is competitive."
"We raise the question of approximating the compressibility of a string with
respect to a fixed compression scheme, in sublinear time. We study this
question in detail for two popular lossless compression schemes: run-length
encoding (RLE) and Lempel-Ziv (LZ), and present sublinear algorithms for
approximating compressibility with respect to both schemes. We also give
several lower bounds that show that our algorithms for both schemes cannot be
improved significantly.
  Our investigation of LZ yields results whose interest goes beyond the initial
questions we set out to study. In particular, we prove combinatorial structural
lemmas that relate the compressibility of a string with respect to Lempel-Ziv
to the number of distinct short substrings contained in it. In addition, we
show that approximating the compressibility with respect to LZ is related to
approximating the support size of a distribution."
"We develop an experimental algorithm for the exact solving of the maximum
independent set problem. The algorithm consecutively finds the maximal
independent sets of vertices in an arbitrary undirected graph such that the
next such set contains more elements than the preceding one. For this purpose,
we use a technique, developed by Ford and Fulkerson for the finite partially
ordered sets, in particular, their method for partition of a poset into the
minimum number of chains with finding the maximum antichain. In the process of
solving, a special digraph is constructed, and a conjecture is formulated
concerning properties of such digraph. This allows to offer of the solution
algorithm. Its theoretical estimation of running time equals to is $O(n^{8})$,
where $n$ is the number of graph vertices. The offered algorithm was tested by
a program on random graphs. The testing the confirms correctness of the
algorithm."
"It is well known that n integers in the range [1,n^c] can be sorted in O(n)
time in the RAM model using radix sorting. More generally, integers in any
range [1,U] can be sorted in O(n sqrt{loglog n}) time. However, these
algorithms use O(n) words of extra memory. Is this necessary?
  We present a simple, stable, integer sorting algorithm for words of size
O(log n), which works in O(n) time and uses only O(1) words of extra memory on
a RAM model. This is the integer sorting case most useful in practice. We
extend this result with same bounds to the case when the keys are read-only,
which is of theoretical interest. Another interesting question is the case of
arbitrary c. Here we present a black-box transformation from any RAM sorting
algorithm to a sorting algorithm which uses only O(1) extra space and has the
same running time. This settles the complexity of in-place sorting in terms of
the complexity of sorting."
"We study the problem of assigning jobs to applicants. Each applicant has a
weight and provides a preference list ranking a subset of the jobs. A matching
M is popular if there is no other matching M' such that the weight of the
applicants who prefer M' over M exceeds the weight of those who prefer M over
M'. This paper gives efficient algorithms to find a popular matching if one
exists."
"The k-forest problem is a common generalization of both the k-MST and the
dense-$k$-subgraph problems. Formally, given a metric space on $n$ vertices
$V$, with $m$ demand pairs $\subseteq V \times V$ and a ``target'' $k\le m$,
the goal is to find a minimum cost subgraph that connects at least $k$ demand
pairs. In this paper, we give an $O(\min\{\sqrt{n},\sqrt{k}\})$-approximation
algorithm for $k$-forest, improving on the previous best ratio of
$O(n^{2/3}\log n)$ by Segev & Segev.
  We then apply our algorithm for k-forest to obtain approximation algorithms
for several Dial-a-Ride problems. The basic Dial-a-Ride problem is the
following: given an $n$ point metric space with $m$ objects each with its own
source and destination, and a vehicle capable of carrying at most $k$ objects
at any time, find the minimum length tour that uses this vehicle to move each
object from its source to destination. We prove that an $\alpha$-approximation
algorithm for the $k$-forest problem implies an
$O(\alpha\cdot\log^2n)$-approximation algorithm for Dial-a-Ride. Using our
results for $k$-forest, we get an $O(\min\{\sqrt{n},\sqrt{k}\}\cdot\log^2 n)$-
approximation algorithm for Dial-a-Ride. The only previous result known for
Dial-a-Ride was an $O(\sqrt{k}\log n)$-approximation by Charikar &
Raghavachari; our results give a different proof of a similar approximation
guarantee--in fact, when the vehicle capacity $k$ is large, we give a slight
improvement on their results."
"In this paper we study noisy sorting without re-sampling. In this problem
there is an unknown order $a_{\pi(1)} < ... < a_{\pi(n)}$ where $\pi$ is a
permutation on $n$ elements. The input is the status of $n \choose 2$ queries
of the form $q(a_i,x_j)$, where $q(a_i,a_j) = +$ with probability at least
$1/2+\ga$ if $\pi(i) > \pi(j)$ for all pairs $i \neq j$, where $\ga > 0$ is a
constant and $q(a_i,a_j) = -q(a_j,a_i)$ for all $i$ and $j$. It is assumed that
the errors are independent. Given the status of the queries the goal is to find
the maximum likelihood order. In other words, the goal is find a permutation
$\sigma$ that minimizes the number of pairs $\sigma(i) > \sigma(j)$ where
$q(\sigma(i),\sigma(j)) = -$. The problem so defined is the feedback arc set
problem on distributions of inputs, each of which is a tournament obtained as a
noisy perturbations of a linear order. Note that when $\ga < 1/2$ and $n$ is
large, it is impossible to recover the original order $\pi$.
  It is known that the weighted feedback are set problem on tournaments is
NP-hard in general. Here we present an algorithm of running time
$n^{O(\gamma^{-4})}$ and sampling complexity $O_{\gamma}(n \log n)$ that with
high probability solves the noisy sorting without re-sampling problem. We also
show that if $a_{\sigma(1)},a_{\sigma(2)},...,a_{\sigma(n)}$ is an optimal
solution of the problem then it is ``close'' to the original order. More
formally, with high probability it holds that $\sum_i |\sigma(i) - \pi(i)| =
\Theta(n)$ and $\max_i |\sigma(i) - \pi(i)| = \Theta(\log n)$.
  Our results are of interest in applications to ranking, such as ranking in
sports, or ranking of search items based on comparisons by experts."
"The Lp regression problem takes as input a matrix $A \in \Real^{n \times d}$,
a vector $b \in \Real^n$, and a number $p \in [1,\infty)$, and it returns as
output a number ${\cal Z}$ and a vector $x_{opt} \in \Real^d$ such that ${\cal
Z} = \min_{x \in \Real^d} ||Ax -b||_p = ||Ax_{opt}-b||_p$. In this paper, we
construct coresets and obtain an efficient two-stage sampling-based
approximation algorithm for the very overconstrained ($n \gg d$) version of
this classical problem, for all $p \in [1, \infty)$. The first stage of our
algorithm non-uniformly samples $\hat{r}_1 = O(36^p d^{\max\{p/2+1, p\}+1})$
rows of $A$ and the corresponding elements of $b$, and then it solves the Lp
regression problem on the sample; we prove this is an 8-approximation. The
second stage of our algorithm uses the output of the first stage to resample
$\hat{r}_1/\epsilon^2$ constraints, and then it solves the Lp regression
problem on the new sample; we prove this is a $(1+\epsilon)$-approximation. Our
algorithm unifies, improves upon, and extends the existing algorithms for
special cases of Lp regression, namely $p = 1,2$. In course of proving our
result, we develop two concepts--well-conditioned bases and subspace-preserving
sampling--that are of independent interest."
"We introduce a new technique to bound the asymptotic performance of splay
trees. The basic idea is to transcribe, in an indirect fashion, the rotations
performed by the splay tree as a Davenport-Schinzel sequence S, none of whose
subsequences are isomorphic to fixed forbidden subsequence. We direct this
technique towards Tarjan's deque conjecture and prove that n deque operations
require O(n alpha^*(n)) time, where alpha^*(n) is the minimum number of
applications of the inverse-Ackermann function mapping n to a constant. We are
optimistic that this approach could be directed towards other open conjectures
on splay trees such as the traversal and split conjectures."
"A pair of complementary algorithms are presented. One of the pair is a fast
method for connecting graphs with an edge. The other is a fast method for
removing edges from a graph. Both algorithms employ the same tree based graph
representation and so, in concert, can arbitrarily modify any graph. Since the
clusters of a percolation model may be described as simple connected graphs, an
efficient Monte Carlo scheme can be constructed that uses the algorithms to
sweep the occupation probability back and forth between two turning points.
This approach concentrates computational sampling time within a region of
interest. A high precision value of pc = 0.59274603(9) was thus obtained, by
Mersenne twister, for the two dimensional square site percolation threshold."
"Tree structures are very often used data structures. Among ordered types of
trees there are many variants whose basic operations such as insert, delete,
search, delete-min are characterized by logarithmic time complexity. In the
article I am going to present the structure whose time complexity for each of
the above operations is $O(\frac{M}{K} + K)$, where M is the size of data type
and K is constant properly matching the size of data type. Properly matched K
will make the structure function as a very effective Priority Queue. The
structure size linearly depends on the number and size of elements. PTrie is a
clever combination of the idea of prefix tree -- Trie, structure of logarithmic
time complexity for insert and remove operations, doubly linked list and
queues."
"In this paper I present general outlook on questions relevant to the basic
graph algorithms; Finding the Shortest Path with Positive Weights and Minimum
Spanning Tree. I will show so far known solution set of basic graph problems
and present my own. My solutions to graph problems are characterized by their
linear worst-case time complexity. It should be noticed that the algorithms
which compute the Shortest Path and Minimum Spanning Tree problems not only
analyze the weight of arcs (which is the main and often the only criterion of
solution hitherto known algorithms) but also in case of identical path weights
they select this path which walks through as few vertices as possible. I have
presented algorithms which use priority queue based on multilevel prefix tree
-- PTrie. PTrie is a clever combination of the idea of prefix tree -- Trie, the
structure of logarithmic time complexity for insert and remove operations,
doubly linked list and queues. In C++ I will implement linear worst-case time
algorithm computing the Single-Destination Shortest-Paths problem and I will
explain its usage."
"Many data analysis applications deal with large matrices and involve
approximating the matrix using a small number of ``components.'' Typically,
these components are linear combinations of the rows and columns of the matrix,
and are thus difficult to interpret in terms of the original features of the
input data. In this paper, we propose and study matrix approximations that are
explicitly expressed in terms of a small number of columns and/or rows of the
data matrix, and thereby more amenable to interpretation in terms of the
original data. Our main algorithmic results are two randomized algorithms which
take as input an $m \times n$ matrix $A$ and a rank parameter $k$. In our first
algorithm, $C$ is chosen, and we let $A'=CC^+A$, where $C^+$ is the
Moore-Penrose generalized inverse of $C$. In our second algorithm $C$, $U$, $R$
are chosen, and we let $A'=CUR$. ($C$ and $R$ are matrices that consist of
actual columns and rows, respectively, of $A$, and $U$ is a generalized inverse
of their intersection.) For each algorithm, we show that with probability at
least $1-\delta$: $$ ||A-A'||_F \leq (1+\epsilon) ||A-A_k||_F, $$ where $A_k$
is the ``best'' rank-$k$ approximation provided by truncating the singular
value decomposition (SVD) of $A$. The number of columns of $C$ and rows of $R$
is a low-degree polynomial in $k$, $1/\epsilon$, and $\log(1/\delta)$. Our two
algorithms are the first polynomial time algorithms for such low-rank matrix
approximations that come with relative-error guarantees; previously, in some
cases, it was not even known whether such matrix decompositions exist. Both of
our algorithms are simple, they take time of the order needed to approximately
compute the top $k$ singular vectors of $A$, and they use a novel, intuitive
sampling method called ``subspace sampling.''"
"We study the design of efficient algorithms for combinatorial pattern
matching. More concretely, we study algorithms for tree matching, string
matching, and string matching in compressed texts."
"There is a growing body of work on sorting and selection in models other than
the unit-cost comparison model. This work is the first treatment of a natural
stochastic variant of the problem where the cost of comparing two elements is a
random variable. Each cost is chosen independently and is known to the
algorithm. In particular we consider the following three models: each cost is
chosen uniformly in the range $[0,1]$, each cost is 0 with some probability $p$
and 1 otherwise, or each cost is 1 with probability $p$ and infinite otherwise.
We present lower and upper bounds (optimal in most cases) for these problems.
We obtain our upper bounds by carefully designing algorithms to ensure that the
costs incurred at various stages are independent and using properties of random
partial orders when appropriate."
"Least squares approximation is a technique to find an approximate solution to
a system of linear equations that has no exact solution. In a typical setting,
one lets $n$ be the number of constraints and $d$ be the number of variables,
with $n \gg d$. Then, existing exact methods find a solution vector in
$O(nd^2)$ time. We present two randomized algorithms that provide very accurate
relative-error approximations to the optimal value and the solution vector of a
least squares approximation problem more rapidly than existing exact
algorithms. Both of our algorithms preprocess the data with the Randomized
Hadamard Transform. One then uniformly randomly samples constraints and solves
the smaller problem on those constraints, and the other performs a sparse
random projection and solves the smaller problem on those projected
coordinates. In both cases, solving the smaller problem provides relative-error
approximations, and, if $n$ is sufficiently larger than $d$, the approximate
solution can be computed in $O(nd \log d)$ time."
"We address the problem of minimizing power consumption when performing
reliable broadcast on a radio network under the following popular model. Each
node in the network is located on a point in a two dimensional grid, and
whenever a node sends a message, all awake nodes within distance r receive the
message. In the broadcast problem, some node wants to successfully send a
message to all other nodes in the network even when up to a 1/2 fraction of the
nodes within every neighborhood can be deleted by an adversary. The set of
deleted nodes is carefully chosen by the adversary to foil our algorithm and
moreover, the set of deleted nodes may change periodically. This models
worst-case behavior due to mobile nodes, static nodes losing power or simply
some points in the grid being unoccupied. A trivial solution requires each node
in the network to be awake roughly 1/2 the time, and a trivial lower bound
shows that each node must be awake for at least a 1/n fraction of the time. Our
first result is an algorithm that requires each node to be awake for only a
1/sqrt(n) fraction of the time in expectation. Our algorithm achieves this
while ensuring correctness with probability 1, and keeping optimal values for
other resource costs such as latency and number of messages sent. We give a
lower-bound that shows that this reduction in power consumption is
asymptotically optimal when latency and number of messages sent must be
optimal. If we can increase the latency and messages sent by only a log*n
factor we give a Las Vegas algorithm that requires each node to be awake for
only a (log*n)/n expected fraction of the time; we give a lower-bound showing
that this second algorithm is near optimal. Finally, we show how to ensure
energy-efficient broadcast in the presence of Byzantine faults."
"We give a new algorithm for performing the distinct-degree factorization of a
polynomial P(x) over GF(2), using a multi-level blocking strategy. The coarsest
level of blocking replaces GCD computations by multiplications, as suggested by
Pollard (1975), von zur Gathen and Shoup (1992), and others. The novelty of our
approach is that a finer level of blocking replaces multiplications by
squarings, which speeds up the computation in GF(2)[x]/P(x) of certain interval
polynomials when P(x) is sparse. As an application we give a fast algorithm to
search for all irreducible trinomials x^r + x^s + 1 of degree r over GF(2),
while producing a certificate that can be checked in less time than the full
search. Naive algorithms cost O(r^2) per trinomial, thus O(r^3) to search over
all trinomials of given degree r. Under a plausible assumption about the
distribution of factors of trinomials, the new algorithm has complexity O(r^2
(log r)^{3/2}(log log r)^{1/2}) for the search over all trinomials of degree r.
Our implementation achieves a speedup of greater than a factor of 560 over the
naive algorithm in the case r = 24036583 (a Mersenne exponent). Using our
program, we have found two new primitive trinomials of degree 24036583 over
GF(2) (the previous record degree was 6972593)."
"We present two algorithms for maintaining the topological order of a directed
acyclic graph with n vertices, under an online edge insertion sequence of m
edges. Efficient algorithms for online topological ordering have many
applications, including online cycle detection, which is to discover the first
edge that introduces a cycle under an arbitrary sequence of edge insertions in
a directed graph. In this paper we present efficient algorithms for the online
topological ordering problem.
  We first present a simple algorithm with running time O(n^{5/2}) for the
online topological ordering problem. This is the current fastest algorithm for
this problem on dense graphs, i.e., when m > n^{5/3}. We then present an
algorithm with running time O((m + nlog n)\sqrt{m}); this is more efficient for
sparse graphs. Our results yield an improved upper bound of O(min(n^{5/2}, (m +
nlog n)sqrt{m})) for the online topological ordering problem."
"Motivated by an application in computational topology, we consider a novel
variant of the problem of efficiently maintaining dynamic rooted trees. This
variant requires merging two paths in a single operation. In contrast to the
standard problem, in which only one tree arc changes at a time, a single merge
operation can change many arcs. In spite of this, we develop a data structure
that supports merges on an n-node forest in O(log^2 n) amortized time and all
other standard tree operations in O(log n) time (amortized, worst-case, or
randomized depending on the underlying data structure). For the special case
that occurs in the motivating application, in which arbitrary arc deletions
(cuts) are not allowed, we give a data structure with an O(log n) time bound
per operation. This is asymptotically optimal under certain assumptions. For
the even-more special case in which both cuts and parent queries are
disallowed, we give an alternative O(log n)-time solution that uses standard
dynamic trees as a black box. This solution also applies to the motivating
application. Our methods use previous work on dynamic trees in various ways,
but the analysis of each algorithm requires novel ideas. We also investigate
lower bounds for the problem under various assumptions."
"We present approximation algorithms for almost all variants of the
multi-criteria traveling salesman problem (TSP).
  First, we devise randomized approximation algorithms for multi-criteria
maximum traveling salesman problems (Max-TSP). For multi-criteria Max-STSP,
where the edge weights have to be symmetric, we devise an algorithm with an
approximation ratio of 2/3 - eps. For multi-criteria Max-ATSP, where the edge
weights may be asymmetric, we present an algorithm with a ratio of 1/2 - eps.
Our algorithms work for any fixed number k of objectives. Furthermore, we
present a deterministic algorithm for bi-criteria Max-STSP that achieves an
approximation ratio of 7/27.
  Finally, we present a randomized approximation algorithm for the asymmetric
multi-criteria minimum TSP with triangle inequality Min-ATSP. This algorithm
achieves a ratio of log n + eps."
"The Metric Traveling Salesman Problem (TSP) is a classical NP-hard
optimization problem. The double-tree shortcutting method for Metric TSP yields
an exponentially-sized space of TSP tours, each of which approximates the
optimal solution within at most a factor of 2. We consider the problem of
finding among these tours the one that gives the closest approximation, i.e.\
the \emph{minimum-weight double-tree shortcutting}. Previously, we gave an
efficient algorithm for this problem, and carried out its experimental
analysis. In this paper, we address the related question of the worst-case
approximation ratio for the minimum-weight double-tree shortcutting method. In
particular, we give lower bounds on the approximation ratio in some specific
metric spaces: the ratio of 2 in the discrete shortest path metric, 1.622 in
the planar Euclidean metric, and 1.666 in the planar Minkowski metric. The
first of these lower bounds is tight; we conjecture that the other two bounds
are also tight, and in particular that the minimum-weight double-tree method
provides a 1.622-approximation for planar Euclidean TSP."
"We consider the problem of finding a feasible single-commodity flow in a
strongly connected network with fixed supplies and demands, provided that the
sum of supplies equals the sum of demands and the minimum arc capacity is at
least this sum. A fast algorithm for this problem improves the worst-case time
bound of the Goldberg-Rao maximum flow method by a constant factor. Erlebach
and Hagerup gave an linear-time feasible flow algorithm. We give an arguably
simpler one."
"We propose a fully dynamic algorithm for maintaining reachability information
in directed graphs. The proposed deterministic dynamic algorithm has an update
time of $O((ins*n^{2}) + (del * (m+n*log(n))))$ where $m$ is the current number
of edges, $n$ is the number of vertices in the graph, $ins$ is the number of
edge insertions and $del$ is the number of edge deletions. Each query can be
answered in O(1) time after each update. The proposed algorithm combines
existing fully dynamic reachability algorithm with well known witness counting
technique to improve efficiency of maintaining reachability information when
edges are deleted. The proposed algorithm improves by a factor of
$O(\frac{n^2}{m+n*log(n)})$ for edge deletion over the best existing fully
dynamic algorithm for maintaining reachability information."
"The restless bandit problem is one of the most well-studied generalizations
of the celebrated stochastic multi-armed bandit problem in decision theory. In
its ultimate generality, the restless bandit problem is known to be PSPACE-Hard
to approximate to any non-trivial factor, and little progress has been made
despite its importance in modeling activity allocation under uncertainty.
  We consider a special case that we call Feedback MAB, where the reward
obtained by playing each of n independent arms varies according to an
underlying on/off Markov process whose exact state is only revealed when the
arm is played. The goal is to design a policy for playing the arms in order to
maximize the infinite horizon time average expected reward. This problem is
also an instance of a Partially Observable Markov Decision Process (POMDP), and
is widely studied in wireless scheduling and unmanned aerial vehicle (UAV)
routing. Unlike the stochastic MAB problem, the Feedback MAB problem does not
admit to greedy index-based optimal policies.
  We develop a novel and general duality-based algorithmic technique that
yields a surprisingly simple and intuitive 2+epsilon-approximate greedy policy
to this problem. We then define a general sub-class of restless bandit problems
that we term Monotone bandits, for which our policy is a 2-approximation. Our
technique is robust enough to handle generalizations of these problems to
incorporate various side-constraints such as blocking plays and switching
costs. This technique is also of independent interest for other restless bandit
problems. By presenting the first (and efficient) O(1) approximations for
non-trivial instances of restless bandits as well as of POMDPs, our work
initiates the study of approximation algorithms in both these contexts."
"Let ${\cal V}$ be a finite set of $n$ elements and ${\cal F}=\{X_1,X_2, >...,
X_m\}$ a family of $m$ subsets of ${\cal V}.$ Two sets $X_i$ and $X_j$ of
${\cal F}$ overlap if $X_i \cap X_j \neq \emptyset,$ $X_j \setminus X_i \neq
\emptyset,$ and $X_i \setminus X_j \neq \emptyset.$ Two sets $X,Y\in {\cal F}$
are in the same overlap class if there is a series $X=X_1,X_2, ..., X_k=Y$ of
sets of ${\cal F}$ in which each $X_iX_{i+1}$ overlaps. In this note, we focus
on efficiently identifying all overlap classes in $O(n+\sum_{i=1}^m |X_i|)$
time. We thus revisit the clever algorithm of Dahlhaus of which we give a clear
presentation and that we simplify to make it practical and implementable in its
real worst case complexity. An useful variant of Dahlhaus's approach is also
explained."
"Orienteering is the following optimization problem: given an edge-weighted
graph (directed or undirected), two nodes s,t and a time limit T, find an s-t
walk of total length at most T that maximizes the number of distinct nodes
visited by the walk. One obtains a generalization, namely orienteering with
time-windows (also referred to as TSP with time-windows), if each node v has a
specified time-window [R(v), D(v)] and a node v is counted as visited by the
walk only if v is visited during its time-window. For the time-window problem,
an O(\log \opt) approximation can be achieved even for directed graphs if the
algorithm is allowed quasi-polynomial time. However, the best known polynomial
time approximation ratios are O(\log^2 \opt) for undirected graphs and O(\log^4
\opt) in directed graphs. In this paper we make some progress towards closing
this discrepancy, and in the process obtain improved approximation ratios in
several natural settings. Let L(v) = D(v) - R(v) denote the length of the
time-window for v and let \lmax = \max_v L(v) and \lmin = \min_v L(v). Our
results are given below with \alpha denoting the known approximation ratio for
orienteering (without time-windows). Currently \alpha = (2+\eps) for undirected
graphs and \alpha = O(\log^2 \opt) in directed graphs.
  1. An O(\alpha \log \lmax) approximation when R(v) and D(v) are integer
valued for each v.
  2. An O(\alpha \max{\log \opt, \log \frac{\lmax}{\lmin}}) approximation.
  3. An O(\alpha \log \frac{\lmax}{\lmin}) approximation when no start and end
points are specified.
  In particular, if \frac{\lmax}{\lmin} is poly-bounded, we obtain an O(\log n)
approximation for the time-window problem in undirected graphs."
"We give an approximation algorithm for packing and covering linear programs
(linear programs with non-negative coefficients). Given a constraint matrix
with n non-zeros, r rows, and c columns, the algorithm computes feasible primal
and dual solutions whose costs are within a factor of 1+eps of the optimal cost
in time O((r+c)log(n)/eps^2 + n)."
"We study scheduling problems motivated by recently developed techniques for
microprocessor thermal management at the operating systems level. The general
scenario can be described as follows. The microprocessor's temperature is
controlled by the hardware thermal management system that continuously monitors
the chip temperature and automatically reduces the processor's speed as soon as
the thermal threshold is exceeded. Some tasks are more CPU-intensive than other
and thus generate more heat during execution. The cooling system operates
non-stop, reducing (at an exponential rate) the deviation of the processor's
temperature from the ambient temperature. As a result, the processor's
temperature, and thus the performance as well, depends on the order of the task
execution. Given a variety of possible underlying architectures, models for
cooling and for hardware thermal management, as well as types of tasks, this
scenario gives rise to a plethora of interesting and never studied scheduling
problems.
  We focus on scheduling real-time jobs in a simplified model for cooling and
thermal management. A collection of unit-length jobs is given, each job
specified by its release time, deadline and heat contribution. If, at some time
step, the temperature of the system is t and the processor executes a job with
heat contribution h, then the temperature at the next step is (t+h)/2. The
temperature cannot exceed the given thermal threshold T. The objective is to
maximize the throughput, that is, the number of tasks that meet their
deadlines. We prove that, in the offline case, computing the optimum schedule
is NP-hard, even if all jobs are released at the same time. In the online case,
we show a 2-competitive deterministic algorithm and a matching lower bound."
"This paper presents a new technique for deterministic length reduction. This
technique improves the running time of the algorithm presented in \cite{LR07}
for performing fast convolution in sparse data. While the regular fast
convolution of vectors $V_1,V_2$ whose sizes are $N_1,N_2$ respectively, takes
$O(N_1 \log N_2)$ using FFT, using the new technique for length reduction, the
algorithm proposed in \cite{LR07} performs the convolution in $O(n_1 \log^3
n_1)$, where $n_1$ is the number of non-zero values in $V_1$. The algorithm
assumes that $V_1$ is given in advance, and $V_2$ is given in running time. The
novel technique presented in this paper improves the convolution time to $O(n_1
\log^2 n_1)$ {\sl deterministically}, which equals the best running time given
achieved by a {\sl randomized} algorithm.
  The preprocessing time of the new technique remains the same as the
preprocessing time of \cite{LR07}, which is $O(n_1^2)$. This assumes and deals
the case where $N_1$ is polynomial in $n_1$. In the case where $N_1$ is
exponential in $n_1$, a reduction to a polynomial case can be used. In this
paper we also improve the preprocessing time of this reduction from $O(n_1^4)$
to $O(n_1^3{\rm polylog}(n_1))$."
"Many applications like pointer analysis and incremental compilation require
maintaining a topological ordering of the nodes of a directed acyclic graph
(DAG) under dynamic updates. All known algorithms for this problem are either
only analyzed for worst-case insertion sequences or only evaluated
experimentally on random DAGs. We present the first average-case analysis of
online topological ordering algorithms. We prove an expected runtime of O(n^2
polylog(n)) under insertion of the edges of a complete DAG in a random order
for the algorithms of Alpern et al. (SODA, 1990), Katriel and Bodlaender (TALG,
2006), and Pearce and Kelly (JEA, 2006). This is much less than the best known
worst-case bound O(n^{2.75}) for this problem."
"Let $T=t_0 ... t_{n-1}$ be a text and $P = p_0 ... p_{m-1}$ a pattern taken
from some finite alphabet set $\Sigma$, and let $\dist$ be a metric on
$\Sigma$. We consider the problem of calculating the sum of distances between
the symbols of $P$ and the symbols of substrings of $T$ of length $m$ for all
possible offsets. We present an $\epsilon$-approximation algorithm for this
problem which runs in time $O(\frac{1}{\epsilon^2}n\cdot
\mathrm{polylog}(n,\abs{\Sigma}))$"
"We study data structures in the presence of adversarial noise. We want to
encode a given object in a succinct data structure that enables us to
efficiently answer specific queries about the object, even if the data
structure has been corrupted by a constant fraction of errors. This new model
is the common generalization of (static) data structures and locally decodable
error-correcting codes. The main issue is the tradeoff between the space used
by the data structure and the time (number of probes) needed to answer a query
about the encoded object. We prove a number of upper and lower bounds on
various natural error-correcting data structure problems. In particular, we
show that the optimal length of error-correcting data structures for the
Membership problem (where we want to store subsets of size s from a universe of
size n) is closely related to the optimal length of locally decodable codes for
s-bit strings."
"We consider online competitive algorithms for the problem of collecting
weighted items from a dynamic set S, when items are added to or deleted from S
over time. The objective is to maximize the total weight of collected items. We
study the general version, as well as variants with various restrictions,
including the following: the uniform case, when all items have the same weight,
the decremental sets, when all items are present at the beginning and only
deletion operations are allowed, and dynamic queues, where the dynamic set is
ordered and only its prefixes can be deleted (with no restriction on
insertions). The dynamic queue case is a generalization of bounded-delay packet
scheduling (also referred to as buffer management). We present several upper
and lower bounds on the competitive ratio for these variants."
"Covering problems are fundamental classical problems in optimization,
computer science and complexity theory. Typically an input to these problems is
a family of sets over a finite universe and the goal is to cover the elements
of the universe with as few sets of the family as possible.
  The variations of covering problems include well known problems like Set
Cover, Vertex Cover, Dominating Set and Facility Location to name a few.
Recently there has been a lot of study on partial covering problems, a natural
generalization of covering problems. Here, the goal is not to cover all the
elements but to cover the specified number of elements with the minimum number
of sets.
  In this paper we study partial covering problems in graphs in the realm of
parameterized complexity. Classical (non-partial) version of all these problems
have been intensively studied in planar graphs and in graphs excluding a fixed
graph $H$ as a minor. However, the techniques developed for parameterized
version of non-partial covering problems cannot be applied directly to their
partial counterparts. The approach we use, to show that various partial
covering problems are fixed parameter tractable on planar graphs, graphs of
bounded local treewidth and graph excluding some graph as a minor, is quite
different from previously known techniques. The main idea behind our approach
is the concept of implicit branching. We find implicit branching technique to
be interesting on its own and believe that it can be used for some other
problems."
"We introduce a parameterized version of set cover that generalizes several
previously studied problems. Given a ground set V and a collection of subsets
S_i of V, a feasible solution is a partition of V such that each subset of the
partition is included in one of the S_i. The problem involves maximizing the
mean subset size of the partition, where the mean is the generalized mean of
parameter p, taken over the elements. For p=-1, the problem is equivalent to
the classical minimum set cover problem. For p=0, it is equivalent to the
minimum entropy set cover problem, introduced by Halperin and Karp. For p=1,
the problem includes the maximum-edge clique partition problem as a special
case. We prove that the greedy algorithm simultaneously approximates the
problem within a factor of (p+1)^1/p for any p in R^+, and that this is the
best possible unless P=NP. These results both generalize and simplify previous
results for special cases. We also consider the corresponding graph coloring
problem, and prove several tractability and inapproximability results. Finally,
we consider a further generalization of the set cover problem in which we aim
at minimizing the sum of some concave function of the part sizes. As an
application, we derive an approximation ratio for a Rent-or-Buy set cover
problem."
"In the k-2VC problem, we are given an undirected graph G with edge costs and
an integer k; the goal is to find a minimum-cost 2-vertex-connected subgraph of
G containing at least k vertices. A slightly more general version is obtained
if the input also specifies a subset S \subseteq V of terminals and the goal is
to find a subgraph containing at least k terminals. Closely related to the
k-2VC problem, and in fact a special case of it, is the k-2EC problem, in which
the goal is to find a minimum-cost 2-edge-connected subgraph containing k
vertices. The k-2EC problem was introduced by Lau et al., who also gave a
poly-logarithmic approximation for it. No previous approximation algorithm was
known for the more general k-2VC problem. We describe an O(\log n \log k)
approximation for the k-2VC problem."
"The measure and conquer approach has proven to be a powerful tool to analyse
exact algorithms for combinatorial problems, like Dominating Set and
Independent Set. In this paper, we propose to use measure and conquer also as a
tool in the design of algorithms. In an iterative process, we can obtain a
series of branch and reduce algorithms. A mathematical analysis of an algorithm
in the series with measure and conquer results in a quasiconvex programming
problem. The solution by computer to this problem not only gives a bound on the
running time, but also can give a new reduction rule, thus giving a new,
possibly faster algorithm. This makes design by measure and conquer a form of
computer aided algorithm design. When we apply the methodology to a Set Cover
modelling of the Dominating Set problem, we obtain the currently fastest known
exact algorithms for Dominating Set: an algorithm that uses $O(1.5134^n)$ time
and polynomial space, and an algorithm that uses $O(1.5063^n)$ time."
"In the Multislope Ski Rental problem, the user needs a certain resource for
some unknown period of time. To use the resource, the user must subscribe to
one of several options, each of which consists of a one-time setup cost
(``buying price''), and cost proportional to the duration of the usage
(``rental rate''). The larger the price, the smaller the rent. The actual usage
time is determined by an adversary, and the goal of an algorithm is to minimize
the cost by choosing the best option at any point in time. Multislope Ski
Rental is a natural generalization of the classical Ski Rental problem (where
the only options are pure rent and pure buy), which is one of the fundamental
problems of online computation. The Multislope Ski Rental problem is an
abstraction of many problems where online decisions cannot be modeled by just
two options, e.g., power management in systems which can be shut down in parts.
In this paper we study randomized algorithms for Multislope Ski Rental. Our
results include the best possible online randomized strategy for any additive
instance, where the cost of switching from one option to another is the
difference in their buying prices; and an algorithm that produces an
$e$-competitive randomized strategy for any (non-additive) instance."
"We provide the first non-trivial result on dynamic breadth-first search (BFS)
in external-memory: For general sparse undirected graphs of initially $n$ nodes
and O(n) edges and monotone update sequences of either $\Theta(n)$ edge
insertions or $\Theta(n)$ edge deletions, we prove an amortized
high-probability bound of $O(n/B^{2/3}+\sort(n)\cdot \log B)$ I/Os per update.
In contrast, the currently best approach for static BFS on sparse undirected
graphs requires $\Omega(n/B^{1/2}+\sort(n))$ I/Os."
"We study the scheduling problem on unrelated machines in the mechanism design
setting. This problem was proposed and studied in the seminal paper (Nisan and
Ronen 1999), where they gave a 1.75-approximation randomized truthful mechanism
for the case of two machines. We improve this result by a 1.6737-approximation
randomized truthful mechanism. We also generalize our result to a
$0.8368m$-approximation mechanism for task scheduling with $m$ machines, which
improve the previous best upper bound of $0.875m(Mu'alem and Schapira 2007)."
"We analyze a simple random process in which a token is moved in the interval
$A=\{0,...,n\$: Fix a probability distribution $\mu$ over $\{1,...,n\$.
Initially, the token is placed in a random position in $A$. In round $t$, a
random value $d$ is chosen according to $\mu$. If the token is in position
$a\geq d$, then it is moved to position $a-d$. Otherwise it stays put. Let $T$
be the number of rounds until the token reaches position 0. We show tight
bounds for the expectation of $T$ for the optimal distribution $\mu$. More
precisely, we show that $\min_\mu\{E_\mu(T)\=\Theta((\log n)^2)$. For the
proof, a novel potential function argument is introduced. The research is
motivated by the problem of approximating the minimum of a continuous function
over $[0,1]$ with a ``blind'' optimization strategy."
"We consider the minimum spanning tree problem in a setting where information
about the edge weights of the given graph is uncertain. Initially, for each
edge $e$ of the graph only a set $A_e$, called an uncertainty area, that
contains the actual edge weight $w_e$ is known. The algorithm can `update' $e$
to obtain the edge weight $w_e \in A_e$. The task is to output the edge set of
a minimum spanning tree after a minimum number of updates. An algorithm is
$k$-update competitive if it makes at most $k$ times as many updates as the
optimum. We present a 2-update competitive algorithm if all areas $A_e$ are
open or trivial, which is the best possible among deterministic algorithms. The
condition on the areas $A_e$ is to exclude degenerate inputs for which no
constant update competitive algorithm can exist. Next, we consider a setting
where the vertices of the graph correspond to points in Euclidean space and the
weight of an edge is equal to the distance of its endpoints. The location of
each point is initially given as an uncertainty area, and an update reveals the
exact location of the point. We give a general relation between the edge
uncertainty and the vertex uncertainty versions of a problem and use it to
derive a 4-update competitive algorithm for the minimum spanning tree problem
in the vertex uncertainty model. Again, we show that this is best possible
among deterministic algorithms."
"We consider the problem of constructing bounded-degree planar geometric
spanners of Euclidean and unit-disk graphs. It is well known that the Delaunay
subgraph is a planar geometric spanner with stretch factor $C_{del\approx
2.42$; however, its degree may not be bounded. Our first result is a very
simple linear time algorithm for constructing a subgraph of the Delaunay graph
with stretch factor $\rho =1+2\pi(k\cos{\frac{\pi{k)^{-1$ and degree bounded by
$k$, for any integer parameter $k\geq 14$. This result immediately implies an
algorithm for constructing a planar geometric spanner of a Euclidean graph with
stretch factor $\rho \cdot C_{del$ and degree bounded by $k$, for any integer
parameter $k\geq 14$. Moreover, the resulting spanner contains a Euclidean
Minimum Spanning Tree (EMST) as a subgraph. Our second contribution lies in
developing the structural results necessary to transfer our analysis and
algorithm from Euclidean graphs to unit disk graphs, the usual model for
wireless ad-hoc networks. We obtain a very simple distributed, {\em
strictly-localized algorithm that, given a unit disk graph embedded in the
plane, constructs a geometric spanner with the above stretch factor and degree
bound, and also containing an EMST as a subgraph. The obtained results
dramatically improve the previous results in all aspects, as shown in the
paper."
"Consider a set of labels $L$ and a set of trees ${\mathcal T} = \{{\mathcal
T}^{(1), {\mathcal T}^{(2), ..., {\mathcal T}^{(k) \$ where each tree
${\mathcal T}^{(i)$ is distinctly leaf-labeled by some subset of $L$. One
fundamental problem is to find the biggest tree (denoted as supertree) to
represent $\mathcal T}$ which minimizes the disagreements with the trees in
${\mathcal T}$ under certain criteria. This problem finds applications in
phylogenetics, database, and data mining. In this paper, we focus on two
particular supertree problems, namely, the maximum agreement supertree problem
(MASP) and the maximum compatible supertree problem (MCSP). These two problems
are known to be NP-hard for $k \geq 3$. This paper gives the first polynomial
time algorithms for both MASP and MCSP when both $k$ and the maximum degree $D$
of the trees are constant."
"We propose a dynamical process for network evolution, aiming at explaining
the emergence of the small world phenomenon, i.e., the statistical observation
that any pair of individuals are linked by a short chain of acquaintances
computable by a simple decentralized routing algorithm, known as greedy
routing. Previously proposed dynamical processes enabled to demonstrate
experimentally (by simulations) that the small world phenomenon can emerge from
local dynamics. However, the analysis of greedy routing using the probability
distributions arising from these dynamics is quite complex because of mutual
dependencies. In contrast, our process enables complete formal analysis. It is
based on the combination of two simple processes: a random walk process, and an
harmonic forgetting process. Both processes reflect natural behaviors of the
individuals, viewed as nodes in the network of inter-individual acquaintances.
We prove that, in k-dimensional lattices, the combination of these two
processes generates long-range links mutually independently distributed as a
k-harmonic distribution. We analyze the performances of greedy routing at the
stationary regime of our process, and prove that the expected number of steps
for routing from any source to any target in any multidimensional lattice is a
polylogarithmic function of the distance between the two nodes in the lattice.
Up to our knowledge, these results are the first formal proof that navigability
in small worlds can emerge from a dynamical process for network evolution. Our
dynamical process can find practical applications to the design of spatial
gossip and resource location protocols."
"From a high volume stream of weighted items, we want to maintain a generic
sample of a certain limited size $k$ that we can later use to estimate the
total weight of arbitrary subsets. This is the classic context of on-line
reservoir sampling, thinking of the generic sample as a reservoir. We present
an efficient reservoir sampling scheme, $\varoptk$, that dominates all previous
schemes in terms of estimation quality.
  $\varoptk$ provides {\em variance optimal unbiased estimation of subset
sums}. More precisely, if we have seen $n$ items of the stream, then for {\em
any} subset size $m$, our scheme based on $k$ samples minimizes the average
variance over all subsets of size $m$. In fact, the optimality is against any
off-line scheme with $k$ samples tailored for the concrete set of items seen.
In addition to optimal average variance, our scheme provides tighter worst-case
bounds on the variance of {\em particular} subsets than previously possible. It
is efficient, handling each new item of the stream in $O(\log k)$ time.
Finally, it is particularly well suited for combination of samples from
different streams in a distributed setting."
"We present an on-line algorithm for maintaining a topological order of a
directed acyclic graph as arcs are added, and detecting a cycle when one is
created. Our algorithm takes O(m^{1/2}) amortized time per arc, where m is the
total number of arcs. For sparse graphs, this bound improves the best previous
bound by a logarithmic factor and is tight to within a constant factor for a
natural class of algorithms that includes all the existing ones. Our main
insight is that the bidirectional search method of previous algorithms does not
require an ordered search, but can be more general. This allows us to avoid the
use of heaps (priority queues) entirely. Instead, the deterministic version of
our algorithm uses (approximate) median-finding. The randomized version of our
algorithm avoids this complication, making it very simple. We extend our
topological ordering algorithm to give the first detailed algorithm for
maintaining the strong components of a directed graph, and a topological order
of these components, as arcs are added. This extension also has an amortized
time bound of O(m^{1/2}) per arc."
"We present a nearly-linear time algorithm that produces high-quality
sparsifiers of weighted graphs. Given as input a weighted graph $G=(V,E,w)$ and
a parameter $\epsilon>0$, we produce a weighted subgraph
$H=(V,\tilde{E},\tilde{w})$ of $G$ such that $|\tilde{E}|=O(n\log
n/\epsilon^2)$ and for all vectors $x\in\R^V$ $(1-\epsilon)\sum_{uv\in
E}(x(u)-x(v))^2w_{uv}\le \sum_{uv\in\tilde{E}}(x(u)-x(v))^2\tilde{w}_{uv} \le
(1+\epsilon)\sum_{uv\in E}(x(u)-x(v))^2w_{uv}. (*)$
  This improves upon the sparsifiers constructed by Spielman and Teng, which
had $O(n\log^c n)$ edges for some large constant $c$, and upon those of
Bencz\'ur and Karger, which only satisfied (*) for $x\in\{0,1\}^V$.
  A key ingredient in our algorithm is a subroutine of independent interest: a
nearly-linear time algorithm that builds a data structure from which we can
query the approximate effective resistance between any two vertices in a graph
in $O(\log n)$ time."
"For a given graph G and integers b,f >= 0, let S be a subset of vertices of G
of size b+1 such that the subgraph of G induced by S is connected and S can be
separated from other vertices of G by removing f vertices. We prove that every
graph on n vertices contains at most n\binom{b+f}{b} such vertex subsets. This
result from extremal combinatorics appears to be very useful in the design of
several enumeration and exact algorithms. In particular, we use it to provide
algorithms that for a given n-vertex graph G - compute the treewidth of G in
time O(1.7549^n) by making use of exponential space and in time O(2.6151^n) and
polynomial space; - decide in time O(({\frac{2n+k+1}{3})^{k+1}\cdot kn^6}) if
the treewidth of G is at most k; - list all minimal separators of G in time
O(1.6181^n) and all potential maximal cliques of G in time O(1.7549^n). This
significantly improves previous algorithms for these problems."
"This article provides an overview of the performance and the theoretical
complexity of approximate and exact methods for various versions of the
shortest path problem. The proposed study aims to improve the resolution of a
more general covering problem within a column generation scheme in which the
shortest path problem is the sub-problem."
"We study the admission control problem in general networks. Communication
requests arrive over time, and the online algorithm accepts or rejects each
request while maintaining the capacity limitations of the network. The
admission control problem has been usually analyzed as a benefit problem, where
the goal is to devise an online algorithm that accepts the maximum number of
requests possible. The problem with this objective function is that even
algorithms with optimal competitive ratios may reject almost all of the
requests, when it would have been possible to reject only a few. This could be
inappropriate for settings in which rejections are intended to be rare events.
  In this paper, we consider preemptive online algorithms whose goal is to
minimize the number of rejected requests. Each request arrives together with
the path it should be routed on. We show an $O(\log^2 (mc))$-competitive
randomized algorithm for the weighted case, where $m$ is the number of edges in
the graph and $c$ is the maximum edge capacity. For the unweighted case, we
give an $O(\log m \log c)$-competitive randomized algorithm. This settles an
open question of Blum, Kalai and Kleinberg raised in \cite{BlKaKl01}. We note
that allowing preemption and handling requests with given paths are essential
for avoiding trivial lower bounds."
"A Bloom filter is a space efficient structure for storing static sets, where
the space efficiency is gained at the expense of a small probability of
false-positives. A Bloomier filter generalizes a Bloom filter to compactly
store a function with a static support. In this article we give a simple
construction of a Bloomier filter. The construction is linear in space and
requires constant time to evaluate. The creation of our Bloomier filter takes
linear time which is faster than the existing construction. We show how one can
improve the space utilization further at the cost of increasing the time for
creating the data structure."
"We examine several online matching problems, with applications to Internet
advertising reservation systems. Consider an edge-weighted bipartite graph G,
with partite sets L, R. We develop an 8-competitive algorithm for the following
secretary problem: Initially given R, and the size of L, the algorithm receives
the vertices of L sequentially, in a random order. When a vertex l \in L is
seen, all edges incident to l are revealed, together with their weights. The
algorithm must immediately either match l to an available vertex of R, or
decide that l will remain unmatched.
  Dimitrov and Plaxton show a 16-competitive algorithm for the transversal
matroid secretary problem, which is the special case with weights on vertices,
not edges. (Equivalently, one may assume that for each l \in L, the weights on
all edges incident to l are identical.) We use a similar algorithm, but
simplify and improve the analysis to obtain a better competitive ratio for the
more general problem. Perhaps of more interest is the fact that our analysis is
easily extended to obtain competitive algorithms for similar problems, such as
to find disjoint sets of edges in hypergraphs where edges arrive online. We
also introduce secretary problems with adversarially chosen groups. Finally, we
give a 2e-competitive algorithm for the secretary problem on graphic matroids,
where, with edges appearing online, the goal is to find a maximum-weight
acyclic subgraph of a given graph."
"In this paper two scheduling models are addressed. First is the standard
model (unicast) where requests (or jobs) are independent. The other is the
broadcast model where broadcasting a page can satisfy multiple outstanding
requests for that page. We consider online scheduling of requests when they
have deadlines. Unlike previous models, which mainly consider the objective of
maximizing throughput while respecting deadlines, here we focus on scheduling
all the given requests with the goal of minimizing the maximum {\em delay
factor}.We prove strong lower bounds on the achievable competitive ratios for
delay factor scheduling even with unit-time requests.For the unicast model we
give algorithms that are $(1 + \eps)$-speed $O({1 \over \eps})$-competitive in
both the single machine and multiple machine settings. In the broadcast model
we give an algorithm for similar-sized pages that is $(2+ \eps)$-speed $O({1
\over \eps^2})$-competitive. For arbitrary page sizes we give an algorithm that
is $(4+\eps)$-speed $O({1 \over \eps^2})$-competitive."
"Motivated by the Quality-of-Service (QoS) buffer management problem, we
consider online scheduling of packets with hard deadlines in a finite capacity
queue. At any time, a queue can store at most $b \in \mathbb Z^+$ packets.
Packets arrive over time. Each packet is associated with a non-negative value
and an integer deadline. In each time step, only one packet is allowed to be
sent. Our objective is to maximize the total value gained by the packets sent
by their deadlines in an online manner. Due to the Internet traffic's chaotic
characteristics, no stochastic assumptions are made on the packet input
sequences. This model is called a {\em finite-queue model}.
  We use competitive analysis to measure an online algorithm's performance
versus an unrealizable optimal offline algorithm who constructs the worst
possible input based on the knowledge of the online algorithm. For the
finite-queue model, we first present a deterministic 3-competitive memoryless
online algorithm. Then, we give a randomized ($\phi^2 = ((1 + \sqrt{5}) / 2)^2
\approx 2.618$)-competitive memoryless online algorithm.
  The algorithmic framework and its theoretical analysis include several
interesting features. First, our algorithms use (possibly) modified
characteristics of packets; these characteristics may not be same as those
specified in the input sequence. Second, our analysis method is different from
the classical potential function approach."
"The problem of approximate string matching is important in many different
areas such as computational biology, text processing and pattern recognition. A
great effort has been made to design efficient algorithms addressing several
variants of the problem, including comparison of two strings, approximate
pattern identification in a string or calculation of the longest common
subsequence that two strings share.
  We designed an output sensitive algorithm solving the edit distance problem
between two strings of lengths n and m respectively in time
O((s-|n-m|)min(m,n,s)+m+n) and linear space, where s is the edit distance
between the two strings. This worst-case time bound sets the quadratic factor
of the algorithm independent of the longest string length and improves existing
theoretical bounds for this problem. The implementation of our algorithm excels
also in practice, especially in cases where the two strings compared differ
significantly in length. Source code of our algorithm is available at
http://www.cs.miami.edu/\~dimitris/edit_distance"
"Finding the largest clique is a notoriously hard problem, even on random
graphs. It is known that the clique number of a random graph G(n,1/2) is almost
surely either k or k+1, where k = 2log n - 2log(log n) - 1. However, a simple
greedy algorithm finds a clique of size only (1+o(1))log n, with high
probability, and finding larger cliques -- that of size even (1+ epsilon)log n
-- in randomized polynomial time has been a long-standing open problem. In this
paper, we study the following generalization: given a random graph G(n,1/2),
find the largest subgraph with edge density at least (1-delta). We show that a
simple modification of the greedy algorithm finds a subset of 2log n vertices
whose induced subgraph has edge density at least 0.951, with high probability.
To complement this, we show that almost surely there is no subset of 2.784log n
vertices whose induced subgraph has edge density 0.951 or more."
"In this paper, we present approximation algorithms for combinatorial
optimization problems under probabilistic constraints. Specifically, we focus
on stochastic variants of two important combinatorial optimization problems:
the k-center problem and the set cover problem, with uncertainty characterized
by a probability distribution over set of points or elements to be covered. We
consider these problems under adaptive and non-adaptive settings, and present
efficient approximation algorithms for the case when underlying distribution is
a product distribution. In contrast to the expected cost model prevalent in
stochastic optimization literature, our problem definitions support
restrictions on the probability distributions of the total costs, via
incorporating constraints that bound the probability with which the incurred
costs may exceed a given threshold."
"The k-means method is a widely used clustering algorithm. One of its
distinguished features is its speed in practice. Its worst-case running-time,
however, is exponential, leaving a gap between practical and theoretical
performance. Arthur and Vassilvitskii (FOCS 2006) aimed at closing this gap,
and they proved a bound of $\poly(n^k, \sigma^{-1})$ on the smoothed
running-time of the k-means method, where n is the number of data points and
$\sigma$ is the standard deviation of the Gaussian perturbation. This bound,
though better than the worst-case bound, is still much larger than the
running-time observed in practice.
  We improve the smoothed analysis of the k-means method by showing two upper
bounds on the expected running-time of k-means. First, we prove that the
expected running-time is bounded by a polynomial in $n^{\sqrt k}$ and
$\sigma^{-1}$. Second, we prove an upper bound of $k^{kd} \cdot \poly(n,
\sigma^{-1})$, where d is the dimension of the data space. The polynomial is
independent of k and d, and we obtain a polynomial bound for the expected
running-time for $k, d \in O(\sqrt{\log n/\log \log n})$.
  Finally, we show that k-means runs in smoothed polynomial time for
one-dimensional instances."
"Recent work has addressed the algorithmic problem of allocating advertisement
space for keywords in sponsored search auctions so as to maximize revenue, most
of which assume that pricing is done via a first-price auction. This does not
realistically model the Generalized Second Price (GSP) auction used in
practice, in which bidders pay the next-highest bid for keywords that they are
allocated. Towards the goal of more realistically modeling these auctions, we
introduce the Second-Price Ad Auctions problem, in which bidders' payments are
determined by the GSP mechanism. We show that the complexity of the
Second-Price Ad Auctions problem is quite different than that of the more
studied First-Price Ad Auctions problem. First, unlike the first-price variant,
for which small constant-factor approximations are known, it is NP-hard to
approximate the Second-Price Ad Auctions problem to any non-trivial factor,
even when the bids are small compared to the budgets. Second, this discrepancy
extends even to the 0-1 special case that we call the Second-Price Matching
problem (2PM). Offline 2PM is APX-hard, and for online 2PM there is no
deterministic algorithm achieving a non-trivial competitive ratio and no
randomized algorithm achieving a competitive ratio better than 2. This
contrasts with the results for the analogous special case in the first-price
model, the standard bipartite matching problem, which is solvable in polynomial
time and which has deterministic and randomized online algorithms achieving
better competitive ratios. On the positive side, we provide a 2-approximation
for offline 2PM and a 5.083-competitive randomized algorithm for online 2PM.
The latter result makes use of a new generalization of a result on the
performance of the ""Ranking"" algorithm for online bipartite matching."
"We present fast algorithms for constructing probabilistic embeddings and
approximate distance oracles in sparse graphs. The main ingredient is a fast
algorithm for sampling the probabilistic partitions of Calinescu, Karloff, and
Rabani in sparse graphs."
"One of the most fundamental problems in large scale network analysis is to
determine the importance of a particular node in a network. Betweenness
centrality is the most widely used metric to measure the importance of a node
in a network. In this paper, we present a randomized parallel algorithm and an
algebraic method for computing betweenness centrality of all nodes in a
network. We prove that any path-comparison based algorithm cannot compute
betweenness in less than O(nm) time."
"In this work, we obtain the following new results.
  1. Given a sequence $D=((h_1,s_1), (h_2,s_2) ..., (h_n,s_n))$ of number
pairs, where $s_i>0$ for all $i$, and a number $L_h$, we propose an O(n)-time
algorithm for finding an index interval $[i,j]$ that maximizes
$\frac{\sum_{k=i}^{j} h_k}{\sum_{k=i}^{j} s_k}$ subject to $\sum_{k=i}^{j} h_k
\geq L_h$.
  2. Given a sequence $D=((h_1,s_1), (h_2,s_2) ..., (h_n,s_n))$ of number
pairs, where $s_i=1$ for all $i$, and an integer $L_s$ with $1\leq L_s\leq n$,
we propose an $O(n\frac{T(L_s^{1/2})}{L_s^{1/2}})$-time algorithm for finding
an index interval $[i,j]$ that maximizes $\frac{\sum_{k=i}^{j}
h_k}{\sqrt{\sum_{k=i}^{j} s_k}}$ subject to $\sum_{k=i}^{j} s_k \geq L_s$,
where $T(n')$ is the time required to solve the all-pairs shortest paths
problem on a graph of $n'$ nodes. By the latest result of Chan \cite{Chan},
$T(n')=O(n'^3 \frac{(\log\log n')^3}{(\log n')^2})$, so our algorithm runs in
subquadratic time $O(nL_s\frac{(\log\log L_s)^3}{(\log L_s)^2})$."
"We study local search algorithms for metric instances of facility location
problems: the uncapacitated facility location problem (UFL), as well as
uncapacitated versions of the $k$-median, $k$-center and $k$-means problems.
All these problems admit natural local search heuristics: for example, in the
UFL problem the natural moves are to open a new facility, close an existing
facility, and to swap a closed facility for an open one; in $k$-medians, we are
allowed only swap moves. The local-search algorithm for $k$-median was analyzed
by Arya et al. (SIAM J. Comput. 33(3):544-562, 2004), who used a clever
``coupling'' argument to show that local optima had cost at most constant times
the global optimum. They also used this argument to show that the local search
algorithm for UFL was 3-approximation; their techniques have since been applied
to other facility location problems.
  In this paper, we give a proof of the $k$-median result which avoids this
coupling argument. These arguments can be used in other settings where the Arya
et al. arguments have been used. We also show that for the problem of opening
$k$ facilities $F$ to minimize the objective function $\Phi_p(F) = \big(\sum_{j
\in V} d(j, F)^p\big)^{1/p}$, the natural swap-based local-search algorithm is
a $\Theta(p)$-approximation. This implies constant-factor approximations for
$k$-medians (when $p=1$), and $k$-means (when $p = 2$), and an $O(\log
n)$-approximation algorithm for the $k$-center problem (which is essentially $p
= \log n$)."
"We present an algorithm for the Single Source Shortest Paths (SSSP) problem
in \emph{$H$-minor free} graphs. For every fixed $H$, if $G$ is a graph with
$n$ vertices having integer edge lengths and $s$ is a designated source vertex
of $G$, the algorithm runs in $\tilde{O}(n^{\sqrt{11.5}-2} \log L) \le
O(n^{1.392} \log L)$ time, where $L$ is the absolute value of the smallest edge
length. The algorithm computes shortest paths and the distances from $s$ to all
vertices of the graph, or else provides a certificate that $G$ is not $H$-minor
free. Our result improves an earlier $O(n^{1.5} \log L)$ time algorithm for
this problem, which follows from a general SSSP algorithm of Goldberg."
"In this paper we present several algorithmic techniques for inferring the
structure of a company when only a limited amount of information is available.
We consider problems with two types of inputs: the number of pairs of employees
with a given property and restricted information about the hierarchical
structure of the company. We provide dynamic programming and greedy algorithms
for these problems."
"In this paper we consider several facility location problems with
applications to cost and social welfare optimization, when the area map is
encoded as a binary (0,1) mxn matrix. We present algorithmic solutions for all
the problems. Some cases are too particular to be used in practical situations,
but they are at least a starting point for more generic solutions."
"In this paper we prove the correctness of Dijkstra's algorithm. We also
discuss it and at the end we show an application."
"A string matching -- and more generally, sequence matching -- algorithm is
presented that has a linear worst-case computing time bound, a low worst-case
bound on the number of comparisons (2n), and sublinear average-case behavior
that is better than that of the fastest versions of the Boyer-Moore algorithm.
The algorithm retains its efficiency advantages in a wide variety of sequence
matching problems of practical interest, including traditional string matching;
large-alphabet problems (as in Unicode strings); and small-alphabet,
long-pattern problems (as in DNA searches). Since it is expressed as a generic
algorithm for searching in sequences over an arbitrary type T, it is well
suited for use in generic software libraries such as the C++ Standard Template
Library. The algorithm was obtained by adding to the Knuth-Morris-Pratt
algorithm one of the pattern-shifting techniques from the Boyer-Moore
algorithm, with provision for use of hashing in this technique. In situations
in which a hash function or random access to the sequences is not available,
the algorithm falls back to an optimized version of the Knuth-Morris-Pratt
algorithm."
"In the budgeted learning problem, we are allowed to experiment on a set of
alternatives (given a fixed experimentation budget) with the goal of picking a
single alternative with the largest possible expected payoff. Approximation
algorithms for this problem were developed by Guha and Munagala by rounding a
linear program that couples the various alternatives together. In this paper we
present an index for this problem, which we call the ratio index, which also
guarantees a constant factor approximation. Index-based policies have the
advantage that a single number (i.e. the index) can be computed for each
alternative irrespective of all other alternatives, and the alternative with
the highest index is experimented upon. This is analogous to the famous Gittins
index for the discounted multi-armed bandit problem.
  The ratio index has several interesting structural properties. First, we show
that it can be computed in strongly polynomial time. Second, we show that with
the appropriate discount factor, the Gittins index and our ratio index are
constant factor approximations of each other, and hence the Gittins index also
gives a constant factor approximation to the budgeted learning problem.
Finally, we show that the ratio index can be used to create an index-based
policy that achieves an O(1)-approximation for the finite horizon version of
the multi-armed bandit problem. Moreover, the policy does not require any
knowledge of the horizon (whereas we compare its performance against an optimal
strategy that is aware of the horizon). This yields the following surprising
result: there is an index-based policy that achieves an O(1)-approximation for
the multi-armed bandit problem, oblivious to the underlying discount factor."
"We consider the following ""multiway cut packing"" problem in undirected
graphs: we are given a graph G=(V,E) and k commodities, each corresponding to a
set of terminals located at different vertices in the graph; our goal is to
produce a collection of cuts {E_1,...,E_k} such that E_i is a multiway cut for
commodity i and the maximum load on any edge is minimized. The load on an edge
is defined to be the number of cuts in the solution crossing the edge. In the
capacitated version of the problem the goal is to minimize the maximum relative
load on any edge--the ratio of the edge's load to its capacity. Multiway cut
packing arises in the context of graph labeling problems where we are given a
partial labeling of a set of items and a neighborhood structure over them, and,
informally, the goal is to complete the labeling in the most consistent way.
This problem was introduced by Rabani, Schulman, and Swamy (SODA'08), who
developed an O(log n/log log n) approximation for it in general graphs, as well
as an improved O(log^2 k) approximation in trees. Here n is the number of nodes
in the graph. We present the first constant factor approximation for this
problem in arbitrary undirected graphs. Our approach is based on the
observation that every instance of the problem admits a near-optimal laminar
solution (that is, one in which no pair of cuts cross each other)."
"An L(2,1)-labeling of a graph $G$ is an assignment $f$ from the vertex set
$V(G)$ to the set of nonnegative integers such that $|f(x)-f(y)|\ge 2$ if $x$
and $y$ are adjacent and $|f(x)-f(y)|\ge 1$ if $x$ and $y$ are at distance 2,
for all $x$ and $y$ in $V(G)$. A $k$-L(2,1)-labeling is an assignment
$f:V(G)\to\{0,..., k\}$, and the L(2,1)-labeling problem asks the minimum $k$,
which we denote by $\lambda(G)$, among all possible assignments. It is known
that this problem is NP-hard even for graphs of treewidth 2, and tree is one of
a very few classes for which the problem is polynomially solvable. The running
time of the best known algorithm for trees had been $\mO(\Delta^{4.5} n)$ for
more than a decade, however, an $\mO(n^{1.75})$-time algorithm has been
proposed recently, which substantially improved the previous one, where
$\Delta$ is the maximum degree of $T$ and $n=|V(T)|$. In this paper, we finally
establish a linear time algorithm for L(2,1)-labeling of trees."
"Single node failures represent more than 85% of all node failures in the
today's large communication networks such as the Internet. Also, these node
failures are usually transient. Consequently, having the routing paths globally
recomputed does not pay off since the failed nodes recover fairly quickly, and
the recomputed routing paths need to be discarded. Instead, we develop
algorithms and protocols for dealing with such transient single node failures
by suppressing the failure (instead of advertising it across the network), and
routing messages to the destination via alternate paths that do not use the
failed node. We compare our solution to that of Ref. [11] wherein the authors
have presented a ""Failure Insensitive Routing"" protocol as a proactive recovery
scheme for handling transient node failures. We show that our algorithms are
faster by an order of magnitude while our paths are equally good. We show via
simulation results that our paths are usually within 15% of the optimal for
randomly generated graph with 100-1000 nodes."
"The Lovasz Local Lemma [EL75] is a powerful tool to prove the existence of
combinatorial objects meeting a prescribed collection of criteria. The
technique can directly be applied to the satisfiability problem, yielding that
a k-CNF formula in which each clause has common variables with at most 2^(k-2)
other clauses is always satisfiable. All hitherto known proofs of the Local
Lemma are non-constructive and do thus not provide a recipe as to how a
satisfying assignment to such a formula can be efficiently found. In his
breakthrough paper [Bec91], Beck demonstrated that if the neighbourhood of each
clause be restricted to O(2^(k/48)), a polynomial time algorithm for the search
problem exists. Alon simplified and randomized his procedure and improved the
bound to O(2^(k/8)) [Alo91]. Srinivasan presented in [Sri08] a variant that
achieves a bound of essentially O(2^(k/4)). In [Mos08], we improved this to
O(2^(k/2)). In the present paper, we give a randomized algorithm that finds a
satisfying assignment to every k-CNF formula in which each clause has a
neighbourhood of at most the asymptotic optimum of 2^(k-5)-1 other clauses and
that runs in expected time polynomial in the size of the formula, irrespective
of k. If k is considered a constant, we can also give a deterministic variant.
In contrast to all previous approaches, our analysis does not anymore invoke
the standard non-constructive versions of the Local Lemma and can therefore be
considered an alternative, constructive proof of it."
"We study optimization problems that are neither approximable in polynomial
time (at least with a constant factor) nor fixed parameter tractable, under
widely believed complexity assumptions. Specifically, we focus on Maximum
Independent Set, Vertex Coloring, Set Cover, and Bandwidth.
  In recent years, many researchers design exact exponential-time algorithms
for these and other hard problems. The goal is getting the time complexity
still of order $O(c^n)$, but with the constant $c$ as small as possible. In
this work we extend this line of research and we investigate whether the
constant $c$ can be made even smaller when one allows constant factor
approximation. In fact, we describe a kind of approximation schemes --
trade-offs between approximation factor and the time complexity.
  We study two natural approaches. The first approach consists of designing a
backtracking algorithm with a small search tree. We present one result of that
kind: a $(4r-1)$-approximation of Bandwidth in time $O^*(2^{n/r})$, for any
positive integer $r$.
  The second approach uses general transformations from exponential-time exact
algorithms to approximations that are faster but still exponential-time. For
example, we show that for any reduction rate $r$, one can transform any
$O^*(c^n)$-time algorithm for Set Cover into a $(1+\ln r)$-approximation
algorithm running in time $O^*(c^{n/r})$. We believe that results of that kind
extend the applicability of exact algorithms for NP-hard problems."
"We study the worst-case communication complexity of distributed algorithms
computing a path problem based on stationary distributions of random walks in a
network $G$ with the caveat that $G$ is also the communication network. The
problem is a natural generalization of shortest path lengths to expected path
lengths, and represents a model used in many practical applications such as
pagerank and eigentrust as well as other problems involving Markov chains
defined by networks.
  For the problem of computing a single stationary probability, we prove an
$\Omega(n^2 \log n)$ bits lower bound; the trivial centralized algorithm costs
$O(n^3)$ bits and no known algorithm beats this. We also prove lower bounds for
the related problems of approximately computing the stationary probabilities,
computing only the ranking of the nodes, and computing the node with maximal
rank. As a corollary, we obtain lower bounds for labelling schemes for the
hitting time between two nodes."
"We give a simple algorithm for decremental graph connectivity that handles
edge deletions in worst-case time $O(k \log n)$ and connectivity queries in
$O(\log k)$, where $k$ is the number of edges deleted so far, and uses
worst-case space $O(m^2)$. We use this to give an algorithm for $k$-edge
witness (``does the removal of a given set of $k$ edges disconnect two vertices
$u,v$?'') with worst-case time $O(k^2 \log n)$ and space $O(k^2 n^2)$. For $k =
o(\sqrt{n})$ these improve the worst-case $O(\sqrt{n})$ bound for deletion due
to Eppstein et al. We also give a decremental connectivity algorithm using
$O(n^2 \log n / \log \log n)$ space, whose time complexity depends on the
toughness and independence number of the input graph. Finally, we show how to
construct a distributed data structure for \kvw by giving a labeling scheme.
This is the first data structure for \kvw that can efficiently distributed
without just giving each vertex a copy of the whole structure. Its complexity
depends on being able to construct a linear layout with good properties."
"We consider the problem of partial order production: arrange the elements of
an unknown totally ordered set T into a target partially ordered set S, by
comparing a minimum number of pairs in T. Special cases include sorting by
comparisons, selection, multiple selection, and heap construction.
  We give an algorithm performing ITLB + o(ITLB) + O(n) comparisons in the
worst case. Here, n denotes the size of the ground sets, and ITLB denotes a
natural information-theoretic lower bound on the number of comparisons needed
to produce the target partial order.
  Our approach is to replace the target partial order by a weak order (that is,
a partial order with a layered structure) extending it, without increasing the
information theoretic lower bound too much. We then solve the problem by
applying an efficient multiple selection algorithm. The overall complexity of
our algorithm is polynomial. This answers a question of Yao (SIAM J. Comput.
18, 1989).
  We base our analysis on the entropy of the target partial order, a quantity
that can be efficiently computed and provides a good estimate of the
information-theoretic lower bound."
"Hash tables are one of the most fundamental data structures in computer
science, in both theory and practice. They are especially useful in external
memory, where their query performance approaches the ideal cost of just one
disk access. Knuth gave an elegant analysis showing that with some simple
collision resolution strategies such as linear probing or chaining, the
expected average number of disk I/Os of a lookup is merely $1+1/2^{\Omega(b)}$,
where each I/O can read a disk block containing $b$ items. Inserting a new item
into the hash table also costs $1+1/2^{\Omega(b)}$ I/Os, which is again almost
the best one can do if the hash table is entirely stored on disk. However, this
assumption is unrealistic since any algorithm operating on an external hash
table must have some internal memory (at least $\Omega(1)$ blocks) to work
with. The availability of a small internal memory buffer can dramatically
reduce the amortized insertion cost to $o(1)$ I/Os for many external memory
data structures. In this paper we study the inherent query-insertion tradeoff
of external hash tables in the presence of a memory buffer. In particular, we
show that for any constant $c>1$, if the query cost is targeted at
$1+O(1/b^{c})$ I/Os, then it is not possible to support insertions in less than
$1-O(1/b^{\frac{c-1}{4}})$ I/Os amortized, which means that the memory buffer
is essentially useless. While if the query cost is relaxed to $1+O(1/b^{c})$
I/Os for any constant $c<1$, there is a simple dynamic hash table with $o(1)$
insertion cost. These results also answer the open question recently posed by
Jensen and Pagh."
"Sorting is a common and ubiquitous activity for computers. It is not
surprising that there exist a plethora of sorting algorithms. For all the
sorting algorithms, it is an accepted performance limit that sorting algorithms
are linearithmic or O(N lg N). The linearithmic lower bound in performance
stems from the fact that the sorting algorithms use the ordering property of
the data. The sorting algorithm uses comparison by the ordering property to
arrange the data elements from an initial permutation into a sorted
permutation.
  Linear O(N) sorting algorithms exist, but use a priori knowledge of the data
to use a specific property of the data and thus have greater performance. In
contrast, the linearithmic sorting algorithms are generalized by using a
universal property of data-comparison, but have a linearithmic performance
lower bound. The trade-off in sorting algorithms is generality for performance
by the chosen property used to sort the data elements.
  A general-purpose, linear sorting algorithm in the context of the trade-off
of performance for generality at first consideration seems implausible. But,
there is an implicit assumption that only the ordering property is universal.
But, as will be discussed and examined, it is not the only universal property
for data elements. The binar sort is a general-purpose sorting algorithm that
uses this other universal property to sort linearly."
"Frequently, randomly organized data is needed to avoid an anomalous operation
of other algorithms and computational processes. An analogy is that a deck of
cards is ordered within the pack, but before a game of poker or solitaire the
deck is shuffled to create a random permutation. Shuffling is used to assure
that an aggregate of data elements for a sequence S is randomly arranged, but
avoids an ordered or partially ordered permutation.
  Shuffling is the process of arranging data elements into a random
permutation. The sequence S as an aggregation of N data elements, there are N!
possible permutations. For the large number of possible permutations, two of
the possible permutations are for a sorted or ordered placement of data
elements--both an ascending and descending sorted permutation. Shuffling must
avoid inadvertently creating either an ascending or descending permutation.
  Shuffling is frequently coupled to another algorithmic function --
pseudo-random number generation. The efficiency and quality of the shuffle is
directly dependent upon the random number generation algorithm utilized. A more
effective and efficient method of shuffling is to use parameterization to
configure the shuffle, and to shuffle into sub-arrays by utilizing the encoding
of the data elements. The binar shuffle algorithm uses the encoding of the data
elements and parameterization to avoid any direct coupling to a random number
generation algorithm, but still remain a linear O(N) shuffle algorithm."
"We study the classical approximate string matching problem, that is, given
strings $P$ and $Q$ and an error threshold $k$, find all ending positions of
substrings of $Q$ whose edit distance to $P$ is at most $k$. Let $P$ and $Q$
have lengths $m$ and $n$, respectively. On a standard unit-cost word RAM with
word size $w \geq \log n$ we present an algorithm using time $$ O(nk \cdot
\min(\frac{\log^2 m}{\log n},\frac{\log^2 m\log w}{w}) + n) $$ When $P$ is
short, namely, $m = 2^{o(\sqrt{\log n})}$ or $m = 2^{o(\sqrt{w/\log w})}$ this
improves the previously best known time bounds for the problem. The result is
achieved using a novel implementation of the Landau-Vishkin algorithm based on
tabulation and word-level parallelism."
"In this paper we study the adaptive prefix coding problem in cases where the
size of the input alphabet is large. We present an online prefix coding
algorithm that uses $O(\sigma^{1 / \lambda + \epsilon}) $ bits of space for any
constants $\eps>0$, $\lambda>1$, and encodes the string of symbols in $O(\log
\log \sigma)$ time per symbol \emph{in the worst case}, where $\sigma$ is the
size of the alphabet. The upper bound on the encoding length is $\lambda n H
(s) +(\lambda \ln 2 + 2 + \epsilon) n + O (\sigma^{1 / \lambda} \log^2 \sigma)$
bits."
"A {\em local graph partitioning algorithm} finds a set of vertices with small
conductance (i.e. a sparse cut) by adaptively exploring part of a large graph
$G$, starting from a specified vertex. For the algorithm to be local, its
complexity must be bounded in terms of the size of the set that it outputs,
with at most a weak dependence on the number $n$ of vertices in $G$. Previous
local partitioning algorithms find sparse cuts using random walks and
personalized PageRank. In this paper, we introduce a randomized local
partitioning algorithm that finds a sparse cut by simulating the {\em
volume-biased evolving set process}, which is a Markov chain on sets of
vertices. We prove that for any set of vertices $A$ that has conductance at
most $\phi$, for at least half of the starting vertices in $A$ our algorithm
will output (with probability at least half), a set of conductance
$O(\phi^{1/2} \log^{1/2} n)$. We prove that for a given run of the algorithm,
the expected ratio between its computational complexity and the volume of the
set that it outputs is $O(\phi^{-1/2} polylog(n))$. In comparison, the best
previous local partitioning algorithm, due to Andersen, Chung, and Lang, has
the same approximation guarantee, but a larger ratio of $O(\phi^{-1}
polylog(n))$ between the complexity and output volume. Using our local
partitioning algorithm as a subroutine, we construct a fast algorithm for
finding balanced cuts. Given a fixed value of $\phi$, the resulting algorithm
has complexity $O((m+n\phi^{-1/2}) polylog(n))$ and returns a cut with
conductance $O(\phi^{1/2} \log^{1/2} n)$ and volume at least $v_{\phi}/2$,
where $v_{\phi}$ is the largest volume of any set with conductance at most
$\phi$."
"Given a set of $m$ agents and a set of $n$ items, where agent $A$ has utility
$u_{A,i}$ for item $i$, our goal is to allocate items to agents to maximize
fairness. Specifically, the utility of an agent is the sum of its utilities for
items it receives, and we seek to maximize the minimum utility of any agent.
While this problem has received much attention recently, its approximability
has not been well-understood thus far: the best known approximation algorithm
achieves an $\tilde{O}(\sqrt{m})$-approximation, and in contrast, the best
known hardness of approximation stands at 2.
  Our main result is an approximation algorithm that achieves an
$\tilde{O}(n^{\eps})$ approximation for any $\eps=\Omega(\log\log n/\log n)$ in
time $n^{O(1/\eps)}$. In particular, we obtain poly-logarithmic approximation
in quasi-polynomial time, and for any constant $\eps > 0$, we obtain
$O(n^{\eps})$ approximation in polynomial time. An interesting aspect of our
algorithm is that we use as a building block a linear program whose integrality
gap is $\Omega(\sqrt m)$. We bypass this obstacle by iteratively using the
solutions produced by the LP to construct new instances with significantly
smaller integrality gaps, eventually obtaining the desired approximation.
  We also investigate the special case of the problem, where every item has a
non-zero utility for at most two agents. We show that even in this restricted
setting the problem is hard to approximate upto any factor better tha 2, and
show a factor $(2+\eps)$-approximation algorithm running in time
$poly(n,1/\eps)$ for any $\eps>0$. This special case can be cast as a graph
edge orientation problem, and our algorithm can be viewed as a generalization
of Eulerian orientations to weighted graphs."
"We study generalized fixed-point equations over idempotent semirings and
provide an efficient algorithm for the detection whether a sequence of Kleene's
iterations stabilizes after a finite number of steps. Previously known
approaches considered only bounded semirings where there are no infinite
descending chains. The main novelty of our work is that we deal with semirings
without the boundedness restriction. Our study is motivated by several
applications from interprocedural dataflow analysis. We demonstrate how the
reachability problem for weighted pushdown automata can be reduced to solving
equations in the framework mentioned above and we describe a few applications
to demonstrate its usability."
"Given a sequence A of 2n real numbers, the Even-Rank-Sum problem asks for the
sum of the n values that are at the even positions in the sorted order of the
elements in A. We prove that, in the algebraic computation-tree model, this
problem has time complexity \Theta(n log n). This solves an open problem posed
by Michael Shamos at the Canadian Conference on Computational Geometry in 2008."
"We consider the following problem: given an unsorted array of $n$ elements,
and a sequence of intervals in the array, compute the median in each of the
subarrays defined by the intervals. We describe a simple algorithm which uses
O(n) space and needs $O(n\log k + k\log n)$ time to answer the first $k$
queries. This improves previous algorithms by a logarithmic factor and matches
a lower bound for $k=O(n)$.
  Since the algorithm decomposes the range of element values rather than the
array, it has natural generalizations to higher dimensional problems -- it
reduces a range median query to a logarithmic number of range counting queries."
"We present here some results on particular elimination schemes for chordal
graphs, namely we show that for any chordal graph we can construct in linear
time a simplicial elimination scheme starting with a pending maximal clique
attached via a minimal separator maximal (resp. minimal) under inclusion among
all minimal separators."
"Let pi_w denote the failure function of the Morris-Pratt algorithm for a word
w. In this paper we study the following problem: given an integer array
A[1..n], is there a word w over arbitrary alphabet such that A[i]=pi_w[i] for
all i? Moreover, what is the minimum required cardinality of the alphabet? We
give a real time linear algorithm for this problem in the unit-cost RAM model
with \Theta(log n) bits word size. Our algorithm returns a word w over minimal
alphabet such that pi_w = A as well and uses just o(n) words of memory. Then we
consider function pi' instead of pi and give an online O(n log n) algorithm for
this case. This is the first polynomial algorithm for online version of this
problem."
"In this paper, we have developed a fully-dynamic algorithm for maintaining
cardinality of maximum-matching in a tree using the construction of top-trees.
The time complexities are as follows:
  1. Initialization Time: $O(n(log(n)))$ to build the Top-tree. 2. Update Time:
$O(log(n))$ 3. Query Time: O(1) to query the cardinality of maximum-matching
and $O(log(n))$ to find if a particular edge is matched."
"We study the weighted generalization of the edge coloring problem where the
weight of each color class (matching) equals to the weight of its heaviest edge
and the goal is to minimize the sum of the colors' weights. We present a
3/2-approximation algorithm for trees."
"Collaborative editing consists in editing a common document shared by several
independent sites. This may give rise to conficts when two different users
perform simultaneous uncompatible operations. Centralized systems solve this
problem by using locks that prevent some modifications to occur and leave the
resolution of confict to users. On the contrary, peer to peer (P2P) editing
doesn't allow locks and the optimistic approach uses a Integration
Transformation IT that reconciliates the conficting operations and ensures
convergence (all copies are identical on each site). Two properties TP1 and
TP2, relating the set of allowed operations Op and the transformation IT, have
been shown to ensure the correctness of the process. The choice of the set Op
is crucial to define an integration operation that satisfies TP1 and TP2. Many
existing algorithms don't satisfy these properties and are indeed incorrect
i.e. convergence is not guaranteed. No algorithm enjoying both properties is
known for strings and little work has been done for XML trees in a pure P2P
framework (that doesn't use time-stamps for instance). We focus on editing
unranked unordered labeled trees, so-called XML-like trees that are considered
for instance in the Harmony pro ject. We show that no transformation satisfying
TP1 and TP2 can exist for a first set of operations but we show that TP1 and
TP2 hold for a richer set of operations. We show how to combine our approach
with any convergent editing process on strings (not necessarily based on
integration transformation) to get a convergent process."
"Analyzing massive data sets has been one of the key motivations for studying
streaming algorithms. In recent years, there has been significant progress in
analysing distributions in a streaming setting, but the progress on graph
problems has been limited. A main reason for this has been the existence of
linear space lower bounds for even simple problems such as determining the
connectedness of a graph. However, in many new scenarios that arise from social
and other interaction networks, the number of vertices is significantly less
than the number of edges. This has led to the formulation of the semi-streaming
model where we assume that the space is (near) linear in the number of vertices
(but not necessarily the edges), and the edges appear in an arbitrary (and
possibly adversarial) order.
  In this paper we focus on graph sparsification, which is one of the major
building blocks in a variety of graph algorithms. There has been a long history
of (non-streaming) sampling algorithms that provide sparse graph approximations
and it a natural question to ask if the sparsification can be achieved using a
small space, and in addition using a single pass over the data? The question is
interesting from the standpoint of both theory and practice and we answer the
question in the affirmative, by providing a one pass
$\tilde{O}(n/\epsilon^{2})$ space algorithm that produces a sparsification that
approximates each cut to a $(1+\epsilon)$ factor. We also show that $\Omega(n
\log \frac1\epsilon)$ space is necessary for a one pass streaming algorithm to
approximate the min-cut, improving upon the $\Omega(n)$ lower bound that arises
from lower bounds for testing connectivity."
"We explore various techniques to compress a permutation $\pi$ over n
integers, taking advantage of ordered subsequences in $\pi$, while supporting
its application $\pi$(i) and the application of its inverse $\pi^{-1}(i)$ in
small time. Our compression schemes yield several interesting byproducts, in
many cases matching, improving or extending the best existing results on
applications such as the encoding of a permutation in order to support iterated
applications $\pi^k(i)$ of it, of integer functions, and of inverted lists and
suffix arrays."
"We study online nonclairvoyant speed scaling to minimize total flow time plus
energy. We first consider the traditional model where the power function is P
(s) = s\^\propto. We give a nonclairvoyant algorithm that is shown to be
O(\propto\^3)-competitive. We then show an \Omega(\propto\^(1/3-\epsilon))
lower bound on the competitive ratio of any nonclairvoyant algorithm. We also
show that there are power functions for which no nonclairvoyant algorithm can
be O(1)-competitive."
"We consider the Work Function Algorithm for the k-server problem. We show
that if the Work Function Algorithm is c-competitive, then it is also strictly
(2c)-competitive. As a consequence of [Koutsoupias and Papadimitriou, JACM
1995] this also shows that the Work Function Algorithm is strictly
(4k-2)-competitive."
"As the World Wide Web is growing rapidly, it is getting increasingly
challenging to gather representative information about it. Instead of crawling
the web exhaustively one has to resort to other techniques like sampling to
determine the properties of the web. A uniform random sample of the web would
be useful to determine the percentage of web pages in a specific language, on a
topic or in a top level domain. Unfortunately, no approach has been shown to
sample the web pages in an unbiased way. Three promising web sampling
algorithms are based on random walks. They each have been evaluated
individually, but making a comparison on different data sets is not possible.
We directly compare these algorithms in this paper. We performed three random
walks on the web under the same conditions and analyzed their outcomes in
detail. We discuss the strengths and the weaknesses of each algorithm and
propose improvements based on experimental results."
"This paper gives a brief overview of computation models for data stream
processing, and it introduces a new model for multi-pass processing of multiple
streams, the so-called mp2s-automata. Two algorithms for solving the set
disjointness problem wi th these automata are presented. The main technical
contribution of this paper is the proof of a lower bound on the size of memory
and the number of heads that are required for solvin g the set disjointness
problem with mp2s-automata."
"We consider the multivariate interlace polynomial introduced by Courcelle
(2008), which generalizes several interlace polynomials defined by Arratia,
Bollobas, and Sorkin (2004) and by Aigner and van der Holst (2004). We present
an algorithm to evaluate the multivariate interlace polynomial of a graph with
n vertices given a tree decomposition of the graph of width k. The best
previously known result (Courcelle 2008) employs a general logical framework
and leads to an algorithm with running time f(k)*n, where f(k) is doubly
exponential in k. Analyzing the GF(2)-rank of adjacency matrices in the context
of tree decompositions, we give a faster and more direct algorithm. Our
algorithm uses 2^{3k^2+O(k)}*n arithmetic operations and can be efficiently
implemented in parallel."
"We consider a robust model proposed by Scarf, 1958, for stochastic
optimization when only the marginal probabilities of (binary) random variables
are given, and the correlation between the random variables is unknown. In the
robust model, the objective is to minimize expected cost against worst possible
joint distribution with those marginals. We introduce the concept of
correlation gap to compare this model to the stochastic optimization model that
ignores correlations and minimizes expected cost under independent Bernoulli
distribution. We identify a class of functions, using concepts of summable cost
sharing schemes from game theory, for which the correlation gap is well-bounded
and the robust model can be approximated closely by the independent
distribution model. As a result, we derive efficient approximation factors for
many popular cost functions, like submodular functions, facility location, and
Steiner tree. As a byproduct, our analysis also yields some new results in the
areas of social welfare maximization and existence of Walrasian equilibria,
which may be of independent interest."
"We consider an online scheduling problem, motivated by the issues present at
the joints of networks using ATM and TCP/IP. Namely, IP packets have to broken
down to small ATM cells and sent out before their deadlines, but cells
corresponding to different packets can be interwoven. More formally, we
consider the online scheduling problem with preemptions, where each job j is
revealed at release time r_j, has processing time p_j, deadline d_j and weight
w_j. A preempted job can be resumed at any time. The goal is to maximize the
total weight of all jobs completed on time. Our main result are as follows: we
prove that if all jobs have processing time exactly k, the deterministic
competitive ratio is between 2.598 and 5, and when the processing times are at
most k, the deterministic competitive ratio is Theta(k/log k)."
"We consider the problem of representing, in a compressed format, a bit-vector
$S$ of $m$ bits with $n$ 1s, supporting the following operations, where $b \in
\{0, 1 \}$: $rank_b(S,i)$ returns the number of occurrences of bit $b$ in the
prefix $S[1..i]$; $select_b(S,i)$ returns the position of the $i$th occurrence
of bit $b$ in $S$. Such a data structure is called \emph{fully indexable
dictionary (FID)} [Raman et al.,2007], and is at least as powerful as
predecessor data structures. Our focus is on space-efficient FIDs on the
\textsc{ram} model with word size $\Theta(\lg m)$ and constant time for all
operations, so that the time cost is independent of the input size. Given the
bitstring $S$ to be encoded, having length $m$ and containing $n$ ones, the
minimal amount of information that needs to be stored is $B(n,m) = \lceil \log
{{m}\choose{n}} \rceil$. The state of the art in building a FID for $S$ is
given in [Patrascu,2008] using $B(m,n)+O(m / ((\log m/ t) ^t)) + O(m^{3/4}) $
bits, to support the operations in $O(t)$ time. Here, we propose a parametric
data structure exhibiting a time/space trade-off such that, for any real
constants $0 < \delta \leq 1/2$, $0 < \eps \leq 1$, and integer $s > 0$, it
uses \[ B(n,m) + O(n^{1+\delta} + n (\frac{m}{n^s})^\eps) \] bits and performs
all the operations in time $O(s\delta^{-1} + \eps^{-1})$. The improvement is
twofold: our redundancy can be lowered parametrically and, fixing $s = O(1)$,
we get a constant-time FID whose space is $B(n,m) + O(m^\eps/\poly{n})$ bits,
for sufficiently large $m$. This is a significant improvement compared to the
previous bounds for the general case."
"Given an undirected graph G=(V,E) and subset of terminals T \subseteq V, the
element-connectivity of two terminals u,v \in T is the maximum number of u-v
paths that are pairwise disjoint in both edges and non-terminals V \setminus T
(the paths need not be disjoint in terminals). Element-connectivity is more
general than edge-connectivity and less general than vertex-connectivity. Hind
and Oellermann gave a graph reduction step that preserves the global
element-connectivity of the graph. We show that this step also preserves local
connectivity, that is, all the pairwise element-connectivities of the
terminals. We give two applications of this reduction step to connectivity and
network design problems:
  1. Given a graph G and disjoint terminal sets T_1, T_2, ..., T_m, we seek a
maximum number of element-disjoint Steiner forests where each forest connects
each T_i. We prove that if each T_i is k-element-connected then there exist
\Omega(\frac{k}{\log h \log m}) element-disjoint Steiner forests, where h =
|\bigcup_i T_i|. If G is planar (or more generally, has fixed genus), we show
that there exist \Omega(k) Steiner forests. Our proofs are constructive, giving
poly-time algorithms to find these forests; these are the first non-trivial
algorithms for packing element-disjoint Steiner Forests.
  2. We give a very short and intuitive proof of a spider-decomposition theorem
of Chuzhoy and Khanna in the context of the single-sink k-vertex-connectivity
problem; this yields a simple and alternative analysis of an O(k \log n)
approximation.
  Our results highlight the effectiveness of the element-connectivity reduction
step; we believe it will find more applications in the future."
"This paper presents different methods for solving parallel machine scheduling
problems with precedence constraints and setup times between the jobs. Limited
discrepancy search methods mixed with local search principles, dominance
conditions and specific lower bounds are proposed. The proposed methods are
evaluated on a set of randomly generated instances and compared with previous
results from the literature and those obtained with an efficient commercial
solver. We conclude that our propositions are quite competitive and our results
even outperform other approaches in most cases."
"The main objective of this survey is to present the important theoretical and
experimental results contributed till date in the area of online algorithms for
the self organizing sequential search problem, also popularly known as the List
Update Problem(LUP) in a chronological way. The survey includes competitiveness
results of deterministic and randomized online algorithms and complexity
results of optimal off line algorithms for the list update problem. We also
present the results associated with list update with look ahead, list update
with locality of reference and other variants of the list update problem. We
investigate research issues, explore scope of future work associated with each
issue so that future researchers can find it useful to work on."
"We present an algorithm for the asymmetric traveling salesman problem on
instances which satisfy the triangle inequality. Like several existing
algorithms, it achieves approximation ratio O(log n). Unlike previous
algorithms, it uses randomized rounding."
"Suppose that a rooted tree T is given for preprocessing. The Level-Ancestor
Problem is to answer quickly queries of the following form. Given a vertex v
and an integer i > 0, find the i-th vertex on the path from the root to v.
Algorithms that achieve a linear time bound for preprocessing and a constant
time bound for a query have been published by Dietz (1991), Alstrup and Holm
(2000), and Bender and Farach (2002). The first two algorithms address dynamic
versions of the problem; the last addresses the static version only and is the
simplest so far. The purpose of this note is to expose another simple
algorithm, derived from a complicated PRAM algorithm by Berkman and Vishkin
(1990,1994). We further show some easy extensions of its functionality, adding
queries for descendants and level successors as well as ancestors, extensions
for which the formerly known algorithms are less suitable."
"In this paper, we describe randomized Shellsort--a simple, randomized,
data-oblivious version of the Shellsort algorithm that always runs in O(n log
n) time and, as we show, succeeds in sorting any given input permutation with
very high probability. Thus, randomized Shellsort is simultaneously simple,
time-optimal, and data-oblivious. Taken together, these properties imply
applications in the design of new efficient privacy-preserving computations
based on the secure multi-party computation (SMC) paradigm. In addition, by a
trivial conversion of this Monte Carlo algorithm to its Las Vegas equivalent,
one gets the first version of Shellsort with a running time that is provably
O(n log n) with very high probability."
"This paper considers pairs of optimization problems that are defined from a
single input and for which it is desired to find a good approximation to either
one of the problems. In many instances, it is possible to efficiently find an
approximation of this type that is better than known inapproximability lower
bounds for either of the two individual optimization problems forming the pair.
In particular, we find either a $(1+\epsilon)$-approximation to $(1,2)$-TSP or
a $1/\epsilon$-approximation to maximum independent set, from a given graph, in
linear time. We show a similar paired approximation result for finding either a
coloring or a long path. However, no such tradeoff exists in some other cases:
for set cover and hitting set problems defined from a single set family, and
for clique and independent set problems on the same graph, it is not possible
to find an approximation when both problems are combined that is better than
the best approximation for either problem on its own."
"Deletion from open-address hash table is not so easy as deletion from chained
hash table, because in open-address table we can't simply mark a slot
containing deleted key as empty. Search for keys may become incorrect. The
classical method to implement deletion is to mark slots in hash table by three
values: ""free"", ""busy"", ""deleted"". That method is easy to implement, but there
are some disadvantages. In this article we consider alternative method of
deletion keys, where we avoid using the mark ""deleted"". The article contains
the implementation of the method in Java."
"Perhaps the two most significant theoretical questions about the programming
of self-assembling agents are: (1) necessary and sufficient conditions to
produce a unique terminal assembly, and (2) error correction. We address both
questions, by reducing two well-studied models of tile assembly to models of
distributed shared memory (DSM), in order to obtain results from the memory
consistency systems induced by tile assembly systems when simulated in the DSM
setting. The Abstract Tile Assembly Model (aTAM) can be simulated by a DSM
system that obeys causal consistency, and the locally deterministic tile
assembly systems in the aTAM correspond exactly to the concurrent-write free
programs that simulate tile assembly in such a model. Thus, the detection of
the failure of local determinism (which had formerly been an open problem)
reduces to the detection of data races in simulating programs. Further, the
Kinetic Tile Assembly Model can be simulated by a DSM system that obeys GWO, a
memory consistency condition defined by Steinke and Nutt. (To our knowledge,
this is the first natural example of a DSM system that obeys GWO, but no
stronger consistency condition.) We combine these results with the observation
that self-assembly algorithms are local algorithms, and there exists a fast
conversion of deterministic local algorithms into deterministic
self-stabilizing algorithms. This provides an ""immediate"" generalization of a
theorem by Soloveichik et al. about the existence of tile assembly systems that
simultaneously perform two forms of self-stabilization: proofreading and
self-healing. Our reductions and proof techniques can be extended to the
programming of self-assembling agents in a variety of media, not just DNA
tiles, and not just two-dimensional surfaces."
"We study the recently introduced Connected Feedback Vertex Set (CFVS) problem
from the view-point of parameterized algorithms. CFVS is the connected variant
of the classical Feedback Vertex Set problem and is defined as follows: given a
graph G=(V,E) and an integer k, decide whether there exists a subset F of V, of
size at most k, such that G[V F] is a forest and G[F] is connected. We show
that Connected Feedback Vertex Set can be solved in time $O(2^{O(k)}n^{O(1)})$
on general graphs and in time $O(2^{O(\sqrt{k}\log k)}n^{O(1)})$ on graphs
excluding a fixed graph H as a minor. Our result on general undirected graphs
uses as subroutine, a parameterized algorithm for Group Steiner Tree, a well
studied variant of Steiner Tree. We find the algorithm for Group Steiner Tree
of independent interest and believe that it will be useful for obtaining
parameterized algorithms for other connectivity problems."
"We consider algorithms to schedule packets with values and deadlines in a
size-bounded buffer. At any time, the buffer can store at most B packets.
Packets arrive over time. Each packet has a non-negative value and an integer
deadline. In each time step, at most one packet can be sent. Packets can be
dropped at any time before they are sent. The objective is to maximize the
total value gained by delivering packets no later than their respective
deadlines. This model generalizes the well-studied bounded-delay model (Hajek.
CISS 2001. Kesselman et al. STOC 2001). We first provide an optimal offline
algorithm for this model. Then we present an alternative proof of the
2-competitive deterministic online algorithm (Fung. arXiv July 2009). We also
prove that the lower bound of competitive ratio of a family of (deterministic
and randomized) algorithms is 2 - 1 / B."
"We construct efficient data structures that are resilient against a constant
fraction of adversarial noise. Our model requires that the decoder answers most
queries correctly with high probability and for the remaining queries, the
decoder with high probability either answers correctly or declares ""don't
know."" Furthermore, if there is no noise on the data structure, it answers all
queries correctly with high probability. Our model is the common generalization
of a model proposed recently by de Wolf and the notion of ""relaxed locally
decodable codes"" developed in the PCP literature.
  We measure the efficiency of a data structure in terms of its length,
measured by the number of bits in its representation, and query-answering time,
measured by the number of bit-probes to the (possibly corrupted)
representation. In this work, we study two data structure problems: membership
and polynomial evaluation. We show that these two problems have constructions
that are simultaneously efficient and error-correcting."
"The lower and the upper irredundance numbers of a graph $G$, denoted $ir(G)$
and $IR(G)$ respectively, are conceptually linked to domination and
independence numbers and have numerous relations to other graph parameters. It
is a long-standing open question whether determining these numbers for a graph
$G$ on $n$ vertices admits exact algorithms running in time less than the
trivial $\Omega(2^n)$ enumeration barrier. We solve these open problems by
devising parameterized algorithms for the dual of the natural parameterizations
of the problems with running times faster than $O^*(4^{k})$. For example, we
present an algorithm running in time $O^*(3.069^{k})$ for determining whether
$IR(G)$ is at least $n-k$. Although the corresponding problem has been known to
be in FPT by kernelization techniques, this paper offers the first
parameterized algorithms with an exponential dependency on the parameter in the
running time. Additionally, our work also appears to be the first example of a
parameterized approach leading to a solution to a problem in exponential time
algorithmics where the natural interpretation as an exact exponential-time
algorithm fails."
"In this paper we describe algorithms for computing the BWT and for building
(compressed) indexes in external memory. The innovative feature of our
algorithms is that they are lightweight in the sense that, for an input of size
$n$, they use only ${n}$ bits of disk working space while all previous
approaches use $\Th{n \log n}$ bits of disk working space. Moreover, our
algorithms access disk data only via sequential scans, thus they take full
advantage of modern disk features that make sequential disk accesses much
faster than random accesses.
  We also present a scan-based algorithm for inverting the BWT that uses
$\Th{n}$ bits of working space, and a lightweight {\em internal-memory}
algorithm for computing the BWT which is the fastest in the literature when the
available working space is $\os{n}$ bits.
  Finally, we prove {\em lower} bounds on the complexity of computing and
inverting the BWT via sequential scans in terms of the classic product:
internal-memory space $\times$ number of passes over the disk data."
"We study the computability and complexity of the exploration problem in a
class of highly dynamic graphs: periodically varying (PV) graphs, where the
edges exist only at some (unknown) times defined by the periodic movements of
carriers. These graphs naturally model highly dynamic infrastructure-less
networks such as public transports with fixed timetables, low earth orbiting
(LEO) satellite systems, security guards' tours, etc. We establish necessary
conditions for the problem to be solved. We also derive lower bounds on the
amount of time required in general, as well as for the PV graphs defined by
restricted classes of carriers movements: simple routes, and circular routes.
We then prove that the limitations on computability and complexity we have
established are indeed tight. In fact we prove that all necessary conditions
are also sufficient and all lower bounds on costs are tight. We do so
constructively presenting two worst case optimal solution algorithms, one for
anonymous systems, and one for those with distinct nodes ids. An added benefit
is that the algorithms are rather simple."
"We study the non-overlapping indexing problem: Given a text T, preprocess it
so that you can answer queries of the form: given a pattern P, report the
maximal set of non-overlapping occurrences of P in T. A generalization of this
problem is the range non-overlapping indexing where in addition we are given
two indexes i,j to report the maximal set of non-overlapping occurrences
between these two indexes. We suggest new solutions for these problems. For the
non-overlapping problem our solution uses O(n) space with query time of O(m +
occ_{NO}). For the range non-overlapping problem we propose a solution with
O(n\log^\epsilon n) space for some 0<\epsilon<1 and O(m + \log\log n +
occ_{ij,NO}) query time."
"Tensors naturally model many real world processes which generate multi-aspect
data. Such processes appear in many different research disciplines, e.g,
chemometrics, computer vision, psychometrics and neuroimaging analysis. Tensor
decompositions such as the Tucker decomposition are used to analyze
multi-aspect data and extract latent factors, which capture the multilinear
data structure. Such decompositions are powerful mining tools, for extracting
patterns from large data volumes. However, most frequently used algorithms for
such decompositions involve the computationally expensive Singular Value
Decomposition.
  In this paper we propose MACH, a new sampling algorithm to compute such
decompositions. Our method is of significant practical value for tensor
streams, such as environmental monitoring systems, IP traffic matrices over
time, where large amounts of data are accumulated and the analysis is
computationally intensive but also in ""post-mortem"" data analysis cases where
the tensor does not fit in the available memory. We provide the theoretical
analysis of our proposed method, and verify its efficacy in monitoring system
applications."
"In this paper we present a new problem, the fast set intersection problem,
which is to preprocess a collection of sets in order to efficiently report the
intersection of any two sets in the collection. In addition we suggest new
solutions for the two-dimensional substring indexing problem and the document
listing problem for two patterns by reduction to the fast set intersection
problem."
"Potential maximal cliques and minimal separators are combinatorial objects
which were introduced and studied in the realm of minimal triangulations
problems including Minimum Fill-in and Treewidth. We discover unexpected
applications of these notions to the field of moderate exponential algorithms.
In particular, we show that given an n-vertex graph G together with its set of
potential maximal cliques Pi_G, and an integer t, it is possible in time |Pi_G|
* n^(O(t)) to find a maximum induced subgraph of treewidth t in G; and for a
given graph F of treewidth t, to decide if G contains an induced subgraph
isomorphic to F. Combined with an improved algorithm enumerating all potential
maximal cliques in time O(1.734601^n), this yields that both problems are
solvable in time 1.734601^n * n^(O(t))."
"In most of the shortest path problems like vehicle routing problems and
network routing problems, we only need an efficient path between two points
source and destination, and it is not necessary to calculate the shortest path
from source to all other nodes. This paper concentrates on this very idea and
presents an algorithm for calculating shortest path for (i) nonnegative
weighted undirected graphs (ii) unweighted undirected graphs. The algorithm
completes its execution in O(E) for all graphs except few in which longer path
(in terms of number of edges) from source to some node makes it best selection
for that node. The main advantage of the algorithms is its simplicity and it
does not need complex data structures for implementations."
"This paper proposes a new view to algorithms, Algorithms as defining dynamic
systems. This view extends the traditional, deterministic view that an
algorithm is a step by step procedure with nondeterminism. As a dynamic system
can be designed by a set of its defining laws, it is also desirable to design
an algorithm by a (possibly nondeterministic) set of defining laws. This
observation requires some changes to algorithm development. We propose a two
step approach, the first step is to design an algorithm via a set of defining
laws of dynamic system. The second step is to translate these laws (written in
a natural language) into a formal language such as linear logic."
"An arc-annotated string is a string of characters, called bases, augmented
with a set of pairs, called arcs, each connecting two bases. Given
arc-annotated strings $P$ and $Q$ the arc-preserving subsequence problem is to
determine if $P$ can be obtained from $Q$ by deleting bases from $Q$. Whenever
a base is deleted any arc with an endpoint in that base is also deleted.
Arc-annotated strings where the arcs are ``nested'' are a natural model of RNA
molecules that captures both the primary and secondary structure of these. The
arc-preserving subsequence problem for nested arc-annotated strings is basic
primitive for investigating the function of RNA molecules. Gramm et al. [ACM
Trans. Algorithms 2006] gave an algorithm for this problem using $O(nm)$ time
and space, where $m$ and $n$ are the lengths of $P$ and $Q$, respectively. In
this paper we present a new algorithm using $O(nm)$ time and $O(n + m)$ space,
thereby matching the previous time bound while significantly reducing the space
from a quadratic term to linear. This is essential to process large RNA
molecules where the space is likely to be a bottleneck. To obtain our result we
introduce several novel ideas which may be of independent interest for related
problems on arc-annotated strings."
"In this paper we propose and study a new complexity model for approximation
algorithms. The main motivation are practical problems over large data sets
that need to be solved many times for different scenarios, e.g., many multicast
trees that need to be constructed for different groups of users. In our model
we allow a preprocessing phase, when some information of the input graph
$G=(V,E)$ is stored in a limited size data structure. Next, the data structure
enables processing queries of the form ``solve problem A for an input
$S\subseteq V$''. We consider problems like {\sc Steiner Forest}, {\sc Facility
Location}, {\sc $k$-Median}, {\sc $k$-Center} and {\sc TSP} in the case when
the graph induces a doubling metric. Our main results are data structures of
near-linear size that are able to answer queries in time close to linear in
$|S|$. This improves over typical worst case reuniting time of approximation
algorithms in the classical setting which is $\Omega(|E|)$ independently of the
query size. In most cases, our approximation guarantees are arbitrarily close
to those in the classical setting. Additionally, we present the first fully
dynamic algorithm for the Steiner tree problem."
"Genome-wide association studies generate very large datasets that require
scalable analysis algorithms. In this report we describe the GEDI software
package, which implements efficient algorithms for performing several common
tasks in the analysis of population genotype data, including genotype error
detection and correction, imputation of both randomly missing and untyped
genotypes, and genotype phasing. Experimental results show that GEDI achieves
high accuracy with a runtime scaling linearly with the number of markers and
samples. The open source C++ code of GEDI, released under the GNU General
Public License, is available for download at
http://dna.engr.uconn.edu/software/GEDI/"
"The Pattern self-Assembly Tile set Synthesis (PATS) problem is to determine a
set of coloured tiles that self-assemble to implement a given rectangular
colour pattern. We give an exhaustive branch-and-bound algorithm to find tile
sets of minimum cardinality for the PATS problem. Our algorithm makes use of a
search tree in the lattice of partitions of the ambient rectangular grid, and
an efficient bounding function to prune this search tree. Empirical data on the
performance of the algorithm shows that it compares favourably to previously
presented heuristic solutions to the problem."
"Kosaraju in ``Computation of squares in a string'' briefly described a
linear-time algorithm for computing the minimal squares starting at each
position in a word. Using the same construction of suffix trees, we generalize
his result and describe in detail how to compute in O(k|w|)-time the minimal
k-th power, with period of length larger than s, starting at each position in a
word w for arbitrary exponent $k\geq2$ and integer $s\geq0$. We provide the
complete proof of correctness of the algorithm, which is somehow not completely
clear in Kosaraju's original paper. The algorithm can be used as a sub-routine
to detect certain types of pseudo-patterns in words, which is our original
intention to study the generalization."
"This paper discusses a new family of bounds for use in similarity search,
related to those used in metric indexing, but based on Ptolemy's inequality,
rather than the metric axioms. Ptolemy's inequality holds for the well-known
Euclidean distance, but is also shown here to hold for quadratic form metrics
in general, with Mahalanobis distance as an important special case. The
inequality is examined empirically on both synthetic and real-world data sets
and is also found to hold approximately, with a very low degree of error, for
important distances such as the angular pseudometric and several Lp norms.
Indexing experiments demonstrate a highly increased filtering power compared to
existing, triangular methods. It is also shown that combining the Ptolemaic and
triangular filtering can lead to better results than using either approach on
its own."
"We consider the Degree-Bounded Survivable Network Design Problem: the
objective is to find a minimum cost subgraph satisfying the given connectivity
requirements as well as the degree bounds on the vertices. If we denote the
upper bound on the degree of a vertex v by b(v), then we present an algorithm
that finds a solution whose cost is at most twice the cost of the optimal
solution while the degree of a degree constrained vertex v is at most 2b(v) +
2. This improves upon the results of Lau and Singh and that of Lau, Naor,
Salavatipour and Singh."
"We present a data structure that stores a sequence $s[1..n]$ over alphabet
$[1..\sigma]$ in $n\Ho(s) + o(n)(\Ho(s){+}1)$ bits, where $\Ho(s)$ is the
zero-order entropy of $s$. This structure supports the queries \access, \rank\
and \select, which are fundamental building blocks for many other compressed
data structures, in worst-case time $\Oh{\lg\lg\sigma}$ and average time
$\Oh{\lg \Ho(s)}$. The worst-case complexity matches the best previous results,
yet these had been achieved with data structures using $n\Ho(s)+o(n\lg\sigma)$
bits. On highly compressible sequences the $o(n\lg\sigma)$ bits of the
redundancy may be significant compared to the the $n\Ho(s)$ bits that encode
the data. Our representation, instead, compresses the redundancy as well.
Moreover, our average-case complexity is unprecedented. Our technique is based
on partitioning the alphabet into characters of similar frequency. The
subsequence corresponding to each group can then be encoded using fast
uncompressed representations without harming the overall compression ratios,
even in the redundancy. The result also improves upon the best current
compressed representations of several other data structures. For example, we
achieve $(i)$ compressed redundancy, retaining the best time complexities, for
the smallest existing full-text self-indexes; $(ii)$ compressed permutations
$\pi$ with times for $\pi()$ and $\pii()$ improved to loglogarithmic; and
$(iii)$ the first compressed representation of dynamic collections of disjoint
sets. We also point out various applications to inverted indexes, suffix
arrays, binary relations, and data compressors. ..."
"The {\em longest common subsequence (LCS)} problem is a classic and
well-studied problem in computer science. LCS is a central problem in
stringology and finds broad applications in text compression, error-detecting
codes and biological sequence comparison. However, in numerous contexts, words
represent cyclic sequences of symbols and LCS must be generalized to consider
all circular shifts of the strings. This occurs especially in computational
biology when genetic material is sequenced form circular DNA or RNA molecules.
This initiates the problem of {\em longest common cyclic subsequence (LCCS)}
which finds the longest subsequence between all circular shifts of two strings.
In this paper, we give an $O(n^2)$ algorithm for solving LCCS problem where $n$
is the number of symbols in the strings."
"We present an algorithm that finds a feedback arc set of size $k$ in a
tournament in time $n^{O(1)}2^{O(\sqrt{k})}$. This is asymptotically faster
than the running time of previously known algorithms for this problem."
"We give the first polynomial-time approximation scheme (PTAS) for the Steiner
forest problem on planar graphs and, more generally, on graphs of bounded
genus. As a first step, we show how to build a Steiner forest spanner for such
graphs. The crux of the process is a clustering procedure called
prize-collecting clustering that breaks down the input instance into separate
subinstances which are easier to handle; moreover, the terminals in different
subinstances are far from each other. Each subinstance has a relatively
inexpensive Steiner tree connecting all its terminals, and the subinstances can
be solved (almost) separately. Another building block is a PTAS for Steiner
forest on graphs of bounded treewidth. Surprisingly, Steiner forest is NP-hard
even on graphs of treewidth 3. Therefore, our PTAS for bounded treewidth graph
needs a nontrivial combination of approximation arguments and dynamic
programming on the tree decomposition. We further show that Steiner forest can
be solved in polynomial time for series-parallel graphs (graphs of treewidth at
most two) by a novel combination of dynamic programming and minimum cut
computations, completing our thorough complexity study of Steiner forest in the
range of bounded treewidth graphs, planar graphs, and bounded genus graphs."
"We settle the question of tight thresholds for offline cuckoo hashing. The
problem can be stated as follows: we have n keys to be hashed into m buckets
each capable of holding a single key. Each key has k >= 3 (distinct) associated
buckets chosen uniformly at random and independently of the choices of other
keys. A hash table can be constructed successfully if each key can be placed
into one of its buckets. We seek thresholds alpha_k such that, as n goes to
infinity, if n/m <= alpha for some alpha < alpha_k then a hash table can be
constructed successfully with high probability, and if n/m >= alpha for some
alpha > alpha_k a hash table cannot be constructed successfully with high
probability. Here we are considering the offline version of the problem, where
all keys and hash values are given, so the problem is equivalent to previous
models of multiple-choice hashing. We find the thresholds for all values of k >
2 by showing that they are in fact the same as the previously known thresholds
for the random k-XORSAT problem. We then extend these results to the setting
where keys can have differing number of choices, and provide evidence in the
form of an algorithm for a conjecture extending this result to cuckoo hash
tables that store multiple keys in a bucket."
"Submodularity is a fundamental phenomenon in combinatorial optimization.
Submodular functions occur in a variety of combinatorial settings such as
coverage problems, cut problems, welfare maximization, and many more.
Therefore, a lot of work has been concerned with maximizing or minimizing a
submodular function, often subject to combinatorial constraints. Many of these
algorithmic results exhibit a common structure. Namely, the function is
extended to a continuous, usually non-linear, function on a convex domain.
Then, this relaxation is solved, and the fractional solution rounded to yield
an integral solution. Often, the continuous extension has a natural
interpretation in terms of distributions on subsets of the ground set. This
interpretation is often crucial to the results and their analysis. The purpose
of this survey is to highlight this connection between extensions,
distributions, relaxations, and optimization in the context of submodular
functions. We also present the first constant factor approximation algorithm
for minimizing symmetric submodular functions subject to a cardinality
constraint."
"The second eigenvalue of the Laplacian matrix and its associated eigenvector
are fundamental features of an undirected graph, and as such they have found
widespread use in scientific computing, machine learning, and data analysis. In
many applications, however, graphs that arise have several \emph{local} regions
of interest, and the second eigenvector will typically fail to provide
information fine-tuned to each local region. In this paper, we introduce a
locally-biased analogue of the second eigenvector, and we demonstrate its
usefulness at highlighting local properties of data graphs in a semi-supervised
manner. To do so, we first view the second eigenvector as the solution to a
constrained optimization problem, and we incorporate the local information as
an additional constraint; we then characterize the optimal solution to this new
problem and show that it can be interpreted as a generalization of a
Personalized PageRank vector; and finally, as a consequence, we show that the
solution can be computed in nearly-linear time. In addition, we show that this
locally-biased vector can be used to compute an approximation to the best
partition \emph{near} an input seed set in a manner analogous to the way in
which the second eigenvector of the Laplacian can be used to obtain an
approximation to the best partition in the entire input graph. Such a primitive
is useful for identifying and refining clusters locally, as it allows us to
focus on a local region of interest in a semi-supervised manner. Finally, we
provide a detailed empirical evaluation of our method by showing how it can
applied to finding locally-biased sparse cuts around an input vertex seed set
in social and information networks."
"We show that, given a string $s$ of length $n$, with constant memory and
logarithmic passes over a constant number of streams we can build a
context-free grammar that generates $s$ and only $s$ and whose size is within
an $\Oh{\min (g \log g, \sqrt{n \log g})}$-factor of the minimum $g$. This
stands in contrast to our previous result that, with polylogarithmic memory and
polylogarithmic passes over a single stream, we cannot build such a grammar
whose size is within any polynomial of $g$."
"It has been shown by Alon et al. that the so-called 'all-pairs shortest-path'
problem can be solved in O((MV)^2.688 * log^3(V)) for graphs with V vertices,
with integer distances bounded by M. We solve the more general problem for
graphs in R (assuming no negative cycles), with expected-case running time
O(V^2.5 * log(V)). While our result appears to violate the Omega(V^3)
requirement of ""Funny Matrix Multiplication"" (due to Kerr), we find that it has
a sub-cubic expected time solution subject to reasonable conditions on the data
distribution. The expected time solution arises when certain sub-problems are
uncorrelated, though we can do better/worse than the expected-case under
positive/negative correlation (respectively). Whether we observe
positive/negative correlation depends on the statistics of the graph in
question. In practice, our algorithm is significantly faster than
Floyd-Warshall, even for dense graphs."
"The general problem of robust optimization is this: one of several possible
scenarios will appear tomorrow, but things are more expensive tomorrow than
they are today. What should you anticipatorily buy today, so that the
worst-case cost (summed over both days) is minimized? Feige et al. and
Khandekar et al. considered the k-robust model where the possible outcomes
tomorrow are given by all demand-subsets of size k, and gave algorithms for the
set cover problem, and the Steiner tree and facility location problems in this
model, respectively.
  In this paper, we give the following simple and intuitive template for
k-robust problems: ""having built some anticipatory solution, if there exists a
single demand whose augmentation cost is larger than some threshold, augment
the anticipatory solution to cover this demand as well, and repeat"". In this
paper we show that this template gives us improved approximation algorithms for
k-robust Steiner tree and set cover, and the first approximation algorithms for
k-robust Steiner forest, minimum-cut and multicut. All our approximation ratios
(except for multicut) are almost best possible.
  As a by-product of our techniques, we also get algorithms for max-min
problems of the form: ""given a covering problem instance, which k of the
elements are costliest to cover?""."
"In this paper, we consider Steiner forest and its generalizations,
prize-collecting Steiner forest and k-Steiner forest, when the vertices of the
input graph are points in the Euclidean plane and the lengths are Euclidean
distances. First, we present a simpler analysis of the polynomial-time
approximation scheme (PTAS) of Borradaile et al. [12] for the Euclidean Steiner
forest problem. This is done by proving a new structural property and modifying
the dynamic programming by adding a new piece of information to each dynamic
programming state. Next we develop a PTAS for a well-motivated case, i.e., the
multiplicative case, of prize-collecting and budgeted Steiner forest. The ideas
used in the algorithm may have applications in design of a broad class of
bicriteria PTASs. At the end, we demonstrate why PTASs for these problems can
be hard in the general Euclidean case (and thus for PTASs we cannot go beyond
the multiplicative case)."
"This paper presents a means with time complexity of at worst O(n^3) to
compute the discrete logarithm on cyclic finite groups of integers modulo p.
The algorithm makes use of reduction of the problem to that of finding the
concurrent zeros of two periodic functions in the real numbers. The problem is
treated as an analog to a form of analog rotor-code computed cipher."
"In this paper, I proposed to utilize partial-order alignment technique as a
heuristic method to cope with the state-space explosion problem in progressive
near-optimal alignment. The key idea of my approach is a formal treatment of
progressive partial order alignment based on the graph product construction."
"In the uncapacitated facility location problem, given a graph, a set of
demands and opening costs, it is required to find a set of facilities R, so as
to minimize the sum of the cost of opening the facilities in R and the cost of
assigning all node demands to open facilities. This paper concerns the robust
fault-tolerant version of the uncapacitated facility location problem (RFTFL).
In this problem, one or more facilities might fail, and each demand should be
supplied by the closest open facility that did not fail. It is required to find
a set of facilities R, so as to minimize the sum of the cost of opening the
facilities in R and the cost of assigning all node demands to open facilities
that did not fail, after the failure of up to \alpha facilities. We present a
polynomial time algorithm that yields a 6.5-approximation for this problem with
at most one failure and a 1.5 + 7.5\alpha-approximation for the problem with at
most \alpha > 1 failures. We also show that the RFTFL problem is NP-hard even
on trees, and even in the case of a single failure."
"Multiplicative inverse is a crucial operation in public key cryptography, and
been widely used in cryptography. Public key cryptography has given rise to
such a need, in which we need to generate a related public and private pair of
numbers, each of which is the inverse of the other. The basic method to find
multiplicative inverses is Extended-Euclidean method. In this paper we will
propose a new algorithm for computing the inverse, based on continues subtract
fraction from integer and divide by fraction to obtain integer that will be
used to compute the inverse d. The authors claim that the proposed method more
efficient and faster than the existed methods."
"The past decade has witnessed many interesting algorithms for maintaining
statistics over a data stream. This paper initiates a theoretical study of
algorithms for monitoring distributed data streams over a time-based sliding
window (which contains a variable number of items and possibly out-of-order
items). The concern is how to minimize the communication between individual
streams and the root, while allowing the root, at any time, to be able to
report the global statistics of all streams within a given error bound. This
paper presents communication-efficient algorithms for three classical
statistics, namely, basic counting, frequent items and quantiles. The
worst-case communication cost over a window is $O(\frac{k} {\epsilon} \log
\frac{\epsilon N}{k})$ bits for basic counting and $O(\frac{k}{\epsilon} \log
\frac{N}{k})$ words for the remainings, where $k$ is the number of distributed
data streams, $N$ is the total number of items in the streams that arrive or
expire in the window, and $\epsilon < 1$ is the desired error bound. Matching
and nearly matching lower bounds are also obtained."
"In this paper, we propose a framework to solve a demand-supply optimization
problem of long-term water resource allocation on a multi-connection reservoir
network which, in two aspects, is different to the problem considered in
previous works. First, while all previous works consider a problem where each
reservoir can transfer water to only one fixed reservoir, we consider a
multi-connection network being constructed in Thailand in which each reservoir
can transfer water to many reservoirs in one period of time. Second, a
demand-supply plan considered here is static, in contrast to a dynamic policy
considered in previous works. Moreover, in order to efficiently develop a
long-term static plan, a severe loss (a risk) is taken into account, i.e. a
risk occurs if the real amount of water stored in each reservoir in each time
period is less than what planned by the optimizer. The multi-connection
function and the risk make the problem rather complex such that traditional
stochastic dynamic programming and deterministic/heuristic approaches are
inappropriate. Our framework is based on a novel convex programming formulation
in which stochastic information can be naturally taken into account and an
optimal solution is guaranteed to be found efficiently. Extensive experimental
results show promising results of the framework."
"The performance of a dynamic dictionary is measured mainly by its update
time, lookup time, and space consumption. In terms of update time and lookup
time there are known constructions that guarantee constant-time operations in
the worst case with high probability, and in terms of space consumption there
are known constructions that use essentially optimal space. However, although
the first analysis of a dynamic dictionary dates back more than 45 years ago
(when Knuth analyzed linear probing in 1963), the trade-off between these
aspects of performance is still not completely understood. In this paper we
settle two fundamental open problems: - We construct the first dynamic
dictionary that enjoys the best of both worlds: it stores n elements using
(1+epsilon)n memory words, and guarantees constant-time operations in the worst
case with high probability. Specifically, for any epsilon = \Omega((\log\log n
/ \log n)^{1/2} ) and for any sequence of polynomially many operations, with
high probability over the randomness of the initialization phase, all
operations are performed in constant time which is independent of epsilon. The
construction is a two-level variant of cuckoo hashing, augmented with a
""backyard"" that handles a large fraction of the elements, together with a
de-amortized perfect hashing scheme for eliminating the dependency on epsilon.
- We present a variant of the above construction that uses only (1+o(1))B bits,
where B is the information-theoretic lower bound for representing a set of size
n taken from a universe of size u, and guarantees constant-time operations in
the worst case with high probability, as before. This problem was open even in
the amortized setting. One of the main ingredients of our construction is a
permutation-based variant of cuckoo hashing, which significantly improves the
space consumption of cuckoo hashing when dealing with a rather small universe."
"We develop a variable depth search heuristic for the quadratic assignment
problem. The heuristic is based on sequential changes in assignments analogous
to the Lin-Kernighan sequential edge moves for the traveling salesman problem.
We treat unstructured problem instances of sizes 60 to 400. When the heuristic
is used in conjunction with robust tabu search, we measure performance
improvements of up to a factor of 15 compared to the use of robust tabu alone.
The performance improvement increases as the problem size increases."
"This paper addresses the uniform random generation of words from a
context-free language (over an alphabet of size $k$), while constraining every
letter to a targeted frequency of occurrence. Our approach consists in a
multidimensional extension of Boltzmann samplers \cite{Duchon2004}. We show
that, under mostly \emph{strong-connectivity} hypotheses, our samplers return a
word of size in $[(1-\varepsilon)n, (1+\varepsilon)n]$ and exact frequency in
$\mathcal{O}(n^{1+k/2})$ expected time. Moreover, if we accept tolerance
intervals of width in $\Omega(\sqrt{n})$ for the number of occurrences of each
letters, our samplers perform an approximate-size generation of words in
expected $\mathcal{O}(n)$ time. We illustrate these techniques on the
generation of Tetris tessellations with uniform statistics in the different
types of tetraminoes."
"We develop, analyze and experiment with a new tool, called MADMX, which
extracts frequent motifs, possibly including don't care characters, from
biological sequences. We introduce density, a simple and flexible measure for
bounding the number of don't cares in a motif, defined as the ratio of solid
(i.e., different from don't care) characters to the total length of the motif.
By extracting only maximal dense motifs, MADMX reduces the output size and
improves performance, while enhancing the quality of the discoveries. The
efficiency of our approach relies on a newly defined combining operation,
dubbed fusion, which allows for the construction of maximal dense motifs in a
bottom-up fashion, while avoiding the generation of nonmaximal ones. We provide
experimental evidence of the efficiency and the quality of the motifs returned
by MADMX"
"We present a new heuristic point-to-point routing algorithm based on
contraction hierarchies (CH). Given an epsilon >= 0, we can prove that the
length of the path computed by our algorithm is at most (1+epsilon) times the
length of the optimal (shortest) path. CH is based on node contraction:
removing nodes from a network and adding shortcut edges to preserve shortest
path distances. Our algorithm tries to avoid shortcuts even when a replacement
path is epsilon times longer."
"We address in this paper a new computational biology problem that aims at
understanding a mechanism that could potentially be used to genetically
manipulate natural insect populations infected by inherited, intra-cellular
parasitic bacteria. In this problem, that we denote by \textsc{Mod/Resc
Parsimony Inference}, we are given a boolean matrix and the goal is to find two
other boolean matrices with a minimum number of columns such that an
appropriately defined operation on these matrices gives back the input. We show
that this is formally equivalent to the \textsc{Bipartite Biclique Edge Cover}
problem and derive some complexity results for our problem using this
equivalence. We provide a new, fixed-parameter tractability approach for
solving both that slightly improves upon a previously published algorithm for
the \textsc{Bipartite Biclique Edge Cover}. Finally, we present experimental
results where we applied some of our techniques to a real-life data set."
"A natural way to deal with multiple, partially conflicting objectives is
turning all the objectives but one into budget constraints. Some classical
polynomial-time optimization problems, such as spanning tree and forest,
shortest path, (perfect) matching, independent set (basis) in a matroid or in
the intersection of two matroids, become NP-hard even with one budget
constraint. Still, for most of these problems deterministic and randomized
polynomial-time approximation schemes are known. In the case of two or more
budgets, typically only multi-criteria approximation schemes are available,
which return slightly infeasible solutions. Not much is known however for the
case of strict budget constraints: filling this gap is the main goal of this
paper.
  We show that shortest path, perfect matching, and spanning tree (and hence
matroid basis and matroid intersection basis) are inapproximable already with
two budget constraints. For the remaining problems, whose set of solutions
forms an independence system, we present deterministic and randomized
polynomial-time approximation schemes for a constant number k of budget
constraints. Our results are based on a variety of techniques:
  1. We present a simple and powerful mechanism to transform multi-criteria
approximation schemes into pure approximation schemes.
  2. We show that points in low dimensional faces of any matroid polytope are
almost integral, an interesting result on its own. This gives a deterministic
approximation scheme for k-budgeted matroid independent set.
  3. We present a deterministic approximation scheme for 2-budgeted matching.
The backbone of this result is a purely topological property of curves in R^2."
"Non-dominated Sorting Genetic Algorithm (NSGA) has established itself as a
benchmark algorithm for Multiobjective Optimization. The determination of
pareto-optimal solutions is the key to its success. However the basic algorithm
suffers from a high order of complexity, which renders it less useful for
practical applications. Among the variants of NSGA, several attempts have been
made to reduce the complexity. Though successful in reducing the runtime
complexity, there is scope for further improvements, especially considering
that the populations involved are frequently of large size. We propose a
variant which reduces the run-time complexity using the simple principle of
space-time trade-off. The improved algorithm is applied to the problem of
classifying types of leukemia based on microarray data. Results of comparative
tests are presented showing that the improved algorithm performs well on large
populations."
"Given a metric space $(X,d_X)$, the earth mover distance between two
distributions over $X$ is defined as the minimum cost of a bipartite matching
between the two distributions. The doubling dimension of a metric $(X, d_X)$ is
the smallest value $\alpha$ such that every ball in $X$ can be covered by
$2^\alpha$ ball of half the radius. We study efficient algorithms for
approximating earth mover distance over metrics with bounded doubling
dimension.
  Given a metric $(X, d_X)$, with $|X| = n$, we can use $\tilde O(n^2)$
preprocessing time to create a data structure of size $\tilde O(n^{1 + \e})$,
such that subsequently queried EMDs can be $O(\alpha_X/\e)$-approximated in
$\tilde O(n)$ time.
  We also show a weaker form of sketching scheme, which we call ""encoding
scheme"". Given $(X, d_X)$, by using $\tilde O(n^2)$ preprocessing time, every
subsequent distribution $\mu$ over $X$ can be encoded into $F(\mu)$ in $\tilde
O(n^{1 + \e})$ time. Given $F(\mu)$ and $F(\nu)$, the EMD between $\mu$ and
$\nu$ can be $O(\alpha_X/\e)$-approximated in $\tilde O(n^\e)$ time."
"Every human likes choices. But today's fast route planning algorithms usually
compute just a single route between source and target. There are beginnings to
compute alternative routes, but this topic has not been studied thoroughly.
Often, the aspect of meaningful alternative routes is neglected from a human
point of view. We fill in this gap by suggesting mathematical definitions for
such routes. As a second contribution we propose heuristics to compute them, as
this is NP-hard in general."
"We consider the following sample selection problem. We observe in an online
fashion a sequence of samples, each endowed by a quality. Our goal is to either
select or reject each sample, so as to maximize the aggregate quality of the
subsample selected so far. There is a natural trade-off here between the rate
of selection and the aggregate quality of the subsample. We show that for a
number of such problems extremely simple and oblivious ""threshold rules"" for
selection achieve optimal tradeoffs between rate of selection and aggregate
quality in a probabilistic sense. In some cases we show that the same threshold
rule is optimal for a large class of quality distributions and is thus
oblivious in a strong sense."
"We present the zipper tree, an $O(\log \log n)$-competitive online binary
search tree that performs each access in $O(\log n)$ worst-case time. This
shows that for binary search trees, optimal worst-case access time and
near-optimal amortized access time can be guaranteed simultaneously."
"We consider the problem of constructing optimal decision trees: given a
collection of tests which can disambiguate between a set of $m$ possible
diseases, each test having a cost, and the a-priori likelihood of the patient
having any particular disease, what is a good adaptive strategy to perform
these tests to minimize the expected cost to identify the disease? We settle
the approximability of this problem by giving a tight $O(\log m)$-approximation
algorithm. We also consider a more substantial generalization, the Adaptive TSP
problem. Given an underlying metric space, a random subset $S$ of cities is
drawn from a known distribution, but $S$ is initially unknown to us--we get
information about whether any city is in $S$ only when we visit the city in
question. What is a good adaptive way of visiting all the cities in the random
subset $S$ while minimizing the expected distance traveled? For this problem,
we give the first poly-logarithmic approximation, and show that this algorithm
is best possible unless we can improve the approximation guarantees for the
well-known group Steiner tree problem."
"We investigate a special case of the Induced Subgraph Isomorphism problem,
where both input graphs are interval graphs. We show the NP-hardness of this
problem, and we prove fixed-parameter tractability of the problem with
non-standard parameterization, where the parameter is the difference
|V(G)|-|V(H)|, with G and H being the larger and the smaller input graph,
respectively. Intuitively, we can interpret this problem as ""cleaning"" the
graph G, regarded as a pattern containing extra vertices indicating errors, in
order to obtain the graph H representing the original pattern. We also prove
W[1]-hardness for the standard parameterization where the parameter is |V(H)|."
"In a column-restricted covering integer program (CCIP), all the non-zero
entries of any column of the constraint matrix are equal. Such programs capture
capacitated versions of covering problems. In this paper, we study the
approximability of CCIPs, in particular, their relation to the integrality gaps
of the underlying 0,1-CIP.
  If the underlying 0,1-CIP has an integrality gap O(gamma), and assuming that
the integrality gap of the priority version of the 0,1-CIP is O(omega), we give
a factor O(gamma + omega) approximation algorithm for the CCIP. Priority
versions of 0,1-CIPs (PCIPs) naturally capture quality of service type
constraints in a covering problem.
  We investigate priority versions of the line (PLC) and the (rooted) tree
cover (PTC) problems. Apart from being natural objects to study, these problems
fall in a class of fundamental geometric covering problems. We bound the
integrality of certain classes of this PCIP by a constant. Algorithmically, we
give a polytime exact algorithm for PLC, show that the PTC problem is APX-hard,
and give a factor 2-approximation algorithm for it."
"We present an algorithm that on input of an $n$-vertex $m$-edge weighted
graph $G$ and a value $k$, produces an {\em incremental sparsifier} $\hat{G}$
with $n-1 + m/k$ edges, such that the condition number of $G$ with $\hat{G}$ is
bounded above by $\tilde{O}(k\log^2 n)$, with probability $1-p$. The algorithm
runs in time
  $$\tilde{O}((m \log{n} + n\log^2{n})\log(1/p)).$$
  As a result, we obtain an algorithm that on input of an $n\times n$ symmetric
diagonally dominant matrix $A$ with $m$ non-zero entries and a vector $b$,
computes a vector ${x}$ satisfying $||{x}-A^{+}b||_A<\epsilon ||A^{+}b||_A $,
in expected time
  $$\tilde{O}(m\log^2{n}\log(1/\epsilon)).$$
  The solver is based on repeated applications of the incremental sparsifier
that produces a chain of graphs which is then used as input to a recursive
preconditioned Chebyshev iteration."
"Iterative rounding and relaxation have arguably become the method of choice
in dealing with unconstrained and constrained network design problems. In this
paper we extend the scope of the iterative relaxation method in two directions:
(1) by handling more complex degree constraints in the minimum spanning tree
problem (namely, laminar crossing spanning tree), and (2) by incorporating
`degree bounds' in other combinatorial optimization problems such as matroid
intersection and lattice polyhedra. We give new or improved approximation
algorithms, hardness results, and integrality gaps for these problems."
"We study policy iteration for infinite-horizon Markov decision processes. It
has recently been shown policy iteration style algorithms have exponential
lower bounds in a two player game setting. We extend these lower bounds to
Markov decision processes with the total reward and average-reward optimality
criteria."
"Memetic Algorithms are known to be a powerful technique in solving hard
optimization problems. To design a memetic algorithm one needs to make a host
of decisions; selecting a population size is one of the most important among
them. Most algorithms in the literature fix the population size to a certain
constant value. This reduces the algorithm's quality since the optimal
population size varies for different instances, local search procedures and
running times. In this paper we propose an adjustable population size. It is
calculated as a function of the running time of the whole algorithm and the
average running time of the local search for the given instance. Note that in
many applications the running time of a heuristic should be limited and
therefore we use this limit as a parameter of the algorithm. The average
running time of the local search procedure is obtained during the algorithm's
run. Some coefficients which are independent with respect to the instance or
the local search are to be tuned before the algorithm run; we provide a
procedure to find these coefficients. The proposed approach was used to develop
a memetic algorithm for the Multidimensional Assignment Problem (MAP or s-AP in
the case of s dimensions) which is an extension of the well-known assignment
problem. MAP is NP-hard and has a host of applications. We show that using
adjustable population size makes the algorithm flexible to perform well for
instances of very different sizes and types and for different running times and
local searches. This allows us to select the most efficient local search for
every instance type. The results of computational experiments for several
instance families and sizes prove that the proposed algorithm performs
efficiently for a wide range of the running times and clearly outperforms the
state-of-the art 3-AP memetic algorithm being given the same time."
"I will present a way to implement graph algorithms which is different from
traditional methods. This work was motivated by the belief that some ideas from
software engineering should be applied to graph algorithms. Re-usability of
software is an important and difficult problem in general, and this is
particularly true for graph algorithms. The scientific literature demonstrates
plenty of applications of graph algorithms as subroutines for other algorithms.
Moreover, many practical problems from various domains may be modeled as graph
problems and hence solved by means of graph algorithms. Chapter 2 introduces
some data structures that will be used in 5 basic graph algorithms in chapter
3. Chapter 4 discusses an implementation of a maximum cardinality matching
algorithm for general graphs. Chapter 5 explains some techniques in C++, which
are useful to implement the data structures and algorithms in an efficient way.
Finally chapter 6 contains some concluding remarks."
"The Lin-Kernighan heuristic is known to be one of the most successful
heuristics for the Traveling Salesman Problem (TSP). It has also proven its
efficiency in application to some other problems. In this paper we discuss
possible adaptations of TSP heuristics for the Generalized Traveling Salesman
Problem (GTSP) and focus on the case of the Lin-Kernighan algorithm. At first,
we provide an easy-to-understand description of the original Lin-Kernighan
heuristic. Then we propose several adaptations, both trivial and complicated.
Finally, we conduct a fair competition between all the variations of the
Lin-Kernighan adaptation and some other GTSP heuristics. It appears that our
adaptation of the Lin-Kernighan algorithm for the GTSP reproduces the success
of the original heuristic. Different variations of our adaptation outperform
all other heuristics in a wide range of trade-offs between solution quality and
running time, making Lin-Kernighan the state-of-the-art GTSP local search."
"We propose an extension of tree-based space-partitioning indexing structures
for data with low intrinsic dimensionality embedded in a high dimensional
space. We call this extension an Angle Tree. Our extension can be applied to
both classical kd-trees as well as the more recent rp-trees. The key idea of
our approach is to store the angle (the ""dihedral angle"") between the data
region (which is a low dimensional manifold) and the random hyperplane that
splits the region (the ""splitter""). We show that the dihedral angle can be used
to obtain a tight lower bound on the distance between the query point and any
point on the opposite side of the splitter. This in turn can be used to
efficiently prune the search space. We introduce a novel randomized strategy to
efficiently calculate the dihedral angle with a high degree of accuracy.
Experiments and analysis on real and synthetic data sets shows that the Angle
Tree is the most efficient known indexing structure for nearest neighbor
queries in terms of preprocessing and space usage while achieving high accuracy
and fast search time."
"We combine the work of Garg and Konemann, and Fleischer with ideas from
dynamic graph algorithms to obtain faster (1-eps)-approximation schemes for
various versions of the multicommodity flow problem. In particular, if eps is
moderately small and the size of every number used in the input instance is
polynomially bounded, the running times of our algorithms match - up to
poly-logarithmic factors and some provably optimal terms - the Omega(mn)
flow-decomposition barrier for single-commodity flow."
"Given a directed acyclic graph with labeled vertices, we consider the problem
of finding the most common label sequences (""traces"") among all paths in the
graph (of some maximum length m). Since the number of paths can be huge, we
propose novel algorithms whose time complexity depends only on the size of the
graph, and on the relative frequency epsilon of the most frequent traces. In
addition, we apply techniques from streaming algorithms to achieve space usage
that depends only on epsilon, and not on the number of distinct traces. The
abstract problem considered models a variety of tasks concerning finding
frequent patterns in event sequences. Our motivation comes from working with a
data set of 2 million RFID readings from baggage trolleys at Copenhagen
Airport. The question of finding frequent passenger movement patterns is mapped
to the above problem. We report on experimental findings for this data set."
"We introduce the Deletable Bloom filter (DlBF) as a new spin on the popular
data structure based on compactly encoding the information of where collisions
happen when inserting elements. The DlBF design enables false-negative-free
deletions at a fraction of the cost in memory consumption, which turns to be
appealing for certain probabilistic filter applications."
"We show a deterministic constant-time parallel algorithm for finding an
almost maximum flow in multisource-multitarget networks with bounded degrees
and bounded edge capacities. As a consequence, we show that the value of the
maximum flow over the number of nodes is a testable parameter on these
networks."
"Given an undirected graph $G$ and an error parameter $\epsilon > 0$, the {\em
graph sparsification} problem requires sampling edges in $G$ and giving the
sampled edges appropriate weights to obtain a sparse graph $G_{\epsilon}$ with
the following property: the weight of every cut in $G_{\epsilon}$ is within a
factor of $(1\pm \epsilon)$ of the weight of the corresponding cut in $G$. If
$G$ is unweighted, an $O(m\log n)$-time algorithm for constructing
$G_{\epsilon}$ with $O(n\log n/\epsilon^2)$ edges in expectation, and an
$O(m)$-time algorithm for constructing $G_{\epsilon}$ with $O(n\log^2
n/\epsilon^2)$ edges in expectation have recently been developed
(Hariharan-Panigrahi, 2010). In this paper, we improve these results by giving
an $O(m)$-time algorithm for constructing $G_{\epsilon}$ with $O(n\log
n/\epsilon^2)$ edges in expectation, for unweighted graphs. Our algorithm is
optimal in terms of its time complexity; further, no efficient algorithm is
known for constructing a sparser $G_{\epsilon}$. Our algorithm is Monte-Carlo,
i.e. it produces the correct output with high probability, as are all efficient
graph sparsification algorithms."
"Estimating the first moment of a data stream defined as $F_1 = \sum_{i \in
\{1, 2, \ldots, n\}} \abs{f_i}$ to within $1 \pm \epsilon$-relative error with
high probability is a basic and influential problem in data stream processing.
A tight space bound of $O(\epsilon^{-2} \log (mM))$ is known from the work of
[Kane-Nelson-Woodruff-SODA10]. However, all known algorithms for this problem
require per-update stream processing time of $\Omega(\epsilon^{-2})$, with the
only exception being the algorithm of [Ganguly-Cormode-RANDOM07] that requires
per-update processing time of $O(\log^2(mM)(\log n))$ albeit with sub-optimal
space $O(\epsilon^{-3}\log^2(mM))$. In this paper, we present an algorithm for
estimating $F_1$ that achieves near-optimality in both space and update
processing time. The space requirement is $O(\epsilon^{-2}(\log n + (\log
\epsilon^{-1})\log(mM)))$ and the per-update processing time is $O( (\log
n)\log (\epsilon^{-1}))$."
"The round-trip distance function on a geographic network (such as a road
network, flight network, or utility distribution grid) defines the ""distance""
from a single vertex to a pair of vertices as the minimum length tour visiting
all three vertices and ending at the starting vertex. Given a geographic
network and a subset of its vertices called ""sites"" (for example a road network
with a list of grocery stores), a two-site round-trip Voronoi diagram labels
each vertex in the network with the pair of sites that minimizes the round-trip
distance from that vertex. Alternatively, given a geographic network and two
sets of sites of different types (for example grocery stores and coffee shops),
a two-color round-trip Voronoi diagram labels each vertex with the pair of
sites of different types minimizing the round-trip distance. In this paper, we
prove several new properties of two-site and two-color round-trip Voronoi
diagrams in a geographic network, including a relationship between the
""doubling density"" of sites and an upper bound on the number of non-empty
Voronoi regions. We show how those lemmas can be used in new algorithms
asymptotically more efficient than previous known algorithms when the networks
have reasonable distribution properties related to doubling density, and we
provide experimental data suggesting that road networks with standard
point-of-interest sites have these properties."
"A data stream is viewed as a sequence of $M$ updates of the form
$(\text{index},i,v)$ to an $n$-dimensional integer frequency vector $f$, where
the update changes $f_i$ to $f_i + v$, and $v$ is an integer and assumed to be
in $\{-m, ..., m\}$. The $p$th frequency moment $F_p$ is defined as
$\sum_{i=1}^n \abs{f_i}^p$. We consider the problem of estimating $F_p$ to
within a multiplicative approximation factor of $1\pm \epsilon$, for $p \in
[0,2]$. Several estimators have been proposed for this problem, including
Indyk's median estimator \cite{indy:focs00}, Li's geometric means estimator
\cite{pinglib:2006}, an \Hss-based estimator \cite{gc:random07}. The first two
estimators require space $\tilde{O}(\epsilon^{-2})$, where the $\tilde{O}$
notation hides polylogarithmic factors in $\epsilon^{-1}, m, n$ and $M$.
Recently, Kane, Nelson and Woodruff in \cite{knw:soda10} present a
space-optimal and novel estimator, called the log-cosine estimator. In this
paper, we present an elementary analysis of the log-cosine estimator in a
stand-alone setting. The analysis in \cite{knw:soda10} is more complicated."
"In this paper we present a modification of a technique by Chiba and Nishizeki
[Chiba and Nishizeki: Arboricity and Subgraph Listing Algorithms, SIAM J.
Comput. 14(1), pp. 210--223 (1985)]. Based on it, we design a data structure
suitable for dynamic graph algorithms. We employ the data structure to
formulate new algorithms for several problems, including counting subgraphs of
four vertices, recognition of diamond-free graphs, cop-win graphs and strongly
chordal graphs, among others. We improve the time complexity for graphs with
low arboricity or h-index."
"The study of {\em balls-into-bins processes} or {\em occupancy problems} has
a long history. These processes can be used to translate realistic problems
into mathematical ones in a natural way. In general, the goal of a
balls-into-bins process is to allocate a set of independent objects (tasks,
jobs, balls) to a set of resources (servers, bins, urns) and, thereby, to
minimize the maximum load. In this paper, we analyze the maximum load for the
{\em chains-into-bins} problem, which is defined as follows. There are $n$
bins, and $m$ objects to be allocated. Each object consists of balls connected
into a chain of length $\ell$, so that there are $m \ell$ balls in total. We
assume the chains cannot be broken, and that the balls in one chain have to be
allocated to $\ell$ consecutive bins. We allow each chain $d$ independent and
uniformly random bin choices for its starting position. The chain is allocated
using the rule that the maximum load of any bin receiving a ball of that chain
is minimized. We show that, for $d \ge 2$ and $m\cdot\ell=O(n)$, the maximum
load is $((\ln \ln m)/\ln d) +O(1)$ with probability $1-\tilde O(1/m^{d-1})$."
"For many algorithmic problems, traditional algorithms that optimise on the
number of instructions executed prove expensive on I/Os. Novel and very
different design techniques, when applied to these problems, can produce
algorithms that are I/O efficient. This thesis adds to the growing chorus of
such results. The computational models we use are the external memory model and
the W-Stream model.
  On the external memory model, we obtain the following results. (1) An I/O
efficient algorithm for computing minimum spanning trees of graphs that
improves on the performance of the best known algorithm. (2) The first external
memory version of soft heap, an approximate meldable priority queue. (3) Hard
heap, the first meldable external memory priority queue that matches the
amortised I/O performance of the known external memory priority queues, while
allowing a meld operation at the same amortised cost. (4) I/O efficient exact,
approximate and randomised algorithms for the minimum cut problem, which has
not been explored before on the external memory model. (5) Some lower and upper
bounds on I/Os for interval graphs.
  On the W-Stream model, we obtain the following results. (1) Algorithms for
various tree problems and list ranking that match the performance of the best
known algorithms and are easier to implement than them. (2) Pass efficient
algorithms for sorting, and the maximal independent set problems, that improve
on the best known algorithms. (3) Pass efficient algorithms for the graphs
problems of finding vertex-colouring, approximate single source shortest paths,
maximal matching, and approximate weighted vertex cover. (4) Lower bounds on
passes for list ranking and maximal matching.
  We propose two variants of the W-Stream model, and design algorithms for the
maximal independent set, vertex-colouring, and planar graph single source
shortest paths problems on those models."
"We consider scheduling packets with values in a capacity-bounded buffer in an
online setting. In this model, there is a buffer with limited capacity $B$. At
any time, the buffer cannot accommodate more than $B$ packets. Packets arrive
over time. Each packet is associated with a non-negative value. Packets leave
the buffer only because they are either sent or dropped. Those packets that
have left the buffer will not be reconsidered for delivery any more. In each
time step, at most one packet in the buffer can be sent. The order in which the
packets are sent should comply with the order of their arrival time. The
objective is to maximize the total value of the packets sent in an online
manner. In this paper, we study a variant of this FIFO buffering model in which
a packet's value is either 1 or $\alpha > 1$. We present a deterministic
memoryless 1.304-competitive algorithm. This algorithm has the same competitive
ratio as the one presented in (Lotker and Patt-Shamir. PODC 2002, Computer
Networks 2003). However, our algorithm is simpler and does not employ any
marking bits. The idea used in our algorithm is novel and different from all
previous approaches applied for the general model and its variants. We do not
proactively preempt one packet when a new packet arrives. Instead, we may
preempt more than one 1-value packet when the buffer contains sufficiently many
$\alpha$-value packets."
"We present a near-linear time algorithm that approximates the edit distance
between two strings within a polylogarithmic factor; specifically, for strings
of length n and every fixed epsilon>0, it can compute a (log n)^O(1/epsilon)
approximation in n^(1+epsilon) time. This is an exponential improvement over
the previously known factor, 2^(O (sqrt(log n))), with a comparable running
time (Ostrovsky and Rabani J.ACM 2007; Andoni and Onak STOC 2009). Previously,
no efficient polylogarithmic approximation algorithm was known for any
computational task involving edit distance (e.g., nearest neighbor search or
sketching).
  This result arises naturally in the study of a new asymmetric query model. In
this model, the input consists of two strings x and y, and an algorithm can
access y in an unrestricted manner, while being charged for querying every
symbol of x. Indeed, we obtain our main result by designing an algorithm that
makes a small number of queries in this model. We then provide a
nearly-matching lower bound on the number of queries.
  Our lower bound is the first to expose hardness of edit distance stemming
from the input strings being ""repetitive"", which means that many of their
substrings are approximately identical. Consequently, our lower bound provides
the first rigorous separation between edit distance and Ulam distance, which is
edit distance on non-repetitive strings, such as permutations."
"Motivated by providing quality-of-service differentiated services in the
Internet, we consider buffer management algorithms for network switches. We
study a multi-buffer model. A network switch consists of multiple size-bounded
buffers such that at any time, the number of packets residing in each
individual buffer cannot exceed its capacity. Packets arrive at the network
switch over time; they have values, deadlines, and designated buffers. In each
time step, at most one pending packet is allowed to be sent and this packet can
be from any buffer. The objective is to maximize the total value of the packets
sent by their respective deadlines. A 9.82-competitive online algorithm has
been provided for this model (Azar and Levy. SWAT 2006), but no offline
algorithms have been known yet. In this paper, We study the offline setting of
the multi-buffer model. Our contributions include a few optimal offline
algorithms for some variants of the model. Each variant has its unique and
interesting algorithmic feature. These offline algorithms help us understand
the model better in designing online algorithms."
"The rank and select operations over a string of length n from an alphabet of
size $\sigma$ have been used widely in the design of succinct data structures.
In many applications, the string itself need be maintained dynamically,
allowing characters of the string to be inserted and deleted. Under the word
RAM model with word size $w=\Omega(\lg n)$, we design a succinct representation
of dynamic strings using $nH_0 + o(n)\lg\sigma + O(w)$ bits to support rank,
select, insert and delete in $O(\frac{\lg n}{\lg\lg n}(\frac{\lg \sigma}{\lg\lg
n}+1))$ time. When the alphabet size is small, i.e. when $\sigma = O(\polylog
(n))$, including the case in which the string is a bit vector, these operations
are supported in $O(\frac{\lg n}{\lg\lg n})$ time. Our data structures are more
efficient than previous results on the same problem, and we have applied them
to improve results on the design and construction of space-efficient text
indexes."
"The problems of random projections and sparse reconstruction have much in
common and individually received much attention. Surprisingly, until now they
progressed in parallel and remained mostly separate. Here, we employ new tools
from probability in Banach spaces that were successfully used in the context of
sparse reconstruction to advance on an open problem in random pojection. In
particular, we generalize and use an intricate result by Rudelson and Vershynin
for sparse reconstruction which uses Dudley's theorem for bounding Gaussian
processes. Our main result states that any set of $N = \exp(\tilde{O}(n))$ real
vectors in $n$ dimensional space can be linearly mapped to a space of dimension
$k=O(\log N\polylog(n))$, while (1) preserving the pairwise distances among the
vectors to within any constant distortion and (2) being able to apply the
transformation in time $O(n\log n)$ on each vector. This improves on the best
known $N = \exp(\tilde{O}(n^{1/2}))$ achieved by Ailon and Liberty and $N =
\exp(\tilde{O}(n^{1/3}))$ by Ailon and Chazelle.
  The dependence in the distortion constant however is believed to be
suboptimal and subject to further investigation. For constant distortion, this
settles the open question posed by these authors up to a $\polylog(n)$ factor
while considerably simplifying their constructions."
"The Generalized Traveling Salesman Problem (GTSP) is a well-known
combinatorial optimization problem with a host of applications. It is an
extension of the Traveling Salesman Problem (TSP) where the set of cities is
partitioned into so-called clusters, and the salesman has to visit every
cluster exactly once.
  While the GTSP is a very important combinatorial optimization problem and is
well studied in many aspects, the local search algorithms used in the
literature are mostly basic adaptations of simple TSP heuristics. Hence, a
thorough and deep research of the neighborhoods and local search algorithms
specific to the GTSP is required.
  We formalize the procedure of adaptation of a TSP neighborhood for the GTSP
and classify all other existing and some new GTSP neighborhoods. For every
neighborhood, we provide efficient exploration algorithms that are often
significantly faster than the ones known from the literature. Finally, we
compare different local search implementations empirically."
"Given an n x n matrix A, we present a simple, element-wise sparsification
algorithm that zeroes out all sufficiently small elements of A and then retains
some of the remaining elements with probabilities proportional to the square of
their magnitudes. We analyze the approximation accuracy of the proposed
algorithm using a recent, elegant non-commutative Bernstein inequality, and
compare our bounds with all existing (to the best of our knowledge)
element-wise matrix sparsification algorithms."
"Analysing Web graphs has applications in determining page ranks, fighting Web
spam, detecting communities and mirror sites, and more. This study is however
hampered by the necessity of storing a major part of huge graphs in the
external memory, which prevents efficient random access to edge (hyperlink)
lists. A number of algorithms involving compression techniques have thus been
presented, to represent Web graphs succinctly but also providing random access.
Those techniques are usually based on differential encodings of the adjacency
lists, finding repeating nodes or node regions in the successive lists, more
general grammar-based transformations or 2-dimensional representations of the
binary matrix of the graph. In this paper we present two Web graph compression
algorithms. The first can be seen as engineering of the Boldi and Vigna (2004)
method. We extend the notion of similarity between link lists, and use a more
compact encoding of residuals. The algorithm works on blocks of varying size
(in the number of input lines) and sacrifices access time for better
compression ratio, achieving more succinct graph representation than other
algorithms reported in the literature. The second algorithm works on blocks of
the same size, in the number of input lines, and its key mechanism is merging
the block into a single ordered list. This method achieves much more attractive
space-time tradeoffs."
"A technique using a systolic array structure is proposed for solving the
common approximate substring (CAS) problem. This approach extends the technique
introduced in earlier work from the computation of the edit-distance between
two strings to the more encompassing CAS problem. A comparison to existing work
is given, and the technique presented is validated and analyzed based on
simulations."
"In this paper we show that set-intersection is harder than distance oracle on
sparse graphs. Given a collection of total size n which consists of m sets
drawn from universe U, the set-intersection problem is to build a data
structure which can answer whether two sets have any intersection. A distance
oracle is a data structure which can answer distance queries on a given graph.
We show that if one can build distance oracle for sparse graph G=(V,E), which
requires s(|V|,|E|) space and answers a (2-\epsilon,c)-approximate distance
query in time t(|V|,|E|) where (2-\epsilon) is a multiplicative error and c is
a constant additive error, then, set-intersection can be solved in t(m+|U|,n)
time using s(m+|U|,n) space."
"Cuckoo hashing is an efficient technique for creating large hash tables with
high space utilization and guaranteed constant access times. There, each item
can be placed in a location given by any one out of k different hash functions.
In this paper we investigate further the random walk heuristic for inserting in
an online fashion new items into the hash table. Provided that k > 2 and that
the number of items in the table is below (but arbitrarily close) to the
theoretically achievable load threshold, we show a polylogarithmic bound for
the maximum insertion time that holds with high probability."
"We give a complete structural characterisation of the map the positive branch
of a one-way pattern implements. We start with the representation of the
positive branch in terms of the phase map decomposition, which is then further
analysed to obtain the primary structure of the matrix M, representing the
phase map decomposition in the computational basis. Using this approach we
obtain some preliminary results on the connection between the columns structure
of a given unitary and the angles of measurements in a pattern that implements
it. We believe this work is a step forward towards a full characterisation of
those unitaries with an efficient one-way model implementation."
"We consider the problem of minimizing a function represented as a sum of
submodular terms. We assume each term allows an efficient computation of {\em
exchange capacities}. This holds, for example, for terms depending on a small
number of variables, or for certain cardinality-dependent terms.
  A naive application of submodular minimization algorithms would not exploit
the existence of specialized exchange capacity subroutines for individual
terms. To overcome this, we cast the problem as a {\em submodular flow} (SF)
problem in an auxiliary graph, and show that applying most existing SF
algorithms would rely only on these subroutines.
  We then explore in more detail Iwata's capacity scaling approach for
submodular flows (Math. Programming, 76(2):299--308, 1997). In particular, we
show how to improve its complexity in the case when the function contains
cardinality-dependent terms."
"A graph is a data structure composed of dots (i.e. vertices) and lines (i.e.
edges). The dots and lines of a graph can be organized into intricate
arrangements. The ability for a graph to denote objects and their relationships
to one another allow for a surprisingly large number of things to be modeled as
a graph. From the dependencies that link software packages to the wood beams
that provide the framing to a house, most anything has a corresponding graph
representation. However, just because it is possible to represent something as
a graph does not necessarily mean that its graph representation will be useful.
If a modeler can leverage the plethora of tools and algorithms that store and
process graphs, then such a mapping is worthwhile. This article explores the
world of graphs in computing and exposes situations in which graphical models
are beneficial."
"Raghavendra (STOC 2008) gave an elegant and surprising result: if Khot's
Unique Games Conjecture (STOC 2002) is true, then for every constraint
satisfaction problem (CSP), the best approximation ratio is attained by a
certain simple semidefinite programming and a rounding scheme for it. In this
paper, we show that similar results hold for constant-time approximation
algorithms in the bounded-degree model. Specifically, we present the
followings: (i) For every CSP, we construct an oracle that serves an access, in
constant time, to a nearly optimal solution to a basic LP relaxation of the
CSP. (ii) Using the oracle, we give a constant-time rounding scheme that
achieves an approximation ratio coincident with the integrality gap of the
basic LP. (iii) Finally, we give a generic conversion from integrality gaps of
basic LPs to hardness results. All of those results are \textit{unconditional}.
Therefore, for every bounded-degree CSP, we give the best constant-time
approximation algorithm among all. A CSP instance is called $\epsilon$-far from
satisfiability if we must remove at least an $\epsilon$-fraction of constraints
to make it satisfiable. A CSP is called testable if there is a constant-time
algorithm that distinguishes satisfiable instances from $\epsilon$-far
instances with probability at least $2/3$. Using the results above, we also
derive, under a technical assumption, an equivalent condition under which a CSP
is testable in the bounded-degree model."
"Deciding whether a graph can be embedded in a grid using only unit-length
edges is NP-complete, even when restricted to binary trees. However, it is not
difficult to devise a number of graph classes for which the problem is
polynomial, even trivial. A natural step, outstanding thus far, was to provide
a broad classification of graphs that make for polynomial or NP-complete
instances. We provide such a classification based on the set of allowed vertex
degrees in the input graphs, yielding a full dichotomy on the complexity of the
problem. As byproducts, the previous NP-completeness result for binary trees
was strengthened to strictly binary trees, and the three-dimensional version of
the problem was for the first time proven to be NP-complete. Our results were
made possible by introducing the concepts of consistent orientations and robust
gadgets, and by showing how the former allows NP-completeness proofs by local
replacement even in the absence of the latter."
"In this paper we study the question of whether or not a static search tree
should ever be unbalanced. We present several methods to restructure an
unbalanced k-ary search tree $T$ into a new tree $R$ that preserves many of the
properties of $T$ while having a height of $\log_k n +1$ which is one unit off
of the optimal height. More specifically, we show that it is possible to ensure
that the depth of the elements in $R$ is no more than their depth in $T$ plus
at most $\log_k \log_k n +2$. At the same time it is possible to guarantee that
the average access time $P(R)$ in tree $R$ is no more than the average access
time $P(T)$ in tree $T$ plus $O(\log_k P(T))$. This suggests that for most
applications, a balanced tree is always a better option than an unbalanced one
since the balanced tree has similar average access time and much better worst
case access time."
"In this paper we present several new and very practical methods and
techniques for range aggregation and selection problems in multidimensional
data structures and other types of sets of values. We also present some new
extensions and applications for some fundamental set maintenance problems."
"We give the first constant-factor approximation algorithm for Sparsest Cut
with general demands in bounded treewidth graphs. In contrast to previous
algorithms, which rely on the flow-cut gap and/or metric embeddings, our
approach exploits the Sherali-Adams hierarchy of linear programming
relaxations."
"An involution on a finite set is a bijection such as I(I(e))=e for all the
element of the set. A fixed-point free involution on a finite set is an
involution such as I(e)=e for none element of the set. In this article, the
fixed-point free involutions are represented as partitions of the set and some
properties linked to this representation are exhibited. Then an optimal
algorithm to list all the fixed-point free involutions is presented. Its
soundness relies on the representation of the fixed-point free involutions as
partitions. Finally, an implementation of the algorithm is proposed, with an
effective data representation."
"In this paper we describe a dynamic external memory data structure that
supports range reporting queries in three dimensions in $O(\log_B^2 N +
\frac{k}{B})$ I/O operations, where $k$ is the number of points in the answer
and $B$ is the block size. This is the first dynamic data structure that
answers three-dimensional range reporting queries in $\log_B^{O(1)} N +
O(\frac{k}{B})$ I/Os."
"We study the extremal competitive ratio of Boolean function evaluation. We
provide the first non-trivial lower and upper bounds for classes of Boolean
functions which are not included in the class of monotone Boolean functions.
For the particular case of symmetric functions our bounds are matching and we
exactly characterize the best possible competitiveness achievable by a
deterministic algorithm. Our upper bound is obtained by a simple polynomial
time algorithm."
"We obtain polynomial-time approximation-preserving reductions (up to a factor
of 1 + \epsilon) from the prize-collecting Steiner tree and prize-collecting
Steiner forest problems in planar graphs to the corresponding problems in
graphs of bounded treewidth. We also give an exact algorithm for the
prize-collecting Steiner tree problem that runs in polynomial time for graphs
of bounded treewidth. This, combined with our reductions, yields a PTAS for the
prize-collecting Steiner tree problem in planar graphs and generalizes the PTAS
of Borradaile, Klein and Mathieu for the Steiner tree problem in planar graphs.
Our results build upon the ideas of Borradaile, Klein and Mathieu and the work
of Bateni, Hajiaghayi and Marx on a PTAS for the Steiner forest problem in
planar graphs. Our main technical result is on the properties of primal-dual
algorithms for Steiner tree and forest problems in general graphs when they are
run with scaled up penalties."
"The notion of vertex sparsification is introduced in \cite{M}, where it was
shown that for any graph $G = (V, E)$ and a subset of $k$ terminals $K \subset
V$, there is a polynomial time algorithm to construct a graph $H = (K, E_H)$ on
just the terminal set so that simultaneously for all cuts $(A, K-A)$, the value
of the minimum cut in $G$ separating $A$ from $K -A$ is approximately the same
as the value of the corresponding cut in $H$.
  We give the first super-constant lower bounds for how well a cut-sparsifier
$H$ can simultaneously approximate all minimum cuts in $G$. We prove a lower
bound of $\Omega(\log^{1/4} k)$ -- this is polynomially-related to the known
upper bound of $O(\log k/\log \log k)$. This is an exponential improvement on
the $\Omega(\log \log k)$ bound given in \cite{LM} which in fact was for a
stronger vertex sparsification guarantee, and did not apply to cut sparsifiers.
  Despite this negative result, we show that for many natural problems, we do
not need to incur a multiplicative penalty for our reduction. We obtain optimal
$O(\log k)$-competitive Steiner oblivious routing schemes, which generalize the
results in \cite{R}. We also demonstrate that for a wide range of graph packing
problems (which includes maximum concurrent flow, maximum multiflow and
multicast routing, among others, as a special case), the integrality gap of the
linear program is always at most $O(\log k)$ times the integrality gap
restricted to trees. This result helps to explain the ubiquity of the $O(\log
k)$ guarantees for such problems.
  Lastly, we use our ideas to give an efficient construction for
vertex-sparsifiers that match the current best existential results -- this was
previously open. Our algorithm makes novel use of Earth-mover constraints."
"Given a capacitated graph $G = (V,E)$ and a set of terminals $K \subseteq V$,
how should we produce a graph $H$ only on the terminals $K$ so that every
(multicommodity) flow between the terminals in $G$ could be supported in $H$
with low congestion, and vice versa? (Such a graph $H$ is called a
flow-sparsifier for $G$.) What if we want $H$ to be a ""simple"" graph? What if
we allow $H$ to be a convex combination of simple graphs?
  Improving on results of Moitra [FOCS 2009] and Leighton and Moitra [STOC
2010], we give efficient algorithms for constructing: (a) a flow-sparsifier $H$
that maintains congestion up to a factor of $O(\log k/\log \log k)$, where $k =
|K|$, (b) a convex combination of trees over the terminals $K$ that maintains
congestion up to a factor of $O(\log k)$, and (c) for a planar graph $G$, a
convex combination of planar graphs that maintains congestion up to a constant
factor. This requires us to give a new algorithm for the 0-extension problem,
the first one in which the preimages of each terminal are connected in $G$.
Moreover, this result extends to minor-closed families of graphs.
  Our improved bounds immediately imply improved approximation guarantees for
several terminal-based cut and ordering problems."
"We study vertex cut and flow sparsifiers that were recently introduced by
Moitra, and Leighton and Moitra. We improve and generalize their results. We
give a new polynomial-time algorithm for constructing O(log k / log log k) cut
and flow sparsifiers, matching the best existential upper bound on the quality
of a sparsifier, and improving the previous algorithmic upper bound of O(log^2
k / log log k). We show that flow sparsifiers can be obtained from linear
operators approximating minimum metric extensions. We introduce the notion of
(linear) metric extension operators, prove that they exist, and give an exact
polynomial-time algorithm for finding optimal operators.
  We then establish a direct connection between flow and cut sparsifiers and
Lipschitz extendability of maps in Banach spaces, a notion studied in
functional analysis since 1930s. Using this connection, we prove a lower bound
of Omega(sqrt{log k/log log k}) for flow sparsifiers and a lower bound of
Omega(sqrt{log k}/log log k) for cut sparsifiers. We show that if a certain
open question posed by Ball in 1992 has a positive answer, then there exist
\tilde O(sqrt{log k}) cut sparsifiers. On the other hand, any lower bound on
cut sparsifiers better than \tilde Omega(sqrt{log k}) would imply a negative
answer to this question."
"Sequence assembly from short reads is an important problem in biology. It is
known that solving the sequence assembly problem exactly on a bi-directed de
Bruijn graph or a string graph is intractable. However finding a Shortest
Double stranded DNA string (SDDNA) containing all the k-long words in the reads
seems to be a good heuristic to get close to the original genome. This problem
is equivalent to finding a cyclic Chinese Postman (CP) walk on the underlying
un-weighted bi-directed de Bruijn graph built from the reads. The Chinese
Postman walk Problem (CPP) is solved by reducing it to a general bi-directed
flow on this graph which runs in O(|E|2 log2(|V |)) time. In this paper we show
that the cyclic CPP on bi-directed graphs can be solved without reducing it to
bi-directed flow. We present a ?(p(|V | + |E|) log(|V |) + (dmaxp)3) time
algorithm to solve the cyclic CPP on a weighted bi-directed de Bruijn graph,
where p = max{|{v|din(v) - dout(v) > 0}|, |{v|din(v) - dout(v) < 0}|} and dmax
= max{|din(v) - dout(v)}. Our algorithm performs asymptotically better than the
bidirected flow algorithm when the number of imbalanced nodes p is much less
than the nodes in the bi-directed graph. From our experimental results on
various datasets, we have noticed that the value of p/|V | lies between 0.08%
and 0.13% with 95% probability."
"We study the use of sampling for efficiently mining the top-K frequent
itemsets of cardinality at most w. To this purpose, we define an approximation
to the top-K frequent itemsets to be a family of itemsets which includes
(resp., excludes) all very frequent (resp., very infrequent) itemsets, together
with an estimate of these itemsets' frequencies with a bounded error. Our first
result is an upper bound on the sample size which guarantees that the top-K
frequent itemsets mined from a random sample of that size approximate the
actual top-K frequent itemsets, with probability larger than a specified value.
We show that the upper bound is asymptotically tight when w is constant. Our
main algorithmic contribution is a progressive sampling approach, combined with
suitable stopping conditions, which on appropriate inputs is able to extract
approximate top-K frequent itemsets from samples whose sizes are smaller than
the general upper bound. In order to test the stopping conditions, this
approach maintains the frequency of all itemsets encountered, which is
practical only for small w. However, we show how this problem can be mitigated
by using a variation of Bloom filters. A number of experiments conducted on
both synthetic and real bench- mark datasets show that using samples
substantially smaller than the original dataset (i.e., of size defined by the
upper bound or reached through the progressive sampling approach) enable to
approximate the actual top-K frequent itemsets with accuracy much higher than
what analytically proved."
"A geodesic is the shortest path between two vertices in a connected network.
The geodesic is the kernel of various network metrics including radius,
diameter, eccentricity, closeness, and betweenness. These metrics are the
foundation of much network research and thus, have been studied extensively in
the domain of single-relational networks (both in their directed and undirected
forms). However, geodesics for single-relational networks do not translate
directly to multi-relational, or semantic networks, where vertices are
connected to one another by any number of edge labels. Here, a more
sophisticated method for calculating a geodesic is necessary. This article
presents a technique for calculating geodesics in semantic networks with a
focus on semantic networks represented according to the Resource Description
Framework (RDF). In this framework, a discrete ""walker"" utilizes an abstract
path description called a grammar to determine which paths to include in its
geodesic calculation. The grammar-based model forms a general framework for
studying geodesic metrics in semantic networks."
"We present techniques for maintaining subgraph frequencies in a dynamic
graph, using data structures that are parameterized in terms of h, the h-index
of the graph. Our methods extend previous results of Eppstein and Spiro for
maintaining statistics for undirected subgraphs of size three to directed
subgraphs and to subgraphs of size four. For the directed case, we provide a
data structure to maintain counts for all 3-vertex induced subgraphs in O(h)
amortized time per update. For the undirected case, we maintain the counts of
size-four subgraphs in O(h^2) amortized time per update. These extensions
enable a number of new applications in Bioinformatics and Social Networking
research."
"The pathwidth of a graph is a measure of how path-like the graph is. Given a
graph G and an integer k, the problem of finding whether there exist at most k
vertices in G whose deletion results in a graph of pathwidth at most one is NP-
complete. We initiate the study of the parameterized complexity of this
problem, parameterized by k. We show that the problem has a quartic
vertex-kernel: We show that, given an input instance (G = (V, E), k); |V| = n,
we can construct, in polynomial time, an instance (G', k') such that (i) (G, k)
is a YES instance if and only if (G', k') is a YES instance, (ii) G' has
O(k^{4}) vertices, and (iii) k' \leq k. We also give a fixed parameter
tractable (FPT) algorithm for the problem that runs in O(7^{k} k \cdot n^{2})
time."
"Pedigree graphs, or family trees, are typically constructed by an expensive
process of examining genealogical records to determine which pairs of
individuals are parent and child. New methods to automate this process take as
input genetic data from a set of extant individuals and reconstruct ancestral
individuals. There is a great need to evaluate the quality of these methods by
comparing the estimated pedigree to the true pedigree.
  In this paper, we consider two main pedigree comparison problems. The first
is the pedigree isomorphism problem, for which we present a linear-time
algorithm for leaf-labeled pedigrees. The second is the pedigree edit distance
problem, for which we present 1) several algorithms that are fast and exact in
various special cases, and 2) a general, randomized heuristic algorithm.
  In the negative direction, we first prove that the pedigree isomorphism
problem is as hard as the general graph isomorphism problem, and that the
sub-pedigree isomorphism problem is NP-hard. We then show that the pedigree
edit distance problem is APX-hard in general and NP-hard on leaf-labeled
pedigrees.
  We use simulated pedigrees to compare our edit-distance algorithms to each
other as well as to a branch-and-bound algorithm that always finds an optimal
solution."
"An independent dominating set D of a graph G = (V,E) is a subset of vertices
such that every vertex in V \ D has at least one neighbor in D and D is an
independent set, i.e. no two vertices of D are adjacent in G. Finding a minimum
independent dominating set in a graph is an NP-hard problem. Whereas it is hard
to cope with this problem using parameterized and approximation algorithms,
there is a simple exact O(1.4423^n)-time algorithm solving the problem by
enumerating all maximal independent sets. In this paper we improve the latter
result, providing the first non trivial algorithm computing a minimum
independent dominating set of a graph in time O(1.3569^n). Furthermore, we give
a lower bound of \Omega(1.3247^n) on the worst-case running time of this
algorithm, showing that the running time analysis is almost tight."
"Formulate the problem as follows. Split a file into n pieces so that it can
be restored without any m parts (1<=m<=n). Such problems are called problems
secret sharing. There exists a set of methods for solving such problems, but
they all require a fairly large number of calculations applied to the problem
posed above. The proposed method does not require calculations, and requires
only the operations of the division of the file into equal (nearly equal) parts
and gluing them in a certain order in one or more files."
"A major factor affecting the readability of a graph drawing is its
resolution. In the graph drawing literature, the resolution of a drawing is
either measured based on the angles formed by consecutive edges incident to a
common node (angular resolution) or by the angles formed at edge crossings
(crossing resolution). In this paper, we evaluate both by introducing the
notion of ""total resolution"", that is, the minimum of the angular and crossing
resolution. To the best of our knowledge, this is the first time where the
problem of maximizing the total resolution of a drawing is studied.
  The main contribution of the paper consists of drawings of asymptotically
optimal total resolution for complete graphs (circular drawings) and for
complete bipartite graphs (2-layered drawings). In addition, we present and
experimentally evaluate a force-directed based algorithm that constructs
drawings of large total resolution."
"Wireless Communication Networks based on Frequency Division Multiplexing (FDM
in short) plays an important role in the field of communications, in which each
request can be satisfied by assigning a frequency. To avoid interference, each
assigned frequency must be different to the neighboring assigned frequencies.
Since frequency is a scarce resource, the main problem in wireless networks is
how to fully utilize the given bandwidth of frequencies. In this paper, we
consider the online call control problem. Given a fixed bandwidth of
frequencies and a sequence of communication requests arrive over time, each
request must be either satisfied immediately after its arrival by assigning an
available frequency, or rejected. The objective of call control problem is to
maximize the number of accepted requests. We study the asymptotic performance
of this problem, i.e., the number of requests in the sequence and the bandwidth
of frequencies are very large. In this paper, we give a 7/3-competitive
algorithm for call control problem in cellular network, improving the previous
2.5-competitive result. Moreover, we investigate the triangle-free cellular
network, propose a 9/4-competitive algorithm and prove that the lower bound of
competitive ratio is at least 5/3."
"We introduce a problem that is a common generalization of the uncapacitated
facility location and minimum latency (ML) problems, where facilities need to
be opened to serve clients and also need to be sequentially activated before
they can provide service. Formally, we are given a set \F of n facilities with
facility-opening costs {f_i}, a set of m clients, and connection costs {c_{ij}}
specifying the cost of assigning a client j to a facility i, a root node r
denoting the depot, and a time metric d on \F\cup{r}. Our goal is to open a
subset F of facilities, find a path P starting at r and spanning F to activate
the open facilities, and connect each client j to a facility \phi(j)\in F, so
as to minimize \sum_{i\in F}f_i +\sum_{clients j}(c_{\phi(j),j}+t_j), where t_j
is the time taken to reach \phi(j) along path P. We call this the minimum
latency uncapacitated facility location (MLUFL) problem.
  Our main result is an O(\log n\max{\log n,\log m})-approximation for MLUFL.
We also show that any improvement in this approximation guarantee, implies an
improvement in the (current-best) approximation factor for group Steiner tree.
We obtain constant approximations for two natural special cases of the problem:
(a) related MLUFL (metric connection costs that are a scalar multiple of the
time metric); (b) metric uniform MLUFL (metric connection costs, unform
time-metric). Our LP-based methods are versatile and easily adapted to yield
approximation guarantees for MLUFL in various more general settings, such as
(i) when the latency-cost of a client is a function of the delay faced by the
facility to which it is connected; and (ii) the k-route version, where k
vehicles are routed in parallel to activate the open facilities. Our LP-based
understanding of MLUFL also offers some LP-based insights into ML, which we
believe is a promising direction for obtaining improvements for ML."
"We consider an extension of the {\em popular matching} problem in this paper.
The input to the popular matching problem is a bipartite graph G = (A U B,E),
where A is a set of people, B is a set of items, and each person a belonging to
A ranks a subset of items in an order of preference, with ties allowed. The
popular matching problem seeks to compute a matching M* between people and
items such that there is no matching M where more people are happier with M
than with M*. Such a matching M* is called a popular matching. However, there
are simple instances where no popular matching exists.
  Here we consider the following natural extension to the above problem:
associated with each item b belonging to B is a non-negative price cost(b),
that is, for any item b, new copies of b can be added to the input graph by
paying an amount of cost(b) per copy. When G does not admit a popular matching,
the problem is to ""augment"" G at minimum cost such that the new graph admits a
popular matching. We show that this problem is NP-hard; in fact, it is NP-hard
to approximate it within a factor of sqrt{n1}/2, where n1 is the number of
people. This problem has a simple polynomial time algorithm when each person
has a preference list of length at most 2. However, if we consider the problem
of ""constructing"" a graph at minimum cost that admits a popular matching that
matches all people, then even with preference lists of length 2, the problem
becomes NP-hard. On the other hand, when the number of copies of each item is
""fixed"", we show that the problem of computing a minimum cost popular matching
or deciding that no popular matching exists can be solved in O(mn1) time, where
m is the number of edges."
"In a recent paper, we introduced the simultaneous representation problem
(defined for any graph class C) and studied the problem for chordal,
comparability and permutation graphs. For interval graphs, the problem is
defined as follows. Two interval graphs G_1 and G_2, sharing some vertices I
(and the corresponding induced edges), are said to be `simultaneous interval
graphs' if there exist interval representations R_1 and R_2 of G_1 and G_2,
such that any vertex of I is mapped to the same interval in both R_1 and R_2.
Equivalently, G_1 and G_2 are simultaneous interval graphs if there exist edges
E' between G_1-I and G_2-I such that G_1 \cup G_2 \cup E' is an interval graph.
  Simultaneous representation problems are related to simultaneous planar
embeddings, and have applications in any situation where it is desirable to
consistently represent two related graphs, for example: interval graphs
capturing overlaps of DNA fragments of two similar organisms; or graphs
connected in time, where one is an updated version of the other.
  In this paper we give an O(n^2*logn) time algorithm for recognizing
simultaneous interval graphs,where n = |G_1 \cup G_2|. This result complements
the polynomial time algorithms for recognizing probe interval graphs and
provides an efficient algorithm for the interval graph sandwich problem for the
special case where the set of optional edges induce a complete bipartite graph."
"Clustering under most popular objective functions is NP-hard, even to
approximate well, and so unlikely to be efficiently solvable in the worst case.
Recently, Bilu and Linial \cite{Bilu09} suggested an approach aimed at
bypassing this computational barrier by using properties of instances one might
hope to hold in practice. In particular, they argue that instances in practice
should be stable to small perturbations in the metric space and give an
efficient algorithm for clustering instances of the Max-Cut problem that are
stable to perturbations of size $O(n^{1/2})$. In addition, they conjecture that
instances stable to as little as O(1) perturbations should be solvable in
polynomial time. In this paper we prove that this conjecture is true for any
center-based clustering objective (such as $k$-median, $k$-means, and
$k$-center). Specifically, we show we can efficiently find the optimal
clustering assuming only stability to factor-3 perturbations of the underlying
metric in spaces without Steiner points, and stability to factor $2+\sqrt{3}$
perturbations for general metrics. In particular, we show for such instances
that the popular Single-Linkage algorithm combined with dynamic programming
will find the optimal clustering. We also present NP-hardness results under a
weaker but related condition."
"In this research endeavor, some Sequence Alignment Algorithms are detailed
that are useful for finding or comparing 1 dimensional (1-D), 2 dimensional
(2-D), 3 dimensional (3-D) sequences in or against a parent or mother database
which is 1 dimensional (1-D), 2 dimensional (2-D), 3 dimensional (3-D)
sequence. Inner Product [1], [2] based schemes are used to lay down such
algorithms. Also,in this research, a Sequence Alignment Algorithms is detailed
that is useful for finding or comparing an N-Dimensional (N-D) sequence in or
against a parent or mother database which N-Dimensional (N-D) sequence. Inner
Product [1], [2] based schemes are used to lay down such an algorithm."
"Algorithms to generate various combinatorial structures find tremendous
importance in computer science. In this paper, we begin by reviewing an
algorithm proposed by Rohl that generates all unique permutations of a list of
elements which possibly contains repetitions, taking some or all of the
elements at a time, in any imposed order. The algorithm uses an auxiliary array
that maintains the number of occurrences of each unique element in the input
list. We provide a proof of correctness of the algorithm. We then show how one
can efficiently generate other combinatorial structures like combinations,
subsets, n-Parenthesizations, derangements and integer partitions &
compositions with minor changes to the same algorithm."
"We consider the offline sorting buffer problem. The input is a sequence of
items of different types. All items must be processed one by one by a server.
The server is equipped with a random-access buffer of limited capacity which
can be used to rearrange items. The problem is to design a scheduling strategy
that decides upon the order in which items from the buffer are sent to the
server. Each type change incurs unit cost, and thus, the cost minimizing
objective is to minimize the total number of type changes for serving the
entire sequence. This problem is motivated by various applications in
manufacturing processes and computer science, and it has attracted significant
attention in the last few years. The main focus has been on online competitive
algorithms. Surprisingly little is known on the basic offline problem. In this
paper, we show that the sorting buffer problem with uniform cost is NP-hard
and, thus, close one of the most fundamental questions for the offline problem.
On the positive side, we give an O(1)-approximation algorithm when the
scheduler is given a buffer only slightly larger than double the original size.
We also give a dynamic programming algorithm for the special case of buffer
size two that solves the problem exactly in linear time, improving on the
standard DP which runs in cubic time."
"Two planar graphs G1 and G2 sharing some vertices and edges are
`simultaneously planar' if they have planar drawings such that a shared vertex
[edge] is represented by the same point [curve] in both drawings. It is an open
problem whether simultaneous planarity can be tested efficiently. We give a
linear-time algorithm to test simultaneous planarity when the two graphs share
a 2-connected subgraph. Our algorithm extends to the case of k planar graphs
where each vertex [edge] is either common to all graphs or belongs to exactly
one of them."
"Scheduling with assignment restrictions is an important special case of
scheduling unrelated machines which has attracted much attention in the recent
past. While a lower bound on approximability of 3/2 is known for its most
general setting, subclasses of the problem admit polynomial-time approximation
schemes. This note provides a PTAS for tree-like hierarchical structures,
improving on a recent 4/3-approximation by Huo and Leung."
"A critical variable of a satisfiable CNF formula is a variable that has the
same value in all satisfying assignments. Using a simple case distinction on
the fraction of critical variables of a CNF formula, we improve the running
time for 3-SAT from O(1.32216^n) by Rolf [2006] to O(1.32153^n). Using a
different approach, Iwama et al. [2010] very recently achieved a running time
of O(1.32113^n). Our method nicely combines with theirs, yielding the currently
fastest known algorithm with running time O(1.32065^n). We also improve the
bound for 4-SAT from O(1.47390^n) [Iwama, Tamaki 2004] to O(1.46928^n), where
O(1.46981^n) can be obtained using the methods of [Iwama, Tamaki 2004] and
[Rolf 2006]."
"We propose and develop an efficient implementation of the robust tabu search
heuristic for sparse quadratic assignment problems. The traditional
implementation of the heuristic applicable to all quadratic assignment problems
is of O(N^2) complexity per iteration for problems of size N. Using multiple
priority queues to determine the next best move instead of scanning all
possible moves, and using adjacency lists to minimize the operations needed to
determine the cost of moves, we reduce the asymptotic complexity per iteration
to O(N log N ). For practical sized problems, the complexity is O(N)."
"Kernelization algorithms, usually a preprocessing step before other more
traditional algorithms, are very special in the sense that they return
(reduced) instances, instead of final results. This characteristic excludes the
freedom of applying a kernelization algorithm for the weighted version of a
problem to its unweighted instances. Thus with only very few special cases,
kernelization algorithms have to be studied separately for weigthed and
unweighted versions of a single problem. {\sc feedback arc set on tournament}
is currently a very popular problem in recent research of parameterized, as
well as approximation computation, and its wide applications in many areas make
it appear in all top conferences. The theory of graph modular decompositions is
a general approach in the study of graph structures, which only had its
surfaces touched in previous work on kernelization algorithms of {\sc feedback
arc set on tournament}. In this paper, we study further properties of graph
modular decompositions and apply them to obtain the first linear kernel for the
unweighted {\sc feedback arc set on tournament} problem, which only admits
linear kernel in its weighted version, while quadratic kernel for the
unweighted."
"Given a point set S and an unknown metric d on S, we study the problem of
efficiently partitioning S into k clusters while querying few distances between
the points. In our model we assume that we have access to one versus all
queries that given a point s in S return the distances between s and all other
points. We show that given a natural assumption about the structure of the
instance, we can efficiently find an accurate clustering using only O(k)
distance queries. Our algorithm uses an active selection strategy to choose a
small set of points that we call landmarks, and considers only the distances
between landmarks and other points to produce a clustering. We use our
algorithm to cluster proteins by sequence similarity. This setting nicely fits
our model because we can use a fast sequence database search program to query a
sequence against an entire dataset. We conduct an empirical study that shows
that even though we query a small fraction of the distances between the points,
we produce clusterings that are close to a desired clustering given by manual
classification."
"Recent cognitive experiments have shown that the negative impact of an edge
crossing on the human understanding of a graph drawing, tends to be eliminated
in the case where the crossing angles are greater than 70 degrees. This
motivated the study of RAC drawings, in which every pair of crossing edges
intersects at right angle. In this work, we demonstrate a class of graphs with
unique RAC combinatorial embedding and we employ members of this class in order
to show that it is NP-hard to decide whether a graph admits a straight-line RAC
drawing."
"A priority queue is presented that supports the operations insert and
find-min in worst-case constant time, and delete and delete-min on element x in
worst-case O(lg(min{w_x, q_x}+2)) time, where w_x (respectively q_x) is the
number of elements inserted after x (respectively before x) and are still
present at the time of the deletion of x. Our priority queue then has both the
working-set and the queueish properties, and more strongly it satisfies these
properties in the worst-case sense. We also define a new distribution-sensitive
property---the time-finger property, which encapsulates and generalizes both
the working-set and queueish properties, and present a priority queue that
satisfies this property.
  In addition, we prove a strong implication that the working-set property is
equivalent to the unified bound (which is the minimum per operation among the
static finger, static optimality, and the working-set bounds). This latter
result is of tremendous interest by itself as it had gone unnoticed since the
introduction of such bounds by Sleater and Tarjan [JACM 1985]. Accordingly, our
priority queue satisfies other distribution-sensitive properties as the static
finger, static optimality, and the unified bound."
"Relative worst order analysis is a supplement or alternative to competitive
analysis which has been shown to give results more in accordance with observed
behavior of online algorithms for a range of different online problems. The
contribution of this paper is twofold. First, it adds the static list accessing
problem to the collection of online problems where relative worst order
analysis gives better results. Second, and maybe more interesting, it adds the
non-trivial supplementary proof technique of list factoring to the theoretical
toolbox for relative worst order analysis."
"We propose a method to exponentially speed up computation of various
fingerprints, such as the ones used to compute similarity and rarity in massive
data sets. Rather then maintaining the full stream of $b$ items of a universe
$[u]$, such methods only maintain a concise fingerprint of the stream, and
perform computations using the fingerprints. The computations are done
approximately, and the required fingerprint size $k$ depends on the desired
accuracy $\epsilon$ and confidence $\delta$. Our technique maintains a single
bit per hash function, rather than a single integer, thus requiring a
fingerprint of length $k = O(\frac{\ln \frac{1}{\delta}}{\epsilon^2})$ bits,
rather than $O(\log u \cdot \frac{\ln \frac{1}{\delta}}{\epsilon^2})$ bits
required by previous approaches. The main advantage of the fingerprints we
propose is that rather than computing the fingerprint of a stream of $b$ items
in time of $O(b \cdot k)$, we can compute it in time $O(b \log k)$. Thus this
allows an exponential speedup for the fingerprint construction, or
alternatively allows achieving a much higher accuracy while preserving
computation time. Our methods rely on a specific family of pseudo-random hashes
for which we can quickly locate hashes resulting in small values."
"LRM-Trees are an elegant way to partition a sequence of values into sorted
consecutive blocks, and to express the relative position of the first element
of each block within a previous block. They were used to encode ordinal trees
and to index integer arrays in order to support range minimum queries on them.
We describe how they yield many other convenient results in a variety of areas,
from data structures to algorithms: some compressed succinct indices for range
minimum queries; a new adaptive sorting algorithm; and a compressed succinct
data structure for permutations supporting direct and indirect application in
time all the shortest as the permutation is compressible."
"We study the L1 minimization problem with additional box constraints. We
motivate the problem with two different views of optimality considerations. We
look into imposing such constraints in projected gradient techniques and
propose a worst case linear time algorithm to perform such projections. We
demonstrate the merits and effectiveness of our algorithms on synthetic as well
as real experiments."
"For almost two decades the question of whether tabu search (TS) or simulated
annealing (SA) performs better for the quadratic assignment problem has been
unresolved. To answer this question satisfactorily, we compare performance at
various values of targeted solution quality, running each heuristic at its
optimal number of iterations for each target. We find that for a number of
varied problem instances, SA performs better for higher quality targets while
TS performs better for lower quality targets."
"In the oblivious buy-at-bulk network design problem in a graph, the task is
to compute a fixed set of paths for every pair of source-destinations in the
graph, such that any set of demands can be routed along these paths. The
demands could be aggregated at intermediate edges where the fusion-cost is
specified by a canonical (non-negative concave) function $f$. We give a novel
algorithm for planar graphs which is oblivious with respect to the demands, and
is also oblivious with respect to the fusion function $f$. The algorithm is
deterministic and computes the fixed set of paths in polynomial time, and
guarantees a $O(\log n)$ approximation ratio for any set of demands and any
canonical fusion function $f$, where $n$ is the number of nodes. The algorithm
is asymptotically optimal, since it is known that this problem cannot be
approximated with better than $\Omega(\log n)$ ratio. To our knowledge, this is
the first tight analysis for planar graphs, and improves the approximation
ratio by a factor of $\log n$ with respect to previously known results."
"This paper introduces a special family of randomized algorithms for Max DICUT
that we call oblivious algorithms. Let the bias of a vertex be the ratio
between the total weight of its outgoing edges and the total weight of all its
edges. An oblivious algorithm selects at random in which side of the cut to
place a vertex v, with probability that only depends on the bias of v,
independently of other vertices. The reader may observe that the algorithm that
ignores the bias and chooses each side with probability 1/2 has an
approximation ratio of 1/4, whereas no oblivious algorithm can have an
approximation ratio better than 1/2 (with an even directed cycle serving as a
negative example). We attempt to characterize the best approximation ratio
achievable by oblivious algorithms, and present results that are nearly tight.
The paper also discusses natural extensions of the notion of oblivious
algorithms, and extensions to the more general problem of Max 2-AND."
"Very recently a new algorithm to the nonnegative single-source shortest path
problem on road networks has been discovered. It is very cache-efficient, but
only on static road networks. We show how to augment it to the time-dependent
scenario. The advantage if the new approach is that it settles nodes, even for
a profile query, by scanning all downward edges. We improve the scanning of the
downward edges with techniques developed for time-dependent many-to-many
computations."
"We give O(log^2 n)-approximation algorithm based on the cut-matching
framework of [10, 13, 14] for computing the sparsest cut on directed graphs.
Our algorithm uses only O(log^2 n) single commodity max-flow computations and
thus breaks the multicommodity-flow barrier for computing the sparsest cut on
directed graphs"
"In this paper, we use a new method to decrease the parameterized complexity
bound for finding the minimum vertex cover of connected max-degree-3 undirected
graphs. The key operation of this method is reduction of the size of a
particular subset of edges which we introduce in this paper and is called as
""real-cycle"" subset. Using ""real-cycle"" reductions alone we compute a
complexity bound $O(1.15855^k)$ where $k$ is size of the optimal vertex cover.
Combined with other techniques, the complexity bound can be further improved to
be $O(1.1504^k)$. This is currently the best complexity bound."
"In two-stage robust optimization the solution to a problem is built in two
stages: In the first stage a partial, not necessarily feasible, solution is
exhibited. Then the adversary chooses the ""worst"" scenario from a predefined
set of scenarios. In the second stage, the first-stage solution is extended to
become feasible for the chosen scenario. The costs at the second stage are
larger than at the first one, and the objective is to minimize the total cost
paid in the two stages.
  We give a 2-approximation algorithm for the robust mincut problem and a
({\gamma}+2)-approximation for the robust shortest path problem, where {\gamma}
is the approximation ratio for the Steiner tree. This improves the factors
(1+\sqrt2) and 2({\gamma}+2) from [Golovin, Goyal and Ravi. Pay today for a
rainy day: Improved approximation algorithms for demand-robust min-cut and
shortest path problems. STACS 2006]. In addition, our solution for robust
shortest path is simpler and more efficient than the earlier ones; this is
achieved by a more direct algorithm and analysis, not using some of the
standard demand-robust optimization techniques."
"Given an undirected graph $G$, a collection $\{(s_1,t_1),..., (s_k,t_k)\}$ of
pairs of vertices, and an integer $p$, the Edge Multicut problem ask if there
is a set $S$ of at most $p$ edges such that the removal of $S$ disconnects
every $s_i$ from the corresponding $t_i$. Vertex Multicut is the analogous
problem where $S$ is a set of at most $p$ vertices. Our main result is that
both problems can be solved in time $2^{O(p^3)}... n^{O(1)}$, i.e.,
fixed-parameter tractable parameterized by the size $p$ of the cutset in the
solution. By contrast, it is unlikely that an algorithm with running time of
the form $f(p)... n^{O(1)}$ exists for the directed version of the problem, as
we show it to be W[1]-hard parameterized by the size of the cutset."
"We show that the set of realizations of a given dimension of a max-plus
linear sequence is a finite union of polyhedral sets, which can be computed
from any realization of the sequence. This yields an (expensive) algorithm to
solve the max-plus minimal realization problem. These results are derived from
general facts on rational expressions over idempotent commutative semirings: we
show more generally that the set of values of the coefficients of a commutative
rational expression in one letter that yield a given max-plus linear sequence
is a semi-algebraic set in the max-plus sense. In particular, it is a finite
union of polyhedral sets."
"In this paper, we consider the following graph partitioning problem: The
input is an undirected graph $G=(V,E),$ a balance parameter $b \in (0,1/2]$ and
a target conductance value $\gamma \in (0,1).$ The output is a cut which, if
non-empty, is of conductance at most $O(f),$ for some function $f(G, \gamma),$
and which is either balanced or well correlated with all cuts of conductance at
most $\gamma.$ Spielman and Teng gave an $\tilde{O}(|E|/\gamma^{2})$-time
algorithm for $f= \sqrt{\gamma \log^{3}|V|}$ and used it to decompose graphs
into a collection of near-expanders. We present a new spectral algorithm for
this problem which runs in time $\tilde{O}(|E|/\gamma)$ for $f=\sqrt{\gamma}.$
Our result yields the first nearly-linear time algorithm for the classic
Balanced Separator problem that achieves the asymptotically optimal
approximation guarantee for spectral methods. Our method has the advantage of
being conceptually simple and relies on a primal-dual semidefinite-programming
SDP approach. We first consider a natural SDP relaxation for the Balanced
Separator problem. While it is easy to obtain from this SDP a certificate of
the fact that the graph has no balanced cut of conductance less than $\gamma,$
somewhat surprisingly, we can obtain a certificate for the stronger correlation
condition. This is achieved via a novel separation oracle for our SDP and by
appealing to Arora and Kale's framework to bound the running time. Our result
contains technical ingredients that may be of independent interest."
"We consider energy-efficient scheduling on multiprocessors, where the speed
of each processor can be individually scaled, and a processor consumes power
$s^{\alpha}$ when running at speed $s$, for $\alpha>1$. A scheduling algorithm
needs to decide at any time both processor allocations and processor speeds for
a set of parallel jobs with time-varying parallelism. The objective is to
minimize the sum of the total energy consumption and certain performance
metric, which in this paper includes total flow time and makespan. For both
objectives, we present instantaneous parallelism clairvoyant (IP-clairvoyant)
algorithms that are aware of the instantaneous parallelism of the jobs at any
time but not their future characteristics, such as remaining parallelism and
work. For total flow time plus energy, we present an $O(1)$-competitive
algorithm, which significantly improves upon the best known non-clairvoyant
algorithm and is the first constant competitive result on multiprocessor speed
scaling for parallel jobs. In the case of makespan plus energy, which is
considered for the first time in the literature, we present an
$O(\ln^{1-1/\alpha}P)$-competitive algorithm, where $P$ is the total number of
processors. We show that this algorithm is asymptotically optimal by providing
a matching lower bound. In addition, we also study non-clairvoyant scheduling
for total flow time plus energy, and present an algorithm that achieves $O(\ln
P)$-competitive for jobs with arbitrary release time and
$O(\ln^{1/\alpha}P)$-competitive for jobs with identical release time. Finally,
we prove an $\Omega(\ln^{1/\alpha}P)$ lower bound on the competitive ratio of
any non-clairvoyant algorithm, matching the upper bound of our algorithm for
jobs with identical release time."
"Given two testable properties $\mathcal{P}_{1}$ and $\mathcal{P}_{2}$, under
what conditions are the union, intersection or set-difference of these two
properties also testable? We initiate a systematic study of these basic
set-theoretic operations in the context of property testing. As an application,
we give a conceptually different proof that linearity is testable, albeit with
much worse query complexity. Furthermore, for the problem of testing
disjunction of linear functions, which was previously known to be one-sided
testable with a super-polynomial query complexity, we give an improved analysis
and show it has query complexity $O(1/\eps^2)$, where $\eps$ is the distance
parameter."
"Let $G=(V,E)$ be a graph on $n$ vertices and $R$ be a set of pairs of
vertices in $V$ called \emph{requests}. A \emph{multicut} is a subset $F$ of
$E$ such that every request $xy$ of $R$ is cut by $F$, \i.e. every $xy$-path of
$G$ intersects $F$. We show that there exists an $O(f(k)n^c)$ algorithm which
decides if there exists a multicut of size at most $k$. In other words, the
\M{} problem parameterized by the solution size $k$ is Fixed-Parameter
Tractable. The proof extends to vertex multicuts."
"We analyze the so-called ppz algorithm for (d,k)-CSP problems for general
values of d (number of values a variable can take) and k (number of literals
per constraint). To analyze its success probability, we prove a correlation
inequality for submodular functions."
"Python implementation of Algorithm X by Knuth is presented. Algorithm X finds
all solutions to the exact cover problem. The exemplary results for
pentominoes, Latin squares and Sudoku are given."
"We study the problem of Upward Point-Set Embeddability, that is the problem
of deciding whether a given upward planar digraph $D$ has an upward planar
embedding into a point set $S$. We show that any switch tree admits an upward
planar straight-line embedding into any convex point set. For the class of
$k$-switch trees, that is a generalization of switch trees (according to this
definition a switch tree is a $1$-switch tree), we show that not every
$k$-switch tree admits an upward planar straight-line embedding into any convex
point set, for any $k \geq 2$. Finally we show that the problem of Upward
Point-Set Embeddability is NP-complete."
"A mixed graph is a graph with both directed and undirected edges. We present
an algorithm for deciding whether a given mixed graph on $n$ vertices contains
a feedback vertex set (FVS) of size at most $k$, in time $2^{O(k)}k! O(n^4)$.
This is the first fixed parameter tractable algorithm for FVS that applies to
both directed and undirected graphs."
"Suppose that each member of a set of agents has a preference list of a subset
of houses, possibly involving ties and each agent and house has their capacity
denoting the maximum number of correspondingly agents/houses that can be
matched to him/her/it. We want to find a matching $M$, for which there is no
other matching $M'$ such that more agents prefer $M'$ to $M$ than $M$ to $M'$.
(What it means that an agent prefers one matching to the other is explained in
the paper.) Popular matchings have been studied quite extensively, especially
in the one-to-one setting. We provide a characterization of popular b-matchings
for two defintions of popularity, show some $NP$-hardness results and for
certain versions describe polynomial algorithms."
"In this paper I have study to Reduce the time Complexity of Earliest Deadline
First (EDF), a global scheduling scheme for Earliest Deadline First in Real
Time System tasks on a Multiprocessors system. Several admission control
algorithms for Earliest Deadline First (EDF) are presented, both for hard and
soft real-time tasks. The average performance of these admission control
algorithms is compared with the performance of known partitioning schemes. I
have applied some modification to the global Earliest Deadline First (EDF)
algorithms to decrease the number of task migration and also to add
predictability to its behavior. The Aim of this work is to provide a
sensitivity analysis for task deadline context of multiprocessor system by
using a new approach of EFDF (Earliest Feasible Deadline First) algorithm. In
order to decrease the number of migrations we prevent a job from moving one
processor to another processor if it is among the m higher priority jobs.
Therefore, a job will continue its execution on the same processor if possible
(processor affinity). The result of these comparisons outlines some situations
where one scheme is preferable over the other. Partitioning schemes are better
suited for hard real-time systems, while a global scheme is preferable for soft
real-time systems."
"Practical data structures for the edit-sensitive parsing (ESP) are proposed.
Given a string S, its ESP tree is equivalent to a context-free grammar G
generating just S, which is represented by a DAG. Using the succinct data
structures for trees and permutations, G is decomposed to two LOUDS bit strings
and single array in (1+\epsilon)n\log n+4n+o(n) bits for any 0<\epsilon <1 and
the number n of variables in G. The time to count occurrences of P in S is in
O(\frac{1}{\epsilon}(m\log n+occ_c(\log m\log u)), whereas m = |P|, u = |S|,
and occ_c is the number of occurrences of a maximal common subtree in ESPs of P
and S. The efficiency of the proposed index is evaluated by the experiments
conducted on several benchmarks complying with the other compressed indexes."
"In a scheduling game, each player owns a job and chooses a machine to execute
it. While the social cost is the maximal load over all machines (makespan), the
cost (disutility) of each player is the completion time of its own job. In the
game, players may follow selfish strategies to optimize their cost and
therefore their behaviors do not necessarily lead the game to an equilibrium.
Even in the case there is an equilibrium, its makespan might be much larger
than the social optimum, and this inefficiency is measured by the price of
anarchy -- the worst ratio between the makespan of an equilibrium and the
optimum. Coordination mechanisms aim to reduce the price of anarchy by
designing scheduling policies that specify how jobs assigned to a same machine
are to be scheduled. Typically these policies define the schedule according to
the processing times as announced by the jobs. One could wonder if there are
policies that do not require this knowledge, and still provide a good price of
anarchy. This would make the processing times be private information and avoid
the problem of truthfulness. In this paper we study these so-called
non-clairvoyant policies. In particular, we study the RANDOM policy that
schedules the jobs in a random order without preemption, and the EQUI policy
that schedules the jobs in parallel using time-multiplexing, assigning each job
an equal fraction of CPU time."
"This paper studies the ""explanation problem"" for tree- and linearly-ordered
array data, a problem motivated by database applications and recently solved
for the one-dimensional tree-ordered case. In this paper, one is given a matrix
A whose rows and columns have semantics: special subsets of the rows and
special subsets of the columns are meaningful, others are not. A submatrix in A
is said to be meaningful if and only if it is the cross product of a meaningful
row subset and a meaningful column subset, in which case we call it an ""allowed
rectangle."" The goal is to ""explain"" A as a sparse sum of weighted allowed
rectangles. Specifically, we wish to find as few weighted allowed rectangles as
possible such that, for all i,j, a_{ij} equals the sum of the weights of all
rectangles which include cell (i,j).
  In this paper we consider the natural cases in which the matrix dimensions
are tree-ordered or linearly-ordered. In the tree-ordered case, we are given a
rooted tree T1 whose leaves are the rows of A and another, T2, whose leaves are
the columns. Nodes of the trees correspond in an obvious way to the sets of
their leaf descendants. In the linearly-ordered case, a set of rows or columns
is meaningful if and only if it is contiguous.
  For tree-ordered data, we prove the explanation problem NP-Hard and give a
randomized 2-approximation algorithm for it. For linearly-ordered data, we
prove the explanation problem NP-Hard and give a 2.56-approximation algorithm.
To our knowledge, these are the first results for the problem of sparsely and
exactly representing matrices by weighted rectangles."
"We study the problem of maximizing constrained non-monotone submodular
functions and provide approximation algorithms that improve existing algorithms
in terms of either the approximation factor or simplicity. Our algorithms
combine existing local search and greedy based algorithms. Different
constraints that we study are exact cardinality and multiple knapsack
constraints. For the multiple-knapsack constraints we achieve a
$(0.25-2\epsilon)$-factor algorithm.
  We also show, as our main contribution, how to use the continuous greedy
process for non-monotone functions and, as a result, obtain a $0.13$-factor
approximation algorithm for maximization over any solvable down-monotone
polytope. The continuous greedy process has been previously used for maximizing
smooth monotone submodular function over a down-monotone polytope
\cite{CCPV08}. This implies a 0.13-approximation for several discrete problems,
such as maximizing a non-negative submodular function subject to a matroid
constraint and/or multiple knapsack constraints."
"A new approach to the static route planning problem, based on a multi-staging
concept and a \emph{scope} notion, is presented. The main goal (besides implied
efficiency of planning) of our approach is to address---with a solid
theoretical foundation---the following two practically motivated aspects: a
\emph{route comfort} and a very \emph{limited storage} space of a small
navigation device, which both do not seem to be among the chief objectives of
many other studies. We show how our novel idea can tackle both these seemingly
unrelated aspects at once, and may also contribute to other established route
planning approaches with which ours can be naturally combined. We provide a
theoretical proof that our approach efficiently computes exact optimal routes
within this concept, as well as we demonstrate with experimental results on
publicly available road networks of the US the good practical performance of
the solution."
"The PDPTW is an optimization vehicles routing problem which must meet
requests for transport between suppliers and customers satisfying precedence,
capacity and time constraints. We present, in this paper, a genetic algorithm
for multi-objective optimization of a dynamic multi pickup and delivery problem
with time windows (Dynamic m-PDPTW). We propose a brief literature review of
the PDPTW, present our approach based on Pareto dominance method and lower
bounds, to give a satisfying solution to the Dynamic m-PDPTW minimizing the
compromise between total travel cost and total tardiness time. Computational
results indicate that the proposed algorithm gives good results with a total
tardiness equal to zero with a tolerable cost."
"We show how to modify the linear-time construction algorithm for suffix
arrays based on induced sorting (Nong et al., DCC'09) such that it computes the
array of longest common prefixes (LCP-array) as well. Practical tests show that
this outperforms recent LCP-array construction algorithms (Gog and Ohlebusch,
ALENEX'11)."
"Two multivehicle routing problems are considered in the framework that a
visit to a location must take place during a specific time window in order to
be counted and all time windows are the same length. In the first problem, the
goal is to visit as many locations as possible using a fixed number of
vehicles. In the second, the goal is to visit all locations using the smallest
number of vehicles possible. For the first problem, we present an approximation
algorithm whose output path collects a reward within a constant factor of
optimal for any fixed number of vehicles. For the second problem, our algorithm
finds a 6-approximation to the problem on a tree metric, whenever a single
vehicle could visit all locations during their time windows."
"A bicriteria approximation algorithm is presented for the unrooted traveling
repairman problem, realizing increased profit in return for increased speedup
of repairman motion. The algorithm generalizes previous results from the case
in which all time windows are the same length to the case in which their
lengths can range between l and 2. This analysis can extend to any range of
time window lengths, following our earlier techniques. This relationship
between repairman profit and speedup is applicable over a range of values that
is dependent on the cost of putting the input in an especially desirable form,
involving what are called ""trimmed windows."" For time windows with lengths
between 1 and 2, the range of values for speedup $s$ for which our analysis
holds is $1 \leq s \leq 6$. In this range, we establish an approximation ratio
that is constant for any specific value of $s$."
"We introduce the first self-index based on the Lempel-Ziv 1977 compression
format (LZ77). It is particularly competitive for highly repetitive text
collections such as sequence databases of genomes of related species, software
repositories, versioned document collections, and temporal text databases. Such
collections are extremely compressible but classical self-indexes fail to
capture that source of compressibility. Our self-index takes in practice a few
times the space of the text compressed with LZ77 (as little as 2.6 times),
extracts 1--2 million characters of the text per second, and finds patterns at
a rate of 10--50 microseconds per occurrence. It is smaller (up to one half)
than the best current self-index for repetitive collections, and faster in many
cases."
"A mode of a multiset $S$ is an element $a \in S$ of maximum multiplicity;
that is, $a$ occurs at least as frequently as any other element in $S$. Given a
list $A[1:n]$ of $n$ items, we consider the problem of constructing a data
structure that efficiently answers range mode queries on $A$. Each query
consists of an input pair of indices $(i, j)$ for which a mode of $A[i:j]$ must
be returned. We present an $O(n^{2-2\epsilon})$-space static data structure
that supports range mode queries in $O(n^\epsilon)$ time in the worst case, for
any fixed $\epsilon \in [0,1/2]$. When $\epsilon = 1/2$, this corresponds to
the first linear-space data structure to guarantee $O(\sqrt{n})$ query time. We
then describe three additional linear-space data structures that provide
$O(k)$, $O(m)$, and $O(|j-i|)$ query time, respectively, where $k$ denotes the
number of distinct elements in $A$ and $m$ denotes the frequency of the mode of
$A$. Finally, we examine generalizing our data structures to higher dimensions."
"We study the setting in which the bits of an unknown infinite binary sequence
x are revealed sequentially to an observer. We show that very limited
assumptions about x allow one to make successful predictions about unseen bits
of x. First, we study the problem of successfully predicting a single 0 from
among the bits of x. In our model we have only one chance to make a prediction,
but may do so at a time of our choosing. We describe and motivate this as the
problem of a frog who wants to cross a road safely.
  Letting N_t denote the number of 1s among the first t bits of x, we say that
x is ""eps-weakly sparse"" if lim inf (N_t/t) <= eps. Our main result is a
randomized algorithm that, given any eps-weakly sparse sequence x, predicts a 0
of x with success probability as close as desired to 1 - \eps. Thus we can
perform this task with essentially the same success probability as under the
much stronger assumption that each bit of x takes the value 1 independently
with probability eps. We apply this result to show how to successfully predict
a bit (0 or 1) under a broad class of possible assumptions on the sequence x.
The assumptions are stated in terms of the behavior of a finite automaton M
reading the bits of x.
  We also propose and solve a variant of the well-studied ""ignorant
forecasting"" problem. For every eps > 0, we give a randomized forecasting
algorithm S_eps that, given sequential access to a binary sequence x, makes a
prediction of the form: ""A p fraction of the next N bits will be 1s."" (The
algorithm gets to choose p, N, and the time of the prediction.) For any fixed
sequence x, the forecast fraction p is accurate to within +-eps with
probability 1 - eps."
"We develop a technique that we call Conflict Packing in the context of
kernelization, obtaining (and improving) several polynomial kernels for editing
problems on dense instances. We apply this technique on several well-studied
problems: Feedback Arc Set in (Bipartite) Tournaments, Dense Rooted Triplet
Inconsistency and Betweenness in Tournaments. For the former, one is given a
(bipartite) tournament $T = (V,A)$ and seeks a set of at most $k$ arcs whose
reversal in $T$ results in an acyclic (bipartite) tournament. While a linear
vertex-kernel is already known for the first problem, using the Conflict
Packing allows us to find a so-called safe partition, the central tool of the
kernelization algorithm in, with simpler arguments. For the case of bipartite
tournaments, the same technique allows us to obtain a quadratic vertex-kernel.
Again, such a kernel was already known to exist, using the concept of so-called
bimodules. We believe however that providing an unifying technique to cope with
such problems is interesting. Regarding Dense Rooted Triplet Inconsistency, one
is given a set of vertices $V$ and a dense collection $\mathcal{R}$ of rooted
binary trees over three vertices of $V$ and seeks a rooted tree over $V$
containing all but at most $k$ triplets from $\mathcal{R}$. As a main
consequence of our technique, we prove that the Dense Rooted Triplet
Inconsistency problem admits a linear vertex-kernel. This result improves the
best known bound of $O(k^2)$ vertices for this problem. Finally, we use this
technique to obtain a linear vertex-kernel for Betweenness in Tournaments,
where one is given a set of vertices $V$ and a dense collection $\mathcal{R}$
of so-called betweenness triplets and seeks a linear ordering of the vertices
containing all but at most $k$ triplets from $\mathcal{R}$."
"In this paper we consider methods for dynamically storing a set of different
objects (""modules"") in a physical array. Each module requires one free
contiguous subinterval in order to be placed. Items are inserted or removed,
resulting in a fragmented layout that makes it harder to insert further
modules. It is possible to relocate modules, one at a time, to another free
subinterval that is contiguous and does not overlap with the current location
of the module. These constraints clearly distinguish our problem from classical
memory allocation. We present a number of algorithmic results, including a
bound of Theta(n^2) on physical sorting if there is a sufficiently large free
space and sum up NP-hardness results for arbitrary initial layouts. For online
scenarios in which modules arrive one at a time, we present a method that
requires O(1) moves per insertion or deletion and amortized cost O(m_i log M)
per insertion or deletion, where m_i is the module's size, M is the size of the
largest module and costs for moves are linear in the size of a module."
"The problem of storing a set of strings --- a string dictionary --- in
compact form appears naturally in many cases. While classically it has
represented a small part of the whole data to be processed (e.g., for Natural
Language processing or for indexing text collections), more recent applications
in Web engines, Web mining, RDF graphs, Internet routing, Bioinformatics, and
many others, make use of very large string dictionaries, whose size is a
significant fraction of the whole data. Thus novel approaches to compress them
efficiently are necessary. In this paper we experimentally compare time and
space performance of some existing alternatives, as well as new ones we
propose. We show that space reductions of up to 20% of the original size of the
strings is possible while supporting fast dictionary searches."
"We consider the complexity of problems related to the combinatorial game
Free-Flood-It, in which players aim to make a coloured graph monochromatic with
the minimum possible number of flooding operations. Our main result is that
computing the length of an optimal sequence is fixed parameter tractable (with
the number of colours present as a parameter) when restricted to rectangular
2xn boards. We also show that, when the number of colours is unbounded, the
problem remains NP-hard on such boards. This resolves a question of Clifford,
Jalsenius, Montanaro and Sach (2010)."
"We provide a polynomial time 4/3 approximation algorithm for TSP on metrics
arising from the metric completion of cubic 3-edge connected graphs."
"We consider the complexity of problems related to the combinatorial game
Free-Flood-It, in which players aim to make a coloured graph monochromatic with
the minimum possible number of flooding operations. Although computing the
minimum number of moves required to flood an arbitrary graph is known to be
NP-hard, we demonstrate a polynomial time algorithm to compute the minimum
number of moves required to link each pair of vertices. We apply this result to
compute in polynomial time the minimum number of moves required to flood a
path, and an additive approximation to this quantity for an arbitrary k x n
board, coloured with a bounded number of colours, for any fixed k. On the other
hand, we show that, for k>=3, determining the minimum number of moves required
to flood a k x n board coloured with at least four colours remains NP-hard."
"Dynamic graphs have emerged as an appropriate model to capture the changing
nature of many modern networks, such as peer-to-peer overlays and mobile ad hoc
networks. Most of the recent research on dynamic networks has only addressed
the undirected dynamic graph model. However, realistic networks such as the
ones identified above are directed. In this paper we present early work in
addressing the properties of directed dynamic graphs. In particular, we explore
the problem of random walk in such graphs. We assume the existence of an
oblivious adversary that makes arbitrary changes in every communication round.
We explore the problem of covering the dynamic graph, that even in the static
case can be exponential, and we establish an upper bound O(d_max n^3 log^2 n)
of the cover time for balanced dynamic graphs."
"A flaw in the greedy approximation algorithm proposed by Zhang et al. for
minimum connected set cover problem is corrected, and a stronger result on the
approximation ratio of the modified greedy algorithm is established. The
results are now consistent with the existing results on connected dominating
set problem which is a special case of the minimum connected set cover problem."
"We study the fundamental problem of reliable interactive communication over a
noisy channel. In a breakthrough sequence of papers published in 1992 and 1993,
Schulman gave non-constructive proofs of the existence of general methods to
emulate any two-party interactive protocol such that: (1) the emulation
protocol takes a constant-factor longer than the original protocol, and (2) if
the emulation protocol is executed over a noisy channel, then the probability
that the emulation protocol fails is exponentially small in the total length of
the protocol. Unfortunately, Schulman's emulation procedures either only work
in a model with a large amount of shared randomness, or are non-constructive in
that they rely on the existence of good tree codes. The only known proofs of
the existence of good tree codes are non-constructive, and finding an explicit
construction remains an important open problem. Indeed, randomly generated tree
codes are not good tree codes with overwhelming probability.
  In this work, we revisit the problem of reliable interactive communication,
and obtain the following results: We introduce a new notion of goodness for a
tree code, and define the notion of a potent tree code. We believe that this
notion is of independent interest. We prove the correctness of an explicit
emulation procedure based on any potent tree code. We show that a randomly
generated tree code (with suitable constant alphabet size) is a potent tree
code with overwhelming probability. Furthermore we are able to partially
derandomize this result using only O(n) random bits, where $n$ is the depth of
the tree.
  These results allow us to obtain the first fully explicit emulation procedure
for reliable interactive communication over noisy channels with a constant
communication overhead, and exponentially small failure probability."
"Several biological problems require the identification of regions in a
sequence where some feature occurs within a target density range: examples
including the location of GC-rich regions, identification of CpG islands, and
sequence matching. Mathematically, this corresponds to searching a string of 0s
and 1s for a substring whose relative proportion of 1s lies between given lower
and upper bounds. We consider the algorithmic problem of locating the longest
such substring, as well as other related problems (such as finding the shortest
substring or a maximal set of disjoint substrings). For locating the longest
such substring, we develop an algorithm that runs in O(n) time, improving upon
the previous best-known O(n log n) result. For the related problems we develop
O(n log log n) algorithms, again improving upon the best-known O(n log n)
results. Practical testing verifies that our new algorithms enjoy significantly
smaller time and memory footprints, and can process sequences that are orders
of magnitude longer as a result."
"The firefighter problem is defined as below. A fire initially breaks out at a
vertex r on a graph G. In each step, a firefighter chooses to protect one
vertex, which is not yet burnt. And the fire spreads out to its unprotected
neighboring vertices afterwards. The objective of the problem is to choose a
sequence of vertices to protect, in order to save maximum number of vertices
from the fire.
  In this paper, we will introduce a parameter k into the firefighter problem
and give several FPT algorithms using a random separation technique of Cai,
Chan and Chan. We will prove firefighter problem is FPT on general graph if we
take total number of vertices burnt to be a parameter. If we parameterize the
number of protected vertices, we discover several FPT algorithms of the
firefighter problem on degree bounded graph and unicyclic graph. Furthermore,
we also study the firefighter problem on weighted and valued graph, and the
problem with multiple fire sources on degree-bounded graph."
"We present an algorithm for multi-hop routing and scheduling of requests in
wireless networks in the \sinr\ model. The goal of our algorithm is to maximize
the throughput or maximize the minimum ratio between the flow and the demand.
  Our algorithm partitions the links into buckets. Every bucket consists of a
set of links that have nearly equivalent reception powers. We denote the number
of nonempty buckets by $\sigdiv$. Our algorithm obtains an approximation ratio
of $O(\sigdiv \cdot \log n)$, where $n$ denotes the number of nodes. For the
case of linear powers $\sigdiv =1$, hence the approximation ratio of the
algorithm is $O(\log n)$. This is the first practical approximation algorithm
for linear powers with an approximation ratio that depends only on $n$ (and not
on the max-to-min distance ratio).
  If the transmission power of each link is part of the input (and arbitrary),
then $\sigdiv = O(\log\Gamma + \log \Delta)$, where $\Gamma$ denotes the ratio
of the max-to-min power, and $\Delta$ denotes the ratio of the max-to-min
distance. Hence, the approximation ratio is $O(\log n \cdot (\log\Gamma + \log
\Delta))$.
  Finally, we consider the case that the algorithm needs to assign powers to
each link in a range $[\pmin,\pmax]$. An extension of the algorithm to this
case achieves an approximation ratio of $O[(\log n + \log \log \Gamma) \cdot
(\log\Gamma + \log \Delta)]$."
"For input $x$, let $F(x)$ denote the set of outputs that are the ""legal""
answers for a computational problem $F$. Suppose $x$ and members of $F(x)$ are
so large that there is not time to read them in their entirety. We propose a
model of {\em local computation algorithms} which for a given input $x$,
support queries by a user to values of specified locations $y_i$ in a legal
output $y \in F(x)$. When more than one legal output $y$ exists for a given
$x$, the local computation algorithm should output in a way that is consistent
with at least one such $y$. Local computation algorithms are intended to
distill the common features of several concepts that have appeared in various
algorithmic subfields, including local distributed computation, local
algorithms, locally decodable codes, and local reconstruction.
  We develop a technique, based on known constructions of small sample spaces
of $k$-wise independent random variables and Beck's analysis in his algorithmic
approach to the Lov{\'{a}}sz Local Lemma, which under certain conditions can be
applied to construct local computation algorithms that run in {\em
polylogarithmic} time and space. We apply this technique to maximal independent
set computations, scheduling radio network broadcasts, hypergraph coloring and
satisfying $k$-SAT formulas."
"We propose a simple linear-time on-line algorithm for constructing a position
heap for a string [Ehrenfeucht et al, 2011]. Our definition of position heap
differs slightly from the one proposed in [Ehrenfeucht et al, 2011] in that it
considers the suffixes ordered from left to right. Our construction is based on
classic suffix pointers and resembles the Ukkonen's algorithm for suffix trees
[Ukkonen, 1995]. Using suffix pointers, the position heap can be extended into
the augmented position heap that allows for a linear-time string matching
algorithm [Ehrenfeucht et al, 2011]."
"The vast majority of scientific community believes that P!=NP, with countless
supporting arguments. The number of people who believe otherwise probably
amounts to as few as those opposing the 2nd Law of Thermodynamics. But isn't
nature elegant enough, not to resource to brute-force search? In this article,
a novel concept of dimensionality is presented, which may lead to a more
efficient class of heuristic implementations to solve NP complete problems.
Thus, broadening the universe of man-machine tractable problems.
Dimensionality, as defined here, will be a closer analog of strain energy in
nature."
"This paper explores the application of a new algebraic method of color
exchanges to the edge coloring of simple graphs. Vizing's theorem states that
the edge coloring of a simple graph $G$ requires either $\Delta$ or $\Delta+1$
colors, where $\Delta$ is the maximum vertex degree of $G$. Holyer proved that
it is {\bf NP}-complete to decide whether $G$ is $\Delta$-edge-colorable even
for cubic graphs. By introducing the concept of complex colors, we show that
the color-exchange operation follows the same multiplication rules as
quaternion. An initially $\Delta$-edge-colored graph $G$ allows
variable-colored edges, which can be eliminated by color exchanges in a manner
similar to variable eliminations in solving systems of linear equations. The
problem is solved if all variables are eliminated and a properly
$\Delta$-edge-colored graph is reached. For a randomly generated graph $G$, we
prove that our algorithm returns a proper $\Delta$-edge-coloring with a
probability of at least 1/2 in $O(\Delta|V||E|^5)$ time if $G$ is
$\Delta$-edge-colorable. Otherwise, the algorithm halts in polynomial time and
signals the impossibility of a solution, meaning that the chromatic index of
$G$ probably equals $\Delta+1$. Animations of the edge-coloring algorithms
proposed in this paper are posted at YouTube
http://www.youtube.com/watch?v=KMnj4UMYl7k."
"We give an efficient algorithm which can obtain a relative error
approximation to the spectral norm of a matrix, combining the power iteration
method with some techniques from matrix reconstruction which use random
sampling."
"The Minimum Fill-in problem is to decide if a graph can be triangulated by
adding at most k edges. Kaplan, Shamir, and Tarjan [FOCS 1994] have shown that
the problem is solvable in time O(2^(O(k)) + k2 * nm) on graphs with n vertices
and m edges and thus is fixed parameter tractable. Here, we give the first
subexponential parameterized algorithm solving Minimum Fill-in in time
O(2^(O(\sqrt{k} log k)) + k2 * nm). This substantially lower the complexity of
the problem. Techniques developed for Minimum Fill-in can be used to obtain
subexponential parameterized algorithms for several related problems including
Minimum Chain Completion, Chordal Graph Sandwich, and Triangulating Colored
Graph."
"Many algorithms have been developed for NP-hard problems on graphs with small
treewidth $k$. For example, all problems that are expressable in linear
extended monadic second order can be solved in linear time on graphs of bounded
treewidth. It turns out that the bottleneck of many algorithms for NP-hard
problems is the computation of a tree decomposition of width $O(k)$. In
particular, by the bidimensional theory, there are many linear extended monadic
second order problems that can be solved on $n$-vertex planar graphs with
treewidth $k$ in a time linear in $n$ and subexponential in $k$ if a tree
decomposition of width $O(k)$ can be found in such a time.
  We present the first algorithm that, on $n$-vertex planar graphs with
treewidth $k$, finds a tree decomposition of width $O(k)$ in such a time. In
more detail, our algorithm has a running time of $O(n k^2 \log k)$. We show the
result as a special case of a result concerning so-called weighted treewidth of
weighted graphs."
"In this paper, we study linear programming based approaches to the maximum
matching problem in the semi-streaming model. The semi-streaming model has
gained attention as a model for processing massive graphs as the importance of
such graphs has increased. This is a model where edges are streamed-in in an
adversarial order and we are allowed a space proportional to the number of
vertices in a graph.
  In recent years, there has been several new results in this semi-streaming
model. However broad techniques such as linear programming have not been
adapted to this model. We present several techniques to adapt and optimize
linear programming based approaches in the semi-streaming model with an
application to the maximum matching problem. As a consequence, we improve
(almost) all previous results on this problem, and also prove new results on
interesting variants."
"We consider the dictionary problem in external memory and improve the update
time of the well-known buffer tree by roughly a logarithmic factor. For any
\lambda >= max {lg lg n, log_{M/B} (n/B)}, we can support updates in time
O(\lambda / B) and queries in sublogarithmic time, O(log_\lambda n). We also
present a lower bound in the cell-probe model showing that our data structure
is optimal.
  In the RAM, hash tables have been used to solve the dictionary problem faster
than binary search for more than half a century. By contrast, our data
structure is the first to beat the comparison barrier in external memory. Ours
is also the first data structure to depart convincingly from the indivisibility
paradigm."
"We consider the fundamental algorithmic problem of finding a cycle of minimum
weight in a weighted graph. In particular, we show that the minimum weight
cycle problem in an undirected n-node graph with edge weights in {1,...,M} or
in a directed n-node graph with edge weights in {-M,..., M} and no negative
cycles can be efficiently reduced to finding a minimum weight triangle in an
Theta(n)-node undirected graph with weights in {1,...,O(M)}. Roughly speaking,
our reductions imply the following surprising phenomenon: a minimum cycle with
an arbitrary number of weighted edges can be ""encoded"" using only three edges
within roughly the same weight interval! This resolves a longstanding open
problem posed by Itai and Rodeh [SIAM J. Computing 1978 and STOC'77].
  A direct consequence of our efficient reductions are O (Mn^{omega})-time
algorithms using fast matrix multiplication (FMM) for finding a minimum weight
cycle in both undirected graphs with integral weights from the interval [1,M]
and directed graphs with integral weights from the interval [-M,M]. The latter
seems to reveal a strong separation between the all pairs shortest paths (APSP)
problem and the minimum weight cycle problem in directed graphs as the fastest
known APSP algorithm has a running time of O(M^{0.681}n^{2.575}) by Zwick [J.
ACM 2002].
  In contrast, when only combinatorial algorithms are allowed (that is, without
FMM) the only known solution to minimum weight cycle is by computing APSP.
Interestingly, any separation between the two problems in this case would be an
amazing breakthrough as by a recent paper by Vassilevska W. and Williams
[FOCS'10], any O(n^{3-eps})-time algorithm (eps>0) for minimum weight cycle
immediately implies a O(n^{3-delta})-time algorithm (delta>0) for APSP."
"It is shown how to enhance any data structure in the pointer model to make it
confluently persistent, with efficient query and update times and limited space
overhead. Updates are performed in $O(\log n)$ amortized time, and following a
pointer takes $O(\log c \log n)$ time where $c$ is the in-degree of a node in
the data structure. In particular, this proves that confluent persistence can
be achieved at a logarithmic cost in the bounded in-degree model used widely in
previous work. This is a $O(n/\log n)$-factor improvement over the previous
known transform to make a data structure confluently persistent."
"We present a framework for approximating the metric TSP based on a novel use
of matchings. Traditionally, matchings have been used to add edges in order to
make a given graph Eulerian, whereas our approach also allows for the removal
of certain edges leading to a decreased cost.
  For the TSP on graphic metrics (graph-TSP), the approach yields a
1.461-approximation algorithm with respect to the Held-Karp lower bound. For
graph-TSP restricted to a class of graphs that contains degree three bounded
and claw-free graphs, we show that the integrality gap of the Held-Karp
relaxation matches the conjectured ratio 4/3. The framework allows for
generalizations in a natural way and also leads to a 1.586-approximation
algorithm for the traveling salesman path problem on graphic metrics where the
start and end vertices are prespecified."
"Herman's algorithm is a synchronous randomized protocol for achieving
self-stabilization in a token ring consisting of N processes. The interaction
of tokens makes the dynamics of the protocol very difficult to analyze. In this
paper we study the expected time to stabilization in terms of the initial
configuration. It is straightforward that the algorithm achieves stabilization
almost surely from any initial configuration, and it is known that the
worst-case expected time to stabilization (with respect to the initial
configuration) is Theta(N^2). Our first contribution is to give an upper bound
of 0.64 N^2 on the expected stabilization time, improving on previous upper
bounds and reducing the gap with the best existing lower bound. We also
introduce an asynchronous version of the protocol, showing a similar O(N^2)
convergence bound in this case. Assuming that errors arise from the corruption
of some number k of bits, where k is fixed independently of the size of the
ring, we show that the expected time to stabilization is O(N). This reveals a
hitherto unknown and highly desirable property of Herman's algorithm: it
recovers quickly from bounded errors. We also show that if the initial
configuration arises by resetting each bit independently and uniformly at
random, then stabilization is significantly faster than in the worst case."
"This paper focuses on reducing memory usage in enumerative model checking,
while maintaining the multi-core scalability obtained in earlier work. We
present a tree-based multi-core compression method, which works by leveraging
sharing among sub-vectors of state vectors.
  An algorithmic analysis of both worst-case and optimal compression ratios
shows the potential to compress even large states to a small constant on
average (8 bytes). Our experiments demonstrate that this holds up in practice:
the median compression ratio of 279 measured experiments is within 17% of the
optimum for tree compression, and five times better than the median compression
ratio of SPIN's COLLAPSE compression.
  Our algorithms are implemented in the LTSmin tool, and our experiments show
that for model checking, multi-core tree compression pays its own way: it comes
virtually without overhead compared to the fastest hash table-based methods."
"The notion of the cover is a generalization of a period of a string, and
there are linear time algorithms for finding the shortest cover. The seed is a
more complicated generalization of periodicity, it is a cover of a superstring
of a given string, and the shortest seed problem is of much higher algorithmic
difficulty. The problem is not well understood, no linear time algorithm is
known. In the paper we give linear time algorithms for some of its versions ---
computing shortest left-seed array, longest left-seed array and checking for
seeds of a given length. The algorithm for the last problem is used to compute
the seed array of a string (i.e., the shortest seeds for all the prefixes of
the string) in $O(n^2)$ time. We describe also a simpler alternative algorithm
computing efficiently the shortest seeds. As a by-product we obtain an
$O(n\log{(n/m)})$ time algorithm checking if the shortest seed has length at
least $m$ and finding the corresponding seed. We also correct some important
details missing in the previously known shortest-seed algorithm (Iliopoulos et
al., 1996)."
"Vertex deletion and edge deletion problems play a central role in
Parameterized Complexity. Examples include classical problems like Feedback
Vertex Set, Odd Cycle Transversal, and Chordal Deletion. Interestingly, the
study of edge contraction problems of this type from a parameterized
perspective has so far been left largely unexplored. We consider two basic edge
contraction problems, which we call Path-Contractibility and
Tree-Contractibility. Both problems take an undirected graph $G$ and an integer
$k$ as input, and the task is to determine whether we can obtain a path or an
acyclic graph, respectively, by contracting at most $k$ edges of $G$. Our main
contribution is an algorithm with running time $4^{k+O(\log^2 k)} + n^{O(1)}$
for Path-Contractibility and an algorithm with running time $4.88^k n^{O(1)}$
for Tree-Contractibility, based on a novel application of the color coding
technique of Alon, Yuster and Zwick. Furthermore, we show that
Path-Contractibility has a kernel with at most $5k+3$ vertices, while
Tree-Contractibility does not have a polynomial kernel unless coNP $\subseteq$
NP/poly. We find the latter result surprising, because of the strong connection
between Tree-Contractibility and Feedback Vertex Set, which is known to have a
vertex kernel with size $O(k^2)$."
"The minimum-cost subset $k$-connected subgraph problem is a cornerstone
problem in the area of network design with vertex connectivity requirements. In
this problem, we are given a graph $G=(V,E)$ with costs on edges and a set of
terminals $T$. The goal is to find a minimum cost subgraph such that every pair
of terminals are connected by $k$ openly (vertex) disjoint paths. In this
paper, we present an approximation algorithm for the subset $k$-connected
subgraph problem which improves on the previous best approximation guarantee of
$O(k^2\log{k})$ by Nutov (FOCS 2009). Our approximation guarantee,
$\alpha(|T|)$, depends upon the number of terminals: [\alpha(|T|) \ \ =\ \
O(|T|^2) & if |T| < 2k O(k \log^2 k) & if 2k\le |T| < k^2 O(k \log k) & if |T|
\ge k^2]
  So, when the number of terminals is {\em large enough}, the approximation
guarantee improves significantly. Moreover, we show that, given an
approximation algorithm for $|T|=k$, we can obtain almost the same
approximation guarantee for any instances with $|T|> k$. This suggests that the
hardest instances of the problem are when $|T|\approx k$."
"In this paper, we study the non-bipartite maximum matching problem in the
semi-streaming model. The maximum matching problem in the semi-streaming model
has received a significant amount of attention lately. While the problem has
been somewhat well solved for bipartite graphs, the known algorithms for
non-bipartite graphs use $2^{\frac1\epsilon}$ passes or $n^{\frac1\epsilon}$
time to compute a $(1-\epsilon)$ approximation. In this paper we provide the
first FPTAS (polynomial in $n,\frac1\epsilon$) for the problem which is
efficient in both the running time and the number of passes. We also show that
we can estimate the size of the matching in $O(\frac1\epsilon)$ passes using
slightly superlinear space.
  To achieve both results, we use the structural properties of the matching
polytope such as the laminarity of the tight sets and total dual integrality.
The algorithms are iterative, and are based on the fractional packing and
covering framework. However the formulations herein require exponentially many
variables or constraints. We use laminarity, metric embeddings and graph
sparsification to reduce the space required by the algorithms in between and
across the iterations. This is the first use of these ideas in the
semi-streaming model to solve a combinatorial optimization problem."
"Countless variants of the Lempel-Ziv compression are widely used in many
real-life applications. This paper is concerned with a natural modification of
the classical pattern matching problem inspired by the popularity of such
compression methods: given an uncompressed pattern s[1..m] and a Lempel-Ziv
representation of a string t[1..N], does s occur in t? Farach and Thorup gave a
randomized O(nlog^2(N/n)+m) time solution for this problem, where n is the size
of the compressed representation of t. We improve their result by developing a
faster and fully deterministic O(nlog(N/n)+m) time algorithm with the same
space complexity. Note that for highly compressible texts, log(N/n) might be of
order n, so for such inputs the improvement is very significant. A (tiny)
fragment of our method can be used to give an asymptotically optimal solution
for the substring hashing problem considered by Farach and Muthukrishnan."
"We show that the Satisfiability (SAT) problem for CNF formulas with
{\beta}-acyclic hypergraphs can be solved in polynomial time by using a special
type of Davis-Putnam resolution in which each resolvent is a subset of a parent
clause. We extend this class to CNF formulas for which this type of
Davis-Putnam resolution still applies and show that testing membership in this
class is NP-complete. We compare the class of {\beta}-acyclic formulas and this
superclass with a number of known polynomial formula classes. We then study the
parameterized complexity of SAT for ""almost"" {\beta}-acyclic instances, using
as parameter the formula's distance from being {\beta}-acyclic. As distance we
use the size of a smallest strong backdoor set and the {\beta}-hypertree width.
As a by-product we obtain the W[1]-hardness of SAT parameterized by the
(undirected) clique-width of the incidence graph, which disproves a conjecture
by Fischer, Makowsky, and Ravve."
"We solve the dynamic Predecessor Problem with high probability (whp) in
constant time, using only $n^{1+\delta}$ bits of memory, for any constant
$\delta > 0$. The input keys are random wrt a wider class of the well studied
and practically important class of $(f_1, f_2)$-smooth distributions introduced
in \cite{and:mat}. It achieves O(1) whp amortized time. Its worst-case time is
$O(\sqrt{\frac{\log n}{\log \log n}})$. Also, we prove whp $O(\log \log \log
n)$ time using only $n^{1+ \frac{1}{\log \log n}}= n^{1+o(1)}$ bits. Finally,
we show whp $O(\log \log n)$ time using O(n) space."
"In computational phylogenetics, the problem of constructing a supertree of a
given set of rooted input trees can be formalized in different ways, to cope
with contradictory information in the input. We consider the Minimum Flip
Supertree problem, where the input trees are transformed into a 0/1/?-matrix,
such that each row represents a taxon, and each column represents an inner node
of one of the input trees. Our goal is to find a perfect phylogeny for the
input matrix requiring a minimum number of 0/1-flips, that is, corrections of
0/1-entries in the matrix. The problem is known to be NP-complete. Here, we
present a parameterized data reduction with polynomial running time. The data
reduction guarantees that the reduced instance has a solution if and only if
the original instance has a solution. We then make our data reduction
parameter-independent by using upper bounds. This allows us to preprocess an
instance, and to solve the reduced instance with an arbitrary method. Different
from an existing data reduction for the consensus tree problem, our reduction
allows us to draw conclusions about certain entries in the matrix. We have
implemented and evaluated our data reduction. Unfortunately, we find that the
Minimum Flip Supertree problem is also hard in practice: The amount of
information that can be derived during data reduction diminishes as instances
get more ""complicated"", and running times for ""complicated"" instances quickly
become prohibitive. Still, our method offers another route of attack for this
relevant phylogenetic problem."
"We present an algorithm for computing $F_p$, the $p$th moment of an
$n$-dimensional frequency vector of a data stream, for $2 < p < \log (n) $, to
within $1\pm \epsilon$ factors, $\epsilon \in [n^{-1/p},1]$ with high constant
probability. Let $m$ be the number of stream records and $M$ be the largest
magnitude of a stream update.
  The algorithm uses space in bits $$ O(p^2\epsilon^{-2}n^{1-2/p}E(p,n) \log
(n) \log (nmM)/\min(\log (n),\epsilon^{4/p-2}))$$ where, $E(p,n) =
(1-2/p)^{-1}(1-n^{-4(1-2/p})$. Here $E(p,n)$ is $ O(1)$ for $p = 2+\Omega(1)$
and $ O(\log n)$ for $p = 2 + O(1/\log (n)$. This improves upon the space
required by current algorithms
\cite{iw:stoc05,bgks:soda06,ako:arxiv10,bo:arxiv10} by a factor of at least
$\Omega(\epsilon^{-4/p} \min(\log (n), \epsilon^{4/p-2}))$. The update time is
$O(\log (n))$. We use a new technique for designing estimators for functions of
the form $\psi(\expect{X})$, where, $X$ is a random variable and $\psi$ is a
smooth function, based on a low-degree Taylor polynomial expansion of
$\psi(\expect{X})$ around an estimate of $\expect{X}$."
"Given an edge-weighted graph $G$ and $\epsilon>0$, a $(1+\epsilon)$-spanner
is a spanning subgraph $G'$ whose shortest path distances approximate those of
$G$ within a $(1+\epsilon)$ factor. If $G$ is from certain minor-closed graph
families (at least bounded genus graphs and apex graphs), then we know that
light spanners exist. That is, we can compute a $(1+\epsilon)$-spanner $G'$
with total edge weight at most a constant times the weight of a minimum
spanning tree. This constant may depend on $\epsilon$ and the graph family, but
not on the particular graph $G$ nor on its edge weighting. For weighted graphs
from several minor-closed graph families, the existence of light spanners has
been essential in the design of approximation schemes for the metric TSP (the
traveling salesman problem) and some similar problems. In this paper we make
some progress towards the conjecture that light spanners exist for every
minor-closed graph family. In particular, we show that they exist for graphs
with bounded pathwidth. We do this via the construction of light enough
monotone spanning trees in such graphs."
"We present a deterministic O(n log log n) time algorithm for finding shortest
cycles and minimum cuts in planar graphs. The algorithm improves the previously
known fastest algorithm by Italiano et al. in STOC'11 by a factor of log n.
This speedup is obtained through the use of dense distance graphs combined with
a divide-and-conquer approach."
"The girth of a graph is the minimum weight of all simple cycles of the graph.
We study the problem of determining the girth of an n-node unweighted
undirected planar graph. The first non-trivial algorithm for the problem, given
by Djidjev, runs in O(n^{5/4} log n) time. Chalermsook, Fakcharoenphol, and
Nanongkai reduced the running time to O(n log^2 n). Weimann and Yuster further
reduced the running time to O(n log n). In this paper, we solve the problem in
O(n) time."
"Although cuckoo hashing has significant applications in both theoretical and
practical settings, a relevant downside is that it requires lookups to multiple
locations. In many settings, where lookups are expensive, cuckoo hashing
becomes a less compelling alternative. One such standard setting is when memory
is arranged in large pages, and a major cost is the number of page accesses. We
propose the study of cuckoo hashing with pages, advocating approaches where
each key has several possible locations, or cells, on a single page, and
additional choices on a second backup page. We show experimentally that with k
cell choices on one page and a single backup cell choice, one can achieve
nearly the same loads as when each key has k+1 random cells to choose from,
with most lookups requiring just one page access, even when keys are placed
online using a simple algorithm. While our results are currently experimental,
they suggest several interesting new open theoretical questions for cuckoo
hashing with pages."
"Cuckoo hashing [4] is a multiple choice hashing scheme in which each item can
be placed in multiple locations, and collisions are resolved by moving items to
their alternative locations. In the classical implementation of two-way cuckoo
hashing, the memory is partitioned into contiguous disjoint fixed-size buckets.
Each item is hashed to two buckets, and may be stored in any of the positions
within those buckets. Ref. [2] analyzed a variation in which the buckets are
contiguous and overlap. However, many systems retrieve data from secondary
storage in same-size blocks called pages. Fetching a page is a relatively
expensive process; but once a page is fetched, its contents can be accessed
orders of magnitude faster. We utilize this property of memory retrieval,
presenting a variant of cuckoo hashing incorporating the following constraint:
each bucket must be fully contained in a single page, but buckets are not
necessarily contiguous. Empirical results show that this modification increases
memory utilization and decreases the number of iterations required to insert an
item. If each item is hashed to two buckets of capacity two, the page size is
8, and each bucket is fully contained in a single page, the memory utilization
equals 89.71% in the classical contiguous disjoint bucket variant, 93.78% in
the contiguous overlapping bucket variant, and increases to 97.46% in our new
non-contiguous bucket variant. When the memory utilization is 92% and we use
breadth first search to look for a vacant position, the number of iterations
required to insert a new item is dramatically reduced from 545 in the
contiguous overlapping buckets variant to 52 in our new non-contiguous bucket
variant. In addition to the empirical results, we present a theoretical lower
bound on the memory utilization of our variation as a function of the page
size."
"Many data structures support dictionaries, also known as maps or associative
arrays, which store and manage a set of key-value pairs. A \emph{multimap} is
generalization that allows multiple values to be associated with the same key.
For example, the inverted file data structure that is used prevalently in the
infrastructure supporting search engines is a type of multimap, where words are
used as keys and document pointers are used as values. We study the multimap
abstract data type and how it can be implemented efficiently online in external
memory frameworks, with constant expected I/O performance. The key technique
used to achieve our results is a combination of cuckoo hashing using buckets
that hold multiple items with a multiqueue implementation to cope with varying
numbers of values per key. Our external-memory results are for the standard
two-level memory model."
"Randomized algorithms for very large matrix problems have received a great
deal of attention in recent years. Much of this work was motivated by problems
in large-scale data analysis, and this work was performed by individuals from
many different research communities. This monograph will provide a detailed
overview of recent work on the theory of randomized matrix algorithms as well
as the application of those ideas to the solution of practical problems in
large-scale data analysis. An emphasis will be placed on a few simple core
ideas that underlie not only recent theoretical advances but also the
usefulness of these tools in large-scale data applications. Crucial in this
context is the connection with the concept of statistical leverage. This
concept has long been used in statistical regression diagnostics to identify
outliers; and it has recently proved crucial in the development of improved
worst-case matrix algorithms that are also amenable to high-quality numerical
implementation and that are useful to domain scientists. Randomized methods
solve problems such as the linear least-squares problem and the low-rank matrix
approximation problem by constructing and operating on a randomized sketch of
the input matrix. Depending on the specifics of the situation, when compared
with the best previously-existing deterministic algorithms, the resulting
randomized algorithms have worst-case running time that is asymptotically
faster; their numerical implementations are faster in terms of clock-time; or
they can be implemented in parallel computing environments where existing
numerical algorithms fail to run at all. Numerous examples illustrating these
observations will be described in detail."
"First, we study the Unconstrained Fault-Tolerant Resource Allocation (UFTRA)
problem (a.k.a. FTFA problem in \cite{shihongftfa}). In the problem, we are
given a set of sites equipped with an unconstrained number of facilities as
resources, and a set of clients with set $\mathcal{R}$ as corresponding
connection requirements, where every facility belonging to the same site has an
identical opening (operating) cost and every client-facility pair has a
connection cost. The objective is to allocate facilities from sites to satisfy
$\mathcal{R}$ at a minimum total cost. Next, we introduce the Constrained
Fault-Tolerant Resource Allocation (CFTRA) problem. It differs from UFTRA in
that the number of resources available at each site $i$ is limited by $R_{i}$.
Both problems are practical extensions of the classical Fault-Tolerant Facility
Location (FTFL) problem \cite{Jain00FTFL}. For instance, their solutions
provide optimal resource allocation (w.r.t. enterprises) and leasing (w.r.t.
clients) strategies for the contemporary cloud platforms.
  In this paper, we consider the metric version of the problems. For UFTRA with
uniform $\mathcal{R}$, we present a star-greedy algorithm. The algorithm
achieves the approximation ratio of 1.5186 after combining with the cost
scaling and greedy augmentation techniques similar to
\cite{Charikar051.7281.853,Mahdian021.52}, which significantly improves the
result of \cite{shihongftfa} using a phase-greedy algorithm. We also study the
capacitated extension of UFTRA and give a factor of 2.89. For CFTRA with
uniform $\mathcal{R}$, we slightly modify the algorithm to achieve
1.5186-approximation. For a more general version of CFTRA, we show that it is
reducible to FTFL using linear programming."
"A simple algorithm with quasi-linear time complexity and linear space
complexity for the evaluation of the hypergeometric series with rational
coefficients is constructed. It is shown that this algorithm is suitable in
practical informatics for constructive analogues of often used constants of
analysis."
"The intersection graph of a collection of trapezoids with corner points lying
on two parallel lines is called a trapezoid graph. Using binary indexed tree
data structure, we improve algorithms for calculating the size and the number
of minimum vertex covers (or independent sets), as well as the total number of
vertex covers, and reduce the time complexity from $O (n^2)$ to $O (n \log n)$,
where $n$ is the number of trapezoids. Furthermore, we present the family of
counterexamples for recently proposed algorithm with time complexity $O (n^2)$
for calculating the maximum cardinality matching in trapezoid graphs."
"In this paper, we introduce and study ""geometric simultaneous RAC drawing
problems"", i.e., a combination of problems on geometric RAC drawings and
geometric simultaneous graph drawings. To the best of our knowledge, this is
the first time where such a combination is attempted."
"Let $H$ be an undirected graph. In the List $H$-Homomorphism Problem, given
an undirected graph $G$ with a list constraint $L(v) \subseteq V(H)$ for each
variable $v \in V(G)$, the objective is to find a list $H$-homomorphism $f:V(G)
\to V(H)$, that is, $f(v) \in L(v)$ for every $v \in V(G)$ and $(f(u),f(v)) \in
E(H)$ whenever $(u,v) \in E(G)$.
  We consider the following problem: given a map $f:V(G) \to V(H)$ as an oracle
access, the objective is to decide with high probability whether $f$ is a list
$H$-homomorphism or \textit{far} from any list $H$-homomorphisms. The
efficiency of an algorithm is measured by the number of accesses to $f$.
  In this paper, we classify graphs $H$ with respect to the query complexity
for testing list $H$-homomorphisms and show the following trichotomy holds: (i)
List $H$-homomorphisms are testable with a constant number of queries if and
only if $H$ is a reflexive complete graph or an irreflexive complete bipartite
graph. (ii) List $H$-homomorphisms are testable with a sublinear number of
queries if and only if $H$ is a bi-arc graph. (iii) Testing list
$H$-homomorphisms requires a linear number of queries if $H$ is not a bi-arc
graph."
"We present space lower bounds for online pattern matching under a number of
different distance measures. Given a pattern of length m and a text that
arrives one character at a time, the online pattern matching problem is to
report the distance between the pattern and a sliding window of the text as
soon as the new character arrives. We require that the correct answer is given
at each position with constant probability. We give Omega(m) bit space lower
bounds for L_1, L_2, L_\infty, Hamming, edit and swap distances as well as for
any algorithm that computes the cross-correlation/convolution. We then show a
dichotomy between distance functions that have wildcard-like properties and
those that do not. In the former case which includes, as an example, pattern
matching with character classes, we give Omega(m) bit space lower bounds. For
other distance functions, we show that there exist space bounds of Omega(log m)
and O(log^2 m) bits. Finally we discuss space lower bounds for non-binary
inputs and show how in some cases they can be improved."
"Partitioning oracles were introduced by Hassidim et al. (FOCS 2009) as a
generic tool for constant-time algorithms. For any epsilon > 0, a partitioning
oracle provides query access to a fixed partition of the input bounded-degree
minor-free graph, in which every component has size poly(1/epsilon), and the
number of edges removed is at most epsilon*n, where n is the number of vertices
in the graph.
  However, the oracle of Hassidimet al. makes an exponential number of queries
to the input graph to answer every query about the partition. In this paper, we
construct an efficient partitioning oracle for graphs with constant treewidth.
The oracle makes only O(poly(1/epsilon)) queries to the input graph to answer
each query about the partition.
  Examples of bounded-treewidth graph classes include k-outerplanar graphs for
fixed k, series-parallel graphs, cactus graphs, and pseudoforests. Our oracle
yields poly(1/epsilon)-time property testing algorithms for membership in these
classes of graphs. Another application of the oracle is a poly(1/epsilon)-time
algorithm that approximates the maximum matching size, the minimum vertex cover
size, and the minimum dominating set size up to an additive epsilon*n in graphs
with bounded treewidth. Finally, the oracle can be used to test in
poly(1/epsilon) time whether the input bounded-treewidth graph is k-colorable
or perfect."
"We investigate \emph{bi-valued} auctions in the digital good setting and
construct an explicit polynomial time deterministic auction. We prove an
unconditional tight lower bound which holds even for random superpolynomial
auctions. The analysis of the construction uses the adoption of the finer lens
of \emph{general competitiveness} which considers additive losses on top of
multiplicative ones. The result implies that general competitiveness is the
right notion to use in this setting, as this optimal auction is uncompetitive
with respect to competitive measures which do not consider additive losses."
"Given an n-vertex graph G=(V,E) and a set R \subseteq {{x,y} | x,y \in V} of
requests, we consider to assign a set of edges to each vertex in G so that for
every request {u, v} in R the union of the edge sets assigned to u and v
contains a path from u to v. The Minimum Certificate Dispersal Problem (MCD) is
defined as one to find an assignment that minimizes the sum of the cardinality
of the edge set assigned to each vertex. This problem has been shown to be
LOGAPX-complete for the most general setting, and APX-hard and 2-approximable
in polynomial time for dense request sets, where R forms a clique. In this
paper, we investigate the complexity of MCD with sparse (tree) structures. We
first show that MCD is APX-hard when R is a tree, even a star. We then explore
the problem from the viewpoint of the maximum degree \Delta of the tree: MCD
for tree request set with constant \Delta is solvable in polynomial time, while
that with \Delta=\Omega(n) is 2.56-approximable in polynomial time but hard to
approximate within 1.01 unless P=NP. As for the structure of G itself, we show
that the problem can be solved in polynomial time if G is a tree."
"We consider the problem of locating a black hole in synchronous anonymous
networks using finite state agents. A black hole is a harmful node in the
network that destroys any agent visiting that node without leaving any trace.
The objective is to locate the black hole without destroying too many agents.
This is difficult to achieve when the agents are initially scattered in the
network and are unaware of the location of each other. Previous studies for
black hole search used more powerful models where the agents had non-constant
memory, were labelled with distinct identifiers and could either write messages
on the nodes of the network or mark the edges of the network. In contrast, we
solve the problem using a small team of finite-state agents each carrying a
constant number of identical tokens that could be placed on the nodes of the
network. Thus, all resources used in our algorithms are independent of the
network size. We restrict our attention to oriented torus networks and first
show that no finite team of finite state agents can solve the problem in such
networks, when the tokens are not movable. In case the agents are equipped with
movable tokens, we determine lower bounds on the number of agents and tokens
required for solving the problem in torus networks of arbitrary size. Further,
we present a deterministic solution to the black hole search problem for
oriented torus networks, using the minimum number of agents and tokens."
"Though competitive analysis has been a very useful performance measure for
the quality of online algorithms, it is recognized that it sometimes fails to
distinguish between algorithms of different quality in practice. A number of
alternative measures have been proposed, but, with a few exceptions, these have
generally been applied only to the online problem they were developed in
connection with. Recently, a systematic study of performance measures for
online algorithms was initiated [Boyar, Irani, Larsen: Eleventh International
Algorithms and Data Structures Symposium 2009], first focusing on a simple
server problem. We continue this work by studying a fundamentally different
online problem, online search, and the Reservation Price Policies in
particular. The purpose of this line of work is to learn more about the
applicability of various performance measures in different situations and the
properties that the different measures emphasize. We investigate the following
analysis techniques: Competitive, Relative Worst Order, Bijective, Average,
Relative Interval, Random Order, and Max/Max. In addition to drawing
conclusions on this work, we also investigate the measures' sensitivity to
integral vs. real-valued domains, and as a part of this work, generalize some
of the known performance measures. Finally, we have established the first
optimality proof for Relative Interval Analysis."
"In this paper we describe data structures for orthogonal range reporting in
external memory that support fast update operations. The query costs either
match the query costs of the best previously known data structures or differ by
a small multiplicative factor."
"In this paper, we present a number of network-analysis algorithms in the
external-memory model. We focus on methods for large naturally sparse graphs,
that is, n-vertex graphs that have O(n) edges and are structured so that this
sparsity property holds for any subgraph of such a graph. We give efficient
external-memory algorithms for the following problems for such graphs: -
Finding an approximate d-degeneracy ordering; - Finding a cycle of length
exactly c; - Enumerating all maximal cliques. Such problems are of interest,
for example, in the analysis of social networks, where they are used to study
network cohesion."
"The problem of finding a longest common subsequence of two main sequences
with some constraint that must be a substring of the result (STR-IC-LCS) was
formulated recently. It is a variant of the constrained longest common
subsequence problem. As the known algorithms for the STR-IC-LCS problem are
cubic-time, the presented quadratic-time algorithm is significantly faster."
"We show that, for any c>0, the (1+1) evolutionary algorithm using an
arbitrary mutation rate p_n = c/n finds the optimum of a linear objective
function over bit strings of length n in expected time Theta(n log n).
Previously, this was only known for c at most 1. Since previous work also shows
that universal drift functions cannot exist for c larger than a certain
constant, we instead define drift functions which depend crucially on the
relevant objective functions (and also on c itself). Using these
carefully-constructed drift functions, we prove that the expected optimisation
time is Theta(n log n). By giving an alternative proof of the multiplicative
drift theorem, we also show that our optimisation-time bound holds with high
probability."
"We study the \emph{bounded-delay model} for Qualify-of-Service buffer
management. Time is discrete. There is a buffer. Unit-length jobs (also called
\emph{packets}) arrive at the buffer over time. Each packet has an integer
release time, an integer deadline, and a positive real value. A packet's
characteristics are not known to an online algorithm until the packet actually
arrives. In each time step, at most one packet can be sent out of the buffer.
The objective is to maximize the total value of the packets sent by their
respective deadlines in an online manner. An online algorithm's performance is
usually measured in terms of \emph{competitive ratio}, when this online
algorithm is compared with a clairvoyant algorithm achieving the best total
value. In this paper, we study a simple and intuitive online algorithm. We
analyze its performance in terms of competitive ratio for the general model and
a few important variants."
"Let $\D = $$ \{d_1,d_2,...d_D\}$ be a given set of $D$ string documents of
total length $n$, our task is to index $\D$, such that the $k$ most relevant
documents for an online query pattern $P$ of length $p$ can be retrieved
efficiently. We propose an index of size $|CSA|+n\log D(2+o(1))$ bits and
$O(t_{s}(p)+k\log\log n+poly\log\log n)$ query time for the basic relevance
metric \emph{term-frequency}, where $|CSA|$ is the size (in bits) of a
compressed full text index of $\D$, with $O(t_s(p))$ time for searching a
pattern of length $p$ . We further reduce the space to $|CSA|+n\log D(1+o(1))$
bits, however the query time will be $O(t_s(p)+k(\log \sigma \log\log
n)^{1+\epsilon}+poly\log\log n)$, where $\sigma$ is the alphabet size and
$\epsilon >0$ is any constant."
"In the SCHED problem we are given a set of n jobs, together with their
processing times and precedence constraints. The task is to order the jobs so
that their total completion time is minimized. SCHED is a special case of the
Traveling Repairman Problem with precedences. A natural dynamic programming
algorithm solves both these problems in 2^n n^O(1) time, and whether there
exists an algorithms solving SCHED in O(c^n) time for some constant c < 2 was
an open problem posted in 2004 by Woeginger. In this paper we answer this
question positively."
"One of the fundamental problem in the theory of sorting is to find the
pessimistic number of comparisons sufficient to sort a given number of
elements. Currently 16 is the lowest number of elements for which we do not
know the exact value. We know that 46 comparisons suffices and that 44 do not.
There is an open question if 45 comparisons are sufficient. We present an
attempt to resolve that problem by performing an exhaustive computer search. We
also present an algorithm for counting linear extensions which substantially
speeds up computations."
"In this paper we present an algorithm, called conauto-2.0, that can
efficiently compute a set of generators of the automorphism group of a graph,
and test whether two graphs are isomorphic, finding an isomorphism if they are.
This algorithm uses the basic individualization/refinement technique, and is an
improved version of the algorithm conauto, which has been shown to be very fast
for random graphs and several families of hard graphs. In this paper, it is
proved that, under some circumstances, it is not only possible to prune the
search space (using already found generators of the automorphism group), but
also to infer new generators without the need of explicitly finding an
automorphism of the graph. This result is especially suited for graphs with
regularly connected components, and can be applied in any isomorphism testing
and canonical labeling algorithm (that use the individualization/refinement
technique) to significantly improve its performance. Additionally, a dynamic
target cell selection function is used to adapt to different graphs. The
resulting algorithm preserves all the nice features of conauto, but reduces the
time for testing graphs with regularly connected components and other hard
graph families. We run extensive experiments, which show that the most popular
algorithms (namely, nauty, bliss, Traces, and saucy) are slower than
conauto-2.0, among others, for the graph families based on components."
"The Travelling Salesman Problem is one the most fundamental and most studied
problems in approximation algorithms. For more than 30 years, the best
algorithm known for general metrics has been Christofides's algorithm with
approximation factor of 3/2, even though the so-called Held-Karp LP relaxation
of the problem is conjectured to have the integrality gap of only 4/3. Very
recently, significant progress has been made for the important special case of
graphic metrics, first by Oveis Gharan et al., and then by Momke and Svensson.
In this paper, we provide an improved analysis for the approach introduced by
Momke and Svensson yielding a bound of 13/9 on the approximation factor, as
well as a bound of 19/12+epsilon for any epsilon>0 for a more general
Travelling Salesman Path Problem in graphic metrics."
"We focus on designing combinatorial algorithms for the Capacitated Network
Design problem (Cap-SNDP). The Cap-SNDP is the problem of satisfying
connectivity requirements when edges have costs and hard capacities. We begin
by showing that the Group Steiner tree problem (GST) is a special case of
Cap-SNDP even when there is connectivity requirement between only one
source-sink pair. This implies the first poly-logarithmic lower bound for the
Cap-SNDP. We next provide combinatorial algorithms for several special cases of
this problem. The Cap-SNDP is equivalent to its special case when every edge
has either zero cost or infinite capacity. We consider a special case, called
Connected Cap-SNDP, where all infinite-capacity edges in the solution are
required to form a connected component containing the sinks. This problem is
motivated by its similarity to the Connected Facility Location problem
[G+01,SW04]. We solve this problem by reducing it to Submodular tree cover
problem, which is a common generalization of Connected Cap-SNDP and Group
Steiner tree problem. We generalize the recursive greedy algorithm [CEK]
achieving a poly-logarithmic approximation algorithm for Submodular tree cover
problem. This result is interesting in its own right and gives the first
poly-logarithmic approximation algorithms for Connected hard capacities set
multi-cover and Connected source location.
  We then study another special case of Cap-SNDP called Unbalanced
point-to-point connection problem. Besides its practical applications to shift
design problems [EKS], it generalizes many problems such as k-MST, Steiner
Forest and Point-to-Point Connection. We give a combinatorial logarithmic
approximation algorithm for this problem by reducing it to degree-bounded SNDP."
"k-means has recently been recognized as one of the best algorithms for
clustering unsupervised data. Since k-means depends mainly on distance
calculation between all data points and the centers, the time cost will be high
when the size of the dataset is large (for example more than 500millions of
points). We propose a two stage algorithm to reduce the time cost of distance
calculation for huge datasets. The first stage is a fast distance calculation
using only a small portion of the data to produce the best possible location of
the centers. The second stage is a slow distance calculation in which the
initial centers used are taken from the first stage. The fast and slow stages
represent the speed of the movement of the centers. In the slow stage, the
whole dataset can be used to get the exact location of the centers. The time
cost of the distance calculation for the fast stage is very low due to the
small size of the training data chosen. The time cost of the distance
calculation for the slow stage is also minimized due to small number of
iterations. Different initial locations of the clusters have been used during
the test of the proposed algorithms. For large datasets, experiments show that
the 2-stage clustering method achieves better speed-up (1-9 times)."
"We introduce a new regression problem which we call the Sum-Based
Hierarchical Smoothing problem. Given a directed acyclic graph and a
non-negative value, called target value, for each vertex in the graph, we wish
to find non-negative values for the vertices satisfying a certain constraint
while minimizing the distance of these assigned values and the target values in
the lp-norm. The constraint is that the value assigned to each vertex should be
no less than the sum of the values assigned to its children. We motivate this
problem with applications in information retrieval and web mining. While our
problem can be solved in polynomial time using linear programming, given the
input size in these applications such a solution might be too slow. We mainly
study the \ell_1-norm case restricting the underlying graphs to rooted trees.
For this case we provide an efficient algorithm, running in O(n^2) time. While
the algorithm is purely combinatorial, its proof of correctness is an elegant
use of linear programming duality. We believe that our approach may be
applicable to similar problems, where comparable hierarchical constraints are
involved, e.g. considering the average of the values assigned to the children
of each vertex. While similar in flavor to other smoothing problems like
Isotonic Regression (see for example [Angelov et al. SODA'06]), our problem is
arguably richer and theoretically more challenging."
"We investigate the problem of succinctly representing an arbitrary
permutation, \pi, on {0,...,n-1} so that \pi^k(i) can be computed quickly for
any i and any (positive or negative) integer power k. A representation taking
(1+\epsilon) n lg n + O(1) bits suffices to compute arbitrary powers in
constant time, for any positive constant \epsilon <= 1. A representation taking
the optimal \ceil{\lg n!} + o(n) bits can be used to compute arbitrary powers
in O(lg n / lg lg n) time.
  We then consider the more general problem of succinctly representing an
arbitrary function, f: [n] \rightarrow [n] so that f^k(i) can be computed
quickly for any i and any integer power k. We give a representation that takes
(1+\epsilon) n lg n + O(1) bits, for any positive constant \epsilon <= 1, and
computes arbitrary positive powers in constant time. It can also be used to
compute f^k(i), for any negative integer k, in optimal O(1+|f^k(i)|) time.
  We place emphasis on the redundancy, or the space beyond the
information-theoretic lower bound that the data structure uses in order to
support operations efficiently. A number of lower bounds have recently been
shown on the redundancy of data structures. These lower bounds confirm the
space-time optimality of some of our solutions. Furthermore, the redundancy of
one of our structures ""surpasses"" a recent lower bound by Golynski [Golynski,
SODA 2009], thus demonstrating the limitations of this lower bound."
"We consider the problem of supporting Rank() and Select() operations on a bit
vector of length m with n 1 bits. The problem is considered in the succinct
index model, where the bit vector is stored in ""read-only"" memory and an
additional data structure, called the index, is created during pre-processing
to help answer the above queries. We give asymptotically optimal
density-sensitive trade-offs, involving both m and n, that relate the size of
the index to the number of accesses to the bit vector (and processing time)
needed to answer the above queries. The results are particularly interesting
for the case where n = o(m)."
"We study the problem of optimal traffic prediction and monitoring in
large-scale networks. Our goal is to determine which subset of K links to
monitor in order to ""best"" predict the traffic on the remaining links in the
network. We consider several optimality criteria. This can be formulated as a
combinatorial optimization problem, belonging to the family of subset selection
problems. Similar NP-hard problems arise in statistics, machine learning and
signal processing. Some include subset selection for regression, variable
selection, and sparse approximation. Exact solutions are computationally
prohibitive. We present both new heuristics as well as new efficient algorithms
implementing the classical greedy heuristic - commonly used to tackle such
combinatorial problems. Our approach exploits connections to principal
component analysis (PCA), and yields new types of performance lower bounds
which do not require submodularity of the objective functions. We show that an
ensemble method applied to our new randomized heuristic algorithm, often
outperforms the classical greedy heuristic in practice. We evaluate our
algorithms under several large-scale networks, including real life networks."
"In this paper, we present a polynomial dynamic programming algorithm that
tests whether a $n$-vertex directed tree $T$ has an upward planar embedding
into a convex point-set $S$ of size $n$. Further, we extend our approach to the
class of outerplanar digraphs. This nontrivial and surprising result implies
that any given digraph can be efficiently tested for an upward planar embedding
into a given convex point set."
"We show that randomization can lead to significant improvements for a few
fundamental problems in distributed tracking. Our basis is the {\em
count-tracking} problem, where there are $k$ players, each holding a counter
$n_i$ that gets incremented over time, and the goal is to track an
$\eps$-approximation of their sum $n=\sum_i n_i$ continuously at all times,
using minimum communication. While the deterministic communication complexity
of the problem is $\Theta(k/\eps \cdot \log N)$, where $N$ is the final value
of $n$ when the tracking finishes, we show that with randomization, the
communication cost can be reduced to $\Theta(\sqrt{k}/\eps \cdot \log N)$. Our
algorithm is simple and uses only O(1) space at each player, while the lower
bound holds even assuming each player has infinite computing power. Then, we
extend our techniques to two related distributed tracking problems: {\em
frequency-tracking} and {\em rank-tracking}, and obtain similar improvements
over previous deterministic algorithms. Both problems are of central importance
in large data monitoring and analysis, and have been extensively studied in the
literature."
"The topic of this paper is the presentation of a new network model designed
for networks consisting of spatial objects. This model allows the development
of more advance representations of systems of networked objects and the study
of geographical phenomena propagated through networks. The capabilities of the
model in simulation of geographical phenomena propagation are also studied and
relevant algorithms are presented. As examples of use, modeling of water supply
network and the simulation of traffic flow in road networks are presented."
"We revisit various string indexing problems with range reporting features,
namely, position-restricted substring searching, indexing substrings with gaps,
and indexing substrings with intervals. We obtain the following main results.
{itemize} We give efficient reductions for each of the above problems to a new
problem, which we call \emph{substring range reporting}. Hence, we unify the
previous work by showing that we may restrict our attention to a single problem
rather than studying each of the above problems individually. We show how to
solve substring range reporting with optimal query time and little space.
Combined with our reductions this leads to significantly improved time-space
trade-offs for the above problems. In particular, for each problem we obtain
the first solutions with optimal time query and $O(n\log^{O(1)} n)$ space,
where $n$ is the length of the indexed string. We show that our techniques for
substring range reporting generalize to \emph{substring range counting} and
\emph{substring range emptiness} variants. We also obtain non-trivial
time-space trade-offs for these problems. {itemize} Our bounds for substring
range reporting are based on a novel combination of suffix trees and range
reporting data structures. The reductions are simple and general and may apply
to other combinations of string indexing with range reporting."
"In this paper, we propose the first exact algorithm for minimizing the
difference of two submodular functions (D.S.), i.e., the discrete version of
the D.C. programming problem. The developed algorithm is a
branch-and-bound-based algorithm which responds to the structure of this
problem through the relationship between submodularity and convexity. The D.S.
programming problem covers a broad range of applications in machine learning
because this generalizes the optimization of a wide class of set functions. We
empirically investigate the performance of our algorithm, and illustrate the
difference between exact and approximate solutions respectively obtained by the
proposed and existing algorithms in feature selection and discriminative
structure learning."
"Previous compact representations of permutations have focused on adding a
small index on top of the plain data $<\pi(1), \pi(2),...\pi(n)>$, in order to
efficiently support the application of the inverse or the iterated permutation.
  In this paper we initiate the study of techniques that exploit the
compressibility of the data itself, while retaining efficient computation of
$\pi(i)$ and its inverse.
  In particular, we focus on exploiting {\em runs}, which are subsets
(contiguous or not) of the domain where the permutation is monotonic.
  Several variants of those types of runs arise in real applications such as
inverted indexes and suffix arrays.
  Furthermore, our improved results on compressed data structures for
permutations also yield better adaptive sorting algorithms."
"We consider the capacitated domination problem, which models a
service-requirement assigning scenario and which is also a generalization of
the dominating set problem. In this problem, we are given a graph with three
parameters defined on the vertex set, which are cost, capacity, and demand. The
objective of this problem is to compute a demand assignment of least cost, such
that the demand of each vertex is fully-assigned to some of its closed
neighbours without exceeding the amount of capacity they provide.
  In this paper, we provide the first constant factor approximation for this
problem on planar graphs, based on a new perspective on the hierarchical
structure of outer-planar graphs. We believe that this new perspective and
technique can be applied to other capacitated covering problems to help tackle
vertices of large degrees."
"We consider the problem of maximizing a monotone submodular function in a
$k$-exchange system. These systems, introduced by Feldman et al., generalize
the matroid k-parity problem in a wide class of matroids and capture many other
combinatorial optimization problems. Feldman et al. show that a simple
non-oblivious local search algorithm attains a $(k + 1)/2$ approximation ratio
for the problem of linear maximization in a $k$-exchange system. Here, we
extend this approach to the case of monotone submodular objective functions. We
give a deterministic, non-oblivious local search algorithm that attains an
approximation ratio of $(k + 3)/2$ for the problem of maximizing a monotone
submodular function in a $k$-exchange system."
"We studied the Fault-Tolerant Facility Placement problem (FTFP) which
generalizes the uncapacitated facility location problem (UFL). In FTFP, we are
given a set F of sites at which facilities can be built, and a set C of clients
with some demands that need to be satisfied by different facilities. A client
$j$ has demand $r_j$. Building one facility at a site $i$ incurs a cost $f_i$,
and connecting one unit of demand from client $j$ to a facility at site
$i\in\fac$ costs $d_{ij}$. $d_{ij}$'s are assumed to form a metric. A feasible
solution specifies the number of facilities to be built at each site and the
way to connect demands from clients to facilities, with the restriction that
demands from the same client must go to different facilities. Facilities at the
same site are considered different. The goal is to find a solution with minimum
total cost. We gave a 1.7245-approximation algorithm to the FTFP problem. Our
technique is via a reduction to the Fault-Tolerant Facility Location problem,
in which each client has demand $r_j$ but each site can have at most one
facility built."
"We present a framework for computing with input data specified by intervals,
representing uncertainty in the values of the input parameters. To compute a
solution, the algorithm can query the input parameters that yield more refined
estimates in form of sub-intervals and the objective is to minimize the number
of queries. The previous approaches address the scenario where every query
returns an exact value. Our framework is more general as it can deal with a
wider variety of inputs and query responses and we establish interesting
relationships between them that have not been investigated previously. Although
some of the approaches of the previous restricted models can be adapted to the
more general model, we require more sophisticated techniques for the analysis
and we also obtain improved algorithms for the previous model.
  We address selection problems in the generalized model and show that there
exist 2-update competitive algorithms that do not depend on the lengths or
distribution of the sub-intervals and hold against the worst case adversary. We
also obtain similar bounds on the competitive ratio for the MST problem in
graphs."
"A binary matrix has the consecutive ones property (C1P) if it is possible to
order the columns so that all 1s are consecutive in every row. In [McConnell,
SODA 2004 768-777] the notion of incompatibility graph of a binary matrix was
introduced and it was shown that odd cycles of this graph provide a certificate
that a matrix does not have the consecutive ones property. A bound of (k+2) was
claimed for the smallest odd cycle of a non-C1P matrix with k columns. In this
note we show that this result can be obtained simply and directly via Tucker
patterns, and that the correct bound is (k+2) when k is even, but (k+3) when k
is odd."
"In order to evaluate, compare, and tune graph algorithms, experiments on well
designed benchmark sets have to be performed. Together with the goal of
reproducibility of experimental results, this creates a demand for a public
archive to gather and store graph instances. Such an archive would ideally
allow annotation of instances or sets of graphs with additional information
like graph properties and references to the respective experiments and results.
Here we examine the requirements, and introduce a new community project with
the aim of producing an easily accessible library of graphs. Through successful
community involvement, it is expected that the archive will contain a
representative selection of both real-world and generated graph instances,
covering significant application areas as well as interesting classes of
graphs."
"We consider a class of pattern matching problems where a normalising
transformation is applied at every alignment. Normalised pattern matching plays
a key role in fields as diverse as image processing and musical information
processing where application specific transformations are often applied to the
input. By considering the class of polynomial transformations of the input, we
provide fast algorithms and the first lower bounds for both new and old
problems. Given a pattern of length m and a longer text of length n where both
are assumed to contain integer values only, we first show O(n log m) time
algorithms for pattern matching under linear transformations even when wildcard
symbols can occur in the input. We then show how to extend the technique to
polynomial transformations of arbitrary degree. Next we consider the problem of
finding the minimum Hamming distance under polynomial transformation. We show
that, for any epsilon>0, there cannot exist an O(n m^(1-epsilon)) time
algorithm for additive and linear transformations conditional on the hardness
of the classic 3SUM problem. Finally, we consider a version of the Hamming
distance problem under additive transformations with a bound k on the maximum
distance that need be reported. We give a deterministic O(nk log k) time
solution which we then improve by careful use of randomisation to O(n sqrt(k
log k) log n) time for sufficiently small k. Our randomised solution outputs
the correct answer at every position with high probability."
"List Accessing Problem is a well studied research problem in the context of
linear search. Input to the list accessing problem is an unsorted linear list
of distinct elements along with a sequence of requests, where each request is
an access operation on an element of the list. A list accessing algorithm
reorganizes the list while processing a request sequence on the list in order
to minimize the access cost. Move-To-Front algorithm has been proved to be the
best performing list accessing online algorithm till date in the literature.
Characterization of the input request sequences corresponding to practical real
life situations is a big challenge for the list accessing problem. As far as
our knowledge is concerned, no characterization for the request sequences has
been done in the literature till date for the list accessing problem. In this
paper, we have characterized the request sequences for the list accessing
problem based on several factors such as size of the list, size of the request
sequence, ordering of elements and frequency of occurrence of elements in the
request sequence. We have made a comprehensive study of MTF list accessing
algorithm and obtained new theoretical results for our characterized special
class of request sequences. Our characterization will open up a new direction
of research for empirical analysis of list accessing algorithms for real life
inputs."
"There are many existing well known cost models for the list accessing
problem. The standard cost model developed by Sleator and Tarjan is most widely
used. In this paper, we have made a comprehensive study of the existing cost
models and proposed a new cost model for the list accessing problem. In our
proposed cost model, for calculating the processing cost of request sequence
using a singly linked list, we consider the access cost, matching cost and
replacement cost. The cost of processing a request sequence is the sum of
access cost, matching cost and replacement cost. We have proposed a novel
method for processing the request sequence which does not consider the
rearrangement of the list and uses the concept of buffering, matching, look
ahead and flag bit."
"Modern graphics processors provide exceptional computa- tional power, but
only for certain computational models. While they have revolutionized
computation in many fields, compression has been largely unnaffected. This
paper aims to explain the current issues and possibili- ties in GPGPU
compression. This is done by a high level overview of the GPGPU computational
model in the context of compression algorithms; along with a more in-depth
analysis of how one would implement bzip2 on a GPGPU architecture."
"The Integer Programming Problem (IP) for a polytope P \subseteq R^n is to
find an integer point in P or decide that P is integer free. We give an
algorithm for an approximate version of this problem, which correctly decides
whether P contains an integer point or whether a (1+\eps) scaling of P around
its barycenter is integer free in time O(1/\eps^2)^n. We reduce this
approximate IP question to an approximate Closest Vector Problem (CVP) in a
""near-symmetric"" semi-norm, which we solve via a sieving technique first
developed by Ajtai, Kumar, and Sivakumar (STOC 2001). Our main technical
contribution is an extension of the AKS sieving technique which works for any
near-symmetric semi-norm. Our results also extend to general convex bodies and
lattices."
"Distance oracles are data structures that provide fast (possibly approximate)
answers to shortest-path and distance queries in graphs. The tradeoff between
the space requirements and the query time of distance oracles is of particular
interest and the main focus of this paper.
  In FOCS'01, Thorup introduced approximate distance oracles for planar graphs.
He proved that, for any eps>0 and for any planar graph on n nodes, there exists
a (1+eps)-approximate distance oracle using space O(n eps^{-1} log n) such that
approximate distance queries can be answered in time O(1/eps).
  Ten years later, we give the first improvements on the space-querytime
tradeoff for planar graphs.
  * We give the first oracle having a space-time product with subquadratic
dependency on 1/eps. For space ~O(n log n) we obtain query time ~O(1/eps)
(assuming polynomial edge weights). The space shows a doubly logarithmic
dependency on 1/eps only. We believe that the dependency on eps may be almost
optimal.
  * For the case of moderate edge weights (average bounded by polylog(n), which
appears to be the case for many real-world road networks), we hit a ""sweet
spot,"" improving upon Thorup's oracle both in terms of eps and n. Our oracle
uses space ~O(n log log n) and it has query time ~O(log log log n + 1/eps).
  (Asymptotic notation in this abstract hides low-degree polynomials in
log(1/eps) and log*(n).)"
"We consider the \emph{two-dimensional range maximum query (2D-RMQ)} problem:
given an array $A$ of ordered values, to pre-process it so that we can find the
position of the smallest element in the sub-matrix defined by a
(user-specified) range of rows and range of columns. We focus on determining
the \emph{effective} entropy of 2D-RMQ, i.e., how many bits are needed to
encode $A$ so that 2D-RMQ queries can be answered \emph{without} access to $A$.
We give tight upper and lower bounds on the expected effective entropy for the
case when $A$ contains independent identically-distributed random values, and
new upper and lower bounds for arbitrary $A$, for the case when $A$ contains
few rows. The latter results improve upon previous upper and lower bounds by
Brodal et al. (ESA 2010). In some cases we also give data structures whose
space usage is close to the effective entropy and answer 2D-RMQ queries
rapidly."
"Motivated by the imminent growth of massive, highly redundant genomic
databases, we study the problem of compressing a string database while
simultaneously supporting fast random access, substring extraction and pattern
matching to the underlying string(s). Bille et al. (2011) recently showed how,
given a straight-line program with $r$ rules for a string $s$ of length $n$, we
can build an $\Oh{r}$-word data structure that allows us to extract any
substring of length $m$ in $\Oh{\log n + m}$ time. They also showed how, given
a pattern $p$ of length $m$ and an edit distance (k \leq m), their data
structure supports finding all \occ approximate matches to $p$ in $s$ in $\Oh{r
(\min (m k, k^4 + m) + \log n) + \occ}$ time. Rytter (2003) and Charikar et al.
(2005) showed that $r$ is always at least the number $z$ of phrases in the LZ77
parse of $s$, and gave algorithms for building straight-line programs with
$\Oh{z \log n}$ rules. In this paper we give a simple $\Oh{z \log n}$-word data
structure that takes the same time for substring extraction but only $\Oh{z
\min (m k, k^4 + m) + \occ}$ time for approximate pattern matching."
"Consider an undirected weighted graph G=(V,E) with |V|=n and |E|=m, where
each vertex v is assigned a label from a set L of \ell labels. We show how to
construct a compact distance oracle that can answer queries of the form: ""what
is the distance from v to the closest lambda-labeled node"" for a given node v
in V and label lambda in L.
  This problem was introduced by Hermelin, Levy, Weimann and Yuster [ICALP
2011] where they present several results for this problem. In the first result,
they show how to construct a vertex-label distance oracle of expected size
O(kn^{1+1/k}) with stretch (4k - 5) and query time O(k). In a second result,
they show how to reduce the size of the data structure to O(kn \ell^{1/k}) at
the expense of a huge stretch, the stretch of this construction grows
exponentially in k, (2^k-1). In the third result they present a dynamic
vertex-label distance oracle that is capable of handling label changes in a
sub-linear time. The stretch of this construction is also exponential in k, (2
3^{k-1}+1).
  We manage to significantly improve the stretch of their constructions,
reducing the dependence on k from exponential to polynomial (4k-5), without
requiring any tradeoff regarding any of the other variables.
  In addition, we introduce the notion of vertex-label spanners: subgraphs that
preserve distances between every node v and label lambda. We present an
efficient construction for vertex-label spanners with stretch-size tradeoff
close to optimal."
"We present a packing-based approximation algorithm for the $k$-Set Cover
problem. We introduce a new local search-based $k$-set packing heuristic, and
call it Restricted $k$-Set Packing. We analyze its tight approximation ratio
via a complicated combinatorial argument. Equipped with the Restricted $k$-Set
Packing algorithm, our $k$-Set Cover algorithm is composed of the $k$-Set
Packing heuristic \cite{schrijver} for $k\geq 7$, Restricted $k$-Set Packing
for $k=6,5,4$ and the semi-local $(2,1)$-improvement \cite{furer} for 3-Set
Cover. We show that our algorithm obtains a tight approximation ratio of
$H_k-0.6402+\Theta(\frac{1}{k})$, where $H_k$ is the $k$-th harmonic number.
For small $k$, our results are 1.8667 for $k=6$, 1.7333 for $k=5$ and 1.5208
for $k=4$. Our algorithm improves the currently best approximation ratio for
the $k$-Set Cover problem of any $k\geq 4$."
"We consider the Generalized Bin Covering (GBC) problem: We are given $m$ bin
types, where each bin of type $i$ has profit $p_i$ and demand $d_i$.
Furthermore, there are $n$ items, where item $j$ has size $s_j$. A bin of type
$i$ is covered if the set of items assigned to it has total size at least the
demand $d_i$. In that case, the profit of $p_i$ is earned and the objective is
to maximize the total profit. To the best of our knowledge, only the cases $p_i
= d_i = 1$ (Bin Covering) and $p_i = d_i$ (Variable-Sized Bin Covering (VSBC))
have been treated before. We study two models of bin supply: In the unit supply
model, we have exactly one bin of each type, i.\,e., we have individual bins.
By contrast, in the infinite supply model, we have arbitrarily many bins of
each type. Clearly, the unit supply model is a generalization of the infinite
supply model. To the best of our knowledge the unit supply model has not been
studied yet.
  Our results for the unit supply model hold not only asymptotically, but for
all instances. This contrasts most of the previous work on \prob{Bin Covering}.
We prove that there is a combinatorial 5-approximation algorithm for GBC with
unit supply, which has running time $\bigO{nm\sqrt{m+n}}$. Furthermore, for
VSBC we show that the natural and fast Next Fit Decreasing ($\NFD$) algorithm
is a 9/4-approximation in the unit supply model. The bound is tight for the
algorithm and close to being best-possible. We show that there is an AFPTAS for
VSBC in the \emph{infinite} supply model."
"We show that there exists a linear-time algorithm that computes the strong
chromatic index of Halin graphs."
"To store and search genomic databases efficiently, researchers have recently
started building compressed self-indexes based on grammars. In this paper we
show how, given a straight-line program with $r$ rules for a string (S [1..n])
whose LZ77 parse consists of $z$ phrases, we can store a self-index for $S$ in
$\Oh{r + z \log \log n}$ space such that, given a pattern (P [1..m]), we can
list the $\occ$ occurrences of $P$ in $S$ in $\Oh{m^2 + \occ \log \log n}$
time. If the straight-line program is balanced and we accept a small
probability of building a faulty index, then we can reduce the $\Oh{m^2}$ term
to $\Oh{m \log m}$. All previous self-indexes are larger or slower in the worst
case."
"We consider a natural generalization of the classical pattern matching
problem: given compressed representations of a pattern p[1..M] and a text
t[1..N] of sizes m and n, respectively, does p occur in t? We develop an
optimal linear time solution for the case when both p and t are compressed
using the LZW method. This improves the previously known O((n+m)log(n+m)) time
solution of Gasieniec and Rytter, and essentially closes the line of research
devoted to studying LZW-compressed exact pattern matching."
"We revisit the range minimum query problem and present a new O(n)-space data
structure that supports queries in O(1) time. Although previous data structures
exist whose asymptotic bounds match ours, our goal is to introduce a new
solution that is simple, intuitive, and practical without increasing costs for
query time or space."
"We study the problem of parameterized matching in a stream where we want to
output matches between a pattern of length m and the last m symbols of the
stream before the next symbol arrives. Parameterized matching is a natural
generalisation of exact matching where an arbitrary one-to-one relabelling of
pattern symbols is allowed. We show how this problem can be solved in constant
time per arriving stream symbol and sublinear, near optimal space with high
probability. Our results are surprising and important: it has been shown that
almost no streaming pattern matching problems can be solved (not even
randomised) in less than Theta(m) space, with exact matching as the only known
problem to have a sublinear, near optimal space solution. Here we demonstrate
that a similar sublinear, near optimal space solution is achievable for an even
more challenging problem. The proof is considerably more complex than that for
exact matching."
"In Online Sum-Radii Clustering, n demand points arrive online and must be
irrevocably assigned to a cluster upon arrival. The cost of each cluster is the
sum of a fixed opening cost and its radius, and the objective is to minimize
the total cost of the clusters opened by the algorithm. We show that the
deterministic competitive ratio of Online Sum-Radii Clustering for general
metric spaces is \Theta(\log n), where the upper bound follows from a
primal-dual algorithm and holds for general metric spaces, and the lower bound
is valid for ternary Hierarchically Well-Separated Trees (HSTs) and for the
Euclidean plane. Combined with the results of (Csirik et al., MFCS 2010), this
result demonstrates that the deterministic competitive ratio of Online
Sum-Radii Clustering changes abruptly, from constant to logarithmic, when we
move from the line to the plane. We also show that Online Sum-Radii Clustering
in metric spaces induced by HSTs is closely related to the Parking Permit
problem introduced by (Meyerson, FOCS 2005). Exploiting the relation to Parking
Permit, we obtain a lower bound of \Omega(\log\log n) on the randomized
competitive ratio of Online Sum-Radii Clustering in tree metrics. Moreover, we
present a simple randomized O(\log n)-competitive algorithm, and a
deterministic O(\log\log n)-competitive algorithm for the fractional version of
the problem."
"We show how to compute the edit distance between two strings of length n up
to a factor of 2^{\~O(sqrt(log n))} in n^(1+o(1)) time. This is the first
sub-polynomial approximation algorithm for this problem that runs in
near-linear time, improving on the state-of-the-art n^(1/3+o(1)) approximation.
Previously, approximation of 2^{\~O(sqrt(log n))} was known only for embedding
edit distance into l_1, and it is not known if that embedding can be computed
in less than quadratic time."
"We reinterpret some online greedy algorithms for a class of nonlinear
""load-balancing"" problems as solving a mathematical program online. For
example, we consider the problem of assigning jobs to (unrelated) machines to
minimize the sum of the alpha^{th}-powers of the loads plus assignment costs
(the online Generalized Assignment Problem); or choosing paths to connect
terminal pairs to minimize the alpha^{th}-powers of the edge loads (online
routing with speed-scalable routers). We give analyses of these online
algorithms using the dual of the primal program as a lower bound for the
optimal algorithm, much in the spirit of online primal-dual results for linear
problems.
  We then observe that a wide class of uni-processor speed scaling problems
(with essentially arbitrary scheduling objectives) can be viewed as such load
balancing problems with linear assignment costs. This connection gives new
algorithms for problems that had resisted solutions using the dominant
potential function approaches used in the speed scaling literature, as well as
alternate, cleaner proofs for other known results."
"Recently Rubinfeld et al. (ICS 2011, pp. 223--238) proposed a new model of
sublinear algorithms called \emph{local computation algorithms}. In this model,
a computation problem $F$ may have more than one legal solution and each of
them consists of many bits. The local computation algorithm for $F$ should
answer in an online fashion, for any index $i$, the $i^{\mathrm{th}}$ bit of
some legal solution of $F$. Further, all the answers given by the algorithm
should be consistent with at least one solution of $F$.
  In this work, we continue the study of local computation algorithms. In
particular, we develop a technique which under certain conditions can be
applied to construct local computation algorithms that run not only in
polylogarithmic time but also in polylogarithmic \emph{space}. Moreover, these
local computation algorithms are easily parallelizable and can answer all
parallel queries consistently. Our main technical tools are pseudorandom
numbers with bounded independence and the theory of branching processes."
"The eviction problem for memory hierarchies is studied for the Hidden Markov
Reference Model (HMRM) of the memory trace, showing how miss minimization can
be naturally formulated in the optimal control setting. In addition to the
traditional version assuming a buffer of fixed capacity, a relaxed version is
also considered, in which buffer occupancy can vary and its average is
constrained. Resorting to multiobjective optimization, viewing occupancy as a
cost rather than as a constraint, the optimal eviction policy is obtained by
composing solutions for the individual addressable items.
  This approach is then specialized to the Least Recently Used Stack Model
(LRUSM), a type of HMRM often considered for traces, which includes V-1
parameters, where V is the size of the virtual space. A gain optimal policy for
any target average occupancy is obtained which (i) is computable in time O(V)
from the model parameters, (ii) is optimal also for the fixed capacity case,
and (iii) is characterized in terms of priorities, with the name of Least
Profit Rate (LPR) policy. An O(log C) upper bound (being C the buffer capacity)
is derived for the ratio between the expected miss rate of LPR and that of OPT,
the optimal off-line policy; the upper bound is tightened to O(1), under
reasonable constraints on the LRUSM parameters. Using the stack-distance
framework, an algorithm is developed to compute the number of misses incurred
by LPR on a given input trace, simultaneously for all buffer capacities, in
time O(log V) per access.
  Finally, some results are provided for miss minimization over a finite
horizon and over an infinite horizon under bias optimality, a criterion more
stringent than gain optimality."
A linear time algorithm to find a set of nearest elements in a mesh.
"We show that there exist linear-time algorithms that compute the strong
chromatic index of Halin graphs, of maximal outerplanar graphs and of
distance-hereditary graphs."
"A 2.75-approximation algorithm is proposed for the unconstrained traveling
tournament problem, which is a variant of the traveling tournament problem. For
the unconstrained traveling tournament problem, this is the first proposal of
an approximation algorithm with a constant approximation ratio. In addition,
the proposed algorithm yields a solution that meets both the no-repeater and
mirrored constraints. Computational experiments show that the algorithm
generates solutions of good quality."
"In the query-commit problem we are given a graph where edges have distinct
probabilities of existing. It is possible to query the edges of the graph, and
if the queried edge exists then its endpoints are irrevocably matched. The goal
is to find a querying strategy which maximizes the expected size of the
matching obtained. This stochastic matching setup is motivated by applications
in kidney exchanges and online dating.
  In this paper we address the query-commit problem from both theoretical and
experimental perspectives. First, we show that a simple class of edges can be
queried without compromising the optimality of the strategy. This property is
then used to obtain in polynomial time an optimal querying strategy when the
input graph is sparse. Next we turn our attentions to the kidney exchange
application, focusing on instances modeled over real data from existing
exchange programs. We prove that, as the number of nodes grows, almost every
instance admits a strategy which matches almost all nodes. This result supports
the intuition that more exchanges are possible on a larger pool of
patient/donors and gives theoretical justification for unifying the existing
exchange programs. Finally, we evaluate experimentally different querying
strategies over kidney exchange instances. We show that even very simple
heuristics perform fairly well, being within 1.5% of an optimal clairvoyant
strategy, that knows in advance the edges in the graph. In such a
time-sensitive application, this result motivates the use of committing
strategies."
"This work is concerned with approximating constraint satisfaction problems
(CSPs) with an additional global cardinality constraints. For example, \maxcut
is a boolean CSP where the input is a graph $G = (V,E)$ and the goal is to find
a cut $S \cup \bar S = V$ that maximizes the numberof crossing edges,
$|E(S,\bar S)|$. The \maxbisection problem is a variant of \maxcut with an
additional global constraint that each side of the cut has exactly half the
vertices, i.e., $|S| = |V|/2$. Several other natural optimization problems like
\minbisection and approximating Graph Expansion can be formulated as CSPs with
global constraints.
  In this work, we formulate a general approach towards approximating CSPs with
global constraints using SDP hierarchies. To demonstrate the approach we
present the following results:
  Using the Lasserre hierarchy, we present an algorithm that runs in time
$O(n^{poly(1/\epsilon)})$ that given an instance of \maxbisection with value
$1-\epsilon$, finds a bisection with value $1-O(\sqrt{\epsilon})$. This
approximation is near-optimal (up to constant factors in $O()$) under the
Unique Games Conjecture.
  By a computer-assisted proof, we show that the same algorithm also achieves a
0.85-approximation for \maxbisection, improving on the previous bound of 0.70
(note that it is \uniquegames hard to approximate better than a 0.878 factor).
The same algorithm also yields a 0.92-approximation for \maxtwosat with
cardinality constraints.
  For every CSP with a global cardinality constraints, we present a generic
conversion from integrality gap instances for the Lasserre hierarchy to a {\it
dictatorship test} whose soundness is at most integrality gap. Dictatorship
testing gadgets are central to hardness results for CSPs, and a generic
conversion of the above nature lies at the core of the tight Unique Games based
hardness result for CSPs. \cite{Raghavendra08}"
"We give a nearly optimal sublinear-time algorithm for approximating the size
of a minimum vertex cover in a graph G. The algorithm may query the degree
deg(v) of any vertex v of its choice, and for each 1 <= i <= deg(v), it may ask
for the i-th neighbor of v. Letting VC_opt(G) denote the minimum size of vertex
cover in G, the algorithm outputs, with high constant success probability, an
estimate VC_estimate(G) such that VC_opt(G) <= VC_estimate(G) <= 2 * VC_opt(G)
+ epsilon*n, where epsilon is a given additive approximation parameter. We
refer to such an estimate as a (2,epsilon)-estimate. The query complexity and
running time of the algorithm are ~O(avg_deg * poly(1/epsilon)), where avg_deg
denotes the average vertex degree in the graph. The best previously known
sublinear algorithm, of Yoshida et al. (STOC 2009), has query complexity and
running time O(d^4/epsilon^2), where d is the maximum degree in the graph.
Given the lower bound of Omega(avg_deg) (for constant epsilon) for obtaining
such an estimate (with any constant multiplicative factor) due to Parnas and
Ron (TCS 2007), our result is nearly optimal.
  In the case that the graph is dense, that is, the number of edges is
Theta(n^2), we consider another model, in which the algorithm may ask, for any
pair of vertices u and v, whether there is an edge between u and v. We show how
to adapt the algorithm that uses neighbor queries to this model and obtain an
algorithm that outputs a (2,epsilon)-estimate of the size of a minimum vertex
cover whose query complexity and running time are ~O(n) * poly(1/epsilon)."
"We consider an online preemptive scheduling problem where jobs with deadlines
arrive sporadically. A commitment requirement is imposed such that the
scheduler has to either accept or decline a job immediately upon arrival. The
scheduler's decision to accept an arriving job constitutes a contract with the
customer; if the accepted job is not completed by its deadline as promised, the
scheduler loses the value of the corresponding job and has to pay an additional
penalty depending on the amount of unfinished workload. The objective of the
online scheduler is to maximize the overall profit, i.e., the total value of
the admitted jobs completed before their deadlines less the penalty paid for
the admitted jobs that miss their deadlines. We show that the maximum
competitive ratio is $3-2\sqrt{2}$ and propose a simple online algorithm to
achieve this competitive ratio. The optimal scheduling includes a threshold
admission and a greedy scheduling policies. The proposed algorithm has direct
applications to the charging of plug-in hybrid electrical vehicles (PHEV) at
garages or parking lots."
"In a software watermarking environment, several graph theoretic watermark
methods use numbers as watermark values, where some of these methods encode the
watermark numbers as graph structures. In this paper we extended the class of
error correcting graphs by proposing an efficient and easily implemented codec
system for encoding watermark numbers as reducible permutation flow-graphs.
More precisely, we first present an efficient algorithm which encodes a
watermark number $w$ as self-inverting permutation $\pi^*$ and, then, an
algorithm which encodes the self-inverting permutation $\pi^*$ as a reducible
permutation flow-graph $F[\pi^*]$ by exploiting domination relations on the
elements of $\pi^*$ and using an efficient DAG representation of $\pi^*$. The
whole encoding process takes O(n) time and space, where $n$ is the binary size
of the number $w$ or, equivalently, the number of elements of the permutation
$\pi^*$. We also propose efficient decoding algorithms which extract the number
$w$ from the reducible permutation flow-graph $F[\pi^*]$ within the same time
and space complexity. The two main components of our proposed codec system,
i.e., the self-inverting permutation $\pi^*$ and the reducible permutation
graph $F[\pi^*]$, incorporate important structural properties which make our
system resilient to attacks."
"We give an $O(n \log^3 n)$ approximation scheme for Steiner forest in planar
graphs, improving on the previous approximation scheme for this problem, which
runs in $O(n^{f(\epsilon)})$ time."
"We give the first polylogarithmic-competitive randomized online algorithm for
the $k$-server problem on an arbitrary finite metric space. In particular, our
algorithm achieves a competitive ratio of O(log^3 n log^2 k log log n) for any
metric space on n points. Our algorithm improves upon the deterministic
(2k-1)-competitive algorithm of Koutsoupias and Papadimitriou [J.ACM'95]
whenever n is sub-exponential in k."
"We show that there exist linear-time algorithms that compute the strong
chromatic index and a maximum induced matching of tree-cographs when the
decomposition tree is a part of the input. We also show that there exists an
efficient algorithm for the strong chromatic index of permutation graphs."
"We study the Minimum Latency Submodular Cover problem (MLSC), which consists
of a metric $(V,d)$ with source $r\in V$ and $m$ monotone submodular functions
$f_1, f_2, ..., f_m: 2^V \rightarrow [0,1]$. The goal is to find a path
originating at $r$ that minimizes the total cover time of all functions. This
generalizes well-studied problems, such as Submodular Ranking [AzarG11] and
Group Steiner Tree [GKR00]. We give a polynomial time $O(\log \frac{1}{\eps}
\cdot \log^{2+\delta} |V|)$-approximation algorithm for MLSC, where
$\epsilon>0$ is the smallest non-zero marginal increase of any
$\{f_i\}_{i=1}^m$ and $\delta>0$ is any constant.
  We also consider the Latency Covering Steiner Tree problem (LCST), which is
the special case of \mlsc where the $f_i$s are multi-coverage functions. This
is a common generalization of the Latency Group Steiner Tree
[GuptaNR10a,ChakrabartyS11] and Generalized Min-sum Set Cover [AzarGY09,
BansalGK10] problems. We obtain an $O(\log^2|V|)$-approximation algorithm for
LCST.
  Finally we study a natural stochastic extension of the Submodular Ranking
problem, and obtain an adaptive algorithm with an $O(\log 1/ \eps)$
approximation ratio, which is best possible. This result also generalizes some
previously studied stochastic optimization problems, such as Stochastic Set
Cover [GoemansV06] and Shared Filter Evaluation [MunagalaSW07, LiuPRY08]."
"We consider string matching with variable length gaps. Given a string $T$ and
a pattern $P$ consisting of strings separated by variable length gaps
(arbitrary strings of length in a specified range), the problem is to find all
ending positions of substrings in $T$ that match $P$. This problem is a basic
primitive in computational biology applications. Let $m$ and $n$ be the lengths
of $P$ and $T$, respectively, and let $k$ be the number of strings in $P$. We
present a new algorithm achieving time $O(n\log k + m +\alpha)$ and space $O(m
+ A)$, where $A$ is the sum of the lower bounds of the lengths of the gaps in
$P$ and $\alpha$ is the total number of occurrences of the strings in $P$
within $T$. Compared to the previous results this bound essentially achieves
the best known time and space complexities simultaneously. Consequently, our
algorithm obtains the best known bounds for almost all combinations of $m$,
$n$, $k$, $A$, and $\alpha$. Our algorithm is surprisingly simple and
straightforward to implement. We also present algorithms for finding and
encoding the positions of all strings in $P$ for every match of the pattern."
"We consider the problem of distinguishing between two arbitrary black-box
distributions defined over the domain [n], given access to $s$ samples from
both. It is known that in the worst case O(n^{2/3}) samples is both necessary
and sufficient, provided that the distributions have L1 difference of at least
{\epsilon}. However, it is also known that in many cases fewer samples suffice.
We identify a new parameter, that provides an upper bound on how many samples
needed, and present an efficient algorithm that requires the number of samples
independent of the domain size. Also for a large subclass of distributions we
provide a lower bound, that matches our upper bound up to a poly-logarithmic
factor."
"Consider an input text string T[1,N] drawn from an unbounded alphabet. We
study partial computation in suffix-based problems for Data Compression and
Text Indexing such as
  (I) retrieve any segment of K<=N consecutive symbols from the Burrows-Wheeler
transform of T, and
  (II) retrieve any chunk of K<=N consecutive entries of the Suffix Array or
the Suffix Tree.
  Prior literature would take O(N log N) comparisons (and time) to solve these
problems by solving the total problem of building the entire Burrows-Wheeler
transform or Text Index for T, and performing a post-processing to single out
the wanted portion.
  We introduce a novel adaptive approach to partial computational problems
above, and solve both the partial problems in O(K log K + N) comparisons and
time, improving the best known running times of O(N log N) for K=o(N).
  These partial-computation problems are intimately related since they share a
common bottleneck: the suffix multi-selection problem, which is to output the
suffixes of rank r_1,r_2,...,r_K under the lexicographic order, where
r_1<r_2<...<r_K, r_i in [1,N]. Special cases of this problem are well known:
K=N is the suffix sorting problem that is the workhorse in Stringology with
hundreds of applications, and K=1 is the recently studied suffix selection.
  We show that suffix multi-selection can be solved in Theta(N log N -
sum_{j=0}^K Delta_j log Delta_j+N) time and comparisons, where r_0=0,
r_{K+1}=N+1, and Delta_j=r_{j+1}-r_j for 0<=j<=K. This is asymptotically
optimal, and also matches the bound in [Dobkin, Munro, JACM 28(3)] for
multi-selection on atomic elements (not suffixes). Matching the bound known for
atomic elements for strings is a long running theme and challenge from 70's,
which we achieve for the suffix multi-selection problem. The partial suffix
problems as well as the suffix multi-selection problem have many applications."
"The goal of (stable) sparse recovery is to recover a $k$-sparse approximation
$x*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal is
to recover $x*$ such that ||x-x*||_p <= C min_{k-sparse x'} ||x-x'||_q for some
constant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or
$p=q=2$, this task can be accomplished using $m=O(k \log (n/k))$ non-adaptive
measurements [CRT06] and that this bound is tight [DIPW10,FPRU10,PW11].
  In this paper we show that if one is allowed to perform measurements that are
adaptive, then the number of measurements can be considerably reduced.
Specifically, for $C=1+eps$ and $p=q=2$ we show - A scheme with $m=O((1/eps)k
log log (n eps/k))$ measurements that uses $O(log* k \log \log (n eps/k))$
rounds. This is a significant improvement over the best possible non-adaptive
bound. - A scheme with $m=O((1/eps) k log (k/eps) + k \log (n/k))$ measurements
that uses /two/ rounds. This improves over the best possible non-adaptive
bound. To the best of our knowledge, these are the first results of this type.
As an independent application, we show how to solve the problem of finding a
duplicate in a data stream of $n$ items drawn from ${1, 2, ..., n-1}$ using
$O(log n)$ bits of space and $O(log log n)$ passes, improving over the best
possible space complexity achievable using a single pass."
"We consider network design problems for information networks where routers
can replicate data but cannot alter it. This functionality allows the network
to eliminate data-redundancy in traffic, thereby saving on routing costs. We
consider two problems within this framework and design approximation
algorithms.
  The first problem we study is the traffic-redundancy aware network design
(RAND) problem. We are given a weighted graph over a single server and many
clients. The server owns a number of different data packets and each client
desires a subset of the packets; the client demand sets form a laminar set
system. Our goal is to connect every client to the source via a single path,
such that the collective cost of the resulting network is minimized. Here the
transportation cost over an edge is its weight times times the number of
distinct packets that it carries.
  The second problem is a facility location problem that we call RAFL. Here the
goal is to find an assignment from clients to facilities such that the total
cost of routing packets from the facilities to clients (along unshared paths),
plus the total cost of ""producing"" one copy of each desired packet at each
facility is minimized.
  We present a constant factor approximation for the RAFL and an O(log P)
approximation for RAND, where P is the total number of distinct packets. We
remark that P is always at most the number of different demand sets desired or
the number of clients, and is generally much smaller."
"We study graph partitioning problems from a min-max perspective, in which an
input graph on n vertices should be partitioned into k parts, and the objective
is to minimize the maximum number of edges leaving a single part. The two main
versions we consider are where the k parts need to be of equal-size, and where
they must separate a set of k given terminals. We consider a common
generalization of these two problems, and design for it an $O(\sqrt{\log n\log
k})$-approximation algorithm. This improves over an $O(\log^2 n)$ approximation
for the second version, and roughly $O(k\log n)$ approximation for the first
version that follows from other previous work. We also give an improved
O(1)-approximation algorithm for graphs that exclude any fixed minor.
  Our algorithm uses a new procedure for solving the Small-Set Expansion
problem. In this problem, we are given a graph G and the goal is to find a
non-empty set $S\subseteq V$ of size $|S| \leq \rho n$ with minimum
edge-expansion. We give an $O(\sqrt{\log{n}\log{(1/\rho)}})$ bicriteria
approximation algorithm for the general case of Small-Set Expansion, and O(1)
approximation algorithm for graphs that exclude any fixed minor."
"We present a new algorithm for computing $m$-th roots over the finite field
$\F_q$, where $q = p^n$, with $p$ a prime, and $m$ any positive integer. In the
particular case $m=2$, the cost of the new algorithm is an expected
$O(\M(n)\log (p) + \CC(n)\log(n))$ operations in $\F_p$, where $\M(n)$ and
$\CC(n)$ are bounds for the cost of polynomial multiplication and modular
polynomial composition. Known results give $\M(n) = O(n\log (n) \log\log (n))$
and $\CC(n) = O(n^{1.67})$, so our algorithm is subquadratic in $n$."
"Pairing heaps are shown to have constant amortized time Insert and Meld, thus
showing that pairing heaps have the same amortized runtimes as Fibonacci heaps
for all operations but Decrease-key."
"We introduce the first grammar-compressed representation of a sequence that
supports searches in time that depends only logarithmically on the size of the
grammar. Given a text $T[1..u]$ that is represented by a (context-free) grammar
of $n$ (terminal and nonterminal) symbols and size $N$ (measured as the sum of
the lengths of the right hands of the rules), a basic grammar-based
representation of $T$ takes $N\lg n$ bits of space. Our representation requires
$2N\lg n + N\lg u + \epsilon\, n\lg n + o(N\lg n)$ bits of space, for any
$0<\epsilon \le 1$. It can find the positions of the $occ$ occurrences of a
pattern of length $m$ in $T$ in $O((m^2/\epsilon)\lg (\frac{\lg u}{\lg n})
+occ\lg n)$ time, and extract any substring of length $\ell$ of $T$ in time
$O(\ell+h\lg(N/h))$, where $h$ is the height of the grammar tree."
"We present a deterministic (1+sqrt(5))/2-approximation algorithm for the s-t
path TSP for an arbitrary metric. Given a symmetric metric cost on n vertices
including two prespecified endpoints, the problem is to find a shortest
Hamiltonian path between the two endpoints; Hoogeveen showed that the natural
variant of Christofides' algorithm is a 5/3-approximation algorithm for this
problem, and this asymptotically tight bound in fact has been the best
approximation ratio known until now. We modify this algorithm so that it
chooses the initial spanning tree based on an optimal solution to the Held-Karp
relaxation rather than a minimum spanning tree; we prove this simple but
crucial modification leads to an improved approximation ratio, surpassing the
20-year-old barrier set by the natural Christofides' algorithm variant. Our
algorithm also proves an upper bound of (1+sqrt(5))/2 on the integrality gap of
the path-variant Held-Karp relaxation. The techniques devised in this paper can
be applied to other optimization problems as well: these applications include
improved approximation algorithms and improved LP integrality gap upper bounds
for the prize-collecting s-t path problem and the unit-weight graphical metric
s-t path TSP."
"A number of recent results on optimization problems involving submodular
functions have made use of the multilinear relaxation of the problem. These
results hold typically in the value oracle model, where the objective function
is accessible via a black box returning f(S) for a given S. We present a
general approach to deriving inapproximability results in the value oracle
model, based on the notion of symmetry gap. Our main result is that for any
fixed instance that exhibits a certain symmetry gap in its multilinear
relaxation, there is a naturally related class of instances for which a better
approximation factor than the symmetry gap would require exponentially many
oracle queries. This unifies several known hardness results for submodular
maximization, and implies several new ones. In particular, we prove that there
is no constant-factor approximation for the problem of maximizing a
non-negative submodular function over the bases of a matroid. We also provide a
closely matching approximation algorithm for this problem."
"We consider the problem of indexing a string $t$ of length $n$ to report the
occurrences of a query pattern $p$ containing $m$ characters and $j$ wildcards.
Let $occ$ be the number of occurrences of $p$ in $t$, and $\sigma$ the size of
the alphabet. We obtain the following results.
  - A linear space index with query time $O(m+\sigma^j \log \log n + occ)$.
This significantly improves the previously best known linear space index by Lam
et al. [ISAAC 2007], which requires query time $\Theta(jn)$ in the worst case.
  - An index with query time $O(m+j+occ)$ using space $O(\sigma^{k^2} n \log^k
\log n)$, where $k$ is the maximum number of wildcards allowed in the pattern.
This is the first non-trivial bound with this query time.
  - A time-space trade-off, generalizing the index by Cole et al. [STOC 2004].
  We also show that these indexes can be generalized to allow variable length
gaps in the pattern. Our results are obtained using a novel combination of
well-known and new techniques, which could be of independent interest."
"The generalized 2-server problem is an online optimization problem where a
sequence of requests has to be served at minimal cost. Requests arrive one by
one and need to be served instantly by at least one of two servers. We consider
the general model where the cost function of the two servers may be different.
Formally, each server moves in its own metric space and a request consists of
one point in each metric space. It is served by moving one of the two servers
to its request point. Requests have to be served without knowledge of the
future requests. The objective is to minimize the total traveled distance. The
special case where both servers move on the real line is known as the
CNN-problem. We show that the generalized work function algorithm is constant
competitive for the generalized 2-server problem."
"In the semi-streaming model, an algorithm receives a stream of edges of a
graph in arbitrary order and uses a memory of size $O(n \mbox{ polylog } n)$,
where $n$ is the number of vertices of a graph. In this work, we present
semi-streaming algorithms that perform one or two passes over the input stream
for maximum matching with no restrictions on the input graph, and for the
important special case of bipartite graphs that we refer to as maximum
bipartite matching (MBM). The Greedy matching algorithm performs one pass over
the input and outputs a $1/2$ approximation. Whether there is a better one-pass
algorithm has been an open question since the appearance of the first paper on
streaming algorithms for matching problems in 2005 [Feigenbaum et al., SODA
2005]. We make the following progress on this problem:
  In the one-pass setting, we show that there is a deterministic semi-streaming
algorithm for MBM with expected approximation factor $1/2+0.005$, assuming that
edges arrive one by one in (uniform) random order. We extend this algorithm to
general graphs, and we obtain a $1/2+0.003$ approximation.
  In the two-pass setting, we do not require the random arrival order
assumption (the edge stream is in arbitrary order). We present a simple
randomized two-pass semi-streaming algorithm for MBM with expected
approximation factor $1/2 + 0.019$. Furthermore, we discuss a more involved
deterministic two-pass semi-streaming algorithm for MBM with approximation
factor $1/2 + 0.019$ and a generalization of this algorithm to general graphs
with approximation factor $1/2 + 0.0071$."
"Many problems in Computer Science can be abstracted to the following
question: given a set of objects and rules respectively, which new objects can
be produced? In the paper, we consider a succinct version of the question:
given a set of binary strings and several operations like conjunction and
disjunction, which new binary strings can be generated? Although it is a
fundamental problem, to the best of our knowledge, the problem hasn't been
studied yet. In this paper, an O(m^2n) algorithm is presented to determine
whether a string s is representable by a set W, where n is the number of
strings in W and each string has the same length m. However, looking for the
minimum subset from a set to represent a given string is shown to be NP-hard.
Also, finding the smallest subset from a set to represent each string in the
original set is NP-hard. We establishes inapproximability results and
approximation algorithms for them. In addition, we prove that counting the
number of strings representable is #P-complete. We then explore how the
problems change when the operator negation is available. For example, if the
operator negation can be used, the number is some power of 2. This difference
maybe help us understand the problem more profoundly."
"For a given set of intervals on the real line, we consider the problem of
ordering the intervals with the goal of minimizing an objective function that
depends on the exposed interval pieces (that is, the pieces that are not
covered by earlier intervals in the ordering). This problem is motivated by an
application in molecular biology that concerns the determination of the
structure of the backbone of a protein.
  We present polynomial-time algorithms for several natural special cases of
the problem that cover the situation where the interval boundaries are
agreeably ordered and the situation where the interval set is laminar. Also the
bottleneck variant of the problem is shown to be solvable in polynomial time.
Finally we prove that the general problem is NP-hard, and that the existence of
a constant-factor-approximation algorithm is unlikely."
"We consider the problem of detecting a cycle in a directed graph that grows
by arc insertions, and the related problems of maintaining a topological order
and the strong components of such a graph. For these problems, we give two
algorithms, one suited to sparse graphs, and the other to dense graphs. The
former takes the minimum of O(m^{3/2}) and O(mn^{2/3}) time to insert m arcs
into an n-vertex graph; the latter takes O(n^2 log(n)) time. Our sparse
algorithm is considerably simpler than a previous O(m^{3/2})-time algorithm; it
is also faster on graphs of sufficient density. The time bound of our dense
algorithm beats the previously best time bound of O(n^{5/2}) for dense graphs.
Our algorithms rely for their efficiency on topologically ordered vertex
numberings; bounds on the size of the numbers give bound on running times."
"The {\em maximum cardinality} and {\em maximum weight matching} problems can
be solved in time $\tilde{O}(m\sqrt{n})$, a bound that has resisted improvement
despite decades of research. (Here $m$ and $n$ are the number of edges and
vertices.) In this article we demonstrate that this ""$m\sqrt{n}$ barrier"" is
extremely fragile, in the following sense. For any $\epsilon>0$, we give an
algorithm that computes a $(1-\epsilon)$-approximate maximum weight matching in
$O(m\epsilon^{-1}\log\epsilon^{-1})$ time, that is, optimal {\em linear time}
for any fixed $\epsilon$. Our algorithm is dramatically simpler than the best
exact maximum weight matching algorithms on general graphs and should be
appealing in all applications that can tolerate a negligible relative error.
  Our second contribution is a new {\em exact} maximum weight matching
algorithm for integer-weighted bipartite graphs that runs in time
$O(m\sqrt{n}\log N)$. This improves on the $O(Nm\sqrt{n})$-time and
$O(m\sqrt{n}\log(nN))$-time algorithms known since the mid 1980s, for $1\ll
\log N \ll \log n$. Here $N$ is the maximum integer edge weight."
"We consider the classical problem of representing a collection of priority
queues under the operations \Findmin{}, \Insert{}, \Decrease{}, \Meld{},
\Delete{}, and \Deletemin{}. In the comparison-based model, if the first four
operations are to be supported in constant time, the last two operations must
take at least logarithmic time. Brodal showed that his worst-case efficient
priority queues achieve these worst-case bounds. Unfortunately, this data
structure is involved and the time bounds hide large constants. We describe a
new variant of the worst-case efficient priority queues that relies on extended
regular counters and provides the same asymptotic time and space bounds as the
original. Due to the conceptual separation of the operations on regular
counters and all other operations, our data structure is simpler and easier to
describe and understand. Also, the constants in the time and space bounds are
smaller. In addition, we give an implementation of our structure on a pointer
machine. For our pointer-machine implementation, \Decrease{} and \Meld{} are
asymptotically slower and require $O(\lg\lg{n})$ worst-case time, where $n$
denotes the number of elements stored in the resulting priority queue."
"We present a $(1+\epsilon)$-approximation algorithm running in
$O(f(\epsilon)\cdot n \log^4 n)$ time for finding the diameter of an undirected
planar graph with non-negative edge lengths."
"We consider online resource allocation problems where given a set of requests
our goal is to select a subset that maximizes a value minus cost type of
objective function. Requests are presented online in random order, and each
request possesses an adversarial value and an adversarial size. The online
algorithm must make an irrevocable accept/reject decision as soon as it sees
each request. The ""profit"" of a set of accepted requests is its total value
minus a convex cost function of its total size. This problem falls within the
framework of secretary problems. Unlike previous work in that area, one of the
main challenges we face is that the objective function can be positive or
negative and we must guard against accepting requests that look good early on
but cause the solution to have an arbitrarily large cost as more requests are
accepted. This requires designing new techniques.
  We study this problem under various feasibility constraints and present
online algorithms with competitive ratios only a constant factor worse than
those known in the absence of costs for the same feasibility constraints. We
also consider a multi-dimensional version of the problem that generalizes
multi-dimensional knapsack within a secretary framework. In the absence of any
feasibility constraints, we present an O(l) competitive algorithm where l is
the number of dimensions; this matches within constant factors the best known
ratio for multi-dimensional knapsack secretary."
"We study the complexity of some algorithmic problems on directed hypergraphs
and their strongly connected components (SCCs). The main contribution is an
almost linear time algorithm computing the terminal strongly connected
components (i.e. SCCs which do not reach any components but themselves).
""Almost linear"" here means that the complexity of the algorithm is linear in
the size of the hypergraph up to a factor alpha(n), where alpha is the inverse
of Ackermann function, and n is the number of vertices. Our motivation to study
this problem arises from a recent application of directed hypergraphs to
computational tropical geometry.
  We also discuss the problem of computing all SCCs. We establish a superlinear
lower bound on the size of the transitive reduction of the reachability
relation in directed hypergraphs, showing that it is combinatorially more
complex than in directed graphs. Besides, we prove a linear time reduction from
the well-studied problem of finding all minimal sets among a given family to
the problem of computing the SCCs. Only subquadratic time algorithms are known
for the former problem. These results strongly suggest that the problem of
computing the SCCs is harder in directed hypergraphs than in directed graphs."
"We consider a natural generalization of the Partial Vertex Cover problem.
Here an instance consists of a graph G = (V,E), a positive cost function c: V->
Z^{+}, a partition $P_1,..., P_r$ of the edge set $E$, and a parameter $k_i$
for each partition $P_i$. The goal is to find a minimum cost set of vertices
which cover at least $k_i$ edges from the partition $P_i$. We call this the
Partition Vertex Cover problem. In this paper, we give matching upper and lower
bound on the approximability of this problem. Our algorithm is based on a novel
LP relaxation for this problem. This LP relaxation is obtained by adding
knapsack cover inequalities to a natural LP relaxation of the problem. We show
that this LP has integrality gap of $O(log r)$, where $r$ is the number of sets
in the partition of the edge set. We also extend our result to more general
settings."
"Clustering a graph means identifying internally dense subgraphs which are
only sparsely interconnected. Formalizations of this notion lead to measures
that quantify the quality of a clustering and to algorithms that actually find
clusterings. Since, most generally, corresponding optimization problems are
hard, heuristic clustering algorithms are used in practice, or other approaches
which are not based on an objective function. In this work we conduct a
comprehensive experimental evaluation of the qualitative behavior of greedy
bottom-up heuristics driven by cut-based objectives and constrained by
intracluster density, using both real-world data and artificial instances. Our
study documents that a greedy strategy based on local movement is superior to
one based on merging. We further reveal that the former approach generally
outperforms alternative setups and reference algorithms from the literature in
terms of its own objective, while a modularity-based algorithm competes
surprisingly well. Finally, we exhibit which combinations of cut-based inter-
and intracluster measures are suitable for identifying a hidden reference
clustering in synthetic random graphs."
"We consider connectivity problems with orientation constraints. Given a
directed graph $D$ and a collection of ordered node pairs $P$ let $P[D]=\{(u,v)
\in P: D {contains a} uv{-path}}$. In the {\sf Steiner Forest Orientation}
problem we are given an undirected graph $G=(V,E)$ with edge-costs and a set $P
\subseteq V \times V$ of ordered node pairs. The goal is to find a minimum-cost
subgraph $H$ of $G$ and an orientation $D$ of $H$ such that $P[D]=P$. We give a
4-approximation algorithm for this problem.
  In the {\sf Maximum Pairs Orientation} problem we are given a graph $G$ and a
multi-collection of ordered node pairs $P$ on $V$. The goal is to find an
orientation $D$ of $G$ such that $|P[D]|$ is maximum. Generalizing the result
of Arkin and Hassin [DAM'02] for $|P|=2$, we will show that for a mixed graph
$G$ (that may have both directed and undirected edges), one can decide in
$n^{O(|P|)}$ time whether $G$ has an orientation $D$ with $P[D]=P$ (for
undirected graphs this problem admits a polynomial time algorithm for any $P$,
but it is NP-complete on mixed graphs). For undirected graphs, we will show
that one can decide whether $G$ admits an orientation $D$ with $|P[D]| \geq k$
in $O(n+m)+2^{O(k\cdot \log \log k)}$ time; hence this decision problem is
fixed-parameter tractable, which answers an open question from Dorn et al.
[AMB'11]. We also show that {\sf Maximum Pairs Orientation} admits ratio
$O(\log |P|/\log\log |P|)$, which is better than the ratio $O(\log n/\log\log
n)$ of Gamzu et al. [WABI'10] when $|P|<n$.
  Finally, we show that the following node-connectivity problem can be solved
in polynomial time: given a graph $G=(V,E)$ with edge-costs, $s,t \in V$, and
an integer $\ell$, find a min-cost subgraph $H$ of $G$ with an orientation $D$
such that $D$ contains $\ell$ internally-disjoint $st$-paths, and $\ell$
internally-disjoint $ts$-paths."
"We consider some generalizations of the Asymmetric Traveling Salesman Path
problem. Suppose we have an asymmetric metric G = (V,A) with two distinguished
nodes s,t. We are also given a positive integer k. The goal is to find k paths
of minimum total cost from s to t whose union spans all nodes. We call this the
k-Person Asymmetric Traveling Salesmen Path problem (k-ATSPP). Our main result
for k-ATSPP is a bicriteria approximation that, for some parameter b >= 1 we
may choose, finds between k and k + k/b paths of total length O(b log |V|)
times the optimum value of an LP relaxation based on the Held-Karp relaxation
for the Traveling Salesman problem. On one extreme this is an O(log
|V|)-approximation that uses up to 2k paths and on the other it is an O(k log
|V|)-approximation that uses exactly k paths.
  Next, we consider the case where we have k pairs of nodes (s_1,t_1), ...,
(s_k,t_k). The goal is to find an s_i-t_i path for every pair such that each
node of G lies on at least one of these paths. Simple approximation algorithms
are presented for the special cases where the metric is symmetric or where s_i
= t_i for each i. We also show that the problem can be approximated within a
factor O(log n) when k=2. On the other hand, we demonstrate that the general
problem cannot be approximated within any bounded ratio unless P = NP."
"A tabulation-based hash function maps a key into d derived characters
indexing random values in tables that are then combined with bitwise xor
operations to give the hash. Thorup and Zhang (2004) presented d-wise
independent tabulation-based hash classes that use linear maps over finite
fields to map a key, considered as a vector (a,b), to derived characters. We
show that a variant where the derived characters are a+b*i for i=0,..., q-1
(using integer arithmetic) yielding (2d-1)-wise independence. Our analysis is
based on an algebraic property that characterizes k-wise independence of
tabulation-based hashing schemes, and combines this characterization with a
geometric argument. We also prove a non-trivial lower bound on the number of
derived characters necessary for k-wise independence with our and related hash
classes."
"We give an approximation algorithm for non-uniform sparsest cut with the
following guarantee: For any $\epsilon,\delta \in (0,1)$, given cost and demand
graphs with edge weights $C, D$ respectively, we can find a set $T\subseteq V$
with $\frac{C(T,V\setminus T)}{D(T,V\setminus T)}$ at most
$\frac{1+\epsilon}{\delta}$ times the optimal non-uniform sparsest cut value,
in time $2^{r/(\delta\epsilon)}\poly(n)$ provided $\lambda_r \ge
\Phi^*/(1-\delta)$. Here $\lambda_r$ is the $r$'th smallest generalized
eigenvalue of the Laplacian matrices of cost and demand graphs; $C(T,V\setminus
T)$ (resp. $D(T,V\setminus T)$) is the weight of edges crossing the
$(T,V\setminus T)$ cut in cost (resp. demand) graph and $\Phi^*$ is the
sparsity of the optimal cut. In words, we show that the non-uniform sparsest
cut problem is easy when the generalized spectrum grows moderately fast. To the
best of our knowledge, there were no results based on higher order spectra for
non-uniform sparsest cut prior to this work.
  Even for uniform sparsest cut, the quantitative aspects of our result are
somewhat stronger than previous methods. Similar results hold for other
expansion measures like edge expansion, normalized cut, and conductance, with
the $r$'th smallest eigenvalue of the normalized Laplacian playing the role of
$\lambda_r$ in the latter two cases.
  Our proof is based on an l1-embedding of vectors from a semi-definite program
from the Lasserre hierarchy. The embedded vectors are then rounded to a cut
using standard threshold rounding. We hope that the ideas connecting
$\ell_1$-embeddings to Lasserre SDPs will find other applications. Another
aspect of the analysis is the adaptation of the column selection paradigm from
our earlier work on rounding Lasserre SDPs [GS11] to pick a set of edges rather
than vertices. This feature is important in order to extend the algorithms to
non-uniform sparsest cut."
"Domains like bioinformatics, version control systems, collaborative editing
systems (wiki), and others, are producing huge data collections that are very
repetitive. That is, there are few differences between the elements of the
collection. This fact makes the compressibility of the collection extremely
high. For example, a collection with all different versions of a Wikipedia
article can be compressed up to the 0.1% of its original space, using the
Lempel-Ziv 1977 (LZ77) compression scheme.
  Many of these repetitive collections handle huge amounts of text data. For
that reason, we require a method to store them efficiently, while providing the
ability to operate on them. The most common operations are the extraction of
random portions of the collection and the search for all the occurrences of a
given pattern inside the whole collection.
  A self-index is a data structure that stores a text in compressed form and
allows to find the occurrences of a pattern efficiently. On the other hand,
self-indexes can extract any substring of the collection, hence they are able
to replace the original text. One of the main goals when using these indexes is
to store them within main memory.
  In this thesis we present a scheme for random text extraction from text
compressed with a Lempel-Ziv parsing. Additionally, we present a variant of
LZ77, called LZ-End, that efficiently extracts text using space close to that
of LZ77.
  The main contribution of this thesis is the first self-index based on
LZ77/LZ-End and oriented to repetitive texts, which outperforms the state of
the art (the RLCSA self-index) in many aspects. Finally, we present a corpus of
repetitive texts, coming from several application domains. We aim at providing
a standard set of texts for research and experimentation, hence this corpus is
publicly available."
"We resolve several fundamental questions in the area of distributed
functional monitoring, initiated by Cormode, Muthukrishnan, and Yi (SODA,
2008). In this model there are $k$ sites each tracking their input and
communicating with a central coordinator that continuously maintain an
approximate output to a function $f$ computed over the union of the inputs. The
goal is to minimize the communication.
  We show the randomized communication complexity of estimating the number of
distinct elements up to a $1+\eps$ factor is $\tilde{\Omega}(k/\eps^2)$,
improving the previous $\Omega(k + 1/\eps^2)$ bound and matching known upper
bounds up to a logarithmic factor. For the $p$-th frequency moment $F_p$, $p >
1$, we improve the previous $\Omega(k + 1/\eps^2)$ communication bound to
$\tilde{\Omega}(k^{p-1}/\eps^2)$. We obtain similar improvements for heavy
hitters, empirical entropy, and other problems. We also show that we can
estimate $F_p$, for any $p > 1$, using $\tilde{O}(k^{p-1}\poly(\eps^{-1}))$
communication. This greatly improves upon the previous
$\tilde{O}(k^{2p+1}N^{1-2/p} \poly(\eps^{-1}))$ bound of Cormode,
Muthukrishnan, and Yi for general $p$, and their $\tilde{O}(k^2/\eps +
k^{1.5}/\eps^3)$ bound for $p = 2$. For $p = 2$, our bound resolves their main
open question.
  Our lower bounds are based on new direct sum theorems for approximate
majority, and yield significant improvements to problems in the data stream
model, improving the bound for estimating $F_p, p > 2,$ in $t$ passes from
$\tilde{\Omega}(n^{1-2/p}/(\eps^{2/p} t))$ to
$\tilde{\Omega}(n^{1-2/p}/(\eps^{4/p} t))$, giving the first bound for
estimating $F_0$ in $t$ passes of $\Omega(1/(\eps^2 t))$ bits of space that
does not use the gap-hamming problem."
"With more than four billion usage of cellular phones worldwide, mobile
advertising has become an attractive alternative to online advertisements. In
this paper, we propose a new targeted advertising policy for Wireless Service
Providers (WSPs) via SMS or MMS- namely {\em AdCell}. In our model, a WSP
charges the advertisers for showing their ads. Each advertiser has a valuation
for specific types of customers in various times and locations and has a limit
on the maximum available budget. Each query is in the form of time and location
and is associated with one individual customer. In order to achieve a
non-intrusive delivery, only a limited number of ads can be sent to each
customer. Recently, new services have been introduced that offer location-based
advertising over cellular network that fit in our model (e.g., ShopAlerts by
AT&T) .
  We consider both online and offline version of the AdCell problem and develop
approximation algorithms with constant competitive ratio. For the online
version, we assume that the appearances of the queries follow a stochastic
distribution and thus consider a Bayesian setting. Furthermore, queries may
come from different distributions on different times. This model generalizes
several previous advertising models such as online secretary problem
\cite{HKP04}, online bipartite matching \cite{KVV90,FMMM09} and AdWords
\cite{saberi05}. ..."
"In this paper we present an implicit dynamic dictionary with the working-set
property, supporting insert(e) and delete(e) in O(log n) time, predecessor(e)
in O(log l_{p(e)}) time, successor(e) in O(log l_{s(e)}) time and search(e) in
O(log min(l_{p(e)},l_{e}, l_{s(e)})) time, where n is the number of elements
stored in the dictionary, l_{e} is the number of distinct elements searched for
since element e was last searched for and p(e) and s(e) are the predecessor and
successor of e, respectively. The time-bounds are all worst-case. The
dictionary stores the elements in an array of size n using no additional space.
In the cache-oblivious model the log is base B and the cache-obliviousness is
due to our black box use of an existing cache-oblivious implicit dictionary.
This is the first implicit dictionary supporting predecessor and successor
searches in the working-set bound. Previous implicit structures required O(log
n) time."
"We consider the file maintenance problem (also called the online labeling
problem) in which n integer items from the set {1,...,r} are to be stored in an
array of size m >= n. The items are presented sequentially in an arbitrary
order, and must be stored in the array in sorted order (but not necessarily in
consecutive locations in the array). Each new item must be stored in the array
before the next item is received. If r<=m then we can simply store item j in
location j but if r>m then we may have to shift the location of stored items to
make space for a newly arrived item. The algorithm is charged each time an item
is stored in the array, or moved to a new location. The goal is to minimize the
total number of such moves done by the algorithm. This problem is non-trivial
when n=<m<r.
  In the case that m=Cn for some C>1, algorithms for this problem with cost
O(log(n)^2) per item have been given [IKR81, Wil92, BCD+02]. When m=n,
algorithms with cost O(log(n)^3) per item were given [Zha93, BS07]. In this
paper we prove lower bounds that show that these algorithms are optimal, up to
constant factors. Previously, the only lower bound known for this range of
parameters was a lower bound of \Omega(log(n)^2) for the restricted class of
smooth algorithms [DSZ05a, Zha93].
  We also provide an algorithm for the sparse case: If the number of items is
polylogarithmic in the array size then the problem can be solved in amortized
constant time per item."
"We study the parameterized complexity of a robust generalization of the
classical Feedback Vertex Set problem, namely the Group Feedback Vertex Set
problem; we are given a graph G with edges labeled with group elements, and the
goal is to compute the smallest set of vertices that hits all cycles of G that
evaluate to a non-null element of the group. This problem generalizes not only
Feedback Vertex Set, but also Subset Feedback Vertex Set, Multiway Cut and Odd
Cycle Transversal. Completing the results of Guillemot [Discr. Opt. 2011], we
provide a fixed-parameter algorithm for the parameterization by the size of the
cutset only. Our algorithm works even if the group is given as a
polynomial-time oracle."
"Given a vertex-labeled graph, each vertex $v$ is attached with a label from a
set of labels. The vertex-label query desires the length of the shortest path
from the given vertex to the set of vertices with the given label. We show how
to construct an oracle if the given graph is planar, such that
$O(\frac{1}{\epsilon}n\log n)$ storing space is needed, and any vertex-label
query could be answered in $O(\frac{1}{\epsilon}\log n\log \rho)$ time with
stretch $1+\epsilon$. $\rho$ is the radius of the given graph, which is half of
the diameter. For the case that $\rho = O(\log n)$, we construct an oracle that
achieves $O(\log n)$ query time, without changing the order of storing space."
"The sudoku minimum number of clues problem is the following question: what is
the smallest number of clues that a sudoku puzzle can have? For several years
it had been conjectured that the answer is 17. We have performed an exhaustive
computer search for 16-clue sudoku puzzles, and did not find any, thus proving
that the answer is indeed 17. In this article we describe our method and the
actual search. As a part of this project we developed a novel way for
enumerating hitting sets. The hitting set problem is computationally hard; it
is one of Karp's 21 classic NP-complete problems. A standard backtracking
algorithm for finding hitting sets would not be fast enough to search for a
16-clue sudoku puzzle exhaustively, even at today's supercomputer speeds. To
make an exhaustive search possible, we designed an algorithm that allowed us to
efficiently enumerate hitting sets of a suitable size."
"Some difficulties regarding the application of the well-known sieve method
are considered in the case when a practical (program) realization of selecting
elements, having a particular property among the elements of a set with a
sufficiently great cardinal number(cardinality). In this paper the problem has
been resolved by using a modified version of the method, utilizing
multidimensional arrays. As a theoretical illustration of the method of the
multidimensional sieve, the problem of obtaining a single representative of
each equivalence class with respect to a given relation of equivalence and
obtaining the cardinality of the respective factor set is considered with
relevant mathematical proofs."
"Trees are fundamental data structure for many areas of computer science and
system engineering. In this report, we show how to ensure eventual consistency
of optimistically replicated trees. In optimistic replication, the different
replicas of a distributed system are allowed to diverge but should eventually
reach the same value if no more mutations occur. A new method to ensure
eventual consistency is to design Conflict-free Replicated Data Types (CRDT).
In this report, we design a collection of tree CRDT using existing set CRDTs.
The remaining concurrency problems particular to tree data structure are
resolved using one or two layers of correction algorithm. For each of these
layer, we propose different and independent policies. Any combination of set
CRDT and policies can be constructed, giving to the distributed application
programmer the entire control of the behavior of the shared data in face of
concurrent mutations. We also propose to order these trees by adding a
positioning layer which is also independent to obtain a collection of ordered
tree CRDTs."
"We briefly report on the current state of a new dynamic algorithm for the
route planning problem based on a concept of scope (the static variant
presented at ESA'11, HM2011A). We first motivate dynamization of the concept of
scope admissibility, and then we briefly describe a modification of the
scope-aware query algorithm of HM2011A to dynamic road networks. Finally, we
outline our future work on this concept."
"We consider the problem of computing the k-sparse approximation to the
discrete Fourier transform of an n-dimensional signal. We show:
  * An O(k log n)-time randomized algorithm for the case where the input signal
has at most k non-zero Fourier coefficients, and
  * An O(k log n log(n/k))-time randomized algorithm for general input signals.
  Both algorithms achieve o(n log n) time, and thus improve over the Fast
Fourier Transform, for any k = o(n). They are the first known algorithms that
satisfy this property. Also, if one assumes that the Fast Fourier Transform is
optimal, the algorithm for the exactly k-sparse case is optimal for any k =
n^{\Omega(1)}.
  We complement our algorithmic results by showing that any algorithm for
computing the sparse Fourier transform of a general signal must use at least
\Omega(k log(n/k)/ log log n) signal samples, even if it is allowed to perform
adaptive sampling."
"This work studies the problem of 2-dimensional searching for the 3-sided
range query of the form $[a, b]\times (-\infty, c]$ in both main and external
memory, by considering a variety of input distributions. We present three sets
of solutions each of which examines the 3-sided problem in both RAM and I/O
model respectively. The presented data structures are deterministic and the
expectation is with respect to the input distribution."
"Given a string of characters, the Burrows-Wheeler Transform rearranges the
characters in it so as to produce another string of the same length which is
more amenable to compression techniques such as move to front, run-length
encoding, and entropy encoders. We present a variant of the transform which
gives rise to similar or better compression value, but, unlike the original,
the transform we present is bijective, in that the inverse transformation
exists for all strings. Our experiments indicate that using our variant of the
transform gives rise to better compression ratio than the original
Burrows-Wheeler transform. We also show that both the transform and its inverse
can be computed in linear time and consuming linear storage."
"Binary relations are an important abstraction arising in many data
representation problems. The data structures proposed so far to represent them
support just a few basic operations required to fit one particular application.
We identify many of those operations arising in applications and generalize
them into a wide set of desirable queries for a binary relation representation.
We also identify reductions among those operations. We then introduce several
novel binary relation representations, some simple and some quite
sophisticated, that not only are space-efficient but also efficiently support a
large subset of the desired queries."
"Given a set of points $P \subset \mathbb{R}^d$, the $k$-means clustering
problem is to find a set of $k$ {\em centers} $C = \{c_1,...,c_k\}, c_i \in
\mathbb{R}^d,$ such that the objective function $\sum_{x \in P} d(x,C)^2$,
where $d(x,C)$ denotes the distance between $x$ and the closest center in $C$,
is minimized. This is one of the most prominent objective functions that have
been studied with respect to clustering.
  $D^2$-sampling \cite{ArthurV07} is a simple non-uniform sampling technique
for choosing points from a set of points. It works as follows: given a set of
points $P \subseteq \mathbb{R}^d$, the first point is chosen uniformly at
random from $P$. Subsequently, a point from $P$ is chosen as the next sample
with probability proportional to the square of the distance of this point to
the nearest previously sampled points.
  $D^2$-sampling has been shown to have nice properties with respect to the
$k$-means clustering problem. Arthur and Vassilvitskii \cite{ArthurV07} show
that $k$ points chosen as centers from $P$ using $D^2$-sampling gives an
$O(\log{k})$ approximation in expectation. Ailon et. al. \cite{AJMonteleoni09}
and Aggarwal et. al. \cite{AggarwalDK09} extended results of \cite{ArthurV07}
to show that $O(k)$ points chosen as centers using $D^2$-sampling give $O(1)$
approximation to the $k$-means objective function with high probability. In
this paper, we further demonstrate the power of $D^2$-sampling by giving a
simple randomized $(1 + \epsilon)$-approximation algorithm that uses the
$D^2$-sampling in its core."
"In this paper, first we give a sequential linear-time algorithm for the
longest path problem in meshes. This algorithm can be considered as an
improvement of [13]. Then based on this sequential algorithm, we present a
constant-time parallel algorithm for the problem which can be run on every
parallel machine."
"A central problem in e-commerce is determining overlapping communities among
individuals or objects in the absence of external identification or tagging. We
address this problem by introducing a framework that captures the notion of
communities or clusters determined by the relative affinities among their
members. To this end we define what we call an affinity system, which is a set
of elements, each with a vector characterizing its preference for all other
elements in the set. We define a natural notion of (potentially overlapping)
communities in an affinity system, in which the members of a given community
collectively prefer each other to anyone else outside the community. Thus these
communities are endogenously formed in the affinity system and are
""self-determined"" or ""self-certified"" by its members.
  We provide a tight polynomial bound on the number of self-determined
communities as a function of the robustness of the community. We present a
polynomial-time algorithm for enumerating these communities. Moreover, we
obtain a local algorithm with a strong stochastic performance guarantee that
can find a community in time nearly linear in the of size the community.
  Social networks fit particularly naturally within the affinity system
framework -- if we can appropriately extract the affinities from the relatively
sparse yet rich information from social networks, our analysis then yields a
set of efficient algorithms for enumerating self-determined communities in
social networks. In the context of social networks we also connect our analysis
with results about $(\alpha,\beta)$-clusters introduced by Mishra, Schreiber,
Stanton, and Tarjan \cite{msst}. In contrast with the polynomial bound we prove
on the number of communities in the affinity system model, we show that there
exists a family of networks with superpolynomial number of
$(\alpha,\beta)$-clusters."
"This paper deals with the problem of computing, in an online fashion, a
maximum benefit multi-commodity flow (\ONMCF), where the flow demands may be
bigger than the edge capacities of the network.
  We present an online, deterministic, centralized, all-or-nothing, bi-criteria
algorithm. The competitive ratio of the algorithm is constant, and the
algorithm augments the capacities by at most a logarithmic factor.
  The algorithm can handle two types of flow requests: (i) low demand requests
that must be routed along a path, and (ii) high demand requests that may be
routed using a multi-path flow.
  Two extensions are discussed: requests with known durations and machine
scheduling."
"Let C be a finite set of N elements and R = r_1,r_2,..., r_m a family of M
subsets of C. A subset X of R verifies the Consecutive Ones Property (C1P) if
there exists a permutation P of C such that each r_i in X is an interval of P.
A Minimal Conflicting Set (MCS) S is a subset of R that does not verify the
C1P, but such that any of its proper subsets does. In this paper, we present a
new simpler and faster algorithm to decide if a given element r in R belongs to
at least one MCS. Our algorithm runs in O(N^2M^2 + NM^7), largely improving the
current O(M^6N^5 (M+N)^2 log(M+N)) fastest algorithm of [Blin {\em et al}, CSR
2011]. The new algorithm is based on an alternative approach considering
minimal forbidden induced subgraphs of interval graphs instead of Tucker
matrices."
"This technical report describes the implementation of exact and parametrized
exponential algorithms, developed during the French ANR Agape during 2010-2012.
The developed algorithms are distributed under the CeCILL license and have been
written in Java using the Jung graph library."
"Let k be a natural number. Let G be a graph and let N_1,...,N_k be k
independent sets in G. The graph G is k-probe distance hereditary if G can be
embedded into a DH-graph by adding edges between vertices that are contained in
the same independent set. We show that there exists a polynomial-time algorithm
to check if a graph G is k-probe distance hereditary."
"Given a graph G and integers b and w. The black-and-white coloring problem
asks if there exist disjoint sets of vertices B and W with |B|=b and |W|=w such
that no vertex in B is adjacent to any vertex in W. In this paper we show that
the problem is polynomial when restricted to permutation graphs."
"The present paper examines the behavior of Shift-insertion sort (insertion
sort with shifting) for normal distribution inputs and is in continuation of
our earlier work on this new algorithm for discrete distribution inputs,
namely, negative binomial. Shift insertion sort is found more sensitive for
main effects but not for all interaction effects compared to conventional
insertion sort."
"Link-based data structures, such as linked lists and binary search trees,
have many well-known rearrangement steps allowing for efficient implementations
of insertion, deletion, and other operations. We describe a rearrangement
primitive designed for link-based, heap-ordered priority queues in the
comparison model, such as those similar to Fibonacci heaps or binomial heaps.
  In its most basic form, the primitive rearranges a collection of heap-ordered
perfect binary trees. Doing so offers a data structure control on the number of
trees involved in such a collection, in particular keeping this number
logarithmic in the number of elements. The rearrangement step is free from an
amortized complexity standpoint (using an appropriate potential function)."
"We describe scalable protocols for solving the secure multi-party computation
(MPC) problem among a large number of parties. We consider both the synchronous
and the asynchronous communication models. In the synchronous setting, our
protocol is secure against a static malicious adversary corrupting less than a
$1/3$ fraction of the parties. In the asynchronous setting, we allow the
adversary to corrupt less than a $1/8$ fraction of parties. For any
deterministic function that can be computed by an arithmetic circuit with $m$
gates, both of our protocols require each party to send a number of field
elements and perform an amount of computation that is $\tilde{O}(m/n + \sqrt
n)$. We also show that our protocols provide perfect and universally-composable
security.
  To achieve our asynchronous MPC result, we define the \emph{threshold
counting problem} and present a distributed protocol to solve it in the
asynchronous setting. This protocol is load balanced, with computation,
communication and latency complexity of $O(\log{n})$, and can also be used for
designing other load-balanced applications in the asynchronous communication
model."
"The efficiency of sorting techniques has a significant impact on the overall
efficiency of a program. The efficiency of Shell, Heap and Treap sorting
techniques in terms of both running time and memory usage was studied,
experiments conducted and results subjected to factor analysis by SPSS. The
study revealed the main factor affecting these sorting techniques was time
taken to sort."
"In our previous work there was some indication that Partition Sort could be
having a more robust average case O(nlogn) complexity than the popular Quick
Sort. In our first study in this paper, we reconfirm this through computer
experiments for inputs from Cauchy distribution for which expectation
theoretically does not exist. Additionally, the algorithm is found to be
sensitive to parameters of the input probability distribution demanding further
investigation on parameterized complexity. The results on this algorithm for
Binomial inputs in our second study are very encouraging in that direction."
"An optimal algorithm is presented about Conflict-Free Coloring for connected
subgraphs of tree of rings. Suppose the number of the rings in the tree is |T|
and the maximum length of rings is |R|. A presented algorithm in [1] for a Tree
of rings used O(log|T|.log|R|) colors but this algorithm uses O(log|T|+log|R|)
colors. The coloring earned by this algorithm has the unique-min property, that
is, the unique color is also minimum."
"We consider problems related to the combinatorial game (Free-)Flood-It, in
which players aim to make a coloured graph monochromatic with the minimum
possible number of flooding operations. We show that the minimum number of
moves required to flood any given graph G is equal to the minimum, taken over
all spanning trees T of G, of the number of moves required to flood T. This
result is then applied to give two polynomial-time algorithms for flood-filling
problems. Firstly, we can compute in polynomial time the minimum number of
moves required to flood a graph with only a polynomial number of connected
subgraphs. Secondly, given any coloured connected graph and a subset of the
vertices of bounded size, the number of moves required to connect this subset
can be computed in polynomial time."
"Biclique-colouring is a colouring of the vertices of a graph in such a way
that no maximal complete bipartite subgraph with at least one edge is
monochromatic. We show that it is coNP-complete to check whether a given
function that associates a colour to each vertex is a biclique-colouring, a
result that justifies the search for structured classes where the
biclique-colouring problem could be efficiently solved. We consider
biclique-colouring restricted to powers of paths and powers of cycles. We
determine the biclique-chromatic number of powers of paths and powers of
cycles. The biclique-chromatic number of a power of a path P_{n}^{k} is max(2k
+ 2 - n, 2) if n >= k + 1 and exactly n otherwise. The biclique-chromatic
number of a power of a cycle C_n^k is at most 3 if n >= 2k + 2 and exactly n
otherwise; we additionally determine the powers of cycles that are
2-biclique-colourable. All proofs are algorithmic and provide polynomial-time
biclique-colouring algorithms for graphs in the investigated classes."
"Network motif algorithms have been a topic of research mainly after the
2002-seminal paper from Milo \emph{et al}, that provided motifs as a way to
uncover the basic building blocks of most networks. In Bioinformatics, motifs
have been mainly applied in the field of gene regulation networks. This paper
proposes new algorithms to exactly count isomorphic pattern motifs of sizes 3,
4 and 5 in directed graphs. Let $G(V,E)$ be a directed graph with $m=|E|$. We
describe an $O({m\sqrt{m}})$ time complexity algorithm to count isomorphic
patterns of size 3. In order to count isomorphic patterns of size 4, we propose
an $O(m^2)$ algorithm. To count patterns with 5 vertices, the algorithm is
$O(m^2n)$. The new algorithms were implemented and compared with FANMOD and
Kavosh motif detection tools. The experiments show that our algorithms are
expressively faster than FANMOD and Kavosh's. We also let our motif-detecting
tool available in the Internet."
"We consider the problem of finding a minimum edge cost subgraph of a graph
satisfying both given node-connectivity requirements and degree upper bounds on
nodes. We present an iterative rounding algorithm of the biset LP relaxation
for this problem. For directed graphs and $k$-out-connectivity requirements
from a root, our algorithm computes a solution that is a 2-approximation on the
cost, and the degree of each node $v$ in the solution is at most $2b(v) + O(k)$
where $b(v)$ is the degree upper bound on $v$. For undirected graphs and
element-connectivity requirements with maximum connectivity requirement $k$,
our algorithm computes a solution that is a $4$-approximation on the cost, and
the degree of each node $v$ in the solution is at most $4b(v)+O(k)$. These
ratios improve the previous $O(\log k)$-approximation on the cost and $O(2^k
b(v))$ approximation on the degrees. Our algorithms can be used to improve
approximation ratios for other node-connectivity problems such as undirected
$k$-out-connectivity, directed and undirected $k$-connectivity, and undirected
rooted $k$-connectivity and subset $k$-connectivity."
"A large fraction of online display advertising is sold via guaranteed
contracts: a publisher guarantees to the advertiser a certain number of user
visits satisfying the targeting predicates of the contract. The publisher is
then tasked with solving the ad serving problem - given a user visit, which of
the thousands of matching contracts should be displayed, so that by the
expiration time every contract has obtained the requisite number of user
visits. The challenges of the problem come from (1) the sheer size of the
problem being solved, with tens of thousands of contracts and billions of user
visits, (2) the unpredictability of user behavior, since these contracts are
sold months ahead of time, when only a forecast of user visits is available and
(3) the minute amount of resources available online, as an ad server must
respond with a matching contract in a fraction of a second.
  We present a solution to the guaranteed delivery ad serving problem using
{\em compact allocation plans}. These plans, computed offline, can be
efficiently queried by the ad server during an ad call; they are small, using
only O(1) space for contract; and are stateless, allowing for distributed
serving without any central coordination. We evaluate this approach on a real
set of user visits and guaranteed contracts and show that the compact
allocation plans are an effective way of solving the guaranteed delivery ad
serving problem."
"Motivated by the problem of optimizing allocation in guaranteed display
advertising, we develop an efficient, lightweight method of generating a
compact {\em allocation plan} that can be used to guide ad server decisions.
The plan itself uses just O(1) state per guaranteed contract, is robust to
noise, and allows us to serve (provably) nearly optimally. The optimization
method we develop is scalable, with a small in-memory footprint, and working in
linear time per iteration. It is also ""stop-anytime"", meaning that
time-critical applications can stop early and still get a good serving
solution. Thus, it is particularly useful for optimizing the large problems
arising in the context of display advertising. We demonstrate the effectiveness
of our algorithm using actual Yahoo! data."
"It is shown that the exponential of a complex power series up to order n can
be implemented via (23/12+o(1))M(n) binary arithmetic operations over complex
field, where M(n) stands for the (smoothed) complexity of multiplication of
polynomials of degree <n in FFT-model. Yet, it is shown how to raise a power
series to a constant power with the complexity (27/8+o(1))M(n)."
"We propose a new greedy algorithm for the maximum cardinality matching
problem. We give experimental evidence that this algorithm is likely to find a
maximum matching in random graphs with constant expected degree c>0,
independent of the value of c. This is contrary to the behavior of commonly
used greedy matching heuristics which are known to have some range of c where
they probably fail to compute a maximum matching."
"Motivated by applications in energy-efficient scheduling in data centers,
Khuller, Li, and Saha introduced the {\em machine activation} problem as a
generalization of the classical optimization problems of set cover and load
balancing on unrelated machines. In this problem, a set of $n$ jobs have to be
distributed among a set of $m$ (unrelated) machines, given the processing time
of each job on each machine, where each machine has a startup cost. The goal is
to produce a schedule of minimum total startup cost subject to a constraint
$\bf L$ on its makespan. While Khuller {\em et al} considered the offline
version of this problem, a typical scenario in scheduling is one where jobs
arrive online and have to be assigned to a machine immediately on arrival. We
give an $(O(\log (mn)\log m), O(\log m))$-competitive randomized online
algorithm for this problem, i.e. the schedule produced by our algorithm has a
makespan of $O({\bf L} \log m)$ with high probability, and a total expected
startup cost of $O(\log (mn)\log m)$ times that of an optimal offline schedule
with makespan $\bf L$. The competitive ratios of our algorithm are (almost)
optimal.
  Our algorithms use the online primal dual framework introduced by Alon {\em
et al} for the online set cover problem, and subsequently developed further by
Buchbinder, Naor, and co-authors. To the best of our knowledge, all previous
applications of this framework have been to linear programs (LPs) with either
packing or covering constraints. One novelty of our application is that we use
this framework for a mixed LP that has both covering and packing constraints.
We hope that the algorithmic techniques developed in this paper to
simultaneously handle packing and covering constraints will be useful for
solving other online optimization problems as well."
"This paper introduces a new data structure, log_vector, with the following
properties: constant time random access to individual elements; constant time
element addition to the end; constant time element removal from the end;
constant time empty data structure creation; amortized constant space per
individual elements; constant additional space used."
"In this paper we give a construction of cut sparsifiers of Benczur and Karger
in the {\em dynamic} streaming setting in a single pass over the data stream.
Previous constructions either required multiple passes or were unable to handle
edge deletions. We use $\tilde{O}(1/\e^2)$ time for each stream update and
$\tilde{O}(n/\e^2)$ time to construct a sparsifier. Our $\e$-sparsifiers have
$O(n\log^3 n/\e^2)$ edges. The main tools behind our result are an application
of sketching techniques of Ahn et al.[SODA'12] to estimate edge connectivity
together with a novel application of sampling with limited independence and
sparse recovery to produce the edges of the sparsifier."
"The Work Function Algorithm is the most effective deterministic on-line
algorithm for the k-server problem. Koutsoupias and Papadimitriou proved WFA is
(2k-1) competitive. However the best known implementation of WFA requires time
O(i^2) to process request r_i and this makes WFA impractical for long sequences
of requests. The O(i^2) time is spent to compute the work function on the whole
history of past requests. In order to make constant the time to process a
request, Rudec and Menger proposed to restrict the history to a moving window
of fixed size. However WFA restricted to a moving window loses its
competitiveness. Here we give a condition that allows WFA to forget the whole
previous history and restart from scratch without losing competitiveness.
Moreover for most of the metric spaces of practical interest (finite or bounded
spaces) there is a constant bound on the length of the history before the
condition is verified and this makes O(1) the time to process each request."
"For two vertices $s$ and $t$ in a graph $G=(V,E)$, the next-to-shortest path
is an $st$-path which length is minimum amongst all $st$-paths strictly longer
than the shortest path length. In this paper we show that, when the graph is
undirected and all edge lengths are nonnegative, the problem can be solved in
linear time if the distances from $s$ and $t$ to all other vertices are given.
This result generalizes the previous work (DOI 10.1007/s00453-011-9601-7) to
allowing zero-length edges."
"Let $G=(V,E)$ be a $k$-edge-connected graph with edge costs $\{c(e):e \in
E\}$ and let $1 \leq \ell \leq k-1$. We show by a simple and short proof, that
$G$ contains an $\ell$-edge cover $I$ such that: $c(I) \leq \frac{\ell}{k}c(E)$
if $G$ is bipartite, or if $\ell |V|$ is even, or if $|E| \geq \frac{k|V|}{2}
+\frac{k}{2\ell}$; otherwise, $c(I) \leq (\frac{\ell}{k}+\frac{1}{k|V|})c(E)$.
The particular case $\ell=k-1$ and unit costs already includes a result of
Cheriyan and Thurimella, that $G$ contains a $(k-1)$-edge-cover of size
$|E|-\lfloor |V|/2 \rfloor$. Using our result, we slightly improve the
approximation ratios for the {\sf $k$-Connected Subgraph} problem (the
node-connectivity version) with uniform and $\beta$-metric costs. We then
consider the dual problem of finding a spanning subgraph of maximum
connectivity $k^*$ with a prescribed number of edges. We give an algorithm that
computes a $(k^*-1)$-connected subgraph, which is tight, since the problem is
NP-hard."
"In many problems, the inputs arrive over time, and must be dealt with
irrevocably when they arrive. Such problems are online problems. A common
method of solving online problems is to first solve the corresponding linear
program, and then round the fractional solution online to obtain an integral
solution.
  We give algorithms for solving linear programs with mixed packing and
covering constraints online. We first consider mixed packing and covering
linear programs, where packing constraints are given offline and covering
constraints are received online. The objective is to minimize the maximum
multiplicative factor by which any packing constraint is violated, while
satisfying the covering constraints. No prior sublinear competitive algorithms
are known for this problem. We give the first such --- a
polylogarithmic-competitive algorithm for solving mixed packing and covering
linear programs online. We also show a nearly tight lower bound.
  Our techniques for the upper bound use an exponential penalty function in
conjunction with multiplicative updates. While exponential penalty functions
are used previously to solve linear programs offline approximately, offline
algorithms know the constraints beforehand and can optimize greedily. In
contrast, when constraints arrive online, updates need to be more complex.
  We apply our techniques to solve two online fixed-charge problems with
congestion. These problems are motivated by applications in machine scheduling
and facility location. The linear program for these problems is more
complicated than mixed packing and covering, and presents unique challenges. We
show that our techniques combined with a randomized rounding procedure give
polylogarithmic-competitive integral solutions. These problems generalize
online set-cover, for which there is a polylogarithmic lower bound. Hence, our
results are close to tight."
"An instance of the maximum mixed graph orientation problem consists of a
mixed graph and a collection of source-target vertex pairs. The objective is to
orient the undirected edges of the graph so as to maximize the number of pairs
that admit a directed source-target path. This problem has recently arisen in
the study of biological networks, and it also has applications in communication
networks.
  In this paper, we identify an interesting local-to-global orientation
property. This property enables us to modify the best known algorithms for
maximum mixed graph orientation and some of its special structured instances,
due to Elberfeld et al. (CPM '11), and obtain improved approximation ratios. We
further proceed by developing an algorithm that achieves an even better
approximation guarantee for the general setting of the problem. Finally, we
study several well-motivated variants of this orientation problem."
"Many emerging computer applications require the processing of large numbers,
larger than what a CPU can handle. In fact, the top of the line PCs can only
manipulate numbers not longer than 32 bits or 64 bits. This is due to the size
of the registers and the data-path inside the CPU. As a result, performing
arithmetic operations such as subtraction on big-integer numbers is to some
extend limited. Different algorithms were designed in an attempt to solve this
problem; they all operate on big-integer numbers by first converting them into
a binary representation then performing bitwise operations on single bits. Such
algorithms are of complexity O(n) where n is the total number of bits in each
operand. This paper proposes two new algorithms for performing arithmetic
subtraction on big-integer numbers. The two algorithms are different in that
one is sequential while the other is parallel. The similarity between them is
that both follow the same concept of dividing the big-integer inputs into
several blocks or tokens of 60 bits (18 digits) each; thus reducing the input
size n in O(n) by a factor of 60. Subtraction of corresponding tokens, one from
each operand, is performed as humans perform subtraction, using a pencil and a
paper in the decimal system. Both algorithms are to be implemented using MS
C#.NET 2005 and tested over a multiple processor system. Further studies can be
done on other arithmetic operations such as addition and multiplication."
"Today's PCs can directly manipulate numbers not longer than 64 bits because
the size of the CPU registers and the data-path are limited. Consequently,
arithmetic operations such as addition, can only be performed on numbers of
that length. To solve the problem of computation on big-integer numbers,
different algorithms were developed. However, these algorithms are considerably
slow because they operate on individual bits; and are only designed to run over
single-processor computers. In this paper, two algorithms for handling
arithmetic addition on big-integer numbers are presented. The first algorithm
is sequential while the second is parallel. Both algorithms, unlike existing
ones, perform addition on blocks or tokens of 60 bits (18 digits), and thus
boosting the execution time by a factor of 60."
"We propose a new approach to competitive analysis in online scheduling by
introducing the novel concept of competitive-ratio approximation schemes. Such
a scheme algorithmically constructs an online algorithm with a competitive
ratio arbitrarily close to the best possible competitive ratio for any online
algorithm. We study the problem of scheduling jobs online to minimize the
weighted sum of completion times on parallel, related, and unrelated machines,
and we derive both deterministic and randomized algorithms which are almost
best possible among all online algorithms of the respective settings. We also
generalize our techniques to arbitrary monomial cost functions and apply them
to the makespan objective. Our method relies on an abstract characterization of
online algorithms combined with various simplifications and transformations. We
also contribute algorithmic means to compute the actual value of the best
possi- ble competitive ratio up to an arbitrary accuracy. This strongly
contrasts all previous manually obtained competitiveness results for algorithms
and, most importantly, it reduces the search for the optimal com- petitive
ratio to a question that a computer can answer. We believe that our concept can
also be applied to many other problems and yields a new perspective on online
algorithms in general."
"We prove that no online algorithm (even randomized, against an oblivious
adversary) is better than 1/2-competitive for welfare maximization with
coverage valuations, unless $NP = RP$. Since the Greedy algorithm is known to
be 1/2-competitive for monotone submodular valuations, of which coverage is a
special case, this proves that Greedy provides the optimal competitive ratio.
On the other hand, we prove that Greedy in a stochastic setting with
i.i.d.items and valuations satisfying diminishing returns is
$(1-1/e)$-competitive, which is optimal even for coverage valuations, unless
$NP=RP$. For online budget-additive allocation, we prove that no algorithm can
be 0.612-competitive with respect to a natural LP which has been used
previously for this problem."
"We consider the Set Once Strip Cover problem, in which n wireless sensors are
deployed over a one-dimensional region. Each sensor has a fixed battery that
drains in inverse proportion to a radius that can be set just once, but
activated at any time. The problem is to find an assignment of radii and
activation times that maximizes the length of time during which the entire
region is covered. We show that this problem is NP-hard. Second, we show that
RoundRobin, the algorithm in which the sensors simply take turns covering the
entire region, has a tight approximation guarantee of 3/2 in both Set Once
Strip Cover and the more general Strip Cover problem, in which each radius may
be set finitely-many times. Moreover, we show that the more general class of
duty cycle algorithms, in which groups of sensors take turns covering the
entire region, can do no better. Finally, we give an optimal O(n^2 log n)-time
algorithm for the related Set Radius Strip Cover problem, in which all sensors
must be activated immediately."
"In this paper, we make use of the Metropolis-type walks due to Nonaka et al.
(2010) to provide a faster solution to the $S$-$T$-connectivity problem in
undirected graphs (USTCON). As our main result, we propose a family of
randomized algorithms for USTCON which achieves a time-space product of $S\cdot
T = \tilde O(n^2)$ in graphs with $n$ nodes and $m$ edges (where the $\tilde
O$-notation disregards poly-logarithmic terms). This improves the previously
best trade-off of $\tilde O(n m)$, due to Feige (1995). Our algorithm consists
in deploying several short Metropolis-type walks, starting from landmark nodes
distributed using the scheme of Broder et al. (1994) on a modified input graph.
In particular, we obtain an algorithm running in time $\tilde O(n+m)$ which is,
in general, more space-efficient than both BFS and DFS. We close the paper by
showing how to fine-tune the Metropolis-type walk so as to match the
performance parameters (e.g., average hitting time) of the unbiased random walk
for any graph, while preserving a worst-case bound of $\tilde O(n^2)$ on cover
time."
"We study whether, when restricted to using polylogarithmic memory and
polylogarithmic passes, we can achieve qualitatively better data compression
with multiple read/write streams than we can with only one. We first show how
we can achieve universal compression using only one pass over one stream. We
then show that one stream is not sufficient for us to achieve good
grammar-based compression. Finally, we show that two streams are necessary and
sufficient for us to achieve entropy-only bounds."
"Consider a directed or an undirected graph with integral edge weights from
the set [-W, W], that does not contain negative weight cycles. In this paper,
we introduce a general framework for solving problems on such graphs using
matrix multiplication. The framework is based on the usage of Baur-Strassen's
theorem and of Strojohann's determinant algorithm. It allows us to give new and
simple solutions to the following problems:
  * Finding Shortest Cycles -- We give a simple \tilde{O}(Wn^{\omega}) time
algorithm for finding shortest cycles in undirected and directed graphs. For
directed graphs (and undirected graphs with non-negative weights) this matches
the time bounds obtained in 2011 by Roditty and Vassilevska-Williams. On the
other hand, no algorithm working in \tilde{O}(Wn^{\omega}) time was previously
known for undirected graphs with negative weights. Furthermore our algorithm
for a given directed or undirected graph detects whether it contains a negative
weight cycle within the same running time.
  * Computing Diameter and Radius -- We give a simple \tilde{O}(Wn^{\omega})
time algorithm for computing a diameter and radius of an undirected or directed
graphs. To the best of our knowledge no algorithm with this running time was
known for undirected graphs with negative weights.
  * Finding Minimum Weight Perfect Matchings -- We present an
\tilde{O}(Wn^{\omega}) time algorithm for finding minimum weight perfect
matchings in undirected graphs. This resolves an open problem posted by
Sankowski in 2006, who presented such an algorithm but only in the case of
bipartite graphs.
  In order to solve minimum weight perfect matching problem we develop a novel
combinatorial interpretation of the dual solution which sheds new light on this
problem. Such a combinatorial interpretation was not know previously, and is of
independent interest."
"We describe an algorithm for compressing a partially ordered set, or
\emph{poset}, so that it occupies space matching the information theory lower
bound (to within lower order terms), in the worst case. Using this algorithm,
we design a succinct data structure for representing a poset that, given two
elements, can report whether one precedes the other in constant time. This is
equivalent to succinctly representing the transitive closure graph of the
poset, and we note that the same method can also be used to succinctly
represent the transitive reduction graph. For an $n$ element poset, the data
structure occupies $n^2/4 + o(n^2)$ bits, in the worst case, which is roughly
half the space occupied by an upper triangular matrix. Furthermore, a slight
extension to this data structure yields a succinct oracle for reachability in
arbitrary directed graphs. Thus, using roughly a quarter of the space required
to represent an arbitrary directed graph, reachability queries can be supported
in constant time."
"We show that any permutation of ${1,2,...,N}$ can be written as the product
of two involutions. As a consequence, any permutation of the elements of an
array can be performed in-place in parallel in time O(1). In the case where the
permutation is the $k$-way perfect shuffle we develop two methods for
efficiently computing such a pair of involutions.
  The first method works whenever $N$ is a power of $k$; in this case the time
is O(N) and space $O(\log^2 N)$. The second method applies to the general case
where $N$ is a multiple of $k$; here the time is $O(N \log N)$ and the space is
$O(\log^2 N)$. If $k=2$ the space usage of the first method can be reduced to
$O(\log N)$ on a machine that has a SADD (population count) instruction."
"Spectral partitioning is a simple, nearly-linear time, algorithm to find
sparse cuts, and the Cheeger inequalities provide a worst-case guarantee for
the quality of the approximation found by the algorithm. Local graph
partitioning algorithms [ST08,ACL06,AP09] run in time that is nearly linear in
the size of the output set, and their approximation guarantee is worse than the
guarantee provided by the Cheeger inequalities by a polylogarithmic
$\log^{\Omega(1)} n$ factor. It has been a long standing open problem to design
a local graph clustering algorithm with an approximation guarantee close to the
guarantee of the Cheeger inequalities and with a running time nearly linear in
the size of the output.
  In this paper we solve this problem; we design an algorithm with the same
guarantee (up to a constant factor) as the Cheeger inequality, that runs in
time slightly super linear in the size of the output. This is the first
sublinear (in the size of the input) time algorithm with almost the same
guarantee as the Cheeger's inequality. As a byproduct of our results, we prove
a bicriteria approximation algorithm for the expansion profile of any graph.
Let $\phi(\gamma) = \min_{\mu(S) \leq \gamma}\phi(S)$. There is a polynomial
time algorithm that, for any $\gamma,\epsilon>0$, finds a set $S$ of measure
$\mu(S)\leq 2\gamma^{1+\epsilon}$, and expansion $\phi(S)\leq
\sqrt{2\phi(\gamma)/\epsilon}$. Our proof techniques also provide a simpler
proof of the structural result of Arora, Barak, Steurer [ABS10], that can be
applied to irregular graphs.
  Our main technical tool is that for any set $S$ of vertices of a graph, a
lazy $t$-step random walk started from a randomly chosen vertex of $S$, will
remain entirely inside $S$ with probability at least $(1-\phi(S)/2)^t$. This
itself provides a new lower bound to the uniform mixing time of any finite
states reversible markov chain."
"We study thresholds for the appearance of a 2-core in random hypergraphs that
are a mixture of a constant number of random uniform hypergraphs each with a
linear number of edges but with different edge sizes. For the case of two
overlapping hypergraphs we give a solution for the optimal (expected) number of
edges of each size such that the 2-core threshold for the resulting mixed
hypergraph is maximized. We show that for adequate edge sizes this threshold
exceeds the maximum 2-core threshold for any random uniform hypergraph, which
can be used to improve the space utilization of several data structures that
rely on this parameter."
"This paper proves that an ""old dog"", namely -- the classical
Johnson-Lindenstrauss transform, ""performs new tricks"" -- it gives a novel way
of preserving differential privacy. We show that if we take two databases, $D$
and $D'$, such that (i) $D'-D$ is a rank-1 matrix of bounded norm and (ii) all
singular values of $D$ and $D'$ are sufficiently large, then multiplying either
$D$ or $D'$ with a vector of iid normal Gaussians yields two statistically
close distributions in the sense of differential privacy. Furthermore, a small,
deterministic and \emph{public} alteration of the input is enough to assert
that all singular values of $D$ are large.
  We apply the Johnson-Lindenstrauss transform to the task of approximating
cut-queries: the number of edges crossing a $(S,\bar S)$-cut in a graph. We
show that the JL transform allows us to \emph{publish a sanitized graph} that
preserves edge differential privacy (where two graphs are neighbors if they
differ on a single edge) while adding only $O(|S|/\epsilon)$ random noise to
any given query (w.h.p). Comparing the additive noise of our algorithm to
existing algorithms for answering cut-queries in a differentially private
manner, we outperform all others on small cuts ($|S| = o(n)$).
  We also apply our technique to the task of estimating the variance of a given
matrix in any given direction. The JL transform allows us to \emph{publish a
sanitized covariance matrix} that preserves differential privacy w.r.t bounded
changes (each row in the matrix can change by at most a norm-1 vector) while
adding random noise of magnitude independent of the size of the matrix (w.h.p).
In contrast, existing algorithms introduce an error which depends on the matrix
dimensions."
"Given an undirected graph $G=(V,E)$ with edge capacities $c_e\geq 1$ for
$e\in E$ and a subset $T$ of $k$ vertices called terminals, we say that a graph
$H$ is a quality-$q$ cut sparsifier for $G$ iff $T\subseteq V(H)$, and for any
partition $(A,B)$ of $T$, the values of the minimum cuts separating $A$ and $B$
in graphs $G$ and $H$ are within a factor $q$ from each other. We say that $H$
is a quality-$q$ flow sparsifier for $G$ iff $T\subseteq V(H)$, and for any set
$D$ of demands over the terminals, the values of the minimum edge congestion
incurred by fractionally routing the demands in $D$ in graphs $G$ and $H$ are
within a factor $q$ from each other.
  So far vertex sparsifiers have been studied in a restricted setting where the
sparsifier $H$ is not allowed to contain any non-terminal vertices, that is
$V(H)=T$. For this setting, efficient algorithms are known for constructing
quality-$O(\log k/\log\log k)$ cut and flow vertex sparsifiers, as well as a
lower bound of $\tilde{\Omega}(\sqrt{\log k})$ on the quality of any flow or
cut sparsifier.
  We study flow and cut sparsifiers in the more general setting where Steiner
vertices are allowed, that is, we no longer require that $V(H)=T$. We show
algorithms to construct constant-quality cut sparsifiers of size $O(C^3)$ in
time $\poly(n)\cdot 2^C$, and constant-quality flow sparsifiers of size
$C^{O(\log\log C)}$ in time $n^{O(\log C)}\cdot 2^C$, where $C$ is the total
capacity of the edges incident on the terminals."
"We study the online preemptive scheduling of intervals and jobs (with
restarts). Each interval or job has an arrival time, a deadline, a length and a
weight. The objective is to maximize the total weight of completed intervals or
jobs. While the deterministic case for intervals was settled a long time ago,
the randomized case remains open. In this paper we first give a 2-competitive
randomized algorithm for the case of equal length intervals. The algorithm is
barely random in the sense that it randomly chooses between two deterministic
algorithms at the beginning and then sticks with it thereafter. Then we extend
the algorithm to cover several other cases of interval scheduling including
monotone instances, C-benevolent instances and D-benevolent instances, giving
the same competitive ratio. These algorithms are surprisingly simple but have
the best competitive ratio against all previous (fully or barely) randomized
algorithms. Next we extend the idea to give a 3-competitive algorithm for equal
length jobs. Finally, we prove a lower bound of 2 on the competitive ratio of
all barely random algorithms that choose between two deterministic algorithms
for scheduling equal length intervals (and hence jobs)."
"The graph exploration problem is to visit all the nodes of a connected graph
by a mobile entity, e.g., a robot. The robot has no a priori knowledge of the
topology of the graph or of its size. Cohen et al. \cite{Ilcinkas08} introduced
label guided graph exploration which allows the system designer to add short
labels to the graph nodes in a preprocessing stage; these labels can guide the
robot in the exploration of the graph. In this paper, we address the problem of
adjustable 1-bit label guided graph exploration. We focus on the labeling
schemes that not only enable a robot to explore the graph but also allow the
system designer to adjust the ratio of the number of different labels. This
flexibility is necessary when maintaining different labels may have different
costs or when the ratio is pre-specified. We present 1-bit labeling (two
colors, namely black and white) schemes for this problem along with a labeling
algorithm for generating the required labels. Given an $n$-node graph and a
rational number $\rho$, we can design a 1-bit labeling scheme such that
$n/b\geq \rho$ where $b$ is the number of nodes labeled black. The robot uses
$O(\rho\log\Delta)$ bits of memory for exploring all graphs of maximum degree
$\Delta$. The exploration is completed in time
$O(n\Delta^{\frac{16\rho+7}{3}}/\rho+\Delta^{\frac{40\rho+10}{3}})$. Moreover,
our labeling scheme can work on graphs containing loops and multiple edges,
while that of Cohen et al. focuses on simple graphs."
"Given a pair of distinct vertices u, v in a graph G, we say that s is a
junction of u, v if there are in G internally vertex disjoint directed paths
from s to u and from s to v. We show how to characterize junctions in directed
acyclic graphs. We also consider the two problems in the following and derive
efficient algorithms to solve them. Given a directed acyclic graph G and a
vertex s in G, how can we find all pairs of vertices of G such that s is a
junction of them? And given a directed acyclic graph G and k pairs of vertices
of G, how can we preprocess G such that all junctions of k given pairs of
vertices could be listed quickly? All junctions of k pairs problem arises in an
application in Anthropology and we apply our algorithm to find such junctions
on kinship networks of some brazilian indian ethnic groups."
"A unit disk graph is the intersection graph of n congruent disks in the
plane. Dominating sets in unit disk graphs are widely studied due to their
application in wireless ad-hoc networks. Because the minimum dominating set
problem for unit disk graphs is NP-hard, numerous approximation algorithms have
been proposed in the literature, including some PTAS. However, since the
proposal of a linear-time 5-approximation algorithm in 1995, the lack of
efficient algorithms attaining better approximation factors has aroused
attention. We introduce a linear-time O(n+m) approximation algorithm that takes
the usual adjacency representation of the graph as input and outputs a
44/9-approximation. This approximation factor is also attained by a second
algorithm, which takes the geometric representation of the graph as input and
runs in O(n log n) time regardless of the number of edges. Additionally, we
propose a 43/9-approximation which can be obtained in O(n^2 m) time given only
the graph's adjacency representation. It is noteworthy that the dominating sets
obtained by our algorithms are also independent sets."
"Access graphs, which have been used previously in connection with competitive
analysis to model locality of reference in paging, are considered in connection
with relative worst order analysis. In this model, FWF is shown to be strictly
worse than both LRU and FIFO on any access graph. LRU is shown to be strictly
better than FIFO on paths and cycles, but they are incomparable on some
families of graphs which grow with the length of the sequences."
"We consider dynamic subgraph connectivity problems for planar graphs. In this
model there is a fixed underlying planar graph, where each edge and vertex is
either ""off"" (failed) or ""on"" (recovered). We wish to answer connectivity
queries with respect to the ""on"" subgraph. The model has two natural variants,
one in which there are $d$ edge/vertex failures that precede all connectivity
queries, and one in which failures/recoveries and queries are intermixed.
  We present a $d$-failure connectivity oracle for planar graphs that processes
any $d$ edge/vertex failures in $sort(d,n)$ time so that connectivity queries
can be answered in $pred(d,n)$ time. (Here $sort$ and $pred$ are the time for
integer sorting and integer predecessor search over a subset of $[n]$ of size
$d$.) Our algorithm has two discrete parts. The first is an algorithm tailored
to triconnected planar graphs. It makes use of Barnette's theorem, which states
that every triconnected planar graph contains a degree-3 spanning tree. The
second part is a generic reduction from general (planar) graphs to triconnected
(planar) graphs. Our algorithm is, moreover, provably optimal. An implication
of Patrascu and Thorup's lower bound on predecessor search is that no
$d$-failure connectivity oracle (even on trees) can beat $pred(d,n)$ query
time.
  We extend our algorithms to the subgraph connectivity model where edge/vertex
failures (but no recoveries) are intermixed with connectivity queries. In
triconnected planar graphs each failure and query is handled in $O(\log n)$
time (amortized), whereas in general planar graphs both bounds become $O(\log^2
n)$."
"It is shown that for cuckoo hashing with a stash as proposed by Kirsch,
Mitzenmacher, and Wieder (2008) families of very simple hash functions can be
used, maintaining the favorable performance guarantees: with stash size $s$ the
probability of a rehash is $O(1/n^{s+1})$, and the evaluation time is $O(s)$.
Instead of the full randomness needed for the analysis of Kirsch et al. and of
Kutzelnigg (2010) (resp. $\Theta(\log n)$-wise independence for standard cuckoo
hashing) the new approach even works with 2-wise independent hash families as
building blocks. Both construction and analysis build upon the work of
Dietzfelbinger and Woelfel (2003). The analysis, which can also be applied to
the fully random case, utilizes a graph counting argument and is much simpler
than previous proofs. As a byproduct, an algorithm for simulating uniform
hashing is obtained. While it requires about twice as much space as the most
space efficient solutions, it is attractive because of its simple and direct
structure."
"We present an optimal, combinatorial 1-1/e approximation algorithm for
monotone submodular optimization over a matroid constraint. Compared to the
continuous greedy algorithm (Calinescu, Chekuri, Pal and Vondrak, 2008), our
algorithm is extremely simple and requires no rounding. It consists of the
greedy algorithm followed by local search. Both phases are run not on the
actual objective function, but on a related non-oblivious potential function,
which is also monotone submodular. Our algorithm runs in randomized time
O(n^8u), where n is the rank of the given matroid and u is the size of its
ground set. We additionally obtain a 1-1/e-eps approximation algorithm running
in randomized time O (eps^-3n^4u). For matroids in which n = o(u), this
improves on the runtime of the continuous greedy algorithm. The improvement is
due primarily to the time required by the pipage rounding phase, which we avoid
altogether. Furthermore, the independence of our algorithm from pipage rounding
techniques suggests that our general approach may be helpful in contexts such
as monotone submodular maximization subject to multiple matroid constraints.
  Our approach generalizes to the case where the monotone submodular function
has restricted curvature. For any curvature c, we adapt our algorithm to
produce a (1-e^-c)/c approximation. This result complements results of Vondrak
(2008), who has shown that the continuous greedy algorithm produces a
(1-e^-c)/c approximation when the objective function has curvature c. He has
also proved that achieving any better approximation ratio is impossible in the
value oracle model."
"We study the problem of finding a small sparse cut in an undirected graph.
Given an undirected graph G=(V,E) and a parameter k <= |E|, the small sparsest
cut problem is to find a subset of vertices S with minimum conductance among
all sets with volume at most k. Using ideas developed in local graph
partitioning algorithms, we obtain the following bicriteria approximation
algorithms for the small sparsest cut problem:
  - If there is a subset U with conductance \phi and vol(U) <= k, then there is
a polynomial time algorithm to find a set S with conductance
O(\sqrt{\phi/\epsilon}) and vol(S) <= k^{1+\epsilon} for any \epsilon > 1/k.
  - If there is a subset U with conductance \phi and vol(U) <= k, then there is
a polynomial time algorithm to find a set S with conductance O(\sqrt{\phi
ln(k)/\epsilon}) and vol(S) <= (1+\epsilon)k for any \epsilon > 2ln(k)/k.
  These algorithms can be implemented locally using truncated random walk, with
running time almost linear to the output size. This provides a local graph
partitioning algorithm with a better conductance guarantee when k is sublinear."
"Consider a finite irreducible Markov chain with invariant distribution $\pi$.
We use the inner product induced by $\pi$ and the associated heat operator to
simplify and generalize some results related to graph partitioning and the
small-set expansion problem. For example, Steurer showed a tight connection
between the number of small eigenvalues of a graph's Laplacian and the
expansion of small sets in that graph. We give a simplified proof which
generalizes to the nonregular, directed case. This result implies an
approximation algorithm for an ""analytic"" version of the Small-Set Expansion
Problem, which, in turn, immediately gives an approximation algorithm for
Small-Set Expansion. We also give a simpler proof of a lower bound on the
probability that a random walk stays within a set; this result was used in some
recent works on finding small sparse cuts."
"A string-like compact data structure for unlabelled rooted trees is given
using 2n bits."
"We consider the problem of preprocessing $N$ points in 2D, each endowed with
a priority, to answer the following queries: given a axis-parallel rectangle,
determine the point with the largest priority in the rectangle. Using the ideas
of the \emph{effective entropy} of range maxima queries and \emph{succinct
indices} for range maxima queries, we obtain a structure that uses O(N) words
and answers the above query in $O(\log N \log \log N)$ time. This is a direct
improvement of Chazelle's result from FOCS 1985 for this problem -- Chazelle
required $O(N/\epsilon)$ words to answer queries in $O((\log N)^{1+\epsilon})$
time for any constant $\epsilon > 0$."
"It is known that the problem of deleting at most k vertices to obtain a
proper interval graph (Proper Interval Vertex Deletion) is fixed parameter
tractable. However, whether the problem admits a polynomial kernel or not was
open. Here, we answers this question in affirmative by obtaining a polynomial
kernel for Proper Interval Vertex Deletion. This resolves an open question of
van Bevern, Komusiewicz, Moser, and Niedermeier."
"The present paper makes a study on Partition sort algorithm for negative
binomial inputs. Comparing the results with those for binomial inputs in our
previous work, we find that this algorithm is sensitive to parameters of both
distributions. But the main effects as well as the interaction effects
involving these parameters and the input size are more significant for negative
binomial case."
"Smart Sort algorithm is a ""smart"" fusion of heap construction procedures (of
Heap sort algorithm) into the conventional ""Partition"" function (of Quick sort
algorithm) resulting in a robust version of Quick sort algorithm. We have also
performed empirical analysis of average case behavior of our proposed algorithm
along with the necessary theoretical analysis for best and worst cases. Its
performance was checked against some standard probability distributions, both
uniform and non-uniform, like Binomial, Poisson, Discrete & Continuous Uniform,
Exponential, and Standard Normal. The analysis exhibited the desired robustness
coupled with excellent performance of our algorithm. Although this paper
assumes the static partition ratios, its dynamic version is expected to yield
still better results."
"The selection problem, where one wishes to locate the $k^{th}$ smallest
element in an unsorted array of size $n$, is one of the basic problems studied
in computer science. The main focus of this work is designing algorithms for
solving the selection problem in the presence of memory faults. These can
happen as the result of cosmic rays, alpha particles, or hardware failures.
  Specifically, the computational model assumed here is a faulty variant of the
RAM model (abbreviated as FRAM), which was introduced by Finocchi and Italiano.
In this model, the content of memory cells might get corrupted adversarially
during the execution, and the algorithm is given an upper bound $\delta$ on the
number of corruptions that may occur.
  The main contribution of this work is a deterministic resilient selection
algorithm with optimal O(n) worst-case running time. Interestingly, the running
time does not depend on the number of faults, and the algorithm does not need
to know $\delta$.
  The aforementioned resilient selection algorithm can be used to improve the
complexity bounds for resilient $k$-d trees developed by Gieseke, Moruz and
Vahrenhold. Specifically, the time complexity for constructing a $k$-d tree is
improved from $O(n\log^2 n + \delta^2)$ to $O(n \log n)$.
  Besides the deterministic algorithm, a randomized resilient selection
algorithm is developed, which is simpler than the deterministic one, and has
$O(n + \alpha)$ expected time complexity and O(1) space complexity (i.e., is
in-place). This algorithm is used to develop the first resilient sorting
algorithm that is in-place and achieves optimal $O(n\log n + \alpha\delta)$
expected running time."
"In this paper, we investigate the explicit deterministic treasure hunt
problem in a $n$-vertex network. This problem was firstly introduced by Ta-Shma
and Zwick in \cite{TZ07} [SODA'07]. Note also it is a variant of the well known
rendezvous problem in which one of the robot (the treasure) is always
stationary. In this paper, we propose an $O(n^{c(1+\frac{1}{\lambda})})$-time
algorithm for the treasure hunt problem, which significantly improves the
currently best known result of running time $O(n^{2c})$ in \cite{TZ07}, where
$c$ is a constant induced from the construction of an universal exploration
sequence in \cite{R05,TZ07}, and $\lambda \gg 1$ is an arbitrary large, but
fixed, integer constant. The treasure hunt problem also motivates the study of
strongly universal exploration sequences. In this paper, we also propose a much
better explicit construction for strongly universal exploration sequences
compared to the one in \cite{TZ07}."
"This note extends the analysis of incremental PageRank in [B. Bahmani, A.
Chowdhury, and A. Goel. Fast Incremental and Personalized PageRank. VLDB 2011].
In that work, the authors prove a running time of $O(\frac{nR}{\epsilon^2}
\ln(m))$ to keep PageRank updated over $m$ edge arrivals in a graph with $n$
nodes when the algorithm stores $R$ random walks per node and the PageRank
teleport probability is $\epsilon$. To prove this running time, they assume
that edges arrive in a random order, and leave it to future work to extend
their running time guarantees to adversarial edge arrival. In this note, we
show that the random edge order assumption is necessary by exhibiting a graph
and adversarial edge arrival order in which the running time is $\Omega \left(R
n m^{\lg{\frac{3}{2}(1-\epsilon)}}\right)$. More generally, for any integer $d
\geq 2$, we construct a graph and adversarial edge order in which the running
time is $\Omega \left(R n m^{\log_d(H_d (1-\epsilon))}\right)$, where $H_d$ is
the $d$th harmonic number."
"We propose a new approach for calculating the Lempel-Ziv factorization of a
string, based on run length encoding (RLE). We present a conceptually simple
off-line algorithm based on a variant of suffix arrays, as well as an on-line
algorithm based on a variant of directed acyclic word graphs (DAWGs). Both
algorithms run in $O(N+n\log n)$ time and O(n) extra space, where N is the size
of the string, $n\leq N$ is the number of RLE factors. The time dependency on N
is only in the conversion of the string to RLE, which can be computed very
efficiently in O(N) time and O(1) extra space (excluding the output). When the
string is compressible via RLE, i.e., $n = o(N)$, our algorithms are, to the
best of our knowledge, the first algorithms which require only o(N) extra space
while running in $o(N\log N)$ time."
"A rerouting sequence is a sequence of shortest st-paths such that consecutive
paths differ in one vertex. We study the the Shortest Path Rerouting Problem,
which asks, given two shortest st-paths P and Q in a graph G, whether a
rerouting sequence exists from P to Q. This problem is PSPACE-hard in general,
but we show that it can be solved in polynomial time if G is planar. To this
end, we introduce a dynamic programming method for reconfiguration problems."
"We describe an algorithm computing an optimal prefix free code from $N$
unsorted positive integer weights in time linear in the number of machine words
holding those weights. This algorithm takes advantage of common non-algebraic
instructions, and of specific results on optimal prefix free codes. This result
improves over the state of the art complexities of $O(N\lg N)$ in the algebraic
decision tree model and $O(N\lg\lg N)$ in the RAM model for the computation of
Huffman's codes, a landmark in compression and coding since 1952."
"In the reordering buffer problem (RBP), a server is asked to process a
sequence of requests lying in a metric space. To process a request the server
must move to the corresponding point in the metric. The requests can be
processed slightly out of order; in particular, the server has a buffer of
capacity k which can store up to k requests as it reads in the sequence. The
goal is to reorder the requests in such a manner that the buffer constraint is
satisfied and the total travel cost of the server is minimized. The RBP arises
in many applications that require scheduling with a limited buffer capacity,
such as scheduling a disk arm in storage systems, switching colors in paint
shops of a car manufacturing plant, and rendering 3D images in computer
graphics.
  We study the offline version of RBP and develop bicriteria approximations.
When the underlying metric is a tree, we obtain a solution of cost no more than
9OPT using a buffer of capacity 4k + 1 where OPT is the cost of an optimal
solution with buffer capacity k. Constant factor approximations were known
previously only for the uniform metric (Avigdor-Elgrabli et al., 2012). Via
randomized tree embeddings, this implies an O(log n) approximation to cost and
O(1) approximation to buffer size for general metrics. Previously the best
known algorithm for arbitrary metrics by Englert et al. (2007) provided an
O(log^2 k log n) approximation without violating the buffer constraint."
"We give an efficient algorithm to generate a graph from a distribution
$\epsilon$-close to $G(n,p)$, in the sense of total variation distance. In
particular, if $p$ is represented with $O(\log n)$-bit accuracy, then, with
high probability, the running time is linear in the expected number of edges of
the output graph (up to poly-logarithmic factors). All our running times
include the complexity of the arithmetic involved in the corresponding
algorithms. Previous standard methods for exact $G(n,p)$ sampling (see e.g.
Batagelj and Brandes, 2005) achieve similar running times, however, under the
assumption that performing real number arithmetic with arbitrary accuracy takes
constant time. We note that the actual accuracy required by these methods is
O(n)-bit per step, which results in quadratic running times.
  The main idea of our $G(n,p)$ generation algorithm is a Metropolis Markov
chain to sample $\epsilon$-close from the binomial distribution. This is a new
method for sampling from the binomial distribution: it is of separate interest
and may find other useful applications. Our analysis accounts for all necessary
bit-accuracy and arithmetic, and our running times are comparable to known
methods for exact binomial sampling.
  We further obtain efficient generation algorithms for random graphs with
given arbitrary degree distributions, Inhomogeneous Random Graphs when the
kernel function is the inner product, and Stochastic Kronecker Graphs. To the
best our knowledge, our work can be viewed as the first effort to simulate
efficient generation of graphs from classical random graph models, while taking
into account implementational considerations as fundamental computational
aspects, and quantifying the tradeoff between accuracy and running time in a
way that can be useful in practice."
"We study the computational complexity of graph planarization via edge
contraction. The problem CONTRACT asks whether there exists a set $S$ of at
most $k$ edges that when contracted produces a planar graph. We work with a
more general problem called $P$-RESTRICTEDCONTRACT in which $S$, in addition,
is required to satisfy a fixed MSOL formula $P(S,G)$. We give an FPT algorithm
in time $O(n^2 f(k))$ which solves $P$-RESTRICTEDCONTRACT, where $P(S,G)$ is
(i) inclusion-closed and (ii) inert contraction-closed (where inert edges are
the edges non-incident to any inclusion minimal solution $S$).
  As a specific example, we can solve the $\ell$-subgraph contractibility
problem in which the edges of a set $S$ are required to form disjoint connected
subgraphs of size at most $\ell$. This problem can be solved in time $O(n^2
f'(k,\ell))$ using the general algorithm. We also show that for $\ell \ge 2$
the problem is NP-complete."
"The paper describes a novel technique that allows to reduce by half the
number of delta values that were required to be computed with complexity O(N)
in most of the heuristics for the quadratic assignment problem. Using the
correlation between the old and new delta values, obtained in this work, a new
formula of complexity O(1) is proposed. Found result leads up to 25%
performance increase in such well-known algorithms as Robust Tabu Search and
others based on it."
"We adapt a well known streaming algorithm for approximating item frequencies
to the matrix sketching setting. The algorithm receives the rows of a large
matrix $A \in \R^{n \times m}$ one after the other in a streaming fashion. It
maintains a sketch matrix $B \in \R^ {1/\eps \times m}$ such that for any unit
vector $x$ [\|Ax\|^2 \ge \|Bx\|^2 \ge \|Ax\|^2 - \eps \|A\|_{f}^2 \.] Sketch
updates per row in $A$ require $O(m/\eps^2)$ operations in the worst case. A
slight modification of the algorithm allows for an amortized update time of
$O(m/\eps)$ operations per row. The presented algorithm stands out in that it
is: deterministic, simple to implement, and elementary to prove. It also
experimentally produces more accurate sketches than widely used approaches
while still being computationally competitive."
"In this paper we present improved bounds for approximating maximum matchings
in bipartite graphs in the streaming model. First, we consider the question of
how well maximum matching can be approximated in a single pass over the input
using \tilde O(n)$ space, where $n$ is the number of vertices in the input
graph. Two natural variants of this problem have been considered in the
literature: (1) the edge arrival setting, where edges arrive in the stream and
(2) the vertex arrival setting, where vertices on one side of the graph arrive
in the stream together with all their incident edges. The latter setting has
also been studied extensively in the context of {\em online algorithms}, where
each arriving vertex has to either be matched irrevocably or discarded upon
arrival. In the online setting, the celebrated algorithm of
Karp-Vazirani-Vazirani achieves a $1-1/e$ approximation. Despite the fact that
the streaming model is less restrictive in that the algorithm is not
constrained to match vertices irrevocably upon arrival, the best known
approximation in the streaming model with vertex arrivals and $\tilde O(n)$
space is the same factor of $1-1/e$.
  We show that no single pass streaming algorithm that uses $\tilde O(n)$ space
can achieve a better than $1-1/e$ approximation to maximum matching, even in
the vertex arrival setting. This leads to the striking conclusion that no
single pass streaming algorithm can do better than online algorithms unless it
uses significantly more than $\tilde O(n)$ space. Additionally, our bound
yields the best known impossibility result for approximating matchings in the
{\em edge arrival} model.
  We also give a simple algorithm that achieves approximation ratio
$1-e^{-k}k^{k-1}/(k-1)!=1-\frac1{\sqrt{2\pi k}}+o(1/k)$ in $k$ passes in the
vertex arrival model using linear space, improving upon previously best known
convergence."
"The GRAPH MOTIF problem asks whether a given multiset of colors appears on a
connected subgraph of a vertex-colored graph. The fastest known parameterized
algorithm for this problem is based on a reduction to the $k$-Multilinear
Detection (k-MlD) problem: the detection of multilinear terms of total degree k
in polynomials presented as circuits. We revisit k-MLD and define k-CMLD, a
constrained version of it which reflects GRAPH MOTIF more faithfully. We then
give a fast algorithm for k-CMLD. As a result we obtain faster parameterized
algorithms for GRAPH MOTIF and variants of it."
"Bucket sort and RADIX sort are two well-known integer sorting algorithms.
This paper measures empirically what is the time usage and memory consumption
for different kinds of input sequences. The algorithms are compared both from a
theoretical standpoint but also on how well they do in six different use cases
using randomized sequences of numbers. The measurements provide data on how
good they are in different real-life situations.
  It was found that bucket sort was faster than RADIX sort, but that bucket
sort uses more memory in most cases. The sorting algorithms performed faster
with smaller integers. The RADIX sort was not quicker with already sorted
inputs, but the bucket sort was."
"In this paper, we present a randomized polynomial-time approximation
algorithm for k-CSPd. In k-CSPd, we are given a set of predicates of arity k
over an alphabet of size d. Our goal is to find an assignment that maximizes
the number of satisfied constraints.
  Our algorithm has approximation factor Omega(kd/d^k) (when k > \Omega(log
d)). This bound is asymptotically optimal assuming the Unique Games Conjecture.
The best previously known algorithm has approximation factor Omega(k log
d/d^k).
  We also give an approximation algorithm for the boolean MAX k-CSP2 problem
with a slightly improved approximation guarantee."
"We introduce an online version of the multiselection problem, in which q
selection queries are requested on an unsorted array of n elements. We provide
the first online algorithm that is 1-competitive with Kaligosi et al. [ICALP
2005] in terms of comparison complexity. Our algorithm also supports online
search queries efficiently.
  We then extend our algorithm to the dynamic setting, while retaining online
functionality, by supporting arbitrary insertions and deletions on the array.
Assuming that the insertion of an element is immediately preceded by a search
for that element, we show that our dynamic online algorithm performs an optimal
number of comparisons, up to lower order terms and an additive O(n) term.
  For the external memory model, we describe the first online multiselection
algorithm that is O(1)-competitive. This result improves upon the work of
Sibeyn [Journal of Algorithms 2006] when q > m, where m is the number of blocks
that can be stored in main memory. We also extend it to support searches,
insertions, and deletions of elements efficiently."
"We study the problem of metrical service systems with multiple servers
(MSSMS), which generalizes two well-known problems -- the $k$-server problem,
and metrical service systems. The MSSMS problem is to service requests, each of
which is an $l$-point subset of a metric space, using $k$ servers, with the
objective of minimizing the total distance traveled by the servers.
  Feuerstein initiated a study of this problem by proving upper and lower
bounds on the deterministic competitive ratio for uniform metric spaces. We
improve Feuerstein's analysis of the upper bound and prove that his algorithm
achieves a competitive ratio of $k({{k+l}\choose{l}}-1)$. In the randomized
online setting, for uniform metric spaces, we give an algorithm which achieves
a competitive ratio $\mathcal{O}(k^3\log l)$, beating the deterministic lower
bound of ${{k+l}\choose{l}}-1$. We prove that any randomized algorithm for
MSSMS on uniform metric spaces must be $\Omega(\log kl)$-competitive. We then
prove an improved lower bound of ${{k+2l-1}\choose{k}}-{{k+l-1}\choose{k}}$ on
the competitive ratio of any deterministic algorithm for $(k,l)$-MSSMS, on
general metric spaces. In the offline setting, we give a pseudo-approximation
algorithm for $(k,l)$-MSSMS on general metric spaces, which achieves an
approximation ratio of $l$ using $kl$ servers. We also prove a matching
hardness result, that a pseudo-approximation with less than $kl$ servers is
unlikely, even for uniform metric spaces. For general metric spaces, we
highlight the limitations of a few popular techniques, that have been used in
algorithm design for the $k$-server problem and metrical service systems."
"We study a natural online variant of the replacement path problem. The
\textit{replacement path problem} asks to find for a given graph $G = (V,E)$,
two designated vertices $s,t\in V$ and a shortest $s$-$t$ path $P$ in $G$, a
\textit{replacement path} $P_e$ for every edge $e$ on the path $P$. The
replacement path $P_e$ is simply a shortest $s$-$t$ path in the graph, which
avoids the \textit{failed} edge $e$. We adapt this problem to deal with the
natural scenario, that the edge which failed is not known at the time of
solution implementation. Instead, our problem assumes that the identity of the
failed edge only becomes available when the routing mechanism tries to cross
the edge. This situation is motivated by applications in distributed networks,
where information about recent changes in the network is only stored locally,
and fault-tolerant optimization, where an adversary tries to delay the
discovery of the materialized scenario as much as possible. Consequently, we
define the \textit{online replacement path problem}, which asks to find a
nominal $s$-$t$ path $Q$ and detours $Q_e$ for every edge on the path $Q$, such
that the worst-case arrival time at the destination is minimized. Our main
contribution is a label setting algorithm, which solves the problem in
undirected graphs in time $O(m \log n)$ and linear space for all sources and a
single destination. We also present algorithms for extensions of the model to
any bounded number of failed edges."
"Frequency Count (FC) algorithm is considered as the static optimal algorithm
for the list accessing problem. In this paper, we have made a study of FC
algorithm and explore its limitation. Using the concept of weak look ahead, we
have proposed a novel Variant of Frequency Count (VFC) list accessing
algorithm. We have evaluated the performance of FC and our proposed VFC
algorithm experimentally using input data set from Calgary Corpus. Our
experiments show that for all request sequences and list generated from the
above data set VFC performs better than FC."
"List accessing problem has been studied as a problem of significant
theoretical and practical interest in the context of linear search. Various
list accessing algorithms have been proposed in the literature and their
performances have been analyzed theoretically and experimentally.
Move-To-Front(MTF),Transpose (TRANS) and Frequency Count (FC) are the three
primitive and widely used list accessing algorithms. Most of the other list
accessing algorithms are the variants of these three algorithms. As mentioned
in the literature as an open problem, direct bounds on the behavior and
performance of these list accessing algorithms are needed to allow realistic
comparisons. MTF has been proved to be the best performing online algorithm
till date in the literature for real life inputs with locality of reference.
Motivated by the above challenging research issue, in this paper, we have
generated four types of input request sequences corresponding to real life
inputs without locality of reference. Using these types of request sequences,
we have made an analytical study for evaluating the performance of MTF list
accessing algorithm to obtain some novel and interesting theoretical results."
"We investigate the behavior of data structures when the input and operations
are generated by an event graph. This model is inspired by Markov chains. We
are given a fixed graph G, whose nodes are annotated with operations of the
type insert, delete and query. The algorithm responds to the requests as it
encounters them during a (random or adversarial) walk in G. We study the limit
behavior of such a walk and give an efficient algorithm for recognizing which
structures can be generated. We also give a near-optimal algorithm for
successor searching if the event graph is a cycle and the walk is adversarial.
For a random walk, the algorithm becomes optimal."
"We describe a data structure that supports access, rank and select queries,
as well as symbol insertions and deletions, on a string $S[1,n]$ over alphabet
$[1..\sigma]$ in time $O(\lg n/\lg\lg n)$, which is optimal even on binary
sequences and in the amortized sense. Our time is worst-case for the queries
and amortized for the updates. This complexity is better than the best previous
ones by a $\Theta(1+\lg\sigma/\lg\lg n)$ factor. We also design a variant where
times are worst-case, yet rank and updates take $O(\lg n)$ time. Our structure
uses $nH_0(S)+o(n\lg\sigma) + O(\sigma\lg n)$ bits, where $H_0(S)$ is the
zero-order entropy of $S$. Finally, we pursue various extensions and
applications of the result."
"We introduce the following elementary scheduling problem. We are given a
collection of n jobs, where each job has an integer length as well as a set Ti
of time intervals in which it can be feasibly scheduled. Given a parameter B,
the processor can schedule up to B jobs at a timeslot t so long as it is
""active"" at t. The goal is to schedule all the jobs in the fewest number of
active timeslots. The machine consumes a fixed amount of energy per active
timeslot, regardless of the number of jobs scheduled in that slot (as long as
the number of jobs is non-zero). In other words, subject to all units of each
job being scheduled in its feasible region and at each slot at most B jobs
being scheduled, we are interested in minimizing the total time during which
the machine is active. We present a linear time algorithm for the case where
jobs are unit length and each Ti is a single interval. For general Ti, we show
that the problem is NP-complete even for B = 3. However when B = 2, we show
that it can be efficiently solved. In addition, we consider a version of the
problem where jobs have arbitrary lengths and can be preempted at any point in
time. For general B, the problem can be solved by linear programming. For B =
2, the problem amounts to finding a triangle-free 2-matching on a special
graph. We extend the algorithm of Babenko et. al. to handle our variant, and
also to handle non-unit length jobs. This yields an O(sqrt(L)m) time algorithm
to solve the preemptive scheduling problem for B = 2, where L is the sum of the
job lengths. We also show that for B = 2 and unit length jobs, the optimal
non-preemptive schedule has at most 4/3 times the active time of the optimal
preemptive schedule; this bound extends to several versions of the problem when
jobs have arbitrary length."
"We present a practical algorithm for the cyclic longest common subsequence
(CLCS) problem that runs in O(mn) time, where m and n are the lengths of the
two input strings. While this is not necessarily an asymptotic improvement over
the existing record, it is far simpler to understand and to implement."
"After reducing the undirected Hamiltonian cycle problem into the TSP problem
with cost 0 or 1, we developed an effective algorithm to compute the optimal
tour of the transformed TSP. Our algorithm is described as a growth process:
initially, constructing 4-vertexes optimal tour; next, one new vertex being
added into the optimal tour in such a way to obtain the new optimal tour; then,
repeating the previous step until all vertexes are included into the optimal
tour. This paper has shown that our constructive algorithm can solve the
undirected Hamiltonian cycle problem in polynomial time. According to
Cook-Levin theorem, we argue that we have provided a constructive proof of
P=NP."
"We study the problem of computing an ensemble of multiple sums where the
summands in each sum are indexed by subsets of size $p$ of an $n$-element
ground set. More precisely, the task is to compute, for each subset of size $q$
of the ground set, the sum over the values of all subsets of size $p$ that are
disjoint from the subset of size $q$. We present an arithmetic circuit that,
without subtraction, solves the problem using $O((n^p+n^q)\log n)$ arithmetic
gates, all monotone; for constant $p$, $q$ this is within the factor $\log n$
of the optimal. The circuit design is based on viewing the summation as a ""set
nucleation"" task and using a tree-projection approach to implement the
nucleation. Applications include improved algorithms for counting heaviest
$k$-paths in a weighted graph, computing permanents of rectangular matrices,
and dynamic feature selection in machine learning."
"Large data sets are increasingly common in cloud and virtualized
environments. For example, transfers of multiple gigabytes are commonplace, as
are replicated blocks of such sizes. There is a need for fast error-correction
or data reconciliation in such settings even when the expected number of errors
is small.
  Motivated by such cloud reconciliation problems, we consider error-correction
schemes designed for large data, after explaining why previous approaches
appear unsuitable. We introduce Biff codes, which are based on Bloom filters
and are designed for large data. For Biff codes with a message of length $L$
and $E$ errors, the encoding time is $O(L)$, decoding time is $O(L + E)$ and
the space overhead is $O(E)$. Biff codes are low-density parity-check codes;
they are similar to Tornado codes, but are designed for errors instead of
erasures. Further, Biff codes are designed to be very simple, removing any
explicit graph structures and based entirely on hash tables. We derive Biff
codes by a simple reduction from a set reconciliation algorithm for a recently
developed data structure, invertible Bloom lookup tables. While the underlying
theory is extremely simple, what makes this code especially attractive is the
ease with which it can be implemented and the speed of decoding. We present
results from a prototype implementation that decodes messages of 1 million
words with thousands of errors in well under a second."
"In the Split Vertex Deletion problem, given a graph G and an integer k, we
ask whether one can delete k vertices from the graph G to obtain a split graph
(i.e., a graph, whose vertex set can be partitioned into two sets: one inducing
a clique and the second one inducing an independent set). In this paper we
study fixed-parameter algorithms for Split Vertex Deletion parameterized by k:
we show that, up to a factor quasipolynomial in k and polynomial in n, the
Split Vertex Deletion problem can be solved in the same time as the
well-studied Vertex Cover problem. Plugging the currently best fixed-parameter
algorithm for Vertex Cover due to Chen et al. [TCS 2010], we obtain an
algorithm that solves Split Vertex Deletion in time O(1.2738^k * k^O(log k) +
n^O(1)).
  To achieve our goal, we prove the following structural result that may be of
independent interest: for any graph G we may compute a family P of size n^O(log
n) containing partitions of V(G) into two parts, such for any two disjoint
subsets X_C, X_I of V(G) where G[X_C] is a clique and G[X_I] is an independent
set, there is a partition in P which contains all vertices of X_C on one side
and all vertices of X_I on the other."
"In the Edge-Disjoint Paths with Congestion problem (EDPwC), we are given an
undirected n-vertex graph G, a collection M={(s_1,t_1),...,(s_k,t_k)} of demand
pairs and an integer c. The goal is to connect the maximum possible number of
the demand pairs by paths, so that the maximum edge congestion - the number of
paths sharing any edge - is bounded by c. When the maximum allowed congestion
is c=1, this is the classical Edge-Disjoint Paths problem (EDP).
  The best current approximation algorithm for EDP achieves an $O(\sqrt
n)$-approximation, by rounding the standard multi-commodity flow relaxation of
the problem. This matches the $\Omega(\sqrt n)$ lower bound on the integrality
gap of this relaxation. We show an $O(poly log k)$-approximation algorithm for
EDPwC with congestion c=2, by rounding the same multi-commodity flow
relaxation. This gives the best possible congestion for a sub-polynomial
approximation of EDPwC via this relaxation. Our results are also close to
optimal in terms of the number of pairs routed, since EDPwC is known to be hard
to approximate to within a factor of $\tilde{\Omega}((\log n)^{1/(c+1)})$ for
any constant congestion c. Prior to our work, the best approximation factor for
EDPwC with congestion 2 was $\tilde O(n^{3/7})$, and the best algorithm
achieving a polylogarithmic approximation required congestion 14."
"Local Search is one of the fundamental approaches to combinatorial
optimization and it is used throughout AI. Several local search algorithms are
based on searching the k-exchange neighborhood. This is the set of solutions
that can be obtained from the current solution by exchanging at most k
elements. As a rule of thumb, the larger k is, the better are the chances of
finding an improved solution. However, for inputs of size n, a na\""ive
brute-force search of the k-exchange neighborhood requires n to the power of
O(k) time, which is not practical even for very small values of k.
  Fellows et al. (IJCAI 2009) studied whether this brute-force search is
avoidable and gave positive and negative answers for several combinatorial
problems. They used the notion of local search in a strict sense. That is, an
improved solution needs to be found in the k-exchange neighborhood even if a
global optimum can be found efficiently.
  In this paper we consider a natural relaxation of local search, called
permissive local search (Marx and Schlotter, IWPEC 2009) and investigate
whether it enhances the domain of tractable inputs. We exemplify this approach
on a fundamental combinatorial problem, Vertex Cover. More precisely, we show
that for a class of inputs, finding an optimum is hard, strict local search is
hard, but permissive local search is tractable.
  We carry out this investigation in the framework of parameterized complexity."
"Meta-heuristics are frequently used to tackle NP-hard combinatorial
optimization problems. With this paper we contribute to the understanding of
the success of 2-opt based local search algorithms for solving the traveling
salesman problem (TSP). Although 2-opt is widely used in practice, it is hard
to understand its success from a theoretical perspective. We take a statistical
approach and examine the features of TSP instances that make the problem either
hard or easy to solve. As a measure of problem difficulty for 2-opt we use the
approximation ratio that it achieves on a given instance. Our investigations
point out important features that make TSP instances hard or easy to be
approximated by 2-opt."
"The \emph{file caching} problem is defined as follows. Given a cache of size
$k$ (a positive integer), the goal is to minimize the total retrieval cost for
the given sequence of requests to files. A file $f$ has size $size(f)$ (a
positive integer) and retrieval cost $cost(f)$ (a non-negative number) for
bringing the file into the cache. A \emph{miss} or \emph{fault} occurs when the
requested file is not in the cache and the file has to be retrieved into the
cache by paying the retrieval cost, and some other file may have to be removed
(\emph{evicted}) from the cache so that the total size of the files in the
cache does not exceed $k$.
  We study the following variants of the online file caching problem.
\textbf{\emph{Caching with Rental Cost} (or \emph{Rental Caching})}: There is a
rental cost $\lambda$ (a positive number) for each file in the cache at each
time unit. The goal is to minimize the sum of the retrieval costs and the
rental costs. \textbf{\emph{Caching with Zapping}}: A file can be \emph{zapped}
by paying a zapping cost $N \ge 1$. Once a file is zapped, all future requests
of the file don't incur any cost. The goal is to minimize the sum of the
retrieval costs and the zapping costs.
  We study these two variants and also the variant which combines these two
(rental caching with zapping). We present deterministic lower and upper bounds
in the competitive-analysis framework. We study and extend the online covering
algorithm from \citep{young02online} to give deterministic online algorithms.
We also present randomized lower and upper bounds for some of these problems."
"An algorithm for the evaluation of the complex exponential function is
proposed which is quasi-linear in time and linear in space. This algorithm is
based on a modified binary splitting method for the hypergeometric series and a
modified Karatsuba method for the fast evaluation of the exponential function.
The time complexity of this algorithm is equal to that of the ordinary
algorithm for the evaluation of the exponential function based on the series
expansion: O(M(n)log(n)^2)."
"In this paper, we study the role non-adaptivity plays in maintaining dynamic
data structures. Roughly speaking, a data structure is non-adaptive if the
memory locations it reads and/or writes when processing a query or update
depend only on the query or update and not on the contents of previously read
cells. We study such non-adaptive data structures in the cell probe model. This
model is one of the least restrictive lower bound models and in particular,
cell probe lower bounds apply to data structures developed in the popular
word-RAM model. Unfortunately, this generality comes at a high cost: the
highest lower bound proved for any data structure problem is only
polylogarithmic. Our main result is to demonstrate that one can in fact obtain
polynomial cell probe lower bounds for non-adaptive data structures.
  To shed more light on the seemingly inherent polylogarithmic lower bound
barrier, we study several different notions of non-adaptivity and identify key
properties that must be dealt with if we are to prove polynomial lower bounds
without restrictions on the data structures.
  Finally, our results also unveil an interesting connection between data
structures and depth-2 circuits. This allows us to translate conjectured hard
data structure problems into good candidates for high circuit lower bounds; in
particular, in the area of linear circuits for linear operators. Building on
lower bound proofs for data structures in slightly more restrictive models, we
also present a number of properties of linear operators which we believe are
worth investigating in the realm of circuit lower bounds."
"A local property reconstructor for a graph property is an algorithm which,
given oracle access to the adjacency list of a graph that is ""close"" to having
the property, provides oracle access to the adjacency matrix of a ""correction""
of the graph, i.e. a graph which has the property and is close to the given
graph. For this model, we achieve local property reconstructors for the
properties of connectivity and $k$-connectivity in undirected graphs, and the
property of strong connectivity in directed graphs. Along the way, we present a
method of transforming a local reconstructor (which acts as a ""adjacency matrix
oracle"" for the corrected graph) into an ""adjacency list oracle"". This allows
us to recursively use our local reconstructor for $(k-1)$-connectivity to
obtain a local reconstructor for $k$-connectivity.
  We also extend this notion of local property reconstruction to parametrized
graph properties (for instance, having diameter at most $D$ for some parameter
$D$) and require that the corrected graph has the property with parameter close
to the original. We obtain a local reconstructor for the low diameter property,
where if the original graph is close to having diameter $D$, then the corrected
graph has diameter roughly 2D.
  We also exploit a connection between local property reconstruction and
property testing, observed by Brakerski, to obtain new tolerant property
testers for all of the aforementioned properties. Except for the one for
connectivity, these are the first tolerant property testers for these
properties."
"In this paper we consider a generalization of the classical k-center problem
with capacities. Our goal is to select k centers in a graph, and assign each
node to a nearby center, so that we respect the capacity constraints on
centers. The objective is to minimize the maximum distance a node has to travel
to get to its assigned center. This problem is NP-hard, even when centers have
no capacity restrictions and optimal factor 2 approximation algorithms are
known. With capacities, when all centers have identical capacities, a 6
approximation is known with no better lower bounds than for the infinite
capacity version.
  While many generalizations and variations of this problem have been studied
extensively, no progress was made on the capacitated version for a general
capacity function. We develop the first constant factor approximation algorithm
for this problem. Our algorithm uses an LP rounding approach to solve this
problem, and works for the case of non-uniform hard capacities, when multiple
copies of a node may not be chosen and can be extended to the case when there
is a hard bound on the number of copies of a node that may be selected. In
addition we establish a lower bound on the integrality gap of 7(5) for
non-uniform (uniform) hard capacities. In addition we prove that if there is a
(3-eps)-factor approximation for this problem then P=NP.
  Finally, for non-uniform soft capacities we present a much simpler
11-approximation algorithm, which we find as one more evidence that hard
capacities are much harder to deal with."
"In this work, we apply a common economic tool, namely money, to coordinate
network packets. In particular, we present a network economy, called
PacketEconomy, where each flow is modeled as a population of rational network
packets, and these packets can self-regulate their access to network resources
by mutually trading their positions in router queues. Every packet of the
economy has its price, and this price determines if and when the packet will
agree to buy or sell a better position. We consider a corresponding Markov
model of trade and show that there are Nash equilibria (NE) where queue
positions and money are exchanged directly between the network packets. This
simple approach, interestingly, delivers improvements even when fiat money is
used. We present theoretical arguments and experimental results to support our
claims."
"The problem of Text Indexing is a fundamental algorithmic problem in which
one wishes to preprocess a text in order to quickly locate pattern queries
within the text. In the ever evolving world of dynamic and on-line data, there
is also a need for developing solutions to index texts which arrive on-line,
i.e. a character at a time, and still be able to quickly locate said patterns.
In this paper, a new solution for on-line indexing is presented by providing an
on-line suffix tree construction in $O(\log \log n + \log\log |\Sigma|)$
worst-case expected time per character, where $n$ is the size of the string,
and $\Sigma$ is the alphabet. This improves upon all previously known on-line
suffix tree constructions for general alphabets, at the cost of having the run
time in expectation.
  The main idea is to reduce the problem of constructing a suffix tree on-line
to an interesting variant of the order maintenance problem, which may be of
independent interest. In the famous order maintenance problem, one wishes to
maintain a dynamic list $L$ of size $n$ under insertions, deletions, and order
queries. In an order query, one is given two nodes from $L$ and must determine
which node precedes the other in $L$. In the Predecessor search on Dynamic
Subsets of an Ordered Dynamic List problem (POLP) it is also necessary to
maintain dynamic subsets of $L$ such that given some $u\in L$ it will be
possible to quickly locate the predecessor of $u$ in any subset. This paper
provides an efficient data structure capable of solving the POLP with
worst-case expected bounds that match the currently best known bounds for
predecessor search in the RAM model, improving over a solution which may be
implicitly obtained from Dietz [Die89].
  Furthermore, this paper improves or simplifies bounds for several additional
applications, including fully-persistent arrays and the Order-Maintenance
Problem."
"In the Constrained Fault-Tolerant Resource Allocation (FTRA) problem, we are
given a set of sites containing facilities as resources, and a set of clients
accessing these resources. Specifically, each site i is allowed to open at most
R_i facilities with cost f_i for each opened facility. Each client j requires
an allocation of r_j open facilities and connecting j to any facility at site i
incurs a connection cost c_ij. The goal is to minimize the total cost of this
resource allocation scenario.
  FTRA generalizes the Unconstrained Fault-Tolerant Resource Allocation
(FTRA_{\infty}) [18] and the classical Fault-Tolerant Facility Location (FTFL)
[13] problems: for every site i, FTRA_{\infty} does not have the constraint
R_i, whereas FTFL sets R_i=1. These problems are said to be uniform if all
r_j's are the same, and general otherwise.
  For the general metric FTRA, we first give an LP-rounding algorithm achieving
the approximation ratio of 4. Then we show the problem reduces to FTFL,
implying the ratio of 1.7245 from [3]. For the uniform FTRA, we provide a
1.52-approximation primal-dual algorithm in O(n^4) time, where n is the total
number of sites and clients. We also consider the Constrained Fault-Tolerant
k-Resource Allocation (k-FTRA) problem where additionally the total number of
facilities can be opened across all sites is bounded by k. For the uniform
k-FTRA, we give the first constant-factor approximation algorithm with a factor
of 4. Note that the above results carry over to FTRA_{\infty} and
k-FTRA_{\infty}."
"It is shown that the counting function of n Boolean variables can be
implemented with the formulae of size O(n^3.06) over the basis of all 2-input
Boolean functions and of size O(n^4.54) over the standard basis. The same
bounds follow for the complexity of any threshold symmetric function of n
variables and particularly for the majority function. Any bit of the product of
binary numbers of length n can be computed by formulae of size O(n^4.06) or
O(n^5.54) depending on basis. Incidentally the bounds O(n^3.23) and O(n^4.82)
on the formula size of any symmetric function of n variables with respect to
the basis are obtained."
"Many combinatorial problems involving weights can be formulated as a
so-called ranged problem. That is, their input consists of a universe $U$, a
(succinctly-represented) set family $\mathcal{F} \subseteq 2^{U}$, a weight
function $\omega:U \rightarrow \{1,...,N\}$, and integers $0 \leq l \leq u \leq
\infty$. Then the problem is to decide whether there is an $X \in \mathcal{F}$
such that $l \leq \sum_{e \in X}\omega(e) \leq u$. Well-known examples of such
problems include Knapsack, Subset Sum, Maximum Matching, and Traveling
Salesman. In this paper, we develop a generic method to transform a ranged
problem into an exact problem (i.e. a ranged problem for which $l=u$). We show
that our method has several intriguing applications in exact exponential
algorithms and parameterized complexity, namely:
  - In exact exponential algorithms, we present new insight into whether Subset
Sum and Knapsack have efficient algorithms in both time and space. In
particular, we show that the time and space complexity of Subset Sum and
Knapsack are equivalent up to a small polynomial factor in the input size. We
also give an algorithm that solves sparse instances of Knapsack efficiently in
terms of space and time. - In parameterized complexity, we present the first
kernelization results on weighted variants of several well-known problems. In
particular, we show that weighted variants of Vertex Cover, Dominating Set,
Traveling Salesman and Knapsack all admit polynomial randomized Turing kernels
when parameterized by $|U|$.
  Curiously, our method relies on a technique more commonly found in
approximation algorithms."
"We present Masai, a read mapper representing the state of the art in terms of
speed and sensitivity. Our tool is an order of magnitude faster than RazerS 3
and mrFAST, 2--3 times faster and more accurate than Bowtie 2 and BWA. The
novelties of our read mapper are filtration with approximate seeds and a method
for multiple backtracking. Approximate seeds, compared to exact seeds, increase
filtration specificity while preserving sensitivity. Multiple backtracking
amortizes the cost of searching a large set of seeds by taking advantage of the
repetitiveness of next-generation sequencing data. Combined together, these two
methods significantly speed up approximate search on genomic datasets. Masai is
implemented in C++ using the SeqAn library. The source code is distributed
under the BSD license and binaries for Linux, Mac OS X and Windows can be
freely downloaded from http://www.seqan.de/projects/masai."
"In this paper we study the problem of finding a maximum induced d-degenerate
subgraph in a given n-vertex graph from the point of view of exact algorithms.
We show that for any fixed d one can find a maximum induced d-degenerate
subgraph in randomized (2-eps_d)^n n^O(1) time, for some constant eps_d>0
depending only on d. Moreover, our algorithm can be used to sample
inclusion-wise maximal induced d-degenerate subgraphs in such a manner that
every such subgraph is output with probability at least (2-eps_d)^-n; hence, we
prove that their number is bounded by (2-eps_d)^n."
"Let P be a set of n points in R^2. Given a rectangle Q = [\alpha_1, \alpha_2]
x [\beta_1, \beta_2], a range skyline query returns the maxima of the points in
P \cap Q. An important variant is the so-called top-open queries, where Q is a
3-sided rectangle whose upper edge is grounded at y = \infty (that is, \beta_2
= \infty). These queries are crucial in numerous database applications. In
internal memory, extensive research has been devoted to designing data
structures that can answer such queries efficiently. In contrast, currently
there is no clear understanding about their exact complexities in external
memory.
  This paper presents several structures of linear size for answering the above
queries with the optimal I/O cost. We show that a top-open query can be solved
in O(log_B(n) + k/B) I/Os, where B is the block size and k is the number of
points in the query result. The query cost can be made O(log log_B(U) + k/B)
when the data points lie in a U x U grid for some integer U >= n, and further
lowered to O(1 + k/B) if U = O(n). The same efficiency also applies to 3-sided
queries where Q is a right-open rectangle. However, the hardness of the problem
increases if Q is a left- or bottom-open 3-sided rectangle. We prove that any
linear-size structure must perform \Omega((n/B)^\eps + k/B) I/Os to solve such
a query in the worst case, where \eps > 0 can be an arbitrarily small constant.
In fact, left- and right-open queries are just as difficult as general
(4-sided) queries, for which we give a linear-size structure with query time
O((n/B)^\eps + k/B). Interestingly, this indicates that 4-sided range skyline
queries have exactly the same hardness as 4-sided range reporting (where the
goal is to report simply the whole P \cap Q). That is, the skyline requirement
does not alter the problem difficulty at all."
"We present a structure in external memory for ""top-k range reporting"", which
uses linear space, answers a query in O(lg_B n + k/B) I/Os, and supports an
update in O(lg_B n) amortized I/Os, where n is the input size, and B is the
block size. This improves the state of the art which incurs O(lg^2_B n)
amortized I/Os per update."
"We prove that the simplex method with the highest gain/most-negative-reduced
cost pivoting rule converges in strongly polynomial time for deterministic
Markov decision processes (MDPs) regardless of the discount factor. For a
deterministic MDP with n states and m actions, we prove the simplex method runs
in O(n^3m^2log^2 n) iterations if the discount factor is uniform and
O(n^5m^3log^2 n) iterations if each action has a distinct discount factor.
Previously the simplex method was known to run in polynomial time only for
discounted MDPs where the discount was bounded away from 1 [Ye11].
  Unlike in the discounted case, the algorithm does not greedily converge to
the optimum, and we require a more complex measure of progress. We identify a
set of layers in which the values of primal variables must lie and show that
the simplex method always makes progress optimizing one layer, and when the
upper layer is updated the algorithm makes a substantial amount of progress. In
the case of nonuniform discounts, we define a polynomial number of ""milestone""
policies and we prove that, while the objective function may not improve
substantially overall, the value of at least one dual variable is always making
progress towards some milestone, and the algorithm will reach the next
milestone in a polynomial number of steps."
"We present a novel approximation algorithm for $k$-median that achieves an
approximation guarantee of
  $1+\sqrt{3}+\epsilon$, improving upon the decade-old ratio of $3+\epsilon$.
Our approach is based on two components, each of which, we believe, is of
independent interest.
  First, we show that in order to give an $\alpha$-approximation algorithm for
$k$-median, it is sufficient to give a \emph{pseudo-approximation algorithm}
that finds an $\alpha$-approximate solution by opening $k+O(1)$ facilities.
This is a rather surprising result as there exist instances for which opening
$k+1$ facilities may lead to a significant smaller cost than if only $k$
facilities were opened.
  Second, we give such a pseudo-approximation algorithm with $\alpha=
1+\sqrt{3}+\epsilon$. Prior to our work, it was not even known whether opening
$k + o(k)$ facilities would help improve the approximation ratio."
"We revisit the longest common extension (LCE) problem, that is, preprocess a
string $T$ into a compact data structure that supports fast LCE queries. An LCE
query takes a pair $(i,j)$ of indices in $T$ and returns the length of the
longest common prefix of the suffixes of $T$ starting at positions $i$ and $j$.
We study the time-space trade-offs for the problem, that is, the space used for
the data structure vs. the worst-case time for answering an LCE query. Let $n$
be the length of $T$. Given a parameter $\tau$, $1 \leq \tau \leq n$, we show
how to achieve either $O(\infrac{n}{\sqrt{\tau}})$ space and $O(\tau)$ query
time, or $O(\infrac{n}{\tau})$ space and $O(\tau \log({|\LCE(i,j)|}/{\tau}))$
query time, where $|\LCE(i,j)|$ denotes the length of the LCE returned by the
query. These bounds provide the first smooth trade-offs for the LCE problem and
almost match the previously known bounds at the extremes when $\tau=1$ or
$\tau=n$. We apply the result to obtain improved bounds for several
applications where the LCE problem is the computational bottleneck, including
approximate string matching and computing palindromes. We also present an
efficient technique to reduce LCE queries on two strings to one string.
Finally, we give a lower bound on the time-space product for LCE data
structures in the non-uniform cell probe model showing that our second
trade-off is nearly optimal."
"We show that the two problems of computing the permanent of an $n\times n$
matrix of $\operatorname{poly}(n)$-bit integers and counting the number of
Hamiltonian cycles in a directed $n$-vertex multigraph with
$\operatorname{exp}(\operatorname{poly}(n))$ edges can be reduced to relatively
few smaller instances of themselves. In effect we derive the first
deterministic algorithms for these two problems that run in $o(2^n)$ time in
the worst case. Classic $\operatorname{poly}(n)2^n$ time algorithms for the two
problems have been known since the early 1960's. Our algorithms run in
$2^{n-\Omega(\sqrt{n/\log n})}$ time."
"We consider differentially private approximate singular vector computation.
Known worst-case lower bounds show that the error of any differentially private
algorithm must scale polynomially with the dimension of the singular vector. We
are able to replace this dependence on the dimension by a natural parameter
known as the coherence of the matrix that is often observed to be significantly
smaller than the dimension both theoretically and empirically. We also prove a
matching lower bound showing that our guarantee is nearly optimal for every
setting of the coherence parameter. Notably, we achieve our bounds by giving a
robust analysis of the well-known power iteration algorithm, which may be of
independent interest. Our algorithm also leads to improvements in worst-case
settings and to better low-rank approximations in the spectral norm."
"A total dominating set of a graph $G=(V,E)$ is a subset $D \subseteq V$ such
that every vertex in $V$ is adjacent to some vertex in $D$. Finding a total
dominating set of minimum size is NP-hard on planar graphs and W[2]-complete on
general graphs when parameterized by the solution size. By the meta-theorem of
Bodlaender et al. [J. ACM, 2016], there exists a linear kernel for Total
Dominating Set on graphs of bounded genus. Nevertheless, it is not clear how
such a kernel can be effectively constructed, and how to obtain explicit
reduction rules with reasonably small constants. Following the approach of
Alber et al. [J. ACM, 2004], we provide an explicit kernel for Total Dominating
Set on planar graphs with at most $410k$ vertices, where $k$ is the size of the
solution. This result complements several known constructive linear kernels on
planar graphs for other domination problems such as Dominating Set, Edge
Dominating Set, Efficient Dominating Set, Connected Dominating Set, or Red-Blue
Dominating Set."
"Linear sketches are powerful algorithmic tools that turn an n-dimensional
input into a concise lower-dimensional representation via a linear
transformation. Such sketches have seen a wide range of applications including
norm estimation over data streams, compressed sensing, and distributed
computing. In almost any realistic setting, however, a linear sketch faces the
possibility that its inputs are correlated with previous evaluations of the
sketch. Known techniques no longer guarantee the correctness of the output in
the presence of such correlations. We therefore ask: Are linear sketches
inherently non-robust to adaptively chosen inputs? We give a strong affirmative
answer to this question. Specifically, we show that no linear sketch
approximates the Euclidean norm of its input to within an arbitrary
multiplicative approximation factor on a polynomial number of adaptively chosen
inputs. The result remains true even if the dimension of the sketch is d = n -
o(n) and the sketch is given unbounded computation time. Our result is based on
an algorithm with running time polynomial in d that adaptively finds a
distribution over inputs on which the sketch is incorrect with constant
probability. Our result implies several corollaries for related problems
including lp-norm estimation and compressed sensing. Notably, we resolve an
open problem in compressed sensing regarding the feasibility of l2/l2-recovery
guarantees in the presence of computationally bounded adversaries."
"We study several stochastic combinatorial problems, including the expected
utility maximization problem, the stochastic knapsack problem and the
stochastic bin packing problem. A common technical challenge in these problems
is to optimize some function of the sum of a set of random variables. The
difficulty is mainly due to the fact that the probability distribution of the
sum is the convolution of a set of distributions, which is not an easy
objective function to work with. To tackle this difficulty, we introduce the
Poisson approximation technique. The technique is based on the Poisson
approximation theorem discovered by Le Cam, which enables us to approximate the
distribution of the sum of a set of random variables using a compound Poisson
distribution.
  We first study the expected utility maximization problem introduced recently
[Li and Despande, FOCS11]. For monotone and Lipschitz utility functions, we
obtain an additive PTAS if there is a multidimensional PTAS for the
multi-objective version of the problem, strictly generalizing the previous
result.
  For the stochastic bin packing problem (introduced in [Kleinberg, Rabani and
Tardos, STOC97]), we show there is a polynomial time algorithm which uses at
most the optimal number of bins, if we relax the size of each bin and the
overflow probability by eps.
  For stochastic knapsack, we show a 1+eps-approximation using eps extra
capacity, even when the size and reward of each item may be correlated and
cancelations of items are allowed. This generalizes the previous work [Balghat,
Goel and Khanna, SODA11] for the case without correlation and cancelation. Our
algorithm is also simpler. We also present a factor 2+eps approximation
algorithm for stochastic knapsack with cancelations. the current known
approximation factor of 8 [Gupta, Krishnaswamy, Molinaro and Ravi, FOCS11]."
"The paper presents an algebraic technique for derivation of fast discrete
cosine transform (DCT) algorithms. The technique is based on the algebraic
signal processing theory (ASP). In ASP a DCT associates with a polynomial
algebra C[x]/p(x). A fast algorithm is obtained as a stepwise decomposition of
C[x]/p(x). In order to reveal the connection between derivation of fast DCT
algorithms and Galois theory we define polynomial algebra over the field of
rational numbers Q instead of complex C. The decomposition of Q[x]/p(x)
requires the extension of the base field Q to splitting field E of polynomial
p(x). Galois theory is used to find intermediate subfields L_i in which
polynomial p(x) is factored. Based on this factorization fast DCT algorithm is
derived."
"The paper investigates relationship between algebraic expressions and graphs.
We consider a digraph called a square rhomboid that is an example of
non-series-parallel graphs. Our intention is to simplify the expressions of
square rhomboids and eventually find their shortest representations. With that
end in view, we describe the new algorithm for generating square rhomboid
expressions, which improves on our previous algorithms."
"The flow accumulation problem for grid terrains takes as input a matrix of
flow directions, that specifies for each cell of the grid to which of its eight
neighbours any incoming water would flow. The problem is to compute, for each
cell c, from how many cells of the terrain water would reach c. We show that
this problem can be solved in O(scan(N)) I/Os for a terrain of N cells. Taking
constant factors in the I/O-efficiency into account, our algorithm may be an
order of magnitude faster than the previously known algorithm that is based on
time-forward processing and needs O(sort(N)) I/Os."
"Given a graph of which the n vertices form a regular two-dimensional grid,
and in which each (possibly weighted and/or directed) edge connects a vertex to
one of its eight neighbours, the following can be done in O(scan(n)) I/Os,
provided M = Omega(B^2): computation of shortest paths with non-negative edge
weights from a single source, breadth-first traversal, computation of a minimum
spanning tree, topological sorting, time-forward processing (if the input is a
plane graph), and an Euler tour (if the input graph is a tree). The
minimum-spanning tree algorithm is cache-oblivious. The best previously
published algorithms for these problems need Theta(sort(n)) I/Os. Estimates of
the actual I/O volume show that the new algorithms may often be very efficient
in practice."
"Many complex questions in biology, physics, and mathematics can be mapped to
the graph isomorphism problem and the closely related graph automorphism
problem. In particular, these problems appear in the context of network
visualization, computational logic, structure recognition, and dynamics of
complex systems. Both problems have previously been suspected, but not proven,
to be NP-complete. In this paper we propose an algorithm that solves both graph
automorphism and isomorphism problems in polynomial time. The algorithm can be
easily implemented and thus opens up a wide range of applications."
"Since its introduction prediction by partial matching (PPM) has always been a
de facto gold standard in lossless text compression, where many variants
improving the compression ratio and speed have been proposed. However, reducing
the high space requirement of PPM schemes did not gain that much attention.
This study focuses on reducing the memory consumption of PPM via the recently
proposed compressed context modeling that uses the compressed representations
of contexts in the statistical model. Differently from the classical context
definition as the string of the preceding characters at a particular position,
CCM considers context as the amount of preceding information that is actually
the bit stream composed by compressing the previous symbols. We observe that by
using the CCM, the data structures, particularly the context trees, can be
implemented in smaller space, and present a trade-off between the compression
ratio and the space requirement. The experiments conducted showed that this
trade-off is especially beneficial in low orders with approximately 20 - 25
percent gain in memory by a sacrifice of up to nearly 7 percent loss in
compression ratio."
"We study the unsplittable flow on a path problem (UFP) where we are given a
path with non-negative edge capacities and tasks, which are characterized by a
subpath, a demand, and a profit. The goal is to find the most profitable subset
of tasks whose total demand does not violate the edge capacities. This problem
naturally arises in many settings such as bandwidth allocation, resource
constrained scheduling, and interval packing.
  A natural task classification defines the size of a task i to be the ratio
delta between the demand of i and the minimum capacity of any edge used by i.
If all tasks have sufficiently small delta, the problem is already well
understood and there is a 1+eps approximation. For the complementary
setting---instances whose tasks all have large delta---much remains unknown,
and the best known polynomial-time procedure gives only (for any constant
delta>0) an approximation ratio of 6+eps.
  In this paper we present a polynomial time 1+eps approximation for the latter
setting. Key to this result is a complex geometrically inspired dynamic
program. Here each task is represented as a segment underneath the capacity
curve, and we identify a proper maze-like structure so that each passage of the
maze is crossed by only O(1) tasks in the computed solution. In combination
with the known PTAS for delta-small tasks, our result implies a 2+eps
approximation for UFP, improving on the previous best 7+eps approximation
[Bonsma et al., FOCS 2011]. We remark that our improved approximation factor
matches the best known approximation ratio for the considerably easier special
case of uniform edge capacities."
"There has been significant interest and progress recently in algorithms that
solve regression problems involving tall and thin matrices in input sparsity
time. These algorithms find shorter equivalent of a n*d matrix where n >> d,
which allows one to solve a poly(d) sized problem instead. In practice, the
best performances are often obtained by invoking these routines in an iterative
fashion. We show these iterative methods can be adapted to give theoretical
guarantees comparable and better than the current state of the art.
  Our approaches are based on computing the importances of the rows, known as
leverage scores, in an iterative manner. We show that alternating between
computing a short matrix estimate and finding more accurate approximate
leverage scores leads to a series of geometrically smaller instances. This
gives an algorithm that runs in $O(nnz(A) + d^{\omega + \theta} \epsilon^{-2})$
time for any $\theta > 0$, where the $d^{\omega + \theta}$ term is comparable
to the cost of solving a regression problem on the small approximation. Our
results are built upon the close connection between randomized matrix
algorithms, iterative methods, and graph sparsification."
"Belief propagation (BP) is a message-passing heuristic for statistical
inference in graphical models such as Bayesian networks and Markov random
fields. BP is used to compute marginal distributions or maximum likelihood
assignments and has applications in many areas, including machine learning,
image processing, and computer vision. However, the theoretical understanding
of the performance of BP is unsatisfactory.
  Recently, BP has been applied to combinatorial optimization problems. It has
been proved that BP can be used to compute maximum-weight matchings and
minimum-cost flows for instances with a unique optimum. The number of
iterations needed for this is pseudo-polynomial and hence BP is not efficient
in general.
  We study belief propagation in the framework of smoothed analysis and prove
that with high probability the number of iterations needed to compute
maximum-weight matchings and minimum-cost flows is bounded by a polynomial if
the weights/costs of the edges are randomly perturbed. To prove our upper
bounds, we use an isolation lemma by Beier and V\""{o}cking (SIAM J. Comput.
2006) for matching and generalize an isolation lemma for min-cost flow by
Gamarnik, Shah, and Wei (Operations Research, 2012). We also prove almost
matching lower tail bounds for the number of iterations that BP needs to
converge."
"We present a new, simple, and efficient approach for computing the Lempel-Ziv
(LZ77) factorization of a string in linear time, based on suffix arrays.
Computational experiments on various data sets show that our approach
constantly outperforms the currently fastest algorithm LZ OG (Ohlebusch and Gog
2011), and can be up to 2 to 3 times faster in the processing after obtaining
the suffix array, while requiring the same or a little more space."
"Dynamic dictionary-based compression schemes are the most daily used data
compression schemes since they appeared in the foundational papers of Ziv and
Lempel in 1977, commonly referred to as LZ77. Their work is the base of
Deflate, gZip, WinZip, 7Zip and many others compression software. All of those
compression schemes use variants of the greedy approach to parse the text into
dictionary phrases. Greedy parsing optimality was proved by Cohn et al. (1996)
for fixed length code and unbounded dictionaries. The optimality of the greedy
parsing was never proved for bounded size dictionary which actually all of
those schemes require. We define the suffix-closed property for dynamic
dictionaries and we show that any LZ77-based dictionary, including the bounded
variants, satisfy this property. Under this condition we prove the optimality
of the greedy parsing as a variant of the proof by Cohn et al."
"Given strings $P$ of length $m$ and $T$ of length $n$ over an alphabet of
size $\sigma$, the string matching with $k$-mismatches problem is to find the
positions of all the substrings in $T$ that are at Hamming distance at most $k$
from $P$. If $T$ can be read only one character at the time the best known
bounds are $O(n\sqrt{k\log k})$ and $O(n + n\sqrt{k/w}\log k)$ in the word-RAM
model with word length $w$. In the RAM models (including $AC^0$ and word-RAM)
it is possible to read up to $\floor{w / \log \sigma}$ characters in constant
time if the characters of $T$ are encoded using $\ceil{\log \sigma}$ bits. The
only solution for $k$-mismatches in packed text works in $O((n \log\sigma/\log
n)\ceil{m \log (k + \log n / \log\sigma) / w} + n^{\varepsilon})$ time, for any
$\varepsilon > 0$. We present an algorithm that runs in time
$O(\frac{n}{\floor{w/(m\log\sigma)}} (1 + \log \min(k,\sigma) \log m /
\log\sigma))$ in the $AC^0$ model if $m=O(w / \log\sigma)$ and $T$ is given
packed. We also describe a simpler variant that runs in time
$O(\frac{n}{\floor{w/(m\log\sigma)}}\log \min(m, \log w / \log\sigma))$ in the
word-RAM model. The algorithms improve the existing bound for $w =
\Omega(\log^{1+\epsilon}n)$, for any $\epsilon > 0$. Based on the introduced
technique, we present algorithms for several other approximate matching
problems."
"We study scheduling problems on a machine with varying speed. Assuming a
known speed function we ask for a cost-efficient scheduling solution. Our main
result is a PTAS for minimizing the total weighted completion time in this
setting. This also implies a PTAS for the closely related problem of scheduling
to minimize generalized global cost functions. The key to our results is a
re-interpretation of the problem within the well-known two-dimensional Gantt
chart: instead of the standard approach of scheduling in the {\em
time-dimension}, we construct scheduling solutions in the weight-dimension.
  We also consider a dynamic problem variant in which deciding upon the speed
is part of the scheduling problem and we are interested in the tradeoff between
scheduling cost and speed-scaling cost, which is typically the energy
consumption. We observe that the optimal order is independent of the energy
consumption and that the problem can be reduced to the setting where the speed
of the machine is fixed, and thus admits a PTAS. Furthermore, we provide an
FPTAS for the NP-hard problem variant in which the machine can run only on a
fixed number of discrete speeds. Finally, we show how our results can be used
to obtain a~$(2+\eps)$-approximation for scheduling preemptive jobs with
release dates on multiple identical parallel machines."
"We consider the first, and most well studied, speed scaling problem in the
algorithmic literature: where the scheduling quality of service measure is a
deadline feasibility constraint, and where the power objective is to minimize
the total energy used. Four online algorithms for this problem have been
proposed in the algorithmic literature. Based on the best upper bound that can
be proved on the competitive ratio, the ranking of the online algorithms from
best to worst is: $\qOA$, $\OA$, $\AVR$, $\BKP$. As a test case on the
effectiveness of competitive analysis to predict the best online algorithm, we
report on an experimental ""horse race"" between these algorithms using instances
based on web server traces. Our main conclusion is that the ranking of our
algorithms based on their performance in our experiments is identical to the
order predicted by competitive analysis. This ranking holds over a large range
of possible power functions, and even if the power objective is temperature."
"In this paper we consider several variants of the pattern matching problem.
In particular, we investigate the following problems: 1) Pattern matching with
k mismatches; 2) Approximate counting of mismatches; and 3) Pattern matching
with mismatches. The distance metric used is the Hamming distance. We present
some novel algorithms and techniques for solving these problems. Both
deterministic and randomized algorithms are offered. Variants of these problems
where there could be wild cards in either the text or the pattern or both are
considered. An experimental evaluation of these algorithms is also presented.
The source code is available at http://www.engr.uconn.edu/~man09004/kmis.zip."
"The suffix array is a data structure that finds numerous applications in
string processing problems for both linguistic texts and biological data. It
has been introduced as a memory efficient alternative for suffix trees. The
suffix array consists of the sorted suffixes of a string. There are several
linear time suffix array construction algorithms (SACAs) known in the
literature. However, one of the fastest algorithms in practice has a worst case
run time of $O(n^2)$. The problem of designing practically and theoretically
efficient techniques remains open. In this paper we present an elegant
algorithm for suffix array construction which takes linear time with high
probability; the probability is on the space of all possible inputs. Our
algorithm is one of the simplest of the known SACAs and it opens up a new
dimension of suffix array construction that has not been explored until now.
Our algorithm is easily parallelizable. We offer parallel implementations on
various parallel models of computing. We prove a lemma on the $\ell$-mers of a
random string which might find independent applications. We also present
another algorithm that utilizes the above algorithm. This algorithm is called
RadixSA and has a worst case run time of $O(n\log{n})$. RadixSA introduces an
idea that may find independent applications as a speedup technique for other
SACAs. An empirical comparison of RadixSA with other algorithms on various
datasets reveals that our algorithm is one of the fastest algorithms to date.
The C++ source code is freely available at
http://www.engr.uconn.edu/~man09004/radixSA.zip"
"For decades, computing the LZ factorization (or LZ77 parsing) of a string has
been a requisite and computationally intensive step in many diverse
applications, including text indexing and data compression. Many algorithms for
LZ77 parsing have been discovered over the years; however, despite the
increasing need to apply LZ77 to massive data sets, no algorithm to date scales
to inputs that exceed the size of internal memory. In this paper we describe
the first algorithm for computing the LZ77 parsing in external memory. Our
algorithm is fast in practice and will allow the next generation of text
indexes to be realised for massive strings and string collections."
"The maximal sum of a sequence ""A"" of ""n"" real numbers is the greatest sum of
all elements of any strictly contiguous and possibly empty subsequence of ""A"",
and it can be computed in ""O(n)"" time by means of Kadane's algorithm. Letting
""A^(x -> p)"" denote the sequence which results from inserting a real number ""x""
between elements ""A[p-1]"" and ""A[p]"", we show how the maximal sum of ""A^(x ->
p)"" can be computed in ""O(1)"" worst-case time for any given ""x"" and ""p"",
provided that an ""O(n)"" time preprocessing step has already been executed on
""A"". In particular, this implies that, given ""m"" pairs ""(x_0, p_0), ...,
(x_{m-1}, p_{m-1})"", we can compute the maximal sums of sequences ""A^(x_0 ->
p_0), ..., A^(x_{m-1} -> p_{m-1})"" in ""O(n+m)"" time, which matches the lower
bound imposed by the problem input size, and also improves on the
straightforward strategy of applying Kadane's algorithm to each sequence
""A^(x_i -> p_i)"", which takes a total of ""Theta(n.m)"" time. Our main
contribution, however, is to obtain the same time bound for the more
complicated problem of computing the greatest sum of all elements of any
strictly or circularly contiguous and possibly empty subsequence of ""A^(x ->
p)"". Our algorithms are easy to implement in practice, and they were motivated
by and find application in a buffer minimization problem on wireless mesh
networks."
"We provide a decremental approximate Distance Oracle that obtains stretch of
$1+\epsilon$ multiplicative and 2 additive and has $\hat{O}(n^{5/2})$ total
cost (where $\hat{O}$ notation suppresses polylogarithmic and
$n^{O(1)/\sqrt{n}}$ factors). The best previous results with $\hat{O}(n^{5/2})$
total cost obtained stretch $3+\epsilon$."
"Zimin words are very special finite words which are closely related to the
pattern-avoidability problem. This problem consists in testing if an instance
of a given pattern with variables occurs in almost all words over any finite
alphabet. The problem is not well understood, no polynomial time algorithm is
known and its NP-hardness is also not known. The pattern-avoidability problem
is equivalent to searching for a pattern (with variables) in a Zimin word. The
main difficulty is potentially exponential size of Zimin words. We use special
properties of Zimin words, especially that they are highly compressible, to
design efficient algorithms for special version of the pattern-matching, called
here ranked matching. It gives a new interpretation of Zimin algorithm in
compressed setting. We discuss the structure of rankings of variables and
compressed representations of values of variables. Moreover, for a ranked
matching we present efficient algorithms to find the shortest instance and the
number of valuations of instances of the pattern."
"Various specifiable combinatorial structures, with d extensive parameters,
can be exactly sampled both by the recursive method, with linear arithmetic
complexity if a heavy preprocessing is performed, or by the Boltzmann method,
with complexity Theta(n^{1+d/2}). We discuss a modified recursive method,
crucially based on the asymptotic expansion of the associated saddle-point
integrals, which can be adopted for a large number of such structures (e.g.
partitions, permutations, lattice walks, trees, random graphs, all with a
variety of prescribed statistics and/or constraints). The new algorithm
requires no preprocessing, still it has linear complexity on average. In terms
of bit complexity, instead of arithmetic, we only have extra logarithmic
factors. For many families of structures, this provides, at our knowledge, the
only known quasi-linear generators. We present the general theory, and detail a
specific example: the partitions of n elements into k non-empty blocks, counted
by the Stirling numbers of the second kind. These objects are involved in the
exact sampling of minimal automata with prescribed alphabet size and number of
states, which is thus performed here with average Theta(n ln n) bit complexity,
outbreaking all previously known Theta(n^{3/2}) algorithms."
"For a connected graph, a vertex separator is a set of vertices whose removal
creates at least two components and a minimum vertex separator is a vertex
separator of least cardinality. The vertex connectivity refers to the size of a
minimum vertex separator. For a connected graph $G$ with vertex connectivity $k
(k \geq 1)$, the connectivity augmentation refers to a set $S$ of edges whose
augmentation to $G$ increases its vertex connectivity by one. A minimum
connectivity augmentation of $G$ is the one in which $S$ is minimum. In this
paper, we focus our attention on connectivity augmentation of trees. Towards
this end, we present a new sequential algorithm for biconnectivity augmentation
in trees by simplifying the algorithm reported in \cite{nsn}. The simplicity is
achieved with the help of edge contraction tool. This tool helps us in getting
a recursive subproblem preserving all connectivity information. Subsequently,
we present a parallel algorithm to obtain a minimum connectivity augmentation
set in trees. Our parallel algorithm essentially follows the overall structure
of sequential algorithm. Our implementation is based on CREW PRAM model with
$O(\Delta)$ processors, where $\Delta$ refers to the maximum degree of a tree.
We also show that our parallel algorithm is optimal whose processor-time
product is O(n) where $n$ is the number of vertices of a tree, which is an
improvement over the parallel algorithm reported in \cite{hsu}."
"We present an $\tilde{O}(m^{10/7})=\tilde{O}(m^{1.43})$-time algorithm for
the maximum s-t flow and the minimum s-t cut problems in directed graphs with
unit capacities. This is the first improvement over the sparse-graph case of
the long-standing $O(m \min(\sqrt{m},n^{2/3}))$ time bound due to Even and
Tarjan [EvenT75]. By well-known reductions, this also establishes an
$\tilde{O}(m^{10/7})$-time algorithm for the maximum-cardinality bipartite
matching problem. That, in turn, gives an improvement over the celebrated
celebrated $O(m \sqrt{n})$ time bound of Hopcroft and Karp [HK73] whenever the
input graph is sufficiently sparse."
"We study algorithms based on local improvements for the $k$-Set Packing
problem. The well-known local improvement algorithm by Hurkens and Schrijver
has been improved by Sviridenko and Ward from $\frac{k}{2}+\epsilon$ to
$\frac{k+2}{3}$, and by Cygan to $\frac{k+1}{3}+\epsilon$ for any $\epsilon>0$.
In this paper, we achieve the approximation ratio $\frac{k+1}{3}+\epsilon$ for
the $k$-Set Packing problem using a simple polynomial-time algorithm based on
the method by Sviridenko and Ward. With the same approximation guarantee, our
algorithm runs in time singly exponential in $\frac{1}{\epsilon^2}$, while the
running time of Cygan's algorithm is doubly exponential in
$\frac{1}{\epsilon}$. On the other hand, we construct an instance with locality
gap $\frac{k+1}{3}$ for any algorithm using local improvements of size
$O(n^{1/5})$, here $n$ is the total number of sets. Thus, our approximation
guarantee is optimal with respect to results achievable by algorithms based on
local improvements."
"We present efficient data structures for submatrix maximum queries in Monge
matrices and Monge partial matrices. For $n\times n$ Monge matrices, we give a
data structure that requires O(n) space and answers submatrix maximum queries
in $O(\log n)$ time. The best previous data structure [Kaplan et al., SODA`12]
required $O(n \log n)$ space and $O(\log^2 n)$ query time. We also give an
alternative data structure with constant query-time and $ O(n^{1+\varepsilon})$
construction time and space for any fixed $\varepsilon<1$. For $n\times n$ {\em
partial} Monge matrices we obtain a data structure with O(n) space and $O(\log
n \cdot \alpha(n))$ query time. The data structure of Kaplan et al. required
$O(n \log n \cdot \alpha(n))$ space and $O(\log^2 n)$ query time.
  Our improvements are enabled by a technique for exploiting the structure of
the upper envelope of Monge matrices to efficiently report column maxima in
skewed rectangular Monge matrices. We hope this technique can be useful in
obtaining faster search algorithms in Monge partial matrices. In addition, we
give a linear upper bound on the number of breakpoints in the upper envelope of
a Monge partial matrix. This shows that the inverse Ackermann $\alpha(n)$ term
in the analysis of the data structure of Kaplan et. al is superfluous."
"Given a weighted $n$-vertex graph $G$ with integer edge-weights taken from a
range $[-M,M]$, we show that the minimum-weight simple path visiting $k$
vertices can be found in time $\tilde{O}(2^k \poly(k) M n^\omega) = O^*(2^k
M)$. If the weights are reals in $[1,M]$, we provide a
$(1+\varepsilon)$-approximation which has a running time of $\tilde{O}(2^k
\poly(k) n^\omega(\log\log M + 1/\varepsilon))$. For the more general problem
of $k$-tree, in which we wish to find a minimum-weight copy of a $k$-node tree
$T$ in a given weighted graph $G$, under the same restrictions on edge weights
respectively, we give an exact solution of running time $\tilde{O}(2^k \poly(k)
M n^3) $ and a $(1+\varepsilon)$-approximate solution of running time
$\tilde{O}(2^k \poly(k) n^3(\log\log M + 1/\varepsilon))$. All of the above
algorithms are randomized with a polynomially-small error probability."
"The input to the NP-hard Point Line Cover problem (PLC) consists of a set $P$
of $n$ points on the plane and a positive integer $k$, and the question is
whether there exists a set of at most $k$ lines which pass through all points
in $P$. A simple polynomial-time reduction reduces any input to one with at
most $k^2$ points. We show that this is essentially tight under standard
assumptions. More precisely, unless the polynomial hierarchy collapses to its
third level, there is no polynomial-time algorithm that reduces every instance
$(P,k)$ of PLC to an equivalent instance with $O(k^{2-\epsilon})$ points, for
any $\epsilon>0$. This answers, in the negative, an open problem posed by
Lokshtanov (PhD Thesis, 2009).
  Our proof uses the machinery for deriving lower bounds on the size of kernels
developed by Dell and van Melkebeek (STOC 2010). It has two main ingredients:
We first show, by reduction from Vertex Cover, that PLC---conditionally---has
no kernel of total size $O(k^{2-\epsilon})$ bits. This does not directly imply
the claimed lower bound on the number of points, since the best known
polynomial-time encoding of a PLC instance with $n$ points requires
$\omega(n^{2})$ bits. To get around this we build on work of Goodman et al.
(STOC 1989) and devise an oracle communication protocol of cost $O(n\log n)$
for PLC; its main building block is a bound of $O(n^{O(n)})$ for the order
types of $n$ points that are not necessarily in general position, and an
explicit algorithm that enumerates all possible order types of n points. This
protocol and the lower bound on total size together yield the stated lower
bound on the number of points.
  While a number of essentially tight polynomial lower bounds on total sizes of
kernels are known, our result is---to the best of our knowledge---the first to
show a nontrivial lower bound for structural/secondary parameters."
"The Joint Replenishment Problem (JRP) deals with optimizing shipments of
goods from a supplier to retailers through a shared warehouse. Each shipment
involves transporting goods from the supplier to the warehouse, at a fixed cost
C, followed by a redistribution of these goods from the warehouse to the
retailers that ordered them, where transporting goods to a retailer $\rho$ has
a fixed cost $c_\rho$. In addition, retailers incur waiting costs for each
order. The objective is to minimize the overall cost of satisfying all orders,
namely the sum of all shipping and waiting costs.
  JRP has been well studied in Operations Research and, more recently, in the
area of approximation algorithms. For arbitrary waiting cost functions, the
best known approximation ratio is 1.8. This ratio can be reduced to 1.574 for
the JRP-D model, where there is no cost for waiting but orders have deadlines.
As for hardness results, it is known that the problem is APX-hard and that the
natural linear program for JRP has integrality gap at least 1.245. Both results
hold even for JRP-D. In the online scenario, the best lower and upper bounds on
the competitive ratio are 2.64 and 3, respectively. The lower bound of 2.64
applies even to the restricted version of JRP, denoted JRP-L, where the waiting
cost function is linear.
  We provide several new approximation results for JRP. In the offline case, we
give an algorithm with ratio 1.791, breaking the barrier of 1.8. In the online
case, we show a lower bound of 2.754 on the competitive ratio for JRP-L (and
thus JRP as well), improving the previous bound of 2.64. We also study the
online version of JRP-D, for which we prove that the optimal competitive ratio
is 2."
"We study the average performance of online greedy matching algorithms on
$G(n,n,p)$, the random bipartite graph with $n$ vertices on each side and edges
occurring independently with probability $p=p(n)$. In the online model,
vertices on one side of the graph are given up front while vertices on the
other side arrive sequentially; when a vertex arrives its edges are revealed
and it must be immediately matched or dropped. We begin by analyzing the
\textsc{oblivious} algorithm, which tries to match each arriving vertex to a
random neighbor, even if the neighbor has already been matched. The algorithm
is shown to have a performance ratio of at least $1-1/e$ for all monotonic
functions $p(n)$, where the performance ratio is defined asymptotically as the
ratio of the expected matching size given by the algorithm to the expected
maximum matching size. Next we show that the conventional \textsc{greedy}
algorithm, which assigns each vertex to a random unmatched neighbor, has a
performance ratio of at least 0.837 for all monotonic functions $p(n)$. Under
the $G(n,n,p)$ model, the performance of \textsc{greedy} is equivalent to the
performance of the well known \textsc{ranking} algorithm, so our results show
that \textsc{ranking} has a performance ratio of at least 0.837. We finally
consider vertex-weighted bipartite matching. Our proofs are based on simple
differential equations that describe the evolution of the matching process."
"Motivated by online advertisement and exchange settings, greedy randomized
algorithms for the maximum matching problem have been studied, in which the
algorithm makes (random) decisions that are essentially oblivious to the input
graph. Any greedy algorithm can achieve performance ratio 0.5, which is the
expected number of matched nodes to the number of nodes in a maximum matching.
  Since Aronson, Dyer, Frieze and Suen proved that the Modified Randomized
Greedy (MRG) algorithm achieves performance ratio 0.5 + \epsilon (where
\epsilon = frac{1}{400000}) on arbitrary graphs in the mid-nineties, no further
attempts in the literature have been made to improve this theoretical ratio for
arbitrary graphs until two papers were published in FOCS 2012. Poloczek and
Szegedy also analyzed the MRG algorithm to give ratio 0.5039, while Goel and
Tripathi used experimental techniques to analyze the Ranking algorithm to give
ratio 0.56. However, we could not reproduce the experimental results of Goel
and Tripathi.
  In this paper, we revisit the Ranking algorithm using the LP framework.
Special care is given to analyze the structural properties of the Ranking
algorithm in order to derive the LP constraints, of which one known as the
\emph{boundary} constraint requires totally new analysis and is crucial to the
success of our LP.
  We use continuous LP relaxation to analyze the limiting behavior as the
finite LP grows. Of particular interest are new duality and complementary
slackness characterizations that can handle the monotone and the boundary
constraints in continuous LP. We believe our work achieves the currently best
theoretical performance ratio of \frac{2(5-\sqrt{7})}{9} \approx 0.523 on
arbitrary graphs. Moreover, experiments suggest that Ranking cannot perform
better than 0.724 in general."
"In the first place, a novel, yet straightforward in-place integer
value-sorting algorithm is presented. It sorts in linear time using constant
amount of additional memory for storing counters and indices beside the input
array. The technique is inspired from the principal idea behind one of the
ordinal theories of ""serial order in behavior"" and explained by the analogy
with the three main stages in the formation and retrieval of memory in
cognitive neuroscience: (i) practicing, (ii) storage and (iii) retrieval. It is
further improved in terms of time complexity as well as specialized for
distinct integers, though still improper for rank-sorting.
  Afterwards, another novel, yet straightforward technique is introduced which
makes this efficient value-sorting technique proper for rank-sorting. Hence,
given an array of n elements each have an integer key, the technique sorts the
elements according to their integer keys in linear time using only constant
amount of additional memory. The devised technique is very practical and
efficient outperforming bucket sort, distribution counting sort and address
calculation sort family of algorithms making it attractive in almost every case
even when space is not a critical resource."
"We study the problem of packing a knapsack without knowing its capacity.
Whenever we attempt to pack an item that does not fit, the item is discarded;
if the item fits, we have to include it in the packing. We show that there is
always a policy that packs a value within factor 2 of the optimum packing,
irrespective of the actual capacity. If all items have unit density, we achieve
a factor equal to the golden ratio. Both factors are shown to be best possible.
In fact, we obtain the above factors using packing policies that are universal
in the sense that they fix a particular order of the items and try to pack the
items in this order, independent of the observations made while packing. We
give efficient algorithms computing these policies. On the other hand, we show
that, for any alpha>1, the problem of deciding whether a given universal policy
achieves a factor of alpha is coNP-complete. If alpha is part of the input, the
same problem is shown to be coNP-complete for items with unit densities.
Finally, we show that it is coNP-hard to decide, for given alpha, whether a set
of items admits a universal policy with factor alpha, even if all items have
unit densities."
"In this paper, we consider the fault-tolerant $k$-median problem and give the
\emph{first} constant factor approximation algorithm for it. In the
fault-tolerant generalization of classical $k$-median problem, each client $j$
needs to be assigned to at least $r_j \ge 1$ distinct open facilities. The
service cost of $j$ is the sum of its distances to the $r_j$ facilities, and
the $k$-median constraint restricts the number of open facilities to at most
$k$. Previously, a constant factor was known only for the special case when all
$r_j$s are the same, and a logarithmic approximation ratio for the general
case. In addition, we present the first polynomial time algorithm for the
fault-tolerant $k$-median problem on a path or a HST by showing that the
corresponding LP always has an integral optimal solution.
  We also consider the fault-tolerant facility location problem, where the
service cost of $j$ can be a weighted sum of its distance to the $r_j$
facilities. We give a simple constant factor approximation algorithm,
generalizing several previous results which only work for nonincreasing weight
vectors."
"We present a dynamic data structure for representing a graph $G$ with
tree-depth at most $D$. Tree-depth is an important graph parameter which arose
in the study of sparse graph classes.
  The structure allows addition and removal of edges and vertices such that the
resulting graph still has tree-depth at most $D$, in time bounds depending only
on $D$. A tree-depth decomposition of the graph is maintained explicitly.
  This makes the data structure useful for dynamization of static algorithms
for graphs with bounded tree-depth. As an example application, we give a
dynamic data structure for MSO-property testing, with time bounds for removal
depending only on $D$ and constant-time testing of the property, while the time
for the initialization and insertion also depends on the size of the formula
expressing the property."
"In this paper we generalize the idea of QuickHeapsort leading to the notion
of QuickXsort. Given some external sorting algorithm X, QuickXsort yields an
internal sorting algorithm if X satisfies certain natural conditions.
  With QuickWeakHeapsort and QuickMergesort we present two examples for the
QuickXsort-construction. Both are efficient algorithms that incur approximately
n log n - 1.26n +o(n) comparisons on the average. A worst case of n log n +
O(n) comparisons can be achieved without significantly affecting the average
case.
  Furthermore, we describe an implementation of MergeInsertion for small n.
Taking MergeInsertion as a base case for QuickMergesort, we establish a
worst-case efficient sorting algorithm calling for n log n - 1.3999n + o(n)
comparisons on average. QuickMergesort with constant size base cases shows the
best performance on practical inputs: when sorting integers it is slower by
only 15% to STL-Introsort."
"We investigate online algorithms for maximum (weight) independent set on
graph classes with bounded inductive independence number like, e.g., interval
and disk graphs with applications to, e.g., task scheduling and spectrum
allocation. In the online setting, it is assumed that nodes of an unknown graph
arrive one by one over time. An online algorithm has to decide whether an
arriving node should be included into the independent set. Unfortunately, this
natural and practically relevant online problem cannot be studied in a
meaningful way within a classical competitive analysis as the competitive ratio
on worst-case input sequences is lower bounded by $\Omega(n)$.
  As a worst-case analysis is pointless, we study online independent set in a
stochastic analysis. Instead of focussing on a particular stochastic input
model, we present a generic sampling approach that enables us to devise online
algorithms achieving performance guarantees for a variety of input models. In
particular, our analysis covers stochastic input models like the secretary
model, in which an adversarial graph is presented in random order, and the
prophet-inequality model, in which a randomly generated graph is presented in
adversarial order. Our sampling approach bridges thus between stochastic input
models of quite different nature. In addition, we show that our approach can be
applied to a practically motivated admission control setting.
  Our sampling approach yields an online algorithm for maximum independent set
with competitive ratio $O(\rho^2)$ with respect to all of the mentioned
stochastic input models. for graph classes with inductive independence number
$\rho$. The approach can be extended towards maximum-weight independent set by
losing only a factor of $O(\log n)$ in the competitive ratio with $n$ denoting
the (expected) number of nodes."
"We consider a problem which has received considerable attention in systems
literature because of its applications to routing in delay tolerant networks
and replica placement in distributed storage systems. In abstract terms the
problem can be stated as follows: Given a random variable $X$ generated by a
known product distribution over $\{0,1\}^n$ and a target value $0 \leq \theta
\leq 1$, output a non-negative vector $w$, with $\|w\|_1 \le 1$, which
maximizes the probability of the event $w \cdot X \ge \theta$. This is a
challenging non-convex optimization problem for which even computing the value
$\Pr[w \cdot X \ge \theta]$ of a proposed solution vector $w$ is #P-hard.
  We provide an additive EPTAS for this problem which, for constant-bounded
product distributions, runs in $ \poly(n) \cdot 2^{\poly(1/\eps)}$ time and
outputs an $\eps$-approximately optimal solution vector $w$ for this problem.
Our approach is inspired by, and extends, recent structural results from the
complexity-theoretic study of linear threshold functions. Furthermore, in spite
of the objective function being non-smooth, we give a \emph{unicriterion} PTAS
while previous work for such objective functions has typically led to a
\emph{bicriterion} PTAS. We believe our techniques may be applicable to get
unicriterion PTAS for other non-smooth objective functions."
"We show an improved parallel algorithm for decomposing an undirected
unweighted graph into small diameter pieces with a small fraction of the edges
in between. These decompositions form critical subroutines in a number of graph
algorithms. Our algorithm builds upon the shifted shortest path approach
introduced in [Blelloch, Gupta, Koutis, Miller, Peng, Tangwongsan, SPAA 2011].
By combining various stages of the previous algorithm, we obtain a
significantly simpler algorithm with the same asymptotic guarantees as the best
sequential algorithm."
"In the online Steiner tree problem, a sequence of points is revealed
one-by-one: when a point arrives, we only have time to add a single edge
connecting this point to the previous ones, and we want to minimize the total
length of edges added. For two decades, we know that the greedy algorithm
maintains a tree whose cost is O(log n) times the Steiner tree cost, and this
is best possible. But suppose, in addition to the new edge we add, we can
change a single edge from the previous set of edges: can we do much better? Can
we maintain a tree that is constant-competitive?
  We answer this question in the affirmative. We give a primal-dual algorithm,
and a novel dual-based analysis, that makes only a single swap per step (in
addition to adding the edge connecting the new point to the previous ones), and
such that the tree's cost is only a constant times the optimal cost.
  Previous results for this problem gave an algorithm that performed an
amortized constant number of swaps: for each n, the number of swaps in the
first n steps was O(n). We also give a simpler tight analysis for this
amortized case."
"For a metric graph $G=(V,E)$ and $R\subset V$, the internal Steiner minimum
tree problem asks for a minimum weight Steiner tree spanning $R$ such that
every vertex in $R$ is not a leaf. This note shows a simple polynomial-time
$2\rho$-approximation algorithm, in which $\rho$ is the approximation ratio for
the Steiner minimum tree problem. The result improves the previous best
approximation ratio $2\rho+1$ for the problem. The ratio is not currently best
but the algorithm is very simple."
"Together with a characteristic function, idempotent permutations uniquely
determine idempotent maps, as well as their linearly ordered arrangement
simultaneously. Furthermore, in-place linear time transformations are possible
between them. Hence, they may be important for succinct data structures,
information storing, sorting and searching.
  In this study, their combinatorial interpretation is given and their
application on sorting is examined. Given an array of n integer keys each in
[1,n], if it is allowed to modify the keys in the range [-n,n], idempotent
permutations make it possible to obtain linearly ordered arrangement of the
keys in O(n) time using only 4log(n) bits, setting the theoretical lower bound
of time and space complexity of sorting. If it is not allowed to modify the
keys out of the range [1,n], then n+4log(n) bits are required where n of them
is used to tag some of the keys."
"We give a polynomial time, $(1+\epsilon)$-approximation algorithm for the
traveling repairman problem (TRP) in the Euclidean plane and on weighted trees.
This improves on the known quasi-polynomial time approximation schemes for
these problems. The algorithm is based on a simple technique that reduces the
TRP to what we call the \emph{segmented TSP}. Here, we are given numbers
$l_1,\dots,l_K$ and $n_1,\dots,n_K$ and we need to find a path that visits at
least $n_h$ points within path distance $l_h$ from the starting point for all
$h\in\{1,\dots,K\}$. A solution is $\alpha$-approximate if at least $n_h$
points are visited within distance $\alpha l_h$. It is shown that any algorithm
that is $\alpha$-approximate for \emph{every constant} $K$ in some metric
space, gives an $\alpha(1+\epsilon)$-approximation for the TRP in the same
metric space. Subsequently, approximation schemes are given for this segmented
TSP problem in the plane and on weighted trees. The segmented TSP with only one
segment ($K=1$) is equivalent to the $k$-TSP for which a
$(2+\epsilon)$-approximation is known for a general metric space. Hence, this
approach through the segmented TSP gives new impulse for improving on the
3.59-approximation for TRP in a general metric space. A similar reduction
applies to many other minimum latency problems. To illustrate the strength of
this approach we apply it to the well-studied scheduling problem of minimizing
total weighted completion time under precedence constraints, $1|prec|\sum
w_{j}C_{j}$, and present a polynomial time approximation scheme for the case of
interval order precedence constraints. This improves on the known
$3/2$-approximation for this problem. Both approximation schemes apply as well
if release dates are added to the problem."
"We present the first fully polynomial approximation schemes for the maximum
weighted (uncapacitated or capacitated) $b$--Matching problem for nonbipartite
graphs that run in time (near) linear in the number of edges, that is, given
any $\delta>0$ the algorithm produces a $(1-\delta)$ approximation in $O(m
\poly(\delta^{-1},\log n))$ time. We provide fractional solutions for the
standard linear programming formulations for these problems and subsequently
also provide fully polynomial (near) linear time approximation schemes for
rounding the fractional solutions. Through these problems as a vehicle, we also
present several ideas in the context of solving linear programs approximately
using fast primal-dual algorithms. First, we show that approximation algorithms
can be used to reduce the width of the formulation, and as a consequence we
induce faster convergence. Second, even though the dual of these problems have
exponentially many variables and an efficient exact computation of dual weights
is infeasible, we can efficiently compute and use a sparse approximation of the
dual weights using a combination of (i) adding perturbation to the constraints
of the polytope and (ii) amplification followed by thresholding of the dual
weights."
"In this paper we consider graph algorithms in models of computation where the
space usage (random accessible storage, in addition to the read only input) is
sublinear in the number of edges $m$ and the access to input data is
constrained. These questions arises in many natural settings, and in particular
in the analysis of MapReduce or similar algorithms that model constrained
parallelism with sublinear central processing. In SPAA 2011, Lattanzi etal.
provided a $O(1)$ approximation of maximum matching using $O(p)$ rounds of
iterative filtering via mapreduce and $O(n^{1+1/p})$ space of central
processing for a graph with $n$ nodes and $m$ edges.
  We focus on weighted nonbipartite maximum matching in this paper. For any
constant $p>1$, we provide an iterative sampling based algorithm for computing
a $(1-\epsilon)$-approximation of the weighted nonbipartite maximum matching
that uses $O(p/\epsilon)$ rounds of sampling, and $O(n^{1+1/p})$ space. The
results extends to $b$-Matching with small changes. This paper combines
adaptive sketching literature and fast primal-dual algorithms based on relaxed
Dantzig-Wolfe decision procedures. Each round of sampling is implemented
through linear sketches and executed in a single round of MapReduce. The paper
also proves that nonstandard linear relaxations of a problem, in particular
penalty based formulations, are helpful in mapreduce and similar settings in
reducing the adaptive dependence of the iterations."
"We consider directed graphs where each edge is labeled with an integer weight
and study the fundamental algorithmic question of computing the value of a
cycle with minimum mean weight. Our contributions are twofold: (1) First we
show that the algorithmic question is reducible in O(n^2) time to the problem
of a logarithmic number of min-plus matrix multiplications of n-by-n matrices,
where n is the number of vertices of the graph. (2) Second, when the weights
are nonnegative, we present the first (1 + {\epsilon})-approximation algorithm
for the problem and the running time of our algorithm is \tilde(O)(n^\omega
log^3(nW/{\epsilon}) / {\epsilon}), where O(n^\omega) is the time required for
the classic n-by-n matrix multiplication and W is the maximum value of the
weights."
"In this paper a class of bottleneck combinatorial optimization problems with
uncertain costs is discussed. The uncertainty is modeled by specifying a
discrete scenario set containing a finite number of cost vectors, called
scenarios. In order to choose a solution the Ordered Weighted Averaging
aggregation operator (shortly OWA) is applied. The OWA operator generalizes
traditional criteria in decision making under uncertainty such as the maximum,
minimum, average, median, or Hurwicz criterion. New complexity and
approximation results in this area are provided. These results are general and
remain valid for many problems, in particular for a wide class of network
problems."
"In this paper we study a single player game consisting of $n$ black checkers
and $m$ white checkers, called shifting the checkers. We have proved that the
minimum number of steps needed to play the game for general $n$ and $m$ is $nm
+ n + m$. We have also presented an optimal algorithm to generate an optimal
move sequence of the game consisting of $n$ black checkers and $m$ white
checkers, and finally, we present an explicit solution for the general game."
"In the area of parameterized complexity, to cope with NP-Hard problems, we
introduce a parameter k besides the input size n, and we aim to design
algorithms (called FPT algorithms) that run in O(f(k)n^d) time for some
function f(k) and constant d. Though FPT algorithms have been successfully
designed for many problems, typically they are not sufficiently fast because of
huge f(k) and d. In this paper, we give FPT algorithms with small f(k) and d
for many important problems including Odd Cycle Transversal and Almost 2-SAT.
More specifically, we can choose f(k) as a single exponential (4^k) and d as
one, that is, linear in the input size. To the best of our knowledge, our
algorithms achieve linear time complexity for the first time for these
problems. To obtain our algorithms for these problems, we consider a large
class of integer programs, called BIP2. Then we show that, in linear time, we
can reduce BIP2 to Vertex Cover Above LP preserving the parameter k, and we can
compute an optimal LP solution for Vertex Cover Above LP using network flow.
Then, we perform an exhaustive search by fixing half-integral values in the
optimal LP solution for Vertex Cover Above LP. A bottleneck here is that we
need to recompute an LP optimal solution after branching. To address this
issue, we exploit network flow to update the optimal LP solution in linear
time."
"The interval graph for a set of intervals on a line consists of one vertex
for each interval, and an edge for each intersecting pair of intervals. A probe
interval graph is a variant that is motivated by an application to genomics,
where the intervals are partitioned into two sets: probes and non-probes. The
graph has an edge between two vertices if they intersect and at least one of
them is a probe. We give a linear-time algorithm for determining whether a
given graph and partition of vertices into probes and non-probes is a probe
interval graph. If it is, we give a layout of intervals that proves this. We
can also determine whether the layout of the intervals is uniquely constrained
within the same time bound. As part of the algorithm, we solve the
consecutive-ones probe matrix problem in linear time, develop algorithms for
operating on PQ trees, and give results that relate PQ trees for different
submatrices of a consecutive-ones matrix."
"Consider the problem of partitioning an arbitrary metric space into pieces of
diameter at most \Delta, such every pair of points is separated with relatively
low probability. We propose a rate-based algorithm inspired by
multiplicatively-weighted Voronoi diagrams, and prove it has optimal
trade-offs. This also gives us another logarithmic approximation algorithm for
the 0-extension problem."
"This paper proposes a general framework for generating cache-oblivious
layouts for binary search trees. A cache-oblivious layout attempts to minimize
cache misses on any hierarchical memory, independent of the number of memory
levels and attributes at each level such as cache size, line size, and
replacement policy. Recursively partitioning a tree into contiguous subtrees
and prescribing an ordering amongst the subtrees, Hierarchical Layouts
generalize many commonly used layouts for trees such as in-order, pre-order and
breadth-first. They also generalize the various flavors of the van Emde Boas
layout, which have previously been used as cache-oblivious layouts.
Hierarchical Layouts thus unify all previous attempts at deriving layouts for
search trees.
  The paper then derives a new locality measure (the Weighted Edge Product)
that mimics the probability of cache misses at multiple levels, and shows that
layouts that reduce this measure perform better. We analyze the various degrees
of freedom in the construction of Hierarchical Layouts, and investigate the
relative effect of each of these decisions in the construction of
cache-oblivious layouts. Optimizing the Weighted Edge Product for complete
binary search trees, we introduce the MinWEP layout, and show that it
outperforms previously used cache-oblivious layouts by almost 20%."
"We consider a special case of the ordinary NP-hard two-machine flow shop
problem with the objective of determining simultaneously a minimal common due
date and the minimal number of tardy jobs. In [S. S. Panwalkar, C. Koulamas, An
O(n^2) algorithm for the variable common due date, minimal tardy jobs
bicriteria two-machine flow shop problem with ordered machines, European
Journal of Operational Research 221 (2012), 7-13.], the authors presented
quadratic algorithm for the problem when each job has its smaller processing
time on the first machine. In this note, we improve the running time of the
algorithm to O(n log n) by efficient implementation using recently introduced
modified binary tree data structure."
"The general Bandpass-$B$ problem is NP-hard and can be approximated by a
reduction into the weighted $B$-set packing problem, with a worst case
performance ratio of $O(B^2)$. When $B = 2$, a maximum weight matching gives a
2-approximation to the problem. In this paper, we call the Bandpass-2 problem
simply the Bandpass problem. The Bandpass problem can be viewed as a variation
of the maximum traveling salesman problem, in which the edge weights are
dynamic rather than given at the front. We present a ${426}{227}$-approximation
algorithm for the problem. Such an improved approximation is built on an
intrinsic structural property proven for the optimal solution and several novel
schemes to partition a $b$-matching into desired matchings."
"This paper studies a combinatorial optimization problem which is obtained by
combining the flow shop scheduling problem and the shortest path problem. The
objective of the obtained problem is to select a subset of jobs that
constitutes a feasible solution to the shortest path problem, and to execute
the selected jobs on the flow shop machines to minimize the makespan. We argue
that this problem is NP-hard even if the number of machines is two, and is
NP-hard in the strong sense for the general case. We propose an intuitive
approximation algorithm for the case where the number of machines is an input,
and an improved approximation algorithm for fixed number of machines."
"We consider several combinatorial optimization problems which combine the
classic shop scheduling problems, namely open shop scheduling or job shop
scheduling, and the shortest path problem. The objective of the obtained
problem is to select a subset of jobs that forms a feasible solution of the
shortest path problem, and to execute the selected jobs on the open (or job)
shop machines to minimize the makespan. We show that these problems are NP-hard
even if the number of machines is two, and cannot be approximated within a
factor less than 2 if the number of machines is an input unless P=NP. We
present several approximation algorithms for these combination problems."
"We consider scheduling of colored packets with transition costs which form a
general metric space. We design a $1 - O(\sqrt{MST(G) / L})$ competitive
algorithm. Our main result is a hardness result of $1 - \Omega(\sqrt{MST(G) /
L})$ which matches the competitive ratio of the algorithm for each metric space
separately. In particular, we improve the hardness result of Azar at el. 2009
for uniform metric spaces.
  We also extend our result to weighted directed graphs which obey the
triangular inequality and show a $1 - O(\sqrt{TSP(G) / L})$ competitive
algorithm and a nearly-matching hardness result. In proving our hardness
results we use some interesting non-standard embedding."
"We obtain an algorithmic meta-theorem for the following optimization problem.
Let \phi\ be a Counting Monadic Second Order Logic (CMSO) formula and t be an
integer. For a given graph G, the task is to maximize |X| subject to the
following: there is a set of vertices F of G, containing X, such that the
subgraph G[F] induced by F is of treewidth at most t, and structure (G[F],X)
models \phi.
  Some special cases of this optimization problem are the following generic
examples. Each of these cases contains various problems as a special subcase:
  1) ""Maximum induced subgraph with at most l copies of cycles of length 0
modulo m"", where for fixed nonnegative integers m and l, the task is to find a
maximum induced subgraph of a given graph with at most l vertex-disjoint cycles
of length 0 modulo m.
  2) ""Minimum \Gamma-deletion"", where for a fixed finite set of graphs \Gamma\
containing a planar graph, the task is to find a maximum induced subgraph of a
given graph containing no graph from \Gamma\ as a minor.
  3) ""Independent \Pi-packing"", where for a fixed finite set of connected
graphs \Pi, the task is to find an induced subgraph G[F] of a given graph G
with the maximum number of connected components, such that each connected
component of G[F] is isomorphic to some graph from \Pi.
  We give an algorithm solving the optimization problem on an n-vertex graph G
in time O(#pmc n^{t+4} f(t,\phi)), where #pmc is the number of all potential
maximal cliques in G and f is a function depending of t and \phi\ only. We also
show how a similar running time can be obtained for the weighted version of the
problem. Pipelined with known bounds on the number of potential maximal
cliques, we deduce that our optimization problem can be solved in time
O(1.7347^n) for arbitrary graphs, and in polynomial time for graph classes with
polynomial number of minimal separators."
"In this paper, we propose a new ranking method inspired from previous results
on the diffusion approach to solve linear equation. We describe new
mathematical equations corresponding to this method and show through
experimental results the potential computational gain. This ranking method is
also compared to the well known PageRank model."
"We are given a set of $n$ jobs and a single processor that can vary its speed
dynamically. Each job $J_j$ is characterized by its processing requirement
(work) $p_j$, its release date $r_j$ and its deadline $d_j$. We are also given
a budget of energy $E$ and we study the scheduling problem of maximizing the
throughput (i.e. the number of jobs which are completed on time). We propose a
dynamic programming algorithm that solves the preemptive case of the problem,
i.e. when the execution of the jobs may be interrupted and resumed later, in
pseudo-polynomial time. Our algorithm can be adapted for solving the weighted
version of the problem where every job is associated with a weight $w_j$ and
the objective is the maximization of the sum of the weights of the jobs that
are completed on time. Moreover, we provide a strongly polynomial time
algorithm to solve the non-preemptive unweighed case when the jobs have the
same processing requirements. For the weighted case, our algorithm can be
adapted for solving the non-preemptive version of the problem in
pseudo-polynomial time."
"In the current paper we have investigated the capacitated distance constraint
vehicle routing problem for the standard approximation algorithm. This problem
consists of a number of different types of vehicles at the depot which differ
at their capacity, cost and maximum distance bound. We have designed an
approximation algorithm for this problem in the case that the tours are
balanced."
"In this paper, we revisit the much studied problem of Pattern Matching with
Swaps (Swap Matching problem, for short). We first present a graph-theoretic
model, which opens a new and so far unexplored avenue to solve the problem.
Then, using the model, we devise two efficient algorithms to solve the swap
matching problem. The resulting algorithms are adaptations of the classic
shift-and algorithm. For patterns having length similar to the word-size of the
target machine, both the algorithms run in linear time considering a fixed
alphabet."
"We study the problem of finding a maximum matching in a graph given by an
input stream listing its edges in some arbitrary order, where the quantity to
be maximized is given by a monotone submodular function on subsets of edges.
This problem, which we call maximum submodular-function matching (MSM), is a
natural generalization of maximum weight matching (MWM), which is in turn a
generalization of maximum cardinality matching (MCM). We give two incomparable
algorithms for this problem with space usage falling in the semi-streaming
range---they store only $O(n)$ edges, using $O(n\log n)$ working memory---that
achieve approximation ratios of $7.75$ in a single pass and $(3+\epsilon)$ in
$O(\epsilon^{-3})$ passes respectively. The operations of these algorithms
mimic those of Zelke's and McGregor's respective algorithms for MWM; the
novelty lies in the analysis for the MSM setting. In fact we identify a general
framework for MWM algorithms that allows this kind of adaptation to the broader
setting of MSM.
  In the sequel, we give generalizations of these results where the
maximization is over ""independent sets"" in a very general sense. This
generalization captures hypermatchings in hypergraphs as well as independence
in the intersection of multiple matroids."
"Various list accessing algorithms have been proposed in the literature and
their performances have been analyzed theoretically and experimentally.
Move-To-Front (MTF) and Transpose (TRANS) are two well known primitive list
accessing algorithms. MTF has been proved to be the best performing online
algorithm till date in the literature for real life inputs and practical
applications with locality of reference. It has been shown that when storage
space is extremely limited and pointers for lists cannot be used, then array
implementation of TRANS gives efficient reorganization. Use of MTF is extensive
in the literature whereas, the use of TRANS is rare. As mentioned as an open
problem in literature, direct bounds on the behavior and performance of various
list accessing algorithms are needed to allow realistic comparisons. Since it
has been shown that no single optimal permutation algorithm exists, it becomes
necessary to characterize the circumstances that indicate the advantage in
using a particular list accessing algorithm. Motivated by above challenging
research issue, in this paper we have made an analytical study for evaluating
the performance of TRANS list accessing algorithm using two special types of
request sequences without locality of reference. We have compared the
performance of TRANS with MTF and observed that TRANS outperforms MTF for these
considered types of request sequences."
"In this paper, we present an improved algorithm for the maximum flow problem
on general networks with $n$ vertices and $m$ arcs. We show how to solve the
problem in $O(mn)$ time, when $m = O(n^{2-\epsilon})$, for some $0 <\epsilon
\leq 1$. This improves upon the results of both Orlin and King, et. al., who
solved the problem in $O(mn + m^{31/16} \log^2 n)$ and $O(mn\log_{m/n\log n}n)$
time, respectively. Our main result is reducing the number of nonsaturating
pushes to $O(mn)$ across all scaling phases. Our algorithm can be seen as
complementary to King, et. al., in the sense that we solve the max-flow problem
in $O(mn)$ time when $m = O(n^{2-\epsilon})$ (all sparse and non-dense
networks), whereas King, et. al. solve it in $O(mn)$ time when $m =
\Omega(n^{1+\epsilon})$ (all dense and non-sparse networks).
  Our improvement is reached by a novel combination of Ahuja and Orlin's excess
scaling method and Orlin's compact flow networks. To our knowledge, this is the
first $O(mn)$ time max-flow algorithm that runs on this range of networks.
Further, we extend the range of Orlin's $O(mn)$ time algorithm from
$O(n^{16/15-\epsilon})$ to $O(n^{2-\epsilon})$, which is an improvement of
approximately $O(n^{0.94})$. Our result also establishes that the problem can
be solved for all $n$ and $m$ using exclusively the push-relabel method. We
also give improved algorithms for parametric flows and efficiently constructing
Gomory-Hu trees, and suggest a new approach to the minimum-cost flow problem."
"We design new approximation algorithms for the Multiway Cut problem,
improving the previously known factor of 1.32388 [Buchbinder et al., 2013].
  We proceed in three steps. First, we analyze the rounding scheme of
Buchbinder et al., 2013 and design a modification that improves the
approximation to (3+sqrt(5))/4 (approximately 1.309017). We also present a
tight example showing that this is the best approximation one can achieve with
the type of cuts considered by Buchbinder et al., 2013: (1) partitioning by
exponential clocks, and (2) single-coordinate cuts with equal thresholds.
  Then, we prove that this factor can be improved by introducing a new rounding
scheme: (3) single-coordinate cuts with descending thresholds. By combining
these three schemes, we design an algorithm that achieves a factor of (10 + 4
sqrt(3))/13 (approximately 1.30217). This is the best approximation factor that
we are able to verify by hand.
  Finally, we show that by combining these three rounding schemes with the
scheme of independent thresholds from Karger et al., 2004, the approximation
factor can be further improved to 1.2965. This approximation factor has been
verified only by computer."
"We provide an extensive list of desirable properties for an O-notation --- as
used in algorithm analysis --- and reduce them to 8 primitive properties. We
prove that the primitive properties are equivalent to the definition of the
O-notation as linear dominance. We abstract the existing definitions of the
O-notation under local linear dominance, and show that it has a
characterization by limits over filters for positive functions. We define the
O-mappings as a general tool for manipulating the O-notation, and show that
Master theorems hold under linear dominance."
"We use exponential start time clustering to design faster and more
work-efficient parallel graph algorithms involving distances. Previous
algorithms usually rely on graph decomposition routines with strict
restrictions on the diameters of the decomposed pieces. We weaken these bounds
in favor of stronger local probabilistic guarantees. This allows more direct
analyses of the overall process, giving: * Linear work parallel algorithms that
construct spanners with $O(k)$ stretch and size $O(n^{1+1/k})$ in unweighted
graphs, and size $O(n^{1+1/k} \log k)$ in weighted graphs. * Hopsets that lead
to the first parallel algorithm for approximating shortest paths in undirected
graphs with $O(m\;\mathrm{polylog}\;n)$ work."
"We consider the pull-based broadcast scheduling model. In this model, there
are n unit-sized pages of information available at the server. Requests arrive
over time at the server asking for a specific page. When the server transmits a
page, all outstanding requests for the page are simultaneously satisfied, and
this is what distinguishes broadcast scheduling from the standard scheduling
setting where each job must be processed separately by the server. Broadcast
scheduling has received a considerable amount of attention due to the
algorithmic challenges that it gives in addition to its applications in
multicast systems and wireless and LAN networks. In this paper, we give the
following new approximation results for two popular objectives:
  - For the objective of minimizing the maximum flow time, we give the first
PTAS. Previously, it was known that the algorithm First-In-First-Out (FIFO) is
a 2-approximation, and it is tight. It has been suggested as an open problem to
obtain a better approximation.
  - For the objective of maximizing the throughput, we give a
0.7759-approximation which improves upon the previous best known
0.75-approximation.
  Our improved results are enabled by our novel rounding schemes and linear
programming which can effectively reduce congestion in schedule which is often
the main bottleneck in designing scheduling algorithms based on linear
programming. We believe that our algorithmic ideas and techniques could be of
potential use for other scheduling problems."
"The GC problem is to identify a pre-determined number of center vertices such
that the distances or costs from (or to) the centers to (or from) other
vertices is minimized. The bottleneck of a path is the minimum capacity of
edges on the path. The Bottleneck Paths (BP) problem is to compute the paths
that give us the maximum bottleneck values between pairs of vertices. The Graph
Bottleneck (GB) problem is to find the minimum bottleneck value out of
bottleneck paths for all possible pairs of vertices. We give two similar
algorithms that are based on binary search to solve the 1-center GC problem and
the GB problem on directed graphs with unit edge costs. We achieve
$\tilde{O}(n^{2.373})$ worst case time complexity for both the 1-center GC
problem and the GB problem, where $n$ is the number of vertices in the graph.
This is better than the straightforward methods of solving the two problems in
$O(n^{2.575})$ and $O(n^{2.688})$ time bounds, respectively.
  We then combine the Bottleneck Paths (BP) problem with the well known
Shortest Paths (SP) problem to compute the shortest paths for all possible flow
values. We call this problem the Shortest Paths for All Flows (SP-AF) problem.
We show that if the flow demand is uncertain, but between two consecutive
capacity values, the unique shortest path can be computed to push that flow. If
the uncertainty stretches over two intervals, we need to prepare two shortest
paths to accommodate the uncertainty, etc. In introducing this new problem, we
define a new semi-ring called the distance/flow semi-ring, and show that the
well known algorithm by Floyd can be used over the distance/flow semi-ring to
solve the All Pairs Shortest Paths for All Flows (APSP-AF) problem."
"Let ${\cal F}$ be a family of graphs. In the ${\cal F}$-Completion problem,
we are given a graph $G$ and an integer $k$ as input, and asked whether at most
$k$ edges can be added to $G$ so that the resulting graph does not contain a
graph from ${\cal F}$ as an induced subgraph. It appeared recently that special
cases of ${\cal F}$-Completion, the problem of completing into a chordal graph
known as Minimum Fill-in, corresponding to the case of ${\cal
F}=\{C_4,C_5,C_6,\ldots\}$, and the problem of completing into a split graph,
i.e., the case of ${\cal F}=\{C_4, 2K_2, C_5\}$, are solvable in parameterized
subexponential time $2^{O(\sqrt{k}\log{k})}n^{O(1)}$. The exploration of this
phenomenon is the main motivation for our research on ${\cal F}$-Completion.
  In this paper we prove that completions into several well studied classes of
graphs without long induced cycles also admit parameterized subexponential time
algorithms by showing that:
  - The problem Trivially Perfect Completion is solvable in parameterized
subexponential time $2^{O(\sqrt{k}\log{k})}n^{O(1)}$, that is ${\cal
F}$-Completion for ${\cal F} =\{C_4, P_4\}$, a cycle and a path on four
vertices.
  - The problems known in the literature as Pseudosplit Completion, the case
where ${\cal F} = \{2K_2, C_4\}$, and Threshold Completion, where ${\cal F} =
\{2K_2, P_4, C_4\}$, are also solvable in time $2^{O(\sqrt{k}\log{k})}
n^{O(1)}$.
  We complement our algorithms for ${\cal F}$-Completion with the following
lower bounds:
  - For ${\cal F} = \{2K_2\}$, ${\cal F} = \{C_4\}$, ${\cal F} = \{P_4\}$, and
${\cal F} = \{2K_2, P_4\}$, ${\cal F}$-Completion cannot be solved in time
$2^{o(k)} n^{O(1)}$ unless the Exponential Time Hypothesis (ETH) fails.
  Our upper and lower bounds provide a complete picture of the subexponential
parameterized complexity of ${\cal F}$-Completion problems for ${\cal
F}\subseteq\{2K_2, C_4, P_4\}$."
"We study approximation algorithms for several variants of the MaxCover
problem, with the focus on algorithms that run in FPT time. In the MaxCover
problem we are given a set N of elements, a family S of subsets of N, and an
integer K. The goal is to find up to K sets from S that jointly cover (i.e.,
include) as many elements as possible. This problem is well-known to be NP-hard
and, under standard complexity-theoretic assumptions, the best possible
polynomial-time approximation algorithm has approximation ratio (1 - 1/e). We
first consider a variant of MaxCover with bounded element frequencies, i.e., a
variant where there is a constant p such that each element belongs to at most p
sets in S. For this case we show that there is an FPT approximation scheme
(i.e., for each B there is a B-approximation algorithm running in FPT time) for
the problem of maximizing the number of covered elements, and a randomized FPT
approximation scheme for the problem of minimizing the number of elements left
uncovered (we take K to be the parameter). Then, for the case where there is a
constant p such that each element belongs to at least p sets from S, we show
that the standard greedy approximation algorithm achieves approximation ratio
exactly (1-e^{-max(pK/|S|, 1)}). We conclude by considering an unrestricted
variant of MaxCover, and show approximation algorithms that run in exponential
time and combine an exact algorithm with a greedy approximation. Some of our
results improve currently known results for MaxVertexCover."
"We consider a robust variant of the classical $k$-median problem, introduced
by Anthony et al. \cite{AnthonyGGN10}. In the \emph{Robust $k$-Median problem},
we are given an $n$-vertex metric space $(V,d)$ and $m$ client sets $\set{S_i
\subseteq V}_{i=1}^m$. The objective is to open a set $F \subseteq V$ of $k$
facilities such that the worst case connection cost over all client sets is
minimized; in other words, minimize $\max_{i} \sum_{v \in S_i} d(F,v)$. Anthony
et al.\ showed an $O(\log m)$ approximation algorithm for any metric and
APX-hardness even in the case of uniform metric. In this paper, we show that
their algorithm is nearly tight by providing $\Omega(\log m/ \log \log m)$
approximation hardness, unless ${\sf NP} \subseteq \bigcap_{\delta >0} {\sf
DTIME}(2^{n^{\delta}})$. This hardness result holds even for uniform and line
metrics. To our knowledge, this is one of the rare cases in which a problem on
a line metric is hard to approximate to within logarithmic factor. We
complement the hardness result by an experimental evaluation of different
heuristics that shows that very simple heuristics achieve good approximations
for realistic classes of instances."
"We consider a variant of the online buffer management problem in network
switches, called the $k$-frame throughput maximization problem ($k$-FTM). This
problem models the situation where a large frame is fragmented into $k$ packets
and transmitted through the Internet, and the receiver can reconstruct the
frame only if he/she accepts all the $k$ packets. Kesselman et al.\ introduced
this problem and showed that its competitive ratio is unbounded even when
$k=2$. They also introduced an ""order-respecting"" variant of $k$-FTM, called
$k$-OFTM, where inputs are restricted in some natural way. They proposed an
online algorithm and showed that its competitive ratio is at most
$\frac{2kB}{\lfloor B/k \rfloor} + k$ for any $B \ge k$, where $B$ is the size
of the buffer. They also gave a lower bound of $\frac{B}{\lfloor 2B/k \rfloor}$
for deterministic online algorithms when $2B \geq k$ and $k$ is a power of 2.
In this paper, we improve upper and lower bounds on the competitive ratio of
$k$-OFTM. Our main result is to improve an upper bound of $O(k^{2})$ by
Kesselman et al.\ to $\frac{5B + \lfloor B/k \rfloor - 4}{\lfloor B/2k \rfloor}
= O(k)$ for $B\geq 2k$. Note that this upper bound is tight up to a
multiplicative constant factor since the lower bound given by Kesselman et al.\
is $\Omega(k)$. We also give two lower bounds. First we give a lower bound of
$\frac{2B}{\lfloor {B/(k-1)} \rfloor} + 1$ on the competitive ratio of
deterministic online algorithms for any $k \geq 2$ and any $B \geq k-1$, which
improves the previous lower bound of $\frac{B}{\lfloor 2B/k \rfloor}$ by a
factor of almost four. Next, we present the first nontrivial lower bound on the
competitive ratio of randomized algorithms. Specifically, we give a lower bound
of $k-1$ against an oblivious adversary for any $k \geq 3$ and any $B$."
"Recently, there has been increasing interest and progress in improvising the
approximation algorithm for well-known NP-Complete problems, particularly the
approximation algorithm for the Vertex-Cover problem. Here we have proposed a
polynomial time efficient algorithm for vertex-cover problem for more
approximate to the optimal solution, which lead to the worst time complexity
?{\theta}(V 2) and space complexity ?{\theta}(V + E). We show that our proposed
method is more approximate with example and theorem proof. Our algorithm also
induces improvement on previous algorithms for the independent set problem on
graphs of small and high degree."
"We present the first approximate distance oracle for sparse directed networks
with time-dependent arc-travel-times determined by continuous, piecewise
linear, positive functions possessing the FIFO property.
  Our approach precomputes $(1+\epsilon)-$approximate distance summaries from
selected landmark vertices to all other vertices in the network. Our oracle
uses subquadratic space and time preprocessing, and provides two sublinear-time
query algorithms that deliver constant and $(1+\sigma)-$approximate
shortest-travel-times, respectively, for arbitrary origin-destination pairs in
the network, for any constant $\sigma > \epsilon$. Our oracle is based only on
the sparsity of the network, along with two quite natural assumptions about
travel-time functions which allow the smooth transition towards asymmetric and
time-dependent distance metrics."
"We study the problem of augmenting a weighted graph by inserting edges of
bounded total cost while minimizing the diameter of the augmented graph. Our
main result is an FPT 4-approximation algorithm for the problem."
"We study the following substring suffix selection problem: given a substring
of a string T of length n, compute its k-th lexicographically smallest suffix.
This a natural generalization of the well-known question of computing the
maximal suffix of a string, which is a basic ingredient in many other problems.
We first revisit two special cases of the problem, introduced by Babenko,
Kolesnichenko and Starikovskaya [CPM'13], in which we are asked to compute the
minimal non-empty and the maximal suffixes of a substring. For the maximal
suffixes problem, we give a linear-space structure with O(1) query time and
linear preprocessing time, i.e., we manage to achieve optimal construction and
optimal query time simultaneously. For the minimal suffix problem, we give a
linear-space data structure with O(\tau) query time and O(n log n / \tau)
preprocessing time, where 1 <= \tau <= log n is a parameter of the data
structure. As a sample application, we show that this data structure can be
used to compute the Lyndon decomposition of any substring of T in O(k \tau)
time, where k is the number of distinct factors in the decomposition.
  Finally, we move to the general case of the substring suffix selection
problem, where using any combinatorial properties seems more difficult.
Nevertheless, we develop a linear-space data structure with O(log^{2+\epsilon}
n) query time."
"We introduce a new problem that combines the well known All Pairs Shortest
Paths (APSP) problem and the All Pairs Bottleneck Paths (APBP) problem to
compute the shortest paths for all pairs of vertices for all possible flow
amounts. We call this new problem the All Pairs Shortest Paths for All Flows
(APSP-AF) problem. We firstly solve the APSP-AF problem on directed graphs with
unit edge costs and real edge capacities in
$\tilde{O}(\sqrt{t}n^{(\omega+9)/4}) = \tilde{O}(\sqrt{t}n^{2.843})$ time,
where $n$ is the number of vertices, $t$ is the number of distinct edge
capacities (flow amounts) and $O(n^{\omega}) < O(n^{2.373})$ is the time taken
to multiply two $n$-by-$n$ matrices over a ring. Secondly we extend the problem
to graphs with positive integer edge costs and present an algorithm with
$\tilde{O}(\sqrt{t}c^{(\omega+5)/4}n^{(\omega+9)/4}) =
\tilde{O}(\sqrt{t}c^{1.843}n^{2.843})$ worst case time complexity, where $c$ is
the upper bound on edge costs."
"We explore the machine-minimizing job scheduling problem, which has a rich
history in the line of research, under an online setting. We consider systems
with arbitrary job arrival times, arbitrary job deadlines, and unit job
execution time. For this problem, we present a lower bound 2.09 on the
competitive factor of \emph{any} online algorithms, followed by designing a
5.2-competitive online algorithm. We also point out a false claim made in an
existing paper of Shi and Ye regarding a further restricted case of the
considered problem. To the best of our knowledge, what we present is the first
concrete result concerning online machine-minimizing job scheduling with
arbitrary job arrival times and deadlines."
"We develop the heuristic PROBI for the probabilistic Euclidean k-median
problem based on a coreset construction by Lammersen et al. Our algorithm
computes a summary of the data and then uses an adapted version of k-means++
(Arthur and Vassilvitskii, 2007) to compute a good solution on the summary. The
summary is maintained in a data stream, so PROBI can be used in a data stream
setting on very large data sets. We experimentally evaluate the quality of the
summary and of the computed solution and compare the running time to state of
the art data stream clustering algorithms."
"In sphere of research of discrete optimization algorithms efficiency the
important place occupies a method of polynomial reducibility of some problems
to others with use of special purpose components. In this paper a novel method
of compact representation for sets of binary sequences in the form of ""compact
triplets structures"" (CTS) and ""compact couples structures"" (CCS) is stated,
supposing both logic and arithmetic interpretation of data. It is shown that
any non-empty CTS in dual interpretation represents some unique Boolean formula
in 3-CNF and the tabular CTS contains all satisfyig sets of the formula as
concatenations of the triplets chosen from the neighbouring tiers. In general,
any 3-CNF formula is transformed by decomposition to a system of discordant
CTS's, each being associated with an individual permutation of variables
constructed by a polynomial algorithm. As a result the problem of the formula
satisfiability is reduced to the following one: ascertain the fact of existence
(or absence) of a ""joint satisfying set"" (JSS) for all discordant structures,
based on the different permutations. Further transformation of each CTS to CCS
is used; correctness of preservation of the allowed sets is reached by simple
algorithmic restrictions on triplets concatenation. Then the procedure of
""inverting of the same name columns"" in the various structures is entered for
the purpose of reducing the problem of JSS revealing to elementary detection of
n-tuples of zeros in the CCS system. The formula is synthesized, being on the
structure a variation of 2-CNF, associated with the calculation procedure
realizing adaptation of the polynomial algorithm of constraints distribution
(well-known in the optimization theory) to the efficient resolving Boolean
formula coded by means of discordant compact structures."
"An edge cover of a graph is a set of edges such that every vertex has at
least an adjacent edge in it. Previously, approximation algorithm for counting
edge covers is only known for 3 regular graphs and it is randomized. We design
a very simple deterministic fully polynomial-time approximation scheme (FPTAS)
for counting the number of edge covers for any graph. Our main technique is
correlation decay, which is a powerful tool to design FPTAS for counting
problems. In order to get FPTAS for general graphs without degree bound, we
make use of a stronger notion called computationally efficient correlation
decay, which is introduced in [Li, Lu, Yin SODA 2012]."
"We study a generalization of the recently introduced order-preserving pattern
matching, where instead of looking for an exact copy of the pattern, we only
require that the relative order between the elements is the same. In our
variant, we additionally allow up to k mismatches between the pattern and the
text, and the goal is to construct an efficient algorithm for small values of
k. For a pattern of length m and a text of length n, our algorithm detects an
order-preserving occurrence with up to k mismatches in O(n(loglogm + kloglogk))
time."
"Bin covering is a dual version of classic bin packing. Thus, the goal is to
cover as many bins as possible, where covering a bin means packing items of
total size at least one in the bin.
  For online bin covering, competitive analysis fails to distinguish between
most algorithms of interest; all ""reasonable"" algorithms have a competitive
ratio of 1/2. Thus, in order to get a better understanding of the combinatorial
difficulties in solving this problem, we turn to other performance measures,
namely relative worst order, random order, and max/max analysis, as well as
analyzing input with restricted or uniformly distributed item sizes. In this
way, our study also supplements the ongoing systematic studies of the relative
strengths of various performance measures.
  Two classic algorithms for online bin packing that have natural dual versions
are Harmonic and Next-Fit. Even though the algorithms are quite different in
nature, the dual versions are not separated by competitive analysis. We make
the case that when guarantees are needed, even under restricted input
sequences, dual Harmonic is preferable. In addition, we establish quite robust
theoretical results showing that if items come from a uniform distribution or
even if just the ordering of items is uniformly random, then dual Next-Fit is
the right choice."
"In this paper, we study a generalization of the classical minimum cut prob-
lem, called Connectivity Preserving Minimum Cut (CPMC) problem, which seeks a
minimum cut to separate a pair (or pairs) of source and destination nodes and
meanwhile ensure the connectivity between the source and its partner node(s).
The CPMC problem is a rather powerful formulation for a set of problems and
finds applications in many other areas, such as network security, image
processing, data mining, pattern recognition, and machine learning. For this
important problem, we consider two variants, connectiv- ity preserving minimum
node cut (CPMNC) and connectivity preserving minimum edge cut (CPMEC). For
CPMNC, we show that it cannot be ap- proximated within {\alpha}logn for some
constant {\alpha} unless P=NP, and cannot be approximated within any poly(logn)
unless NP has quasi-polynomial time algorithms. The hardness results hold even
for graphs with unit weight and bipartite graphs. Particularly, we show that
polynomial time solutions exist for CPMEC in planar graphs and for CPMNC in
some special planar graphs. The hardness of CPMEC in general graphs remains
open, but the polynomial time algorithm in planar graphs still has important
practical applications."
"Given an elementary chain of vertex set V, seen as a labelling of V by the
set {1, ...,n=|V|}, and another discrete structure over $V$, say a graph G, the
problem of common intervals is to compute the induced subgraphs G[I], such that
$I$ is an interval of [1, n] and G[I] satisfies some property Pi (as for
example Pi= ""being connected""). This kind of problems comes from comparative
genomic in bioinformatics, mainly when the graph $G$ is a chain or a tree
(Heber and Stoye 2001, Heber and Savage 2005, Bergeron et al 2008).
  When the family of intervals is closed under intersection, we present here
the combination of two approaches, namely the idea of potential beginning
developed in Uno, Yagiura 2000 and Bui-Xuan et al 2005 and the notion of
generator as defined in Bergeron et al 2008. This yields a very simple generic
algorithm to compute all common intervals, which gives optimal algorithms in
various applications. For example in the case where $G$ is a tree, our
framework yields the first linear time algorithms for the two properties:
""being connected"" and ""being a path"". In the case where $G$ is a chain, the
problem is known as: common intervals of two permutations (Uno and Yagiura
2000), our algorithm provides not only the set of all common intervals but also
with some easy modifications a tree structure that represents this set."
"Self-avoiding walks (SAWs) were introduced in chemistry to model the
real-life behavior of chain-like entities such as solvents and polymers, whose
physical volume prohibits multiple occupation of the same spatial point. In
mathematics, a SAW lives in the n-dimensional lattices.
  In this paper, SAWs are a metaphor for walks across faces of n-dimensional
dice, or more formally, a hyperhedron family H(Theta, b, n). Each face is
assigned a label {x:Theta(x)}; x represents a unique n-dimensional coordinate
string, Theta(x) is the value of the function. The walk searches Theta(x) for
optima by following five simple rules: (1) select a random coordinate and mark
it as the `initial pivot'; (2) probe all unmarked adjacent coordinates, then
select and mark the coordinate with the 'best value' as the new pivot; (3)
continue the walk until either the 'best value' <= `target value' or the walk
is being blocked by adjacent coordinates that are already pivots; (4) if the
walk is blocked, restart the walk from a randomly selected `new initial pivot';
(5) if needed, manage the memory overflow with a streaming-like buffer of
appropriate size.
  Hard instances from a number of problem domains, including the 2D protein
folding problem, with up to (2^{25})*(3^{24}) coordinates, have been solved
with SAWs in less than 1,000,000 steps -- while also exceeding the quality of
best known solutions to date."
"In this paper, we construct a data structure to efficiently compute the
longest increasing subsequence of a sequence subject to dynamic updates. Our
data structure supports a query for the longest increasing subsequence in
$O(r+\log n)$ worst-case time and supports inserts anywhere in the sequence in
$O \left(r\log{n/r}\right)$ worst-case time (where $r$ is the length of the
longest increasing subsequence). The same data structure with a minor
modification supports $O(\log n)$ worst-case time insertions if the insertions
are performed at the end of the sequence. The data structure presented can also
be augmented to support delete operations in the same worst-case time as
insertions."
"In this paper, we introduce a variant of spectral sparsification, called
probabilistic $(\varepsilon,\delta)$-spectral sparsification. Roughly speaking,
it preserves the cut value of any cut $(S,S^{c})$ with an $1\pm\varepsilon$
multiplicative error and a $\delta\left|S\right|$ additive error. We show how
to produce a probabilistic $(\varepsilon,\delta)$-spectral sparsifier with
$O(n\log n/\varepsilon^{2})$ edges in time $\tilde{O}(n/\varepsilon^{2}\delta)$
time for unweighted undirected graph. This gives fastest known sub-linear time
algorithms for different cut problems on unweighted undirected graph such as
  - An $\tilde{O}(n/OPT+n^{3/2+t})$ time $O(\sqrt{\log n/t})$-approximation
algorithm for the sparsest cut problem and the balanced separator problem.
  - A $n^{1+o(1)}/\varepsilon^{4}$ time approximation minimum s-t cut algorithm
with an $\varepsilon n$ additive error."
"In this paper we analyze the expected time complexity of the auction
algorithm for the matching problem on random bipartite graphs. We prove that
the expected time complexity of the auction algorithm for bipartite matching is
$O\left(\frac{N\log^2(N)}{\log\left(Np\right)}\right)$ on sequential machines.
This is equivalent to other augmenting path algorithms such as the HK
algorithm. Furthermore, we show that the algorithm can be implemented on
parallel machines with $O(\log(N))$ processors and shared memory with an
expected time complexity of $O(N\log(N))$."
"A factor $u$ of a word $w$ is a cover of $w$ if every position in $w$ lies
within some occurrence of $u$ in $w$. A word $w$ covered by $u$ thus
generalizes the idea of a repetition, that is, a word composed of exact
concatenations of $u$. In this article we introduce a new notion of
$\alpha$-partial cover, which can be viewed as a relaxed variant of cover, that
is, a factor covering at least $\alpha$ positions in $w$. We develop a data
structure of $O(n)$ size (where $n=|w|$) that can be constructed in $O(n\log
n)$ time which we apply to compute all shortest $\alpha$-partial covers for a
given $\alpha$. We also employ it for an $O(n\log n)$-time algorithm computing
a shortest $\alpha$-partial cover for each $\alpha=1,2,\ldots,n$."
"A spanning tree of an unweighted graph is a minimum average stretch spanning
tree if it minimizes the ratio of sum of the distances in the tree between the
end vertices of the graph edges and the number of graph edges. We consider the
problem of computing a minimum average stretch spanning tree in polygonal
2-trees, a super class of 2-connected outerplanar graphs. For a polygonal
2-tree on $n$ vertices, we present an algorithm to compute a minimum average
stretch spanning tree in $O(n \log n)$ time. This algorithm also finds a
minimum fundamental cycle basis in polygonal 2-trees."
"In this paper we describe compressed indexes that support pattern matching
queries for strings with wildcards. For a constant size alphabet our data
structure uses $O(n\log^{\varepsilon}n)$ bits for any $\varepsilon>0$ and
reports all $\mathrm{occ}$ occurrences of a wildcard string in $O(m+\sigma^g
\cdot\mu(n) + \mathrm{occ})$ time, where $\mu(n)=o(\log\log\log n)$, $\sigma$
is the alphabet size, $m$ is the number of alphabet symbols and $g$ is the
number of wildcard symbols in the query string. We also present an $O(n)$-bit
index with $O((m+\sigma^g+\mathrm{occ})\log^{\varepsilon}n)$ query time and an
$O(n(\log\log n)^2)$-bit index with $O((m+\sigma^g+\mathrm{occ})\log\log n)$
query time. These are the first non-trivial data structures for this problem
that need $o(n\log n)$ bits of space."
"We give a bi-criteria approximation algorithm for the Minimum Nonuniform
Partitioning problem, recently introduced by Krauthgamer, Naor, Schwartz and
Talwar (2014). In this problem, we are given a graph $G=(V,E)$ on $n$ vertices
and $k$ numbers $\rho_1,\dots, \rho_k$. The goal is to partition the graph into
$k$ disjoint sets $P_1,\dots, P_k$ satisfying $|P_i|\leq \rho_i n$ so as to
minimize the number of edges cut by the partition. Our algorithm has an
approximation ratio of $O(\sqrt{\log n \log k})$ for general graphs, and an
$O(1)$ approximation for graphs with excluded minors. This is an improvement
upon the $O(\log n)$ algorithm of Krauthgamer, Naor, Schwartz and Talwar
(2014). Our approximation ratio matches the best known ratio for the Minimum
(Uniform) $k$-Partitioning problem.
  We extend our results to the case of ""unrelated weights"" and to the case of
""unrelated $d$-dimensional weights"". In the former case, different vertices may
have different weights and the weight of a vertex may depend on the set $P_i$
the vertex is assigned to. In the latter case, each vertex $u$ has a
$d$-dimensional weight $r(u,i) = (r_1(u,i), \dots, r_d(u,i))$ if $u$ is
assigned to $P_i$. Each set $P_i$ has a $d$-dimensional capacity $c(i) =
(c_1(i),\dots, c_d(i))$. The goal is to find a partition such that $\sum_{u\in
{P_i}} r(u,i) \leq c(i)$ coordinate-wise."
"We present a message-passing based parallel version of the Space Saving
algorithm designed to solve the $k$--majority problem. The algorithm determines
in parallel frequent items, i.e., those whose frequency is greater than a given
threshold, and is therefore useful for iceberg queries and many other different
contexts. We apply our algorithm to the detection of frequent items in both
real and synthetic datasets whose probability distribution functions are a
Hurwitz and a Zipf distribution respectively. Also, we compare its parallel
performances and accuracy against a parallel algorithm recently proposed for
merging summaries derived by the Space Saving or Frequent algorithms."
"We present a data structure that allows to maintain in logarithmic time all
partial sums of elements of a linear array during incremental changes of
element's values."
"We show that the compressed suffix array and the compressed suffix tree for a
string of length $n$ over an integer alphabet of size $\sigma\leq n$ can both
be built in $O(n)$ (randomized) time using only $O(n\log\sigma)$ bits of
working space. The previously fastest construction algorithms that used
$O(n\log\sigma)$ bits of space took times $O(n\log\log\sigma)$ and
$O(n\log^{\epsilon}n)$ respectively (where $\epsilon$ is any positive constant
smaller than $1$). In the passing, we show that the Burrows-Wheeler transform
of a string of length $n$ over an alphabet of size $\sigma$ can be built in
deterministic $O(n)$ time and space $O(n\log\sigma)$. We also show that within
the same time and space, we can carry many sequence analysis tasks and
construct some variants of the compressed suffix array and compressed suffix
tree."
"We present a new uniform random sampler for binary trees with $n$ internal
nodes consuming $2n + \Theta(\log(n)^2)$ random bits on average. This makes it
quasi-optimal and out-performs the classical Remy algorithm. We also present a
sampler for unary-binary trees with $n$ nodes taking $\Theta(n)$ random bits on
average. Both are the first linear-time algorithms to be optimal up to a
constant."
"In this paper I present a conjecture for a recursive algorithm that finds
each permutation of combining two sets of objects (AKA the Shuffle Product).
This algorithm provides an efficient way to navigate this problem, as each
atomic operation yields a permutation of the union. The permutations of the
union of the two sets are represented as binary integers which are then
manipulated mathematically to find the next permutation. The routes taken to
find each of the permutations then form a series of associations or adjacencies
which can be represented in a tree graph which appears to possess some
properties of a fractal.
  This algorithm was discovered while attempting to identify every possible
end-state of a Tic-Tac-Toe (Naughts and Crosses) board. It was found to be a
viable and efficient solution to the problem, and now---in its more generalized
state---it is my belief that it may find applications among a wide range of
theoretical and applied sciences.
  I hypothesize that, due to the fractal-like nature of the tree it traverses,
this algorithm sheds light on a more generic principle of combinatorics and as
such could be further generalized to perhaps be applied to the union of any
number of sets."
"In this paper we consider the problem of approximating frequency moments in
the streaming model. Given a stream $D = \{p_1,p_2,\dots,p_m\}$ of numbers from
$\{1,\dots, n\}$, a frequency of $i$ is defined as $f_i = |\{j: p_j = i\}|$.
The $k$-th \emph{frequency moment} of $D$ is defined as $F_k = \sum_{i=1}^n
f_i^k$.
  In this paper we give an upper bound on the space required to find a $k$-th
frequency moment of $O(n^{1-2/k})$ bits that matches, up to a constant factor,
the lower bound of Woodruff and Zhang (STOC 12) for constant $\epsilon$ and
constant $k$. Our algorithm makes a single pass over the stream and works for
any constant $k > 3$."
"In binary jumbled pattern matching we wish to preprocess a binary string $S$
in order to answer queries $(i,j)$ which ask for a substring of $S$ that is of
size $i$ and has exactly $j$ 1-bits. The problem naturally generalizes to
node-labeled trees and graphs by replacing ""substring"" with ""connected
subgraph"".
  In this paper, we give an ${n^2}/{2^{\Omega(\log n/\log \log n)^{1/2}}}$ time
solution for both strings and trees. This odd-looking time complexity improves
the state of the art $O(n^2/\log^2 n)$ solutions by more than any
poly-logarithmic factor. It originates from the recent seminal algorithm of
Williams for min-plus matrix multiplication. We obtain the result by giving a
black box reduction from trees to strings. This is then combined with a
reduction from strings to min-plus matrix multiplications."
"In this paper we present improved results on the problem of counting
triangles in edge streamed graphs. For graphs with $m$ edges and at least $T$
triangles, we show that an extra look over the stream yields a two-pass
treaming algorithm that uses $O(\frac{m}{\eps^{2.5}\sqrt{T}}\polylog(m))$ space
and outputs a $(1+\eps)$ approximation of the number of triangles in the graph.
This improves upon the two-pass streaming tester of Braverman, Ostrovsky and
Vilenchik, ICALP 2013, which distinguishes between triangle-free graphs and
graphs with at least $T$ triangle using $O(\frac{m}{T^{1/3}})$ space. Also, in
terms of dependence on $T$, we show that more passes would not lead to a better
space bound. In other words, we prove there is no constant pass streaming
algorithm that distinguishes between triangle-free graphs from graphs with at
least $T$ triangles using $O(\frac{m}{T^{1/2+\rho}})$ space for any constant
$\rho \ge 0$."
"Consider the problem of finding a point in an n-point metric space with the
minimum average distance to all points. We show that this problem has no
deterministic $o(n^2)$-query $(4-\Omega(1))$-approximation algorithms."
"This application for learning APPROXIMATION ALGORITHM has been designed in
Java which will make user comfortable in learning the very complex subject
""NP-Completeness"" and the solution to NP-Complete problem using approximation
algorithm."
"We give a generalized definition of stretch that simplifies the efficient
construction of low-stretch embeddings suitable for graph algorithms. The
generalization, based on discounting highly stretched edges by taking their
$p$-th power for some $0 < p < 1$, is directly related to performances of
existing algorithms. This discounting of high-stretch edges allows us to treat
many classes of edges with coarser granularity. It leads to a two-pass approach
that combines bottom-up clustering and top-down decompositions to construct
these embeddings in $\mathcal{O}(m\log\log{n})$ time. Our algorithm
parallelizes readily and can also produce generalizations of low-stretch
subgraphs."
"A $\textit{compression scheme}$ $A$ for a class $\mathbb{G}$ of graphs
consists of an encoding algorithm $\textit{Encode}_A$ that computes a binary
string $\textit{Code}_A(G)$ for any given graph $G$ in $\mathbb{G}$ and a
decoding algorithm $\textit{Decode}_A$ that recovers $G$ from
$\textit{Code}_A(G)$. A compression scheme $A$ for $\mathbb{G}$ is
$\textit{optimal}$ if both $\textit{Encode}_A$ and $\textit{Decode}_A$ run in
linear time and the number of bits of $\textit{Code}_A(G)$ for any $n$-node
graph $G$ in $\mathbb{G}$ is information-theoretically optimal to within
lower-order terms. Trees and plane triangulations were the only known
nontrivial graph classes that admit optimal compression schemes. Based upon
Goodrich's separator decomposition for planar graphs and Djidjev and
Venkatesan's planarizers for bounded-genus graphs, we give an optimal
compression scheme for any hereditary (i.e., closed under taking subgraphs)
class $\mathbb{G}$ under the premise that any $n$-node graph of $\mathbb{G}$ to
be encoded comes with a genus-$o(\frac{n}{\log^2 n})$ embedding. By Mohar's
linear-time algorithm that embeds a bounded-genus graph on a genus-$O(1)$
surface, our result implies that any hereditary class of genus-$O(1)$ graphs
admits an optimal compression scheme. For instance, our result yields the
first-known optimal compression schemes for planar graphs, plane graphs, graphs
embedded on genus-$1$ surfaces, graphs with genus $2$ or less, $3$-colorable
directed plane graphs, $4$-outerplanar graphs, and forests with degree at most
$5$. For non-hereditary graph classes, we also give a methodology for obtaining
optimal compression schemes. From this methodology, we give the first known
optimal compression schemes for triangulations of genus-$O(1)$ surfaces and
floorplans."
"The $k$-center problem is a classic facility location problem, where given an
edge-weighted graph $G = (V,E)$ one is to find a subset of $k$ vertices $S$,
such that each vertex in $V$ is ""close"" to some vertex in $S$. The
approximation status of this basic problem is well understood, as a simple
2-approximation algorithm is known to be tight. Consequently different
extensions were studied.
  In the capacitated version of the problem each vertex is assigned a capacity,
which is a strict upper bound on the number of clients a facility can serve,
when located at this vertex. A constant factor approximation for the
capacitated $k$-center was obtained last year by Cygan, Hajiaghayi and Khuller
[FOCS'12], which was recently improved to a 9-approximation by An, Bhaskara and
Svensson [arXiv'13].
  In a different generalization of the problem some clients (denoted as
outliers) may be disregarded. Here we are additionally given an integer $p$ and
the goal is to serve exactly $p$ clients, which the algorithm is free to
choose. In 2001 Charikar et al. [SODA'01] presented a 3-approximation for the
$k$-center problem with outliers.
  In this paper we consider a common generalization of the two extensions
previously studied separately, i.e. we work with the capacitated $k$-center
with outliers. We present the first constant factor approximation algorithm
with approximation ratio of 25 even for the case of non-uniform hard
capacities."
"The k-means++ seeding algorithm is one of the most popular algorithms that is
used for finding the initial $k$ centers when using the k-means heuristic. The
algorithm is a simple sampling procedure and can be described as follows: Pick
the first center randomly from the given points. For $i > 1$, pick a point to
be the $i^{th}$ center with probability proportional to the square of the
Euclidean distance of this point to the closest previously $(i-1)$ chosen
centers.
  The k-means++ seeding algorithm is not only simple and fast but also gives an
$O(\log{k})$ approximation in expectation as shown by Arthur and Vassilvitskii.
There are datasets on which this seeding algorithm gives an approximation
factor of $\Omega(\log{k})$ in expectation. However, it is not clear from these
results if the algorithm achieves good approximation factor with reasonably
high probability (say $1/poly(k)$). Brunsch and R\""{o}glin gave a dataset where
the k-means++ seeding algorithm achieves an $O(\log{k})$ approximation ratio
with probability that is exponentially small in $k$. However, this and all
other known lower-bound examples are high dimensional. So, an open problem was
to understand the behavior of the algorithm on low dimensional datasets. In
this work, we give a simple two dimensional dataset on which the seeding
algorithm achieves an $O(\log{k})$ approximation ratio with probability
exponentially small in $k$. This solves open problems posed by Mahajan et al.
and by Brunsch and R\""{o}glin."
"Given a set of rectangular modules with fixed area and variable dimensions,
and a fixed rectangular circuit. The placement of Fixed-Outline Floorplanning
with Soft Modules (FOFSM) aims to determine the dimensions and position of each
module on the circuit. We present a two-stage Iterative Merging Placement (IMP)
algorithm for the FOFSM with zero deadspace constraint. The first stage
iteratively merges two modules with the least area into a composite module to
achieve a final composite module, and builds up a slicing tree in a bottom-up
hierarchy. The second stage recursively determines the relative relationship
(left-right or top-bottom) of the sibling modules in the slicing tree in a
top-down hierarchy, and the dimensions and position of each leaf module are
determined automatically. Compared with zero-dead-space (ZDS) algorithm, the
only algorithm guarantees a feasible layout under some condition, we prove that
the proposed IMP could construct a feasible layout under a more relaxed
condition. Besides, IMP is more scalable in handling FOFSM considering the
wirelength or without the zero deadspace constraint."
"We give an improved analysis of the simple $D^2$-sampling based PTAS for the
$k$-means clustering problem given by Jaiswal, Kumar, and Sen (Algorithmica,
2013). The improvement on the running time is from $O\left(nd \cdot
2^{\tilde{O}(k^2/\epsilon)}\right)$ to $O\left(nd \cdot
2^{\tilde{O}(k/\epsilon)}\right)$."
"The list coloring problem is a variant of vertex coloring where a vertex may
be colored only a color from a prescribed set. Several applications of vertex
coloring are more appropriately modelled as instances of list coloring and thus
we argue that it is an important problem to consider. Regardless of the
importance of list coloring, few published algorithms exist for it. In this
paper we review the only two existing ones we could find and propose an exact
branch and bound one. We conduct an experimental evaluation of the three
algorithms."
"We consider several Vehicle Routing Problems (VRP) with profits, which seek
to select a subset of customers, each one being associated with a profit, and
to design service itineraries. When the sum of profits is maximized under
distance constraints, the problem is usually called team orienteering problem.
The capacitated profitable tour problem seeks to maximize profits minus travel
costs under capacity constraints. Finally, in the VRP with private fleet and
common carrier, some customers can be delegated to an external carrier subject
to a cost. Three families of combined decisions must be taken: customers
selection, assignment to vehicles, and sequencing of deliveries for each route.
  We propose a new neighborhood search for these problems which explores an
exponential number of solutions in pseudo polynomial time. The search is
conducted with standard VRP neighborhoods on an ""exhaustive"" solution
representation, visiting all customers. Since visiting all customers is usually
infeasible or sub-optimal, an efficient ""Select"" algorithm, based on resource
constrained shortest paths, is repeatedly used on any new route to find the
optimal subsequence of visits to customers. The good performance of these
neighborhood structures is demonstrated by extensive computational experiments
with a local search, an iterated local search and a hybrid genetic algorithm.
Intriguingly, even a local-improvement method to the first local optimum of
this neighborhood achieves an average gap of 0.09% on classic team orienteering
benchmark instances, rivaling with the current state-of-the-art metaheuristics.
Promising research avenues on hybridizations with more standard routing
neighborhoods are also open."
"This paper describes the transition of a male-pessimal matching set to
optimal when it is a man-oriented approach by deleting a pair from matching set
considering the score based approach. A descriptive explanation of the proposed
algorithm both in a sequential and parallel manner is given. The comparison
based theoretical analysis shows that the best case of the algorithm is lower
bound of n3."
"A binary matrix has the Consecutive Ones Property (C1P) if its columns can be
ordered in such a way that all 1s on each row are consecutive. These matrices
are used for DNA physical mapping and ancestral genome reconstruction in
computational biology on the other hand they represents a class of convex
bipartite graphs and are of interest of algorithm graph theory researchers.
Tucker gave a forbidden submartices characterization of matrices that have C1P
property in 1972. Booth and Lucker (1976) gave a first linear time recognition
algorithm for matrices with C1P property and then in 2002, Habib, et al. gave a
simpler linear time recognition algorithm. There has been substantial amount of
works on efficiently finding minimum size forbidden submatrix. Our algorithm is
at least $n$ times faster than the existing algorithm where $n$ is the number
of columns of the input matrix."
"We present novel variants of fully online LCA (FOLCA), a fully online grammar
compression that builds a straight line program (SLP) and directly encodes it
into a succinct representation in an online manner. FOLCA enables a direct
encoding of an SLP into a succinct representation that is asymptotically
equivalent to an information theoretic lower bound for representing an SLP
(Maruyama et al., SPIRE'13). The compression of FOLCA takes linear time
proportional to the length of an input text and its working space depends only
on the size of the SLP, which enables us to apply FOLCA to large-scale
repetitive texts. Recent repetitive texts, however, include some noise. For
example, current sequencing technology has significant error rates, which
embeds noise into genome sequences. For such noisy repetitive texts, FOLCA
working in the SLP size consumes a large amount of memory. We present two
variants of FOLCA working in constant space by leveraging the idea behind
stream mining techniques. Experiments using 100 human genomes corresponding to
about 300GB from the 1000 human genomes project revealed the applicability of
our method to large-scale, noisy repetitive texts."
"In this paper we consider the problem of computing the $2$-vertex-connected
components ($2$-vccs) of directed graphs. We present two new algorithms for
solving this problem. The first algorithm runs in $O(mn^{2})$ time, the second
in $O(nm)$ time. Furthermore, we show that the old algorithm of Erusalimskii
and Svetlov runs in $O(nm^{2})$ time. In this paper, we investigate the
relationship between $2$-vccs and dominator trees. We also present an algorithm
for computing the $3$-vertex-connected components ($3$-vccs) of a directed
graph in $O(n^{3}m)$ time, and we show that the $k$-vertex-connected components
($k$-vccs) of a directed graph can be computed in $O(mn^{2k-3})$ time. Finally,
we consider three applications of our new algorithms, which are approximation
algorithms for problems that are generalization of the problem of approximating
the smallest $2$-vertex-connected spanning subgraph of $2$-vertex-connected
directed graph."
"In this paper, we introduce an exact algorithm with a time complexity of
$O^*(1.325^m)$ for the {\sc weighted mutually exclusive maximum set cover}
problem, where $m$ is the number of subsets in the problem. This is an NP-hard
motivated and abstracted from a bioinformatics problem of identifying signaling
pathways based gene mutations. Currently, this problem is addressed using
heuristic algorithms, which cannot guarantee the performance of the solution.
By providing a relatively efficient exact algorithm, our approach will like
increase the capability of finding better solutions in the application of
cancer research."
"In this paper, we introduce two new algorithm to find a Hamilton Circuit in a
graph G=(V,E). One algorithm is use a multistage graph as a special NFAs to
find all Hamilton Circuit in exponential time; while another is use O(|V|)
variant multistage graph as a special state set NFAs to find a fuzzy data of
all Hamilton Circuit in polynomial time. The fuzzy data of the data contain
those data, and the fuzzy data is not empty, if and only if the data is also
not empty. And, the data is also not empty if and only if there are Hamilton
Circuit in the graph. And we can find a Hamilton Circuit by the fuzzy data. Our
result implies NP=P."
"This paper addresses a new vehicle routing problem that simultaneously
involves time windows, split collection and linear weight-related cost, which
is a generalization of the split delivery vehicle routing problem with time
windows (SDVRPTW). This problem consists of determining least-cost vehicle
routes to serve a set of customers while respecting the restrictions of vehicle
capacity and time windows. The travel cost per unit distance is a linear
function of the vehicle weight and the customer demand can be fulfilled by
multiple vehicles. To solve this problem, we propose a exact
branch-and-price-and-cut algorithm, where the pricing subproblem is a
resource-constrained elementary least-cost path problem. We first prove that at
least an optimal solution to the pricing subproblem is associated with an
extreme collection pattern, and then design a tailored and novel label-setting
algorithm to solve it. Computational results show that our proposed algorithm
can handle both the SDVRPTW and our problem effectively."
"We report an N-Body approach to computing the Fock exchange matrix with and
without permutational symmetry. The method achieves an O(N lg N) computational
complexity through an embedded metric-query, allowing hierarchical application
of direct SCF criteria. The advantages of permutational symmetry are found to
be 4-fold for small systems, but decreasing with increasing system size and/or
more permissive neglect criteria. This work sets the stage for: (1) the
introduction of range queries in multi-level multipole schemes for rank
reduction, and (2) recursive task parallelism."
"Betweenness centrality is a classic measure that quantifies the importance of
a graph element (vertex or edge) according to the fraction of shortest paths
passing through it. This measure is notoriously expensive to compute, and the
best known algorithm runs in O(nm) time. The problems of efficiency and
scalability are exacerbated in a dynamic setting, where the input is an
evolving graph seen edge by edge, and the goal is to keep the betweenness
centrality up to date. In this paper we propose the first truly scalable
algorithm for online computation of betweenness centrality of both vertices and
edges in an evolving graph where new edges are added and existing edges are
removed. Our algorithm is carefully engineered with out-of-core techniques and
tailored for modern parallel stream processing engines that run on clusters of
shared-nothing commodity hardware. Hence, it is amenable to real-world
deployment. We experiment on graphs that are two orders of magnitude larger
than previous studies. Our method is able to keep the betweenness centrality
measures up to date online, i.e., the time to update the measures is smaller
than the inter-arrival time between two consecutive updates."
"We put forth a new string matching algorithm which matches the pattern from
neither the left nor the right end, instead a special position. Comparing with
the Knuth-Morris-Pratt algorithm and the Boyer-Moore algorithm, the new
algorithm is more flexible to pick the position for starting comparisons. The
option really brings it a saving in cost."
"Suppose we concatenate two directed graphs, each isomorphic to a $d$
dimensional butterfly (but not necessarily identical to each other). Select any
set of $2^k$ input and $2^k$ output nodes on the resulting graph. Then there
exist node disjoint paths from the input nodes to the output nodes. If we take
two standard butterflies and permute the order of the layers, then the result
holds on sets of any size, not just powers of two."
"We consider some flow-time minimization problems in the unrelated machines
setting. In this setting, there is a set of $m$ machines and a set of $n$ jobs,
and each job $j$ has a machine dependent processing time of $p_{ij}$ on machine
$i$. The flow-time of a job is the total time the job spends in the system
(completion time minus its arrival time), and is one of the most natural
quality of service measure. We show the following two results: an
$O(\min(\log^2 n,\log n \log P))$ approximation algorithm for minimizing the
total-flow time, and an $O(\log n)$ approximation for minimizing the maximum
flow-time. Here $P$ is the ratio of maximum to minimum job size. These are the
first known poly-logarithmic guarantees for both the problems."
"We introduce the {Destructive Object Handling} (DOH) problem, which models
aspects of many real-world allocation problems, such as shipping explosive
munitions, scheduling processes in a cluster with fragile nodes, re-using
passwords across multiple websites, and quarantining patients during a disease
outbreak. In these problems, objects must be assigned to handlers, but each
object has a probability of destroying itself and all the other objects
allocated to the same handler. The goal is to maximize the expected value of
the objects handled successfully.
  We show that finding the optimal allocation is
$\mathsf{NP}$-$\mathsf{complete}$, even if all the handlers are identical. We
present an FPTAS when the number of handlers is constant. We note in passing
that the same technique also yields a first FPTAS for the weapons-target
allocation problem \cite{manne_wta} with a constant number of targets. We study
the structure of DOH problems and find that they have a sort of phase
transition -- in some instances it is better to spread risk evenly among the
handlers, in others, one handler should be used as a ``sacrificial lamb''. We
show that the problem is solvable in polynomial time if the destruction
probabilities depend only on the handler to which an object is assigned; if all
the handlers are identical and the objects all have the same value; or if each
handler can be assigned at most one object.
  Finally, we empirically evaluate several heuristics based on a combination of
greedy and genetic algorithms. The proposed heuristics return fairly high
quality solutions to very large problem instances (upto 250 objects and 100
handlers) in tens of seconds."
"In this paper, we describe the result of our experiments on Algorithms for
the Food-Selection Problem, which is the fundamental problem first stated and
addressed in the seminal paper \cite{pigout}. Because the key aspect of any
experimental evaluation is the \textbf{reproducibility}, we detail deeply the
setup of all our experiments, thus leaving to the interested eater the
opportunity to reproduce all the results described in this paper. More
specifically, we describe all the answers we provided to the questions proposed
in \cite{pigout}: Where can I have dinner tonight? What is the typical Roman
cuisine that I should (not) miss? Where can I find the best coffee or gelato in
town?"
"In a graph $G$, a vertex subset $S\subseteq V(G)$ is said to be a dominating
set of $G$ if every vertex not in $S$ is adjacent to a vertex in $S$. A
dominating set $S$ of a graph $G$ is called a paired-dominating set if the
induced subgraph $G[S]$ contains a perfect matching. The paired-domination
problem involves finding a smallest paired-dominating set of $G$. Given an
intersection model of an interval graph $G$ with sorted endpoints, Cheng et al.
designed an $O(m+n)$-time algorithm for interval graphs and an $O(m(m+n))$-time
algorithm for circular-arc graphs. In this paper, to solve the
paired-domination problem in interval graphs, we propose an $O(n)$-time
algorithm that searches for a minimum paired-dominating set of $G$
incrementally in a greedy manner. Then, we extend the results to design an
algorithm for circular-arc graphs that also runs in $O(n)$ time."
"Robin Hood hashing is a variation on open addressing hashing designed to
reduce the maximum search time as well as the variance in the search time for
elements in the hash table. While the case of insertions only using Robin Hood
hashing is well understood, the behavior with deletions has remained open. Here
we show that Robin Hood hashing can be analyzed under the framework of
finite-level finite-dimensional jump Markov chains. This framework allows us to
re-derive some past results for the insertion-only case with some new insight,
as well as provide a new analysis for a standard deletion model, where we
alternate between deleting a random old key and inserting a new one. In
particular, we show that a simple but apparently unstudied approach for
handling deletions with Robin Hood hashing offers good performance even under
high loads."
"Between the leaves and the nodes of a complete binary tree, a separate
parent-child-sister hierarchy is employed independent of the
parent-child-sister hierarchy used for the rest of the tree. Two different
versions of such a local hierarchy are introduced. The result of the first
proposed hierarchy is a faster and smaller footprint, while the second one
provides the size variation functionality without a significant computational
overhead. This novel approach brings considerable memory gains and performance
boosts to the complete binary tree based algorithms."
"The unit cost model is both convenient and largely realistic for describing
integer decision algorithms over (+,*). Additional operations like division
with remainder or bitwise conjunction, although equally supported by computing
hardware, may lead to a considerable drop in complexity. We show a variety of
concrete problems to benefit from such NON-arithmetic primitives by presenting
and analyzing corresponding fast algorithms."
"Numerous studies show that most known real-world complex networks share
similar properties in their connectivity and degree distribution. They are
called small worlds. This article gives a method to turn random graphs into
Small World graphs by the dint of random walks."
"There exists an injective, information-preserving function that maps a
semantic network (i.e a directed labeled network) to a directed network (i.e. a
directed unlabeled network). The edge label in the semantic network is
represented as a topological feature of the directed network. Also, there
exists an injective function that maps a directed network to an undirected
network (i.e. an undirected unlabeled network). The edge directionality in the
directed network is represented as a topological feature of the undirected
network. Through function composition, there exists an injective function that
maps a semantic network to an undirected network. Thus, aside from space
constraints, the semantic network construct does not have any modeling
functionality that is not possible with either a directed or undirected network
representation. Two proofs of this idea will be presented. The first is a proof
of the aforementioned function composition concept. The second is a simpler
proof involving an undirected binary encoding of a semantic network."
"The generalized traveling salesman problem (GTSP) is an extension of the
well-known traveling salesman problem. In GTSP, we are given a partition of
cities into groups and we are required to find a minimum length tour that
includes exactly one city from each group. The recent studies on this subject
consider different variations of a memetic algorithm approach to the GTSP. The
aim of this paper is to present a new memetic algorithm for GTSP with a
powerful local search procedure. The experiments show that the proposed
algorithm clearly outperforms all of the known heuristics with respect to both
solution quality and running time. While the other memetic algorithms were
designed only for the symmetric GTSP, our algorithm can solve both symmetric
and asymmetric instances."
"The generalized traveling salesman problem (GTSP) is an extension of the
well-known traveling salesman problem. In GTSP, we are given a partition of
cities into groups and we are required to find a minimum length tour that
includes exactly one city from each group. The aim of this paper is to present
a problem reduction algorithm that deletes redundant vertices and edges,
preserving the optimal solution. The algorithm's running time is O(N^3) in the
worst case, but it is significantly faster in practice. The algorithm has
reduced the problem size by 15-20% on average in our experiments and this has
decreased the solution time by 10-60% for each of the considered solvers."
"Let X[0..n-1] and Y[0..m-1] be two sorted arrays, and define the mxn matrix A
by A[j][i]=X[i]+Y[j]. Frederickson and Johnson gave an efficient algorithm for
selecting the k-th smallest element from A. We show how to make this algorithm
IO-efficient. Our cache-oblivious algorithm performs O((m+n)/B) IOs, where B is
the block size of memory transfers."
"The Hierarchical Memory Model (HMM) of computation is similar to the standard
Random Access Machine (RAM) model except that the HMM has a non-uniform memory
organized in a hierarchy of levels numbered 1 through h. The cost of accessing
a memory location increases with the level number, and accesses to memory
locations belonging to the same level cost the same. Formally, the cost of a
single access to the memory location at address a is given by m(a), where m: N
-> N is the memory cost function, and the h distinct values of m model the
different levels of the memory hierarchy.
  We study the problem of constructing and storing a binary search tree (BST)
of minimum cost, over a set of keys, with probabilities for successful and
unsuccessful searches, on the HMM with an arbitrary number of memory levels,
and for the special case h=2.
  While the problem of constructing optimum binary search trees has been well
studied for the standard RAM model, the additional parameter m for the HMM
increases the combinatorial complexity of the problem. We present two dynamic
programming algorithms to construct optimum BSTs bottom-up. These algorithms
run efficiently under some natural assumptions about the memory hierarchy. We
also give an efficient algorithm to construct a BST that is close to optimum,
by modifying a well-known linear-time approximation algorithm for the RAM
model. We conjecture that the problem of constructing an optimum BST for the
HMM with an arbitrary memory cost function m is NP-complete."
"We consider the problem of computing L1-distances between every pair
ofcprobability densities from a given family. We point out that the technique
of Cauchy random projections (Indyk'06) in this context turns into stochastic
integrals with respect to Cauchy motion.
  For piecewise-linear densities these integrals can be sampled from if one can
sample from the stochastic integral of the function x->(1,x). We give an
explicit density function for this stochastic integral and present an efficient
sampling algorithm. As a consequence we obtain an efficient algorithm to
approximate the L1-distances with a small relative error.
  For piecewise-polynomial densities we show how to approximately sample from
the distributions resulting from the stochastic integrals. This also results in
an efficient algorithm to approximate the L1-distances, although our inability
to get exact samples worsens the dependence on the parameters."
"The Min Energy broadcast problem consists in assigning transmission ranges to
the nodes of an ad-hoc network in order to guarantee a directed spanning tree
from a given source node and, at the same time, to minimize the energy
consumption (i.e. the energy cost) yielded by the range assignment. Min energy
broadcast is known to be NP-hard.
  We consider random-grid networks where nodes are chosen independently at
random from the $n$ points of a $\sqrt n \times \sqrt n$ square grid in the
plane. The probability of the existence of a node at a given point of the grid
does depend on that point, that is, the probability distribution can be
non-uniform.
  By using information-theoretic arguments, we prove a lower bound
$(1-\epsilon) \frac n{\pi}$ on the energy cost of any feasible solution for
this problem. Then, we provide an efficient solution of energy cost not larger
than $1.1204 \frac n{\pi}$.
  Finally, we present a fully-distributed protocol that constructs a broadcast
range assignment of energy cost not larger than $8n$,thus still yielding
constant approximation. The energy load is well balanced and, at the same time,
the work complexity (i.e. the energy due to all message transmissions of the
protocol) is asymptotically optimal. The completion time of the protocol is
only an $O(\log n)$ factor slower than the optimum. The approximation quality
of our distributed solution is also experimentally evaluated.
  All bounds hold with probability at least $1-1/n^{\Theta(1)}$."
"Contraction hierarchies are a simple hierarchical routing technique that has
proved extremely efficient for static road networks. We explain how to
generalize them to networks with time-dependent edge weights. This is the first
hierarchical speedup technique for time-dependent routing that allows
bidirectional query algorithms."
"We conclude a sequence of work by giving near-optimal sketching and streaming
algorithms for estimating Shannon entropy in the most general streaming model,
with arbitrary insertions and deletions. This improves on prior results that
obtain suboptimal space bounds in the general model, and near-optimal bounds in
the insertion-only model without sketching. Our high-level approach is simple:
we give algorithms to estimate Renyi and Tsallis entropy, and use them to
extrapolate an estimate of Shannon entropy. The accuracy of our estimates is
proven using approximation theory arguments and extremal properties of
Chebyshev polynomials, a technique which may be useful for other problems. Our
work also yields the best-known and near-optimal additive approximations for
entropy, and hence also for conditional entropy and mutual information."
"We study the minimum backlog problem (MBP). This online problem arises, e.g.,
in the context of sensor networks. We focus on two main variants of MBP.
  The discrete MBP is a 2-person game played on a graph $G=(V,E)$. The player
is initially located at a vertex of the graph. In each time step, the adversary
pours a total of one unit of water into cups that are located on the vertices
of the graph, arbitrarily distributing the water among the cups. The player
then moves from her current vertex to an adjacent vertex and empties the cup at
that vertex. The player's objective is to minimize the backlog, i.e., the
maximum amount of water in any cup at any time.
  The geometric MBP is a continuous-time version of the MBP: the cups are
points in the two-dimensional plane, the adversary pours water continuously at
a constant rate, and the player moves in the plane with unit speed. Again, the
player's objective is to minimize the backlog.
  We show that the competitive ratio of any algorithm for the MBP has a lower
bound of $\Omega(D)$, where $D$ is the diameter of the graph (for the discrete
MBP) or the diameter of the point set (for the geometric MBP). Therefore we
focus on determining a strategy for the player that guarantees a uniform upper
bound on the absolute value of the backlog.
  For the absolute value of the backlog there is a trivial lower bound of
$\Omega(D)$, and the deamortization analysis of Dietz and Sleator gives an
upper bound of $O(D\log N)$ for $N$ cups. Our main result is a tight upper
bound for the geometric MBP: we show that there is a strategy for the player
that guarantees a backlog of $O(D)$, independently of the number of cups."
"We introduce several generalizations of classical computer science problems
obtained by replacing simpler objective functions with general submodular
functions. The new problems include submodular load balancing, which
generalizes load balancing or minimum-makespan scheduling, submodular sparsest
cut and submodular balanced cut, which generalize their respective graph cut
problems, as well as submodular function minimization with a cardinality lower
bound. We establish upper and lower bounds for the approximability of these
problems with a polynomial number of queries to a function-value oracle. The
approximation guarantees for most of our algorithms are of the order of
sqrt(n/ln n). We show that this is the inherent difficulty of the problems by
proving matching lower bounds. We also give an improved lower bound for the
problem of approximately learning a monotone submodular function. In addition,
we present an algorithm for approximately learning submodular functions with
special structure, whose guarantee is close to the lower bound. Although quite
restrictive, the class of functions with this structure includes the ones that
are used for lower bounds both by us and in previous work. This demonstrates
that if there are significantly stronger lower bounds for this problem, they
rely on more general submodular functions."
"The paper presents a simple, linear time, in-place algorithm for performing a
2-way in-shuffle which can be used with little modification for certain other
k-way shuffles."
"The Noah's Ark Problem (NAP) is an NP-Hard optimization problem with
relevance to ecological conservation management. It asks to maximize the
phylogenetic diversity (PD) of a set of taxa given a fixed budget, where each
taxon is associated with a cost of conservation and a probability of
extinction. NAP has received renewed interest with the rise in availability of
genetic sequence data, allowing PD to be used as a practical measure of
biodiversity. However, only simplified instances of the problem, where one or
more parameters are fixed as constants, have as of yet been addressed in the
literature. We present NAPX, the first algorithm for the general version of NAP
that returns a $1 - \epsilon$ approximation of the optimal solution. It runs in
$O(\frac{n B^2 h^2 \log^2n}{\log^2(1 - \epsilon)})$ time where $n$ is the
number of species, and $B$ is the total budget and $h$ is the height of the
input tree. We also provide improved bounds for its expected running time."
"The celebrated multi-armed bandit problem in decision theory models the basic
trade-off between exploration, or learning about the state of a system, and
exploitation, or utilizing the system. In this paper we study the variant of
the multi-armed bandit problem where the exploration phase involves costly
experiments and occurs before the exploitation phase; and where each play of an
arm during the exploration phase updates a prior belief about the arm. The
problem of finding an inexpensive exploration strategy to optimize a certain
exploitation objective is NP-Hard even when a single play reveals all
information about an arm, and all exploration steps cost the same.
  We provide the first polynomial time constant-factor approximation algorithm
for this class of problems. We show that this framework also generalizes
several problems of interest studied in the context of data acquisition in
sensor networks. Our analyses also extends to switching and setup costs, and to
concave utility objectives.
  Our solution approach is via a novel linear program rounding technique based
on stochastic packing. In addition to yielding exploration policies whose
performance is within a small constant factor of the adaptive optimal policy, a
nice feature of this approach is that the resulting policies explore the arms
sequentially without revisiting any arm. Sequentiality is a well-studied
concept in decision theory, and is very desirable in domains where multiple
explorations can be conducted in parallel, for instance, in the sensor network
context."
"We investigate the problem of computing a minimum set of solutions that
approximates within a specified accuracy $\epsilon$ the Pareto curve of a
multiobjective optimization problem. We show that for a broad class of
bi-objective problems (containing many important widely studied problems such
as shortest paths, spanning tree, and many others), we can compute in
polynomial time an $\epsilon$-Pareto set that contains at most twice as many
solutions as the minimum such set. Furthermore we show that the factor of 2 is
tight for these problems, i.e., it is NP-hard to do better. We present upper
and lower bounds for three or more objectives, as well as for the dual problem
of computing a specified number $k$ of solutions which provide a good
approximation to the Pareto curve."
"This paper provides a systematic study of several proposed measures for
online algorithms in the context of a specific problem, namely, the two server
problem on three colinear points. Even though the problem is simple, it
encapsulates a core challenge in online algorithms which is to balance
greediness and adaptability. We examine Competitive Analysis, the Max/Max
Ratio, the Random Order Ratio, Bijective Analysis and Relative Worst Order
Analysis, and determine how these measures compare the Greedy Algorithm, Double
Coverage, and Lazy Double Coverage, commonly studied algorithms in the context
of server problems. We find that by the Max/Max Ratio and Bijective Analysis,
Greedy is the best of the three algorithms. Under the other measures, Double
Coverage and Lazy Double Coverage are better, though Relative Worst Order
Analysis indicates that Greedy is sometimes better. Only Bijective Analysis and
Relative Worst Order Analysis indicate that Lazy Double Coverage is better than
Double Coverage. Our results also provide the first proof of optimality of an
algorithm under Relative Worst Order Analysis."
"In this paper, some issues concerning the Chinese remaindering representation
are discussed. Some new converting methods, including an efficient
probabilistic algorithm based on a recent result of von zur Gathen and
Shparlinski \cite{Gathen-Shparlinski}, are described. An efficient refinement
of the NC$^1$ division algorithm of Chiu, Davida and Litow
\cite{Chiu-Davida-Litow} is given, where the number of moduli is reduced by a
factor of $\log n$."
"It is known that if a 2-universal hash function $H$ is applied to elements of
a {\em block source} $(X_1,...,X_T)$, where each item $X_i$ has enough
min-entropy conditioned on the previous items, then the output distribution
$(H,H(X_1),...,H(X_T))$ will be ``close'' to the uniform distribution. We
provide improved bounds on how much min-entropy per item is required for this
to hold, both when we ask that the output be close to uniform in statistical
distance and when we only ask that it be statistically close to a distribution
with small collision probability. In both cases, we reduce the dependence of
the min-entropy on the number $T$ of items from $2\log T$ in previous work to
$\log T$, which we show to be optimal. This leads to corresponding improvements
to the recent results of Mitzenmacher and Vadhan (SODA `08) on the analysis of
hashing-based algorithms and data structures when the data items come from a
block source."
"We describe a new approximation algorithm for Max Cut. Our algorithm runs in
$\tilde O(n^2)$ time, where $n$ is the number of vertices, and achieves an
approximation ratio of $.531$. On instances in which an optimal solution cuts a
$1-\epsilon$ fraction of edges, our algorithm finds a solution that cuts a
$1-4\sqrt{\epsilon} + 8\epsilon-o(1)$ fraction of edges.
  Our main result is a variant of spectral partitioning, which can be
implemented in nearly linear time. Given a graph in which the Max Cut optimum
is a $1-\epsilon$ fraction of edges, our spectral partitioning algorithm finds
a set $S$ of vertices and a bipartition $L,R=S-L$ of $S$ such that at least a
$1-O(\sqrt \epsilon)$ fraction of the edges incident on $S$ have one endpoint
in $L$ and one endpoint in $R$. (This can be seen as an analog of Cheeger's
inequality for the smallest eigenvalue of the adjacency matrix of a graph.)
Iterating this procedure yields the approximation results stated above.
  A different, more complicated, variant of spectral partitioning leads to an
$\tilde O(n^3)$ time algorithm that cuts $1/2 + e^{-\Omega(1/\eps)}$ fraction
of edges in graphs in which the optimum is $1/2 + \epsilon$."
"The Multidimensional Assignment Problem (MAP) (abbreviated s-AP in the case
of s dimensions) is an extension of the well-known assignment problem. The most
studied case of MAP is 3-AP, though the problems with larger values of s also
have a large number of applications. We consider several known neighborhoods,
generalize them and propose some new ones. The heuristics are evaluated both
theoretically and experimentally and dominating algorithms are selected. We
also demonstrate a combination of two neighborhoods may yield a heuristics
which is superior to both of its components."
"We present randomized approximation algorithms for multi-criteria Max-TSP.
For Max-STSP with k > 1 objective functions, we obtain an approximation ratio
of $1/k - \eps$ for arbitrarily small $\eps > 0$. For Max-ATSP with k objective
functions, we obtain an approximation ratio of $1/(k+1) - \eps$."
"In their seminal work, Alon, Matias, and Szegedy introduced several sketching
techniques, including showing that 4-wise independence is sufficient to obtain
good approximations of the second frequency moment. In this work, we show that
their sketching technique can be extended to product domains $[n]^k$ by using
the product of 4-wise independent functions on $[n]$. Our work extends that of
Indyk and McGregor, who showed the result for $k = 2$. Their primary motivation
was the problem of identifying correlations in data streams. In their model, a
stream of pairs $(i,j) \in [n]^2$ arrive, giving a joint distribution $(X,Y)$,
and they find approximation algorithms for how close the joint distribution is
to the product of the marginal distributions under various metrics, which
naturally corresponds to how close $X$ and $Y$ are to being independent. By
using our technique, we obtain a new result for the problem of approximating
the $\ell_2$ distance between the joint distribution and the product of the
marginal distributions for $k$-ary vectors, instead of just pairs, in a single
pass. Our analysis gives a randomized algorithm that is a $(1 \pm \epsilon)$
approximation (with probability $1-\delta$) that requires space logarithmic in
$n$ and $m$ and proportional to $3^k$."
"We study the problem of estimating the Earth Mover's Distance (EMD) between
probability distributions when given access only to samples. We give closeness
testers and additive-error estimators over domains in $[0, \Delta]^d$, with
sample complexities independent of domain size - permitting the testability
even of continuous distributions over infinite domains. Instead, our algorithms
depend on other parameters, such as the diameter of the domain space, which may
be significantly smaller. We also prove lower bounds showing the dependencies
on these parameters to be essentially optimal. Additionally, we consider
whether natural classes of distributions exist for which there are algorithms
with better dependence on the dimension, and show that for highly clusterable
data, this is indeed the case. Lastly, we consider a variant of the EMD,
defined over tree metrics instead of the usual L1 metric, and give optimal
algorithms."
"In many applications we are required to increase the deployment of a
distributed monitoring system on an evolving network. In this paper we present
a new method for finding candidate locations for additional deployment in the
network. This method is based on the Group Betweenness Centrality (GBC) measure
that is used to estimate the influence of a group of nodes over the information
flow in the network. The new method assists in finding the location of k
additional monitors in the evolving network, such that the portion of
additional traffic covered is at least (1-1/e) of the optimal."
"We consider the online list s-batch problem, where all the jobs have
processing time 1 and we seek to minimize the sum of the completion times of
the jobs. We give a Java program which is used to verify that the
competitiveness of this problem is 619/583."
"In a bounded max-coloring of a vertex/edge weighted graph, each color class
is of cardinality at most $b$ and of weight equal to the weight of the heaviest
vertex/edge in this class. The bounded max-vertex/edge-coloring problems ask
for such a coloring minimizing the sum of all color classes' weights.
  In this paper we present complexity results and approximation algorithms for
those problems on general graphs, bipartite graphs and trees. We first show
that both problems are polynomial for trees, when the number of colors is
fixed, and $H_b$ approximable for general graphs, when the bound $b$ is fixed.
For the bounded max-vertex-coloring problem, we show a 17/11-approximation
algorithm for bipartite graphs, a PTAS for trees as well as for bipartite
graphs when $b$ is fixed. For unit weights, we show that the known 4/3 lower
bound for bipartite graphs is tight by providing a simple 4/3 approximation
algorithm. For the bounded max-edge-coloring problem, we prove approximation
factors of $3-2/\sqrt{2b}$, for general graphs, $\min\{e, 3-2/\sqrt{b}\}$, for
bipartite graphs, and 2, for trees. Furthermore, we show that this problem is
NP-complete even for trees. This is the first complexity result for
max-coloring problems on trees."
"We give the first L_1-sketching algorithm for integer vectors which produces
nearly optimal sized sketches in nearly linear time. This answers the first
open problem in the list of open problems from the 2006 IITK Workshop on
Algorithms for Data Streams. Specifically, suppose Alice receives a vector x in
{-M,...,M}^n and Bob receives y in {-M,...,M}^n, and the two parties share
randomness. Each party must output a short sketch of their vector such that a
third party can later quickly recover a (1 +/- eps)-approximation to ||x-y||_1
with 2/3 probability given only the sketches. We give a sketching algorithm
which produces O(eps^{-2}log(1/eps)log(nM))-bit sketches in O(n*log^2(nM))
time, independent of eps. The previous best known sketching algorithm for L_1
is due to [Feigenbaum et al., SICOMP 2002], which achieved the optimal sketch
length of O(eps^{-2}log(nM)) bits but had a running time of O(n*log(nM)/eps^2).
Notice that our running time is near-linear for every eps, whereas for
sufficiently small values of eps, the running time of the previous algorithm
can be as large as quadratic. Like their algorithm, our sketching procedure
also yields a small-space, one-pass streaming algorithm which works even if the
entries of x,y are given in arbitrary order."
"Given an embedded planar acyclic digraph G, we define the problem of acyclic
hamiltonian path completion with crossing minimization (Acyclic-HPCCM) to be
the problem of determining a hamiltonian path completion set of edges such
that, when these edges are embedded on G, they create the smallest possible
number of edge crossings and turn G to a hamiltonian acyclic digraph. Our
results include: 1. We provide a characterization under which a planar
st-digraph G is hamiltonian. 2. For an outerplanar st-digraph G, we define the
st-polygon decomposition of G and, based on its properties, we develop a
linear-time algorithm that solves the Acyclic-HPCCM problem. 3. For the class
of planar st-digraphs, we establish an equivalence between the Acyclic-HPCCM
problem and the problem of determining an upward 2-page topological book
embedding with minimum number of spine crossings. We infer (based on this
equivalence) for the class of outerplanar st-digraphs an upward topological
2-page book embedding with minimum number of spine crossings. To the best of
our knowledge, it is the first time that edge-crossing minimization is studied
in conjunction with the acyclic hamiltonian completion problem and the first
time that an optimal algorithm with respect to spine crossing minimization is
presented for upward topological book embeddings."
"Let P be a set of n points in the Euclidean plane and let O be the origin
point in the plane. In the k-tour cover problem (called frequently the
capacitated vehicle routing problem), the goal is to minimize the total length
of tours that cover all points in P, such that each tour starts and ends in O
and covers at most k points from P.
  The k-tour cover problem is known to be NP-hard. It is also known to admit
constant factor approximation algorithms for all values of k and even a
polynomial-time approximation scheme (PTAS) for small values of k, i.e.,
k=O(log n / log log n).
  We significantly enlarge the set of values of k for which a PTAS is provable.
We present a new PTAS for all values of k <= 2^{log^{\delta}n}, where \delta =
\delta(\epsilon). The main technical result proved in the paper is a novel
reduction of the k-tour cover problem with a set of n points to a small set of
instances of the problem, each with O((k/\epsilon)^O(1)) points."
"The diameter of a graph is among its most basic parameters. Since a few
years, it moreover became a key issue to compute it for massive graphs in the
context of complex network analysis. However, known algorithms, including the
ones producing approximate values, have too high a time and/or space complexity
to be used in such cases. We propose here a new approach relying on very simple
and fast algorithms that compute (upper and lower) bounds for the diameter. We
show empirically that, on various real-world cases representative of complex
networks studied in the literature, the obtained bounds are very tight (and
even equal in some cases). This leads to rigorous and very accurate estimations
of the actual diameter in cases which were previously untractable in practice."
"Memory becomes a limiting factor in contemporary applications, such as
analyses of the Webgraph and molecular sequences, when many objects need to be
counted simultaneously. Robert Morris [Communications of the ACM, 21:840--842,
1978] proposed a probabilistic technique for approximate counting that is
extremely space-efficient. The basic idea is to increment a counter containing
the value $X$ with probability $2^{-X}$. As a result, the counter contains an
approximation of $\lg n$ after $n$ probabilistic updates stored in $\lg\lg n$
bits. Here we revisit the original idea of Morris, and introduce a binary
floating-point counter that uses a $d$-bit significand in conjunction with a
binary exponent. The counter yields a simple formula for an unbiased estimation
of $n$ with a standard deviation of about $0.6\cdot n2^{-d/2}$, and uses
$d+\lg\lg n$ bits.
  We analyze the floating-point counter's performance in a general framework
that applies to any probabilistic counter, and derive practical formulas to
assess its accuracy."
"We describe a data structure that maintains the number of triangles in a
dynamic undirected graph, subject to insertions and deletions of edges and of
degree-zero vertices. More generally it can be used to maintain the number of
copies of each possible three-vertex subgraph in time O(h) per update, where h
is the h-index of the graph, the maximum number such that the graph contains
$h$ vertices of degree at least h. We also show how to maintain the h-index
itself, and a collection of h high-degree vertices in the graph, in constant
time per update. Our data structure has applications in social network analysis
using the exponential random graph model (ERGM); its bound of O(h) time per
edge is never worse than the Theta(sqrt m) time per edge necessary to list all
triangles in a static graph, and is strictly better for graphs obeying a power
law degree distribution. In order to better understand the behavior of the
h-index statistic and its implications for the performance of our algorithms,
we also study the behavior of the h-index on a set of 136 real-world networks."
"We study the problem of abstracting a table of data about individuals so that
no selection query can identify fewer than k individuals. We show that it is
impossible to achieve arbitrarily good polynomial-time approximations for a
number of natural variations of the generalization technique, unless P = NP,
even when the table has only a single quasi-identifying attribute that
represents a geographic or unordered attribute:
  Zip-codes: nodes of a planar graph generalized into connected subgraphs
  GPS coordinates: points in R2 generalized into non-overlapping rectangles
  Unordered data: text labels that can be grouped arbitrarily. In addition to
impossibility results, we provide approximation algorithms for these difficult
single-attribute generalization problems, which, of course, apply to
multiple-attribute instances with one that is quasi-identifying. We show
theoretically and experimentally that our approximation algorithms can come
reasonably close to optimal solutions. Incidentally, the generalization problem
for unordered data can be viewed as a novel type of bin packing
problem--min-max bin covering--which may be of independent interest."
"We provide a smoothed analysis of Hoare's find algorithm and we revisit the
smoothed analysis of quicksort.
  Hoare's find algorithm - often called quickselect - is an easy-to-implement
algorithm for finding the k-th smallest element of a sequence. While the
worst-case number of comparisons that Hoare's find needs is quadratic, the
average-case number is linear. We analyze what happens between these two
extremes by providing a smoothed analysis of the algorithm in terms of two
different perturbation models: additive noise and partial permutations.
  Moreover, we provide lower bounds for the smoothed number of comparisons of
quicksort and Hoare's find for the median-of-three pivot rule, which usually
yields faster algorithms than always selecting the first element: The pivot is
the median of the first, middle, and last element of the sequence. We show that
median-of-three does not yield a significant improvement over the classic rule:
the lower bounds for the classic rule carry over to median-of-three."
"Many data dissemination and publish-subscribe systems that guarantee the
privacy and authenticity of the participants rely on symmetric key
cryptography. An important problem in such a system is to maintain the shared
group key as the group membership changes. We consider the problem of
determining a key hierarchy that minimizes the average communication cost of an
update, given update frequencies of the group members and an edge-weighted
undirected graph that captures routing costs. We first present a
polynomial-time approximation scheme for minimizing the average number of
multicast messages needed for an update. We next show that when routing costs
are considered, the problem is NP-hard even when the underlying routing network
is a tree network or even when every group member has the same update
frequency. Our main result is a polynomial time constant-factor approximation
algorithm for the general case where the routing network is an arbitrary
weighted graph and group members have nonuniform update frequencies."
"We present an O(n^3 log^2 n)-time algorithm for the following problem: given
a finite metric space X, create a star-topology network with the points of X as
its leaves, such that the distances in the star are at least as large as in X,
with minimum dilation. As part of our algorithm, we solve in the same time
bound the parametric negative cycle detection problem: given a directed graph
with edge weights that are increasing linear functions of a parameter lambda,
find the smallest value of lambda such that the graph contains no
negative-weight cycles."
"We propose new succinct representations of ordinal trees, which have been
studied extensively. It is known that any $n$-node static tree can be
represented in $2n + o(n)$ bits and a number of operations on the tree can be
supported in constant time under the word-RAM model. However the data
structures are complicated and difficult to dynamize. We propose a simple and
flexible data structure, called the range min-max tree, that reduces the large
number of relevant tree operations considered in the literature to a few
primitives that are carried out in constant time on sufficiently small trees.
The result is extended to trees of arbitrary size, achieving $2n + O(n
/\polylog(n))$ bits of space. The redundancy is significantly lower than any
previous proposal. Our data structure builds on the range min-max tree to
achieve $2n+O(n/\log n)$ bits of space and $O(\log n)$ time for all the
operations. We also propose an improved data structure using $2n+O(n\log\log
n/\log n)$ bits and improving the time to the optimal $O(\log n/\log \log n)$
for most operations. Furthermore, we support sophisticated operations that
allow attaching and detaching whole subtrees, in time $\Order(\log^{1+\epsilon}
n / \log\log n)$. Our techniques are of independent interest. One allows
representing dynamic bitmaps and sequences supporting rank/select and indels,
within zero-order entropy bounds and optimal time $O(\log n / \log\log n)$ for
all operations on bitmaps and polylog-sized alphabets, and $O(\log n \log
\sigma / (\log\log n)^2)$ on larger alphabet sizes $\sigma$. This improves upon
the best existing bounds for entropy-bounded storage of dynamic sequences,
compressed full-text self-indexes, and compressed-space construction of the
Burrows-Wheeler transform."
"In this paper, it is demonstrated that the DNA-based algorithm [Ho et al.
2005] for solving an instance of the clique problem to any a graph G = (V, E)
with n vertices and p edges and its complementary graph G1 = (V, E1) with n
vertices and m = (((n*(n-1))/2)-p) edges can be implemented by Hadamard gates,
NOT gates, CNOT gates, CCNOT gates, Grover's operators, and quantum
measurements on a quantum computer. It is also demonstrated that if Grovers
algorithm is employed to accomplish the readout step in the DNA-based
algorithm, the quantum implementation of the DNA-based algorithm is equivalent
to the oracle work (in the language of Grover's algorithm), that is, the target
state labeling preceding Grover,s searching steps. It is shown that one oracle
work can be completed with O((2 * n) * (n + 1) * (n + 2) / 3) NOT gates, one
CNOT gate and O((4 * m) + (((2 * n) * (n + 1) * (n + 14)) / 6)) CCNOT gates.
This is to say that for the quantum implementation of the DNA-based algorithm
[Ho et al. 2005] a faster labeling of the target state is attained, which also
implies a speedy solution to an instance of the clique problem."
"In this work we study the validity of the so-called curse of dimensionality
for indexing of databases for similarity search. We perform an asymptotic
analysis, with a test model based on a sequence of metric spaces $(\Omega_d)$
from which we pick datasets $X_d$ in an i.i.d. fashion. We call the subscript
$d$ the dimension of the space $\Omega_d$ (e.g. for $\mathbb{R}^d$ the
dimension is just the usual one) and we allow the size of the dataset $n=n_d$
to be such that $d$ is superlogarithmic but subpolynomial in $n$.
  We study the asymptotic performance of pivot-based indexing schemes where the
number of pivots is $o(n/d)$. We pick the relatively simple cost model of
similarity search where we count each distance calculation as a single
computation and disregard the rest.
  We demonstrate that if the spaces $\Omega_d$ exhibit the (fairly common)
concentration of measure phenomenon the performance of similarity search using
such indexes is asymptotically linear in $n$. That is for large enough $d$ the
difference between using such an index and performing a search without an index
at all is negligeable. Thus we confirm the curse of dimensionality in this
setting."
"The purpose of this article is to incite clever ways to attack problems. It
advocates in favor of more elegant algorithms, in place of brute force (albeit
its very well crafted) usages."
"It is well-known that, given a probability distribution over $n$ characters,
in the worst case it takes (\Theta (n \log n)) bits to store a prefix code with
minimum expected codeword length. However, in this paper we first show that,
for any $0<\epsilon<1/2$ with (1 / \epsilon = \Oh{\polylog{n}}), it takes
$\Oh{n \log \log (1 / \epsilon)}$ bits to store a prefix code with expected
codeword length within $\epsilon$ of the minimum. We then show that, for any
constant (c > 1), it takes $\Oh{n^{1 / c} \log n}$ bits to store a prefix code
with expected codeword length at most $c$ times the minimum. In both cases, our
data structures allow us to encode and decode any character in $\Oh{1}$ time."
"In 2005 Li et al. gave a phi-competitive deterministic online algorithm for
scheduling of packets with agreeable deadlines with a very interesting
analysis. This is known to be optimal due to a lower bound by Hajek. We claim
that the algorithm by Li et al. can be slightly simplified, while retaining its
competitive ratio. Then we introduce randomness to the modified algorithm and
argue that the competitive ratio against oblivious adversary is at most 4/3.
Note that this still leaves a gap between the best known lower bound of 5/4 by
Chin et al. for randomised algorithms against oblivious adversary."
"Constant-factor, polynomial-time approximation algorithms are presented for
two variations of the traveling salesman problem with time windows. In the
first variation, the traveling repairman problem, the goal is to find a tour
that visits the maximum possible number of locations during their time windows.
In the second variation, the speeding deliveryman problem, the goal is to find
a tour that uses the minimum possible speedup to visit all locations during
their time windows. For both variations, the time windows are of unit length,
and the distance metric is based on a weighted, undirected graph. Algorithms
with improved approximation ratios are given for the case when the input is
defined on a tree rather than a general graph. The algorithms are also extended
to handle time windows whose lengths fall in any bounded range."
"he segment minimization problem consists of finding the smallest set of
integer matrices that sum to a given intensity matrix, such that each summand
has only one non-zero value, and the non-zeroes in each row are consecutive.
This has direct applications in intensity-modulated radiation therapy, an
effective form of cancer treatment. We develop three approximation algorithms
for matrices with arbitrarily many rows. Our first two algorithms improve the
approximation factor from the previous best of $1+\log_2 h $ to (roughly) $3/2
\cdot (1+\log_3 h)$ and $11/6\cdot(1+\log_4{h})$, respectively, where $h$ is
the largest entry in the intensity matrix. We illustrate the limitations of the
specific approach used to obtain these two algorithms by proving a lower bound
of $\frac{(2b-2)}{b}\cdot\log_b{h} + \frac{1}{b}$ on the approximation
guarantee. Our third algorithm improves the approximation factor from $2 \cdot
(\log D+1)$ to $24/13 \cdot (\log D+1)$, where $D$ is (roughly) the largest
difference between consecutive elements of a row of the intensity matrix.
Finally, experimentation with these algorithms shows that they perform well
with respect to the optimum and outperform other approximation algorithms on
77% of the 122 test cases we consider, which include both real world and
synthetic data."
"We consider a somehow peculiar Token/Bucket problem which at first sight
looks confusing and difficult to solve. The winning approach to solve the
problem consists in going back to the simple and traditional methods to solve
computer science problems like the one taught to us by Knuth. Somehow the main
trick is to be able to specify clearly what needs to be achieved, and then the
solution, even if complex, appears almost by itself."
"We offer a theoretical validation of the curse of dimensionality in the
pivot-based indexing of datasets for similarity search, by proving, in the
framework of statistical learning, that in high dimensions no pivot-based
indexing scheme can essentially outperform the linear scan.
  A study of the asymptotic performance of pivot-based indexing schemes is
performed on a sequence of datasets modeled as samples $X_d$ picked in i.i.d.
fashion from metric spaces $\Omega_d$. We allow the size of the dataset $n=n_d$
to be such that $d$, the ``dimension'', is superlogarithmic but subpolynomial
in $n$. The number of pivots is allowed to grow as $o(n/d)$. We pick the least
restrictive cost model of similarity search where we count each distance
calculation as a single computation and disregard the rest.
  We demonstrate that if the intrinsic dimension of the spaces $\Omega_d$ in
the sense of concentration of measure phenomenon is $O(d)$, then the
performance of similarity search pivot-based indexes is asymptotically linear
in $n$."
"The Multidimensional Assignment Problem (MAP or s-AP in the case of s
dimensions) is an extension of the well-known assignment problem. The most
studied case of MAP is 3-AP, though the problems with larger values of s have
also a number of applications. In this paper we propose a memetic algorithm for
MAP that is a combination of a genetic algorithm with a local search procedure.
The main contribution of the paper is an idea of dynamically adjusted
generation size, that yields an outstanding flexibility of the algorithm to
perform well for both small and large fixed running times. The results of
computational experiments for several instance families show that the proposed
algorithm produces solutions of very high quality in a reasonable time and
outperforms the state-of-the art 3-AP memetic algorithm."
"In this paper we consider several constrained activity scheduling problems in
the time and space domains, like finding activity orderings which optimize the
values of several objective functions (time scheduling) or finding optimal
locations where certain types of activities will take place (space scheduling).
We present novel, efficient algorithmic solutions for all the considered
problems, based on the dynamic programming and greedy techniques. In each case
we compute exact, optimal solutions."
"In classical scheduling problems, we are given jobs and machines, and have to
schedule all the jobs to minimize some objective function. What if each job has
a specified profit, and we are no longer required to process all jobs -- we can
schedule any subset of jobs whose total profit is at least a (hard) target
profit requirement, while still approximately minimizing the objective
function?
  We refer to this class of problems as scheduling with outliers. This model
was initiated by Charikar and Khuller (SODA'06) on the minimum max-response
time in broadcast scheduling. We consider three other well-studied scheduling
objectives: the generalized assignment problem, average weighted completion
time, and average flow time, and provide LP-based approximation algorithms for
them. For the minimum average flow time problem on identical machines, we give
a logarithmic approximation algorithm for the case of unit profits based on
rounding an LP relaxation; we also show a matching integrality gap. For the
average weighted completion time problem on unrelated machines, we give a
constant factor approximation. The algorithm is based on randomized rounding of
the time-indexed LP relaxation strengthened by the knapsack-cover inequalities.
For the generalized assignment problem with outliers, we give a simple
reduction to GAP without outliers to obtain an algorithm whose makespan is
within 3 times the optimum makespan, and whose cost is at most (1 + \epsilon)
times the optimal cost."
"We consider online algorithms for pull-based broadcast scheduling. In this
setting there are n pages of information at a server and requests for pages
arrive online. When the server serves (broadcasts) a page p, all outstanding
requests for that page are satisfied. We study two related metrics, namely
maximum response time (waiting time) and maximum delay-factor and their
weighted versions. We obtain the following results in the worst-case online
competitive model.
  - We show that FIFO (first-in first-out) is 2-competitive even when the page
sizes are different. Previously this was known only for unit-sized pages [10]
via a delicate argument. Our proof differs from [10] and is perhaps more
intuitive.
  - We give an online algorithm for maximum delay-factor that is
O(1/eps^2)-competitive with (1+\eps)-speed for unit-sized pages and with
(2+\eps)-speed for different sized pages. This improves on the algorithm in
[12] which required (2+\eps)-speed and (4+\eps)-speed respectively. In addition
we show that the algorithm and analysis can be extended to obtain the same
results for maximum weighted response time and delay factor.
  - We show that a natural greedy algorithm modeled after LWF
(Longest-Wait-First) is not O(1)-competitive for maximum delay factor with any
constant speed even in the setting of standard scheduling with unit-sized jobs.
This complements our upper bound and demonstrates the importance of the
tradeoff made in our algorithm."
"We consider online algorithms for broadcast scheduling. In the pull-based
broadcast model there are $n$ unit-sized pages of information at a server and
requests arrive online for pages. When the server transmits a page $p$, all
outstanding requests for that page are satisfied. The longest-wait-first} (LWF)
algorithm is a natural algorithm that has been shown to have good empirical
performance. In this paper we make two main contributions to the analysis of
LWF and broadcast scheduling. \begin{itemize} \item We give an intuitive and
easy to understand analysis of LWF which shows that it is
$O(1/\eps^2)$-competitive for average flow-time with $(4+\eps)$ speed. Using a
more involved analysis, we show that LWF is $O(1/\eps^3)$-competitive for
average flow-time with $(3.4+\epsilon)$ speed. \item We show that a natural
extension of LWF is O(1)-speed O(1)-competitive for more general objective
functions such as average delay-factor and $L_k$ norms of delay-factor (for
fixed $k$). \end{itemize}"
"Logconcave functions represent the current frontier of efficient algorithms
for sampling, optimization and integration in R^n. Efficient sampling
algorithms to sample according to a probability density (to which the other two
problems can be reduced) relies on good isoperimetry which is known to hold for
arbitrary logconcave densities. In this paper, we extend this frontier in two
ways: first, we characterize convexity-like conditions that imply good
isoperimetry, i.e., what condition on function values along every line
guarantees good isoperimetry? The answer turns out to be the set of
(1/(n-1))-harmonic concave functions in R^n; we also prove that this is the
best possible characterization along every line, of functions having good
isoperimetry. Next, we give the first efficient algorithm for sampling
according to such functions with complexity depending on a smoothness
parameter. Further, noting that the multivariate Cauchy density is an important
distribution in this class, we exploit certain properties of the Cauchy density
to give an efficient sampling algorithm based on random walks with a mixing
time that matches the current best bounds known for sampling logconcave
functions."
"We consider a stochastic perturbation of a FitzHugh-Nagumo system. We show
that it is possible to generate oscillations for values of parameters which do
not allow oscillations for the deterministic system. We also study the
appearance of a new equilibrium point and new bifurcation parameters due to the
noisy component."
"The multidimensional assignment problem (MAP) (abbreviated s-AP in the case
of s dimensions) is an extension of the well-known assignment problem. The most
studied case of MAP is 3-AP, though the problems with larger values of s have
also a number of applications. In this paper we consider four fast construction
heuristics for MAP. One of the heuristics is new. A modification of the
heuristics is proposed to optimize the access to slow computer memory. The
results of computational experiments for several instance families are provided
and discussed."
"In the Scheduling Machines with Capacity Constraints problem, we are given k
identical machines, each of which can process at most m_i jobs. M jobs are also
given, where job j has a non-negative processing time length t_j >= 0. The task
is to find a schedule such that the makespan is minimized and the capacity
constraints are met. In this paper, we present a 3-approximation algorithm
using an extension of Iterative Rounding Method introduced by Jain. To the best
of the authors' knowledge, this is the first attempt to apply Iterative
Rounding Method to scheduling problem with capacity constraints."
"We consider two well-known natural variants of bin packing, and show that
these packing problems admit asymptotic fully polynomial time approximation
schemes (AFPTAS). In bin packing problems, a set of one-dimensional items of
size at most 1 is to be assigned (packed) to subsets of sum at most 1 (bins).
It has been known for a while that the most basic problem admits an AFPTAS. In
this paper, we develop methods that allow to extend this result to other
variants of bin packing. Specifically, the problems which we study in this
paper, for which we design asymptotic fully polynomial time approximation
schemes, are the following. The first problem is ""Bin packing with cardinality
constraints"", where a parameter k is given, such that a bin may contain up to k
items. The goal is to minimize the number of bins used. The second problem is
""Bin packing with rejection"", where every item has a rejection penalty
associated with it. An item needs to be either packed to a bin or rejected, and
the goal is to minimize the number of used bins plus the total rejection
penalty of unpacked items. This resolves the complexity of two important
variants of the bin packing problem. Our approximation schemes use a novel
method for packing the small items. This new method is the core of the improved
running times of our schemes over the running times of the previous results,
which are only asymptotic polynomial time approximation schemes (APTAS)."
"Following the work of Anily et al., we consider a variant of bin packing,
called ""bin packing with general cost structures"" (GCBP) and design an
asymptotic fully polynomial time approximation scheme (AFPTAS) for this
problem. In the classic bin packing problem, a set of one-dimensional items is
to be assigned to subsets of total size at most 1, that is, to be packed into
unit sized bins. However, in GCBP, the cost of a bin is not 1 as in classic bin
packing, but it is a non-decreasing and concave function of the number of items
packed in it, where the cost of an empty bin is zero. The construction of the
AFPTAS requires novel techniques for dealing with small items, which are
developed in this work. In addition, we develop a fast approximation algorithm
which acts identically for all non-decreasing and concave functions, and has an
asymptotic approximation ratio of 1.5 for all functions simultaneously."
"We provide geometrical interpretation of the Master Theorem to solve
divide-and-conquer recurrences. We show how different cases of the recurrences
correspond to different kinds of fractal images. Fractal dimension and
Hausdorff measure are shown to be closely related to the solution of such
recurrences."
"This paper presents a new algorithm based on integrating Genetic Algorithms
and Tabu Search methods to solve the Job Shop Scheduling problem. The idea of
the proposed algorithm is derived from Genetic Algorithms. Most of the
scheduling problems require either exponential time or space to generate an
optimal answer. Job Shop scheduling (JSS) is the general scheduling problem and
it is a NP-complete problem, but it is difficult to find the optimal solution.
This paper applies Genetic Algorithms and Tabu Search for Job Shop Scheduling
problem and compares the results obtained by each. With the implementation of
our approach the JSS problems reaches optimal solution and minimize the
makespan."
"The sort transform (ST) is a modification of the Burrows-Wheeler transform
(BWT). Both transformations map an arbitrary word of length n to a pair
consisting of a word of length n and an index between 1 and n. The BWT sorts
all rotation conjugates of the input word, whereas the ST of order k only uses
the first k letters for sorting all such conjugates. If two conjugates start
with the same prefix of length k, then the indices of the rotations are used
for tie-breaking. Both transforms output the sequence of the last letters of
the sorted list and the index of the input within the sorted list. In this
paper, we discuss a bijective variant of the BWT (due to Scott), proving its
correctness and relations to other results due to Gessel and Reutenauer (1993)
and Crochemore, Desarmenien, and Perrin (2005). Further, we present a novel
bijective variant of the ST."
"We study a number of multi-route cut problems: given a graph G=(V,E) and
connectivity thresholds k_(u,v) on pairs of nodes, the goal is to find a
minimum cost set of edges or vertices the removal of which reduces the
connectivity between every pair (u,v) to strictly below its given threshold.
These problems arise in the context of reliability in communication networks;
They are natural generalizations of traditional minimum cut problems where the
thresholds are either 1 (we want to completely separate the pair) or infinity
(we don't care about the connectivity for the pair). We provide the first
non-trivial approximations to a number of variants of the problem including for
both node-disjoint and edge-disjoint connectivity thresholds. A main
contribution of our work is an extension of the region growing technique for
approximating minimum multicuts to the multi-route setting. When the
connectivity thresholds are either 2 or infinity (the ""2-route cut"" case), we
obtain polylogarithmic approximations while satisfying the thresholds exactly.
For arbitrary connectivity thresholds this approach leads to bicriteria
approximations where we approximately satisfy the thresholds and approximately
minimize the cost. We present a number of different algorithms achieving
different cost-connectivity tradeoffs."
"The Lovasz Local Lemma (LLL) is a powerful result in probability theory that
states that the probability that none of a set of bad events happens is nonzero
if the probability of each event is small compared to the number of events that
depend on it. It is often used in combination with the probabilistic method for
non-constructive existence proofs. A prominent application is to k-CNF
formulas, where LLL implies that, if every clause in the formula shares
variables with at most d <= 2^k/e other clauses then such a formula has a
satisfying assignment. Recently, a randomized algorithm to efficiently
construct a satisfying assignment was given by Moser. Subsequently Moser and
Tardos gave a randomized algorithm to construct the structures guaranteed by
the LLL in a very general algorithmic framework. We address the main problem
left open by Moser and Tardos of derandomizing these algorithms efficiently.
Specifically, for a k-CNF formula with m clauses and d <= 2^{k/(1+\eps)}/e for
some \eps\in (0,1), we give an algorithm that finds a satisfying assignment in
time \tilde{O}(m^{2(1+1/\eps)}). This improves upon the deterministic
algorithms of Moser and of Moser-Tardos with running time m^{\Omega(k^2)} which
is superpolynomial for k=\omega(1) and upon other previous algorithms which
work only for d\leq 2^{k/16}/e. Our algorithm works efficiently for a general
version of LLL under the algorithmic framework of Moser and Tardos, and is also
parallelizable, i.e., has polylogarithmic running time using polynomially many
processors."
"This paper ties the line of work on algorithms that find an
O(sqrt(log(n)))-approximation to the sparsest cut together with the line of
work on algorithms that run in sub-quadratic time by using only
single-commodity flows. We present an algorithm that simultaneously achieves
both goals, finding an O(sqrt(log(n)/eps))-approximation using O(n^eps log^O(1)
n) max-flows. The core of the algorithm is a stronger, algorithmic version of
Arora et al.'s structure theorem, where we show that matching-chaining argument
at the heart of their proof can be viewed as an algorithm that finds good
augmenting paths in certain geometric multicommodity flow networks. By using
that specialized algorithm in place of a black-box solver, we are able to solve
those instances much more efficiently. We also show the cut-matching game
framework can not achieve an approximation any better than Omega(log(n)/log
log(n)) without re-routing flow."
"We successfully contract timetable networks with realistic transfer times.
Contraction gradually removes nodes from the graph and adds shortcuts to
preserve shortest paths. This reduces query times to 1 ms with preprocessing
times around 6 minutes on all tested instances. We achieve this by an improved
contraction algorithm and by using a station graph model. Every node in our
graph has a one-to-one correspondence to a station and every edge has an
assigned collection of connections. Our graph model does not need parallel
edges. The query algorithm does not compute a single earliest arrival time at a
station but a set of arriving connections that allow best transfer
opportunities."
"We consider the class of packing integer programs (PIPs) that are column
sparse, i.e. there is a specified upper bound k on the number of constraints
that each variable appears in. We give an (ek+o(k))-approximation algorithm for
k-column sparse PIPs, improving on recent results of $k^2\cdot 2^k$ and
$O(k^2)$. We also show that the integrality gap of our linear programming
relaxation is at least 2k-1; it is known that k-column sparse PIPs are
$\Omega(k/ \log k)$-hard to approximate. We also extend our result (at the loss
of a small constant factor) to the more general case of maximizing a submodular
objective over k-column sparse packing constraints."
"Most recent papers addressing the algorithmic problem of allocating
advertisement space for keywords in sponsored search auctions assume that
pricing is done via a first-price auction, which does not realistically model
the Generalized Second Price (GSP) auction used in practice. Towards the goal
of more realistically modeling these auctions, we introduce the Second-Price Ad
Auctions problem, in which bidders' payments are determined by the GSP
mechanism. We show that the complexity of the Second-Price Ad Auctions problem
is quite different than that of the more studied First-Price Ad Auctions
problem. First, unlike the first-price variant, for which small constant-factor
approximations are known, it is NP-hard to approximate the Second-Price Ad
Auctions problem to any non-trivial factor. Second, this discrepancy extends
even to the 0-1 special case that we call the Second-Price Matching problem
(2PM). In particular, offline 2PM is APX-hard, and for online 2PM there is no
deterministic algorithm achieving a non-trivial competitive ratio and no
randomized algorithm achieving a competitive ratio better than 2. This stands
in contrast to the results for the analogous special case in the first-price
model, the standard bipartite matching problem, which is solvable in polynomial
time and which has deterministic and randomized online algorithms achieving
better competitive ratios. On the positive side, we provide a 2-approximation
for offline 2PM and a 5.083-competitive randomized algorithm for online 2PM.
The latter result makes use of a new generalization of a classic result on the
performance of the ""Ranking"" algorithm for online bipartite matching."
"In this article we discuss a data structure, which combines advantages of two
different ways for representing graphs: adjacency matrix and collection of
adjacency lists. This data structure can fast add and search edges (advantages
of adjacency matrix), use linear amount of memory, let to obtain adjacency list
for certain vertex (advantages of collection of adjacency lists). Basic
knowledge of linked lists and hash tables is required to understand this
article. The article contains examples of implementation on Java."
"The aim of power management policies is to reduce the amount of energy
consumed by computer systems while maintaining satisfactory level of
performance. One common method for saving energy is to simply suspend the
system during the idle times. No energy is consumed in the suspend mode.
However, the process of waking up the system itself requires a certain fixed
amount of energy, and thus suspending the system is beneficial only if the idle
time is long enough to compensate for this additional energy expenditure. In
the specific problem studied in the paper, we have a set of jobs with release
times and deadlines that need to be executed on a single processor. Preemptions
are allowed. The processor requires energy L to be woken up and, when it is on,
it uses one unit of energy per one unit of time. It has been an open problem
whether a schedule minimizing the overall energy consumption can be computed in
polynomial time. We solve this problem in positive, by providing an O(n^5)-time
algorithm. In addition we provide an O(n^4)-time algorithm for computing the
minimum energy schedule when all jobs have unit length."
"We consider the problem of choosing Euclidean points to maximize the sum of
their weighted pairwise distances, when each point is constrained to a ball
centered at the origin. We derive a dual minimization problem and show strong
duality holds (i.e., the resulting upper bound is tight) when some locally
optimal configuration of points is affinely independent. We sketch a polynomial
time algorithm for finding a near-optimal set of points."
"Following previous theoretical work by Srinivasan (FOCS 2001) and the first
author (STACS 2006) and a first experimental evaluation on random instances
(ALENEX 2009), we investigate how the recently developed different approaches
to generate randomized roundings satisfying disjoint cardinality constraints
behave when used in two classical algorithmic problems, namely low-congestion
routing in networks and max-coverage problems in hypergraphs.
  We generally find that all randomized rounding algorithms work well, much
better than what is guaranteed by existing theoretical work. The derandomized
versions produce again significantly better rounding errors, with running times
still negligible compared to the one for solving the corresponding LP. It thus
seems worth preferring them over the randomized variants.
  The data created in these experiments lets us propose and investigate the
following new ideas. For the low-congestion routing problems, we suggest to
solve a second LP, which yields the same congestion, but aims at producing a
solution that is easier to round. Experiments show that this reduces the
rounding errors considerably, both in combination with randomized and
derandomized rounding.
  For the max-coverage instances, we generally observe that the greedy
heuristics also performs very good. We develop a strengthened method of
derandomized rounding, and a simple greedy/rounding hybrid approach using
greedy and LP-based rounding elements, and observe that both these improvements
yield again better solutions than both earlier approaches on their own.
  For unit disk max-domination, we also develop a PTAS. Contrary to all other
algorithms investigated, it performs not much better in experiments than in
theory; thus, unless extremely good solutions are to be obtained with huge
computational resources, greedy, LP-based rounding or hybrid approaches are
preferable."
"The Traveling Tournament Problem (TTP) is a challenging combinatorial
optimization problem that has attracted the interest of researchers around the
world. This paper proposes an improved search neighbourhood for the TTP that
has been tested in a simulated annealing context. The neighbourhood encompasses
both feasible and infeasible schedules, and can be generated efficiently. For
the largest TTP challenge problems with up to 40 teams, solutions found using
this neighbourhood are the best currently known, and for smaller problems with
10 teams, three solutions found were subsequently proven optimal."
"We present randomized algorithms for some well-studied, hard combinatorial
problems: the k-path problem, the p-packing of q-sets problem, and the
q-dimensional p-matching problem. Our algorithms solve these problems with high
probability in time exponential only in the parameter (k, p, q) and using
polynomial space; the constant bases of the exponentials are significantly
smaller than in previous works. For example, for the k-path problem the
improvement is from 2 to 1.66. We also show how to detect if a d-regular graph
admits an edge coloring with $d$ colors in time within a polynomial factor of
O(2^{(d-1)n/2}).
  Our techniques build upon and generalize some recently published ideas by I.
Koutis (ICALP 2009), R. Williams (IPL 2009), and A. Bj\""orklund (STACS 2010,
FOCS 2010)."
"We show how one can use certain deterministic algorithms for higher-value
constraint satisfaction problems (CSPs) to speed up deterministic local search
for 3-SAT. This way, we improve the deterministic worst-case running time for
3-SAT to O(1.439^n)."
"We study the following vertex-weighted online bipartite matching problem:
$G(U, V, E)$ is a bipartite graph. The vertices in $U$ have weights and are
known ahead of time, while the vertices in $V$ arrive online in an arbitrary
order and have to be matched upon arrival. The goal is to maximize the sum of
weights of the matched vertices in $U$. When all the weights are equal, this
reduces to the classic \emph{online bipartite matching} problem for which Karp,
Vazirani and Vazirani gave an optimal $\left(1-\frac{1}{e}\right)$-competitive
algorithm in their seminal work~\cite{KVV90}. Our main result is an optimal
$\left(1-\frac{1}{e}\right)$-competitive randomized algorithm for general
vertex weights. We use \emph{random perturbations} of weights by appropriately
chosen multiplicative factors. Our solution constitutes the first known
generalization of the algorithm in~\cite{KVV90} in this model and provides new
insights into the role of randomization in online allocation problems. It also
effectively solves the problem of \emph{online budgeted allocations}
\cite{MSVV05} in the case when an agent makes the same bid for any desired
item, even if the bid is comparable to his budget - complementing the results
of \cite{MSVV05, BJN07} which apply when the bids are much smaller than the
budgets."
"We study the maximum flow problem in directed H-minor-free graphs where H can
be drawn in the plane with one crossing. If a structural decomposition of the
graph as a clique-sum of planar graphs and graphs of constant complexity is
given, we show that a maximum flow can be computed in O(n log n) time. In
particular, maximum flows in directed K_{3,3}-minor-free graphs and directed
K_5-minor-free graphs can be computed in O(n log n) time without additional
assumptions."
"In the online packet buffering problem (also known as the unweighted FIFO
variant of buffer management), we focus on a single network packet switching
device with several input ports and one output port. This device forwards
unit-size, unit-value packets from input ports to the output port. Buffers
attached to input ports may accumulate incoming packets for later transmission;
if they cannot accommodate all incoming packets, their excess is lost. A packet
buffering algorithm has to choose from which buffers to transmit packets in
order to minimize the number of lost packets and thus maximize the throughput.
  We present a tight lower bound of e/(e-1) ~ 1.582 on the competitive ratio of
the throughput maximization, which holds even for fractional or randomized
algorithms. This improves the previously best known lower bound of 1.4659 and
matches the performance of the algorithm Random Schedule. Our result
contradicts the claimed performance of the algorithm Random Permutation; we
point out a flaw in its original analysis."
"We consider the problem of maximizing a nonnegative (possibly non-monotone)
submodular set function with or without constraints. Feige et al. [FOCS'07]
showed a 2/5-approximation for the unconstrained problem and also proved that
no approximation better than 1/2 is possible in the value oracle model.
Constant-factor approximation was also given for submodular maximization
subject to a matroid independence constraint (a factor of 0.309 Vondrak
[FOCS'09]) and for submodular maximization subject to a matroid base
constraint, provided that the fractional base packing number is at least 2 (a
1/4-approximation, Vondrak [FOCS'09]).
  In this paper, we propose a new algorithm for submodular maximization which
is based on the idea of {\em simulated annealing}. We prove that this algorithm
achieves improved approximation for two problems: a 0.41-approximation for
unconstrained submodular maximization, and a 0.325-approximation for submodular
maximization subject to a matroid independence constraint.
  On the hardness side, we show that in the value oracle model it is impossible
to achieve a 0.478-approximation for submodular maximization subject to a
matroid independence constraint, or a 0.394-approximation subject to a matroid
base constraint in matroids with two disjoint bases. Even for the special case
of cardinality constraint, we prove it is impossible to achieve a
0.491-approximation. (Previously it was conceivable that a 1/2-approximation
exists for these problems.) It is still an open question whether a
1/2-approximation is possible for unconstrained submodular maximization."
"We consider the online stochastic matching problem proposed by Feldman et al.
[FMMM09] as a model of display ad allocation. We are given a bipartite graph;
one side of the graph corresponds to a fixed set of bins and the other side
represents the set of possible ball types. At each time step, a ball is sampled
independently from the given distribution and it needs to be matched upon its
arrival to an empty bin. The goal is to maximize the number of allocations.
  We present an online algorithm for this problem with a competitive ratio of
0.702. Before our result, algorithms with a competitive ratio better than
$1-1/e$ were known under the assumption that the expected number of arriving
balls of each type is integral. A key idea of the algorithm is to collect
statistics about the decisions of the optimum offline solution using Monte
Carlo sampling and use those statistics to guide the decisions of the online
algorithm. We also show that our algorithm achieves a competitive ratio of
0.705 when the rates are integral.
  On the hardness side, we prove that no online algorithm can have a
competitive ratio better than 0.823 under the known distribution model (and
henceforth under the permutation model). This improves upon the 5/6 hardness
result proved by Goel and Mehta \cite{GM08} for the permutation model."
"We present an efficient algorithm to find non-empty minimizers of a symmetric
submodular function over any family of sets closed under inclusion. This for
example includes families defined by a cardinality constraint, a knapsack
constraint, a matroid independence constraint, or any combination of such
constraints. Our algorithm make $O(n^3)$ oracle calls to the submodular
function where $n$ is the cardinality of the ground set. In contrast, the
problem of minimizing a general submodular function under a cardinality
constraint is known to be inapproximable within $o(\sqrt{n/\log n})$ (Svitkina
and Fleischer [2008]).
  The algorithm is similar to an algorithm of Nagamochi and Ibaraki [1998] to
find all nontrivial inclusionwise minimal minimizers of a symmetric submodular
function over a set of cardinality $n$ using $O(n^3)$ oracle calls. Their
procedure in turn is based on Queyranne's algorithm [1998] to minimize a
symmetric submodular"
"In the Matroid Secretary Problem, introduced by Babaioff et al. [SODA 2007],
the elements of a given matroid are presented to an online algorithm in random
order. When an element is revealed, the algorithm learns its weight and decides
whether or not to select it under the restriction that the selected elements
form an independent set in the matroid. The objective is to maximize the total
weight of the chosen elements. In the most studied version of this problem, the
algorithm has no information about the weights beforehand. We refer to this as
the zero information model. In this paper we study a different model, also
proposed by Babaioff et al., in which the relative order of the weights is
random in the matroid. To be precise, in the random assignment model, an
adversary selects a collection of weights that are randomly assigned to the
elements of the matroid. Later, the elements are revealed to the algorithm in a
random order independent of the assignment.
  Our main result is the first constant competitive algorithm for the matroid
secretary problem in the random assignment model. This solves an open question
of Babaioff et al. Our algorithm achieves a competitive ratio of $2e^2/(e-1)$.
It exploits the notion of principal partition of a matroid, its decomposition
into uniformly dense minors, and a $2e$-competitive algorithm for uniformly
dense matroids we also develop. As additional results, we present simple
constant competitive algorithms in the zero information model for various
classes of matroids including cographic, low density and the case when every
element is in a small cocircuit. In the same model, we also give a
$ke$-competitive algorithm for $k$-column sparse linear matroids, and a new
$O(\log r)$-competitive algorithm for general matroids of rank $r$ which only
uses the relative order of the weights seen and not their numerical value, as
previously needed."
"The replacement paths problem for directed graphs is to find for given nodes
s and t and every edge e on the shortest path between them, the shortest path
between s and t which avoids e. For unweighted directed graphs on n vertices,
the best known algorithm runtime was \tilde{O}(n^{2.5}) by Roditty and Zwick.
For graphs with integer weights in {-M,...,M}, Weimann and Yuster recently
showed that one can use fast matrix multiplication and solve the problem in
O(Mn^{2.584}) time, a runtime which would be O(Mn^{2.33}) if the exponent
\omega of matrix multiplication is 2.
  We improve both of these algorithms. Our new algorithm also relies on fast
matrix multiplication and runs in O(M n^{\omega} polylog(n)) time if \omega>2
and O(n^{2+\eps}) for any \eps>0 if \omega=2. Our result shows that, at least
for small integer weights, the replacement paths problem in directed graphs may
be easier than the related all pairs shortest paths problem in directed graphs,
as the current best runtime for the latter is \Omega(n^{2.5}) time even if
\omega=2."
"Let us call a sequence of numbers heapable if they can be sequentially
inserted to form a binary tree with the heap property, where each insertion
subsequent to the first occurs at a leaf of the tree, i.e. below a previously
placed number. In this paper we consider a variety of problems related to
heapable sequences and subsequences that do not appear to have been studied
previously. Our motivation for introducing these concepts is two-fold. First,
such problems correspond to natural extensions of the well-known secretary
problem for hiring an organization with a hierarchical structure. Second, from
a purely combinatorial perspective, our problems are interesting variations on
similar longest increasing subsequence problems, a problem paradigm that has
led to many deep mathematical connections.
  We provide several basic results. We obtain an efficient algorithm for
determining the heapability of a sequence, and also prove that the question of
whether a sequence can be arranged in a complete binary heap is NP-hard.
Regarding subsequences we show that, with high probability, the longest
heapable subsequence of a random permutation of n numbers has length (1 - o(1))
n, and a subsequence of length (1 - o(1)) n can in fact be found online with
high probability. We similarly show that for a random permutation a subsequence
that yields a complete heap of size \alpha n for a constant \alpha can be found
with high probability. Our work highlights the interesting structure underlying
this class of subsequence problems, and we leave many further interesting
variations open for future work."
"We study the problem of ranking with submodular valuations. An instance of
this problem consists of a ground set $[m]$, and a collection of $n$ monotone
submodular set functions $f^1, \ldots, f^n$, where each $f^i: 2^{[m]} \to R_+$.
An additional ingredient of the input is a weight vector $w \in R_+^n$. The
objective is to find a linear ordering of the ground set elements that
minimizes the weighted cover time of the functions. The cover time of a
function is the minimal number of elements in the prefix of the linear ordering
that form a set whose corresponding function value is greater than a unit
threshold value.
  Our main contribution is an $O(\ln(1 / \epsilon))$-approximation algorithm
for the problem, where $\epsilon$ is the smallest non-zero marginal value that
any function may gain from some element. Our algorithm orders the elements
using an adaptive residual updates scheme, which may be of independent
interest. We also prove that the problem is $\Omega(\ln(1 / \epsilon))$-hard to
approximate, unless P = NP. This implies that the outcome of our algorithm is
optimal up to constant factors."
"A natural probabilistic model for motif discovery has been used to
experimentally test the quality of motif discovery programs. In this model,
there are $k$ background sequences, and each character in a background sequence
is a random character from an alphabet $\Sigma$. A motif $G=g_1g_2...g_m$ is a
string of $m$ characters. Each background sequence is implanted a
probabilistically generated approximate copy of $G$. For a probabilistically
generated approximate copy $b_1b_2...b_m$ of $G$, every character $b_i$ is
probabilistically generated such that the probability for $b_i\neq g_i$ is at
most $\alpha$. We develop three algorithms that under the probabilistic model
can find the implanted motif with high probability via a tradeoff between
computational time and the probability of mutation. The methods developed in
this paper have been used in the software implementation. We observed some
encouraging results that show improved performance for motif detection compared
with other softwares."
"Query evaluation in an XML database requires reconstructing XML subtrees
rooted at nodes found by an XML query. Since XML subtree reconstruction can be
expensive, one approach to improve query response time is to use reconstruction
views - materialized XML subtrees of an XML document, whose nodes are
frequently accessed by XML queries. For this approach to be efficient, the
principal requirement is a framework for view selection. In this work, we are
the first to formalize and study the problem of XML reconstruction view
selection. The input is a tree $T$, in which every node $i$ has a size $c_i$
and profit $p_i$, and the size limitation $C$. The target is to find a subset
of subtrees rooted at nodes $i_1,\cdots, i_k$ respectively such that
$c_{i_1}+\cdots +c_{i_k}\le C$, and $p_{i_1}+\cdots +p_{i_k}$ is maximal.
Furthermore, there is no overlap between any two subtrees selected in the
solution. We prove that this problem is NP-hard and present a fully
polynomial-time approximation scheme (FPTAS) as a solution."
"Motivated by applications in online dating and kidney exchange, the
stochastic matching problem was introduced by Chen, Immorlica, Karlin, Mahdian
and Rudra (2009). They have proven a 4-approximation of a simple greedy
strategy, but conjectured that it is in fact a 2-approximation. In this paper
we confirm this hypothesis."
"The random walk with choice is a well known variation to the random walk that
first selects a subset of $d$ neighbours nodes and then decides to move to the
node which maximizes the value of a certain metric; this metric captures the
number of (past) visits of the walk to the node. In this paper we propose an
enhancement to the random walk with choice by considering a new metric that
captures not only the actual visits to a given node, but also the intensity of
the visits to the neighbourhood of the node. We compare the random walk with
choice with its enhanced counterpart. Simulation results show a significant
improvement in cover time, maximum node load and load balancing, mainly in
random geometric graphs."
"In this paper, we consider lower bounds on the query complexity for testing
CSPs in the bounded-degree model.
  First, for any ``symmetric'' predicate $P:{0,1}^{k} \to {0,1}$ except \equ
where $k\geq 3$, we show that every (randomized) algorithm that distinguishes
satisfiable instances of CSP(P) from instances $(|P^{-1}(0)|/2^k-\epsilon)$-far
from satisfiability requires $\Omega(n^{1/2+\delta})$ queries where $n$ is the
number of variables and $\delta>0$ is a constant that depends on $P$ and
$\epsilon$. This breaks a natural lower bound $\Omega(n^{1/2})$, which is
obtained by the birthday paradox. We also show that every one-sided error
tester requires $\Omega(n)$ queries for such $P$. These results are hereditary
in the sense that the same results hold for any predicate $Q$ such that
$P^{-1}(1) \subseteq Q^{-1}(1)$. For EQU, we give a one-sided error tester
whose query complexity is $\tilde{O}(n^{1/2})$. Also, for 2-XOR (or,
equivalently E2LIN2), we show an $\Omega(n^{1/2+\delta})$ lower bound for
distinguishing instances between $\epsilon$-close to and $(1/2-\epsilon)$-far
from satisfiability.
  Next, for the general k-CSP over the binary domain, we show that every
algorithm that distinguishes satisfiable instances from instances
$(1-2k/2^k-\epsilon)$-far from satisfiability requires $\Omega(n)$ queries. The
matching NP-hardness is not known, even assuming the Unique Games Conjecture or
the $d$-to-$1$ Conjecture. As a corollary, for Maximum Independent Set on
graphs with $n$ vertices and a degree bound $d$, we show that every
approximation algorithm within a factor $d/\poly\log d$ and an additive error
of $\epsilon n$ requires $\Omega(n)$ queries. Previously, only super-constant
lower bounds were known."
"In this paper we consider the following modification of the iterative search
problem. We are given a tree $T$, so that a dynamic catalog $C(v)$ is
associated with every tree node $v$. For any $x$ and for any node-to-root path
$\pi$ in $T$, we must find the predecessor of $x$ in $\cup_{v\in \pi} C(v)$. We
present a linear space dynamic data structure that supports such queries in
$O(t(n)+|\pi|)$ time, where $t(n)$ is the time needed to search in one catalog
and $|\pi|$ denotes the number of nodes on path $\pi$. We also consider the
reporting variant of this problem, in which for any $x_1$, $x_2$ and for any
path $\pi'$ all elements of $\cup_{v\in \pi'} (C(v)\cap [x_1,x_2])$ must be
reported; here $\pi'$ denotes a path between an arbitrary node $v_0$ and its
ancestor $v_1$. We show that such queries can be answered in $O(t(n)+|\pi'|+
k)$ time, where $k$ is the number of elements in the answer. To illustrate
applications of our technique, we describe the first dynamic data structures
for the stabbing-max problem, the horizontal point location problem, and the
orthogonal line-segment intersection problem with optimal $O(\log n/\log \log
n)$ query time and poly-logarithmic update time."
"We study LP-rounding approximation algorithms for metric uncapacitated
facility-location problems. We first give a new analysis for the algorithm of
Chudak and Shmoys, which differs from the analysis of Byrka and Aardal in that
now we do not need any bound based on the solution to the dual LP program.
Besides obtaining the optimal bifactor approximation as do Byrka and Aardal, we
can now also show that the algorithm with scaling parameter equaling 1.58 is,
in fact, an 1.58-approximation algorithm. More importantly, we suggest an
approach based on additional randomization and analyses such as ours, which
could achieve or approach the conjectured optimal 1.46...--approximation for
this basic problem.
  Next, using essentially the same techniques, we obtain improved approximation
algorithms in the 2-stage stochastic variant of the problem, where we must open
a subset of facilities having only stochastic information about the future
demand from the clients. For this problem we obtain a 2.2975-approximation
algorithm in the standard setting, and a 2.4957-approximation in the more
restricted, per-scenario setting.
  We then study robust fault-tolerant facility location, introduced by Chechik
and Peleg: solutions here are designed to provide low connection cost in case
of failure of up to $k$ facilities. Chechik and Peleg gave a 6.5-approximation
algorithm for $k=1$ and a ($7.5k + 1.5$)-approximation algorithm for general
$k$. We improve this to an LP-rounding $(k+5+4/k)$-approximation algorithm. We
also observe that in case of oblivious failures the expected approximation
ratio can be reduced to $k + 1.5$, and that the integrality gap of the natural
LP-relaxation of the problem is at least $k + 1$."
"In this paper we initiate the study of minimizing power consumption in the
broadcast scheduling model. In this setting there is a wireless transmitter.
Over time requests arrive at the transmitter for pages of information. Multiple
requests may be for the same page. When a page is transmitted, all requests for
that page receive the transmission simulteneously. The speed the transmitter
sends data at can be dynamically scaled to conserve energy. We consider the
problem of minimizing flow time plus energy, the most popular scheduling metric
considered in the standard scheduling model when the scheduler is energy aware.
We will assume that the power consumed is modeled by an arbitrary convex
function. For this problem there is a $\Omega(n)$ lower bound. Due to the lower
bound, we consider the resource augmentation model of Gupta \etal
\cite{GuptaKP10}. Using resource augmentation, we give a scalable algorithm.
Our result also gives a scalable non-clairvoyant algorithm for minimizing
weighted flow time plus energy in the standard scheduling model."
"Given a graph G = (V,E) and an integer k, an edge modification problem for a
graph property P consists in deciding whether there exists a set of edges F of
size at most k such that the graph H = (V,E \vartriangle F) satisfies the
property P. In the P edge-completion problem, the set F of edges is constrained
to be disjoint from E; in the P edge-deletion problem, F is a subset of E; no
constraint is imposed on F in the P edge-edition problem. A number of
optimization problems can be expressed in terms of graph modification problems
which have been extensively studied in the context of parameterized complexity.
When parameterized by the size k of the edge set F, it has been proved that if
P is an hereditary property characterized by a finite set of forbidden induced
subgraphs, then the three P edge-modification problems are FPT. It was then
natural to ask whether these problems also admit a polynomial size kernel.
Using recent lower bound techniques, Kratsch and Wahlstrom answered this
question negatively. However, the problem remains open on many natural graph
classes characterized by forbidden induced subgraphs. Kratsch and Wahlstrom
asked whether the result holds when the forbidden subgraphs are paths or cycles
and pointed out that the problem is already open in the case of P4-free graphs
(i.e. cographs). This paper provides positive and negative results in that line
of research. We prove that parameterized cograph edge modification problems
have cubic vertex kernels whereas polynomial kernels are unlikely to exist for
the Pl-free and Cl-free edge-deletion problems for large enough l."
"We give a space-optimal algorithm with update time
O(log^2(1/eps)loglog(1/eps)) for (1+eps)-approximating the pth frequency
moment, 0 < p < 2, of a length-n vector updated in a data stream. This provides
a nearly exponential improvement in the update time complexity over the
previous space-optimal algorithm of [Kane-Nelson-Woodruff, SODA 2010], which
had update time Omega(1/eps^2)."
"This entry for the SIGSPATIAL Special July 2010 issue on Similarity Searching
in Metric Spaces discusses the notion of intrinsic dimensionality of data in
the context of similarity search."
"In this work we introduce a new linear time compression algorithm, called
""Re-pair for Trees"", which compresses ranked ordered trees using linear
straight-line context-free tree grammars. Such grammars generalize
straight-line context-free string grammars and allow basic tree operations,
like traversal along edges, to be executed without prior decompression. Our
algorithm can be considered as a generalization of the ""Re-pair"" algorithm
developed by N. Jesper Larsson and Alistair Moffat in 2000. The latter
algorithm is a dictionary-based compression algorithm for strings. We also
introduce a succinct coding which is specialized in further compressing the
grammars generated by our algorithm. This is accomplished without loosing the
ability do directly execute queries on this compressed representation of the
input tree. Finally, we compare the grammars and output files generated by a
prototype of the Re-pair for Trees algorithm with those of similar compression
algorithms. The obtained results show that that our algorithm outperforms its
competitors in terms of compression ratio, runtime and memory usage."
"For every list of integers x_1, ..., x_m there is some j such that x_1 + ...
+ x_j - x_{j+1} - ... - x_m \approx 0. So the list can be nearly balanced and
for this we only need one alternation between addition and subtraction. But
what if the x_i are k-dimensional integer vectors? Using results from
topological degree theory we show that balancing is still possible, now with k
alternations.
  This result is useful in multi-objective optimization, as it allows a
polynomial-time computable balance of two alternatives with conflicting costs.
The application to two multi-objective optimization problems yields the
following results:
  - A randomized 1/2-approximation for multi-objective maximum asymmetric
traveling salesman, which improves and simplifies the best known approximation
for this problem.
  - A deterministic 1/2-approximation for multi-objective maximum weighted
satisfiability."
"We give a time-randomness tradeoff for the quasi-random rumor spreading
protocol proposed by Doerr, Friedrich and Sauerwald [SODA 2008] on complete
graphs. In this protocol, the goal is to spread a piece of information
originating from one vertex throughout the network. Each vertex is assumed to
have a (cyclic) list of its neighbors. Once a vertex is informed by one of its
neighbors, it chooses a position in its list uniformly at random and then
informs its neighbors starting from that position and proceeding in order of
the list. Angelopoulos, Doerr, Huber and Panagiotou [Electron.~J.~Combin.~2009]
showed that after $(1+o(1))(\log_2 n + \ln n)$ rounds, the rumor will have been
broadcasted to all nodes with probability $1 - o(1)$.
  We study the broadcast time when the amount of randomness available at each
node is reduced in natural way. In particular, we prove that if each node can
only make its initial random selection from every $\ell$-th node on its list,
then there exists lists such that $(1-\varepsilon) (\log_2 n + \ln n - \log_2
\ell - \ln \ell)+\ell-1$ steps are needed to inform every vertex with
probability at least $1-O\bigl(\exp\bigl(-\frac{n^\varepsilon}{2\ln
n}\bigr)\bigr)$. This shows that a further reduction of the amount of
randomness used in a simple quasi-random protocol comes at a loss of
efficiency."
"We present a Monte Carlo algorithm for Hamiltonicity detection in an
$n$-vertex undirected graph running in $O^*(1.657^{n})$ time. To the best of
our knowledge, this is the first superpolynomial improvement on the worst case
runtime for the problem since the $O^*(2^n)$ bound established for TSP almost
fifty years ago (Bellman 1962, Held and Karp 1962). It answers in part the
first open problem in Woeginger's 2003 survey on exact algorithms for NP-hard
problems.
  For bipartite graphs, we improve the bound to $O^*(1.414^{n})$ time. Both the
bipartite and the general algorithm can be implemented to use space polynomial
in $n$.
  We combine several recently resurrected ideas to get the results. Our main
technical contribution is a new reduction inspired by the algebraic sieving
method for $k$-Path (Koutis ICALP 2008, Williams IPL 2009). We introduce the
Labeled Cycle Cover Sum in which we are set to count weighted arc labeled cycle
covers over a finite field of characteristic two. We reduce Hamiltonicity to
Labeled Cycle Cover Sum and apply the determinant summation technique for Exact
Set Covers (Bj\""orklund STACS 2010) to evaluate it."
"We focus the use of \emph{row sampling} for approximating matrix algorithms.
We give applications to matrix multipication; sparse matrix reconstruction;
and, \math{\ell_2} regression. For a matrix \math{\matA\in\R^{m\times d}} which
represents \math{m} points in \math{d\ll m} dimensions, all of these tasks can
be achieved in \math{O(md^2)} via the singular value decomposition (SVD). For
appropriate row-sampling probabilities (which typically depend on the norms of
the rows of the \math{m\times d} left singular matrix of \math{\matA} (the
\emph{leverage scores}), we give row-sampling algorithms with linear (up to
polylog factors) dependence on the stable rank of \math{\matA}. This result is
achieved through the application of non-commutative Bernstein bounds.
  We then give, to our knowledge, the first algorithms for computing
approximations to the appropriate row-sampling probabilities without going
through the SVD of \math{\matA}. Thus, these are the first \math{o(md^2)}
algorithms for row-sampling based approximations to the matrix algorithms which
use leverage scores as the sampling probabilities. The techniques we use to
approximate sampling according to the leverage scores uses some powerful recent
results in the theory of random projections for embedding, and may be of some
independent interest. We confess that one may perform all these matrix tasks
more efficiently using these same random projection methods, however the
resulting algorithms are in terms of a small number of linear combinations of
all the rows. In many applications, the actual rows of \math{\matA} have some
physical meaning and so methods based on a small number of the actual rows are
of interest."
"We present an approximate distance oracle for a point set S with n points and
doubling dimension {\lambda}. For every {\epsilon}>0, the oracle supports
(1+{\epsilon})-approximate distance queries in (universal) constant time,
occupies space [{\epsilon}^{-O({\lambda})} + 2^{O({\lambda} log {\lambda})}]n,
and can be constructed in [2^{O({\lambda})} log3 n + {\epsilon}^{-O({\lambda})}
+ 2^{O({\lambda} log {\lambda})}]n expected time. This improves upon the best
previously known constructions, presented by Har-Peled and Mendel. Furthermore,
the oracle can be made fully dynamic with expected O(1) query time and only
2^{O({\lambda})} log n + {\epsilon}^{-O({\lambda})} + 2^{O({\lambda} log
{\lambda})} update time. This is the first fully dynamic
(1+{\epsilon})-distance oracle."
"Given n elements with nonnegative integer weights w1,..., wn and an integer
capacity C, we consider the counting version of the classic knapsack problem:
find the number of distinct subsets whose weights add up to at most the given
capacity. We give a deterministic algorithm that estimates the number of
solutions to within relative error 1+-eps in time polynomial in n and 1/eps
(fully polynomial approximation scheme). More precisely, our algorithm takes
time O(n^3 (1/eps) log (n/eps)). Our algorithm is based on dynamic programming.
Previously, randomized polynomial time approximation schemes were known first
by Morris and Sinclair via Markov chain Monte Carlo techniques, and
subsequently by Dyer via dynamic programming and rejection sampling."
"We present a general method of designing fast approximation algorithms for
cut-based minimization problems in undirected graphs. In particular, we develop
a technique that given any such problem that can be approximated quickly on
trees, allows approximating it almost as quickly on general graphs while only
losing a poly-logarithmic factor in the approximation guarantee.
  To illustrate the applicability of our paradigm, we focus our attention on
the undirected sparsest cut problem with general demands and the balanced
separator problem. By a simple use of our framework, we obtain poly-logarithmic
approximation algorithms for these problems that run in time close to linear.
  The main tool behind our result is an efficient procedure that decomposes
general graphs into simpler ones while approximately preserving the cut-flow
structure. This decomposition is inspired by the cut-based graph decomposition
of R\""acke that was developed in the context of oblivious routing schemes, as
well as, by the construction of the ultrasparsifiers due to Spielman and Teng
that was employed to preconditioning symmetric diagonally-dominant matrices."
"We study a discrete diffusion process introduced in some combinatorial games
called FLOODIT and MADVIRUS that can be played online and whose computational
complexity has been recently studied by Arthur et al (FUN'2010). The flooding
dynamics used in those games can be defined for any colored graph. It has been
shown in a first report (in french, hal-00509488 on HAL archive) that studying
this dynamics directly on general graph is a valuable approach to understand
its specificities and extract uncluttered key patterns or algorithms that can
be applied with success to particular cases like the square grid of FLOODIT or
the hexagonal grid of MADVIRUS, and many other classes of graphs. This report
is the translation from french to english of the section in the french report
showing that the variant of the problem called 2-FREE-FLOOD-IT can be solved
with a polynomial algorithm, answering a question raised in the previous study
of FLOODIT by Arthur et al."
"We introduce a $2$-approximation algorithm for the minimum total covering
number problem."
"The Maximum Betweenness Centrality problem (MBC) can be defined as follows.
Given a graph find a $k$-element node set $C$ that maximizes the probability of
detecting communication between a pair of nodes $s$ and $t$ chosen uniformly at
random. It is assumed that the communication between $s$ and $t$ is realized
along a shortest $s$--$t$ path which is, again, selected uniformly at random.
The communication is detected if the communication path contains a node of $C$.
Recently, Dolev et al. (2009) showed that MBC is NP-hard and gave a
$(1-1/e)$-approximation using a greedy approach. We provide a reduction of MBC
to Maximum Coverage that simplifies the analysis of the algorithm of Dolev et
al. considerably. Our reduction allows us to obtain a new algorithm with the
same approximation ratio for a (generalized) budgeted version of MBC. We
provide tight examples showing that the analyses of both algorithms are best
possible. Moreover, we prove that MBC is APX-complete and provide an exact
polynomial-time algorithm for MBC on tree graphs."
"Consider a sequence of bits where we are trying to predict the next bit from
the previous bits. Assume we are allowed to say 'predict 0' or 'predict 1', and
our payoff is +1 if the prediction is correct and -1 otherwise. We will say
that at each point in time the loss of an algorithm is the number of wrong
predictions minus the number of right predictions so far. In this paper we are
interested in algorithms that have essentially zero (expected) loss over any
string at any point in time and yet have small regret with respect to always
predicting 0 or always predicting 1. For a sequence of length $T$ our algorithm
has regret $14\epsilon T $ and loss $2\sqrt{T}e^{-\epsilon^2 T} $ in
expectation for all strings. We show that the tradeoff between loss and regret
is optimal up to constant factors.
  Our techniques extend to the general setting of $N$ experts, where the
related problem of trading off regret to the best expert for regret to the
`special' expert has been studied by Even-Dar et al. (COLT'07). We obtain
essentially zero loss with respect to the special expert and optimal
loss/regret tradeoff, improving upon the results of Even-Dar et al and settling
the main question left open in their paper.
  The strong loss bounds of the algorithm have some surprising consequences. A
simple iterative application of our algorithm gives essentially optimal regret
bounds at multiple time scales, bounds with respect to $k$-shifting optima as
well as regret bounds with respect to higher norms of the input sequence."
"Kernelization algorithms for the {\sc cluster editing} problem have been a
popular topic in the recent research in parameterized computation. Thus far
most kernelization algorithms for this problem are based on the concept of {\it
critical cliques}. In this paper, we present new observations and new
techniques for the study of kernelization algorithms for the {\sc cluster
editing} problem. Our techniques are based on the study of the relationship
between {\sc cluster editing} and graph edge-cuts. As an application, we
present an ${\cal O}(n^2)$-time algorithm that constructs a $2k$ kernel for the
{\it weighted} version of the {\sc cluster editing} problem. Our result meets
the best kernel size for the unweighted version for the {\sc cluster editing}
problem, and significantly improves the previous best kernel of quadratic size
for the weighted version of the problem."
"We consider the following general scheduling problem: The input consists of n
jobs, each with an arbitrary release time, size, and a monotone function
specifying the cost incurred when the job is completed at a particular time.
The objective is to find a preemptive schedule of minimum aggregate cost. This
problem formulation is general enough to include many natural scheduling
objectives, such as weighted flow, weighted tardiness, and sum of flow squared.
Our main result is a randomized polynomial-time algorithm with an approximation
ratio O(log log nP), where P is the maximum job size. We also give an O(1)
approximation in the special case when all jobs have identical release times.
The main idea is to reduce this scheduling problem to a particular geometric
set-cover problem which is then solved using the local ratio technique and
Varadarajan's quasi-uniform sampling technique. This general algorithmic
approach improves the best known approximation ratios by at least an
exponential factor (and much more in some cases) for essentially all of the
nontrivial common special cases of this problem. Our geometric interpretation
of scheduling may be of independent interest."
"Consider a random graph model where each possible edge $e$ is present
independently with some probability $p_e$. Given these probabilities, we want
to build a large/heavy matching in the randomly generated graph. However, the
only way we can find out whether an edge is present or not is to query it, and
if the edge is indeed present in the graph, we are forced to add it to our
matching. Further, each vertex $i$ is allowed to be queried at most $t_i$
times. How should we adaptively query the edges to maximize the expected weight
of the matching? We consider several matching problems in this general
framework (some of which arise in kidney exchanges and online dating, and
others arise in modeling online advertisements); we give LP-rounding based
constant-factor approximation algorithms for these problems. Our main results
are the following:
  We give a 4 approximation for weighted stochastic matching on general graphs,
and a 3 approximation on bipartite graphs. This answers an open question from
[Chen etal ICALP 09]. Combining our LP-rounding algorithm with the natural
greedy algorithm, we give an improved 3.46 approximation for unweighted
stochastic matching on general graphs.
  We introduce a generalization of the stochastic online matching problem
[Feldman etal FOCS 09] that also models preference-uncertainty and timeouts of
buyers, and give a constant factor approximation algorithm."
"We study the problem of learning to rank from pairwise preferences, and solve
a long-standing open problem that has led to development of many heuristics but
no provable results for our particular problem. Given a set $V$ of $n$
elements, we wish to linearly order them given pairwise preference labels. A
pairwise preference label is obtained as a response, typically from a human, to
the question ""which if preferred, u or v?$ for two elements $u,v\in V$. We
assume possible non-transitivity paradoxes which may arise naturally due to
human mistakes or irrationality. The goal is to linearly order the elements
from the most preferred to the least preferred, while disagreeing with as few
pairwise preference labels as possible. Our performance is measured by two
parameters: The loss and the query complexity (number of pairwise preference
labels we obtain). This is a typical learning problem, with the exception that
the space from which the pairwise preferences is drawn is finite, consisting of
${n\choose 2}$ possibilities only. We present an active learning algorithm for
this problem, with query bounds significantly beating general (non active)
bounds for the same error guarantee, while almost achieving the information
theoretical lower bound. Our main construct is a decomposition of the input
s.t. (i) each block incurs high loss at optimum, and (ii) the optimal solution
respecting the decomposition is not much worse than the true opt. The
decomposition is done by adapting a recent result by Kenyon and Schudy for a
related combinatorial optimization problem to the query efficient setting. We
thus settle an open problem posed by learning-to-rank theoreticians and
practitioners: What is a provably correct way to sample preference labels? To
further show the power and practicality of our solution, we show how to use it
in concert with an SVM relaxation."
"Alon and Krivelevich (SIAM J. Discrete Math. 15(2): 211-227 (2002)) show that
if a graph is {\epsilon}-far from bipartite, then the subgraph induced by a
random subset of O(1/{\epsilon}) vertices is bipartite with high probability.
We conjecture that the induced subgraph is {\Omega}~({\epsilon})-far from
bipartite with high probability. Gonen and Ron (RANDOM 2007) proved this
conjecture in the case when the degrees of all vertices are at most
O({\epsilon}n). We give a more general proof that works for any d-regular (or
almost d-regular) graph for arbitrary degree d. Assuming this conjecture, we
prove that bipartiteness is testable with one-sided error in time
O(1/{\epsilon}^c), where c is a constant strictly smaller than two, improving
upon the tester of Alon and Krivelevich. As it is known that non-adaptive
testers for bipartiteness require {\Omega}(1/{\epsilon}^2) queries (Bogdanov
and Trevisan, CCC 2004), our result shows, assuming the conjecture, that
adaptivity helps in testing bipartiteness."
"One of the classic results in scheduling theory is the 2-approximation
algorithm by Lenstra, Shmoys, and Tardos for the problem of scheduling jobs to
minimize makespan on unrelated machines, i.e., job j requires time p_{ij} if
processed on machine i. More than two decades after its introduction it is
still the algorithm of choice even in the restricted model where processing
times are of the form p_{ij} in {p_j, \infty}. This problem, also known as the
restricted assignment problem, is NP-hard to approximate within a factor less
than 1.5 which is also the best known lower bound for the general version.
  Our main result is a polynomial time algorithm that estimates the optimal
makespan of the restricted assignment problem within a factor 33/17 + \epsilon
\approx 1.9412 + \epsilon, where \epsilon > 0 is an arbitrarily small constant.
The result is obtained by upper bounding the integrality gap of a certain
strong linear program, known as configuration LP, that was previously
successfully used for the related Santa Claus problem. Similar to the strongest
analysis for that problem our proof is based on a local search algorithm that
will eventually find a schedule of the mentioned approximation guarantee, but
is not known to converge in polynomial time."
"We study a combinatorial problem arising from microarrays synthesis. The
synthesis is done by a light-directed chemical process. The objective is to
minimize unintended illumination that may contaminate the quality of
experiments. Unintended illumination is measured by a notion called border
length and the problem is called Border Minimization Problem (BMP). The
objective of the BMP is to place a set of probe sequences in the array and find
an embedding (deposition of nucleotides/residues to the array cells) such that
the sum of border length is minimized. A variant of the problem, called P-BMP,
is that the placement is given and the concern is simply to find the embedding.
Approximation algorithms have been previously proposed for the problem but it
is unknown whether the problem is NP-hard or not. In this paper, we give a
thorough study of different variations of BMP by giving NP-hardness proofs and
improved approximation algorithms. We show that P-BMP, 1D-BMP, and BMP are all
NP-hard. Contrast with the previous result that 1D-P-BMP is polynomial time
solvable, the interesting implications include (i) the array dimension (1D or
2D) differentiates the complexity of P-BMP; (ii) for 1D array, whether
placement is given differentiates the complexity of BMP; (iii) BMP is NP-hard
regardless of the dimension of the array. Another contribution of the paper is
improving the approximation for BMP from $O(n^{1/2} \log^2 n)$ to $O(n^{1/4}
\log^2 n)$, where $n$ is the total number of sequences."
"Suppose we have n keys, n access probabilities for the keys, and n+1 access
probabilities for the gaps between the keys. Let h_min(n) be the minimal height
of a binary search tree for n keys. We consider the problem to construct an
optimal binary search tree with near minimal height, i.e.\ with height h <=
h_min(n) + Delta for some fixed Delta. It is shown, that for any fixed Delta
optimal binary search trees with near minimal height can be constructed in time
O(n^2). This is as fast as in the unrestricted case.
  So far, the best known algorithms for the construction of height-restricted
optimal binary search trees have running time O(L n^2), whereby L is the
maximal permitted height. Compared to these algorithms our algorithm is at
least faster by a factor of log n, because L is lower bounded by log n."
"We present a new data structure called the \emph{Compressed Random Access
Memory} (CRAM) that can store a dynamic string $T$ of characters, e.g.,
representing the memory of a computer, in compressed form while achieving
asymptotically almost-optimal bounds (in terms of empirical entropy) on the
compression ratio. It allows short substrings of $T$ to be decompressed and
retrieved efficiently and, significantly, characters at arbitrary positions of
$T$ to be modified quickly during execution \emph{without decompressing the
entire string}. This can be regarded as a new type of data compression that can
update a compressed file directly. Moreover, at the cost of slightly increasing
the time spent per operation, the CRAM can be extended to also support
insertions and deletions. Our key observation that the empirical entropy of a
string does not change much after a small change to the string, as well as our
simple yet efficient method for maintaining an array of variable-length blocks
under length modifications, may be useful for many other applications as well."
"We show that for every fixed undirected graph $H$, there is a $O(|V(G)|^3)$
time algorithm that tests, given a graph $G$, if $G$ contains $H$ as a
topological subgraph (that is, a subdivision of $H$ is subgraph of $G$). This
shows that topological subgraph testing is fixed-parameter tractable, resolving
a longstanding open question of Downey and Fellows from 1992. As a corollary,
for every $H$ we obtain an $O(|V(G)|^3)$ time algorithm that tests if there is
an immersion of $H$ into a given graph $G$. This answers another open question
raised by Downey and Fellows in 1992."
"Due to its optimality on a single machine for the problem of minimizing
average flow time, Shortest-Remaining-Processing-Time (\srpt) appears to be the
most natural algorithm to consider for the problem of minimizing average flow
time on multiple identical machines. It is known that $\srpt$ achieves the best
possible competitive ratio on multiple machines up to a constant factor. Using
resource augmentation, $\srpt$ is known to achieve total flow time at most that
of the optimal solution when given machines of speed $2- \frac{1}{m}$. Further,
it is known that $\srpt$'s competitive ratio improves as the speed increases;
$\srpt$ is $s$-speed $\frac{1}{s}$-competitive when $s \geq 2- \frac{1}{m}$.
  However, a gap has persisted in our understanding of $\srpt$. Before this
work, the performance of $\srpt$ was not known when $\srpt$ is given
$(1+\eps)$-speed when $0 < \eps < 1-\frac{1}{m}$, even though it has been
thought that $\srpt$ is $(1+\eps)$-speed $O(1)$-competitive for over a decade.
Resolving this question was suggested in Open Problem 2.9 from the survey
""Online Scheduling"" by Pruhs, Sgall, and Torng \cite{PruhsST}, and we answer
the question in this paper. We show that $\srpt$ is \emph{scalable} on $m$
identical machines. That is, we show $\srpt$ is $(1+\eps)$-speed
$O(\frac{1}{\eps})$-competitive for $\eps >0$. We complement this by showing
that $\srpt$ is $(1+\eps)$-speed $O(\frac{1}{\eps^2})$-competitive for the
objective of minimizing the $\ell_k$-norms of flow time on $m$ identical
machines. Both of our results rely on new potential functions that capture the
structure of \srpt. Our results, combined with previous work, show that $\srpt$
is the best possible online algorithm in essentially every aspect when
migration is permissible."
"Consider an optimization problem with $n$ binary variables and $d+1$ linear
objective functions. Each valid solution $x \in \{0,1\}^n$ gives rise to an
objective vector in $\R^{d+1}$, and one often wants to enumerate the Pareto
optima among them. In the worst case there may be exponentially many Pareto
optima; however, it was recently shown that in (a generalization of) the
smoothed analysis framework, the expected number is polynomial in $n$.
Unfortunately, the bound obtained had a rather bad dependence on $d$; roughly
$n^{d^d}$. In this paper we show a significantly improved bound of $n^{2d}$.
  Our proof is based on analyzing two algorithms. The first algorithm, on input
a Pareto optimal $x$, outputs a ""testimony"" containing clues about $x$'s
objective vector, $x$'s coordinates, and the region of space $B$ in which $x$'s
objective vector lies. The second algorithm can be regarded as a {\em
speculative} execution of the first -- it can uniquely reconstruct $x$ from the
testimony's clues and just \emph{some} of the probability space's outcomes. The
remainder of the probability space's outcomes are just enough to bound the
probability that $x$'s objective vector falls into the region $B$."
"We study sorting algorithms based on randomized round-robin comparisons.
Specifically, we study Spin-the-bottle sort, where comparisons are
unrestricted, and Annealing sort, where comparisons are restricted to a
distance bounded by a \emph{temperature} parameter. Both algorithms are simple,
randomized, data-oblivious sorting algorithms, which are useful in
privacy-preserving computations, but, as we show, Annealing sort is much more
efficient. We show that there is an input permutation that causes
Spin-the-bottle sort to require $\Omega(n^2\log n)$ expected time in order to
succeed, and that in $O(n^2\log n)$ time this algorithm succeeds with high
probability for any input. We also show there is an implementation of Annealing
sort that runs in $O(n\log n)$ time and succeeds with very high probability."
"In a ground-breaking paper, Indyk and Woodruff (STOC 05) showed how to
compute $F_k$ (for $k>2$) in space complexity $O(\mbox{\em poly-log}(n,m)\cdot
n^{1-\frac2k})$, which is optimal up to (large) poly-logarithmic factors in $n$
and $m$, where $m$ is the length of the stream and $n$ is the upper bound on
the number of distinct elements in a stream. The best known lower bound for
large moments is $\Omega(\log(n)n^{1-\frac2k})$. A follow-up work of
Bhuvanagiri, Ganguly, Kesh and Saha (SODA 2006) reduced the poly-logarithmic
factors of Indyk and Woodruff to $O(\log^2(m)\cdot (\log n+ \log m)\cdot
n^{1-{2\over k}})$. Further reduction of poly-log factors has been an elusive
goal since 2006, when Indyk and Woodruff method seemed to hit a natural
""barrier."" Using our simple recursive sketch, we provide a different yet simple
approach to obtain a $O(\log(m)\log(nm)\cdot (\log\log n)^4\cdot n^{1-{2\over
k}})$ algorithm for constant $\epsilon$ (our bound is, in fact, somewhat
stronger, where the $(\log\log n)$ term can be replaced by any constant number
of $\log $ iterations instead of just two or three, thus approaching $log^*n$.
Our bound also works for non-constant $\epsilon$ (for details see the body of
the paper). Further, our algorithm requires only $4$-wise independence, in
contrast to existing methods that use pseudo-random generators for computing
large frequency moments."
"The celebrated dimension reduction lemma of Johnson and Lindenstrauss has
numerous computational and other applications. Due to its application in
practice, speeding up the computation of a Johnson-Lindenstrauss style
dimension reduction is an important question. Recently, Dasgupta, Kumar, and
Sarlos (STOC 2010) constructed such a transform that uses a sparse matrix. This
is motivated by the desire to speed up the computation when applied to sparse
input vectors, a scenario that comes up in applications. The sparsity of their
construction was further improved by Kane and Nelson (ArXiv 2010).
  We improve the previous bound on the number of non-zero entries per column of
Kane and Nelson from $O(1/\epsilon \log(1/\delta)\log(k/\delta))$ (where the
target dimension is $k$, the distortion is $1\pm \epsilon$, and the failure
probability is $\delta$) to $$ O\left({1\over\epsilon}
\left({\log(1/\delta)\log\log\log(1/\delta) \over
\log\log(1/\delta)}\right)^2\right). $$
  We also improve the amount of randomness needed to generate the matrix. Our
results are obtained by connecting the moments of an order 2 Rademacher chaos
to the combinatorial properties of random Eulerian multigraphs. Estimating the
chance that a random multigraph is composed of a given number of node-disjoint
Eulerian components leads to a new tail bound on the chaos. Our estimates may
be of independent interest, and as this part of the argument is decoupled from
the analysis of the coefficients of the chaos, we believe that our methods can
be useful in the analysis of other chaoses."
"In this paper we study minimum cut and maximum flow problems on planar
graphs, both in static and in dynamic settings. First, we present an algorithm
that given an undirected planar graph computes the minimum cut between any two
given vertices in O(n log log n) time. Second, we show how to achieve the same
O(n log log n) bound for the problem of computing maximum flows in undirected
planar graphs. To the best of our knowledge, these are the first algorithms for
those two problems that break the O(n log n) barrier, which has been standing
for more than 25 years. Third, we present a fully dynamic algorithm that is
able to maintain information about minimum cuts and maximum flows in a plane
graph (i.e., a planar graph with a fixed embedding): our algorithm is able to
insert edges, delete edges and answer min-cut and max-flow queries between any
pair of vertices in O(n^(2/3) log^3 n) time per operation. This result is based
on a new dynamic shortest path algorithm for planar graphs which may be of
independent interest. We remark that this is the first known non-trivial
algorithm for min-cut and max-flow problems in a dynamic setting."
"In this paper, we explore worst-case solutions for the problems of single and
multiple matching on strings in the word RAM model with word length w. In the
first problem, we have to build a data structure based on a pattern p of length
m over an alphabet of size sigma such that we can answer to the following
query: given a text T of length n, where each character is encoded using
log(sigma) bits return the positions of all the occurrences of p in T (in the
following we refer by occ to the number of reported occurrences). For the
multi-pattern matching problem we have a set S of d patterns of total length m
and a query on a text T consists in finding all positions of all occurrences in
T of the patterns in S. As each character of the text is encoded using log
sigma bits and we can read w bits in constant time in the RAM model, we assume
that we can read up to (w/log sigma) consecutive characters of the text in one
time step. This implies that the fastest possible query time for both problems
is O((n(log sigma/w)+occ). In this paper we present several different results
for both problems which come close to that best possible query time. We first
present two different linear space data structures for the first and second
problem: the first one answers to single pattern matching queries in time
O(n(1/m+log sigma/w)+occ) while the second one answers to multiple pattern
matching queries to O(n((log d+log y+log log d)/y+log sigma/w)+occ) where y is
the length of the shortest pattern in the case of multiple pattern-matching. We
then show how a simple application of the four russian technique permits to get
data structures with query times independent of the length of the shortest
pattern (the length of the only pattern in case of single string matching) at
the expense of using more space."
"Suppose we are asked to preprocess a string \(s [1..n]\) such that later,
given a substring's endpoints, we can quickly count how many distinct
characters it contains. In this paper we give a data structure for this problem
that takes \(n H_0 (s) + \Oh{n} + \oh{n H_0 (s)}\) bits, where \(H_0 (s)\) is
the 0th-order empirical entropy of $s$, and answers queries in $\Oh{\log^{1 +
\epsilon} n}$ time for any constant \(\epsilon > 0\). We also show how our data
structure can be made partially dynamic."
"Suppose we have just performed searches in a self-index for two patterns $A$
and $B$ and now we want to search for their concatenation \A B); how can we
best make use of our previous computations? In this paper we consider this
problem and, more generally, how we can store a dynamic library of patterns
that we can easily manipulate in interesting ways. We give a space- and
time-efficient data structure for this problem that is compatible with many of
the best self-indexes."
"We examine directed spanners through flow-based linear programming
relaxations. We design an $\~O(n^{2/3})$-approximation algorithm for the
directed $k$-spanner problem that works for all $k\geq 1$, which is the first
sublinear approximation for arbitrary edge-lengths. Even in the more restricted
setting of unit edge-lengths, our algorithm improves over the previous
$\~O(n^{1-1/k})$ approximation of Bhattacharyya et al. when $k\ge 4$. For the
special case of $k=3$ we design a different algorithm achieving an
$\~O(\sqrt{n})$-approximation, improving the previous $\~O(n^{2/3})$. Both of
our algorithms easily extend to the fault-tolerant setting, which has recently
attracted attention but not from an approximation viewpoint. We also prove a
nearly matching integrality gap of $\Omega(n^{\frac13 - \epsilon})$ for any
constant $\epsilon > 0$.
  A virtue of all our algorithms is that they are relatively simple.
Technically, we introduce a new yet natural flow-based relaxation, and show how
to approximately solve it even when its size is not polynomial. The main
challenge is to design a rounding scheme that ""coordinates"" the choices of
flow-paths between the many demand pairs while using few edges overall. We
achieve this, roughly speaking, by randomization at the level of vertices."
"Given a metric space on n points, an {\alpha}-approximate universal algorithm
for the Steiner tree problem outputs a distribution over rooted spanning trees
such that for any subset X of vertices containing the root, the expected cost
of the induced subtree is within an {\alpha} factor of the optimal Steiner tree
cost for X. An {\alpha}-approximate differentially private algorithm for the
Steiner tree problem takes as input a subset X of vertices, and outputs a tree
distribution that induces a solution within an {\alpha} factor of the optimal
as before, and satisfies the additional property that for any set X' that
differs in a single vertex from X, the tree distributions for X and X' are
""close"" to each other. Universal and differentially private algorithms for TSP
are defined similarly. An {\alpha}-approximate universal algorithm for the
Steiner tree problem or TSP is also an {\alpha}-approximate differentially
private algorithm. It is known that both problems admit O(logn)-approximate
universal algorithms, and hence O(log n)-approximate differentially private
algorithms as well. We prove an {\Omega}(logn) lower bound on the approximation
ratio achievable for the universal Steiner tree problem and the universal TSP,
matching the known upper bounds. Our lower bound for the Steiner tree problem
holds even when the algorithm is allowed to output a more general solution of a
distribution on paths to the root."
"This paper introduces a novel method for compact representation of sets of
n-dimensional binary sequences in a form of compact triplets structures (CTS),
supposing both logic and arithmetic interpretations of data. Suitable
illustration of CTS application is the unique graph-combinatorial model for the
classic intractable 3-Satisfiability problem and a polynomial algorithm for the
model synthesis. The method used for Boolean formulas analysis and
classification by means of the model is defined as a bijective mapping
principle for sets of components of discordant structures to a basic set. The
statistic computer-aided experiment showed efficiency of the algorithm in a
large scale of problem dimension parameters, including those that make
enumeration procedures of no use. The formulated principle expands resources of
constructive approach to investigation of intractable problems."
"We present two methods to compress the description of a route in a road
network, i.e., of a path in a directed graph. The first method represents a
path by a sequence of via edges. The subpaths between the via edges have to be
unique shortest paths. Instead of via edges also via nodes can be used, though
this requires some simple preprocessing. The second method uses contraction
hierarchies to replace subpaths of the original path by shortcuts. The two
methods can be combined with each other. Also, we propose the application to
mobile server based routing: We compute the route on a server which has access
to the latest information about congestions for example. Then we transmit the
computed route to the car using some mobile radio communication. There, we
apply the compression to save costs and transmission time. If the compression
works well, we can transmit routes even when the bandwidth is low. Although we
have not evaluated our ideas with realistic data yet, they are quite promising."
"Randomized algorithms are often enjoyed for their simplicity, but the hash
functions used to yield the desired theoretical guarantees are often neither
simple nor practical. Here we show that the simplest possible tabulation
hashing provides unexpectedly strong guarantees.
  The scheme itself dates back to Carter and Wegman (STOC'77). Keys are viewed
as consisting of c characters. We initialize c tables T_1, ..., T_c mapping
characters to random hash codes. A key x=(x_1, ..., x_q) is hashed to T_1[x_1]
xor ... xor T_c[x_c].
  While this scheme is not even 4-independent, we show that it provides many of
the guarantees that are normally obtained via higher independence, e.g.,
Chernoff-type concentration, min-wise hashing for estimating set intersection,
and cuckoo hashing."
"We present a new randomized algorithm for computing the diameter of a
weighted directed graph. The algorithm runs in
$\Ot(M^{\w/(\w+1)}n^{(\w^2+3)/(\w+1)})$ time, where $\w < 2.376$ is the
exponent of fast matrix multiplication, $n$ is the number of vertices of the
graph, and the edge weights are integers in $\{-M,...,0,...,M\}$. For bounded
integer weights the running time is $O(n^{2.561})$ and if $\w=2+o(1)$ it is
$\Ot(n^{7/3})$. This is the first algorithm that computes the diameter of an
integer weighted directed graph polynomially faster than any known All-Pairs
Shortest Paths (APSP) algorithm. For bounded integer weights, the fastest
algorithm for APSP runs in $O(n^{2.575})$ time for the present value of $\w$
and runs in $\Ot(n^{2.5})$ time if $\w=2+o(1)$.
  For directed graphs with {\em positive} integer weights in $\{1,...,M\}$ we
obtain a deterministic algorithm that computes the diameter in $\Ot(Mn^\w)$
time. This extends a simple $\Ot(n^\w)$ algorithm for computing the diameter of
an {\em unweighted} directed graph to the positive integer weighted setting and
is the first algorithm in this setting whose time complexity matches that of
the fastest known Diameter algorithm for {\em undirected} graphs.
  The diameter algorithms are consequences of a more general result. We
construct algorithms that for any given integer $d$, report all ordered pairs
of vertices having distance {\em at most} $d$. The diameter can therefore be
computed using binary search for the smallest $d$ for which all pairs are
reported."
"A binary matrix satisfies the consecutive ones property (COP) if its columns
can be permuted such that the ones in each row of the resulting matrix are
consecutive. Equivalently, a family of sets F = {Q_1,..,Q_m}, where Q_i is
subset of R for some universe R, satisfies the COP if the symbols in R can be
permuted such that the elements of each set Q_i occur consecutively, as a
contiguous segment of the permutation of R's symbols. We consider the COP
version on multisets and prove that counting its solutions is difficult
(#P-complete). We prove completeness results also for counting the frontiers of
PQ-trees, which are typically used for testing the COP on sets, thus showing
that a polynomial algorithm is unlikely to exist when dealing with multisets.
We use a combinatorial approach based on parsimonious reductions from the
Hamiltonian path problem, showing that the decisional version of our problems
is therefore NP-complete."
"Let $A$ be a static array storing $n$ elements from a totally ordered set. We
present a data structure of optimal size at most $n\log_2(3+2\sqrt{2})+o(n)$
bits that allows us to answer the following queries on $A$ in constant time,
without accessing $A$: (1) previous smaller value queries, where given an index
$i$, we wish to find the first index to the left of $i$ where $A$ is strictly
smaller than at $i$, and (2) next smaller value queries, which search to the
right of $i$. As an additional bonus, our data structure also allows to answer
a third kind of query: given indices $i<j$, find the position of the minimum in
$A[i..j]$. Our data structure has direct consequences for the space-efficient
storage of suffix trees."
"In this paper discussed procedure of separation of the original problem with
several vehicles to a number of simpler problems with one vehicle which based
on the matrix approach."
"We present two adaptive schemes for dynamically choosing the number of
parallel instances in parallel evolutionary algorithms. This includes the
choice of the offspring population size in a (1+$\lambda$) EA as a special
case. Our schemes are parameterless and they work in a black-box setting where
no knowledge on the problem is available. Both schemes double the number of
instances in case a generation ends without finding an improvement. In a
successful generation, the first scheme resets the system to one instance,
while the second scheme halves the number of instances. Both schemes provide
near-optimal speed-ups in terms of the parallel time. We give upper bounds for
the asymptotic sequential time (i.e., the total number of function evaluations)
that are not larger than upper bounds for a corresponding non-parallel
algorithm derived by the fitness-level method."
This article has been withdrawn.
"We give a memoryless scale-invariant randomized algorithm for the Buffer
Management with Bounded Delay problem that is e/(e-1)-competitive against an
adaptive adversary, together with better performance guarantees for many
restricted variants, including the s-bounded instances. In particular, our
algorithm attains the optimum competitive ratio of 4/3 on 2-bounded instances.
  Both the algorithm and its analysis are applicable to a more general problem,
called Collecting Items, in which only the relative order between packets'
deadlines is known. Our algorithm is the optimal randomized memoryless
algorithm against adaptive adversary for that problem in a strong sense.
  While some of provided upper bounds were already known, in general, they were
attained by several different algorithms."
"A hitting set for a collection of sets is a set that has a non-empty
intersection with each set in the collection; the hitting set problem is to
find a hitting set of minimum cardinality. Motivated by instances of the
hitting set problem where the number of sets to be hit is large, we introduce
the notion of implicit hitting set problems. In an implicit hitting set problem
the collection of sets to be hit is typically too large to list explicitly;
instead, an oracle is provided which, given a set H, either determines that H
is a hitting set or returns a set that H does not hit. We show a number of
examples of classic implicit hitting set problems, and give a generic algorithm
for solving such problems optimally. The main contribution of this paper is to
show that this framework is valuable in developing approximation algorithms. We
illustrate this methodology by presenting a simple on-line algorithm for the
minimum feedback vertex set problem on random graphs. In particular our
algorithm gives a feedback vertex set of size n-(1/p)\log{np}(1-o(1)) with
probability at least 3/4 for the random graph G_{n,p} (the smallest feedback
vertex set is of size n-(2/p)\log{np}(1+o(1))). We also consider a planted
model for the feedback vertex set in directed random graphs. Here we show that
a hitting set for a polynomial-sized subset of cycles is a hitting set for the
planted random graph and this allows us to exactly recover the planted feedback
vertex set."
"The Parikh vector p(s) of a string s is defined as the vector of
multiplicities of the characters. Parikh vector q occurs in s if s has a
substring t with p(t)=q. We present two novel algorithms for searching for a
query q in a text s. One solves the decision problem over a binary text in
constant time, using a linear size index of the text. The second algorithm, for
a general finite alphabet, finds all occurrences of a given Parikh vector q and
has sub-linear expected time complexity; we present two variants, which both
use a linear size index of the text."
"We study frequency allocation in wireless networks. A wireless network is
modeled by an undirected graph, with vertices corresponding to cells. In each
vertex we have a certain number of requests, and each of those requests must be
assigned a different frequency. Edges represent conflicts between cells,
meaning that frequencies in adjacent vertices must be different as well. The
objective is to minimize the total number of used frequencies.
  The offline version of the problem is known to be NP-hard. In the incremental
version, requests for frequencies arrive over time and the algorithm is
required to assign a frequency to a request as soon as it arrives. Competitive
incremental algorithms have been studied for several classes of graphs. For
paths, the optimal (asymptotic) ratio is known to be 4/3, while for
hexagonal-cell graphs it is between 1.5 and 1.9126. For k-colorable graphs, the
ratio of (k+1)/2 can be achieved.
  In this paper, we prove nearly tight bounds on the asymptotic competitive
ratio for bipartite graphs, showing that it is between 1.428 and 1.433. This
improves the previous lower bound of 4/3 and upper bound of 1.5. Our proofs are
based on reducing the incremental problem to a purely combinatorial
(equivalent) problem of constructing set families with certain intersection
properties."
"We give a simple polynomial time approximation scheme for the weighted
matroid matching problem on strongly base orderable matroids. We also show that
even the unweighted version of this problem is NP-complete and not in
oracle-coNP."
"In a recent paper from SODA11 \cite{kminwise} the authors introduced a
general framework for exponential time improvement of \minwise based algorithms
by defining and constructing almost \kmin independent family of hash functions.
Here we take it a step forward and reduce the space and the independent needed
for representing the functions, by defining and constructing a \dkmin
independent family of hash functions. Surprisingly, for most cases only 8-wise
independent is needed for exponential time and space improvement. Moreover, we
bypass the $O(\log{\frac{1}{\epsilon}})$ independent lower bound for
approximately \minwise functions \cite{patrascu10kwise-lb}, as we use
alternative definition. In addition, as the independent's degree is a small
constant it can be implemented efficiently.
  Informally, under this definition, all subsets of size $d$ of any fixed set
$X$ have an equal probability to have hash values among the minimal $k$ values
in $X$, where the probability is over the random choice of hash function from
the family. This property measures the randomness of the family, as choosing a
truly random function, obviously, satisfies the definition for $d=k=|X|$. We
define and give an efficient time and space construction of approximately
\dkmin independent family of hash functions. The degree of independent required
is optimal, i.e. only $O(d)$ for $2 \le d < k=O(\frac{d}{\epsilon^2})$, where
$\epsilon \in (0,1)$ is the desired error bound. This construction can be used
to improve many \minwise based algorithms, such as
\cite{sizeEstimationFramework,Datar02estimatingrarity,NearDuplicate,SimilaritySearch,DBLP:conf/podc/CohenK07},
as will be discussed here. To our knowledge such definitions, for hash
functions, were never studied and no construction was given before."
"In the unsplittable flow problem on a path, we are given a capacitated path
$P$ and $n$ tasks, each task having a demand, a profit, and start and end
vertices. The goal is to compute a maximum profit set of tasks, such that for
each edge $e$ of $P$, the total demand of selected tasks that use $e$ does not
exceed the capacity of $e$. This is a well-studied problem that has been
studied under alternative names, such as resource allocation, bandwidth
allocation, resource constrained scheduling, temporal knapsack and interval
packing.
  We present a polynomial time constant-factor approximation algorithm for this
problem. This improves on the previous best known approximation ratio of
$O(\log n)$. The approximation ratio of our algorithm is $7+\epsilon$ for any
$\epsilon>0$.
  We introduce several novel algorithmic techniques, which might be of
independent interest: a framework which reduces the problem to instances with a
bounded range of capacities, and a new geometrically inspired dynamic program
which solves a special case of the maximum weight independent set of rectangles
problem to optimality. In the setting of resource augmentation, wherein the
capacities can be slightly violated, we give a $(2+\epsilon)$-approximation
algorithm. In addition, we show that the problem is strongly NP-hard even if
all edge capacities are equal and all demands are either~1,~2, or~3."
"In the stochastic knapsack problem, we are given a knapsack of size B, and a
set of jobs whose sizes and rewards are drawn from a known probability
distribution. However, we know the actual size and reward only when the job
completes. How should we schedule jobs to maximize the expected total reward?
We know O(1)-approximations when we assume that (i) rewards and sizes are
independent random variables, and (ii) we cannot prematurely cancel jobs. What
can we say when either or both of these assumptions are changed?
  The stochastic knapsack problem is of interest in its own right, but
techniques developed for it are applicable to other stochastic packing
problems. Indeed, ideas for this problem have been useful for budgeted learning
problems, where one is given several arms which evolve in a specified
stochastic fashion with each pull, and the goal is to pull the arms a total of
B times to maximize the reward obtained. Much recent work on this problem focus
on the case when the evolution of the arms follows a martingale, i.e., when the
expected reward from the future is the same as the reward at the current state.
What can we say when the rewards do not form a martingale?
  In this paper, we give constant-factor approximation algorithms for the
stochastic knapsack problem with correlations and/or cancellations, and also
for budgeted learning problems where the martingale condition is not satisfied.
Indeed, we can show that previously proposed LP relaxations have large
integrality gaps. We propose new time-indexed LP relaxations, and convert the
fractional solutions into distributions over strategies, and then use the LP
values and the time ordering information from these strategies to devise a
randomized adaptive scheduling algorithm. We hope our LP formulation and
decomposition methods may provide a new way to address other correlated bandit
problems with more general contexts."
"A hypergraph ${\cal F}$ is a set family defined on vertex set $V$. The dual
of ${\cal F}$ is the set of minimal subsets $H$ of $V$ such that $F\cap H \ne
\emptyset$ for any $F\in {\cal F}$. The computation of the dual is equivalent
to many problems, such as minimal hitting set enumeration of a subset family,
minimal set cover enumeration, and the enumeration of hypergraph transversals.
Although many algorithms have been proposed for solving the problem, to the
best of our knowledge, none of them can work on large-scale input with a large
number of output minimal hitting sets. This paper focuses on developing time-
and space-efficient algorithms for solving the problem. We propose two new
algorithms with new search methods, new pruning methods, and fast techniques
for the minimality check. The computational experiments show that our
algorithms are quite fast even for large-scale input for which existing
algorithms do not terminate in a practical time."
"A linear graph is a graph whose vertices are totally ordered. Biological and
linguistic sequences with interactions among symbols are naturally represented
as linear graphs. Examples include protein contact maps, RNA secondary
structures and predicate-argument structures. Our algorithm, linear graph miner
(LGM), leverages the vertex order for efficient enumeration of frequent
subgraphs. Based on the reverse search principle, the pattern space is
systematically traversed without expensive duplication checking. Disconnected
subgraph patterns are particularly important in linear graphs due to their
sequential nature. Unlike conventional graph mining algorithms detecting
connected patterns only, LGM can detect disconnected patterns as well. The
utility and efficiency of LGM are demonstrated in experiments on protein
contact maps."
"Does there exist O(1)-competitive (self-adjusting) binary search tree (BST)
algorithms? This is a well-studied problem. A simple offline BST algorithm
GreedyFuture was proposed independently by Lucas and Munro, and they
conjectured it to be O(1)-competitive. Recently, Demaine et al. gave a
geometric view of the BST problem. This view allowed them to give an online
algorithm GreedyArb with the same cost as GreedyFuture. However, no
o(n)-competitive ratio was known for GreedyArb. In this paper we make progress
towards proving O(1)-competitive ratio for GreedyArb by showing that it is
O(\log n)-competitive."
"We present an improved algorithm for solving symmetrically diagonally
dominant linear systems. On input of an $n\times n$ symmetric diagonally
dominant matrix $A$ with $m$ non-zero entries and a vector $b$ such that
$A\bar{x} = b$ for some (unknown) vector $\bar{x}$, our algorithm computes a
vector $x$ such that $||{x}-\bar{x}||_A < \epsilon ||\bar{x}||_A $
{$||\cdot||_A$ denotes the A-norm} in time $${\tilde O}(m\log n \log
(1/\epsilon)).$$
  The solver utilizes in a standard way a `preconditioning' chain of
progressively sparser graphs. To claim the faster running time we make a
two-fold improvement in the algorithm for constructing the chain. The new chain
exploits previously unknown properties of the graph sparsification algorithm
given in [Koutis,Miller,Peng, FOCS 2010], allowing for stronger preconditioning
properties. We also present an algorithm of independent interest that
constructs nearly-tight low-stretch spanning trees in time
$\tilde{O}(m\log{n})$, a factor of $O(\log{n})$ faster than the algorithm in
[Abraham,Bartal,Neiman, FOCS 2008]. This speedup directly reflects on the
construction time of the preconditioning chain."
"At SODA 2009, Demaine et al. presented a novel connection between binary
search trees (BSTs) and subsets of points on the plane. This connection was
independently discovered by Derryberry et al. As part of their results, Demaine
et al. considered GreedyFuture, an offline BST algorithm that greedily
rearranges the search path to minimize the cost of future searches. They showed
that GreedyFuture is actually an online algorithm in their geometric view, and
that there is a way to turn GreedyFuture into an online BST algorithm with only
a constant factor increase in total search cost. Demaine et al. conjectured
this algorithm was dynamically optimal, but no upper bounds were given in their
paper. We prove the first non-trivial upper bounds for the cost of search
operations using GreedyFuture including giving an access lemma similar to that
found in Sleator and Tarjan's classic paper on splay trees."
"In a classical covering problem, we are given a set of requests that we need
to satisfy (fully or partially), by buying a subset of items at minimum cost.
For example, in the k-MST problem we want to find the cheapest tree spanning at
least k nodes of an edge-weighted graph. Here nodes and edges represent
requests and items, respectively.
  In this paper, we initiate the study of a new family of multi-layer covering
problems. Each such problem consists of a collection of h distinct instances of
a standard covering problem (layers), with the constraint that all layers share
the same set of requests. We identify two main subfamilies of these problems: -
in a union multi-layer problem, a request is satisfied if it is satisfied in at
least one layer; - in an intersection multi-layer problem, a request is
satisfied if it is satisfied in all layers. To see some natural applications,
consider both generalizations of k-MST. Union k-MST can model a problem where
we are asked to connect a set of users to at least one of two communication
networks, e.g., a wireless and a wired network. On the other hand, intersection
k-MST can formalize the problem of connecting a subset of users to both
electricity and water.
  We present a number of hardness and approximation results for union and
intersection versions of several standard optimization problems: MST, Steiner
tree, set cover, facility location, TSP, and their partial covering variants."
"In processing large quantities of data, a fundamental problem is to obtain a
summary which supports approximate query answering. Random sampling yields
flexible summaries which naturally support subset-sum queries with unbiased
estimators and well-understood confidence bounds.
  Classic sample-based summaries, however, are designed for arbitrary subset
queries and are oblivious to the structure in the set of keys. The particular
structure, such as hierarchy, order, or product space (multi-dimensional),
makes range queries much more relevant for most analysis of the data.
  Dedicated summarization algorithms for range-sum queries have also been
extensively studied. They can outperform existing sampling schemes in terms of
accuracy on range queries per summary size. Their accuracy, however, rapidly
degrades when, as is often the case, the query spans multiple ranges. They are
also less flexible - being targeted for range sum queries alone - and are often
quite costly to build and use.
  In this paper we propose and evaluate variance optimal sampling schemes that
are structure-aware. These summaries improve over the accuracy of existing
structure-oblivious sampling schemes on range queries while retaining the
benefits of sample-based summaries: flexible summaries, with high accuracy on
both range queries and arbitrary subset queries."
"We initiate the study of the Bipartite Contraction problem from the
perspective of parameterized complexity. In this problem we are given a graph
$G$ and an integer $k$, and the task is to determine whether we can obtain a
bipartite graph from $G$ by a sequence of at most $k$ edge contractions. Our
main result is an $f(k) n^{O(1)}$ time algorithm for Bipartite Contraction.
Despite a strong resemblance between Bipartite Contraction and the classical
Odd Cycle Transversal (OCT) problem, the methods developed to tackle OCT do not
seem to be directly applicable to Bipartite Contraction. Our algorithm is based
on a novel combination of the irrelevant vertex technique, introduced by
Robertson and Seymour, and the concept of important separators. Both techniques
have previously been used as key components of algorithms for fundamental
problems in parameterized complexity. However, to the best of our knowledge,
this is the first time the two techniques are applied in unison."
"Dial a ride problems consist of a metric space (denoting travel time between
vertices) and a set of m objects represented as source-destination pairs, where
each object requires to be moved from its source to destination vertex. We
consider the multi-vehicle Dial a ride problem, with each vehicle having
capacity k and its own depot-vertex, where the objective is to minimize the
maximum completion time (makespan) of the vehicles. We study the ""preemptive""
version of the problem, where an object may be left at intermediate vertices
and transported by more than one vehicle, while being moved from source to
destination. Our main results are an O(log^3 n)-approximation algorithm for
preemptive multi-vehicle Dial a ride, and an improved O(log t)-approximation
for its special case when there is no capacity constraint. We also show that
the approximation ratios improve by a log-factor when the underlying metric is
induced by a fixed-minor-free graph."
"The Hierarchical Heavy Hitters problem extends the notion of frequent items
to data arranged in a hierarchy. This problem has applications to network
traffic monitoring, anomaly detection, and DDoS detection. We present a new
streaming approximation algorithm for computing Hierarchical Heavy Hitters that
has several advantages over previous algorithms. It improves on the worst-case
time and space bounds of earlier algorithms, is conceptually simple and
substantially easier to implement, offers improved accuracy guarantees, is
easily adopted to a distributed or parallel setting, and can be efficiently
implemented in commodity hardware such as ternary content addressable memory
(TCAMs). We present experimental results showing that for parameters of primary
practical interest, our two-dimensional algorithm is superior to existing
algorithms in terms of speed and accuracy, and competitive in terms of space,
while our one-dimensional algorithm is also superior in terms of speed and
accuracy for a more limited range of parameters."
"The significant progress in constructing graph spanners that are sparse
(small number of edges) or light (low total weight) has skipped spanners that
are everywhere-sparse (small maximum degree). This disparity is in line with
other network design problems, where the maximum-degree objective has been a
notorious technical challenge. Our main result is for the Lowest Degree
2-Spanner (LD2S) problem, where the goal is to compute a 2-spanner of an input
graph so as to minimize the maximum degree. We design a polynomial-time
algorithm achieving approximation factor $\tilde O(\Delta^{3-2\sqrt{2}})
\approx \tilde O(\Delta^{0.172})$, where $\Delta$ is the maximum degree of the
input graph. The previous $\tilde O(\Delta^{1/4})$ -approximation was proved
nearly two decades ago by Kortsarz and Peleg [SODA 1994, SICOMP 1998].
  Our main conceptual contribution is to establish a formal connection between
LD2S and a variant of the Densest k-Subgraph (DkS) problem. Specifically, we
design for both problems strong relaxations based on the Sherali-Adams linear
programming (LP) hierarchy, and show that ""faithful"" randomized rounding of the
DkS-variant can be used to round LD2S solutions. Our notion of faithfulness
intuitively means that all vertices and edges are chosen with probability
proportional to their LP value, but the precise formulation is more subtle.
  Unfortunately, the best algorithms known for DkS use the Lov\'asz-Schrijver
LP hierarchy in a non-faithful way [Bhaskara, Charikar, Chlamtac, Feige, and
Vijayaraghavan, STOC 2010]. Our main technical contribution is to overcome this
shortcoming, while still matching the gap that arises in random graphs by
planting a subgraph with same log-density."
"A covering integer program (CIP) is a mathematical program of the form: min
{c^T x : Ax >= 1, 0 <= x <= u, x integer}, where A is an m x n matrix, and c
and u are n-dimensional vectors, all having non-negative entries. In the online
setting, the constraints (i.e., the rows of the constraint matrix A) arrive
over time, and the algorithm can only increase the coordinates of vector x to
maintain feasibility. As an intermediate step, we consider solving the covering
linear program (CLP) online, where the integrality requirement on x is dropped.
  Our main results are (a) an O(log k)-competitive online algorithm for solving
the CLP, and (b) an O(log k log L)-competitive randomized online algorithm for
solving the CIP. Here k<=n and L<=m respectively denote the maximum number of
non-zero entries in any row and column of the constraint matrix A. By a result
of Feige and Korman, this is the best possible for polynomial-time online
algorithms, even in the special case of set cover."
"Recently Raghavendra and Tan (SODA 2012) gave a 0.85-approximation algorithm
for the Max Bisection problem. We improve their algorithm to a
0.8776-approximation. As Max Bisection is hard to approximate within
$\alpha_{GW} + \epsilon \approx 0.8786$ under the Unique Games Conjecture
(UGC), our algorithm is nearly optimal. We conjecture that Max Bisection is
approximable within $\alpha_{GW}-\epsilon$, i.e., the bisection constraint
(essentially) does not make Max Cut harder.
  We also obtain an optimal algorithm (assuming the UGC) for the analogous
variant of Max 2-Sat. Our approximation ratio for this problem exactly matches
the optimal approximation ratio for Max 2-Sat, i.e., $\alpha_{LLZ} + \epsilon
\approx 0.9401$, showing that the bisection constraint does not make Max 2-Sat
harder. This improves on a 0.93-approximation for this problem due to
Raghavendra and Tan."
"Renaming is a fundamental problem in distributed computing, which consists of
a set of processes picking distinct names from a given namespace. The paper
presents algorithms that solve order-preserving renaming in synchronous message
passing systems with Byzantine processes. To the best of our knowledge, this
work is the first to address order-preserving renaming in the given model.
Although this problem can be solved by using consensus, it is known that
renaming is ""weaker"" than consensus, therefore we are mainly concerned with the
efficiency of performing renaming and make three contributions in this
direction. We present an order-preserving renaming algorithm for $N > 3t$ with
target namespace of size $N+t-1$ and logarithmic step complexity (where $N$ is
the number of processes and $t$ is an upper bound on the number of faults).
Similarly to the existing crash-tolerant solution, our algorithm employs the
ideas from the approximate agreement problem. We show that our algorithm has
constant step complexity if $N>t^2+2t$ and achieves tight namespace of size
$N$. Finally, we present an algorithm that solves order-preserving renaming in
just 2 communication steps, if $N > 2t^2 + t$."
"We consider the classical secret sharing problem in the case where all agents
are selfish but rational. In recent work, Kol and Naor show that, when there
are two players, in the non-simultaneous communication model, i.e. when rushing
is possible, there is no Nash equilibrium that ensures both players learn the
secret. However, they describe a mechanism for this problem, for any number of
players, that is an epsilon-Nash equilibrium, in that no player can gain more
than epsilon utility by deviating from it. Unfortunately, the Kol and Naor
mechanism, and, to the best of our knowledge, all previous mechanisms for this
problem require each agent to send O(n) messages in expectation, where n is the
number of agents. This may be problematic for some applications of rational
secret sharing such as secure multi-party computation and simulation of a
mediator.
  We address this issue by describing mechanisms for rational secret sharing
that are designed for large n. Both of our results hold for n > 2, and are Nash
equilbria, rather than just epsilon-Nash equilbria.
  Our first result is a mechanism for n-out-of-n rational secret sharing that
is scalable in the sense that it requires each agent to send only an expected
O(log n) bits. Moreover, the latency of this mechanism is O(log n) in
expectation, compared to O(n) expected latency for the Kol and Naor result. Our
second result is a mechanism for a relaxed variant of rational m-out-of-n
secret sharing where m = Theta(n). It requires each processor to send O(log n)
bits and has O(log n) latency. Both of our mechanisms are non-cryptographic,
and are not susceptible to backwards induction."
"A very well-known machine model in scheduling allows the machines to be
unrelated, modelling jobs that might have different characteristics on each
machine. Due to its generality, many optimization problems of this form are
very difficult to tackle and typically APX-hard. However, in many applications
the number of different types of machines, such as processor cores, GPUs, etc.
is very limited. In this paper, we address this point and study the assignment
of jobs to unrelated machines in the case that each machine belongs to one of a
fixed number of types and the machines of each type are identical. We present
polynomial time approximation schemes (PTASs) for minimizing the makespan for
multidimensional jobs with a fixed number of dimensions and for minimizing the
L_p-norm. In particular, our results subsume and generalize the existing PTASs
for a constant number of unrelated machines and for an arbitrary number of
identical machines for these problems. We employ a number of techniques which
go beyond the previously known results, including a new counting argument and a
method for making the concept of sparse extreme point solutions usable for a
convex program."
"We consider the task of optimizing the B-tree data structure, used
extensively in operating systems and databases, for sustainable usage on
multi-level flash memory. Empirical evidence shows that this new flash memory
tree, or FM Tree, extends the operational lifespan of each block of flash
memory by a factor of roughly 27 to 70 times, while still supporting
logarithmic-time search tree operations."