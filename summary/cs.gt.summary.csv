summary
"This paper analyzes individually-rational ex post equilibrium in the VC
(Vickrey-Clarke) combinatorial auctions. If $\Sigma$ is a family of bundles of
goods, the organizer may restrict the participants by requiring them to submit
their bids only for bundles in $\Sigma$. The $\Sigma$-VC combinatorial auctions
(multi-good auctions) obtained in this way are known to be
individually-rational truth-telling mechanisms. In contrast, this paper deals
with non-restricted VC auctions, in which the buyers restrict themselves to
bids on bundles in $\Sigma$, because it is rational for them to do so. That is,
it may be that when the buyers report their valuation of the bundles in
$\Sigma$, they are in an equilibrium. We fully characterize those $\Sigma$ that
induce individually rational equilibrium in every VC auction, and we refer to
the associated equilibrium as a bundling equilibrium. The number of bundles in
$\Sigma$ represents the communication complexity of the equilibrium. A special
case of bundling equilibrium is partition-based equilibrium, in which $\Sigma$
is a field, that is, it is generated by a partition. We analyze the tradeoff
between communication complexity and economic efficiency of bundling
equilibrium, focusing in particular on partition-based equilibrium."
"In most of microeconomic theory, consumers are assumed to exhibit decreasing
marginal utilities. This paper considers combinatorial auctions among such
submodular buyers. The valuations of such buyers are placed within a hierarchy
of valuations that exhibit no complementarities, a hierarchy that includes also
OR and XOR combinations of singleton valuations, and valuations satisfying the
gross substitutes property. Those last valuations are shown to form a
zero-measure subset of the submodular valuations that have positive measure.
While we show that the allocation problem among submodular valuations is
NP-hard, we present an efficient greedy 2-approximation algorithm for this case
and generalize it to the case of limited complementarities. No such
approximation algorithm exists in a setting allowing for arbitrary
complementarities. Some results about strategic aspects of combinatorial
auctions among players with decreasing marginal utilities are also presented."
"Some important classical mechanisms considered in Microeconomics and Game
Theory require the solution of a difficult optimization problem. This is true
of mechanisms for combinatorial auctions, which have in recent years assumed
practical importance, and in particular of the gold standard for combinatorial
auctions, the Generalized Vickrey Auction (GVA). Traditional analysis of these
mechanisms - in particular, their truth revelation properties - assumes that
the optimization problems are solved precisely. In reality, these optimization
problems can usually be solved only in an approximate fashion. We investigate
the impact on such mechanisms of replacing exact solutions by approximate ones.
Specifically, we look at a particular greedy optimization method. We show that
the GVA payment scheme does not provide for a truth revealing mechanism. We
introduce another scheme that does guarantee truthfulness for a restricted
class of players. We demonstrate the latter property by identifying natural
properties for combinatorial auctions and showing that, for our restricted
class of players, they imply that truthful strategies are dominant. Those
properties have applicability beyond the specific auction studied."
"A model for decision making that generalizes Expected Utility Maximization is
presented. This model, Expected Qualitative Utility Maximization, encompasses
the Maximin criterion. It relaxes both the Independence and the Continuity
postulates. Its main ingredient is the definition of a qualitative order on
nonstandard models of the real numbers and the consideration of nonstandard
utilities. Expected Qualitative Utility Maximization is characterized by an
original weakening of von Neumann-Morgenstern's postulates. Subjective
probabilities may be defined from those weakened postulates, as Anscombe and
Aumann did from the original postulates. Subjective probabilities are numbers,
not matrices as in the Subjective Expected Lexicographic Utility approach. JEL
no.: D81 Keywords: Utility Theory, Non-Standard Utilities, Qualitative Decision
Theory"
"Certain services may be provided in a continuous, one-dimensional, ordered
range of different qualities and a customer requiring a service of quality q
can only be offered a quality superior or equal to q. Only a discrete set of
different qualities will be offered, and a service provider will provide the
same service (of fixed quality b) to all customers requesting qualities of
service inferior or equal to b. Assuming all services (of quality b) are priced
identically, a monopolist will choose the qualities of service and the prices
that maximize profit but, under perfect competition, a service provider will
choose the (inferior) quality of service that can be priced at the lowest
price. Assuming significant economies of scale, two fundamentally different
regimes are possible: either a number of different classes of service are
offered (DC regime), or a unique class of service offers an unbounded quality
of service (UC regime). The DC regime appears in one of two sub-regimes: one,
BDC, in which a finite number of classes is offered, the qualities of service
offered are bounded and requests for high-quality services are not met, or UDC
in which an infinite number of classes of service are offered and every request
is met. The types of the demand curve and of the economies of scale, not the
pace of technological change, determine the regime and the class boundaries.
The price structure in the DC regime obeys very general laws."
"The consideration of nonstandard models of the real numbers and the
definition of a qualitative ordering on those models provides a generalization
of the principle of maximization of expected utility. It enables the decider to
assign probabilities of different orders of magnitude to different events or to
assign utilities of different orders of magnitude to different outcomes. The
properties of this generalized notion of rationality are studied in the
frameworks proposed by von Neumann and Morgenstern and later by Anscombe and
Aumann. It is characterized by an original weakening of their postulates in two
different situations: nonstandard probabilities and standard utilities on one
hand and standard probabilities and nonstandard utilities on the other hand.
This weakening concerns both Independence and Continuity. It is orthogonal with
the weakening proposed by lexicographic orderings."
"In multiagent settings where the agents have different preferences,
preference aggregation is a central issue. Voting is a general method for
preference aggregation, but seminal results have shown that all general voting
protocols are manipulable. One could try to avoid manipulation by using voting
protocols where determining a beneficial manipulation is hard. Especially among
computational agents, it is reasonable to measure this hardness by
computational complexity. Some earlier work has been done in this area, but it
was assumed that the number of voters and candidates is unbounded. We derive
hardness results for practical multiagent settings where the number of
candidates is small but the number of voters can be large. We show that with
complete information about the others' votes, individual manipulation is easy,
and coalitional manipulation is easy with unweighted voters. However,
constructive coalitional manipulation with weighted voters is intractable for
all of the voting protocols under study, except for the nonrandomized Cup.
Destructive manipulation tends to be easier. Randomizing over instantiations of
the protocols (such as schedules of the Cup protocol) can be used to make
manipulation hard. Finally, we show that under weak assumptions, if weighted
coalitional manipulation with complete information about the others' votes is
hard in some voting protocol, then individual and unweighted manipulation is
hard when there is uncertainty about the others' votes."
"The 2 x 2 games, in particular the Prisoner's Dilemma, have been extensively
used in studies into reciprocal cooperation and, to a lesser extent, kin
selection. This paper examines the suitability of the 2 x 2 games for modelling
the evolution of cooperation through reciprocation and kin selection. This
examination is not restricted to the Prisoner's Dilemma, but includes the other
non-trivial symmetric 2 x 2 games. We show that the popularity of the
Prisoner's Dilemma for modelling social and biotic interaction is justified by
its superiority according to these criteria. Indeed, the Prisoner's Dilemma is
unique in providing the simplest support for reciprocal cooperation, and
additive kin-selected altruism. However, care is still required in choosing the
particular Prisoner's Dilemma payoff matrix to use. This paper reviews the
impact of non-linear payoffs for the application of Hamilton's rule to typical
altruistic interactions, and derives new results for cases in which the roles
of potential altruist and beneficiary are separated. In doing so we find the
same equilibrium condition holds in continuous games between relatives, and in
discrete games with roles."
"Peer-To-Peer (P2P) networks are self-organizing, distributed systems, with no
centralized authority or infrastructure. Because of the voluntary
participation, the availability of resources in a P2P system can be highly
variable and unpredictable. In this paper, we use ideas from Game Theory to
study the interaction of strategic and rational peers, and propose a
differential service-based incentive scheme to improve the system's
performance."
"A bidimensional representation of the space of 2x2 Symmetric Games in the
strategic representation is proposed. This representation provides a tool for
the classification of 2x2 symmetric games, quantification of the fraction of
them having a certain feature, and predictions of changes in the
characteristics of a game when a change in done on the payoff matrix that
defines it."
"This paper investigates the different effects of chaotic switching on
Parrondo's games, as compared to random and periodic switching. The rate of
winning of Parrondo's games with chaotic switching depends on coefficient(s)
defining the chaotic generator, initial conditions of the chaotic sequence and
the proportion of Game A played. Maximum rate of winning can be obtained with
all the above mentioned factors properly set, and this occurs when chaotic
switching approaches periodic behavior."
"We show, that a simple generalization of the Deferred Acceptance Procedure
with firms proposing due to Gale and Shapley(1962), yeild outcomes for a
two-sided contract choice problem, which necessarily belong to the core and are
Weakly Pareto Optimal for firms. Under additional assumptions: (a) given any
two distinct workers, the set of yields acheivable by a firm with the first
worker is disjoint from the set of yields acheivable by it with the second, and
(b) the contract choice problem is pair-wise efficient, we prove that there is
no stable outcome at which a firm can get more than what it gets at the unique
outcome of our procedure."
"This paper proves the existence of non-empty cores for directed network
problems with quotas and for those combinatorial allocation problems which
permit only exclusive allocations."
"In this note we consider a society that partitions itself into disjoint
jurisdictions, each choosing a location of its public project and a taxation
scheme to finance it. The set of public project is multi-dimensional, and their
costs could vary from jurisdiction to jurisdiction. We impose two principles,
egalitarianism, that requires the equalization of the total cost for all agents
in the same jurisdiction, and efficiency, that implies the minimization of the
aggregate total cost within jurisdiction. We show that these two principles
always yield a core-stable partition but a Nash stable partition may fail to
exist."
"On information security outsourcing market, an important reason that firms do
not want to let outside firms(usually called MSSPs-Managed Security Service
Providers) to take care of their security need is that they worry about service
quality MSSPs provide because they cannot monitor effort of the MSSPs. Since
MSSPs action is unobservable to buyers, MSSPs can lower cost by working less
hard than required in the contract and get higher profit. In the asymmetric
information literature, this possible secret shirking behavior is termed as
moral hazard problem. This paper considers a game theoretic economic framework
to show that under information asymmetry, an optimal contract can be designed
so that MSSPs will stick to their promised effort level. We also show that the
optimal contract should be performance-based, i.e., payment to MSSP should base
on performance of MSSP's security service period by period. For comparison, we
also showed that if the moral hazard problem does not exist, the optimal
contract does not depend on MSSP's performance. A contract that specifies
constant payment to MSSP will be optimal. Besides these, we show that for no
matter under perfect information scenario or imperfect information scenario,
the higher the transaction cost is, the lower payment to MSSPs will be."
"We consider a resource allocation problem where individual users wish to send
data across a network to maximize their utility, and a cost is incurred at each
link that depends on the total rate sent through the link. It is known that as
long as users do not anticipate the effect of their actions on prices, a simple
proportional pricing mechanism can maximize the sum of users' utilities minus
the cost (called aggregate surplus). Continuing previous efforts to quantify
the effects of selfish behavior in network pricing mechanisms, we consider the
possibility that users anticipate the effect of their actions on link prices.
Under the assumption that the links' marginal cost functions are convex, we
establish existence of a Nash equilibrium. We show that the aggregate surplus
at a Nash equilibrium is no worse than a factor of 4*sqrt{2} - 5 times the
optimal aggregate surplus; thus, the efficiency loss when users are selfish is
no more than approximately 34%."
"We consider the untyped lambda calculus with constructors and recursively
defined constants. We construct a domain-theoretic model such that any term not
denoting bottom is strongly normalising provided all its `stratified
approximations' are. From this we derive a general normalisation theorem for
applied typed lambda-calculi: If all constants have a total value, then all
typeable terms are strongly normalising. We apply this result to extensions of
G\""odel's system T and system F extended by various forms of bar recursion for
which strong normalisation was hitherto unknown."
"Two natural strategy elimination procedures have been studied for strategic
games. The first one involves the notion of (strict, weak, etc) dominance and
the second the notion of rationalizability. In the case of dominance the
criterion of order independence allowed us to clarify which notions and under
what circumstances are robust. In the case of rationalizability this criterion
has not been considered. In this paper we investigate the problem of order
independence for rationalizability by focusing on three naturally entailed
reduction relations on games. These reduction relations are distinguished by
the adopted reference point for the notion of a better response. Additionally,
they are parametrized by the adopted system of beliefs. We show that for one
reduction relation the outcome of its (possibly transfinite) iterations does
not depend on the order of elimination of the strategies. This result does not
hold for the other two reduction relations. However, under a natural assumption
the iterations of all three reduction relations yield the same outcome. The
obtained order independence results apply to the frameworks considered in
Bernheim 84 and Pearce 84. For finite games the iterations of all three
reduction relations coincide and the order independence holds for three natural
systems of beliefs considered in the literature."
"In this paper we present a novel generic mapping between Graphical Games and
Markov Random Fields so that pure Nash equilibria in the former can be found by
statistical inference on the latter. Thus, the problem of deciding whether a
graphical game has a pure Nash equilibrium, a well-known intractable problem,
can be attacked by well-established algorithms such as Belief Propagation,
Junction Trees, Markov Chain Monte Carlo and Simulated Annealing. Large classes
of graphical games become thus tractable, including all classes already known,
but also new classes such as the games with O(log n) treewidth."
"Mechanism design uses the tools of economics and game theory to design rules
of interaction for economic transactions that will,in principle, yield some de-
sired outcome. In the last few years this field has received much interest of
researchers in computer science, especially with the Internet developing as a
platform for communications and connections among enormous numbers of computers
and humans. Arguably the most positive result in mechanism de- sign is truthful
and there are only one general truthful mechanisms so far : the generalized
Vickrey-Clarke-Groves (VCG) mechanism. But VCG mecha- nism has one shortage:
The implementation of truthfulness is on the cost of decreasing the revenue of
the mechanism. (e.g., Ning Chen and Hong Zhu. [1999]). We introduce three new
characters of mechanism:partly truthful, criti- cal, consistent, and introduce
a new mechanism: X mechanism that satisfy the above three characters. Like VCG
mechanism, X mechanism also generalizes from Vickery Auction and is consistent
with Vickery auction in many ways, but the extended methods used in X mechanism
is different from that in VCG mechanism . This paper will demonstrate that X
mechanism better than VCG mechanism in optimizing utility of mechanism, which
is the original intention of mechanism design. So partly truthful,critical and
consistent are at least as important as truthful in mechanism design, and they
beyond truthful in many situations.As a result, we conclude that partly
truthful,critical and consistent are three new directions in mechanism design."
"In this work, we propose a game theoretic framework to analyze the behavior
of cognitive radios for distributed adaptive channel allocation. We define two
different objective functions for the spectrum sharing games, which capture the
utility of selfish users and cooperative users, respectively. Based on the
utility definition for cooperative users, we show that the channel allocation
problem can be formulated as a potential game, and thus converges to a
deterministic channel allocation Nash equilibrium point. Alternatively, a
no-regret learning implementation is proposed for both scenarios and it is
shown to have similar performance with the potential game when cooperation is
enforced, but with a higher variability across users. The no-regret learning
formulation is particularly useful to accommodate selfish users.
Non-cooperative learning games have the advantage of a very low overhead for
information exchange in the network.
  We show that cooperation based spectrum sharing etiquette improves the
overall network performance at the expense of an increased overhead required
for information exchange."
"We discuss bundle auctions within the framework of an integer allocation
problem. We show that for multi-unit auctions, of which bundle auctions are a
special case, market equilibrium and constrained market equilibrium are
equivalent concepts. This equivalence, allows us to obtain a computable
necessary and sufficient condition for the existence of constrained market
equilibrium for bundle auctions. We use this result to obtain a necessary and
sufficient condition for the existence of market equilibrium for multi-unit
auctions. After obtaining the induced bundle auction of a nonnegative TU game,
we show that the existence of market equilibrium implies the existence of a
possibly different market equilibrium as well, which corresponds very naturally
to an outcome in the matching core of the TU game. Consequently we show that
the matching core of the nonnegative TU game is non-empty if and only if the
induced market game has a market equilibrium."
"In {\em set-system auctions}, there are several overlapping teams of agents,
and a task that can be completed by any of these teams. The buyer's goal is to
hire a team and pay as little as possible. Recently, Karlin, Kempe and Tamir
introduced a new definition of {\em frugality ratio} for this setting.
Informally, the frugality ratio is the ratio of the total payment of a
mechanism to perceived fair cost. In this paper, we study this together with
alternative notions of fair cost, and how the resulting frugality ratios relate
to each other for various kinds of set systems.
  We propose a new truthful polynomial-time auction for the vertex cover
problem (where the feasible sets correspond to the vertex covers of a given
graph), based on the {\em local ratio} algorithm of Bar-Yehuda and Even. The
mechanism guarantees to find a winning set whose cost is at most twice the
optimal. In this situation, even though it is NP-hard to find a lowest-cost
feasible set, we show that {\em local optimality} of a solution can be used to
derive frugality bounds that are within a constant factor of best possible. To
prove this result, we use our alternative notions of frugality via a
bootstrapping technique, which may be of independent interest."
"We make three different types of contributions to cost-sharing: First, we
identify several new classes of combinatorial cost functions that admit
incentive-compatible mechanisms achieving both a constant-factor approximation
of budget-balance and a polylogarithmic approximation of the social cost
formulation of efficiency. Second, we prove a new, optimal lower bound on the
approximate efficiency of every budget-balanced Moulin mechanism for Steiner
tree or SSRoB cost functions. This lower bound exposes a latent approximation
hierarchy among different cost-sharing problems. Third, we show that weakening
the definition of incentive-compatibility to strategyproofness can permit
exponentially more efficient approximately budget-balanced mechanisms, in
particular for set cover cost-sharing problems."
"Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study ""prefix
position auctions"" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an ""envy-free"" or ""symmetric"" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions."
"The rationalizability concept was introduced in \cite{Ber84} and
  \cite{Pea84} to assess what can be inferred by rational players in a
non-cooperative game in the presence of common knowledge. However, this notion
can be defined in a number of ways that differ in seemingly unimportant minor
details. We shed light on these differences, explain their impact, and clarify
for which games these definitions coincide. Then we apply the same analysis to
explain the differences and similarities between various ways the iterated
elimination of strictly dominated strategies was defined in the literature.
This allows us to clarify the results of \cite{DS02} and \cite{CLL05} and
improve upon them. We also consider the extension of these results to strict
dominance by a mixed strategy. Our approach is based on a general study of the
operators on complete lattices. We allow transfinite iterations of the
considered operators and clarify the need for them. The advantage of such a
general approach is that a number of results, including order independence for
some of the notions of rationalizability and strict dominance, come for free."
"Ordinal automata are used to model physical systems with Zeno behavior. Using
automata and games techniques we solve a control problem formulated and left
open by Demri and Nowak in 2005. It involves partial observability and a new
synchronization between the controller and the environment."
"We submitted two kinds of strategies to the iterated prisoner's dilemma (IPD)
competitions organized by Graham Kendall, Paul Darwen and Xin Yao in 2004 and
2005. Our strategies performed exceedingly well in both years. One type is an
intelligent and optimistic enhanced version of the well known TitForTat
strategy which we named OmegaTitForTat. It recognizes common behaviour patterns
and detects and recovers from repairable mutual defect deadlock situations,
otherwise behaving much like TitForTat. The second type consists of a set of
strategies working together as a team. These group strategies have one
distinguished individual Godfather strategy that plays OmegaTitForTat against
non-members while heavily profiting from the behaviour of the other members of
his group, the Hitman. The Hitman willingly let themselves being abused by
their Godfather while themselves lowering the scores of all other players as
much as possible, thus further maximizing the performance of their Godfather in
relation to other participants. The study of collusion in the simplified
framework of the iterated prisoner's dilemma allows us to draw parallels to
many common aspects of reality both in Nature as well as Human Society, and
therefore further extends the scope of the iterated prisoner's dilemma as a
metaphor for the study of cooperative behaviour in a new and natural direction.
We further provide evidence that it will be unavoidable that such group
strategies will dominate all future iterated prisoner's dilemma competitions as
they can be stealthy camouflaged as non-group strategies with arbitrary
subtlety. Moreover, we show that the general problem of recognizing stealth
colluding strategies is undecidable in the theoretical sense."
"We consider the machine covering problem for selfish related machines. For a
constant number of machines, m, we show a monotone polynomial time
approximation scheme (PTAS) with running time that is linear in the number of
jobs. It uses a new technique for reducing the number of jobs while remaining
close to the optimal solution. We also present an FPTAS for the classical
machine covering problem (the previous best result was a PTAS) and use this to
give a monotone FPTAS.
  Additionally, we give a monotone approximation algorithm with approximation
ratio \min(m,(2+\eps)s_1/s_m) where \eps>0 can be chosen arbitrarily small and
s_i is the (real) speed of machine i. Finally we give improved results for two
machines.
  Our paper presents the first results for this problem in the context of
selfish machines."
"We give a new simple proof of the decidability of the First Order Theory of
(omega^omega^i,+) and the Monadic Second Order Theory of (omega^i,<), improving
the complexity in both cases. Our algorithm is based on tree automata and a new
representation of (sets of) ordinals by (infinite) trees."
"We present a number of models for the adword auctions used for pricing
advertising slots on search engines such as Google, Yahoo! etc. We begin with a
general problem formulation which allows the privately known valuation per
click to be a function of both the identity of the advertiser and the slot. We
present a compact characterization of the set of all deterministic incentive
compatible direct mechanisms for this model. This new characterization allows
us to conclude that there are incentive compatible mechanisms for this auction
with a multi-dimensional type-space that are {\em not} affine maximizers. Next,
we discuss two interesting special cases: slot independent valuation and slot
independent valuation up to a privately known slot and zero thereafter. For
both of these special cases, we characterize revenue maximizing and efficiency
maximizing mechanisms and show that these mechanisms can be computed with a
worst case computational complexity $O(n^2m^2)$ and $O(n^2m^3)$ respectively,
where $n$ is number of bidders and $m$ is number of slots. Next, we
characterize optimal rank based allocation rules and propose a new mechanism
that we call the customized rank based allocation. We report the results of a
numerical study that compare the revenue and efficiency of the proposed
mechanisms. The numerical results suggest that customized rank-based allocation
rule is significantly superior to the rank-based allocation rules."
"We propose a formulation of a general-sum bimatrix game as a bipartite
directed graph with the objective of establishing a correspondence between the
set of the relevant structures of the graph (in particular elementary cycles)
and the set of the Nash equilibria of the game. We show that finding the set of
elementary cycles of the graph permits the computation of the set of
equilibria. For games whose graphs have a sparse adjacency matrix, this serves
as a good heuristic for computing the set of equilibria. The heuristic also
allows the discarding of sections of the support space that do not yield any
equilibrium, thus serving as a useful pre-processing step for algorithms that
compute the equilibria through support enumeration."
"Motivated by applications in peer-to-peer and overlay networks we define and
study the \emph{Bounded Degree Network Formation} (BDNF) game. In an
$(n,k)$-BDNF game, we are given $n$ nodes, a bound $k$ on the out-degree of
each node, and a weight $w_{vu}$ for each ordered pair $(v,u)$ representing the
traffic rate from node $v$ to node $u$. Each node $v$ uses up to $k$ directed
links to connect to other nodes with an objective to minimize its average
distance, using weights $w_{vu}$, to all other destinations. We study the
existence of pure Nash equilibria for $(n,k)$-BDNF games. We show that if the
weights are arbitrary, then a pure Nash wiring may not exist. Furthermore, it
is NP-hard to determine whether a pure Nash wiring exists for a given
$(n,k)$-BDNF instance. A major focus of this paper is on uniform $(n,k)$-BDNF
games, in which all weights are 1. We describe how to construct a pure Nash
equilibrium wiring given any $n$ and $k$, and establish that in all pure Nash
wirings the cost of individual nodes cannot differ by more than a factor of
nearly 2, whereas the diameter cannot exceed $O(\sqrt{n \log_k n})$. We also
analyze best-response walks on the configuration space defined by the uniform
game, and show that starting from any initial configuration, strong
connectivity is reached within $\Theta(n^2)$ rounds. Convergence to a pure Nash
equilibrium, however, is not guaranteed. We present simulation results that
suggest that loop-free best-response walks always exist, but may not be
polynomially bounded. We also study a special family of \emph{regular} wirings,
the class of Abelian Cayley graphs, in which all nodes imitate the same wiring
pattern, and show that if $n$ is sufficiently large no such regular wiring can
be a pure Nash equilibrium."
"We propose an abstraction-based model checking method which relies on
refinement of an under-approximation of the feasible behaviors of the system
under analysis. The method preserves errors to safety properties, since all
analyzed behaviors are feasible by definition. The method does not require an
abstract transition relation to be generated, but instead executes the concrete
transitions while storing abstract versions of the concrete states, as
specified by a set of abstraction predicates. For each explored transition the
method checks, with the help of a theorem prover, whether there is any loss of
precision introduced by abstraction. The results of these checks are used to
decide termination or to refine the abstraction by generating new abstraction
predicates. If the (possibly infinite) concrete system under analysis has a
finite bisimulation quotient, then the method is guaranteed to eventually
explore an equivalent finite bisimilar structure. We illustrate the application
of the approach for checking concurrent programs."
"In 1983, Aldous proved that randomization can speedup local search. For
example, it reduces the query complexity of local search over [1:n]^d from
Theta (n^{d-1}) to O (d^{1/2}n^{d/2}). It remains open whether randomization
helps fixed-point computation. Inspired by this open problem and recent
advances on equilibrium computation, we have been fascinated by the following
question:
  Is a fixed-point or an equilibrium fundamentally harder to find than a local
optimum? In this paper, we give a nearly-tight bound of Omega(n)^{d-1} on the
randomized query complexity for computing a fixed point of a discrete Brouwer
function over [1:n]^d. Since the randomized query complexity of global
optimization over [1:n]^d is Theta (n^{d}), the randomized query model over
[1:n]^d strictly separates these three important search problems: Global
optimization is harder than fixed-point computation, and fixed-point
computation is harder than local search. Our result indeed demonstrates that
randomization does not help much in fixed-point computation in the query model;
the deterministic complexity of this problem is Theta (n^{d-1})."
"We prove that the single-player game clobber is solvable in linear time when
played on a line or on a cycle. For this purpose, we show that this game is
equivalent to an optimization problem on a set of words defined by seven
classes of forbidden patterns. We also prove that, playing on the cycle, it is
always possible to remove at least 2n/3 pawns, and we give a conformation for
which it is not possible to do better, answering questions recently asked by
Faria et al."
"There has been a remarkable increase in work at the interface of computer
science and game theory in the past decade. In this article I survey some of
the main themes of work in the area, with a focus on the work in computer
science. Given the length constraints, I make no attempt at being
comprehensive, especially since other surveys are also available, and a
comprehensive survey book will appear shortly."
"We prove an n-EXPTIME lower bound for the problem of deciding the winner in a
reachability game on Higher Order Pushdown Automata (HPDA) of level n. This
bound matches the known upper bound for parity games on HPDA. As a consequence
the mu-calculus model checking over graphs given by n-HPDA is n-EXPTIME
complete."
"In the customary VCG (Vickrey-Clarke-Groves) mechanism truth-telling is a
dominant strategy. In this paper we study the sequential VCG mechanism and show
that other dominant strategies may then exist. We illustrate how this fact can
be used to minimize taxes using examples concerned with Clarke tax and public
projects."
"A model of providing service in a P2P network is analyzed. It is shown that
by adding a scrip system, a mechanism that admits a reasonable Nash equilibrium
that reduces free riding can be obtained. The effect of varying the total
amount of money (scrip) in the system on efficiency (i.e., social welfare) is
analyzed, and it is shown that by maintaining the appropriate ratio between the
total amount of money and the number of agents, efficiency is maximized. The
work has implications for many online systems, not only P2P networks but also a
wide variety of online forums for which scrip systems are popular, but formal
analyses have been lacking."
"We discuss the design of efficient scrip systems and develop tools for
empirically analyzing them. For those interested in the empirical study of
scrip systems, we demonstrate how characteristics of agents in a system can be
inferred from the equilibrium distribution of money. From the perspective of a
system designer, we examine the effect of the money supply on social welfare
and show that social welfare is maximized by increasing the money supply up to
the point that the system experiences a ``monetary crash,'' where money is
sufficiently devalued that no agent is willing to perform a service. We also
examine the implications of the presence of altruists and hoarders on the
performance of the system. While a small number of altruists may improve social
welfare, too many can also cause the system to experience a monetary crash,
which may be bad for social welfare. Hoarders generally decrease social welfare
but, surprisingly, they also promote system stability by helping prevent
monetary crashes. In addition, we provide new technical tools for analyzing and
computing equilibria by showing that our model exhibits strategic
complementarities, which implies that there exist equilibria in pure strategies
that can be computed efficiently."
"We compare here the relative strength of four widely used procedures on
finite strategic games: iterated elimination of weakly/strictly dominated
strategies by a pure/mixed strategy. A complication is that none of these
procedures is based on a monotonic operator. To deal with this problem we use
'global' versions of these operators."
"In the context of applied game theory in networking environments, a number of
concepts have been proposed to measure both efficiency and optimality of
resource allocations, the most famous certainly being the price of anarchy and
the Jain index. Yet, very few have tried to question these measures and compare
them one to another, in a general framework, which is the aim of the present
article."
"We present a deterministic exploration mechanism for sponsored search
auctions, which enables the auctioneer to learn the relevance scores of
advertisers, and allows advertisers to estimate the true value of clicks
generated at the auction site. This exploratory mechanism deviates only
minimally from the mechanism being currently used by Google and Yahoo! in the
sense that it retains the same pricing rule, similar ranking scheme, as well
as, similar mathematical structure of payoffs. In particular, the estimations
of the relevance scores and true-values are achieved by providing a chance to
lower ranked advertisers to obtain better slots. This allows the search engine
to potentially test a new pool of advertisers, and correspondingly, enables new
advertisers to estimate the value of clicks/leads generated via the auction.
Both these quantities are unknown a priori, and their knowledge is necessary
for the auction to operate efficiently. We show that such an exploration policy
can be incorporated without any significant loss in revenue for the auctioneer.
We compare the revenue of the new mechanism to that of the standard mechanism
at their corresponding symmetric Nash equilibria and compute the cost of
uncertainty, which is defined as the relative loss in expected revenue per
impression. We also bound the loss in efficiency, as well as, in user
experience due to exploration, under the same solution concept (i.e. SNE). Thus
the proposed exploration mechanism learns the relevance scores while
incorporating the incentive constraints from the advertisers who are selfish
and are trying to maximize their own profits, and therefore, the exploration is
essentially achieved via mechanism design. We also discuss variations of the
new mechanism such as truthful implementations."
"A mediator is a well-known construct in game theory, and is an entity that
plays on behalf of some of the agents who choose to use its services, while the
rest of the agents participate in the game directly. We initiate a game
theoretic study of sponsored search auctions, such as those used by Google and
Yahoo!, involving {\em incentive driven} mediators. We refer to such mediators
as {\em for-profit} mediators, so as to distinguish them from mediators
introduced in prior work, who have no monetary incentives, and are driven by
the altruistic goal of implementing certain desired outcomes. We show that in
our model, (i) players/advertisers can improve their payoffs by choosing to use
the services of the mediator, compared to directly participating in the
auction; (ii) the mediator can obtain monetary benefit by managing the
advertising burden of its group of advertisers; and (iii) the payoffs of the
mediator and the advertisers it plays for are compatible with the incentive
constraints from the advertisers who do dot use its services. A simple
intuition behind the above result comes from the observation that the mediator
has more information about and more control over the bid profile than any
individual advertiser, allowing her to reduce the payments made to the
auctioneer, while still maintaining incentive constraints. Further, our results
indicate that there are significant opportunities for diversification in the
internet economy and we should expect it to continue to develop richer
structure, with room for different types of agents to coexist."
"Most work in game theory assumes that players are perfect reasoners and have
common knowledge of all significant aspects of the game. In earlier work, we
proposed a framework for representing and analyzing games with possibly unaware
players, and suggested a generalization of Nash equilibrium appropriate for
games with unaware players that we called generalized Nash equilibrium. Here,
we use this framework to analyze other solution concepts that have been
considered in the game-theory literature, with a focus on sequential
equilibrium. We also provide some insight into the notion of generalized Nash
equilibrium by proving that it is closely related to the notion of
rationalizability when we restrict the analysis to games in normal form and no
unawareness is involved."
"We describe a new system for the simulation of simultaneous moves between
noncolocational players. This has applications in the burgeoning
Rock-Paper-Scissors by mail movement."
"We provide an epistemic analysis of arbitrary strategic games based on
possibility correspondences. We first establish a generic result that links
true common beliefs (and, respectively, common knowledge) of players'
rationality defined by means of `monotonic' properties, with the iterated
elimination of strategies that do not satisfy these properties. It allows us to
deduce the customary results concerned with true common beliefs of rationality
and iterated elimination of strictly dominated strategies as simple
corollaries. This approach relies on Tarski's Fixpoint Theorem. We also provide
an axiomatic presentation of this generic result. This allows us to clarify the
proof-theoretic principles assumed in players' reasoning. Finally, we provide
an alternative characterization of the iterated elimination of strategies based
on the concept of a public announcement. It applies to `global properties'.
Both classes of properties include the notions of rationalizability and the
iterated elimination of strictly dominated strategies."
"We investigate market forces that would lead to the emergence of new classes
of players in the sponsored search market. We report a 3-fold diversification
triggered by two inherent features of the sponsored search market, namely,
capacity constraints and collusion-vulnerability of current mechanisms. In the
first scenario, we present a comparative study of two models motivated by
capacity constraints - one where the additional capacity is provided by
for-profit agents, who compete for slots in the original auction, draw traffic,
and run their own sub-auctions, and the other, where the additional capacity is
provided by the auctioneer herself, by essentially acting as a mediator and
running a single combined auction. This study was initiated by us in
\cite{SRGR07}, where the mediator-based model was studied. In the present work,
we study the auctioneer-based model and show that this model seems inferior to
the mediator-based model in terms of revenue or efficiency guarantee due to
added capacity. In the second scenario, we initiate a game theoretic study of
current sponsored search auctions, involving incentive driven mediators who
exploit the fact that these mechanisms are not collusion-resistant. In
particular, we show that advertisers can improve their payoffs by using the
services of the mediator compared to directly participating in the auction, and
that the mediator can also obtain monetary benefit, without violating incentive
constraints from the advertisers who do not use its services. We also point out
that the auctioneer can not do very much via mechanism design to avoid such
for-profit mediation without losing badly in terms of revenue, and therefore,
the mediators are likely to prevail."
"We study a class of games in which a finite number of agents each controls a
quantity of flow to be routed through a network, and are able to split their
own flow between multiple paths through the network. Recent work on this model
has contrasted the social cost of Nash equilibria with the best possible social
cost. Here we show that additional costs are incurred in situations where a
selfish ``leader'' agent allocates his flow, and then commits to that choice so
that other agents are compelled to minimise their own cost based on the first
agent's choice. We find that even in simple networks, the leader can often
improve his own cost at the expense of increased social cost. Focusing on the
2-player case, we give upper and lower bounds on the worst-case additional cost
incurred."
"The auction theory literature has so far focused mostly on the design of
mechanisms that takes the revenue or the efficiency as a yardstick. However,
scenarios where the {\it capacity}, which we define as \textit{``the number of
bidders the auctioneer wants to have a positive probability of getting the
item''}, is a fundamental concern are ubiquitous in the information economy.
For instance, in sponsored search auctions (SSA's) or in online ad-exchanges,
the true value of an ad-slot for an advertiser is inherently derived from the
conversion-rate, which in turn depends on whether the advertiser actually
obtained the ad-slot or not; thus, unless the capacity of the underlying
auction is large, key parameters, such as true valuations and
advertiser-specific conversion rates, will remain unknown or uncertain leading
to inherent inefficiencies in the system. In general, the same holds true for
all information goods/digital goods. We initiate a study of mechanisms, which
take capacity as a yardstick, in addition to revenue/efficiency. We show that
in the case of a single indivisible item one simple way to incorporate capacity
constraints is via designing mechanisms to sell probability distributions, and
that under certain conditions, such optimal probability distributions could be
identified using a Linear programming approach. We define a quantity called
{\it price of capacity} to capture the tradeoff between capacity and
revenue/efficiency. We also study the case of sponsored search auctions.
Finally, we discuss how general such an approach via probability spikes can be
made, and potential directions for future investigations."
"We consider bargaining problems which involve two participants, with a
nonempty closed, bounded convex bargaining set of points in the real plane
representing all realizable bargains. We also assume that there is no definite
threat or disagreement point which will provide the default bargain if the
players cannot agree on some point in the bargaining set. However, there is a
nondeterministic threat: if the players fail to agree on a bargain, one of them
will be chosen at random with equal probability, and that chosen player will
select any realizable bargain as the solution, subject to a reasonable
restriction."
"We analyze the computational complexity of market maker pricing algorithms
for combinatorial prediction markets. We focus on Hanson's popular logarithmic
market scoring rule market maker (LMSR). Our goal is to implicitly maintain
correct LMSR prices across an exponentially large outcome space. We examine
both permutation combinatorics, where outcomes are permutations of objects, and
Boolean combinatorics, where outcomes are combinations of binary events. We
look at three restrictive languages that limit what traders can bet on. Even
with severely limited languages, we find that LMSR pricing is $\SP$-hard, even
when the same language admits polynomial-time matching without the market
maker. We then propose an approximation technique for pricing permutation
markets based on a recent algorithm for online permutation learning. The
connections we draw between LMSR pricing and the vast literature on online
learning with expert advice may be of independent interest."
"In sponsored search, a number of advertising slots is available on a search
results page, and have to be allocated among a set of advertisers competing to
display an ad on the page. This gives rise to a bipartite matching market that
is typically cleared by the way of an automated auction. Several auction
mechanisms have been proposed, with variants of the Generalized Second Price
(GSP) being widely used in practice.
  A rich body of work on bipartite matching markets builds upon the stable
marriage model of Gale and Shapley and the assignment model of Shapley and
Shubik. We apply insights from this line of research into the structure of
stable outcomes and their incentive properties to advertising auctions.
  We model advertising auctions in terms of an assignment model with linear
utilities, extended with bidder and item specific maximum and minimum prices.
Auction mechanisms like the commonly used GSP or the well-known
Vickrey-Clarke-Groves (VCG) are interpreted as simply computing a
\emph{bidder-optimal stable matching} in this model, for a suitably defined set
of bidder preferences. In our model, the existence of a stable matching is
guaranteed, and under a non-degeneracy assumption a bidder-optimal stable
matching exists as well. We give an algorithm to find such matching in
polynomial time, and use it to design truthful mechanism that generalizes GSP,
is truthful for profit-maximizing bidders, implements features like
bidder-specific minimum prices and position-specific bids, and works for rich
mixtures of bidders and preferences."
"This paper studies the effects of introducing altruistic agents into atomic
congestion games. Altruistic behavior is modeled by a trade-off between selfish
and social objectives. In particular, we assume agents optimize a linear
combination of personal delay of a strategy and the resulting increase in
social cost. Our model can be embedded in the framework of congestion games
with player-specific latency functions. Stable states are the Nash equilibria
of these games, and we examine their existence and the convergence of
sequential best-response dynamics. Previous work shows that for symmetric
singleton games with convex delays Nash equilibria are guaranteed to exist. For
concave delay functions we observe that there are games without Nash equilibria
and provide a polynomial time algorithm to decide existence for symmetric
singleton games with arbitrary delay functions. Our algorithm can be extended
to compute best and worst Nash equilibria if they exist. For more general
congestion games existence becomes NP-hard to decide, even for symmetric
network games with quadratic delay functions. Perhaps surprisingly, if all
delay functions are linear, then there is always a Nash equilibrium in any
congestion game with altruists and any better-response dynamics converges. In
addition to these results for uncoordinated dynamics, we consider a scenario in
which a central altruistic institution can motivate agents to act
altruistically. We provide constructive and hardness results for finding the
minimum number of altruists to stabilize an optimal congestion profile and more
general mechanisms to incentivize agents to adopt favorable behavior."
"We study the mechanism design problem of scheduling unrelated machines and we
completely characterize the decisive truthful mechanisms for two players when
the domain contains both positive and negative values. We show that the class
of truthful mechanisms is very limited: A decisive truthful mechanism
partitions the tasks into groups so that the tasks in each group are allocated
independently of the other groups. Tasks in a group of size at least two are
allocated by an affine minimizer and tasks in singleton groups by a
task-independent mechanism. This characterization is about all truthful
mechanisms, including those with unbounded approximation ratio.
  A direct consequence of this approach is that the approximation ratio of
mechanisms for two players is 2, even for two tasks. In fact, it follows that
for two players, VCG is the unique algorithm with optimal approximation 2.
  This characterization provides some support that any decisive truthful
mechanism (for 3 or more players) partitions the tasks into groups some of
which are allocated by affine minimizers, while the rest are allocated by a
threshold mechanism (in which a task is allocated to a player when it is below
a threshold value which depends only on the values of the other players). We
also show here that the class of threshold mechanisms is identical to the class
of additive mechanisms."
"Coalitional voting games appear in different forms in multi-agent systems,
social choice and threshold logic. In this paper, the complexity of comparison
of influence between players in coalitional voting games is characterized. The
possible representations of simple games considered are simple games
represented by winning coalitions, minimal winning coalitions, weighted voting
game or a multiple weighted voting game. The influence of players is gauged
from the viewpoint of basic player types, desirability relations and classical
power indices such as Shapley-Shubik index, Banzhaf index, Holler index,
Deegan-Packel index and Chow parameters. Among other results, it is shown that
for a simple game represented by minimal winning coalitions, although it is
easy to verify whether a player has zero or one voting power, computing the
Banzhaf value of the player is #P-complete. Moreover, it is proved that
multiple weighted voting games are the only representations for which it is
NP-hard to verify whether the game is linear or not. For a simple game with a
set W^m of minimal winning coalitions and n players, a O(n.|W^m|+(n^2)log(n))
algorithm is presented which returns `no' if the game is non-linear and returns
the strict desirability ordering otherwise. The complexity of transforming
simple games into compact representations is also examined."
"Recent mobile equipment (as well as the norm IEEE 802.21) now offers the
possibility for users to switch from one technology to another (vertical
handover). This allows flexibility in resource assignments and, consequently,
increases the potential throughput allocated to each user. In this paper, we
design a fully distributed algorithm based on trial and error mechanisms that
exploits the benefits of vertical handover by finding fair and efficient
assignment schemes. On the one hand, mobiles gradually update the fraction of
data packets they send to each network based on the rewards they receive from
the stations. On the other hand, network stations send rewards to each mobile
that represent the impact each mobile has on the cell throughput. This reward
function is closely related to the concept of marginal cost in the pricing
literature. Both the station and the mobile algorithms are simple enough to be
implemented in current standard equipment. Based on tools from evolutionary
games, potential games and replicator dynamics, we analytically show the
convergence of the algorithm to solutions that are efficient and fair in terms
of throughput. Moreover, we show that after convergence, each user is connected
to a single network cell which avoids costly repeated vertical handovers.
Several simple heuristics based on this algorithm are proposed to achieve fast
convergence. Indeed, for implementation purposes, the number of iterations
should remain in the order of a few tens. We also compare, for different loads,
the quality of their solutions."
"Simulation and bisimulation metrics for stochastic systems provide a
quantitative generalization of the classical simulation and bisimulation
relations. These metrics capture the similarity of states with respect to
quantitative specifications written in the quantitative {\mu}-calculus and
related probabilistic logics. We first show that the metrics provide a bound
for the difference in long-run average and discounted average behavior across
states, indicating that the metrics can be used both in system verification,
and in performance evaluation. For turn-based games and MDPs, we provide a
polynomial-time algorithm for the computation of the one-step metric distance
between states. The algorithm is based on linear programming; it improves on
the previous known exponential-time algorithm based on a reduction to the
theory of reals. We then present PSPACE algorithms for both the decision
problem and the problem of approximating the metric distance between two
states, matching the best known algorithms for Markov chains. For the
bisimulation kernel of the metric our algorithm works in time O(n^4) for both
turn-based games and MDPs; improving the previously best known O(n^9\cdot
log(n)) time algorithm for MDPs. For a concurrent game G, we show that
computing the exact distance between states is at least as hard as computing
the value of concurrent reachability games and the square-root-sum problem in
computational geometry. We show that checking whether the metric distance is
bounded by a rational r, can be done via a reduction to the theory of real
closed fields, involving a formula with three quantifier alternations, yielding
O(|G|^O(|G|^5)) time complexity, improving the previously known reduction,
which yielded O(|G|^O(|G|^7)) time complexity. These algorithms can be iterated
to approximate the metrics using binary search."
"We study social cost losses in Facility Location games, where $n$ selfish
agents install facilities over a network and connect to them, so as to forward
their local demand (expressed by a non-negative weight per agent). Agents using
the same facility share fairly its installation cost, but every agent pays
individually a (weighted) connection cost to the chosen location. We study the
Price of Stability (PoS) of pure Nash equilibria and the Price of Anarchy of
strong equilibria (SPoA), that generalize pure equilibria by being resilient to
coalitional deviations. A special case of recently studied network design
games, Facility Location merits separate study as a classic model with numerous
applications and individual characteristics: our analysis for unweighted agents
on metric networks reveals constant upper and lower bounds for the PoS, while
an $O(\ln n)$ upper bound implied by previous work is tight for non-metric
networks. Strong equilibria do not always exist, even for the unweighted metric
case. We show that $e$-approximate strong equilibria exist ($e=2.718...$). The
SPoA is generally upper bounded by $O(\ln W)$ ($W$ is the sum of agents'
weights), which becomes tight $\Theta(\ln n)$ for unweighted agents. For the
unweighted metric case we prove a constant upper bound. We point out several
challenging open questions that arise."
"It is well-known that for several natural decision problems no budget
balanced Groves mechanisms exist. This has motivated recent research on
designing variants of feasible Groves mechanisms (termed as `redistribution of
VCG (Vickrey-Clarke-Groves) payments') that generate reduced deficit. With this
in mind, we study sequential mechanisms and consider optimal strategies that
could reduce the deficit resulting under the simultaneous mechanism. We show
that such strategies exist for the sequential pivotal mechanism of the
well-known public project problem. We also exhibit an optimal strategy with the
property that a maximal social welfare is generated when each player follows
it. Finally, we show that these strategies can be achieved by an implementation
in Nash equilibrium."
"Given a network with a continuum of users at some origins, suppose that the
users wish to reach specific destinations, but that they are not indifferent to
the time needed to reach their destination. They may have several possibilities
(of routes or deparure time), but their choices modify the travel times on the
network. Hence, each user faces the following problem: given a pattern of
travel times for the different possible routes that reach the destination, find
a shortest path. The situation in a context of perfect information is a
so-called Nash equilibrium, and the question whether there is such an
equilibrium and of finding it if it exists is the so-called equilibrium
assignment problem. It arises for various kind of networks, such as computers,
communication or transportation network. When each user occupies permanently
the whole route from the origin to its destination, we call it the static
assignment problem, which has been extensively studied with pioneers works by
Wardrop or Beckmann. A less studied, but more realistic, and maybe more
difficult, problem is when the time needed to reach an arc is taken into
account. We speak then of a dynamic assignment problem. Several models have
been proposed. For some of them, the existence of an equilibrium has been
proved, but always under some technical assumptions or in a very special case
(a network with one arc for the case when the users may chose their departure
time). The present paper proposes a compact model, with minimal and natural
assumptions. For this model, we prove that there is always an equilibrium. To
our knowledge, this imply all previous results about existence of an
equilibrium for the dynamic assignment problem."
"A common objective in mechanism design is to choose the outcome (for example,
allocation of resources) that maximizes the sum of the agents' valuations,
without introducing incentives for agents to misreport their preferences. The
class of Groves mechanisms achieves this; however, these mechanisms require the
agents to make payments, thereby reducing the agents' total welfare.
  In this paper we introduce a measure for comparing two mechanisms with
respect to the final welfare they generate. This measure induces a partial
order on mechanisms and we study the question of finding minimal elements with
respect to this partial order. In particular, we say a non-deficit Groves
mechanism is welfare undominated if there exists no other non-deficit Groves
mechanism that always has a smaller or equal sum of payments. We focus on two
domains: (i) auctions with multiple identical units and unit-demand bidders,
and (ii) mechanisms for public project problems. In the first domain we
analytically characterize all welfare undominated Groves mechanisms that are
anonymous and have linear payment functions, by showing that the family of
optimal-in-expectation linear redistribution mechanisms, which were introduced
in [6] and include the Bailey-Cavallo mechanism [1,2], coincides with the
family of welfare undominated Groves mechanisms that are anonymous and linear
in the setting we study. In the second domain we show that the classic VCG
(Clarke) mechanism is welfare undominated for the class of public project
problems with equal participation costs, but is not undominated for a more
general class."
"For some well-known games, such as the Traveler's Dilemma or the Centipede
Game, traditional game-theoretic solution concepts--and most notably Nash
equilibrium--predict outcomes that are not consistent with empirical
observations. In this paper, we introduce a new solution concept, iterated
regret minimization, which exhibits the same qualitative behavior as that
observed in experiments in many games of interest, including Traveler's
Dilemma, the Centipede Game, Nash bargaining, and Bertrand competition. As the
name suggests, iterated regret minimization involves the iterated deletion of
strategies that do not minimize regret."
"We are interested in mechanisms that maximize social welfare. In [1] this
problem was studied for multi-unit auctions with unit demand bidders and for
the public project problem, and in each case social welfare undominated
mechanisms in the class of feasible and incentive compatible mechanisms were
identified. One way to improve upon these optimality results is by allowing the
players to move sequentially. With this in mind, we study here sequential
versions of two feasible Groves mechanisms used for single item auctions: the
Vickrey auction and the Bailey-Cavallo mechanism. Because of the absence of
dominant strategies in this sequential setting, we focus on a weaker concept of
an optimal strategy. For each mechanism we introduce natural optimal strategies
and observe that in each mechanism these strategies exhibit different
behaviour. However, we then show that among all optimal strategies, the one we
introduce for each mechanism maximizes the social welfare when each player
follows it. The resulting social welfare can be larger than the one obtained in
the simultaneous setting. Finally, we show that, when interpreting both
mechanisms as simultaneous ones, the vectors of the proposed strategies form a
Pareto optimal Nash equilibrium in the class of optimal strategies."
"We introduce the concept of Conversion/Preference Games, or CP games for
short. CP games generalize the standard notion of strategic games. First we
exemplify the use of CP games. Second we formally introduce and define the
CP-games formalism. Then we sketch two `real-life' applications, namely a
connection between CP games and gene regulation networks, and the use of CP
games to formalize implied information in Chinese Wall security. We end with a
study of a particular fixed-point construction over CP games and of the
resulting existence of equilibria in possibly infinite games."
"We consider games played on finite graphs, whose goal is to obtain a trace
belonging to a given set of winning traces. We focus on those states from which
Player 1 cannot force a win. We explore and compare several criteria for
establishing what is the preferable behavior of Player 1 from those states.
  Along the way, we prove several results of theoretical and practical
interest, such as a characterization of admissible strategies, which also
provides a simple algorithm for computing such strategies for various common
goals, and the equivalence between the existence of positional winning
strategies and the existence of positional subgame perfect strategies."
"We prove two determinacy and decidability results about two-players
stochastic reachability games with partial observation on both sides and
finitely many states, signals and actions."
"We prove that optimal strategies exist in every perfect-information
stochastic game with finitely many states and actions and a tail winning
condition."
"This paper presents a new lower bound for the discrete strategy improvement
algorithm for solving parity games due to Voege and Jurdziski. First, we
informally show which structures are difficult to solve for the algorithm.
Second, we outline a family of games of quadratic size on which the algorithm
requires exponentially many strategy iterations, answering in the negative the
long-standing question whether this algorithm runs in polynomial time.
Additionally we note that the same family of games can be used to prove a
similar result w.r.t. the strategy improvement variant by Schewe."
"The current art in optimal combinatorial auctions is limited to handling the
case of single units of multiple items, with each bidder bidding on exactly one
bundle (single minded bidders). This paper extends the current art by proposing
an optimal auction for procuring multiple units of multiple items when the
bidders are single minded. The auction minimizes the cost of procurement while
satisfying Bayesian incentive compatibility and interim individual rationality.
Under appropriate regularity conditions, this optimal auction also satisfies
dominant strategy incentive compatibility."
"Cake-cutting protocols aim at dividing a ``cake'' (i.e., a divisible
resource) and assigning the resulting portions to several players in a way that
each of the players feels to have received a ``fair'' amount of the cake. An
important notion of fairness is envy-freeness: No player wishes to switch the
portion of the cake received with another player's portion. Despite intense
efforts in the past, it is still an open question whether there is a
\emph{finite bounded} envy-free cake-cutting protocol for an arbitrary number
of players, and even for four players. We introduce the notion of degree of
guaranteed envy-freeness (DGEF) as a measure of how good a cake-cutting
protocol can approximate the ideal of envy-freeness while keeping the protocol
finite bounded (trading being disregarded). We propose a new finite bounded
proportional protocol for any number n \geq 3 of players, and show that this
protocol has a DGEF of 1 + \lceil (n^2)/2 \rceil. This is the currently best
DGEF among known finite bounded cake-cutting protocols for an arbitrary number
of players. We will make the case that improving the DGEF even further is a
tough challenge, and determine, for comparison, the DGEF of selected known
finite bounded cake-cutting protocols."
"In general, the games are played on a host graph, where each node is a
selfish independent agent (player) and each edge has a fixed link creation cost
\alpha. Together the agents create a network (a subgraph of the host graph)
while selfishly minimizing the link creation costs plus the sum of the
distances to all other players (usage cost). In this paper, we pursue two
important facets of the network creation game. First, we study extensively a
natural version of the game, called the cooperative model, where nodes can
collaborate and share the cost of creating any edge in the host graph. We prove
the first nontrivial bounds in this model, establishing that the price of
anarchy is polylogarithmic in n for all values of &#945; in complete host
graphs. This bound is the first result of this type for any version of the
network creation game; most previous general upper bounds are polynomial in n.
Interestingly, we also show that equilibrium graphs have polylogarithmic
diameter for the most natural range of \alpha (at most n polylg n). Second, we
study the impact of the natural assumption that the host graph is a general
graph, not necessarily complete. This model is a simple example of nonuniform
creation costs among the edges (effectively allowing weights of \alpha and
\infty). We prove the first assemblage of upper and lower bounds for this
context, stablishing nontrivial tight bounds for many ranges of \alpha, for
both the unilateral and cooperative versions of network creation. In
particular, we establish polynomial lower bounds for both versions and many
ranges of \alpha, even for this simple nonuniform cost model, which sharply
contrasts the conjectured constant bounds for these games in complete (uniform)
graphs."
"We consider auctions in which greedy algorithms, paired with first-price or
critical-price payment rules, are used to resolve multi-parameter combinatorial
allocation problems. We study the price of anarchy for social welfare in such
auctions. We show for a variety of equilibrium concepts, including Bayes-Nash
equilibrium and correlated equilibrium, the resulting price of anarchy bound is
close to the approximation factor of the underlying greedy algorithm."
"We study envy-free mechanisms for scheduling tasks on unrelated machines
(agents) that approximately minimize the makespan. For indivisible tasks, we
put forward an envy-free poly-time mechanism that approximates the minimal
makespan to within a factor of $O(\log m)$, where $m$ is the number of
machines. We also show a lower bound of $\Omega(\log m / \log\log m)$. This
improves the recent result of Hartline {\sl et al.} \cite{Ahuva:2008} who give
an upper bound of $(m+1)/2$, and a lower bound of $2-1/m$. For divisible tasks,
we show that there always exists an envy-free poly-time mechanism with optimal
makespan."
"We study the envy free pricing problem faced by a seller who wishes to
maximize revenue by setting prices for bundles of items. If there is an
unlimited supply of items and agents are single minded then we show that
finding the revenue maximizing envy free allocation/pricing can be solved in
polynomial time by reducing it to an instance of weighted independent set on a
perfect graph.
  We define an allocation/pricing as \textit{multi envy free} if no agent
wishes to replace her allocation with the union of the allocations of some set
of other agents and her price with the sum of their prices. We show that it is
\textit{coNP}-hard to decide if a given allocation/pricing is multi envy free.
We also show that revenue maximization multi envy free allocation/pricing is
\textit{APX} hard.
  Furthermore, we give efficient algorithms and hardness results for various
variants of the highway problem."
"The principal problem in algorithmic mechanism design is in merging the
incentive constraints imposed by selfish behavior with the algorithmic
constraints imposed by computational intractability. This field is motivated by
the observation that the preeminent approach for designing incentive compatible
mechanisms, namely that of Vickrey, Clarke, and Groves; and the central
approach for circumventing computational obstacles, that of approximation
algorithms, are fundamentally incompatible: natural applications of the VCG
approach to an approximation algorithm fails to yield an incentive compatible
mechanism. We consider relaxing the desideratum of (ex post) incentive
compatibility (IC) to Bayesian incentive compatibility (BIC), where
truthtelling is a Bayes-Nash equilibrium (the standard notion of incentive
compatibility in economics). For welfare maximization in single-parameter agent
settings, we give a general black-box reduction that turns any approximation
algorithm into a Bayesian incentive compatible mechanism with essentially the
same approximation factor."
"Inspired by Internet ad auction applications, we study the problem of
allocating a single item via an auction when bidders place very different
values on the item. We formulate this as the problem of prior-free auction and
focus on designing a simple mechanism that always allocates the item. Rather
than designing sophisticated pricing methods like prior literature, we design
better allocation methods. In particular, we propose quasi-proportional
allocation methods in which the probability that an item is allocated to a
bidder depends (quasi-proportionally) on the bids.
  We prove that corresponding games for both all-pay and winners-pay
quasi-proportional mechanisms admit pure Nash equilibria and this equilibrium
is unique. We also give an algorithm to compute this equilibrium in polynomial
time. Further, we show that the revenue of the auctioneer is promisingly high
compared to the ultimate, i.e., the highest value of any of the bidders, and
show bounds on the revenue of equilibria both analytically, as well as using
experiments for specific quasi-proportional functions. This is the first known
revenue analysis for these natural mechanisms (including the special case of
proportional mechanism which is common in network resource allocation
problems)."
"We study the design of mechanisms in combinatorial auction domains. We focus
on settings where the auction is repeated, motivated by auctions for licenses
or advertising space. We consider models of agent behaviour in which they
either apply common learning techniques to minimize the regret of their bidding
strategies, or apply short-sighted best-response strategies. We ask: when can a
black-box approximation algorithm for the base auction problem be converted
into a mechanism that approximately preserves the original algorithm's
approximation factor on average over many iterations? We present a general
reduction for a broad class of algorithms when agents minimize external regret.
We also present a new mechanism for the combinatorial auction problem that
attains an $O(\sqrt{m})$ approximation on average when agents apply
best-response dynamics."
"The basic optimal transportation problem consists in finding the most
effective way of moving masses from one location to another, while minimizing
the transportation cost. Such concept has been found to be useful to understand
various mathematical, economical, and control theory phenomena, such as
Witsenhausen's counterexam-ple in stochastic control theory, principal-agent
problem in microeco- nomic theory, location and planning problems, etc. In this
work, we focus on mobile association problems: the determina-tion of the cells
corresponding to each base station, i.e., the locations at which intelligent
mobile terminals prefer to connect to a given base station rather than to
others. This work combines game theory and optimal transport theory to
characterize the solution based on fluid approximations. We characterize the
optimal solution from both the global network and the mobile user points of
view."
"If you recommend a product to me and I buy it, how much should you be paid by
the seller? And if your sole interest is to maximize the amount paid to you by
the seller for a sequence of recommendations, how should you recommend
optimally if I become more inclined to ignore you with each irrelevant
recommendation you make? Finding an answer to these questions is a key
challenge in all forms of marketing that rely on and explore social ties;
ranging from personal recommendations to viral marketing.
  In the first part of this paper, we show that there can be no pricing
mechanism that is ""truthful"" with respect to the seller, and we use solution
concepts from coalitional game theory, namely the Core, the Shapley Value, and
the Nash Bargaining Solution, to derive provably ""fair"" prices for settings
with one or multiple recommenders. We then investigate pricing mechanisms for
the setting where recommenders have different ""purchase arguments"". Here we
show that it might be beneficial for the recommenders to withhold some of their
arguments, unless anonymity-proof solution concepts, such as the
anonymity-proof Shapley value, are used.
  In the second part of this paper, we analyze the setting where the
recommendee loses trust in the recommender for each irrelevant recommendation.
Here we prove that even if the recommendee regains her initial trust on each
successful recommendation, the expected total profit the recommender can make
over an infinite period is bounded. This can only be overcome when the
recommendee also incrementally regains trust during periods without any
recommendation. Here, we see an interesting connection to ""banner blindness"",
suggesting that showing fewer ads can lead to a higher long-term profit."
"Bargaining networks model the behavior of a set of players that need to reach
pairwise agreements for making profits. Nash bargaining solutions are special
outcomes of such games that are both stable and balanced. Kleinberg and Tardos
proved a sharp algorithmic characterization of such outcomes, but left open the
problem of how the actual bargaining process converges to them. A partial
answer was provided by Azar et al. who proposed a distributed algorithm for
constructing Nash bargaining solutions, but without polynomial bounds on its
convergence rate. In this paper, we introduce a simple and natural model for
this process, and study its convergence rate to Nash bargaining solutions. At
each time step, each player proposes a deal to each of her neighbors. The
proposal consists of a share of the potential profit in case of agreement. The
share is chosen to be balanced in Nash's sense as far as this is feasible (with
respect to the current best alternatives for both players). We prove that,
whenever the Nash bargaining solution is unique (and satisfies a positive gap
condition) this dynamics converges to it in polynomial time. Our analysis is
based on an approximate decoupling phenomenon between the dynamics on different
substructures of the network. This approach may be of general interest for the
analysis of local algorithms on networks."
"We study bottleneck routing games where the social cost is determined by the
worst congestion on any edge in the network. Bottleneck games have been studied
in the literature by having the player's utility costs to be determined by the
worst congested edge in their paths. However, the Nash equilibria of such games
are inefficient since the price of anarchy can be very high with respect to the
parameters of the game. In order to obtain smaller price of anarchy we explore
{\em exponential bottleneck games} where the utility costs of the players are
exponential functions on the congestion of the edges in their paths. We find
that exponential bottleneck games are very efficient giving a poly-log bound on
the price of anarchy: O(log L log |E|), where L is the largest path length in
the players strategy sets and E is the set of edges in the graph."
"Escalation is a typical feature of infinite games. Therefore tools conceived
for studying infinite mathematical structures, namely those deriving from
coinduction are essential. Here we use coinduction, or backward coinduction (to
show its connection with the same concept for finite games) to study carefully
and formally the infinite games especially those called dollar auctions, which
are considered as the paradigm of escalation. Unlike what is commonly admitted,
we show that, provided one assumes that the other agent will always stop,
bidding is rational, because it results in a subgame perfect equilibrium. We
show that this is not the only rational strategy profile (the only subgame
perfect equilibrium). Indeed if an agent stops and will stop at every step, we
claim that he is rational as well, if one admits that his opponent will never
stop, because this corresponds to a subgame perfect equilibrium. Amazingly, in
the infinite dollar auction game, the behavior in which both agents stop at
each step is not a Nash equilibrium, hence is not a subgame perfect
equilibrium, hence is not rational."
"Matching markets play a prominent role in economic theory. A prime example of
such a market is the sponsored search market. Here, as in other markets of that
kind, market equilibria correspond to feasible, envy free, and bidder optimal
outcomes. For settings without budgets such an outcome always exists and can be
computed in polynomial-time by the so-called Hungarian Method. Moreover, every
mechanism that computes such an outcome is incentive compatible. We show that
the Hungarian Method can be modified so that it finds a feasible, envy free,
and bidder optimal outcome for settings with budgets. We also show that in
settings with budgets no mechanism that computes such an outcome can be
incentive compatible for all inputs. For inputs in general position, however,
the presented mechanism---as any other mechanism that computes such an outcome
for settings with budgets---is incentive compatible."
"We study the design of truthful mechanisms for set systems, i.e., scenarios
where a customer needs to hire a team of agents to perform a complex task. In
this setting, frugality [Archer&Tardos'02] provides a measure to evaluate the
""cost of truthfulness"", that is, the overpayment of a truthful mechanism
relative to the ""fair"" payment. We propose a uniform scheme for designing
frugal truthful mechanisms for general set systems. Our scheme is based on
scaling the agents' bids using the eigenvector of a matrix that encodes the
interdependencies between the agents. We demonstrate that the r-out-of-k-system
mechanism and the \sqrt-mechanism for buying a path in a graph [Karlin et.
al'05] can be viewed as instantiations of our scheme. We then apply our scheme
to two other classes of set systems, namely, vertex cover systems and k-path
systems, in which a customer needs to purchase k edge-disjoint source-sink
paths. For both settings, we bound the frugality of our mechanism in terms of
the largest eigenvalue of the respective interdependency matrix. We show that
our mechanism is optimal for a large subclass of vertex cover systems
satisfying a simple local sparsity condition. For k-path systems, while our
mechanism is within a factor of k + 1 from optimal, we show that it is, in
fact, optimal, when one uses a modified definition of frugality proposed in
[Elkind et al.'07]. Our lower bound argument combines spectral techniques and
Young's inequality, and is applicable to all set systems. As both r-out-of-k
systems and single path systems can be viewed as special cases of k-path
systems, our result improves the lower bounds of [Karlin et al.'05] and answers
several open questions proposed in that paper."
"The paper studies one-shot two-player games with non-Bayesian uncertainty.
The players have an attitude that ranges from optimism to pessimism in the face
of uncertainty. Given the attitudes, each player forms a belief about the set
of possible strategies of the other player. If these beliefs are consistent,
one says that they form an uncertainty equilibrium. One then considers a
two-phase game where the players first choose their attitude and then play the
resulting game. The paper illustrates these notions with a number of games
where the approach provides a new insight into the plausible strategies of the
players."
"This paper derives a model of screening contracts in the presence of positive
network effects when building an electronic commerce network (e-commerce)
between a large firm and a small and medium sized enterprise (SME) supplier
based on Compte (2008). Compte (2008) main insight is that when several
potential candidates compete for the task, the principal will in general
improve the performance of his firm by inducing the member candidates to assess
their competence before signing the contract (through an appropriate choice of
contracts). The large firm (principal) must choose between different SME
suppliers (agents) to build a business to business e-commerce network. In the
presence of positive network externalities, we show that social surplus
increases."
"We exhibit the rich structure of the set of correlated equilibria by
analyzing the simplest of polynomial games: the mixed extension of matching
pennies. We show that while the correlated equilibrium set is convex and
compact, the structure of its extreme points can be quite complicated. In
finite games the ratio of extreme correlated to extreme Nash equilibria can be
greater than exponential in the size of the strategy spaces. In polynomial
games there can exist extreme correlated equilibria which are not finitely
supported; we construct a large family of examples using techniques from
ergodic theory. We show that in general the set of correlated equilibrium
distributions of a polynomial game cannot be described by conditions on
finitely many moments (means, covariances, etc.), in marked contrast to the set
of Nash equilibria which is always expressible in terms of finitely many
moments."
"Iterated regret minimization has been introduced recently by J.Y. Halpern and
R. Pass in classical strategic games. For many games of interest, this new
solution concept provides solutions that are judged more reasonable than
solutions offered by traditional game concepts -- such as Nash equilibrium --.
Although computing iterated regret on explicit matrix game is conceptually and
computationally easy, nothing is known about computing the iterated regret on
games whose matrices are defined implicitly using game tree, game DAG or, more
generally game graphs. In this paper, we investigate iterated regret
minimization for infinite duration two-player quantitative non-zero sum games
played on graphs.
  We consider reachability objectives that are not necessarily antagonist.
Edges are weighted by integers -- one for each player --, and the payoffs are
defined by the sum of the weights along the paths. Depending on the class of
graphs, we give either polynomial or pseudo-polynomial time algorithms to
compute a strategy that minimizes the regret for a fixed player. We finally
give algorithms to compute the strategies of the two players that minimize the
iterated regret for trees, and for graphs with strictly positive weights only."
"This paper presents a technique for approximating, up to any precision, the
set of subgame-perfect equilibria (SPE) in discounted repeated games. The
process starts with a single hypercube approximation of the set of SPE. Then
the initial hypercube is gradually partitioned on to a set of smaller adjacent
hypercubes, while those hypercubes that cannot contain any point belonging to
the set of SPE are simultaneously withdrawn.
  Whether a given hypercube can contain an equilibrium point is verified by an
appropriate mathematical program. Three different formulations of the algorithm
for both approximately computing the set of SPE payoffs and extracting players'
strategies are then proposed: the first two that do not assume the presence of
an external coordination between players, and the third one that assumes a
certain level of coordination during game play for convexifying the set of
continuation payoffs after any repeated game history.
  A special attention is paid to the question of extracting players' strategies
and their representability in form of finite automata, an important feature for
artificial agent systems."
"We study a novel class of mechanism design problems in which the outcomes are
constrained by the payments. This basic class of mechanism design problems
captures many common economic situations, and yet it has not been studied, to
our knowledge, in the past. We focus on the case of procurement auctions in
which sellers have private costs, and the auctioneer aims to maximize a utility
function on subsets of items, under the constraint that the sum of the payments
provided by the mechanism does not exceed a given budget. Standard mechanism
design ideas such as the VCG mechanism and its variants are not applicable
here. We show that, for general functions, the budget constraint can render
mechanisms arbitrarily bad in terms of the utility of the buyer. However, our
main result shows that for the important class of submodular functions, a
bounded approximation ratio is achievable. Better approximation results are
obtained for subclasses of the submodular functions. We explore the space of
budget feasible mechanisms in other domains and give a characterization under
more restricted conditions."
"The existing literature on optimal auctions focuses on optimizing the
expected revenue of the seller, and is appropriate for risk-neutral sellers. In
this paper, we identify good mechanisms for risk-averse sellers. As is standard
in the economics literature, we model the risk-aversion of a seller by endowing
the seller with a monotone concave utility function. We then seek robust
mechanisms that are approximately optimal for all sellers, no matter what their
levels of risk-aversion are. We have two main results for multi-unit auctions
with unit-demand bidders whose valuations are drawn i.i.d. from a regular
distribution. First, we identify a posted-price mechanism called the Hedge
mechanism, which gives a universal constant factor approximation; we also show
for the unlimited supply case that this mechanism is in a sense the best
possible. Second, we show that the VCG mechanism gives a universal constant
factor approximation when the number of bidders is even only a small multiple
of the number of items. Along the way we point out that Myerson's
characterization of the optimal mechanisms fails to extend to
utility-maximization for risk-averse sellers, and establish interesting
properties of regular distributions and monotone hazard rate distributions."
"We consider two-player turn-based games with zero-reachability and
zero-safety objectives generated by extended vector addition systems with
states. Although the problem of deciding the winner in such games is
undecidable in general, we identify several decidable and even tractable
subcases of this problem obtained by restricting the number of counters and/or
the sets of target configurations."
"We investigate the power of randomness in the context of a fundamental
Bayesian optimal mechanism design problem--a single seller aims to maximize
expected revenue by allocating multiple kinds of resources to ""unit-demand""
agents with preferences drawn from a known distribution. When the agents'
preferences are single-dimensional Myerson's seminal work [Myerson '81] shows
that randomness offers no benefit--the optimal mechanism is always
deterministic. In the multi-dimensional case, where each agent's preferences
are given by different values for each of the available services, Briest et al.
[Briest, Chawla, Kleinberg, and Weinberg '10] recently showed that the gap
between the expected revenue obtained by an optimal randomized mechanism and an
optimal deterministic mechanism can be unbounded even when a single agent is
offered only 4 services. However, this large gap is attained through unnatural
instances where values of the agent for different services are correlated in a
specific way. We show that when the agent's values involve no correlation or a
specific kind of positive correlation, the benefit of randomness is only a
small constant factor (4 and 8 respectively). Our model of positively
correlated values (that we call additive values) is a natural model for
unit-demand agents and items that are substitutes. Our results extend to
multiple agent settings as well."
"The classical model of signaling games assumes that the receiver exactly know
the type space (private information) of the sender and be able to discriminate
each type of the sender distinctly. However, the justification of this
assumption is questionable. It is more reasonable to let the receiver recognize
the pattern of the sender. In this paper, we investigate what happens if the
assumption is relaxed. A framework of signaling games with pattern recognition
and an example are given."
"Much work has been done on the computation of market equilibria. However due
to strategic play by buyers, it is not clear whether these are actually
observed in the market. Motivated by the observation that a buyer may derive a
better payoff by feigning a different utility function and thereby manipulating
the Fisher market equilibrium, we formulate the {\em Fisher market game} in
which buyers strategize by posing different utility functions. We show that
existence of a {\em conflict-free allocation} is a necessary condition for the
Nash equilibria (NE) and also sufficient for the symmetric NE in this game.
There are many NE with very different payoffs, and the Fisher equilibrium
payoff is captured at a symmetric NE. We provide a complete polyhedral
characterization of all the NE for the two-buyer market game. Surprisingly, all
the NE of this game turn out to be symmetric and the corresponding payoffs
constitute a piecewise linear concave curve. We also study the correlated
equilibria of this game and show that third-party mediation does not help to
achieve a better payoff than NE payoffs."
"We consider a class of infinite-state stochastic games generated by stateless
pushdown automata (or, equivalently, 1-exit recursive state machines), where
the winning objective is specified by a regular set of target configurations
and a qualitative probability constraint `>0' or `=1'. The goal of one player
is to maximize the probability of reaching the target set so that the
constraint is satisfied, while the other player aims at the opposite. We show
that the winner in such games can be determined in PTIME for the `>0'
constraint, and both in NP and coNP for the `=1' constraint. Further, we prove
that the winning regions for both players are regular, and we design algorithms
which compute the associated finite-state automata. Finally, we show that
winning strategies can be synthesized effectively."
"We deal in this paper with strategical languages of infinite words, that is
those generated by a nondeterministic strategy in the sense of game theory. We
first show the existence of a minimal strategy for such languages, for which we
give an explicit expression. Then we characterize the family of strategical
languages as that of closed ones, in the topological space of infinite words.
Finally, we give a definition of a Nash equilibrium for such languages, that we
illustrate with a famous example."
"We consider dominant strategy implementation in private values settings, when
agents have multi-dimensional types, the set of alternatives is finite,
monetary transfers are allowed, and agents have quasi-linear utilities. We show
that any implementable and neutral social choice function must be a weighted
welfare maximizer if the type space of every agent is an $m$-dimensional open
interval, where $m$ is the number of alternatives. When the type space of every
agent is unrestricted, Roberts' theorem with neutrality \cite{Roberts79}
becomes a corollary to our result. Our proof technique uses a {\em social
welfare ordering} approach, commonly used in aggregation literature in social
choice theory. We also prove the general (affine maximizer) version of Roberts'
theorem for unrestricted type spaces of agents using this approach."
"In this paper, we propose a new analytical framework to solve medium access
problem for secondary users (SUs) in cognitive radio networks. Partially
Observable Stochastic Games (POSG) and Decentralized Markov Decision Process
(Dec-POMDP) are two multi-agent Markovian decision processes which are used to
present a solution. A primary network with two SUs is considered as an example
to demonstrate our proposed framework. Two different scenarios are assumed. In
the first scenario, SUs compete to acquire the licensed channel which is
modeled using POSG framework. In the second one, SUs cooperate to access
channel for which the solution is based on Dec-POMDP. Besides, the dominant
strategy for both of the above mentioned scenarios is presented for a three
slot horizon length."
"We study strategy improvement algorithms for mean-payoff and parity games. We
describe a structural property of these games, and we show that these
structures can affect the behaviour of strategy improvement. We show how
awareness of these structures can be used to accelerate strategy improvement
algorithms. We call our algorithms non-oblivious because they remember
properties of the game that they have discovered in previous iterations. We
show that non-oblivious strategy improvement algorithms perform well on
examples that are known to be hard for oblivious strategy improvement. Hence,
we argue that previous strategy improvement algorithms fail because they ignore
the structural properties of the game that they are solving."
"It is well known that the rock-paper-scissors game has no pure saddle point.
We show that this holds more generally: A symmetric two-player zero-sum game
has a pure saddle point if and only if it is not a generalized
rock-paper-scissors game. Moreover, we show that every finite symmetric
quasiconcave two-player zero-sum game has a pure saddle point. Further
sufficient conditions for existence are provided. We apply our theory to a rich
collection of examples by noting that the class of symmetric two-player
zero-sum games coincides with the class of relative payoff games associated
with symmetric two-player games. This allows us to derive results on the
existence of a finite population evolutionary stable strategies."
"We study {\em bottleneck routing games} where the social cost is determined
by the worst congestion on any edge in the network. In the literature,
bottleneck games assume player utility costs determined by the worst congested
edge in their paths. However, the Nash equilibria of such games are inefficient
since the price of anarchy can be very high and proportional to the size of the
network. In order to obtain smaller price of anarchy we introduce {\em
exponential bottleneck games} where the utility costs of the players are
exponential functions of their congestions. We find that exponential bottleneck
games are very efficient and give a poly-log bound on the price of anarchy:
$O(\log L \cdot \log |E|)$, where $L$ is the largest path length in the
players' strategy sets and $E$ is the set of edges in the graph. By adjusting
the exponential utility costs with a logarithm we obtain games whose player
costs are almost identical to those in regular bottleneck games, and at the
same time have the good price of anarchy of exponential games."
"We study auctions with additive valuations where agents have a limit on the
number of goods they may receive. We refer to such valuations as {\em
capacitated} and seek mechanisms that maximize social welfare and are
simultaneously incentive compatible, envy-free, individually rational, and have
no positive transfers. If capacities are infinite, then sequentially repeating
the 2nd price Vickrey auction meets these requirements. In 1983, Leonard showed
that for unit capacities, VCG with Clarke Pivot payments is also envy free. For
capacities that are all unit or all infinite, the mechanism produces a
Walrasian pricing (subject to capacity constraints). Here, we consider general
capacities. For homogeneous capacities (all capacities equal) we show that VCG
with Clarke Pivot payments is envy free (VCG with Clarke Pivot payments is
always incentive compatible, individually rational, and has no positive
transfers). Contrariwise, there is no incentive compatible Walrasian pricing.
For heterogeneous capacities, we show that there is no mechanism with all 4
properties, but at least in some cases, one can achieve both incentive
compatibility and envy freeness."
"We study mechanisms for an allocation of goods among agents, where agents
have no incentive to lie about their true values (incentive compatible) and for
which no agent will seek to exchange outcomes with another (envy-free).
Mechanisms satisfying each requirement separately have been studied
extensively, but there are few results on mechanisms achieving both. We are
interested in those allocations for which there exist payments such that the
resulting mechanism is simultaneously incentive compatible and envy-free.
Cyclic monotonicity is a characterization of incentive compatible allocations,
local efficiency is a characterization for envy-free allocations. We combine
the above to give a characterization for allocations which are both incentive
compatible and envy free. We show that even for allocations that allow payments
leading to incentive compatible mechanisms, and other payments leading to envy
free mechanisms, there may not exist any payments for which the mechanism is
simultaneously incentive compatible and envy-free. The characterization that we
give lets us compute the set of Pareto-optimal mechanisms that trade off envy
freeness for incentive compatibility."
"We address the problem of fair division, or cake cutting, with the goal of
finding truthful mechanisms. In the case of a general measure space (""cake"")
and non-atomic, additive individual preference measures - or utilities - we
show that there exists a truthful ""mechanism"" which ensures that each of the k
players gets at least 1/k of the cake. This mechanism also minimizes risk for
truthful players. Furthermore, in the case where there exist at least two
different measures we present a different truthful mechanism which ensures that
each of the players gets more than 1/k of the cake.
  We then turn our attention to partitions of indivisible goods with bounded
utilities and a large number of goods. Here we provide similar mechanisms, but
with slightly weaker guarantees. These guarantees converge to those obtained in
the non-atomic case as the number of goods goes to infinity."
"We propose and analyze a broad family of games played by resource-constrained
players, which are characterized by the following central features: 1) each
user has a multi-dimensional action space, subject to a single sum resource
constraint; 2) each user's utility in a particular dimension depends on an
additive coupling between the user's action in the same dimension and the
actions of the other users; and 3) each user's total utility is the sum of the
utilities obtained in each dimension. Familiar examples of such multi-user
environments in communication systems include power control over
frequency-selective Gaussian interference channels and flow control in Jackson
networks. In settings where users cannot exchange messages in real-time, we
study how users can adjust their actions based on their local observations. We
derive sufficient conditions under which a unique Nash equilibrium exists and
the best-response algorithm converges globally and linearly to the Nash
equilibrium. In settings where users can exchange messages in real-time, we
focus on user choices that optimize the overall utility. We provide the
convergence conditions of two distributed action update mechanisms, gradient
play and Jacobi update."
"We study the problem of characterizing revenue optimal auctions for
single-minded buyers. Each buyer is interested only in a specific bundle of
items and has a value for the same. Both his bundle and its value are his
private information. The bundles that buyers are interested in and their
corresponding values are assumed to be realized from known probability
distributions independent across the buyers. We identify revenue optimal
auctions with a simple structure, if the conditional distribution of any
buyer's valuation is nondecreasing, in the hazard rates ordering of probability
distributions, as a function of the bundle the buyer is interested in. The
revenue optimal auction is given by the solution of a maximum weight
independent set problem. We provide a novel graphical construction of the
weights and highlight important properties of the resulting auction."
"We study efficiency loss in Bayesian revenue optimal auctions. We quantify
this as the worst case ratio of loss in the realized social welfare to the
social welfare that can be realized by an efficient auction. Our focus is on
auctions with single-parameter buyers and where buyers' valuation sets are
finite. For binary valued single-parameter buyers with independent (not
necessarily identically distributed) private valuations, we show that the worst
case efficiency loss ratio (ELR) is no worse than it is with only one buyer;
moreover, it is at most 1/2. Moving beyond the case of binary valuations but
restricting to single item auctions, where buyers' private valuations are
independent and identically distributed, we obtain bounds on the worst case ELR
as a function of number of buyers, cardinality of buyers' valuation set, and
ratio of maximum to minimum possible values that buyers can have for the item."
"An population-centric analysis for a version of the p-beauty contest game is
given for the two-player, finite population, and infinite population cases.
Winning strategies are characterized in terms of iterative thinking relative to
the population. To win the game one needs to iterate more times than the
ambient population, but not too many more times depending on the population
size and the value of p."
"Very recently, Hartline and Lucier studied single-parameter mechanism design
problems in the Bayesian setting. They proposed a black-box reduction that
converted Bayesian approximation algorithms into Bayesian-Incentive-Compatible
(BIC) mechanisms while preserving social welfare. It remains a major open
question if one can find similar reduction in the more important
multi-parameter setting. In this paper, we give positive answer to this
question when the prior distribution has finite and small support. We propose a
black-box reduction for designing BIC multi-parameter mechanisms. The reduction
converts any algorithm into an eps-BIC mechanism with only marginal loss in
social welfare. As a result, for combinatorial auctions with sub-additive
agents we get an eps-BIC mechanism that achieves constant approximation."
"We conduct a computational analysis of fair and optimal partitions in
additively separable hedonic games. We show that, for strict preferences, a
Pareto optimal partition can be found in polynomial time while verifying
whether a given partition is Pareto optimal is coNP-complete, even when
preferences are symmetric and strict. Moreover, computing a partition with
maximum egalitarian or utilitarian social welfare or one which is both Pareto
optimal and individually rational is NP-hard. We also prove that checking
whether there exists a partition which is both Pareto optimal and envy-free is
$\Sigma_{2}^{p}$-complete. Even though an envy-free partition and a Nash stable
partition are both guaranteed to exist for symmetric preferences, checking
whether there exists a partition which is both envy-free and Nash stable is
NP-complete."
"Sponsored search mechanisms have drawn much attention from both academic
community and industry in recent years since the seminal papers of [13] and
[14]. However, most of the existing literature concentrates on the mechanism
design and analysis within the scope of only one search engine in the market.
In this paper we propose a mathematical framework for modeling the interaction
of publishers, advertisers and end users in a competitive market. We first
consider the monopoly market model and provide optimal solutions for both ex
ante and ex post cases, which represents the long-term and short-term revenues
of search engines respectively. We then analyze the strategic behaviors of end
users and advertisers under duopoly and prove the existence of equilibrium for
both search engines to co-exist from ex-post perspective. To show the more
general ex ante results, we carry out extensive simulations under different
parameter settings. Our analysis and observation in this work can provide
useful insight in regulating the sponsored search market and protecting the
interests of advertisers and end users."
"We examine perfect information stochastic mean-payoff games - a class of
games containing as special sub-classes the usual mean-payoff games and parity
games. We show that deterministic memoryless strategies that are optimal for
discounted games with state-dependent discount factors close to 1 are optimal
for priority mean-payoff games establishing a strong link between these two
classes."
"Graph games of infinite length are a natural model for open reactive
processes: one player represents the controller, trying to ensure a given
specification, and the other represents a hostile environment. The evolution of
the system depends on the decisions of both players, supplemented by chance.
  In this work, we focus on the notion of randomised strategy. More
specifically, we show that three natural definitions may lead to very different
results: in the most general cases, an almost-surely winning situation may
become almost-surely losing if the player is only allowed to use a weaker
notion of strategy. In more reasonable settings, translations exist, but they
require infinite memory, even in simple cases. Finally, some traditional
problems becomes undecidable for the strongest type of strategies."
"The solution of parity games over pushdown graphs (Walukiewicz '96) was the
first step towards an effective theory of infinite-state games. It was shown
that winning strategies for pushdown games can be implemented again as pushdown
automata. We continue this study and investigate the connection between game
presentations and winning strategies in altogether six cases of game arenas,
among them realtime pushdown systems, visibly pushdown systems, and counter
systems. In four cases we show by a uniform proof method that we obtain
strategies implementable by the same type of pushdown machine as given in the
game arena. We prove that for the two remaining cases this correspondence
fails. In the conclusion we address the question of an abstract criterion that
explains the results."
"Consider a matching problem on a graph where disjoint sets of vertices are
privately owned by self-interested agents. An edge between a pair of vertices
indicates compatibility and allows the vertices to match. We seek a mechanism
to maximize the number of matches despite self-interest, with agents that each
want to maximize the number of their own vertices that match. Each agent can
choose to hide some of its vertices, and then privately match the hidden
vertices with any of its own vertices that go unmatched by the mechanism. A
prominent application of this model is to kidney exchange, where agents
correspond to hospitals and vertices to donor-patient pairs. Here hospitals may
game an exchange by holding back pairs and harm social welfare. In this paper
we seek to design mechanisms that are strategyproof, in the sense that agents
cannot benefit from hiding vertices, and approximately maximize efficiency,
i.e., produce a matching that is close in cardinality to the maximum
cardinality matching. Our main result is the design and analysis of the
eponymous Mix-and-Match mechanism; we show that this randomized mechanism is
strategyproof and provides a 2-approximation. Lower bounds establish that the
mechanism is near optimal."
"The whitespace-discovery problem describes two parties, Alice and Bob, trying
to establish a communication channel over one of a given large segment of
whitespace channels. Subsets of the channels are occupied in each of the local
environments surrounding Alice and Bob, as well as in the global environment
between them (Eve). In the absence of a common clock for the two parties, the
goal is to devise time-invariant (stationary) strategies minimizing the
synchronization time. This emerged from recent applications in discovery of
wireless devices.
  We model the problem as follows. There are $N$ channels, each of which is
open (unoccupied) with probability $p_1,p_2,q$ independently for Alice, Bob and
Eve respectively. Further assume that $N \gg 1/(p_1 p_2 q)$ to allow for
sufficiently many open channels. Both Alice and Bob can detect which channels
are locally open and every time-slot each of them chooses one such channel for
an attempted sync. One aims for strategies that, with high probability over the
environments, guarantee a shortest possible expected sync time depending only
on the $p_i$'s and $q$.
  Here we provide a stationary strategy for Alice and Bob with a guaranteed
expected sync time of $O(1 / (p_1 p_2 q^2))$ given that each party also has
knowledge of $p_1,p_2,q$. When the parties are oblivious of these
probabilities, analogous strategies incur a cost of a poly-log factor, i.e.\
$\tilde{O}(1 / (p_1 p_2 q^2))$. Furthermore, this performance guarantee is
essentially optimal as we show that any stationary strategies of Alice and Bob
have an expected sync time of at least $\Omega(1/(p_1 p_2 q^2))$."
"We consider two sided matching markets consisting of agents with
non-transferable utilities; agents from the opposite sides form matching pairs
(e.g., buyers-sellers) and negotiate the terms of their math which may include
a monetary transfer. Competitive equilibria are the elements of the core of
this game.
  We present the first combinatorial characterization of competitive equilibria
that relates the utility of each agent at equilibrium to the equilibrium
utilities of other agents in a strictly smaller market excluding that agent;
thus automatically providing a constructive proof of existence of competitive
equilibria in such markets.
  Our characterization also yields a group strategyproof mechanism for
allocating indivisible goods to unit demand buyers with non-quasilinear
utilities that highly resembles the Vickrey Clarke Groves (VCG) mechanism. As a
direct application of this, we present a group strategyproof welfare maximizing
mechanism for Ad-Auctions without requiring the usual assumption that search
engine and advertisers have consistent estimates of the clickthrough rates."
"In the context of strategic games, we provide an axiomatic proof of the
statement Common knowledge of rationality implies that the players will choose
only strategies that survive the iterated elimination of strictly dominated
strategies. Rationality here means playing only strategies one believes to be
best responses. This involves looking at two formal languages. One is
first-order, and is used to formalise optimality conditions, like avoiding
strictly dominated strategies, or playing a best response. The other is a modal
fixpoint language with expressions for optimality, rationality and belief.
Fixpoints are used to form expressions for common belief and for iterated
elimination of non-optimal strategies."
"One major function of social networks (e.g., massive online social networks)
is the dissemination of information such as scientific knowledge, news, and
rumors. Information can be propagated by the users of the network via natural
connections in written, oral or electronic form. The information passing from a
sender to a receiver intrinsically involves both of them considering their
self-perceived knowledge, reputation, and popularity, which further determine
their decisions of whether or not to forward the information and whether or not
to provide feedback. To understand such human aspects of the information
dissemination, we propose a game theoretical model of the information
forwarding and feedback mechanisms in a social network that take into account
the personalities of the sender and the receiver (including their perceived
knowledgeability, reputation, and desire for popularity) and the global
characteristics of the network."
"MiBoard (Multiplayer Interactive Board Game) is an online, turn-based board
game, which is a supplement of the iSTART (Interactive Strategy Training for
Active Reading and Thinking) application. MiBoard is developed to test the
hypothesis that integrating game characteristics (point rewards, game-like
interaction, and peer feedback) into the iSTART trainer will significantly
improve its effectiveness on students' learning. It was shown by M. Rowe that a
physical board game did in fact enhance students' performance. MiBoard is a
computer-based version of Rowe's board game that eliminates constraints on
locality while retaining the crucial practice components that were the game's
objective. MiBoard gives incentives for participation and provides a more
enjoyable and social practice environment compared to the online individual
practice component of the original trainer."
"MiBoard (Multiplayer Interactive Board Game) is an online, turn-based board
game, which is a supplement of the iSTART (Interactive Strategy Training for
Active Reading and Thinking) application. MiBoard is developed to test the
hypothesis that integrating game characteristics (point rewards, game-like
interaction, and peer feedback) into the iSTART trainer will significantly
improve its effectiveness on students' learning. It was shown by M. Rowe that a
physical board game did in fact enhance students' performance. MiBoard is a
computer-based version of Rowe's board game that eliminates constraints on
locality while retaining the crucial practice components that were the game's
objective. MiBoard gives incentives for participation and provides a more
enjoyable and social practice environment compared to the online individual
practice component of the original trainer"
"Increasing user engagement is constant challenge for Intelligent Tutoring
Systems researchers. A current trend in the ITS field is to increase engagement
of proven learning systems by integrating them within games, or adding in game
like components. Incorporating proven learning methods within a game based
environment is expected to add to the overall experience without detracting
from the original goals, however, the current study demonstrates two important
issues with regard to ITS design. First, effective designs from the physical
world do not always translate into the digital world. Second, games do not
necessarily improve engagement, and in some cases, they may have the opposite
effect. The current study discusses the development and a brief assessment of
MiBoard a multiplayer collaborative online board game designed to closely
emulate a previously developed physical board game, iSTART: The Board Game."
"MiBoard (Multiplayer Interactive Board Game) is an online, turnbased board
game that was developed to assess the integration of game characteristics
(point rewards, game-like interaction, and peer feedback) and how that might
affect student engagement and learning efficacy. This online board game was
designed to fit within the Extended Practice module of iSTART (Interactive
Strategy Training for Active Reading and Thinking). Unfortunately, preliminary
research shows that MiBoard actually reduces engagement and does not benefit
the quality of student self-explanations when compared to the original Extended
Practice module. Consequently the MiBoard framework has been revamped to create
Self-Explanation Showdown, a faster-paced, less analytically oriented game that
adds competition to the creation of self-explanations. Students are evaluated
on the quality of their self-explanations using the same assessment algorithms
from iSTART Extended Practice module (this includes both word-based and
LSA-based assessments). The technical issues involved in development of MiBoard
and Self- Explanation Showdown are described. The lessons learned from the
MiBoard experience are also discussed in this paper."
"We present a formal model for studying fashion trends, in terms of three
parameters of fashionable items: (1) their innate utility; (2) individual
boredom associated with repeated usage of an item; and (3) social influences
associated with the preferences from other people. While there are several
works that emphasize the effect of social influence in understanding fashion
trends, in this paper we show how boredom plays a strong role in both
individual and social choices. We show how boredom can be used to explain the
cyclic choices in several scenarios such as an individual who has to pick a
restaurant to visit every day, or a society that has to repeatedly `vote' on a
single fashion style from a collection. We formally show that a society that
votes for a single fashion style can be viewed as a single individual cycling
through different choices.
  In our model, the utility of an item gets discounted by the amount of boredom
that has accumulated over the past; this boredom increases with every use of
the item and decays exponentially when not used. We address the problem of
optimally choosing items for usage, so as to maximize over-all satisfaction,
i.e., composite utility, over a period of time. First we show that the simple
greedy heuristic of always choosing the item with the maximum current composite
utility can be arbitrarily worse than the optimal. Second, we prove that even
with just a single individual, determining the optimal strategy for choosing
items is NP-hard. Third, we show that a simple modification to the greedy
algorithm that simply doubles the boredom of each item is a provably close
approximation to the optimal strategy. Finally, we present an experimental
study over real-world data collected from query logs to compare our algorithms."
"We investigate the extent to which price updates can increase the revenue of
a seller with little prior information on demand. We study prior-free revenue
maximization for a seller with unlimited supply of n item types facing m myopic
buyers present for k < log n days. For the static (k = 1) case, Balcan et al.
[2] show that one random item price (the same on each item) yields revenue
within a \Theta(log m + log n) factor of optimum and this factor is tight. We
define the hereditary maximizers property of buyer valuations (satisfied by any
multi-unit or gross substitutes valuation) that is sufficient for a significant
improvement of the approximation factor in the dynamic (k > 1) setting. Our
main result is a non-increasing, randomized, schedule of k equal item prices
with expected revenue within a O((log m + log n) / k) factor of optimum for
private valuations with hereditary maximizers. This factor is almost tight: we
show that any pricing scheme over k days has a revenue approximation factor of
at least (log m + log n) / (3k). We obtain analogous matching lower and upper
bounds of \Theta((log n) / k) if all valuations have the same maximum. We
expect our upper bound technique to be of broader interest; for example, it can
significantly improve the result of Akhlaghpour et al. [1]. We also initiate
the study of revenue maximization given allocative externalities (i.e.
influences) between buyers with combinatorial valuations. We provide a rather
general model of positive influence of others' ownership of items on a buyer's
valuation. For affine, submodular externalities and valuations with hereditary
maximizers we present an influence-and-exploit (Hartline et al. [13]) marketing
strategy based on our algorithm for private valuations. This strategy preserves
our approximation factor, despite an affine increase (due to externalities) in
the optimum revenue."
"We study the computational complexity of basic decision problems for
one-counter simple stochastic games (OC-SSGs), under various objectives.
OC-SSGs are 2-player turn-based stochastic games played on the transition graph
of classic one-counter automata. We study primarily the termination objective,
where the goal of one player is to maximize the probability of reaching counter
value 0, while the other player wishes to avoid this. Partly motivated by the
goal of understanding termination objectives, we also study certain ""limit"" and
""long run average"" reward objectives that are closely related to some
well-studied objectives for stochastic games with rewards. Examples of problems
we address include: does player 1 have a strategy to ensure that the counter
eventually hits 0, i.e., terminates, almost surely, regardless of what player 2
does? Or that the liminf (or limsup) counter value equals infinity with a
desired probability? Or that the long run average reward is >0 with desired
probability? We show that the qualitative termination problem for OC-SSGs is in
NP intersection coNP, and is in P-time for 1-player OC-SSGs, or equivalently
for one-counter Markov Decision Processes (OC-MDPs). Moreover, we show that
quantitative limit problems for OC-SSGs are in NP intersection coNP, and are in
P-time for 1-player OC-MDPs. Both qualitative limit problems and qualitative
termination problems for OC-SSGs are already at least as hard as Condon's
quantitative decision problem for finite-state SSGs."
"The Prisoner's Dilemma is a simple model that captures the essential
contradiction between individual rationality and global rationality. Although
the one-shot Prisoner's Dilemma is usually viewed simple, in this paper we will
categorize it into five different types. For the type-4 Prisoner's Dilemma
game, we will propose a self-enforcing algorithmic model to help
non-cooperative agents obtain Pareto-efficient payoffs. The algorithmic model
is based on an algorithm using complex numbers and can work in macro
applications."
"In this note, we argue that there is a bug in [Tirole, J., ""Hierarchies and
bureaucracies: On the role of collusion in organizations,"" {\em Journal of Law,
Economics and Organization}, vol.2, 181-214, 1986]."
"We study admission control mechanisms for wireless access networks where (i)
each user has a minimum service requirement, (ii) the capacity of the access
network is limited, and (iii) the access point is not allowed to use monetary
mechanisms to guarantee that users do not lie when disclosing their minimum
service requirements. To guarantee truthfulness, we use auction theory to
design a mechanism where users compete to be admitted into the network. We
propose admission control mechanisms under which the access point intelligently
allocates resources based on the announced minimum service requirements to
ensure that users have no incentive to lie and the capacity constraint is
fulfilled. We also prove the properties that any feasible mechanism should
have."
"Routing games are used to to understand the impact of individual users'
decisions on network efficiency. Most prior work on routing games uses a
simplified model of network flow where all flow exists simultaneously, and
users care about either their maximum delay or their total delay. Both of these
measures are surrogates for measuring how long it takes to get all of a user's
traffic through the network. We attempt a more direct study of how competition
affects network efficiency by examining routing games in a flow over time
model. We give an efficiently computable Stackelberg strategy for this model
and show that the competitive equilibrium under this strategy is no worse than
a small constant times the optimal, for two natural measures of optimality."
"Given a rank-1 bimatrix game (A,B), i.e., where rank(A+B)=1, we construct a
suitable linear subspace of the rank-1 game space and show that this subspace
is homeomorphic to its Nash equilibrium correspondence. Using this
homeomorphism, we give the first polynomial time algorithm for computing an
exact Nash equilibrium of a rank-1 bimatrix game. This settles an open question
posed in Kannan and Theobald (SODA 2007) and Theobald (2007). In addition, we
give a novel algorithm to enumerate all the Nash equilibria of a rank-1 game
and show that a similar technique may also be applied for finding a Nash
equilibrium of any bimatrix game. This technique also proves the existence,
oddness and the index theorem of Nash equilibria in a bimatrix game. Further,
we extend the rank-1 homeomorphism result to a fixed rank game space, and give
a fixed point formulation on $[0,1]^k$ for solving a rank-k game. The
homeomorphism and the fixed point formulation are piece-wise linear and
considerably simpler than the classical constructions."
"One of the most challenging problems in Opportunistic Spectrum Access (OSA)
is to design channel sensing-based protocol in multi secondary users (SUs)
network. Quality of Service (QoS) requirements for SUs have significant
implications on this protocol design. In this paper, we propose a new method to
find joint policies for SUs which not only guarantees QoS requirements but also
maximizes network throughput. We use Decentralized Partially Observable Markov
Decision Process (Dec-POMDP) to formulate interactions between SUs. Meanwhile,
a tractable approach for Dec-POMDP is utilized to extract sub-optimum joint
policies for large horizons. Among these policies, the joint policy which
guarantees QoS requirements is selected as the joint sensing strategy for SUs.
To show the efficiency of the proposed method, we consider two SUs trying to
access two-channel primary users (PUs) network modeled by discrete Markov
chains. Simulations demonstrate three interesting findings: 1- Optimum joint
policies for large horizons can be obtained using the proposed method. 2- There
exists a joint policy for the assumed QoS constraints. 3- Our method
outperforms other related works in terms of network throughput."
"Recent results, establishing evidence of intractability for such restrictive
utility functions as additively separable, piecewise-linear and concave, under
both Fisher and Arrow-Debreu market models, have prompted the question of
whether we have failed to capture some essential elements of real markets,
which seem to do a good job of finding prices that maintain parity between
supply and demand.
  The main point of this paper is to show that even non-separable, quasiconcave
utility functions can be handled efficiently in a suitably chosen, though
natural, realistic and useful, market model; our model allows for perfect price
discrimination. Our model supports unique equilibrium prices and, for the
restriction to concave utilities, satisfies both welfare theorems."
"The Robinson-Goforth topology of swaps in adjoining payoffs elegantly
arranges 2x2 ordinal games in accordance with important properties including
symmetry, number of dominant strategies and Nash Equilibria, and alignment of
interests. Adding payoff families based on Nash Equilibria illustrates an
additional aspect of this order and aids visualization of the topology. Making
ties through half-swaps not only creates simpler games within the topology,
but, in reverse, breaking ties shows the evolution of preferences, yielding a
natural ordering for the topology of 2x2 games with ties. An ordinal game not
only represents an equivalence class of games with real values, but also a
discrete equivalent of the normalized version of those games. The topology
provides coordinates which could be used to identify related games in a
semantic web ontology and facilitate comparative analysis of agent-based
simulations and other research in game theory, as well as charting
relationships and potential moves between games as a tool for institutional
analysis and design."
"We study {\em bottleneck congestion games} where the social cost is
determined by the worst congestion of any resource. These games directly relate
to network routing problems and also job-shop scheduling problems. In typical
bottleneck congestion games, the utility costs of the players are determined by
the worst congested resources that they use. However, the resulting Nash
equilibria are inefficient, since the price of anarchy is proportional on the
number of resources which can be high. Here we show that we can get smaller
price of anarchy with the bottleneck social cost metric. We introduce the {\em
polynomial bottleneck games} where the utility costs of the players are
polynomial functions of the congestion of the resources that they use. In
particular, the delay function for any resource $r$ is $C_{r}^\M$, where $C_r$
is the congestion measured as the number of players that use $r$, and $\M \geq
1$ is an integer constant that defines the degree of the polynomial. The
utility cost of a player is the sum of the individual delays of the resources
that it uses. The social cost of the game remains the same, namely, it is the
worst bottleneck resource congestion: $\max_{r} C_r$. We show that polynomial
bottleneck games are very efficient and give price of anarchy
$O(|R|^{1/(\M+1)})$, where $R$ is the set of resources. This price of anarchy
is tight, since we demonstrate a game with price of anarchy
$\Omega(|R|^{1/(\M+1)})$, for any $\M \geq 1$. We obtain our tight bounds by
using two proof techniques: {\em transformation}, which we use to convert
arbitrary games to simpler games, and {\em expansion}, which we use to bound
the price of anarchy in a simpler game."
"An important task in the analysis of multiagent systems is to understand how
groups of selfish players can form coalitions, i.e., work together in teams. In
this paper, we study the dynamics of coalition formation under bounded
rationality. We consider settings where each team's profit is given by a convex
function, and propose three profit-sharing schemes, each of which is based on
the concept of marginal utility. The agents are assumed to be myopic, i.e.,
they keep changing teams as long as they can increase their payoff by doing so.
We study the properties (such as closeness to Nash equilibrium or total profit)
of the states that result after a polynomial number of such moves, and prove
bounds on the price of anarchy and the price of stability of the corresponding
games."
"In the theory of social choice the research is focused around the projection
of individual preference orders to the social preference order. Also, the
justification of the preference order formalism begins with the concept of
utility i.e. an alternative is preferred to another one if the utility over the
first is higher then the utility over the second. In this paper is proposed an
ideal model of measuring utilities by considering individuals and alternatives
no more as atomic concepts but as being composed by other entities. Furthermore
is proposed a formal definition of evaluation processes based on utilities."
"We adapt the method used by Jaynes to derive the equilibria of statistical
physics to instead derive equilibria of bounded rational game theory. We
analyze the dependence of these equilibria on the parameters of the underlying
game, focusing on hysteresis effects. In particular, we show that by gradually
imposing individual-specific tax rates on the players of the game, and then
gradually removing those taxes, the players move from a poor equilibrium to one
that is better for all of them."
"This article investigates selfish behavior in games where players are
embedded in a social context. A framework is presented which allows us to
measure the Windfall of Friendship, i.e., how much players benefit (compared to
purely selfish environments) if they care about the welfare of their friends in
the social network graph. As a case study, a virus inoculation game is
examined. We analyze the corresponding Nash equilibria and show that the
Windfall of Friendship can never be negative. However, we find that if the
valuation of a friend is independent of the total number of friends, the social
welfare may not increase monotonically with the extent to which players care
for each other; intriguingly, in the corresponding scenario where the relative
importance of a friend declines, the Windfall is monotonic again. This article
also studies convergence of best-response sequences. It turns out that in
social networks, convergence times are typically higher and hence constitute a
price of friendship. While such phenomena may be known on an anecdotal level,
our framework allows us to quantify these effects analytically. Our formal
insights on the worst case equilibria are complemented by simulations shedding
light onto the structure of other equilibria."
"We consider the line planning problem in public transportation, under a
robustness perspective. We present a mechanism for robust line planning in the
case of multiple line pools, when the line operators have a different utility
function per pool. We conduct an experimental study of our mechanism on both
synthetic and real-world data that shows fast convergence to the optimum. We
also explore a wide range of scenarios, varying from an arbitrary initial state
(to be solved) to small disruptions in a previously optimal solution (to be
recovered). Our experiments with the latter scenario show that our mechanism
can be used as an online recovery scheme causing the system to re-converge to
its optimum extremely fast."
"This paper develops a game-theoretic framework for the design and analysis of
a new class of incentive schemes called intervention schemes. We formulate
intervention games, propose a solution concept of intervention equilibrium, and
prove its existence in a finite intervention game. We apply our framework to
resource sharing scenarios in wireless communications, whose non-cooperative
outcomes without intervention yield suboptimal performance. We derive
analytical results and analyze illustrative examples in the cases of imperfect
and perfect monitoring. In the case of imperfect monitoring, intervention
schemes can improve the suboptimal performance of non-cooperative equilibrium
when the intervention device has a sufficiently accurate monitoring technology,
although it may not be possible to achieve the best feasible performance. In
the case of perfect monitoring, the best feasible performance can be obtained
with an intervention scheme when the intervention device has a sufficiently
strong intervention capability."
"We consider the impact of incomplete information on incentives for node
cooperation in parallel relay networks with one source node, one destination
node, and multiple relay nodes. All nodes are selfish and strategic, interested
in maximizing their own profit instead of the social welfare. We consider the
practical situation where the channel state on any given relay path is not
observable to the source or to the other relays. We examine different
bargaining relationships between the source and the relays, and propose a
framework for analyzing the efficiency loss induced by incomplete information.
We analyze the source of the efficiency loss, and quantify the amount of
inefficiency which results."
"We consider the problem of fairly dividing a heterogeneous cake between a
number of players with different tastes. In this setting, it is known that
fairness requirements may result in a suboptimal division from the social
welfare standpoint. Here, we show that in some cases, discarding some of the
cake and fairly dividing only the remainder may be socially preferable to any
fair division of the entire cake. We study this phenomenon, providing
asymptotically-tight bounds on the social improvement achievable by such
discarding."
"We consider a game played between a hider, who hides a static object in one
of several possible positions in a bounded planar region, and a searcher, who
wishes to reach the object by querying sensors placed in the plane. The
searcher is a mobile agent, and whenever it physically visits a sensor, the
sensor returns a random direction, corresponding to a half-plane in which the
hidden object is located. We first present a novel search heuristic and
characterize bounds on the expected distance covered before reaching the
object. Next, we model this game as a large-dimensional zero-sum dynamic game
and we apply a recently introduced randomized sampling technique that provides
a probabilistic level of security to the hider. We observe that, when the
randomized sampling approach is only allowed to select a very small number of
samples, the cost of the heuristic is comparable to the security level provided
by the randomized procedure. However, as we allow the number of samples to
increase, the randomized procedure provides a higher probabilistic security
level."
"We consider Markov Decision Processes (MDPs) with mean-payoff parity and
energy parity objectives. In system design, the parity objective is used to
encode \omega-regular specifications, and the mean-payoff and energy objectives
can be used to model quantitative resource constraints. The energy condition
requires that the resource level never drops below 0, and the mean-payoff
condition requires that the limit-average value of the resource consumption is
within a threshold. While these two (energy and mean-payoff) classical
conditions are equivalent for two-player games, we show that they differ for
MDPs. We show that the problem of deciding whether a state is almost-sure
winning (i.e., winning with probability 1) in energy parity MDPs is in NP \cap
coNP, while for mean-payoff parity MDPs, the problem is solvable in polynomial
time, improving a recent PSPACE bound."
"We study social welfare in one-sided matching markets where the goal is to
efficiently allocate n items to n agents that each have a complete, private
preference list and a unit demand over the items. Our focus is on allocation
mechanisms that do not involve any monetary payments. We consider two natural
measures of social welfare: the ordinal welfare factor which measures the
number of agents that are at least as happy as in some unknown, arbitrary
benchmark allocation, and the linear welfare factor which assumes an agent's
utility linearly decreases down his preference lists, and measures the total
utility to that achieved by an optimal allocation. We analyze two matching
mechanisms which have been extensively studied by economists. The first
mechanism is the random serial dictatorship (RSD) where agents are ordered in
accordance with a randomly chosen permutation, and are successively allocated
their best choice among the unallocated items. The second mechanism is the
probabilistic serial (PS) mechanism of Bogomolnaia and Moulin [8], which
computes a fractional allocation that can be expressed as a convex combination
of integral allocations. The welfare factor of a mechanism is the infimum over
all instances. For RSD, we show that the ordinal welfare factor is
asymptotically 1/2, while the linear welfare factor lies in the interval [.526,
2/3]. For PS, we show that the ordinal welfare factor is also 1/2 while the
linear welfare factor is roughly 2/3. To our knowledge, these results are the
first non-trivial performance guarantees for these natural mechanisms."
"In two-player games on graph, the players construct an infinite path through
the game graph and get a reward computed by a payoff function over infinite
paths. Over weighted graphs, the typical and most studied payoff functions
compute the limit-average or the discounted sum of the rewards along the path.
Beside their simple definition, these two payoff functions enjoy the property
that memoryless optimal strategies always exist.
  In an attempt to construct other simple payoff functions, we define a class
of payoff functions which compute an (infinite) weighted average of the
rewards. This new class contains both the limit-average and discounted sum
functions, and we show that they are the only members of this class which
induce memoryless optimal strategies, showing that there is essentially no
other simple payoff functions."
"We consider Markov decision processes (MDPs) with \omega-regular
specifications given as parity objectives. We consider the problem of computing
the set of almost-sure winning states from where the objective can be ensured
with probability 1. The algorithms for the computation of the almost-sure
winning set for parity objectives iteratively use the solutions for the
almost-sure winning set for B\""uchi objectives (a special case of parity
objectives). Our contributions are as follows: First, we present the first
subquadratic symbolic algorithm to compute the almost-sure winning set for MDPs
with B\""uchi objectives; our algorithm takes O(n \sqrt{m}) symbolic steps as
compared to the previous known algorithm that takes O(n^2) symbolic steps,
where $n$ is the number of states and $m$ is the number of edges of the MDP. In
practice MDPs have constant out-degree, and then our symbolic algorithm takes
O(n \sqrt{n}) symbolic steps, as compared to the previous known $O(n^2)$
symbolic steps algorithm. Second, we present a new algorithm, namely win-lose
algorithm, with the following two properties: (a) the algorithm iteratively
computes subsets of the almost-sure winning set and its complement, as compared
to all previous algorithms that discover the almost-sure winning set upon
termination; and (b) requires O(n \sqrt{K}) symbolic steps, where K is the
maximal number of edges of strongly connected components (scc's) of the MDP.
The win-lose algorithm requires symbolic computation of scc's. Third, we
improve the algorithm for symbolic scc computation; the previous known
algorithm takes linear symbolic steps, and our new algorithm improves the
constants associated with the linear number of steps. In the worst case the
previous known algorithm takes 5n symbolic steps, whereas our new algorithm
takes 4n symbolic steps."
"We study Markov decision processes (MDPs) with multiple limit-average (or
mean-payoff) functions. We consider two different objectives, namely,
expectation and satisfaction objectives. Given an MDP with k limit-average
functions, in the expectation objective the goal is to maximize the expected
limit-average value, and in the satisfaction objective the goal is to maximize
the probability of runs such that the limit-average value stays above a given
vector. We show that under the expectation objective, in contrast to the case
of one limit-average function, both randomization and memory are necessary for
strategies even for epsilon-approximation, and that finite-memory randomized
strategies are sufficient for achieving Pareto optimal values. Under the
satisfaction objective, in contrast to the case of one limit-average function,
infinite memory is necessary for strategies achieving a specific value (i.e.
randomized finite-memory strategies are not sufficient), whereas memoryless
randomized strategies are sufficient for epsilon-approximation, for all
epsilon>0. We further prove that the decision problems for both expectation and
satisfaction objectives can be solved in polynomial time and the trade-off
curve (Pareto curve) can be epsilon-approximated in time polynomial in the size
of the MDP and 1/epsilon, and exponential in the number of limit-average
functions, for all epsilon>0. Our analysis also reveals flaws in previous work
for MDPs with multiple mean-payoff functions under the expectation objective,
corrects the flaws, and allows us to obtain improved results."
"Traditionally, most consumers of electricity pay for their consumptions
according to a fixed rate. With the advancement of Smart Grid technologies,
large-scale implementation of variable-rate metering becomes more practical. As
a result, consumers will be able to control their electricity consumption in an
automated fashion, where one possible scheme is to have each individual
maximize its own utility as a noncooperative game. In this paper,
noncooperative games are formulated among the electricity consumers in Smart
Grid with two real-time pricing schemes, where the Nash equilibrium operation
points are investigated for their uniqueness and load balancing properties. The
first pricing scheme charges a price according to the average cost of
electricity borne by the retailer and the second one charges according to a
time-variant increasing-block price, where for each scheme, a zero-revenue
model and a constant-rate revenue model are considered. In addition, the
relationship between the studied games and certain competitive routing games
from the computer networking community, known as atomic flow games, is
established, for which it is shown that the proposed noncooperative game
formulation falls under the class of atomic splittable flow games. The Nash
equilibrium is shown to exist for four different combined cases corresponding
to the two pricing schemes and the two revenue models, and is unique for three
of the cases under certain conditions. It is further shown that both pricing
schemes lead to similar electricity loading patterns when consumers are only
interested in minimizing the electricity costs without any other profit
considerations. Finally, the conditions under which the increasing-block
pricing scheme is preferred over the average-cost based pricing scheme are
discussed."
"One-counter MDPs (OC-MDPs) and one-counter simple stochastic games (OC-SSGs)
are 1-player, and 2-player turn-based zero-sum, stochastic games played on the
transition graph of classic one-counter automata (equivalently, pushdown
automata with a 1-letter stack alphabet). A key objective for the analysis and
verification of these games is the termination objective, where the players aim
to maximize (minimize, respectively) the probability of hitting counter value
0, starting at a given control state and given counter value. Recently, we
studied qualitative decision problems (""is the optimal termination value = 1?"")
for OC-MDPs (and OC-SSGs) and showed them to be decidable in P-time (in NP and
coNP, respectively). However, quantitative decision and approximation problems
(""is the optimal termination value ? p"", or ""approximate the termination value
within epsilon"") are far more challenging. This is so in part because optimal
strategies may not exist, and because even when they do exist they can have a
highly non-trivial structure. It thus remained open even whether any of these
quantitative termination problems are computable. In this paper we show that
all quantitative approximation problems for the termination value for OC-MDPs
and OC-SSGs are computable. Specifically, given a OC-SSG, and given epsilon >
0, we can compute a value v that approximates the value of the OC-SSG
termination game within additive error epsilon, and furthermore we can compute
epsilon-optimal strategies for both players in the game. A key ingredient in
our proofs is a subtle martingale, derived from solving certain LPs that we can
associate with a maximizing OC-MDP. An application of Azuma's inequality on
these martingales yields a computable bound for the ""wealth"" at which a ""rich
person's strategy"" becomes epsilon-optimal for OC-MDPs."
"This paper presents a new exponential lower bound for the two most popular
deterministic variants of the strategy improvement algorithms for solving
parity, mean payoff, discounted payoff and simple stochastic games. The first
variant improves every node in each step maximizing the current valuation
locally, whereas the second variant computes the globally optimal improvement
in each step. We outline families of games on which both variants require
exponentially many strategy iterations."
"For Bayesian combinatorial auctions, we present a general framework for
approximately reducing the mechanism design problem for multiple buyers to
single buyer sub-problems. Our framework can be applied to any setting which
roughly satisfies the following assumptions: (i) buyers' types must be
distributed independently (not necessarily identically), (ii) objective
function must be linearly separable over the buyers, and (iii) except for the
supply constraints, there should be no other inter-buyer constraints. Our
framework is general in the sense that it makes no explicit assumption about
buyers' valuations, type distributions, and single buyer constraints (e.g.,
budget, incentive compatibility, etc).
  We present two generic multi buyer mechanisms which use single buyer
mechanisms as black boxes; if an $\alpha$-approximate single buyer mechanism
can be constructed for each buyer, and if no buyer requires more than
$\frac{1}{k}$ of all units of each item, then our generic multi buyer
mechanisms are $\gamma_k\alpha$-approximation of the optimal multi buyer
mechanism, where $\gamma_k$ is a constant which is at least
$1-\frac{1}{\sqrt{k+3}}$. Observe that $\gamma_k$ is at least 1/2 (for $k=1$)
and approaches 1 as $k \to \infty$. As a byproduct of our construction, we
present a generalization of prophet inequalities. Furthermore, as applications
of our framework, we present multi buyer mechanisms with improved approximation
factor for several settings from the literature."
"Games on graphs provide a natural model for reactive non-terminating systems.
In such games, the interaction of two players on an arena results in an
infinite path that describes a run of the system. Different settings are used
to model various open systems in computer science, as for instance turn-based
or concurrent moves, and deterministic or stochastic transitions. In this
paper, we are interested in turn-based games, and specifically in deterministic
parity games and stochastic reachability games (also known as simple stochastic
games). We present a simple, direct and efficient reduction from deterministic
parity games to simple stochastic games: it yields an arena whose size is
linear up to a logarithmic factor in size of the original arena."
"We study in depth the class of games with opacity condition, which are
two-player games with imperfect information in which one of the players only
has imperfect information, and where the winning condition relies on the
information he has along the play. Those games are relevant for security
aspects of computing systems: a play is opaque whenever the player who has
imperfect information never ""knows"" for sure that the current position is one
of the distinguished ""secret"" positions. We study the problems of deciding the
existence of a winning strategy for each player, and we call them the
opacity-violate problem and the opacity-guarantee problem. Focusing on the
player with perfect information is new in the field of games with
imperfect-information because when considering classical winning conditions it
amounts to solving the underlying perfect-information game. We establish the
EXPTIME-completeness of both above-mentioned problems, showing that our winning
condition brings a gap of complexity for the player with perfect information,
and we exhibit the relevant opacity-verify problem, which noticeably
generalizes approaches considered in the literature for opacity analysis in
discrete-event systems. In the case of blindfold games, this problem relates to
the two initial ones, yielding the determinacy of blindfold games with opacity
condition and the PSPACE-completeness of the three problems."
"In this paper, the 2-dimensional decentralized parallel interference channel
(IC) with 2 transmitter-receiver pairs is modelled as a non-cooperative static
game. Each transmitter is assumed to be a fully rational entity with complete
information on the game, aiming to maximize its own individual spectral
efficiency by tuning its own power allocation (PA) vector. Two scenarios are
analysed. First, we consider that transmitters can split their transmit power
between both dimensions (PA game). Second, we consider that each transmitter is
limited to use only one dimension (channel selection CS game). In the first
scenario, the game might have either one or three NE in pure strategies (PS).
However, two or infinitely many NE in PS might also be observed with zero
probability. In the second scenario, there always exists either one or two NE
in PS. We show that in both games there always exists a non-zero probability of
observing more than one NE. More interestingly, using Monte-Carlo simulations,
we show that the highest and lowest network spectral efficiency at any of the
NE in the CS game are always higher than the ones in the PA."
"Computation plays a major role in decision making. Even if an agent is
willing to ascribe a probability to all states and a utility to all outcomes,
and maximize expected utility, doing so might present serious computational
problems. Moreover, computing the outcome of a given act might be difficult. In
a companion paper we develop a framework for game theory with costly
computation, where the objects of choice are Turing machines. Here we apply
that framework to decision theory. We show how well-known phenomena like
first-impression-matters biases (i.e., people tend to put more weight on
evidence they hear early on), belief polarization (two people with different
prior beliefs, hearing the same evidence, can end up with diametrically opposed
conclusions), and the status quo bias (people are much more likely to stick
with what they already have) can be easily captured in that framework. Finally,
we use the framework to define some new notions: value of computational
information (a computational variant of value of information) and and
computational value of conversation."
"We propose a new interpretation of the strange phenomena that some authors
have observed about the Wald game. This interpretation is possible thanks to
the new language of \emph{loadings} that Morrison and the author have
introduced in a previous work. Using the theory of loadings and allowed
strategies, we are also able to prove that Wald's game admits a \emph{natural}
solution and, as one can expect, the game turns out to be fair for this
solution. As a technical tool, we introduce the notion of \emph{embedding a
game into another game} that could be of interest from a theoretical point of
view. \emph{En passant} we find a very easy example of a game which is loadable
in infinitely many different ways."
"We propose a new method for opportunistic power control in multi-carrier
interference channels for delay-tolerant data services. In doing so, we utilize
a game theoretic framework with novel constraints, where each user tries to
maximize its utility in a distributed and opportunistic manner, while
satisfying the game's constraints by adapting its transmit power to its
channel. In this scheme, users transmit with more power on good sub-channels
and do the opposite on bad sub-channels. In this way, in addition to the
allocated power on each sub-channel, the total power of all users also depends
on channel conditions. Since each user's power level depends on power levels of
other users, the game belongs to the \emph{generalized} Nash equilibrium (GNE)
problems, which in general, is hard to analyze. We show that the proposed game
has a GNE, and derive the sufficient conditions for its uniqueness. Besides, we
propose a new pricing scheme for maximizing each user's throughput in an
opportunistic manner under its total power constraint; and provide the
sufficient conditions for the algorithm's convergence and its GNE's uniqueness.
Simulations confirm that our proposed scheme yields a higher throughput for
each user and/or has a significantly improved efficiency as compared to other
existing opportunistic methods."
"We formulate the resource allocation problem for the uplink of code division
multiple access (CDMA) networks using a game theoretic framework, propose an
efficient and distributed algorithm for a joint rate and power allocation, and
show that the proposed algorithm converges to the unique Nash equilibrium (NE)
of the game. Our choice for the utility function enables each user to adapt its
transmit power and throughput to its channel. Due to users' selfish behavior,
the output of the game (its NE) may not be a desirable one. To avoid such
cases, we use pricing to control each user's behavior, and analytically show
that similar to the no-pricing case, our pricing-based algorithm converges to
the unique NE of the game, at which, each user achieves its target
signal-to-interference-plus-noise ratio (SINR). We also extend our distributed
resource allocation scheme to multi-cell environments for base station
assignment. Simulation results confirm that our algorithm is computationally
efficient and its signalling overhead is low. In particular, we will show that
in addition to its ability to attain the required QoS of users, our scheme
achieves better fairness in allocating resources and can significantly reduce
transmit power as compared to existing schemes."
"We study a routing game in which one of the players unilaterally acts
altruistically by taking into consideration the latency cost of other players
as well as his own. By not playing selfishly, a player can not only improve the
other players' equilibrium utility but also improve his own equilibrium
utility. To quantify the effect, we define a metric called the Value of
Unilateral Altruism (VoU) to be the ratio of the equilibrium utility of the
altruistic user to the equilibrium utility he would have received in Nash
equilibrium if he were selfish. We show by example that the VoU, in a game with
nonlinear latency functions and atomic players, can be arbitrarily large. Since
the Nash equilibrium social welfare of this example is arbitrarily far from
social optimum, this example also has a Price of Anarchy (PoA) that is
unbounded. The example is driven by there being a small number of players since
the same example with non-atomic players yields a Nash equilibrium that is
fully efficient."
"We consider the following generalization of the classical pursuit-evasion
problem, which we call k-capture. A group of n pursuers (hyenas) wish to
capture an evader (lion) who is free to move in an m-dimensional Euclidean
space, the pursuers and the evader can move with the same maximum speed, and at
least k pursuers must simultaneously reach the evader's location to capture it.
If fewer than k pursuers reach the evader, then those pursuers get destroyed by
the evader. Under what conditions can the evader be k-captured? We study this
problem in the discrete time, continuous space model and prove that k-capture
is possible if and only there exists a time when the evader lies in the
interior of the pursuers' k-Hull. When the pursuit occurs inside a compact,
convex subset of the Euclidean space, we show through an easy constructive
strategy that k-capture is always possible."
"We consider the problem of locating a public facility on a line, where a set
of $n$ strategic agents report their \emph{locations} and a mechanism
determines, either deterministically or randomly, the location of the facility.
Game theoretic perspectives of the facility location problem advanced in two
main directions. The first direction is concerned with the characterization of
\emph{strategyproof} (SP) mechanisms; i.e., mechanisms that induce truthful
reporting as a dominant strategy; and the second direction quantifies how well
various objective functions can be approximated when restricted to SP
mechanisms. The current paper provides contributions in both directions. First,
we construct a parameterized randomized SP mechanism, and show that all of the
previously proposed deterministic and randomized SP mechanisms for the current
settings can be formalized as special cases of this mechanism. Second, we give
tight results for the approximation ratio of SP mechanisms with respect to the
objective of minimizing the sum of squares of distances to the agents
(\emph{miniSOS}). Holzman \cite{Holzman1990} provided an axiomatic foundation
for this function, showing that it is the unique function that satisfies
unanimity, continuity and invariance. We devise a randomized mechanism that
gives a 1.5-approximation for the miniSOS function, and show that no other
randomized SP mechanism can provide a better approximation. This mechanism
chooses the average location with probability 1/2 and a \emph{random dictator}
with probability 1/2. For deterministic mechanisms, we show that the median
mechanism provides a 2-approximation, and this is tight. Together, our study
provides fundamental understanding of the miniSOS objective function and makes
a step toward the characterization of randomized SP facility location
mechanisms."
"The class of weakly acyclic games, which includes potential games and
dominance-solvable games, captures many practical application domains. In a
weakly acyclic game, from any starting state, there is a sequence of
better-response moves that leads to a pure Nash equilibrium; informally, these
are games in which natural distributed dynamics, such as better-response
dynamics, cannot enter inescapable oscillations. We establish a novel link
between such games and the existence of pure Nash equilibria in subgames.
Specifically, we show that the existence of a unique pure Nash equilibrium in
every subgame implies the weak acyclicity of a game. In contrast, the possible
existence of multiple pure Nash equilibria in every subgame is insufficient for
weak acyclicity in general; here, we also systematically identify the special
cases (in terms of the number of players and strategies) for which this is
sufficient to guarantee weak acyclicity."
"In many settings agents participate in multiple different auctions that are
not necessarily implemented simultaneously. Future opportunities affect
strategic considerations of the players in each auction, introducing
externalities. Motivated by this consideration, we study a setting of a market
of buyers and sellers, where each seller holds one item, bidders have
combinatorial valuations and sellers hold item auctions sequentially.
  Our results are qualitatively different from those of simultaneous auctions,
proving that simultaneity is a crucial aspect of previous work. We prove that
if sellers hold sequential first price auctions then for unit-demand bidders
(matching market) every subgame perfect equilibrium achieves at least half of
the optimal social welfare, while for submodular bidders or when second price
auctions are used, the social welfare can be arbitrarily worse than the
optimal. We also show that a first price sequential auction for buying or
selling a base of a matroid is always efficient, and implements the VCG
outcome.
  An important tool in our analysis is studying first and second price auctions
with externalities (bidders have valuations for each possible winner outcome),
which can be of independent interest. We show that a Pure Nash Equilibrium
always exists in a first price auction with externalities."
"Voting theory has become increasingly integrated with computational social
choice and multiagent systems. Computational complexity has been extensively
used as a shield against manipulation of voting systems, however for several
voting schemes this complexity may cause calculating the winner to be
computationally difficult. Of the many voting systems that have been studied
with regard to election manipulation, a few have been found to have an
unweighted coalitional manipulation problem that is NP-hard for a constant
number of manipulators despite having a winner problem that is in P. We survey
this interesting class of voting systems and the work that has analyzed their
complexity."
"There is only one technique for prior-free optimal mechanism design that
generalizes beyond the structurally benevolent setting of digital goods. This
technique uses random sampling to estimate the distribution of agent values and
then employs the Bayesian optimal mechanism for this estimated distribution on
the remaining players. Though quite general, even for digital goods, this
random sampling auction has a complicated analysis and is known to be
suboptimal. To overcome these issues we generalize the consensus technique from
Goldberg and Hartline (2003) to structurally rich environments that include,
e.g., single-minded combinatorial auctions."
"In many online systems, individuals provide services for each other; the
recipient of the service obtains a benefit but the provider of the service
incurs a cost. If benefit exceeds cost, provision of the service increases
social welfare and should therefore be encouraged -- but the individuals
providing the service gain no (immediate) benefit from providing the service
and hence have an incentive to withhold service. Hence there is scope for
designing a system that improves welfare by encouraging exchange. To operate
successfully within the confines of the online environment, such a system
should be distributed, practicable, and consistent with individual incentives.
This paper proposes and analyzes a simple such system that relies on the
exchange of {\em tokens}; the emphasis is on the design of a protocol (number
of tokens and suggested strategies). We provide estimates for the efficiency of
such protocols and show that choosing the right protocol will lead to almost
full efficiency if agents are sufficiently patient. However, choosing the wrong
protocols may lead to an enormous loss of efficiency."
This paper has been withdrawn by the author.
"We consider the problem of converting an arbitrary approximation algorithm
for a single-parameter optimization problem into a computationally efficient
truthful mechanism. We ask for reductions that are black-box, meaning that they
require only oracle access to the given algorithm and in particular do not
require explicit knowledge of the problem constraints. Such a reduction is
known to be possible, for example, for the social welfare objective when the
goal is to achieve Bayesian truthfulness and preserve social welfare in
expectation. We show that a black-box reduction for the social welfare
objective is not possible if the resulting mechanism is required to be truthful
in expectation and to preserve the worst-case approximation ratio of the
algorithm to within a subpolynomial factor. Further, we prove that for other
objectives such as makespan, no black-box reduction is possible even if we only
require Bayesian truthfulness and an average-case performance guarantee."
"We investigate complexity issues related to pure Nash equilibria of strategic
games. We show that, even in very restrictive settings, determining whether a
game has a pure Nash Equilibrium is NP-hard, while deciding whether a game has
a strong Nash equilibrium is SigmaP2-complete. We then study practically
relevant restrictions that lower the complexity. In particular, we are
interested in quantitative and qualitative restrictions of the way each players
payoff depends on moves of other players. We say that a game has small
neighborhood if the utility function for each player depends only on (the
actions of) a logarithmically small number of other players. The dependency
structure of a game G can be expressed by a graph DG(G) or by a hypergraph
H(G). By relating Nash equilibrium problems to constraint satisfaction problems
(CSPs), we show that if G has small neighborhood and if H(G) has bounded
hypertree width (or if DG(G) has bounded treewidth), then finding pure Nash and
Pareto equilibria is feasible in polynomial time. If the game is graphical,
then these problems are LOGCFL-complete and thus in the class NC2 of highly
parallelizable problems."
"The paper is devoted to quantization of extensive games with the use of both
the Marinatto-Weber and the Eisert-Wilkens-Lewenstein concept of quantum game.
We revise the current conception of quantum ultimatum game and we show why the
proposal is unacceptable. To support our comment, we present the new idea of
the quantum ultimatum game. Our scheme also makes a point of departure for a
protocol to quantize extensive games."
"Covering and packing problems can be modeled as games to encapsulate
interesting social and engineering settings. These games have a high Price of
Anarchy in their natural formulation. However, existing research applicable to
specific instances of these games has only been able to prove fast convergence
to arbitrary equilibria. This paper studies general classes of covering and
packing games with learning dynamics models that incorporate a central
authority who broadcasts weak, socially beneficial signals to agents that
otherwise only use local information in their decision-making. Rather than
illustrating convergence to an arbitrary equilibrium that may have very high
social cost, we show that these systems quickly achieve near-optimal
performance.
  In particular, we show that in the public service advertising model, reaching
a small constant fraction of the agents is enough to bring the system to a
state within a log n factor of optimal in a broad class of set cover and set
packing games or a constant factor of optimal in the special cases of vertex
cover and maximum independent set, circumventing social inefficiency of bad
local equilibria that could arise without a central authority. We extend these
results to the learn-then-decide model, in which agents use any of a broad
class of learning algorithms to decide in a given round whether to behave
according to locally optimal behavior or the behavior prescribed by the
broadcast signal. The new techniques we use for analyzing these games could be
of broader interest for analyzing more general classic optimization problems in
a distributed fashion."
"We analyze the problem of computing a correlated equilibrium that optimizes
some objective (e.g., social welfare). Papadimitriou and Roughgarden [2008]
gave a sufficient condition for the tractability of this problem; however, this
condition only applies to a subset of existing representations. We propose a
different algorithmic approach for the optimal CE problem that applies to all
compact representations, and give a sufficient condition that generalizes that
of Papadimitriou and Roughgarden. In particular, we reduce the optimal CE
problem to the deviation-adjusted social welfare problem, a combinatorial
optimization problem closely related to the optimal social welfare problem.
This framework allows us to identify new classes of games for which the optimal
CE problem is tractable; we show that graphical polymatrix games on tree graphs
are one example. We also study the problem of computing the optimal coarse
correlated equilibrium, a solution concept closely related to CE. Using a
similar approach we derive a sufficient condition for this problem, and use it
to prove that the problem is tractable for singleton congestion games."
"We consider a model of priced resource sharing that combines both queueing
behavior and strategic behavior. We study a priority service model where a
single server allocates its capacity to agents in proportion to their payment
to the system, and users from different classes act to minimize the sum of
their cost for processing delay and payment. As the exact processing time of
this system is hard to compute, we introduce the notion of heavy traffic
equilibrium as an approximation of the Nash equilibrium, derived by considering
the asymptotic regime where the system load approaches capacity. We discuss
efficiency and revenue, and in particular provide a bound for the price of
anarchy of the heavy traffic equilibrium."
"Motivated by the sequence form formulation of Koller et al. (GEB'96), this
paper defines {\em bilinear games}, and proposes efficient algorithms for its
rank based subclasses. Bilinear games are two-player non-cooperative
single-shot games with compact polytopal strategy sets and two payoff matrices
(A,B) such that when (x,y) is the played strategy profile, the payoffs of the
players are xAy and xBy respectively. We show that bilinear games are very
general and capture many interesting classes of games like bimatrix games, two
player Bayesian games, polymatrix games, two-player extensive form games with
perfect recall etc. as special cases, and hence are hard to solve in general.
  Existence of a (symmetric) Nash equilibrium for (symmetric) bilinear games
follow directly from the known results. For a given bilinear game, we define
its {\em Best Response Polytopes} (BRPs) and characterize the set of Nash
equilibria as {\em fully-labeled} pairs in the BRPs. We consider a rank based
hierarchy of bilinear games, where rank of a game (A,B) is defined as
rank(A+B). In this paper, we give polynomial time algorithms to compute Nash
equilibrium for special classes of bilinear games:
  (i) Rank-1 games (i.e., rank(A+B)=1). (ii) FPTAS for constant rank games
(i.e., rank(A+B) is constant). (iii) When rank(A) or rank(B) is constant. This
improves the results by Lipton et al. (EC'03) and Kannan et al. (ET'09), for
bimatrix games with low rank matrices."
"A major achievement of mechanism design theory is a general method for the
construction of truthful mechanisms called VCG (Vickrey, Clarke, Groves). When
applying this method to complex problems such as combinatorial auctions, a
difficulty arises: VCG mechanisms are required to compute optimal outcomes and
are, therefore, computationally infeasible. However, if the optimal outcome is
replaced by the results of a sub-optimal algorithm, the resulting mechanism
(termed VCG-based) is no longer necessarily truthful. The first part of this
paper studies this phenomenon in depth and shows that it is near universal.
Specifically, we prove that essentially all reasonable approximations or
heuristics for combinatorial auctions as well as a wide class of cost
minimization problems yield non-truthful VCG-based mechanisms. We generalize
these results for affine maximizers.
  The second part of this paper proposes a general method for circumventing the
above problem. We introduce a modification of VCG-based mechanisms in which the
agents are given a chance to improve the output of the underlying algorithm.
When the agents behave truthfully, the welfare obtained by the mechanism is at
least as good as the one obtained by the algorithms output. We provide a strong
rationale for truth-telling behavior. Our method satisfies individual
rationality as well."
"There has been much recent work on the revenue-raising properties of truthful
mechanisms for selling goods to selfish bidders. Typically the revenue of a
mechanism is compared against a benchmark (such as, the maximum revenue
obtainable by an omniscient seller selling at a fixed price to at least two
customers), with a view to understanding how much lower the mechanism's revenue
is than the benchmark, in the worst case.
  We study this issue in the context of {\em lotteries}, where the seller may
sell a probability of winning an item. We are interested in two general issues.
Firstly, we aim at using the true optimum revenue as benchmark for our
auctions. Secondly, we study the extent to which the expressive power resulting
from lotteries, helps to improve the worst-case ratio. We study this in the
well-known context of {\em digital goods}, where the production cost is zero.
We show that in this scenario, collusion-resistant lotteries (these are
lotteries for which no coalition of bidders exchanging side payments has an
advantage in lying) are as powerful as truthful ones."
"In elections, a set of candidates ranked consecutively (though possibly in
different order) by all voters is called a clone set, and its members are
called clones. A clone structure is a family of all clone sets of a given
election. In this paper we study properties of clone structures. In particular,
we give an axiomatic characterization of clone structures, show their
hierarchical structure, and analyze clone structures in single-peaked and
single-crossing elections. We give a polynomial-time algorithm that finds a
minimal collection of clones that need to be collapsed for an election to
become single-peaked, and we show that this problem is NP-hard for
single-crossing elections."
"Suppose an unpredictable evader is free to move around in a polygonal
environment of arbitrary complexity that is under full camera surveillance. How
many pursuers, each with the same maximum speed as the evader, are necessary
and sufficient to guarantee a successful capture of the evader? The pursuers
always know the evader's current position through the camera network, but need
to physically reach the evader to capture it. We allow the evader the knowledge
of the current positions of all the pursuers as well---this accords with the
standard worst-case analysis model, but also models a practical situation where
the evader has ""hacked"" into the surveillance system.
  Our main result is to prove that three pursuers are always sufficient and
sometimes necessary to capture the evader. The bound is independent of the
number of vertices or holes in the polygonal environment. The result should be
contrasted with the incomplete information pursuit-evasion where at least
{\Omega}(\surd h + log n) pursuers are required just for detecting the evader
in an environment with n vertices and h holes."
"Combinatorial Auctions are a central problem in Algorithmic Mechanism Design:
pricing and allocating goods to buyers with complex preferences in order to
maximize some desired objective (e.g., social welfare, revenue, or profit). The
problem has been well-studied in the case of limited supply (one copy of each
item), and in the case of digital goods (the seller can produce additional
copies at no cost). Yet in the case of resources---oil, labor, computing
cycles, etc.---neither of these abstractions is just right: additional supplies
of these resources can be found, but at increasing difficulty (marginal cost)
as resources are depleted.
  In this work, we initiate the study of the algorithmic mechanism design
problem of combinatorial pricing under increasing marginal cost. The goal is to
sell these goods to buyers with unknown and arbitrary combinatorial valuation
functions to maximize either the social welfare, or the seller's profit;
specifically we focus on the setting of \emph{posted item prices} with buyers
arriving online. We give algorithms that achieve {\em constant factor}
approximations for a class of natural cost functions---linear, low-degree
polynomial, logarithmic---and that give logarithmic approximations for more
general increasing marginal cost functions (along with a necessary additive
loss). We show that these bounds are essentially best possible for these
settings."
"We present a general technique, based on a primal-dual formulation, for
analyzing the quality of self-emerging solutions in weighted congestion games.
With respect to traditional combinatorial approaches, the primal-dual schema
has at least three advantages: first, it provides an analytic tool which can
always be used to prove tight upper bounds for all the cases in which we are
able to characterize exactly the polyhedron of the solutions under analysis;
secondly, in each such a case the complementary slackness conditions give us an
hint on how to construct matching lower bounding instances; thirdly, proofs
become simpler and easy to check. For the sake of exposition, we first apply
our technique to the problems of bounding the prices of anarchy and stability
of exact and approximate pure Nash equilibria, as well as the approximation
ratio of the solutions achieved after a one-round walk starting from the empty
strategy profile, in the case of affine latency functions and we show how all
the known upper bounds for these measures (and some of their generalizations)
can be easily reobtained under a unified approach. Then, we use the technique
to attack the more challenging setting of polynomial latency functions. In
particular, we obtain the first known upper bounds on the price of stability of
pure Nash equilibria and on the approximation ratio of the solutions achieved
after a one-round walk starting from the empty strategy profile for unweighted
players in the cases of quadratic and cubic latency functions. We believe that
our technique, thanks to its versatility, may prove to be a powerful tool also
in several other applications."
"We investigate the interrelation between graph searching games and games with
imperfect information. As key consequence we obtain that parity games with
bounded imperfect information can be solved in PTIME on graphs of bounded
DAG-width which generalizes several results for parity games on graphs of
bounded complexity. We use a new concept of graph searching where several cops
try to catch multiple robbers instead of just a single robber. The main
technical result is that the number of cops needed to catch r robbers
monotonously is at most r times the DAG-width of the graph. We also explore
aspects of this new concept as a refinement of directed path-width which
accentuates its connection to the concept of imperfect information."
"Structured game representations have recently attracted interest as models
for multi-agent artificial intelligence scenarios, with rational behavior most
commonly characterized by Nash equilibria. This paper presents efficient, exact
algorithms for computing Nash equilibria in structured game representations,
including both graphical games and multi-agent influence diagrams (MAIDs). The
algorithms are derived from a continuation method for normal-form and
extensive-form games due to Govindan and Wilson; they follow a trajectory
through a space of perturbed games and their equilibria, exploiting game
structure through fast computation of the Jacobian of the payoff function. They
are theoretically guaranteed to find at least one equilibrium of the game, and
may find more. Our approach provides the first efficient algorithm for
computing exact equilibria in graphical games with arbitrary topology, and the
first algorithm to exploit fine-grained structural properties of MAIDs.
Experimental results are presented demonstrating the effectiveness of the
algorithms and comparing them to predecessors. The running time of the
graphical game algorithm is similar to, and often better than, the running time
of previous approximate algorithms. The algorithm for MAIDs can effectively
solve games that are much larger than those solvable by previous methods."
"We develop a game-theoretic framework for the study of competition between
firms who have budgets to ""seed"" the initial adoption of their products by
consumers located in a social network. The payoffs to the firms are the
eventual number of adoptions of their product through a competitive stochastic
diffusion process in the network. This framework yields a rich class of
competitive strategies, which depend in subtle ways on the stochastic dynamics
of adoption, the relative budgets of the players, and the underlying structure
of the social network.
  We identify a general property of the adoption dynamics --- namely,
decreasing returns to local adoption --- for which the inefficiency of resource
use at equilibrium (the Price of Anarchy) is uniformly bounded above, across
all networks. We also show that if this property is violated the Price of
Anarchy can be unbounded, thus yielding sharp threshold behavior for a broad
class of dynamics.
  We also introduce a new notion, the Budget Multiplier, that measures the
extent that imbalances in player budgets can be amplified at equilibrium. We
again identify a general property of the adoption dynamics --- namely,
proportional local adoption between competitors --- for which the (pure
strategy) Budget Multiplier is uniformly bounded above, across all networks. We
show that a violation of this property can lead to unbounded Budget Multiplier,
again yielding sharp threshold behavior for a broad class of dynamics."
"Many packing, scheduling and covering problems that were previously
considered by computer science literature in the context of various
transportation and production problems, appear also suitable for describing and
modeling various fundamental aspects in networks optimization such as routing,
resource allocation, congestion control, etc. Various combinatorial problems
were already studied from the game theoretic standpoint, and we attempt to
complement to this body of research.
  Specifically, we consider the bin packing problem both in the classic and
parametric versions, the job scheduling problem and the machine covering
problem in various machine models. We suggest new interpretations of such
problems in the context of modern networks and study these problems from a game
theoretic perspective by modeling them as games, and then concerning various
game theoretic concepts in these games by combining tools from game theory and
the traditional combinatorial optimization. In the framework of this research
we introduce and study models that were not considered before, and also improve
upon previously known results."
"Parity games are a much researched class of games in NP intersect CoNP that
are not known to be in P. Consequently, researchers have considered specialised
algorithms for the case where certain graph parameters are small. In this
paper, we study parity games on graphs with bounded treewidth, and graphs with
bounded DAG width. We show that parity games with bounded DAG width can be
solved in O(n^(k+3) k^(k + 2) (d + 1)^(3k + 2)) time, where n, k, and d are the
size, treewidth, and number of priorities in the parity game. This is an
improvement over the previous best algorithm, given by Berwanger et al., which
runs in n^O(k^2) time. We also show that, if a tree decomposition is provided,
then parity games with bounded treewidth can be solved in O(n k^(k + 5) (d +
1)^(3k + 5)) time. This improves over previous best algorithm, given by
Obdrzalek, which runs in O(n d^(2(k+1)^2)) time. Our techniques can also be
adapted to show that the problem of solving parity games with bounded treewidth
lies in the complexity class NC^2, which is the class of problems that can be
efficiently parallelized. This is in stark contrast to the general parity game
problem, which is known to be P-hard, and thus unlikely to be contained in NC."
"We study single-good auctions in a setting where each player knows his own
valuation only within a constant multiplicative factor \delta{} in (0,1), and
the mechanism designer knows \delta. The classical notions of implementation in
dominant strategies and implementation in undominated strategies are naturally
extended to this setting, but their power is vastly different.
  On the negative side, we prove that no dominant-strategy mechanism can
guarantee social welfare that is significantly better than that achievable by
assigning the good to a random player.
  On the positive side, we provide tight upper and lower bounds for the
fraction of the maximum social welfare achievable in undominated strategies,
whether deterministically or probabilistically."
"The aim of this of this paper is to study infinite games and to prove
formally some properties in this framework. As a consequence we show that the
behavior (the madness) of people which leads to speculative crashes or
escalation can be fully rational. Indeed it proceeds from the statement that
resources are infinite. The reasoning is based on the concept of coinduction
conceived by computer scientists to model infinite computations and used by
economic agents unknowingly. When used consciously, this concept is not as
simple as induction and we could paraphrase Newton: ""Modeling the madness of
people is more difficult than modeling the motion of planets""."
"In this paper, we study the decentralized parallel multiple access channel
(MAC) when transmitters selfishly maximize their individual spectral efficiency
by selecting a single channel to transmit. More specifically, we investigate
the set of Nash equilibria (NE) of decentralized networks comprising several
transmitters communicating with a single receiver that implements single user
decoding. This scenario is modeled as a one-shot game where the players (the
transmitters) have discrete action sets (the channels). We show that the
corresponding game has always at least one NE in pure strategies, but,
depending on certain parameters, the game might possess several NE. We provide
an upper bound for the maximum number of NE as a function of the number of
transmitters and available channels. The main contribution of this paper is a
mathematical proof of the existence of a Braess-type paradox. In particular, it
is shown that under the assumption of a fully loaded network, when transmitters
are allowed to use all the available channels, the corresponding sum spectral
efficiency achieved at the NE is lower or equal than the sum spectral
efficiency achieved when transmitters can use only one channel. A formal proof
of this observation is provided in the case of small networks. For general
scenarios, we provide numerical examples that show that the same effect holds
as long as the network is kept fully loaded. We conclude the paper by
considering the case of successive interference cancellation at the receiver.
In this context, we show that the power allocation vectors at the NE are
capacity maximizers. Finally, simulations are presented to verify our
theoretical results."
"We study the inefficiency of equilibria for various classes of games when
players are (partially) altruistic. We model altruistic behavior by assuming
that player i's perceived cost is a convex combination of 1-\alpha_i times his
direct cost and \alpha_i times the social cost. Tuning the parameters \alpha_i
allows smooth interpolation between purely selfish and purely altruistic
behavior. Within this framework, we study altruistic extensions of linear
congestion games, fair cost-sharing games and valid utility games.
  We derive (tight) bounds on the price of anarchy of these games for several
solution concepts. Thereto, we suitably adapt the smoothness notion introduced
by Roughgarden and show that it captures the essential properties to determine
the robust price of anarchy of these games. Our bounds show that for congestion
games and cost-sharing games, the worst-case robust price of anarchy increases
with increasing altruism, while for valid utility games, it remains constant
and is not affected by altruism. However, the increase in the price of anarchy
is not a universal phenomenon: for symmetric singleton linear congestion games,
we derive a bound on the pure price of anarchy that decreases as the level of
altruism increases. Since the bound is also strictly lower than the robust
price of anarchy, it exhibits a natural example in which Nash equilibria are
more efficient than more permissive notions of equilibrium."
"We efficiently solve the optimal multi-dimensional mechanism design problem
for independent bidders with arbitrary demand constraints when either the
number of bidders is a constant or the number of items is a constant. In the
first setting, we need that each bidder's values for the items are sampled from
a possibly correlated, item-symmetric distribution, allowing different
distributions for each bidder. In the second setting, we allow the values of
each bidder for the items to be arbitrarily correlated, but assume that the
distribution of bidder types is bidder-symmetric.
  For all eps>0, we obtain an additive eps-approximation, when the value
distributions are bounded, or a multiplicative (1-eps)-approximation when the
value distributions are unbounded, but satisfy the Monotone Hazard Rate
condition, covering a widely studied class of distributions in Economics. Our
runtime is polynomial in max{#items,#bidders}, and not the size of the support
of the joint distribution of all bidders' values for all items, which is
typically exponential in both the number of items and the number of bidders.
Our mechanisms are randomized, explicitly price bundles, and can sometimes
accommodate budget constraints.
  Our results are enabled by establishing several new tools and structural
properties of Bayesian mechanisms. We provide a symmetrization technique
turning any truthful mechanism into one that has the same revenue and respects
all symmetries in the underlying value distributions. We also prove that
item-symmetric mechanisms satisfy a natural monotonicity property which, unlike
cyclic-monotonicity, can be harnessed algorithmically. Finally, we provide a
technique that turns any given eps-BIC mechanism (i.e. one where incentive
constraints are violated by eps) into a truly-BIC mechanism at the cost of
O(sqrt{eps}) revenue. We expect our tools to be used beyond the settings we
consider here."
"A network creation game simulates a decentralized and non-cooperative
building of a communication network. Informally, there are $n$ players sitting
on the network nodes, which attempt to establish a reciprocal communication by
activating, incurring a certain cost, any of their incident links. The goal of
each player is to have all the other nodes as close as possible in the
resulting network, while buying as few links as possible. According to this
intuition, any model of the game must then appropriately address a balance
between these two conflicting objectives. Motivated by the fact that a player
might have a strong requirement about its centrality in the network, in this
paper we introduce a new setting in which if a player maintains its (either
maximum or average) distance to the other nodes within a given associated
bound, then its cost is simply equal to the number of activated edges,
otherwise its cost is unbounded. We study the problem of understanding the
structure of associated pure Nash equilibria of the resulting games, that we
call MaxBD and SumBD, respectively. For both games, we show that computing the
best response of a player is an NP-hard problem. Next, we show that when
distance bounds associated with players are non-uniform, then equilibria can be
arbitrarily bad. On the other hand, for MaxBD, we show that when nodes have a
uniform bound $R$ on the maximum distance, then the Price of Anarchy (PoA) is
lower and upper bounded by 2 and $O(n^{\frac{1}{\lfloor\log_3 R\rfloor+1}})$
for $R \geq 3$, while for the interesting case R=2, we are able to prove that
the PoA is $\Omega(\sqrt{n})$ and $O(\sqrt{n \log n})$. For the uniform SumBD
we obtain similar (asymptotically) results, and moreover we show that the PoA
becomes constant as soon as the bound on the average distance is
$n^{\omega(\frac{1}{\sqrt{\log n}})}$."
"We obtain a characterization of feasible, Bayesian, multi-item multi-bidder
auctions with independent, additive bidders as distributions over hierarchical
mechanisms. Combined with cyclic-monotonicity our results provide a complete
characterization of feasible, Bayesian Incentive Compatible (BIC) auctions for
this setting.
  Our characterization is enabled by a novel, constructive proof of Border's
theorem, and a new generalization of this theorem to independent (but not
necessarily iid) bidders. For one item and independent bidders, we show that
any feasible reduced form auction can be implemented as a distribution over
hierarchical mechanisms. We also give a polytime algorithm for determining
feasibility of a reduced form, or finding a separation hyperplane from feasible
reduced forms. Finally, we provide polytime algorithms to find and exactly
sample from a distribution over hierarchical mechanisms consistent with a given
feasible reduced form.
  Our results generalize to multi-item reduced forms for independent, additive
bidders. For multiple items, additive bidders with hard demand constraints, and
arbitrary value correlation across items or bidders, we give a proper
generalization of Border's theorem, and characterize feasible reduced forms as
multicommodity flows in related multicommodity flow instances. We show that our
generalization holds for a broader class of feasibility constraints, including
intersections of any two matroids.
  As a corollary we obtain revenue-optimal, BIC mechanisms in multi-item
multi-bidder settings, when each bidder has arbitrarily correlated values over
the items and additive valuations over bundles, and bidders are independent.
Their runtime is polynomial in the total number of bidder types (instead of
type profiles), and is improved to poly(#items, #bidders) using recent
structural results on optimal BIC auctions in item-symmetric settings."
"Gimbert and Horn gave an algorithm for solving simple stochastic games with
running time O(r! n) where n is the number of positions of the simple
stochastic game and r is the number of its coin toss positions. Chatterjee et
al. pointed out that a variant of strategy iteration can be implemented to
solve this problem in time 4^r r^{O(1)} n^{O(1)}. In this paper, we show that
an algorithm combining value iteration with retrograde analysis achieves a time
bound of O(r 2^r (r log r + n)), thus improving both time bounds. While the
algorithm is simple, the analysis leading to this time bound is involved, using
techniques of extremal combinatorics to identify worst case instances for the
algorithm."
"In a sponsored search auction the advertisement slots on a search result page
are generally ordered by click-through rate. Bidders have a valuation, which is
usually assumed to be linear in the click-through rate, a budget constraint,
and receive at most one slot per search result page (round). We study
multi-round sponsored search auctions, where the different rounds are linked
through the budget constraints of the bidders and the valuation of a bidder for
all rounds is the sum of the valuations for the individual rounds. All
mechanisms published so far either study one-round sponsored search auctions or
the setting where every round has only one slot and all slots have the same
click-through rate, which is identical to a multi-item auction.
  This paper contains the following three results: (1) We give the first
mechanism for the multi-round sponsored search problem where different slots
have different click-through rates. Our mechanism is incentive compatible in
expectation, individually rational in expectation, Pareto optimal in
expectation, and also ex-post Pareto optimal for each realized outcome. (2)
Additionally we study the combinatorial setting, where each bidder is only
interested in a subset of the rounds. We give a deterministic, incentive
compatible, individually rational, and Pareto optimal mechanism for the setting
where all slots have the same click-through rate. (3) We present an
impossibility result for auctions where bidders have diminishing marginal
valuations. Specifically, we show that even for the multi-unit (one slot per
round) setting there is no incentive compatible, individually rational, and
Pareto optimal mechanism for private diminishing marginal valuations and public
budgets."
"A central issue in applying auction theory in practice is the problem of
dealing with budget-constrained agents. A desirable goal in practice is to
design incentive compatible, individually rational, and Pareto optimal auctions
while respecting the budget constraints. Achieving this goal is particularly
challenging in the presence of nontrivial combinatorial constraints over the
set of feasible allocations.
  Toward this goal and motivated by AdWords auctions, we present an auction for
{\em polymatroidal} environments satisfying the above properties. Our auction
employs a novel clinching technique with a clean geometric description and only
needs an oracle access to the submodular function defining the polymatroid. As
a result, this auction not only simplifies and generalizes all previous
results, it applies to several new applications including AdWords Auctions,
bandwidth markets, and video on demand. In particular, our characterization of
the AdWords auction as polymatroidal constraints might be of independent
interest. This allows us to design the first mechanism for Ad Auctions taking
into account simultaneously budgets, multiple keywords and multiple slots.
  We show that it is impossible to extend this result to generic polyhedral
constraints. This also implies an impossibility result for multi-unit auctions
with decreasing marginal utilities in the presence of budget constraints."
"In this paper, we approach the classical problem of clustering using solution
concepts from cooperative game theory such as Nucleolus and Shapley value. We
formulate the problem of clustering as a characteristic form game and develop a
novel algorithm DRAC (Density-Restricted Agglomerative Clustering) for
clustering. With extensive experimentation on standard data sets, we compare
the performance of DRAC with that of well known algorithms. We show an
interesting result that four prominent solution concepts, Nucleolus, Shapley
value, Gately point and \tau-value coincide for the defined characteristic form
game. This vindicates the choice of the characteristic function of the
clustering game and also provides strong intuitive foundation for our approach."
"We consider a slotted-ALOHA LAN with loss-averse, noncooperative greedy
users. To avoid non-Pareto equilibria, particularly deadlock, we assume
probabilistic loss-averse behavior. This behavior is modeled as a modulated
white noise term, in addition to the greedy term, creating a diffusion process
modeling the game. We observe that when player's modulate with their
throughput, a more efficient exploration of play-space results, and so finding
a Pareto equilibrium is more likely over a given interval of time."
"We consider concurrent games played on graphs. At every round of a game, each
player simultaneously and independently selects a move; the moves jointly
determine the transition to a successor state. Two basic objectives are the
safety objective to stay forever in a given set of states, and its dual, the
reachability objective to reach a given set of states. First, we present a
simple proof of the fact that in concurrent reachability games, for all
$\epsilon>0$, memoryless $\epsilon$-optimal strategies exist. A memoryless
strategy is independent of the history of plays, and an $\epsilon$-optimal
strategy achieves the objective with probability within $\epsilon$ of the value
of the game. In contrast to previous proofs of this fact, our proof is more
elementary and more combinatorial. Second, we present a strategy-improvement
(a.k.a.\ policy-iteration) algorithm for concurrent games with reachability
objectives. We then present a strategy-improvement algorithm for concurrent
games with safety objectives. Our algorithms yield sequences of player-1
strategies which ensure probabilities of winning that converge monotonically to
the value of the game. Our result is significant because the
strategy-improvement algorithm for safety games provides, for the first time, a
way to approximate the value of a concurrent safety game from below. Previous
methods could approximate the values of these games only from one direction,
and as no rates of convergence are known, they did not provide a practical way
to solve these games."
"Information exchange systems differ in many ways, but all share a common
vulnerability to selfish behavior and free-riding. In this paper, we build
incentives schemes based on social norms. Social norms prescribe a social
strategy for the users in the system to follow and deploy reputation schemes to
reward or penalize users depending on their behaviors. Because users in these
systems often have only limited capability to observe the global system
information, e.g. the reputation distribution of the users participating in the
system, their beliefs about the reputation distribution are heterogeneous and
biased. Such belief heterogeneity causes a positive fraction of users to not
follow the social strategy. In such practical scenarios, the standard
equilibrium analysis deployed in the economics literature is no longer directly
applicable and hence, the system design needs to consider these differences. To
investigate how the system designs need to change when the participating users
have only limited observations, we focus on a simple social norm with binary
reputation labels but allow adjusting the punishment severity through
randomization. First, we model the belief heterogeneity using a suitable
Bayesian belief function. Next, we formalize the users' optimal decision
problems and derive in which scenarios they follow the prescribed social
strategy. With this result, we then study the system dynamics and formally
define equilibrium in the sense that the system is stable when users
strategically optimize their decisions. By rigorously studying two specific
cases where users' belief distribution is constant or is linearly influenced by
the true reputation distribution, we prove that the optimal reputation update
rule is to choose the mildest possible punishment. This result is further
confirmed for higher order beliefs in simulations."
"In this paper, we examine \emph{hedonic coalition formation games} in which
each player's preferences over partitions of players depend only on the members
of his coalition. We present three main results in which restrictions on the
preferences of the players guarantee the existence of stable partitions for
various notions of stability. The preference restrictions pertain to \emph{top
responsiveness} and \emph{bottom responsiveness} which model optimistic and
pessimistic behavior of players respectively. The existence results apply to
natural subclasses of \emph{additive separable hedonic games} and \emph{hedonic
games with \B-preferences}. It is also shown that our existence results cannot
be strengthened to the case of stronger known stability concepts."
"We study a model of congestible resources, where pricing and scheduling are
intertwined. Motivated by the problem of pricing cloud instances, we model a
cloud computing service as linked $GI/GI/\cdot$ queuing systems where the
provider chooses to offer a fixed pricing service, a dynamic market based
service, or a hybrid of both, where jobs can be preempted in the market-based
service. Users (jobs), who are heterogeneous in both the value they place on
service and their cost for waiting, then choose between the services offered.
Combining insights from auction theory with queuing theory we are able to
characterize user equilibrium behavior, and show its insensitivity to the
precise market design mechanism used. We then provide theoretical and
simulation based evidence suggesting that a fixed price typically, though not
always, generates a higher expected revenue than the hybrid system for the
provider."
"The Generalized Second Price (GSP) auction is the primary auction used for
monetizing the use of the Internet. It is well-known that truthtelling is not a
dominant strategy in this auction and that inefficient equilibria can arise. In
this paper we study the space of equilibria in GSP, and quantify the efficiency
loss that can arise in equilibria under a wide range of sources of uncertainty,
as well as in the full information setting. The traditional Bayesian game
models uncertainty in the valuations (types) of the participants. The
Generalized Second Price (GSP) auction gives rise to a further form of
uncertainty: the selection of quality factors resulting in uncertainty about
the behavior of the underlying ad allocation algorithm. The bounds we obtain
apply to both forms of uncertainty, and are robust in the sense that they apply
under various perturbations of the solution concept, extending to models with
information asymmetries and bounded rationality in the form of learning
strategies.
  We present a constant bound (2.927) on the factor of the efficiency loss
(\emph{price of anarchy}) of the corresponding game for the Bayesian model of
partial information about other participants and about ad quality factors. For
the full information setting, we prove a surprisingly low upper bound of 1.282
on the price of anarchy over pure Nash equilibria, nearly matching a lower
bound of 1.259 for the case of three advertisers. Further, we do not require
that the system reaches equilibrium, and give similarly low bounds also on the
quality degradation for any no-regret learning outcome. Our conclusion is that
the number of advertisers in the auction has almost no impact on the price of
anarchy, and that the efficiency of GSP is very robust with respect to the
belief and rationality assumptions imposed on the participants."
"We examine the history of cake cutting mechanisms and discuss the efficiency
of their allocations. In the case of piecewise uniform preferences, we define a
game that in the presence of strategic agents has equilibria that are not
dominated by the allocations of any mechanism. We identify that the equilibria
of this game coincide with the allocations of an existing cake cutting
mechanism."
"The development of cooperative relations within and between firms plays an
important role in the successful implementation of business strategy. How to
produce such relations is less well understood. We build on work in relational
contract theory and the evolution of cooperation to examine the conditions
under which group based incentives outperform individual based incentives and
how they produce more cooperative behavior. Group interactions are modeled as
iterated games in which individuals learn optimal strategies under individual
and group based reward mechanisms. The space of possible games is examined and
it is found that, when individual and group interests are not aligned, group
evaluation and reward systems lead to higher group performance and,
counter-intuitively, higher individual performance. Such groups include
individuals who, quite differently to free-riders, sacrifice their own
performance for the good of the group. We discuss the implications of these
results for the design of incentive systems."
"The family of Groves mechanisms, which includes the well-known VCG mechanism
(also known as the Clarke mechanism), is a family of efficient and
strategy-proof mechanisms. Unfortunately, the Groves mechanisms are generally
not budget balanced. That is, under such mechanisms, payments may flow into or
out of the system of the agents, resulting in deficits or reduced utilities for
the agents. We consider the following problem: within the family of Groves
mechanisms, we want to identify mechanisms that give the agents the highest
utilities, under the constraint that these mechanisms must never incur
deficits.
  We adopt a prior-free approach. We introduce two general measures for
comparing mechanisms in prior-free settings. We say that a non-deficit Groves
mechanism $M$ {\em individually dominates} another non-deficit Groves mechanism
$M'$ if for every type profile, every agent's utility under $M$ is no less than
that under $M'$, and this holds with strict inequality for at least one type
profile and one agent. We say that a non-deficit Groves mechanism $M$ {\em
collectively dominates} another non-deficit Groves mechanism $M'$ if for every
type profile, the agents' total utility under $M$ is no less than that under
$M'$, and this holds with strict inequality for at least one type profile. The
above definitions induce two partial orders on non-deficit Groves mechanisms.
We study the maximal elements corresponding to these two partial orders, which
we call the {\em individually undominated} mechanisms and the {\em collectively
undominated} mechanisms, respectively."
"We consider dynamic cooperative games, where the worth of coalitions varies
over time according to the history of allocations. When defining the core of a
dynamic game, we allow the possibility for coalitions to deviate at any time
and thereby to give rise to a new environment. A coalition that considers a
deviation needs to take the consequences into account because from the
deviation point on, the game is no longer played with the original set of
players. The deviating coalition becomes the new grand coalition which, in
turn, induces a new dynamic game. The stage games of the new dynamical game
depend on all previous allocation including those that have materialized from
the deviating time on.
  We define three types of core solutions: fair core, stable core and credible
core. We characterize the first two in case where the instantaneous game
depends on the last allocation (rather than on the whole history of
allocations) and the third in the general case. The analysis and the results
resembles to a great extent the theory of non-cooperative dynamic games."
"Budget feasible mechanism design studies procurement combinatorial auctions
where the sellers have private costs to produce items, and the
buyer(auctioneer) aims to maximize a social valuation function on subsets of
items, under the budget constraint on the total payment. One of the most
important questions in the field is ""which valuation domains admit truthful
budget feasible mechanisms with `small' approximations (compared to the social
optimum)?"" Singer showed that additive and submodular functions have such
constant approximations. Recently, Dobzinski, Papadimitriou, and Singer gave an
O(log^2 n)-approximation mechanism for subadditive functions; they also
remarked that: ""A fundamental question is whether, regardless of computational
constraints, a constant-factor budget feasible mechanism exists for subadditive
functions.""
  We address this question from two viewpoints: prior-free worst case analysis
and Bayesian analysis. For the prior-free framework, we use an LP that
describes the fractional cover of the valuation function; it is also connected
to the concept of approximate core in cooperative game theory. We provide an
O(I)-approximation mechanism for subadditive functions, via the worst case
integrality gap I of LP. This implies an O(log n)-approximation for subadditive
valuations, O(1)-approximation for XOS valuations, and for valuations with a
constant I. XOS valuations are an important class of functions that lie between
submodular and subadditive classes. We give another polynomial time O(log
n/loglog n) sub-logarithmic approximation mechanism for subadditive valuations.
  For the Bayesian framework, we provide a constant approximation mechanism for
all subadditive functions, using the above prior-free mechanism for XOS
valuations as a subroutine. Our mechanism allows correlations in the
distribution of private information and is universally truthful."
"We study an abstract optimal auction problem for a single good or service.
This problem includes environments where agents have budgets, risk preferences,
or multi-dimensional preferences over several possible configurations of the
good (furthermore, it allows an agent's budget and risk preference to be known
only privately to the agent). These are the main challenge areas for auction
theory. A single-agent problem is to optimize a given objective subject to a
constraint on the maximum probability with which each type is allocated,
a.k.a., an allocation rule. Our approach is a reduction from multi-agent
mechanism design problem to collection of single-agent problems. We focus on
maximizing revenue, but our results can be applied to other objectives (e.g.,
welfare).
  An optimal multi-agent mechanism can be computed by a linear/convex program
on interim allocation rules by simultaneously optimizing several single-agent
mechanisms subject to joint feasibility of the allocation rules. For
single-unit auctions, Border \citeyearpar{B91} showed that the space of all
jointly feasible interim allocation rules for $n$ agents is a
$\NumTypes$-dimensional convex polytope which can be specified by $2^\NumTypes$
linear constraints, where $\NumTypes$ is the total number of all agents' types.
Consequently, efficiently solving the mechanism design problem requires a
separation oracle for the feasibility conditions and also an algorithm for
ex-post implementation of the interim allocation rules. We show that the
polytope of jointly feasible interim allocation rules is the projection of a
higher dimensional polytope which can be specified by only $O(\NumTypes^2)$
linear constraints. Furthermore, our proof shows that finding a preimage of the
interim allocation rules in the higher dimensional polytope immediately gives
an ex-post implementation."
"We consider a general class of Bayesian Games where each players utility
depends on his type (possibly multidimensional) and on the strategy profile and
where players' types are distributed independently. We show that if their full
information version for any fixed instance of the type profile is a smooth game
then the Price of Anarchy bound implied by the smoothness property, carries
over to the Bayes-Nash Price of Anarchy. We show how some proofs from the
literature (item bidding auctions, greedy auctions) can be cast as smoothness
proofs or be simplified using smoothness. For first price item bidding with
fractionally subadditive bidders we actually manage to improve by much the
existing result \cite{Hassidim2011a} from 4 to $\frac{e}{e-1}\approx 1.58$.
This also shows a very interesting separation between first and second price
item bidding since second price item bidding has PoA at least 2 even under
complete information. For a larger class of Bayesian Games where the strategy
space of a player also changes with his type we are able to show that a
slightly stronger definition of smoothness also implies a Bayes-Nash PoA bound.
We show how weighted congestion games actually satisfy this stronger definition
of smoothness. This allows us to show that the inefficiency bounds of weighted
congestion games known in the literature carry over to incomplete versions
where the weights of the players are private information. We also show how an
incomplete version of a natural class of monotone valid utility games, called
effort market games are universally $(1,1)$-smooth. Hence, we show that
incomplete versions of effort market games where the abilities of the players
and their budgets are private information has Bayes-Nash PoA at most 2."
"We study auctions whose bidders are embedded in a social or economic network.
As a result, even bidders who do not win the auction themselves might derive
utility from the auction, namely, when a friend wins. On the other hand, when
an enemy or competitor wins, a bidder might derive negative utility. Such spite
and altruism will alter the bidding strategies. A simple and natural model for
bidders' utilities in these settings posits that the utility of a losing bidder
i as a result of bidder j winning is a constant (positive or negative) fraction
of bidder j's utility."
"We analyze the Schelling model of segregation in which a society of n
individuals live in a ring. Each individual is one of two races and is only
satisfied with his location so long as at least half his 2w nearest neighbors
are of the same race as him. In the dynamics, randomly-chosen unhappy
individuals successively swap locations. We consider the average size of
monochromatic neighborhoods in the final stable state. Our analysis is the
first rigorous analysis of the Schelling dynamics. We note that, in contrast to
prior approximate analyses, the final state is nearly integrated: the average
size of monochromatic neighborhoods is independent of n and polynomial in w."
"Display advertisements on the web are sold via ad exchanges that use real
time auction. We describe the challenges of designing a suitable auction, and
present a simple auction called the Optional Second Price (OSP) auction that is
currently used in Doubleclick Ad Exchange."
"In an epsilon-Nash equilibrium, a player can gain at most epsilon by changing
his behaviour. Recent work has addressed the question of how best to compute
epsilon-Nash equilibria, and for what values of epsilon a polynomial-time
algorithm exists. An epsilon-well-supported Nash equilibrium (epsilon-WSNE) has
the additional requirement that any strategy that is used with non-zero
probability by a player must have payoff at most epsilon less than the best
response. A recent algorithm of Kontogiannis and Spirakis shows how to compute
a 2/3-WSNE in polynomial time, for bimatrix games. Here we introduce a new
technique that leads to an improvement to the worst-case approximation
guarantee."
"In this paper we show that for any mechanism design problem with the
objective of maximizing social welfare, the exponential mechanism can be
implemented as a truthful mechanism while still preserving differential
privacy. Our instantiation of the exponential mechanism can be interpreted as a
generalization of the VCG mechanism in the sense that the VCG mechanism is the
extreme case when the privacy parameter goes to infinity. To our knowledge,
this is the first general tool for designing mechanisms that are both truthful
and differentially private."
"Research regarding the stable marriage and roommate problem has a long and
distinguished history in mathematics, computer science and economics. Stability
in this context is predominantly core stability or one of its variants in which
each deviation is by a group of players. There has been little focus in
matching theory on stability concepts such as Nash stability and individual
stability in which the deviation is by a single player. Such stability concepts
are suitable especially when trust for the other party is limited, complex
coordination is not feasible, or when only unmatched agents can be approached.
Furthermore, weaker stability notions such as individual stability may in
principle circumvent the negative existence and computational complexity
results in matching theory. We characterize the computational complexity of
checking the existence and computing individual-based stable matchings for the
marriage and roommate settings. One of our key computational results for the
stable marriage setting also carries over to different classes of hedonic games
for which individual-based stability has already been of much interest."
"Myerson's classic result provides a full description of how a seller can
maximize revenue when selling a single item. We address the question of revenue
maximization in the simplest possible multi-item setting: two items and a
single buyer who has independently distributed values for the items, and an
additive valuation. In general, the revenue achievable from selling two
independent items may be strictly higher than the sum of the revenues
obtainable by selling each of them separately. In fact, the structure of
optimal (i.e., revenue-maximizing) mechanisms for two items even in this simple
setting is not understood.
  In this paper we obtain approximate revenue optimization results using two
simple auctions: that of selling the items separately, and that of selling them
as a single bundle. Our main results (which are of a ""direct sum"" variety, and
apply to any distributions) are as follows. Selling the items separately
guarantees at least half the revenue of the optimal auction; for identically
distributed items, this becomes at least 73% of the optimal revenue. For the
case of k>2 items, we show that selling separately guarantees at least a
c/log^2(k) fraction of the optimal revenue; for identically distributed items,
the bundling auction yields at least a c/log(k) fraction of the optimal
revenue."
"A game-theoretic model of scrip (artificial currency) systems is analyzed. It
is shown that relative entropy can be used to characterize the distribution of
agent wealth when all agents use threshold strategies---that is, they volunteer
to do work iff they have below a threshold amount of money. Monotonicity of
agents' best-reply functions is used to show that scrip systems have pure
strategy equilibria where all agents use threshold strategies. An algorithm is
given that can compute such an equilibrium and the resulting distribution of
wealth."
"We consider a scenario in which a database stores sensitive data of users and
an analyst wants to estimate statistics of the data. The users may suffer a
cost when their data are used in which case they should be compensated. The
analyst wishes to get an accurate estimate, while the users want to maximize
their utility. We want to design a mechanism that can estimate statistics
accurately without compromising users' privacy.
  Since users' costs and sensitive data may be correlated, it is important to
protect the privacy of both data and cost. We model this correlation by
assuming that a user's unknown sensitive data determines a distribution from a
set of publicly known distributions and a user's cost is drawn from that
distribution. We propose a stronger model of privacy preserving mechanism where
users are compensated whenever they reveal information about their data to the
mechanism. In this model, we design a Bayesian incentive compatible and privacy
preserving mechanism that guarantees accuracy and protects the privacy of both
cost and data."
"Risk is a well-known turn based board game where the primary objective is
nothing less than the world domination. Gameplay is based on battles between
armies located in adjacent territories on the map of Earth. The combat's
outcome is decided by rolling dice, and therefore a probabilistic approach can
be taken. Although several results are derived, the conclusions suggest that
the gameplay is highly depending on luck."
"In resource buying games a set of players jointly buys a subset of a finite
resource set E (e.g., machines, edges, or nodes in a digraph). The cost of a
resource e depends on the number (or load) of players using e, and has to be
paid completely by the players before it becomes available. Each player i needs
at least one set of a predefined family S_i in 2^E to be available. Thus,
resource buying games can be seen as a variant of congestion games in which the
load-dependent costs of the resources can be shared arbitrarily among the
players. A strategy of player i in resource buying games is a tuple consisting
of one of i's desired configurations S_i together with a payment vector p_i in
R^E_+ indicating how much i is willing to contribute towards the purchase of
the chosen resources. In this paper, we study the existence and computational
complexity of pure Nash equilibria (PNE, for short) of resource buying games.
In contrast to classical congestion games for which equilibria are guaranteed
to exist, the existence of equilibria in resource buying games strongly depends
on the underlying structure of the S_i's and the behavior of the cost
functions. We show that for marginally non-increasing cost functions, matroids
are exactly the right structure to consider, and that resource buying games
with marginally non-decreasing cost functions always admit a PNE."
"Focusing on a femtocell communications market, we study the entrant network
service provider's (NSP's) long-term decision: whether to enter the market and
which spectrum sharing technology to select to maximize its profit. This
long-term decision is closely related to the entrant's pricing strategy and the
users' aggregate demand, which we model as medium-term and short-term
decisions, respectively. We consider two markets, one with no incumbent and the
other with one incumbent. For both markets, we show the existence and
uniqueness of an equilibrium point in the user subscription dynamics, and
provide a sufficient condition for the convergence of the dynamics. For the
market with no incumbent, we derive upper and lower bounds on the optimal price
and market share that maximize the entrant's revenue, based on which the
entrant selects an available technology to maximize its long-term profit. For
the market with one incumbent, we model competition between the two NSPs as a
non-cooperative game, in which the incumbent and the entrant choose their
market shares independently, and provide a sufficient condition that guarantees
the existence of at least one pure Nash equilibrium. Finally, we formalize the
problem of entry and spectrum sharing scheme selection for the entrant and
provide numerical results to complement our analysis."
"We consider the age-old problem of allocating items among different agents in
a way that is efficient and fair. Two papers, by Dolev et al. and Ghodsi et
al., have recently studied this problem in the context of computer systems.
Both papers had similar models for agent preferences, but advocated different
notions of fairness. We formalize both fairness notions in economic terms,
extending them to apply to a larger family of utilities. Noting that in
settings with such utilities efficiency is easily achieved in multiple ways, we
study notions of fairness as criteria for choosing between different efficient
allocations. Our technical results are algorithms for finding fair allocations
corresponding to two fairness notions: Regarding the notion suggested by Ghodsi
et al., we present a polynomial-time algorithm that computes an allocation for
a general class of fairness notions, in which their notion is included. For the
other, suggested by Dolev et al., we show that a competitive market equilibrium
achieves the desired notion of fairness, thereby obtaining a polynomial-time
algorithm that computes such a fair allocation and solving the main open
problem raised by Dolev et al."
"The buying and selling of information is taking place at a scale
unprecedented in the history of commerce, thanks to the formation of online
marketplaces for user data. Data providing agencies sell user information to
advertisers to allow them to match ads to viewers more effectively. In this
paper we study the design of optimal mechanisms for a monopolistic data
provider to sell information to a buyer, in a model where both parties have
(possibly correlated) private signals about a state of the world, and the buyer
uses information learned from the seller, along with his own signal, to choose
an action (e.g., displaying an ad) whose payoff depends on the state of the
world.
  We provide sufficient conditions under which there is a simple one-round
protocol (i.e. a protocol where the buyer and seller each sends a single
message, and there is a single money transfer) achieving optimal revenue. In
these cases we present a polynomial-time algorithm that computes the optimal
mechanism. Intriguingly, we show that multiple rounds of partial information
disclosure (interleaved by payment to the seller) are sometimes necessary to
achieve optimal revenue if the buyer is allowed to abort his interaction with
the seller prematurely. We also prove some negative results about the inability
of simple mechanisms for selling information to approximate more complicated
ones in the worst case."
"We consider a monopoly seller who optimally auctions a single object to a
single potential buyer, with a known distribution of valuations. We show that a
tight lower bound on the seller's expected revenue is $1/e$ times the geometric
expectation of the buyer's valuation, and that this bound is uniquely achieved
for the equal revenue distribution. We show also that when the valuation's
expectation and geometric expectation are close, then the seller's expected
revenue is close to the expected valuation."
"Weakly acyclic games form a natural generalization of the class of games that
have the finite improvement property (FIP). In such games one stipulates that
from any initial joint strategy some finite improvement path exists. We
classify weakly acyclic games using the concept of a scheduler introduced in
arXiv:1202.2209. We also show that finite games that can be solved by the
iterated elimination of never best response strategies are weakly acyclic.
Finally, we explain how the schedulers allow us to improve the bounds on
finding a Nash equilibrium in a weakly acyclic game."
"The goal of this paper is to critically evaluate a heuristic algorithm for
the Inverse Banzhaf Index problem by Laruelle and Widgr\'en. Few qualitative
results are known about the approximation quality of the heuristics for this
problem. The intuition behind the operation of this approximation algorithm is
analysed and evaluated. We found that the algorithm can not handle general
inputs well, and often fails to improve inputs. It is also shown to diverge
after only tens of iterations. We present three alternative extensions of the
algorithm that do not alter the complexity but can result in up to a factor 6.5
improvement in solution quality."
"The Maskin's theorem is a fundamental work in the theory of mechanism design.
In this paper, we propose that if agents report messages to the designer
through channels (e.g., Internet), agents can construct a self-enforcing
agreement such that any Pareto-inefficient social choice rule satisfying
monotonicity and no-veto will not be Nash implementable when an additional
condition is satisfied. The key points are: 1) The agreement is unobservable to
the designer, and the designer cannot prevent the agents from constructing such
agreement; 2) The agents act non-cooperatively, and the Maskin mechanism remain
unchanged from the designer's perspective."
"Alice and Bob want to cut a cake; however, in contrast to the usual problems
of fair division, they want to cut it unfairly. More precisely, they want to
cut it in ratio $(a:b)$. (We can assume gcd(a,b)=1.) Let f(a,b) be the number
of cuts will this take (assuming both act in their own self interest). It is
known that f(a,b) \le \ceil{lg(a+b)}. We show that (1) for all a,b, f(a,b) \ge
lg(lg(a+b)) + (2) for an infinite number of (a,b), f(a,b) \le 1+lg(lg(a+b)."
"Changes in payoffs can transform Prisoner's Dilemma and other social dilemmas
into harmonious win-win games. Using the Robinson-Goforth topology of 2x2
games, this paper analyzes how payoff swaps turn Prisoner's Dilemma into other
games, compares Prisoner's Dilemmas with other families of games, traces paths
that affect the difficulty of transforming Prisoner's Dilemma and other social
dilemmas into win-win games, and shows how ties connect simpler and more
complex games. Charts illustrate the relationships between the 144 strict
ordinal 2x2 games, the 38 symmetric 2x2 ordinal games with and without ties,
and the complete set of 1,413 2x2 ordinal games. Payoffs from the symmetric
ordinal 2x2 games combine to form asymmetric games, generating coordinates for
a simple labeling scheme to uniquely identify and locate all asymmetric ordinal
2x2 games. The expanded topology elegantly maps relationships between 2x2 games
with and without ties, enables a systematic understanding of the potential for
transformations in social dilemmas and other strategic interactions, offers a
tool for institutional analysis and design, and locates a variety of
interesting games for further research."
"Schulze voting is a recently introduced voting system enjoying unusual
popularity and a high degree of real-world use, with users including the
Wikimedia foundation, several branches of the Pirate Party, and MTV. It is a
Condorcet voting system that determines the winners of an election using
information about paths in a graph representation of the election. We resolve
the complexity of many electoral control cases for Schulze voting. We find that
it falls short of the best known voting systems in terms of control resistance,
demonstrating vulnerabilities of concern to some prospective users of the
system."
"Auctions in which agents' payoffs are random variables have received
increased attention in recent years. In particular, recent work in algorithmic
mechanism design has produced mechanisms employing internal randomization,
partly in response to limitations on deterministic mechanisms imposed by
computational complexity. For many of these mechanisms, which are often
referred to as truthful-in-expectation, incentive compatibility is contingent
on the assumption that agents are risk-neutral. These mechanisms have been
criticized on the grounds that this assumption is too strong, because ""real""
agents are typically risk averse, and moreover their precise attitude towards
risk is typically unknown a-priori. In response, researchers in algorithmic
mechanism design have sought the design of universally-truthful mechanisms ---
mechanisms for which incentive-compatibility makes no assumptions regarding
agents' attitudes towards risk.
  We show that any truthful-in-expectation mechanism can be generically
transformed into a mechanism that is incentive compatible even when agents are
risk averse, without modifying the mechanism's allocation rule. The transformed
mechanism does not require reporting of agents' risk profiles. Equivalently,
our result can be stated as follows: Every (randomized) allocation rule that is
implementable in dominant strategies when players are risk neutral is also
implementable when players are endowed with an arbitrary and unknown concave
utility function for money."
"We present a polynomial-time algorithm that always finds an (approximate)
Nash equilibrium for repeated two-player stochastic games. The algorithm
exploits the folk theorem to derive a strategy profile that forms an
equilibrium by buttressing mutually beneficial behavior with threats, where
possible. One component of our algorithm efficiently searches for an
approximation of the egalitarian point, the fairest pareto-efficient solution.
The paper concludes by applying the algorithm to a set of grid games to
illustrate typical solutions the algorithm finds. These solutions compare very
favorably to those found by competing algorithms, resulting in strategies with
higher social welfare, as well as guaranteed computational efficiency."
"The intuition that profit is optimized by maximizing marginal revenue is a
guiding principle in microeconomics. In the classical auction theory for agents
with linear utility and single-dimensional preferences, Bulow and Roberts
(1989) show that the optimal auction of Myerson (1981) is in fact optimizing
marginal revenue. In particular Myerson's virtual values are exactly the
derivative of an appropriate revenue curve.
  This paper considers mechanism design in environments where the agents have
multi-dimensional and non-linear preferences. Understanding good auctions for
these environments is considered to be the main challenge in Bayesian optimal
mechanism design. In these environments maximizing marginal revenue may not be
optimal and there is sometimes no direct way to implement the marginal revenue
maximization. Our contributions are three fold: we characterize the settings
for which marginal revenue maximization is optimal (by identifying an important
condition that we call revenue linearity), we give simple procedures for
implementing marginal revenue maximization in general, and we show that
marginal revenue maximization is approximately optimal. Our approximation
factor smoothly degrades in a term that quantifies how far the environment is
from ideal (where marginal revenue maximization is optimal). Because the
marginal revenue mechanism is optimal for single-dimensional agents, our
generalization immediately approximately extends many results for
single-dimensional agents.
  One of the biggest open questions in Bayesian algorithmic mechanism design is
developing methodologies that are not brute-force in the size of the agent type
space. Our methods identify a subproblem that, e.g., for unit-demand agents
with values drawn from product distributions, enables approximation mechanisms
that are polynomial in the dimension."
"In many natural settings agents participate in multiple different auctions
that are not simultaneous. In such auctions, future opportunities affect
strategic considerations of the players. The goal of this paper is to develop a
quantitative understanding of outcomes of such sequential auctions. In earlier
work (Paes Leme et al. 2012) we initiated the study of the price of anarchy in
sequential auctions. We considered sequential first price auctions in the full
information model, where players are aware of all future opportunities, as well
as the valuation of all players. In this paper, we study efficiency in
sequential auctions in the Bayesian environment, relaxing the informational
assumption on the players. We focus on two environments, both studied in the
full information model in Paes Leme et al. 2012, matching markets and matroid
auctions. In the full information environment, a sequential first price cut
auction for matroid settings is efficient. In Bayesian environments this is no
longer the case, as we show using a simple example with three players. Our main
result is a bound of $1+\frac{e}{e-1}\approx 2.58$ on the price of anarchy in
both matroid auctions and single-value matching markets (even with correlated
types) and a bound of $2\frac{e}{e-1}\approx 3.16$ for general matching markets
with independent types. To bound the price of anarchy we need to consider
possible deviations at an equilibrium. In a sequential Bayesian environment the
effect of deviations is more complex than in one-shot games; early bids allow
others to infer information about the player's value. We create effective
deviations despite the presence of this difficulty by introducing a bluffing
technique of independent interest."
"We give an auction for downward-closed environments that generalizes the
random sampling profit extraction auction for digital goods of Fiat et al.
(2002). The mechanism divides the agents in to a market and a sample using a
biased coin and attempts to extract the optimal revenue from the sample from
the market. The latter step is done with the downward-closed profit extractor
of Ha and Hartline (2012). The auction is a 11-approximation to the envyfree
benchmark in downward-closed permutation environments. This is an improvement
on the previously best known results of 12.5 for matroid and 30.4 for
downward-closed permutation environments that are due to Devanur et al. (2012)
and Ha and Hartline (2012), respectively."
"We consider congestion games on networks with nonatomic users and
user-specific costs. We are interested in the uniqueness property defined by
Milchtaich [Milchtaich, I. 2005. Topological conditions for uniqueness of
equilibrium in networks. Math. Oper. Res. 30 225-244] as the uniqueness of
equilibrium flows for all assignments of strictly increasing cost functions. He
settled the case with two-terminal networks. As a corollary of his result, it
is possible to prove that some other networks have the uniqueness property as
well by adding common fictitious origin and destination.
  In the present work, we find a necessary condition for networks with several
origin-destination pairs to have the uniqueness property in terms of excluded
minors or subgraphs. As a key result, we characterize completely bidirectional
rings for which the uniqueness property holds: it holds precisely for nine
networks and those obtained from them by elementary operations. For other
bidirectional rings, we exhibit affine cost functions yielding to two distinct
equilibrium flows. Related results are also proven. For instance, we
characterize networks having the uniqueness property for any choice of
origin-destination pairs."
"We introduce robust learning equilibrium. The idea of learning equilibrium is
that learning algorithms in multi-agent systems should themselves be in
equilibrium rather than only lead to equilibrium. That is, learning equilibrium
is immune to strategic deviations: Every agent is better off using its
prescribed learning algorithm, if all other agents follow their algorithms,
regardless of the unknown state of the environment. However, a learning
equilibrium may not be immune to non strategic mistakes. For example, if for a
certain period of time there is a failure in the monitoring devices (e.g., the
correct input does not reach the agents), then it may not be in equilibrium to
follow the algorithm after the devices are corrected. A robust learning
equilibrium is immune also to such non-strategic mistakes. The existence of
(robust) learning equilibrium is especially challenging when the monitoring
devices are 'weak'. That is, the information available to each agent at each
stage is limited. We initiate a study of robust learning equilibrium with
general monitoring structure and apply it to the context of auctions. We prove
the existence of robust learning equilibrium in repeated first-price auctions,
and discuss its properties."
"Two-player complete-information game trees are perhaps the simplest possible
setting for studying general-sum games and the computational problem of finding
equilibria. These games admit a simple bottom-up algorithm for finding subgame
perfect Nash equilibria efficiently. However, such an algorithm can fail to
identify optimal equilibria, such as those that maximize social welfare. The
reason is that, counterintuitively, probabilistic action choices are sometimes
needed to achieve maximum payoffs. We provide a novel polynomial-time algorithm
for this problem that explicitly reasons about stochastic decisions and
demonstrate its use in an example card game."
"We propose a parametric family of measures of fairness in allocations of
TU-cooperative games. Their definition is based on generalized Renyi Entropy,
is related to the Cowell-Kuga generalized entropy indices in welfare economics,
and aims to parallel the spirit of the notion of price of anarchy in the case
of convex TU-cooperative games.
  Since computing these indices is NP-complete in general, we first upper bound
the performance of a ""reverse greedy"" algorithm for approximately computing
worst-case fairness. The result provides a general additive error guarantee in
terms of two (problem dependent) packing constants. We then particularize this
result to the class of induced subset games. For such games computing
worst-case fairness is NP-complete, and the additive guarantee constant can be
explicitly computed. We compare this result to the performance of an alternate
algorithm based on ""biased orientations""."
"Chinese auctions are a combination between a raffle and an auction and are
held in practice at charity events or festivals. In a Chinese auction, multiple
players compete for several items by buying tickets, which can be used to win
the items. In front of each item there is a basket, and the players can bid by
placing tickets in the basket(s) corresponding to the item(s) they are trying
to win. After all the players have placed their tickets, a ticket is drawn at
random from each basket and the item is given to the owner of the winning
ticket. While a player is never guaranteed to win an item, they can improve
their chances of getting it by increasing the number of tickets for that item.
  In this paper we investigate the existence of pure Nash equilibria in both
the continuous and discrete settings. When the players have continuous budgets,
we show that a pure Nash equilibrium may not exist for asymmetric games when
some valuations are zero. In that case we prove that the auctioneer can
stabilize the game by placing his own ticket in each basket. On the other hand,
when all the valuations are strictly positive, a pure Nash equilibrium is
guaranteed to exist, and the equilibrium strategies are symmetric when both
valuations and budgets are symmetric. We also study Chinese auctions with
discrete budgets, for which we give both existence results and counterexamples.
While the literature on rent-seeking contests traditionally focuses on
continuous costly tickets, the discrete variant is very natural and more
closely models the version of the auction held in practice."
"We analyze the value to e-commerce website operators of offering privacy
options to users, e.g., of allowing users to opt out of ad targeting. In
particular, we assume that site operators have some control over the cost that
a privacy option imposes on users and ask when it is to their advantage to make
such costs low. We consider both the case of a single site and the case of
multiple sites that compete both for users who value privacy highly and for
users who value it less. One of our main results in the case of a single site
is that, under normally distributed utilities, if a privacy-sensitive user is
worth at least $\sqrt{2} - 1$ times as much to advertisers as a
privacy-insensitive user, the site operator should strive to make the cost of a
privacy option as low as possible. In the case of multiple sites, we show how a
Prisoner's-Dilemma situation can arise: In the equilibrium in which both sites
are obliged to offer a privacy option at minimal cost, both sites obtain lower
revenue than they would if they colluded and neither offered a privacy option."
"In this paper we study resource allocation in decentralized information local
public good networks. A network is a local public good network if each user's
actions directly affect the utility of an arbitrary subset of network users. We
consider networks where each user knows only that part of the network that
either affects or is affected by it. Furthermore, each user's utility and
action space are its private information, and each user is a self utility
maximizer. This network model is motivated by several applications including
wireless communications and online advertising. For this network model we
formulate a decentralized resource allocation problem and develop a
decentralized resource allocation mechanism (game form) that possesses the
following properties: (i) All Nash equilibria of the game induced by the
mechanism result in allocations that are optimal solutions of the corresponding
centralized resource allocation problem (Nash implementation). (ii) All users
voluntarily participate in the allocation process specified by the mechanism
(individual rationality). (iii) The mechanism results in budget balance at all
Nash equilibria and off equilibrium."
"We consider infinite-state turn-based stochastic games of two players, Box
and Diamond, who aim at maximizing and minimizing the expected total reward
accumulated along a run, respectively. Since the total accumulated reward is
unbounded, the determinacy of such games cannot be deduced directly from
Martin's determinacy result for Blackwell games. Nevertheless, we show that
these games are determined both for unrestricted (i.e., history-dependent and
randomized) strategies and deterministic strategies, and the equilibrium value
is the same. Further, we show that these games are generally not determined for
memoryless strategies. Then, we consider a subclass of
Diamond-finitely-branching games and show that they are determined for all of
the considered strategy types, where the equilibrium value is always the same.
We also examine the existence and type of (epsilon-)optimal strategies for both
players."
"We consider an extension of strategic normal form games with a phase of
negotiations before the actual play of the game, where players can make binding
offers for transfer of utilities to other players after the play of the game,
in order to provide additional incentives for each other to play designated
strategies. Such offers are conditional on the recipients playing the specified
strategies and they effect transformations of the payoff matrix of the game by
accordingly transferring payoffs between players. We introduce and analyze
solution concepts for 2-player normal form games with such preplay offers under
various assumptions for the preplay negotiation phase and obtain results for
existence of efficient negotiation strategies of the players. Then we extend
the framework to coalitional preplay offers in N-player games, as well as to
extensive form games with inter-play offers for side payments."
"We consider transformations of normal form games by binding preplay offers of
players for payments of utility to other players conditional on them playing
designated in the offers strategies. The game-theoretic effect of such preplay
offers is transformation of the payoff matrix of the game by transferring
payoffs between players. Here we analyze and completely characterize the
possible transformations of the payoff matrix of a normal form game by sets of
preplay offers."
"Subtraction games is a class of combinatorial games. It was solved since the
Sprague-Grundy Theory was put forward. This paper described a new algorithm for
subtraction games. The new algorithm can find win or lost positions in
subtraction games. In addition, it is much simpler than Sprague-Grundy Theory
in one pile of the games."
"We introduce and study strongly truthful mechanisms and their applications.
We use strongly truthful mechanisms as a tool for implementation in undominated
strategies for several problems,including the design of externality resistant
auctions and a variant of multi-dimensional scheduling."
"Cournot dynamical game is studied on a graph. The stability of the system is
studied. Prisoner's dilemma game is used to model natural gas transmission."
"A case study of the Singapore road network provides empirical evidence that
road pricing can significantly affect commuter trip timing behaviors. In this
paper, we propose a model of trip timing decisions that reasonably matches the
observed commuters' behaviors. Our model explicitly captures the difference in
individuals' sensitivity to price, travel time and early or late arrival at
destination. New pricing schemes are suggested to better spread peak travel and
reduce traffic congestion. Simulation results based on the proposed model are
provided in comparison with the real data for the Singapore case study."
"Subtraction games is a class of impartial combinatorial games, They with
finite subtraction sets are known to have periodic nim-sequences. So people try
to find the regular of the games. But for specific of Sprague-Grundy Theory, it
is too difficult to find, they obtained some conclusions just by simple
observing. This paper used PTFN algorithm to analyze the period of the
Subtraction games. It is more suitable than Sprague-Grundy Theory, and this
paper obtained four conclusions by PTFN algorithm . This algorithm provide a
new direction to study the subtraction games' period."
"We study a more powerful variant of false-name manipulation in Internet
auctions: an agent can submit multiple false-name bids, but then, once the
allocation and payments have been decided, withdraw some of her false-name
identities (have some of her false-name identities refuse to pay). While these
withdrawn identities will not obtain the items they won, their initial presence
may have been beneficial to the agent's other identities. We define a mechanism
to be false-name-proof with withdrawal (FNPW) if the aforementioned
manipulation is never beneficial. FNPW is a stronger condition than
false-name-proofness (FNP)."
"We initiate the study of efficient mechanism design with guaranteed good
properties even when players participate in multiple different mechanisms
simultaneously or sequentially. We define the class of smooth mechanisms,
related to smooth games defined by Roughgarden, that can be thought of as
mechanisms that generate approximately market clearing prices. We show that
smooth mechanisms result in high quality outcome in equilibrium both in the
full information setting and in the Bayesian setting with uncertainty about
participants, as well as in learning outcomes. Our main result is to show that
such mechanisms compose well: smoothness locally at each mechanism implies
efficiency globally.
  For mechanisms where good performance requires that bidders do not bid above
their value, we identify the notion of a weakly smooth mechanism. Weakly smooth
mechanisms, such as the Vickrey auction, are approximately efficient under the
no-overbidding assumption. Similar to smooth mechanisms, weakly smooth
mechanisms behave well in composition, and have high quality outcome in
equilibrium (assuming no overbidding) both in the full information setting and
in the Bayesian setting, as well as in learning outcomes.
  In most of the paper we assume participants have quasi-linear valuations. We
also extend some of our results to settings where participants have budget
constraints."
"We show that the multiplicative weight update method provides a simple recipe
for designing and analyzing optimal Bayesian Incentive Compatible (BIC)
auctions, and reduces the time complexity of the problem to pseudo-polynomial
in parameters that depend on single agent instead of depending on the size of
the joint type space. We use this framework to design computationally efficient
optimal auctions that satisfy ex-post Individual Rationality in the presence of
constraints such as (hard, private) budgets and envy-freeness. We also design
optimal auctions when buyers and a seller's utility functions are non-linear.
Scenarios with such functions include (a) auctions with ""quitting rights"", (b)
cost to borrow money beyond budget, (c) a seller's and buyers' risk aversion.
Finally, we show how our framework also yields optimal auctions for variety of
auction settings considered in Cai et al, Alaei et al, albeit with
pseudo-polynomial running times."
"Myerson's seminal work provides a computationally efficient revenue-optimal
auction for selling one item to multiple bidders. Generalizing this work to
selling multiple items at once has been a central question in economics and
algorithmic game theory, but its complexity has remained poorly understood. We
answer this question by showing that a revenue-optimal auction in multi-item
settings cannot be found and implemented computationally efficiently, unless
ZPP contains P^#P. This is true even for a single additive bidder whose values
for the items are independently distributed on two rational numbers with
rational probabilities. Our result is very general: we show that it is hard to
compute any encoding of an optimal auction of any format (direct or indirect,
truthful or non-truthful) that can be implemented in expected polynomial time.
In particular, under well-believed complexity-theoretic assumptions,
revenue-optimization in very simple multi-item settings can only be tractably
approximated.
  We note that our hardness result applies to randomized mechanisms in a very
simple setting, and is not an artifact of introducing combinatorial structure
to the problem by allowing correlation among item values, introducing
combinatorial valuations, or requiring the mechanism to be deterministic (whose
structure is readily combinatorial). Our proof is enabled by a
flow-interpretation of the solutions of an exponential-size linear program for
revenue maximization with an additional supermodularity constraint."
"We present our results on Uniform Price Auctions, one of the standard
sealed-bid multi-unit auction formats, for selling multiple identical units of
a single good to multi-demand bidders. Contrary to the truthful and
economically efficient multi-unit Vickrey auction, the Uniform Price Auction
encourages strategic bidding and is socially inefficient in general. The
uniform pricing rule is, however, widely popular by its appeal to the natural
anticipation, that identical items should be identically priced. In this work
we study equilibria of the Uniform Price Auction for bidders with (symmetric)
submodular valuation functions, over the number of units that they win. We
investigate pure Nash equilibria of the auction in undominated strategies; we
produce a characterization of these equilibria that allows us to prove that a
fraction 1-1/e of the optimum social welfare is always recovered in undominated
pure Nash equilibrium -- and this bound is essentially tight. Subsequently, we
study the auction under the incomplete information setting and prove a bound of
4-2/k on the economic inefficiency of (mixed) Bayes Nash equilibria that are
supported by undominated strategies."
"Driven by both safety concerns and commercial interests, vehicular ad hoc
networks (VANETs) have recently received considerable attentions. In this
paper, we address popular content distribution (PCD) in VANETs, in which one
large popular file is downloaded from a stationary roadside unit (RSU), by a
group of on-board units (OBUs) driving through an area of interest (AoI) along
a highway. Due to high speeds of vehicles and deep fadings of
vehicle-to-roadside (V2R) channels, some of the vehicles may not finish
downloading the entire file but only possess several pieces of it. To
successfully send a full copy to each OBU, we propose a cooperative approach
based on the coalition formation games, in which OBUs exchange their possessed
pieces by broadcasting to and receiving from their neighbors. Simulation
results show that our proposed approach presents a considerable performance
improvement relative to the non-cooperative approach, in which the OBUs
broadcast randomly selected pieces to their neighbors as along as the spectrum
is detected to be unoccupied."
"In this paper we show that the price of stability of Shapley network design
games on undirected graphs with k players is at most (k^3(k+1)/2-k^2) /
(1+k^3(k+1)/2-k^2) H_k = (1 - \Theta(1/k^4)) H_k, where H_k denotes the k-th
harmonic number. This improves on the known upper bound of H_k, which is also
valid for directed graphs but for these, in contrast, is tight. Hence, we give
the first non-trivial upper bound on the price of stability for undirected
Shapley network design games that is valid for an arbitrary number of players.
Our bound is proved by analyzing the price of stability restricted to Nash
equilibria that minimize the potential function of the game. We also present a
game with k=3 players in which such a restricted price of stability is 1.634.
This shows that the analysis of Bil\`o and Bove (Journal of Interconnection
Networks, Volume 12, 2011) is tight. In addition, we give an example for three
players that improves the lower bound on the (unrestricted) price of stability
to 1.571."
"This paper continues the study, initiated by Cole and Fleischer, of the
behavior of a tatonnement price update rule in Ongoing Fisher Markets. The
prior work showed fast convergence toward an equilibrium when the goods
satisfied the weak gross substitutes property and had bounded demand and income
elasticities.
  The current work shows that fast convergence also occurs for the following
types of markets:
  - All pairs of goods are complements to each other, and - the demand and
income elasticities are suitably bounded.
  In particular, these conditions hold when all buyers in the market are
equipped with CES utilities, where all the parameters $\rho$, one per buyer,
satisfy $-1 < \rho \le 0$.
  In addition, we extend the above result to markets in which a mixture of
complements and substitutes occur. This includes characterizing a class of
nested CES utilities for which fast convergence holds.
  An interesting technical contribution, which may be of independent interest,
is an amortized analysis for handling asynchronous events in settings in which
there are a mix of continuous changes and discrete events."
"The rank of a bimatrix game (A,B) is the rank of the matrix A+B. We give a
construction of rank-1 games with exponentially many equilibria, which answers
an open problem by Kannan and Theobald (2010)."
"We present a model of truthful elicitation which generalizes and extends
mechanisms, scoring rules, and a number of related settings that do not quite
qualify as one or the other. Our main result is a characterization theorem,
yielding characterizations for all of these settings, including a new
characterization of scoring rules for non-convex sets of distributions. We
generalize this model to eliciting some property of the agent's private
information, and provide the first general characterization for this setting.
We also show how this yields a new proof of a result in mechanism design due to
Saks and Yu."
"Consider an abstract social choice setting with incomplete information, where
the number of alternatives is large. Albeit natural, implementing VCG
mechanisms may not be feasible due to the prohibitive communication
constraints. However, if players restrict attention to a subset of the
alternatives, feasibility may be recovered.
  This paper characterizes the class of subsets which induce an ex-post
equilibrium in the original game. It turns out that a crucial condition for
such subsets to exist is the existence of a type-independent optimal social
alternative, for each player. We further analyze the welfare implications of
these restrictions.
  This work follows work by Holzman, Kfir-Dahav, Monderer and Tennenholtz
(2004) and Holzman and Monderer (2004) where similar analysis is done for
combinatorial auctions."
"This paper concerns the analysis of the Shapley value in matching games.
Matching games constitute a fundamental class of cooperative games which help
understand and model auctions and assignments. In a matching game, the value of
a coalition of vertices is the weight of the maximum size matching in the
subgraph induced by the coalition. The Shapley value is one of the most
important solution concepts in cooperative game theory.
  After establishing some general insights, we show that the Shapley value of
matching games can be computed in polynomial time for some special cases:
graphs with maximum degree two, and graphs that have a small modular
decomposition into cliques or cocliques (complete k-partite graphs are a
notable special case of this). The latter result extends to various other
well-known classes of graph-based cooperative games.
  We continue by showing that computing the Shapley value of unweighted
matching games is #P-complete in general. Finally, a fully polynomial-time
randomized approximation scheme (FPRAS) is presented. This FPRAS can be
considered the best positive result conceivable, in view of the #P-completeness
result."
"We consider the fundamental mechanism design problem of approximate social
welfare maximization under general cardinal preferences on a finite number of
alternatives and without money. The well-known range voting scheme can be
thought of as a non-truthful mechanism for exact social welfare maximization in
this setting. With m being the number of alternatives, we exhibit a randomized
truthful-in-expectation ordinal mechanism implementing an outcome whose
expected social welfare is at least an Omega(m^{-3/4}) fraction of the social
welfare of the socially optimal alternative. On the other hand, we show that
for sufficiently many agents and any truthful-in-expectation ordinal mechanism,
there is a valuation profile where the mechanism achieves at most an
O(m^{-{2/3}) fraction of the optimal social welfare in expectation. We get
tighter bounds for the natural special case of m = 3, and in that case
furthermore obtain separation results concerning the approximation ratios
achievable by natural restricted classes of truthful-in-expectation mechanisms.
In particular, we show that for m = 3 and a sufficiently large number of
agents, the best mechanism that is ordinal as well as mixed-unilateral has an
approximation ratio between 0.610 and 0.611, the best ordinal mechanism has an
approximation ratio between 0.616 and 0.641, while the best mixed-unilateral
mechanism has an approximation ratio bigger than 0.660. In particular, the best
mixed-unilateral non-ordinal (i.e., cardinal) mechanism strictly outperforms
all ordinal ones, even the non-mixed-unilateral ordinal ones."
"We introduce a new solution concept for selecting optimal strategies in
strategic form games which we call periodic strategies and the solution concept
periodicity. As we will explicitly demonstrate, the periodicity solution
concept has implications for non-trivial realistic games, which renders this
solution concept very valuable. The most striking application of periodicity is
that in mixed strategy strategic form games, we were able to find solutions
that result to values for the utility function of each player, that are equal
to the Nash equilibrium ones, with the difference that in the Nash strategies
playing, the payoffs strongly depend on what the opponent plays, while in the
periodic strategies case, the payoffs of each player are completely robust
against what the opponent plays. We formally define and study periodic
strategies in two player perfect information strategic form games, with pure
strategies and generalize the results to include multiplayer games with perfect
information. We prove that every non-trivial finite game has at least one
periodic strategy, with non-trivial meaning a game with non-degenerate payoffs.
In principle the algorithm we provide, holds true for every non-trivial game,
because in degenerate games, inconsistencies can occur. In addition, we also
address the incomplete information games in the context of Bayesian games, in
which case generalizations of Bernheim's rationalizability offers us the
possibility to embed the periodicity concept in the Bayesian games framework.
Applying the algorithm of periodic strategies in the case where mixed
strategies are used, we find some very interesting outcomes with useful
quantitative features for some classes of games."
"We study optimal equilibria in multi-player games. An equilibrium is optimal
for a player, if her payoff is maximal. A tempting approach to solving this
problem is to seek optimal Nash equilibria, the standard form of equilibria
where no player has an incentive to deviate from her strategy. We argue that a
player with the power to define an equilibrium is in a position, where she
should not be interested in the symmetry of a Nash equilibrium, and ignore the
question of whether or not her outcome can be improved if the other strategies
are fixed. That is, she would only have to make sure that the other players
have no incentive to deviate. This defines a greater class of equilibria, which
may have better (and cannot have worse) optimal equilibria for the designated
powerful player. We apply this strategy to concurrent bimatrix games and to
turn based multi-player mean-payoff games. For the latter, we show that such
political equilibria as well as Nash equilibria always exist, and provide
simple examples where the political equilibrium is superior. We show that
constructing political and Nash equilibria are NP-complete problems. We also
show that, for a fixed number of players, the hardest part is to solve the
underlying two-player mean-payoff games: using an MPG oracle, the problem is
solvable in polynomial time. It is therefore in UP and CoUP, and can be solved
in pseudo polynomial and expected subexponential time."
"We study the paradigmatic fair division problem of allocating a divisible
good among agents with heterogeneous preferences, commonly known as cake
cutting. Classical cake cutting protocols are susceptible to manipulation. Do
their strategic outcomes still guarantee fairness?
  To address this question we adopt a novel algorithmic approach, by designing
a concrete computational framework for fair division---the class of Generalized
Cut and Choose (GCC) protocols}---and reasoning about the game-theoretic
properties of algorithms that operate in this model. The class of GCC protocols
includes the most important discrete cake cutting protocols, and turns out to
be compatible with the study of fair division among strategic agents. In
particular, GCC protocols are guaranteed to have approximate subgame perfect
Nash equilibria, or even exact equilibria if the protocol's tie-breaking rule
is flexible. We further observe that the (approximate) equilibria of
proportional GCC protocols---which guarantee each of the $n$ agents a
$1/n$-fraction of the cake---must be (approximately) proportional. Finally, we
design a protocol in this framework with the property that its Nash equilibrium
allocations coincide with the set of (contiguous) envy-free allocations."
"We introduce a framework for studying the effect of cooperation on the
quality of outcomes in utility games. Our framework is a coalitional analog of
the smoothness framework of non-cooperative games. Coalitional smoothness
implies bounds on the strong price of anarchy, the loss of quality of
coalitionally stable outcomes, as well as bounds on coalitional versions of
coarse correlated equilibria and sink equilibria, which we define as
out-of-equilibrium myopic behavior as determined by a natural coalitional
version of best-response dynamics.
  Our coalitional smoothness framework captures existing results bounding the
strong price of anarchy of network design games. We show that in any monotone
utility-maximization game, if each player's utility is at least his marginal
contribution to the welfare, then the strong price of anarchy is at most 2.
This captures a broad class of games, including games with a very high price of
anarchy. Additionally, we show that in potential games the strong price of
anarchy is close to the price of stability, the quality of the best Nash
equilibrium."
"We introduce the notion of exchangeable equilibria of a symmetric bimatrix
game, defined as those correlated equilibria in which players' strategy choices
are conditionally independently and identically distributed given some hidden
variable. We give several game-theoretic interpretations and a version of the
""revelation principle"". Geometrically, the set of exchangeable equilibria is
convex and lies between the symmetric Nash equilibria and the symmetric
correlated equilibria. Exchangeable equilibria can achieve higher expected
utility than symmetric Nash equilibria."
"We study the structure and evolution of the Internet's Autonomous System (AS)
interconnection topology as a game with heterogeneous players. In this network
formation game, the utility of a player depends on the network structure, e.g.,
the distances between nodes and the cost of links. We analyze static properties
of the game, such as the prices of anarchy and stability and provide explicit
results concerning the generated topologies. Furthermore, we discuss dynamic
aspects, demonstrating linear convergence rate and showing that only a
restricted subset of equilibria is feasible under realistic dynamics. We also
consider the case where utility (or monetary) transfers are allowed between the
players."
"Social dilemmas are situations in which collective interests are at odds with
private interests: pollution, depletion of natural resources, and intergroup
conflicts, are at their core social dilemmas.
  Because of their multidisciplinarity and their importance, social dilemmas
have been studied by economists, biologists, psychologists, sociologists, and
political scientists. These studies typically explain tendency to cooperation
by dividing people in proself and prosocial types, or appealing to forms of
external control or, in iterated social dilemmas, to long-term strategies.
  But recent experiments have shown that cooperation is possible even in
one-shot social dilemmas without forms of external control and the rate of
cooperation typically depends on the payoffs. This makes impossible a
predictive division between proself and prosocial people and proves that people
have attitude to cooperation by nature.
  The key innovation of this article is in fact to postulate that humans have
attitude to cooperation by nature and consequently they do not act a priori as
single agents, as assumed by standard economic models, but they forecast how a
social dilemma would evolve if they formed coalitions and then they act
according to their most optimistic forecast. Formalizing this idea we propose
the first predictive model of human cooperation able to organize a number of
different experimental findings that are not explained by the standard model.
We show also that the model makes satisfactorily accurate quantitative
predictions of population average behavior in one-shot social dilemmas."
"We prove that in a normal form n-player game with m actions for each player,
there exists an approximate Nash equilibrium where each player randomizes
uniformly among a set of O(log(m) + log(n)) pure strategies. This result
induces an $N^{\log \log N}$ algorithm for computing an approximate Nash
equilibrium in games where the number of actions is polynomial in the number of
players (m=poly(n)), where $N=nm^n$ is the size of the game (the input size).
  In addition, we establish an inverse connection between the entropy of Nash
equilibria in the game, and the time it takes to find such an approximate Nash
equilibrium using the random sampling algorithm."
"Since economic mechanisms are often applied to very different instances of
the same problem, it is desirable to identify mechanisms that work well in a
wide range of circumstances. We pursue this goal for a position auction setting
and specifically seek mechanisms that guarantee good outcomes under both
complete and incomplete information. A variant of the generalized first-price
mechanism with multi-dimensional bids turns out to be the only standard
mechanism able to achieve this goal, even when types are one-dimensional. The
fact that expressiveness beyond the type space is both necessary and sufficient
for this kind of robustness provides an interesting counterpoint to previous
work on position auctions that has highlighted the benefits of simplicity. From
a technical perspective our results are interesting because they establish
equilibrium existence for a multi-dimensional bid space, where standard
techniques break down. The structure of the equilibrium bids moreover provides
an intuitive explanation for why first-price payments may be able to support
equilibria in a wider range of circumstances than second-price payments."
"Stackelberg games are a classic example of bilevel optimization problems,
which are often encountered in game theory and economics. These are complex
problems with a hierarchical structure, where one optimization task is nested
within the other. Despite a number of studies on handling bilevel optimization
problems, these problems still remain a challenging territory, and existing
methodologies are able to handle only simple problems with few variables under
assumptions of continuity and differentiability. In this paper, we consider a
special case of a multi-period multi-leader-follower Stackelberg competition
model with non-linear cost and demand functions and discrete production
variables. The model has potential applications, for instance in aircraft
manufacturing industry, which is an oligopoly where a few giant firms enjoy a
tremendous commitment power over the other smaller players. We solve cases with
different number of leaders and followers, and show how the entrance or exit of
a player affects the profits of the other players. In the presence of various
model complexities, we use a computationally intensive nested evolutionary
strategy to find an optimal solution for the model. The strategy is evaluated
on a test-suite of bilevel problems, and it has been shown that the method is
successful in handling difficult bilevel problems."
"Bilevel programming problems are often found in practice. In this paper, we
handle one such bilevel application problem from the domain of environmental
economics. The problem is a Stakelberg game with multiple objectives at the
upper level, and a single objective at the lower level. The leader in this case
is the regulating authority, and it tries to maximize its total tax revenue
over multiple periods while trying to minimize the environmental damages caused
by a mining company. The follower is the mining company whose sole objective is
to maximize its total profit over multiple periods under the limitations set by
the leader. The solution to the model contains the optimal taxation and
extraction decisions to be made by the players in each of the time periods. We
construct a simplistic model for the Stackelberg game and provide an analytical
solution to the problem. Thereafter, the model is extended to incorporate
realism and is solved using a bilevel evolutionary algorithm capable of
handling multiple objectives."
"A 5x5 board is the smallest board on which one can set up all kind of chess
pieces as a start position. We consider Gardner's minichess variant in which
all pieces are set as in a standard chessboard (from Rook to King). This game
has roughly 9x10^{18} legal positions and is comparable in this respect with
checkers. We weakly solve this game, that is we prove its game-theoretic value
and give a strategy to draw against best play for White and Black sides. Our
approach requires surprisingly small computing power. We give a human readable
proof. The way the result is obtained is generic and could be generalized to
bigger chess settings or to other games."
"A mediator implements a correlated equilibrium when it proposes a strategy to
each player confidentially such that the mediator's proposal is the best
interest for every player to follow. In this paper, we present a mediator that
implements the best correlated equilibrium for an extended El Farol game with
symmetric players. The extended El Farol game we consider incorporates both
negative and positive network effects.
  We study the degree to which this type of mediator can decrease the overall
social cost. In particular, we give an exact characterization of Mediation
Value (MV) and Enforcement Value (EV) for this game. MV is the ratio of the
minimum social cost over all Nash equilibria to the minimum social cost over
all mediators, and EV is the ratio of the minimum social cost over all
mediators to the optimal social cost. This sort of exact characterization is
uncommon for games with both kinds of network effects. An interesting outcome
of our results is that both the MV and EV values can be unbounded for our game."
"A central theme in computational social choice is to study the extent to
which voting systems computationally resist manipulative attacks seeking to
influence the outcome of elections, such as manipulation (i.e., strategic
voting), control, and bribery. Bucklin and fallback voting are among the voting
systems with the broadest resistance (i.e., NP-hardness) to control attacks.
However, only little is known about their behavior regarding manipulation and
bribery attacks. We comprehensively investigate the computational resistance of
Bucklin and fallback voting for many of the common manipulation and bribery
scenarios; we also complement our discussion by considering several campaign
management problems for Bucklin and fallback."
"Gale and Sotomayor (1985) have shown that in the Gale-Shapley matching
algorithm (1962), the proposed-to side W (referred to as women there) can
strategically force the W-optimal stable matching as the M-optimal one by
truncating their preference lists, each woman possibly blacklisting all but one
man. As Gusfield and Irving have already noted in 1989, no results are known
regarding achieving this feat by means other than such preference-list
truncation, i.e. by also permuting preference lists.
  We answer Gusfield and Irving's open question by providing tight upper bounds
on the amount of blacklists and their combined size, that are required by the
women to force a given matching as the M-optimal stable matching, or, more
generally, as the unique stable matching. Our results show that the coalition
of all women can strategically force any matching as the unique stable
matching, using preference lists in which at most half of the women have
nonempty blacklists, and in which the average blacklist size is less than 1.
This allows the women to manipulate the market in a manner that is far more
inconspicuous, in a sense, than previously realized. When there are less women
than men, we show that in the absence of blacklists for men, the women can
force any matching as the unique stable matching without blacklisting anyone,
while when there are more women than men, each to-be-unmatched woman may have
to blacklist as many as all men. Together, these results shed light on the
question of how much, if at all, do given preferences for one side a priori
impose limitations on the set of stable matchings under various conditions. All
of the results in this paper are constructive, providing efficient algorithms
for calculating the desired strategies."
"The problem of efficient sharing of a resource is nearly ubiquitous. Except
for pure public goods, each agent's use creates a negative externality; often
the negative externality is so strong that efficient sharing is impossible in
the short run. We show that, paradoxically, the impossibility of efficient
sharing in the short run enhances the possibility of efficient sharing in the
long run, even if outcomes depend stochastically on actions, monitoring is
limited and users are not patient. We base our analysis on the familiar
framework of repeated games with imperfect public monitoring, but we extend the
framework to view the monitoring structure as chosen by a designer who balances
the benefits and costs of more accurate observations and reports. Our
conclusions are much stronger than in the usual folk theorems: we do not
require a rich signal structure or patient users and provide an explicit online
construction of equilibrium strategies."
"This paper provides an analysis of different formal representations of
beliefs in epistemic game theory. The aim is to attempt a synthesis of
different structures of beliefs in the presence of indeterminate probabilities.
Special attention is also paid to the decision-theoretic principle known as the
thesis of no subjective probability for self-action. Conditions in cope with
this principle are given which underlie the interrelationships between
different models of beliefs, and it is shown that under these conditions
different doxastic structures can be coherently unified."
"To ensure that social networks (e.g. opinion consensus, cooperative
estimation, distributed learning and adaptation etc.) proliferate and
efficiently operate, the participating agents need to collaborate with each
other by repeatedly sharing information. However, sharing information is often
costly for the agents while resulting in no direct immediate benefit for them.
Hence, lacking incentives to collaborate, strategic agents who aim to maximize
their own individual utilities will withhold rather than share information,
leading to inefficient operation or even collapse of networks. In this paper,
we develop a systematic framework for designing distributed rating protocols
aimed at incentivizing the strategic agents to collaborate with each other by
sharing information. The proposed incentive protocols exploit the ongoing
nature of the agents' interactions to assign ratings and through them,
determine future rewards and punishments: agents that have behaved as directed
enjoy high ratings -- and hence greater future access to the information of
others; agents that have not behaved as directed enjoy low ratings -- and hence
less future access to the information of others. Unlike existing rating
protocols, the proposed protocol operates in a distributed manner, online, and
takes into consideration the underlying interconnectivity of agents as well as
their heterogeneity. We prove that in many deployment scenarios the price of
anarchy (PoA) obtained by adopting the proposed rating protocols is one. In
settings in which the PoA is larger than one, we show that the proposed rating
protocol still significantly outperforms existing incentive mechanisms such as
Tit-for-Tat. Importantly, the proposed rating protocols can also operate
efficiently in deployment scenarios where the strategic agents interact over
time-varying network topologies where new agents join the network over time."
"We study the efficiency of sequential first-price item auctions at (subgame
perfect) equilibrium. This auction format has recently attracted much
attention, with previous work establishing positive results for unit-demand
valuations and negative results for submodular valuations. This leaves a large
gap in our understanding between these valuation classes. In this work we
resolve this gap on the negative side. In particular, we show that even in the
very restricted case in which each bidder has either an additive valuation or a
unit-demand valuation, there exist instances in which the inefficiency at
equilibrium grows linearly with the minimum of the number of items and the
number of bidders. Moreover, these inefficient equilibria persist even under
iterated elimination of weakly dominated strategies. Our main result implies
linear inefficiency for many natural settings, including auctions with gross
substitute valuations, capacitated valuations, budget-additive valuations, and
additive valuations with hard budget constraints on the payments. Another
implication is that the inefficiency in sequential auctions is driven by the
maximum number of items contained in any player's optimal set, and this is
tight. For capacitated valuations, our results imply a lower bound that equals
the maximum capacity of any bidder, which is tight following the upper-bound
technique established by Paes Leme et al. \cite{PaesLeme2012}."
"We consider a dynamical approach to game in extensive forms. By restricting
the convertibility relation over strategy profiles, we obtain a semi-potential
(in the sense of Kukushkin), and we show that in finite games the corresponding
restriction of better-response dynamics will converge to a Nash equilibrium in
quadratic (finite) time. Convergence happens on a per-player basis, and even in
the presence of players with cyclic preferences, the players with acyclic
preferences will stabilize. Thus, we obtain a candidate notion for rationality
in the presence of irrational agents. Moreover, the restriction of
convertibility can be justified by a conservative updating of beliefs about the
other players strategies.
  For infinite games in extensive form we can retain convergence to a Nash
equilibrium (in some sense), if the preferences are given by continuous payoff
functions; or obtain a transfinite convergence if the outcome sets of the game
are $\Delta^0_2$-sets."
"How users in a dynamic system perform learning and make decision become more
and more important in numerous research fields. Although there are some works
in the social learning literatures regarding how to construct belief on an
uncertain system state, few study has been conducted on incorporating social
learning with decision making. Moreover, users may have multiple concurrent
decisions on different objects/resources and their decisions usually negatively
influence each other's utility, which makes the problem even more challenging.
In this paper, we propose an Indian Buffet Game to study how users in a dynamic
system learn the uncertain system state and make multiple concurrent decisions
by not only considering the current myopic utility, but also taking into
account the influence of subsequent users' decisions. We analyze the proposed
Indian Buffet Game under two different scenarios: customers request multiple
dishes without budget constraint and with budget constraint. For both cases, we
design recursive best response algorithms to find the subgame perfect Nash
equilibrium for customers and characterize special properties of the Nash
equilibrium profile under homogeneous setting. Moreover, we introduce a
non-Bayesian social learning algorithm for customers to learn the system state,
and theoretically prove its convergence. Finally, we conduct simulations to
validate the effectiveness and efficiency of the proposed algorithms."
"This paper presents an analysis of complexity of double dummy bridge. Values
of both, a state-space (search-space) complexity and a game tree complexity
have been estimated.
  -----
  Oszacowanie z{\l}o\.zono\'sci problemu rozgrywki w otwarte karty w bryd\.zu
  Artyku{\l} zawiera analiz{\ke} z{\l}o\.zono\'sci problemu rozgrywki w otwarte
karty w bryd\.zu, przy u\.zyciu miar zaproponowanych przez Louisa Victora
Allisa. Oszacowane s{\ka} w nim z{\l}o\.zono\'sci przestrzeni stan\'ow i drzewa
wspomnianej gry."
"The Generalized Second Price (GSP) auction used typically to model sponsored
search auctions does not include the notion of budget constraints, which is
present in practice. Motivated by this, we introduce the different variants of
GSP auctions that take budgets into account in natural ways. We examine their
stability by focusing on the existence of Nash equilibria and envy-free
assignments. We highlight the differences between these mechanisms and find
that only some of them exhibit both notions of stability. This shows the
importance of carefully picking the right mechanism to ensure stable outcomes
in the presence of budgets"
"We introduce the class of pay or play games, which captures scenarios in
which each decision maker is faced with a choice between two actions: one with
a fixed payoff and an- other with a payoff dependent on others' selected
actions. This is, arguably, the simplest setting that models selection among
certain and uncertain outcomes in a multi-agent system. We study the properties
of equilibria in such games from both a game-theoretic perspective and a
computational perspective. Our main positive result establishes the existence
of a semi-strong equilibrium in every such game. We show that although simple,
pay of play games contain a large variety of well-studied environments, e.g.,
vaccination games. We discuss the interesting implications of our results for
these environments."
"In an epsilon-approximate Nash equilibrium, a player can gain at most epsilon
in expectation by unilateral deviation. An epsilon well-supported approximate
Nash equilibrium has the stronger requirement that every pure strategy used
with positive probability must have payoff within epsilon of the best response
payoff. Daskalakis, Mehta and Papadimitriou conjectured that every win-lose
bimatrix game has a 2/3-well-supported Nash equilibrium that uses supports of
cardinality at most three. Indeed, they showed that such an equilibrium will
exist subject to the correctness of a graph-theoretic conjecture. Regardless of
the correctness of this conjecture, we show that the barrier of a 2/3 payoff
guarantee cannot be broken with constant size supports; we construct win-lose
games that require supports of cardinality at least Omega((log n)^(1/3)) in any
epsilon-well supported equilibrium with epsilon < 2/3. The key tool in showing
the validity of the construction is a proof of a bipartite digraph variant of
the well-known Caccetta-Haggkvist conjecture. A probabilistic argument shows
that there exist epsilon-well-supported equilibria with supports of cardinality
O(log n/(epsilon^2)), for any epsilon> 0; thus, the polylogarithmic cardinality
bound presented cannot be greatly improved. We also show that for any delta >
0, there exist win-lose games for which no pair of strategies with support
sizes at most two is a (1-delta)-well-supported Nash equilibrium. In contrast,
every bimatrix game with payoffs in [0,1] has a 1/2-approximate Nash
equilibrium where the supports of the players have cardinality at most two."
"In some games, additional information hurts a player, e.g., in games with
first-mover advantage, the second-mover is hurt by seeing the first-mover's
move. What properties of a game determine whether it has such negative ""value
of information"" for a particular player? Can a game have negative value of
information for all players? To answer such questions, we generalize the
definition of marginal utility of a good to define the marginal utility of a
parameter vector specifying a game. So rather than analyze the global structure
of the relationship between a game's parameter vector and player behavior, as
in previous work, we focus on the local structure of that relationship. This
allows us to prove that generically, every game can have negative marginal
value of information, unless one imposes a priori constraints on allowed
changes to the game's parameter vector. We demonstrate these and related
results numerically, and discuss their implications."
"This paper initiates the study of the testable implications of choice data in
settings where agents have privacy preferences. We adapt the standard
conceptualization of consumer choice theory to a situation where the consumer
is aware of, and has preferences over, the information revealed by her choices.
The main message of the paper is that little can be inferred about consumers'
preferences once we introduce the possibility that the consumer has concerns
about privacy. This holds even when consumers' privacy preferences are assumed
to be monotonic and separable. This motivates the consideration of stronger
assumptions and, to that end, we introduce an additive model for privacy
preferences that does have testable implications."
"We study the design of truthful auctions for selling identical items in
unlimited supply (e.g., digital goods) to n unit demand buyers. This classic
problem stands out from profit-maximizing auction design literature as it
requires no probabilistic assumptions on buyers' valuations and employs the
framework of competitive analysis. Our objective is to optimize the worst-case
performance of an auction, measured by the ratio between a given benchmark and
revenue generated by the auction.
  We establish a sufficient and necessary condition that characterizes
competitive ratios for all monotone benchmarks. The characterization identifies
the worst-case distribution of instances and reveals intrinsic relations
between competitive ratios and benchmarks in the competitive analysis. With the
characterization at hand, we show optimal competitive auctions for two natural
benchmarks.
  The most well-studied benchmark $\mathcal{F}^{(2)}(\cdot)$ measures the
envy-free optimal revenue where at least two buyers win. Goldberg et al. [13]
showed a sequence of lower bounds on the competitive ratio for each number of
buyers n. They conjectured that all these bounds are tight. We show that
optimal competitive auctions match these bounds. Thus, we confirm the
conjecture and settle a central open problem in the design of digital goods
auctions. As one more application we examine another economically meaningful
benchmark, which measures the optimal revenue across all limited-supply Vickrey
auctions. We identify the optimal competitive ratios to be
$(\frac{n}{n-1})^{n-1}-1$ for each number of buyers n, that is $e-1$ as $n$
approaches infinity."
"Typically the cost of a product, a good or a service has many components.
Those components come from different complex steps in the supply chain of the
product from sourcing to distribution. This economic point of view also takes
place in the determination of goods and services in wireless networks. Indeed,
before transmitting customer data, a network operator has to lease some
frequency range from a spectrum owner and also has to establish agreements with
electricity suppliers. The goal of this paper is to compare two pricing
schemes, namely a power-based and a flat rate, and give a possible explanation
why flat rate pricing schemes are more common than power based pricing ones in
a deregulated wireless market. We suggest a hierarchical game-theoretical model
of a three level supply chain: the end users, the service provider and the
spectrum owner. The end users intend to transmit data on a wireless network.
The amount of traffic sent by the end users depends on the available frequency
bandwidth as well as the price they have to pay for their transmission. A
natural question arises for the service provider: how to design an efficient
pricing scheme in order to maximize his profit. Moreover he has to take into
account the lease charge he has to pay to the spectrum owner and how many
frequency bandwidth to rent. The spectrum owner itself also looks for
maximizing its profit and has to determine the lease price to the service
provider. The equilibrium at each level of our supply chain model are
established and several properties are investigated. In particular, in the case
of a power-based pricing scheme, the service provider and the spectrum owner
tend to share the gross provider profit. Whereas, considering the flat rate
pricing scheme, if the end users are going to exploit the network intensively,
then the tariffs of the suppliers (spectrum owner and service provider)
explode."
"We consider a single buyer with a combinatorial preference that would like to
purchase related products and services from different vendors, where each
vendor supplies exactly one product. We study the general case where subsets of
products can be substitutes as well as complementary and analyze the game that
is induced on the vendors, where a vendor's strategy is the price that he asks
for his product. This model generalizes both Bertrand competition (where
vendors are perfect substitutes) and Nash bargaining (where they are perfect
complements), and captures a wide variety of scenarios that can appear in
complex crowd sourcing or in automatic pricing of related products.
  We study the equilibria of such games and show that a pure efficient
equilibrium always exists. In the case of submodular buyer preferences we fully
characterize the set of pure Nash equilibria, essentially showing uniqueness.
For the even more restricted ""substitutes"" buyer preferences we also prove
uniqueness over {\em mixed} equilibria. Finally we begin the exploration of
natural generalizations of our setting such as when services have costs, when
there are multiple buyers or uncertainty about the the buyer's valuation, and
when a single vendor supplies multiple products."
"Under the assumption of complete rationality, Nash equilibrium is the only
reasonable strategy (set) of the finitely repeated prisoner's dilemma. In fact,
some strategies only slightly deviate from the so-called rationality, and the
corresponding payoff may much better than that of Nash equilibrium. This
article points out, even under the rational assumptions, the players have
reason to seek a mutually beneficial agreement (Pareto dominated compare to
Nash equilibrium) and a weak and optional constraints, so that the agreement
can be successfully implemented. If the constraint does not harm the interests
of the participants, or the adversely affects of the constraint are negligible,
then the finitely repeated prisoner's dilemma becomes a bargaining problem
issues on the strategy sequences and the problem to seek the constraints. The
quantification of the constraints, the so-called security deposit in this
paper, is nearly a concept of distance from an agreement (a strategy set) to
the complete rationality."
"In this report we construct two mechanisms that fully implement social
welfare maximising allocation in Nash equilibria for the case of a single
infinitely divisible good subject to multiple inequality constraints. The first
mechanism achieves weak budget balance, while the second is an extension of the
first, and achieves strong budget balance. One important application of this
mechanism is unicast service on the Internet where a network operator wishes to
allocate rates among strategic users in such a way that maximise overall user
satisfaction while respecting capacity constraints on every link in the
network. The emphasis of this work is on full implementation, which means that
all Nash equilibria of the induced game result in the optimal allocations of
the centralized allocation problem."
"We study algorithms for combinatorial market design problems, where a set of
heterogeneous and indivisible objects are priced and sold to potential buyers
subject to equilibrium constraints. Extending the CWE notion introduced by
Feldman et al. [STOC 2013], we introduce the concept of a Market-Clearing
Combinatorial Walrasian Equilibium (MC-CWE) as a natural relaxation of the
classical Walrasian equilibrium (WE) solution concept. The only difference
between a MC-CWE and a WE is the ability for the seller to bundle the items
prior to sale. This innocuous and natural bundling operation imposes a plethora
of algorithmic and economic challenges and opportunities. Unlike WE, which is
guaranteed to exist only for (gross) substitutes valuations, a MC-CWE always
exists. The main algorithmic challenge, therefore, is to design computationally
efficient mechanisms that generate MC-CWE outcomes that approximately maximize
social welfare. For a variety of valuation classes encompassing substitutes and
complements (including super-additive, single-minded and budget-additive
valuations), we design polynomial-time MC-CWE mechanisms that provide tight
welfare approximation results."
"We derive optimal strategies for a bidding agent that participates in
multiple, simultaneous second-price auctions with perfect substitutes. We prove
that, if everyone else bids locally in a single auction, the global bidder
should always place non-zero bids in all available auctions, provided there are
no budget constraints. With a budget, however, the optimal strategy is to bid
locally if this budget is equal or less than the valuation. Furthermore, for a
wide range of valuation distributions, we prove that the problem of finding the
optimal bids reduces to two dimensions if all auctions are identical. Finally,
we address markets with both sequential and simultaneous auctions,
non-identical auctions, and the allocative efficiency of the market."
"Voting is a general method for aggregating the preferences of multiple
agents. Each agent ranks all the possible alternatives, and based on this, an
aggregate ranking of the alternatives (or at least a winning alternative) is
produced. However, when there are many alternatives, it is impractical to
simply ask agents to report their complete preferences. Rather, the agents
preferences, or at least the relevant parts thereof, need to be elicited. This
is done by asking the agents a (hopefully small) number of simple queries about
their preferences, such as comparison queries, which ask an agent to compare
two of the alternatives. Prior work on preference elicitation in voting has
focused on the case of unrestricted preferences. It has been shown that in this
setting, it is sometimes necessary to ask each agent (almost) as many queries
as would be required to determine an arbitrary ranking of the alternatives. In
contrast, in this paper, we focus on single-peaked preferences. We show that
such preferences can be elicited using only a linear number of comparison
queries, if either the order with respect to which preferences are
single-peaked is known, or at least one other agents complete preferences are
known. We show that using a sublinear number of queries does not suffice. We
also consider the case of cardinally single-peaked preferences. For this case,
we show that if the alternatives cardinal positions are known, then an agents
preferences can be elicited using only a logarithmic number of queries;
however, we also show that if the cardinal positions are not known, then a
sublinear number of queries does not suffice. We present experimental results
for all elicitation algorithms. We also consider the problem of only eliciting
enough information to determine the aggregate ranking, and show that even for
this more modest objective, a sublinear number of queries per agent does not
suffice for known ordinal or unknown cardinal positions. Finally, we discuss
whether and how these techniques can be applied when preferences are almost
single-peaked."
"We consider schemes for obtaining truthful reports on a common but hidden
signal from large groups of rational, self-interested agents. One example are
online feedback mechanisms, where users provide observations about the quality
of a product or service so that other users can have an accurate idea of what
quality they can expect. However, (i) providing such feedback is costly, and
(ii) there are many motivations for providing incorrect feedback.
  Both problems can be addressed by reward schemes which (i) cover the cost of
obtaining and reporting feedback, and (ii) maximize the expected reward of a
rational agent who reports truthfully. We address the design of such
incentive-compatible rewards for feedback generated in environments with pure
adverse selection. Here, the correlation between the true knowledge of an agent
and her beliefs regarding the likelihoods of reports of other agents can be
exploited to make honest reporting a Nash equilibrium.
  In this paper we extend existing methods for designing incentive-compatible
rewards by also considering collusion. We analyze different scenarios, where,
for example, some or all of the agents collude. For each scenario we
investigate whether a collusion-resistant, incentive-compatible reward scheme
exists, and use automated mechanism design to specify an algorithm for deriving
an efficient reward mechanism."
"Vickrey-Clarke-Groves (VCG) mechanisms are often used to allocate tasks to
selfish and rational agents. VCG mechanisms are incentive compatible, direct
mechanisms that are efficient (i.e., maximise social utility) and individually
rational (i.e., agents prefer to join rather than opt out). However, an
important assumption of these mechanisms is that the agents will ""always""
successfully complete their allocated tasks. Clearly, this assumption is
unrealistic in many real-world applications, where agents can, and often do,
fail in their endeavours. Moreover, whether an agent is deemed to have failed
may be perceived differently by different agents. Such subjective perceptions
about an agents probability of succeeding at a given task are often captured
and reasoned about using the notion of ""trust"". Given this background, in this
paper we investigate the design of novel mechanisms that take into account the
trust between agents when allocating tasks.
  Specifically, we develop a new class of mechanisms, called ""trust-based
mechanisms"", that can take into account multiple subjective measures of the
probability of an agent succeeding at a given task and produce allocations that
maximise social utility, whilst ensuring that no agent obtains a negative
utility. We then show that such mechanisms pose a challenging new combinatorial
optimisation problem (that is NP-complete), devise a novel representation for
solving the problem, and develop an effective integer programming solution
(that can solve instances with about 2x10^5 possible allocations in 40
seconds)."
"A Nash Equilibrium (NE) is a strategy profile resilient to unilateral
deviations, and is predominantly used in the analysis of multiagent systems. A
downside of NE is that it is not necessarily stable against deviations by
coalitions. Yet, as we show in this paper, in some cases, NE does exhibit
stability against coalitional deviations, in that the benefits from a joint
deviation are bounded. In this sense, NE approximates strong equilibrium.
  Coalition formation is a key issue in multiagent systems. We provide a
framework for quantifying the stability and the performance of various
assignment policies and solution concepts in the face of coalitional
deviations. Within this framework we evaluate a given configuration according
to three measures: (i) IR_min: the maximal number alpha, such that there exists
a coalition in which the minimal improvement ratio among the coalition members
is alpha, (ii) IR_max: the maximal number alpha, such that there exists a
coalition in which the maximal improvement ratio among the coalition members is
alpha, and (iii) DR_max: the maximal possible damage ratio of an agent outside
the coalition.
  We analyze these measures in job scheduling games on identical machines. In
particular, we provide upper and lower bounds for the above three measures for
both NE and the well-known assignment rule Longest Processing Time (LPT).
  Our results indicate that LPT performs better than a general NE. However, LPT
is not the best possible approximation. In particular, we present a polynomial
time approximation scheme (PTAS) for the makespan minimization problem which
provides a schedule with IR_min of 1+epsilon for any given epsilon. With
respect to computational complexity, we show that given an NE on m >= 3
identical machines or m >= 2 unrelated machines, it is NP-hard to determine
whether a given coalition can deviate such that every member decreases its
cost."
"We present partial strategyproofness, a new, relaxed notion of
strategyproofness for studying the incentive properties of non-strategyproof
assignment mechanisms. Informally, a mechanism is partially strategyproof if it
makes truthful reporting a dominant strategy for those agents whose preference
intensities differ sufficiently between any two objects. We demonstrate that
partial strategyproofness is axiomatically motivated and that it provides a
unified perspective on incentives in the assignment domain. We then apply it to
derive novel insights about the incentive properties of the Probabilistic
Serial mechanism and different variants of the Boston mechanism."
"We present an incentive-compatible polynomial-time approximation scheme for
multi-unit auctions with general k-minded player valuations. The mechanism
fully optimizes over an appropriately chosen sub-range of possible allocations
and then uses VCG payments over this sub-range. We show that obtaining a fully
polynomial-time incentive-compatible approximation scheme, at least using VCG
payments, is NP-hard. For the case of valuations given by black boxes, we give
a polynomial-time incentive-compatible 2-approximation mechanism and show that
no better is possible, at least using VCG payments."
"In many multiagent domains a set of agents exert effort towards a joint
outcome, yet the individual effort levels cannot be easily observed. A typical
example for such a scenario is routing in communication networks, where the
sender can only observe whether the packet reached its destination, but often
has no information about the actions of the intermediate routers, which
influences the final outcome. We study a setting where a principal needs to
motivate a team of agents whose combination of hidden efforts stochastically
determines an outcome. In a companion paper we devise and study a basic
combinatorial agency model for this setting, where the principal is restricted
to inducing a pure Nash equilibrium. Here we study various implications of this
restriction. First, we show that, in contrast to the case of observable
efforts, inducing a mixed-strategies equilibrium may be beneficial for the
principal. Second, we present a sufficient condition for technologies for which
no gain can be generated. Third, we bound the principals gain for various
families of technologies. Finally, we study the robustness of mixed equilibria
to coalitional deviations and the computational hardness of the optimal mixed
equilibria."
"Coalitional games serve the purpose of modeling payoff distribution problems
in scenarios where agents can collaborate by forming coalitions in order to
obtain higher worths than by acting in isolation. In the classical Transferable
Utility (TU) setting, coalition worths can be freely distributed amongst
agents. However, in several application scenarios, this is not the case and the
Non-Transferable Utility setting (NTU) must be considered, where additional
application-oriented constraints are imposed on the possible worth
distributions. In this paper, an approach to define NTU games is proposed which
is based on describing allowed distributions via a set of mixed-integer linear
constraints applied to an underlying TU game. It is shown that such games allow
non-transferable conditions on worth distributions to be specified in a natural
and succinct way. The properties and the relationships among the most prominent
solution concepts for NTU games that hold when they are applied on
(mixed-integer) constrained games are investigated. Finally, a thorough
analysis is carried out to assess the impact of issuing constraints on the
computational complexity of some of these solution concepts."
"In the usual models of cooperative game theory, the outcome of a coalition
formation process is either the grand coalition or a coalition structure that
consists of disjoint coalitions. However, in many domains where coalitions are
associated with tasks, an agent may be involved in executing more than one
task, and thus may distribute his resources among several coalitions. To tackle
such scenarios, we introduce a model for cooperative games with overlapping
coalitions--or overlapping coalition formation (OCF) games. We then explore the
issue of stability in this setting. In particular, we introduce a notion of the
core, which generalizes the corresponding notion in the traditional
(non-overlapping) scenario. Then, under some quite general conditions, we
characterize the elements of the core, and show that any element of the core
maximizes the social welfare. We also introduce a concept of balancedness for
overlapping coalitional games, and use it to characterize coalition structures
that can be extended to elements of the core. Finally, we generalize the notion
of convexity to our setting, and show that under some natural assumptions
convex games have a non-empty core. Moreover, we introduce two alternative
notions of stability in OCF that allow a wider range of deviations, and explore
the relationships among the corresponding definitions of the core, as well as
the classic (non-overlapping) core and the Aubin core. We illustrate the
general properties of the three cores, and also study them from a computational
perspective, thus obtaining additional insights into their fundamental
structure."
"There are p heterogeneous objects to be assigned to n competing agents (n >
p) each with unit demand. It is required to design a Groves mechanism for this
assignment problem satisfying weak budget balance, individual rationality, and
minimizing the budget imbalance. This calls for designing an appropriate rebate
function. When the objects are identical, this problem has been solved which we
refer as WCO mechanism. We measure the performance of such mechanisms by the
redistribution index. We first prove an impossibility theorem which rules out
linear rebate functions with non-zero redistribution index in heterogeneous
object assignment. Motivated by this theorem, we explore two approaches to get
around this impossibility. In the first approach, we show that linear rebate
functions with non-zero redistribution index are possible when the valuations
for the objects have a certain type of relationship and we design a mechanism
with linear rebate function that is worst case optimal. In the second approach,
we show that rebate functions with non-zero efficiency are possible if
linearity is relaxed. We extend the rebate functions of the WCO mechanism to
heterogeneous objects assignment and conjecture them to be worst case optimal."
"There has been significant recent interest in game-theoretic approaches to
security, with much of the recent research focused on utilizing the
leader-follower Stackelberg game model. Among the major applications are the
ARMOR program deployed at LAX Airport and the IRIS program in use by the US
Federal Air Marshals (FAMS). The foundational assumption for using Stackelberg
games is that security forces (leaders), acting first, commit to a randomized
strategy; while their adversaries (followers) choose their best response after
surveillance of this randomized strategy. Yet, in many situations, a leader may
face uncertainty about the follower's surveillance capability. Previous work
fails to address how a leader should compute her strategy given such
uncertainty. We provide five contributions in the context of a general class of
security games. First, we show that the Nash equilibria in security games are
interchangeable, thus alleviating the equilibrium selection problem. Second,
under a natural restriction on security games, any Stackelberg strategy is also
a Nash equilibrium strategy; and furthermore, the solution is unique in a class
of security games of which ARMOR is a key exemplar. Third, when faced with a
follower that can attack multiple targets, many of these properties no longer
hold. Fourth, we show experimentally that in most (but not all) games where the
restriction does not hold, the Stackelberg strategy is still a Nash equilibrium
strategy, but this is no longer true when the attacker can attack multiple
targets. Finally, as a possible direction for future research, we propose an
extensive-form game model that makes the defender's uncertainty about the
attacker's ability to observe explicit."
"We prove new positive and negative results concerning the existence of
truthful and individually rational mechanisms for purchasing private data from
individuals with unbounded and sensitive privacy preferences. We strengthen the
impossibility results of Ghosh and Roth (EC 2011) by extending it to a much
wider class of privacy valuations. In particular, these include privacy
valuations that are based on ({\epsilon}, {\delta})-differentially private
mechanisms for non-zero {\delta}, ones where the privacy costs are measured in
a per-database manner (rather than taking the worst case), and ones that do not
depend on the payments made to players (which might not be observable to an
adversary). To bypass this impossibility result, we study a natural special
setting where individuals have mono- tonic privacy valuations, which captures
common contexts where certain values for private data are expected to lead to
higher valuations for privacy (e.g. having a particular disease). We give new
mech- anisms that are individually rational for all players with monotonic
privacy valuations, truthful for all players whose privacy valuations are not
too large, and accurate if there are not too many players with too-large
privacy valuations. We also prove matching lower bounds showing that in some
respects our mechanism cannot be improved significantly."
"This paper shows that it is computationally hard to decide (or test) if a
consumption data set is consistent with separable preferences."
"This article discusses two contributions to decision-making in complex
partially observable stochastic games. First, we apply two state-of-the-art
search techniques that use Monte-Carlo sampling to the task of approximating a
Nash-Equilibrium (NE) in such games, namely Monte-Carlo Tree Search (MCTS) and
Monte-Carlo Counterfactual Regret Minimization (MCCFR). MCTS has been proven to
approximate a NE in perfect-information games. We show that the algorithm
quickly finds a reasonably strong strategy (but not a NE) in a complex
imperfect information game, i.e. Poker. MCCFR on the other hand has theoretical
NE convergence guarantees in such a game. We apply MCCFR for the first time in
Poker. Based on our experiments, we may conclude that MCTS is a valid approach
if one wants to learn reasonably strong strategies fast, whereas MCCFR is the
better choice if the quality of the strategy is most important. Our second
contribution relates to the observation that a NE is not a best response
against players that are not playing a NE. We present Monte-Carlo Restricted
Nash Response (MCRNR), a sample-based algorithm for the computation of
restricted Nash strategies. These are robust best-response strategies that (1)
exploit non-NE opponents more than playing a NE and (2) are not (overly)
exploitable by other strategies. We combine the advantages of two
state-of-the-art algorithms, i.e. MCCFR and Restricted Nash Response (RNR).
MCRNR samples only relevant parts of the game tree. We show that MCRNR learns
quicker than standard RNR in smaller games. Also we show in Poker that MCRNR
learns robust best-response strategies fast, and that these strategies exploit
opponents more than playing a NE does."
"The voting system in the Legislative Council of Hong Kong (Legco) is
sometimes unicameral and sometimes bicameral, depending on whether the bill is
proposed by the Hong Kong government. Therefore, although without any
representative within Legco, the Hong Kong government has certain degree of
legislative power --- as if there is a virtual representative of the Hong Kong
government within the Legco. By introducing such a virtual representative of
the Hong Kong government, we show that Legco is a three-dimensional voting
system. We also calculate two power indices of the Hong Kong government through
this virtual representative and consider the $C$-dimension and the
$W$-dimension of Legco. Finally, some implications of this Legco model to the
current constitutional reform in Hong Kong will be given."
"This paper analyzes and compares different incentive mechanisms for a master
to motivate the collaboration of smartphone users on both data acquisition and
distributed computing applications. To collect massive sensitive data from
users, we propose a reward-based collaboration mechanism, where the master
announces a total reward to be shared among collaborators, and the
collaboration is successful if there are enough users wanting to collaborate.
We show that if the master knows the users' collaboration costs, then he can
choose to involve only users with the lowest costs. However, without knowing
users' private information, then he needs to offer a larger total reward to
attract enough collaborators. Users will benefit from knowing their costs
before the data acquisition. Perhaps surprisingly, the master may benefit as
the variance of users' cost distribution increases.
  To utilize smartphones' computation resources to solve complex computing
problems, we study how the master can design an optimal contract by specifying
different task-reward combinations for different user types. Under complete
information, we show that the master involves a user type as long as the
master's preference characteristic outweighs that type's unit cost. All
collaborators achieve a zero payoff in this case. If the master does not know
users' private cost information, however, he will conservatively target at a
smaller group of users with small costs, and has to give most benefits to the
collaborators."
"We consider zero-sum stochastic games with perfect information and finitely
many states and actions. The payoff is computed by a payoff function which
associates to each infinite sequence of states and actions a real number. We
prove that if the the payoff function is both shift-invariant and submixing,
then the game is half-positional, i.e. the first player has an optimal strategy
which is both deterministic and stationary. This result relies on the existence
of $\epsilon$-subgame-perfect equilibria in shift-invariant games, a second
contribution of the paper."
"Analyzing simple and natural price-adjustment processes that converge to a
market equilibrium is a fundamental question in economics. Such an analysis may
have implications in economic theory, computational economics, and distributed
systems. T\^atonnement, proposed by Walras in 1874, is a process by which
prices go up in response to excess demand, and down in response to excess
supply. This paper analyzes the convergence of a time-discrete t\^atonnement
process, a problem that recently attracted considerable attention of computer
scientists. We prove that the simple t\^atonnement process that we consider
converges (efficiently) to equilibrium prices and allocation in markets with
nested CES-Leontief utilities, generalizing some of the previous convergence
proofs for more restricted types of utility functions."
"We consider the multi-unit random assignment problem in which agents express
preferences over objects and objects are allocated to agents randomly based on
the preferences. The most well-established preference relation to compare
random allocations of objects is stochastic dominance (SD) which also leads to
corresponding notions of envy-freeness, efficiency, and weak strategyproofness.
We show that there exists no rule that is anonymous, neutral, efficient and
weak strategyproof. For single-unit random assignment, we show that there
exists no rule that is anonymous, neutral, efficient and weak
group-strategyproof. We then study a generalization of the PS (probabilistic
serial) rule called multi-unit-eating PS and prove that multi-unit-eating PS
satisfies envy-freeness, weak strategyproofness, and unanimity."
"With the rapidly growing demand for the cloud services, a need for efficient
methods to trade computing resources increases. Commonly used fixed-price model
is not always the best approach for trading cloud resources, because of its
inflexible and static nature. Dynamic trading systems, which make use of market
mechanisms, show promise for more efficient resource allocation and pricing in
the cloud. However, most of the existing mechanisms ignore the seller's costs
of providing the resources. In order to address it, we propose a single-sided
market mechanism for trading virtual machine instances in the cloud, where the
cloud provider can express the reservation prices for traded cloud services. We
investigate the theoretical properties of the proposed mechanism and prove that
it is truthful, i.e. the buyers do not have an incentive to lie about their
true valuation of the resources. We perform extensive experiments in order to
investigate the impact of the reserve price on the market outcome. Our
experiments show that the proposed mechanism yields near optimal allocations
and has a low execution time."
"I introduce cooperative product games (CPGs), a cooperative game where every
player has a weight, and the value of a coalition is the product of the weights
of the players in the coalition. I only look at games where the weights are at
least $2$.
  I show that no player in such a game can be a dummy. I show that the game is
convex, and therefore always has a non-empty core. I provide a simple method
for finding a payoff vector in the core."
"We consider a setting where one has to organize one or several group
activities for a set of agents. Each agent will participate in at most one
activity, and her preferences over activities depend on the number of
participants in the activity. The goal is to assign agents to activities based
on their preferences. We put forward a general model for this setting, which is
a natural generalization of anonymous hedonic games. We then focus on a special
case of our model, where agents' preferences are binary, i.e., each agent
classifies all pairs of the form ""(activity, group size)"" into ones that are
acceptable and ones that are not. We formulate several solution concepts for
this scenario, and study them from the computational point of view, providing
hardness results for the general case as well as efficient algorithms for
settings where agents' preferences satisfy certain natural constraints."
"One natural constraint in the sponsored search advertising framework arises
from the fact that there is a limit on the number of available slots,
especially for the popular keywords, and as a result, a significant pool of
advertisers are left out. We study the emergence of diversification in the
adword market triggered by such capacity constraints in the sense that new
market mechanisms, as well as, new for-profit agents are likely to emerge to
combat or to make profit from the opportunities created by shortages in
ad-space inventory. We propose a model where the additional capacity is
provided by for-profit agents (or, mediators), who compete for slots in the
original auction, draw traffic, and run their own sub-auctions. The quality of
the additional capacity provided by a mediator is measured by its {\it fitness}
factor. We compute revenues and payoffs for all the different parties at a {\it
symmetric Nash equilibrium} (SNE) when the mediator-based model is operated by
a mechanism currently being used by Google and Yahoo!, and then compare these
numbers with those obtained at a corresponding SNE for the same mechanism, but
without any mediators involved in the auctions. Such calculations allow us to
determine the value of the additional capacity. Our results show that the
revenue of the auctioneer, as well as the social value (i.e. efficiency),
always increase when mediators are involved; moreover even the payoffs of {\em
all} the bidders will increase if the mediator has a high enough fitness. Thus,
our analysis indicates that there are significant opportunities for
diversification in the internet economy and we should expect it to continue to
develop richer structure, with room for different types of agents and
mechanisms to coexist."
"We propose an abstract approach to coalition formation that focuses on simple
merge and split rules transforming partitions of a group of players. We
identify conditions under which every iteration of these rules yields a unique
partition. The main conceptual tool is a specific notion of a stable partition.
The results are parametrized by a preference relation between partitions of a
group of players and naturally apply to coalitional TU-games, hedonic games and
exchange economy games."
"We study competition between wireless devices with incomplete information
about their opponents. We model such interactions as Bayesian interference
games. Each wireless device selects a power profile over the entire available
bandwidth to maximize its data rate. Such competitive models represent
situations in which several wireless devices share spectrum without any central
authority or coordinated protocol.
  In contrast to games where devices have complete information about their
opponents, we consider scenarios where the devices are unaware of the
interference they cause to other devices. Such games, which are modeled as
Bayesian games, can exhibit significantly different equilibria. We first
consider a simple scenario of simultaneous move games, where we show that the
unique Bayes-Nash equilibrium is where both devices spread their power equally
across the entire bandwidth. We then extend this model to a two-tiered spectrum
sharing case where users act sequentially. Here one of the devices, called the
primary user, is the owner of the spectrum and it selects its power profile
first. The second device (called the secondary user) then responds by choosing
a power profile to maximize its Shannon capacity. In such sequential move
games, we show that there exist equilibria in which the primary user obtains a
higher data rate by using only a part of the bandwidth.
  In a repeated Bayesian interference game, we show the existence of reputation
effects: an informed primary user can bluff to prevent spectrum usage by a
secondary user who suffers from lack of information about the channel gains.
The resulting equilibrium can be highly inefficient, suggesting that
competitive spectrum sharing is highly suboptimal."
"We study a pricing game in multi-hop relay networks where nodes price their
services and route their traffic selfishly and strategically. In this game,
each node (1) announces pricing functions which specify the payments it demands
from its respective customers depending on the amount of traffic they route to
it and (2) allocates the total traffic it receives to its service providers.
The profit of a node is the difference between the revenue earned from
servicing others and the cost of using others' services. We show that the
socially optimal routing of such a game can always be induced by an equilibrium
where no node can increase its profit by unilaterally changing its pricing
functions or routing decision. On the other hand, there may also exist
inefficient equilibria. We characterize the loss of efficiency by deriving the
price of anarchy at inefficient equilibria. We show that the price of anarchy
is finite for oligopolies with concave marginal cost functions, while it is
infinite for general topologies and cost functions."
"This paper considers the problem of how to allocate power among competing
users sharing a frequency-selective interference channel. We model the
interaction between selfish users as a non-cooperative game. As opposed to the
existing iterative water-filling algorithm that studies the myopic users, this
paper studies how a foresighted user, who knows the channel state information
and response strategies of its competing users, should optimize its
transmission strategy. To characterize this multi-user interaction, the
Stackelberg equilibrium is introduced, and the existence of this equilibrium
for the investigated non-cooperative game is shown. We analyze this interaction
in more detail using a simple two-user example, where the foresighted user
determines its transmission strategy by solving as a bi-level program which
allows him to account for the myopic user's response. It is analytically shown
that a foresighted user can improve its performance, if it has the necessary
information about its competitors. Since the optimal solution of Stackelberg
equilibrium is computationally prohibitive, we propose a practical
low-complexity approach based on Lagrangian duality theory. Numerical
simulations verify the performance improvements. Possible ways to acquire the
required information and to extend the formulation to more than two users are
also discussed."
"We study the problem of computing approximate Nash equilibria (epsilon-Nash
equilibria) in normal form games, where the number of players is a small
constant. We consider the approach of looking for solutions with constant
support size. It is known from recent work that in the 2-player case, a
1/2-Nash equilibrium can be easily found, but in general one cannot achieve a
smaller value of epsilon than 1/2. In this paper we extend those results to the
k-player case, and find that epsilon = 1-1/k is feasible, but cannot be
improved upon. We show how stronger results for the 2-player case may be used
in order to slightly improve upon the epsilon = 1-1/k obtained in the k-player
case."
"Sponsored search involves running an auction among advertisers who bid in
order to have their ad shown next to search results for specific keywords.
Currently, the most popular auction for sponsored search is the ""Generalized
Second Price"" (GSP) auction in which advertisers are assigned to slots in the
decreasing order of their ""score,"" which is defined as the product of their bid
and click-through rate. In the past few years, there has been significant
research on the game-theoretic issues that arise in an advertiser's interaction
with the mechanism as well as possible redesigns of the mechanism, but this
ranking order has remained standard.
  From a search engine's perspective, the fundamental question is: what is the
best assignment of advertisers to slots? Here ""best"" could mean ""maximizing
user satisfaction,"" ""most efficient,"" ""revenue-maximizing,"" ""simplest to
interact with,"" or a combination of these. To answer this question we need to
understand the behavior of a search engine user when she sees the displayed
ads, since that defines the commodity the advertisers are bidding on, and its
value. Most prior work has assumed that the probability of a user clicking on
an ad is independent of the other ads shown on the page.
  We propose a simple Markovian user model that does not make this assumption.
We then present an algorithm to determine the most efficient assignment under
this model, which turns out to be different than that of GSP. A truthful
auction then follows from an application of the Vickrey-Clarke-Groves (VCG)
mechanism. Further, we show that our assignment has many of the desirable
properties of GSP that makes bidding intuitive. At the technical core of our
result are a number of insights about the structure of the optimal assignment."
"We study the convergence time of the best response dynamics in
player-specific singleton congestion games. It is well known that this dynamics
can cycle, although from every state a short sequence of best responses to a
Nash equilibrium exists. Thus, the random best response dynamics, which selects
the next player to play a best response uniformly at random, terminates in a
Nash equilibrium with probability one. In this paper, we are interested in the
expected number of best responses until the random best response dynamics
terminates.
  As a first step towards this goal, we consider games in which each player can
choose between only two resources. These games have a natural representation as
(multi-)graphs by identifying nodes with resources and edges with players. For
the class of games that can be represented as trees, we show that the
best-response dynamics cannot cycle and that it terminates after O(n^2) steps
where n denotes the number of resources. For the class of games represented as
cycles, we show that the best response dynamics can cycle. However, we also
show that the random best response dynamics terminates after O(n^2) steps in
expectation.
  Additionally, we conjecture that in general player-specific singleton
congestion games there exists no polynomial upper bound on the expected number
of steps until the random best response dynamics terminates. We support our
conjecture by presenting a family of games for which simulations indicate a
super-polynomial convergence time."
"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising."
"The value of a finite-state two-player zero-sum stochastic game with
limit-average payoff can be approximated to within $\epsilon$ in time
exponential in a polynomial in the size of the game times polynomial in
logarithmic in $\frac{1}{\epsilon}$, for all $\epsilon>0$."
"We consider Young (1985)'s characterization of the Shapley value, and give a
new proof of this axiomatization. Moreover, as applications of the new proof,
we show that Young (1985)'s axiomatization of the Shapley value works on
various well-known subclasses of TU games."
"Viral marketing takes advantage of preexisting social networks among
customers to achieve large changes in behaviour. Models of influence spread
have been studied in a number of domains, including the effect of ""word of
mouth"" in the promotion of new products or the diffusion of technologies. A
social network can be represented by a graph where the nodes are individuals
and the edges indicate a form of social relationship. The flow of influence
through this network can be thought of as an increasing process of active
nodes: as individuals become aware of new technologies, they have the potential
to pass them on to their neighbours. The goal of marketing is to trigger a
large cascade of adoptions. In this paper, we develop a mathematical model that
allows to analyze the dynamics of the cascading sequence of nodes switching to
the new technology. To this end we describe a continuous-time and a
discrete-time models and analyse the proportion of nodes that adopt the new
technology over time."
"When modeling game situations of incomplete information one usually considers
the players' hierarchies of beliefs, a source of all sorts of complications.
Hars\'anyi (1967-68)'s idea henceforth referred to as the ""Hars\'anyi program""
is that hierarchies of beliefs can be replaced by ""types"". The types constitute
the ""type space"". In the purely measurable framework Heifetz and Samet (1998)
formalize the concept of type spaces and prove the existence and the uniqueness
of a universal type space. Meier (2001) shows that the purely measurable
universal type space is complete, i.e., it is a consistent object. With the aim
of adding the finishing touch to these results, we will prove in this paper
that in the purely measurable framework every hierarchy of beliefs can be
represented by a unique element of the complete universal type space."
"In this paper we extend a popular non-cooperative network creation game (NCG)
to allow for disconnected equilibrium networks. There are n players, each is a
vertex in a graph, and a strategy is a subset of players to build edges to. For
each edge a player must pay a cost \alpha, and the individual cost for a player
represents a trade-off between edge costs and shortest path lengths to all
other players. We extend the model to a penalized game (PCG), for which we
reduce the penalty counted towards the individual cost for a pair of
disconnected players to a finite value \beta. Our analysis concentrates on
existence, structure, and cost of disconnected Nash and strong equilibria.
Although the PCG is not a potential game, pure Nash equilibria always and pure
strong equilibria very often exist. We provide tight conditions under which
disconnected Nash (strong) equilibria can evolve. Components of these
equilibria must be Nash (strong) equilibria of a smaller NCG. However, in
contrast to the NCG, for almost all parameter values no tree is a stable
component. Finally, we present a detailed characterization of the price of
anarchy that reveals cases in which the price of anarchy is \Theta(n) and thus
several orders of magnitude larger than in the NCG. Perhaps surprisingly, the
strong price of anarchy increases to at most 4. This indicates that global
communication and coordination can be extremely valuable to overcome socially
inferior topologies in distributed selfish network design."
"We consider approximating the minmax value of a multi-player game in
strategic form. Tightening recent bounds by Borgs et al., we observe that
approximating the value with a precision of epsilon log n digits (for any
constant epsilon>0 is NP-hard, where n is the size of the game. On the other
hand, approximating the value with a precision of c log log n digits (for any
constant c >= 1) can be done in quasi-polynomial time. We consider the
parameterized complexity of the problem, with the parameter being the number of
pure strategies k of the player for which the minmax value is computed. We show
that if there are three players, k=2 and there are only two possible rational
payoffs, the minmax value is a rational number and can be computed exactly in
linear time. In the general case, we show that the value can be approximated
with any polynomial number of digits of accuracy in time n^(O(k)). On the other
hand, we show that minmax value approximation is W[1]-hard and hence not likely
to be fixed parameter tractable. Concretely, we show that if k-CLIQUE requires
time n^(Omega(k)) then so does minmax value computation."
"We introduce a new simple game, which is referred to as the complementary
weighted multiple majority game (C-WMMG for short). C-WMMG models a basic
cooperation rule, the complementary cooperation rule, and can be taken as a
sister model of the famous weighted majority game (WMG for short). In this
paper, we concentrate on the two dimensional C-WMMG. An interesting property of
this case is that there are at most $n+1$ minimal winning coalitions (MWC for
short), and they can be enumerated in time $O(n\log n)$, where $n$ is the
number of players. This property guarantees that the two dimensional C-WMMG is
more handleable than WMG. In particular, we prove that the main power indices,
i.e. the Shapley-Shubik index, the Penrose-Banzhaf index, the Holler-Packel
index, and the Deegan-Packel index, are all polynomially computable. To make a
comparison with WMG, we know that it may have exponentially many MWCs, and none
of the four power indices is polynomially computable (unless P=NP). Still for
the two dimensional case, we show that local monotonicity holds for all of the
four power indices. In WMG, this property is possessed by the Shapley-Shubik
index and the Penrose-Banzhaf index, but not by the Holler-Packel index or the
Deegan-Packel index. Since our model fits very well the cooperation and
competition in team sports, we hope that it can be potentially applied in
measuring the values of players in team sports, say help people give more
objective ranking of NBA players and select MVPs, and consequently bring new
insights into contest theory and the more general field of sports economics. It
may also provide some interesting enlightenments into the design of
non-additive voting mechanisms. Last but not least, the threshold version of
C-WMMG is a generalization of WMG, and natural variants of it are closely
related with the famous airport game and the stable marriage/roommates problem."
"In this paper, we address the selfish bin covering problem, which is greatly
related both to the bin covering problem, and to the weighted majority game.
What we mainly concern is how much the lack of coordination harms the social
welfare. Besides the standard PoA and PoS, which are based on Nash equilibrium,
we also take into account the strong Nash equilibrium, and several other new
equilibria. For each equilibrium, the corresponding PoA and PoS are given, and
the problems of computing an arbitrary equilibrium, as well as approximating
the best one, are also considered."
"In Newcomb's paradox you choose to receive either the contents of a
particular closed box, or the contents of both that closed box and another one.
Before you choose though, an antagonist uses a prediction algorithm to deduce
your choice, and fills the two boxes based on that deduction. Newcomb's paradox
is that game theory's expected utility and dominance principles appear to
provide conflicting recommendations for what you should choose. A recent
extension of game theory provides a powerful tool for resolving paradoxes
concerning human choice, which formulates such paradoxes in terms of Bayes
nets. Here we apply this to ol to Newcomb's scenario. We show that the
conflicting recommendations in Newcomb's scenario use different Bayes nets to
relate your choice and the algorithm's prediction. These two Bayes nets are
incompatible. This resolves the paradox: the reason there appears to be two
conflicting recommendations is that the specification of the underlying Bayes
net is open to two, conflicting interpretations. We then show that the accuracy
of the prediction algorithm in Newcomb's paradox, the focus of much previous
work, is irrelevant. We similarly show that the utility functions of you and
the antagonist are irrelevant. We end by showing that Newcomb's paradox is
time-reversal invariant; both the paradox and its resolution are unchanged if
the algorithm makes its `prediction' \emph{after} you make your choice rather
than before."
"We study a Maker/Breaker game described by Beck. As a result we disprove a
conjecture of Beck on positional games, establish a connection between this
game and SAT and construct an unsatisfiable k-CNF formula with few occurrences
per variable, thereby improving a previous result by Hoory and Szeider and
showing that the bound obtained from the Lovasz Local Lemma is tight up to a
constant factor. The Maker/Breaker game we study is as follows. Maker and
Breaker take turns in choosing vertices from a given n-uniform hypergraph F,
with Maker going first. Maker's goal is to completely occupy a hyperedge and
Breaker tries to avoid this. Beck conjectures that if the maximum neighborhood
size of F is at most 2^(n-1) then Breaker has a winning strategy. We disprove
this conjecture by establishing an n-uniform hypergraph with maximum
neighborhood size 3*2^(n - 3) where Maker has a winning strategy. Moreover, we
show how to construct an n-uniform hypergraph with maximum degree (2^(n-1))/n
where maker has a winning strategy. Finally, we establish a connection between
SAT and the Maker/Breaker game we study. We can use this connection to derive
new results in SAT. Kratochvil, Savicky and Tuza showed that for every k >= 3
there is an integer f(k) such that every (k,f(k))-formula is satisfiable, but
(k,f(k) + 1)-SAT is already NP-complete (it is not known whether f(k) is
computable). Kratochvil, Savicky and Tuza also gave the best known lower bound
f(k) = Omega(2^k/k), which is a consequence of the Lovasz Local Lemma. We prove
that, in fact, f(k) = Theta(2^k/k), improving upon the best known upper bound
O((log k) * 2^k/k) by Hoory and Szeider."
"We consider the Item Pricing problem for revenue maximization in the limited
supply setting, where a single seller with $n$ items caters to $m$ buyers with
unknown subadditive valuation functions who arrive in a sequence. The seller
sets the prices on individual items. Each buyer buys a subset of yet unsold
items that maximizes her utility. Our goal is to design pricing strategies that
guarantee an expected revenue that is within a small factor $\alpha$ of the
maximum possible social welfare -- an upper bound on the maximum revenue that
can be generated. Most earlier work has focused on the unlimited supply
setting, where selling items to some buyer does not affect their availability
to the future buyers. Balcan et. al. (EC 2008) studied the limited supply
setting, giving a randomized strategy that assigns a single price to all items
(uniform strategy), and never changes it (static strategy), that gives an
$2^{O(\sqrt{\log n \log \log n})}$-approximation, and moreover, no static
uniform pricing strategy can give better than $2^{\Omega(\log^{1/4} n)}$-
approximation. We improve this lower bound to $2^{\Omega(sqrt{\log n})}$.
  We consider dynamic uniform strategies, which can change the price upon the
arrival of each buyer but the price on all unsold items is the same at all
times, and static non-uniform strategies, which can assign different prices to
different items but can never change it after setting it initially. We design
such pricing strategies that give a poly-logarithmic approximation to maximum
revenue. Thus in the limited supply setting, our results highlight a strong
separation between the power of dynamic and non-uniform pricing versus static
uniform pricing. To our knowledge, this is the first non-trivial analysis of
dynamic and non-uniform pricing schemes for revenue maximization."
"An important aspect of mechanism design in social choice protocols and
multiagent systems is to discourage insincere and manipulative behaviour. We
examine the computational complexity of false-name manipulation in weighted
voting games which are an important class of coalitional voting games. Weighted
voting games have received increased interest in the multiagent community due
to their compact representation and ability to model coalitional formation
scenarios. Bachrach and Elkind in their AAMAS 2008 paper examined divide and
conquer false-name manipulation in weighted voting games from the point of view
of Shapley-Shubik index. We analyse the corresponding case of the Banzhaf index
and check how much the Banzhaf index of a player increases or decreases if it
splits up into sub-players. A pseudo-polynomial algorithm to find the optimal
split is also provided. Bachrach and Elkind also mentioned manipulation via
merging as an open problem. In the paper, we examine the cases where a player
annexes other players or merges with them to increase their Banzhaf index or
Shapley-Shubik index payoff. We characterize the computational complexity of
such manipulations and provide limits to the manipulation. The annexation
non-monotonicity paradox is also discovered in the case of the Banzhaf index.
The results give insight into coalition formation and manipulation."
"Game theoretic equilibria are mathematical expressions of rationality.
Rational agents are used to model not only humans and their software
representatives, but also organisms, populations, species and genes,
interacting with each other and with the environment. Rational behaviors are
achieved not only through conscious reasoning, but also through spontaneous
stabilization at equilibrium points.
  Formal theories of rationality are usually guided by informal intuitions,
which are acquired by observing some concrete economic, biological, or network
processes. Treating such processes as instances of computation, we reconstruct
and refine some basic notions of equilibrium and rationality from the some
basic structures of computation.
  It is, of course, well known that equilibria arise as fixed points; the point
is that semantics of computation of fixed points seems to be providing novel
methods, algebraic and coalgebraic, for reasoning about them."
"The idea of this paper is an advanced game concept. This concept is expected
to model non-monetary bilateral cooperations between self-interested agents.
Such non-monetary cases are social cooperations like allocation of high level
jobs or sexual relationships among humans. In a barter double auction, there is
a big amount of agents. Every agent has a vector of parameters which specifies
his demand and a vector which specifies his offer. Two agents can achieve a
commitment through barter exchange. The subjective satisfaction level (a number
between 0% and 100%) of an agent is as high as small is the distance between
his demand and the accepted offer. This paper introduces some facets of this
complex game concept."
"This paper investigates inventory management in a multi channel distribution
system consisting of one manufacturer and an arbitrary number of retailers that
face stochastic demand. Existence of the pure Nash equilibrium is proved and
parameter restriction which implies uniqueness of it is derived. Also the
Stackelberg game where the manufacturer plays a roll as a leader is discussed.
Under specified parameter restrictions which guarantee profitability,
sufficient condition for uniqueness of Stackelberg equilibrium is obtained. In
addition comparison with simultaneous move game is made. The result shows that
when whole prices are equal to production cost, manufacturer carries more
inventory than simultaneous move game. Keywords: Inventory management,
Substitution, Nash equilibrium, Stackelberg equilibrium."
"Under certain assumptions in terms of information and models, equilibria
correspond to possible stable outcomes in conflicting or cooperative scenarios
where rational entities interact. For wireless engineers, it is of paramount
importance to be able to predict and even ensure such states at which the
network will effectively operate. In this article, we provide non-exhaustive
methodologies for characterizing equilibria in wireless games in terms of
existence, uniqueness, selection, and efficiency."
"We compare the expected efficiency of revenue maximizing (or {\em optimal})
mechanisms with that of efficiency maximizing ones. We show that the efficiency
of the revenue maximizing mechanism for selling a single item with k +
log_{e/(e-1)} k + 1 bidders is at least as much as the efficiency of the
efficiency maximizing mechanism with k bidders, when bidder valuations are
drawn i.i.d. from a Monotone Hazard Rate distribution. Surprisingly, we also
show that this bound is tight within a small additive constant of 5.7. In other
words, Theta(log k) extra bidders suffice for the revenue maximizing mechanism
to match the efficiency of the efficiency maximizing mechanism, while o(log k)
do not. This is in contrast to the result of Bulow and Klemperer comparing the
revenue of the two mechanisms, where only one extra bidder suffices. More
precisely, they show that the revenue of the efficiency maximizing mechanism
with k+1 bidders is no less than the revenue of the revenue maximizing
mechanism with k bidders.
  We extend our result for the case of selling t identical items and show that
2.2 log k + t Theta(log log k) extra bidders suffice for the revenue maximizing
mechanism to match the efficiency of the efficiency maximizing mechanism.
  In order to prove our results, we do a classification of Monotone Hazard Rate
(MHR) distributions and identify a family of MHR distributions, such that for
each class in our classification, there is a member of this family that is
pointwise lower than every distribution in that class. This lets us prove
interesting structural theorems about distributions with Monotone Hazard Rate."
"Population protocols have been introduced as a model of sensor networks
consisting of very limited mobile agents with no control over their own
movement: A collection of anonymous agents, modeled by finite automata,
interact in pairs according to some rules.
  Predicates on the initial configurations that can be computed by such
protocols have been characterized under several hypotheses.
  We discuss here whether and when the rules of interactions between agents can
be seen as a game from game theory. We do so by discussing several basic
protocols."
"In the matroid buyback problem, an algorithm observes a sequence of bids and
must decide whether to accept each bid at the moment it arrives, subject to a
matroid constraint on the set of accepted bids. Decisions to reject bids are
irrevocable, whereas decisions to accept bids may be canceled at a cost which
is a fixed fraction of the bid value. We present a new randomized algorithm for
this problem, and we prove matching upper and lower bounds to establish that
the competitive ratio of this algorithm, against an oblivious adversary, is the
best possible. We also observe that when the adversary is adaptive, no
randomized algorithm can improve the competitive ratio of the optimal
deterministic algorithm. Thus, our work completely resolves the question of
what competitive ratios can be achieved by randomized algorithms for the
matroid buyback problem."
"Scheideler has shown that peer-to-peer overlays networks can only survive
Byzantine attacks if malicious nodes are not able to predict what is going to
be the topology of the network for a given sequence of join and leave
operations. In this paper we investigate adversarial strategies by following
specific games. Our analysis demonstrates first that an adversary can very
quickly subvert DHT-based overlays by simply never triggering leave operations.
We then show that when all nodes (honest and malicious ones) are imposed on a
limited lifetime, the system eventually reaches a stationary regime where the
ratio of polluted clusters is bounded, independently from the initial amount of
corruption in the system."
"This paper discusses a special type of multi-user communication scenario, in
which users' utilities are linearly impacted by their competitors' actions.
First, we explicitly characterize the Nash equilibrium and Pareto boundary of
the achievable utility region. Second, the price of anarchy incurred by the
non-collaborative Nash strategy is quantified. Third, to improve the
performance in the non-cooperative scenarios, we investigate the properties of
an alternative solution concept named conjectural equilibrium, in which
individual users compensate for their lack of information by forming internal
beliefs about their competitors. The global convergence of the best response
and Jacobi update dynamics that achieve various conjectural equilibria are
analyzed. It is shown that the Pareto boundaries of the investigated linearly
coupled games can be sustained as stable conjectural equilibria if the belief
functions are properly initialized. The investigated models apply to a variety
of realistic applications encountered in the multiple access design, including
wireless random access and flow control."
"We model the interaction of several radio devices aiming to obtain wireless
connectivity by using a set of base stations (BS) as a non-cooperative game.
Each radio device aims to maximize its own spectral efficiency (SE) in two
different scenarios: First, we let each player to use a unique BS (BS
selection) and second, we let them to simultaneously use several BSs (BS
Sharing). In both cases, we show that the resulting game is an exact potential
game. We found that the BS selection game posses multiple Nash equilibria (NE)
while the BS sharing game posses a unique one. We provide fully decentralized
algorithms which always converge to a NE in both games. We analyze the price of
anarchy and the price of stability for the case of BS selection. Finally, we
observed that depending on the number of transmitters, the BS selection
technique might provide a better global performance (network spectral
efficiency) than BS sharing, which suggests the existence of a Braess type
paradox."
"In revenue maximization of selling a digital product in a social network, the
utility of an agent is often considered to have two parts: a private valuation,
and linearly additive influences from other agents. We study the incomplete
information case where agents know a common distribution about others' private
valuations, and make decisions simultaneously. The ""rational behavior"" of
agents in this case is captured by the well-known Bayesian Nash equilibrium.
  Two challenging questions arise: how to compute an equilibrium and how to
optimize a pricing strategy accordingly to maximize the revenue assuming agents
follow the equilibrium? In this paper, we mainly focus on the natural model
where the private valuation of each agent is sampled from a uniform
distribution, which turns out to be already challenging.
  Our main result is a polynomial-time algorithm that can exactly compute the
equilibrium and the optimal price, when pairwise influences are non-negative.
If negative influences are allowed, computing any equilibrium even
approximately is PPAD-hard. Our algorithm can also be used to design an FPTAS
for optimizing discriminative price profile."
"Two standard algorithms for approximately solving two-player zero-sum
concurrent reachability games are value iteration and strategy iteration. We
prove upper and lower bounds of 2^(m^(Theta(N))) on the worst case number of
iterations needed by both of these algorithms for providing non-trivial
approximations to the value of a game with N non-terminal positions and m
actions for each player in each position. In particular, both algorithms have
doubly-exponential complexity. Even when the game given as input has only one
non-terminal position, we prove an exponential lower bound on the worst case
number of iterations needed to provide non-trivial approximations."
"Motivated by P2P networks and content delivery applications, we study
Capacitated Selfish Replication (CSR) games, which involve nodes on a network
making strategic choices regarding the content to replicate in their caches.
Selfish Replication games were introduced in [Chun et al, PODC2004}, who
analyzed the uncapacitated case leaving the capacitated version as an open
direction.
  In this work, we study pure Nash equilibria of CSR games with an emphasis on
hierarchical networks. Our main result is an exact polynomial-time algorithm
for finding a Nash Equilibrium in any hierarchical network using a new
technique which we term ""fictional players"". We show that this technique
extends to a general framework of natural preference orders, orders that are
entirely arbitrary except for two natural constraints - ""Nearer is better"" and
""Independence of irrelevant alternatives"".
  Using our axiomatic framework, we next study CSR games on arbitrary networks
and delineate the boundary between intractability and effective computability
in terms of the network structure, object preferences, and the total number of
objects. We also show the existence of equilibria for general undirected
networks when either object preferences are binary or there are two objects.
For general CSR games, however, we show that it is NP-hard to determine whether
equilibria exist. We also show that the existence of equilibria in strongly
connected networks with two objects and binary object preferences can be
determined in polynomial time via a reduction to the well-studied even-cycle
problem. Finally, we introduce a fractional version of CSR games (F-SCR) with
application to content distribution using erasure codes. We show that while
every F-CSR game instance possesses an equilibrium, finding an equilibrium in
an F-CSR game is PPAD-complete."
"In this note we describe a time symmetric version of the classical game Go.
Time symmetry means that players move simultaneously without knowledge of the
other player's move. In particular the classical Komi rule is removed, as well
as the Ko rule. This is a perfect information game, (up to the fact that the
other playe r's move is not not known on the given turn) to resolve the natural
issues that can occur with simultaneity we use ideas inspired by quantum
mechanics, but instead of a dice roll there is a certain deterministic
``quantum state'' reduction. This requires introduction of 2 new rules."
"We consider the problem of designing truthful auctions, when the bidders'
valuations have a public and a private component. In particular, we consider
combinatorial auctions where the valuation of an agent $i$ for a set $S$ of
items can be expressed as $v_if(S)$, where $v_i$ is a private single parameter
of the agent, and the function $f$ is publicly known. Our motivation behind
studying this problem is two-fold: (a) Such valuation functions arise naturally
in the case of ad-slots in broadcast media such as Television and Radio. For an
ad shown in a set $S$ of ad-slots, $f(S)$ is, say, the number of {\em unique}
viewers reached by the ad, and $v_i$ is the valuation per-unique-viewer. (b)
From a theoretical point of view, this factorization of the valuation function
simplifies the bidding language, and renders the combinatorial auction more
amenable to better approximation factors. We present a general technique, based
on maximal-in-range mechanisms, that converts any $\alpha$-approximation
non-truthful algorithm ($\alpha \leq 1$) for this problem into
$\Omega(\frac{\alpha}{\log{n}})$ and $\Omega(\alpha)$-approximate truthful
mechanisms which run in polynomial time and quasi-polynomial time,
respectively."
"Budget feasible mechanisms, recently initiated by Singer (FOCS 2010), extend
algorithmic mechanism design problems to a realistic setting with a budget
constraint. We consider the problem of designing truthful budget feasible
mechanisms for general submodular functions: we give a randomized mechanism
with approximation ratio $7.91$ (improving the previous best-known result 112),
and a deterministic mechanism with approximation ratio $8.34$. Further we study
the knapsack problem, which is special submodular function, give a $2+\sqrt{2}$
approximation deterministic mechanism (improving the previous best-known result
6), and a 3 approximation randomized mechanism. We provide a similar result for
an extended knapsack problem with heterogeneous items, where items are divided
into groups and one can pick at most one item from each group.
  Finally we show a lower bound of approximation ratio of $1+\sqrt{2}$ for
deterministic mechanisms and 2 for randomized mechanisms for knapsack, as well
as the general submodular functions. Our lower bounds are unconditional, which
do not rely on any computational or complexity assumptions."
"We present a direct reduction from k-player games to 2-player games that
preserves approximate Nash equilibrium. Previously, the computational
equivalence of computing approximate Nash equilibrium in k-player and 2-player
games was established via an indirect reduction. This included a sequence of
works defining the complexity class PPAD, identifying complete problems for
this class, showing that computing approximate Nash equilibrium for k-player
games is in PPAD, and reducing a PPAD-complete problem to computing approximate
Nash equilibrium for 2-player games. Our direct reduction makes no use of the
concept of PPAD, thus eliminating some of the difficulties involved in
following the known indirect reduction."
"The problem of arriving at a principled method of pricing goods and services
was very satisfactorily solved for conventional goods; however, this solution
is not applicable to digital goods. This paper studies pricing of a special
class of digital goods, which we call {\em semantically substitutable digital
goods}. After taking into consideration idiosyncrasies of goods in this class,
we define a market model for it, together with a notion of equilibrium. We
prove existence of equilibrium prices for our market model using Kakutani's
fixed point theorem.
  The far reaching significance of a competitive equilibrium is made explicit
in the Fundamental Theorems of Welfare Economics. There are basic reasons due
to which these theorems are not applicable to digital goods. This naturally
leads to the question of whether the allocations of conventional goods are
rendered inefficient or ""socially unfair"" in the mixed economy we have
proposed. We prove that that is not the case and that in this sense, the
intended goal of Welfare Economics is still achieved in the mixed economy."
"We study the properties of Braess's paradox in the context of the model of
congestion games with flow over time introduced by Koch and Skutella. We
compare them to the well known properties of Braess's paradox for Wardrop's
model of games with static flows. We show that there are networks which do not
admit Braess's paradox in Wardrop's model, but which admit it in the model with
flow over time. Moreover, there is a topology that admits a much more severe
Braess's ratio for this model. Further, despite its symmetry for games with
static flow, we show that Braess's paradox is not symmetric for flows over
time. We illustrate that there are network topologies which exhibit Braess's
paradox, but for which the transpose does not. Finally, we conjecture a
necessary and sufficient condition of existence of Braess's paradox in a
network, and prove the condition of existence of the paradox either in the
network or in its transpose."
"Ye showed recently that the simplex method with Dantzig pivoting rule, as
well as Howard's policy iteration algorithm, solve discounted Markov decision
processes (MDPs), with a constant discount factor, in strongly polynomial time.
More precisely, Ye showed that both algorithms terminate after at most
$O(\frac{mn}{1-\gamma}\log(\frac{n}{1-\gamma}))$ iterations, where $n$ is the
number of states, $m$ is the total number of actions in the MDP, and
$0<\gamma<1$ is the discount factor. We improve Ye's analysis in two respects.
First, we improve the bound given by Ye and show that Howard's policy iteration
algorithm actually terminates after at most
$O(\frac{m}{1-\gamma}\log(\frac{n}{1-\gamma}))$ iterations. Second, and more
importantly, we show that the same bound applies to the number of iterations
performed by the strategy iteration (or strategy improvement) algorithm, a
generalization of Howard's policy iteration algorithm used for solving 2-player
turn-based stochastic games with discounted zero-sum rewards. This provides the
first strongly polynomial algorithm for solving these games, resolving a long
standing open problem."
"In this article we briefly present our contributions toward Trading Agent
Competition (TAC); an international forum for promotion of research into the
trading agent problems. Moreover, we present some strategies proposed and used
in the development of our TAC Agent and resultant brief information after its
participation in a real time trading environment. In the end we conclude with
needed improvements and future recommendations."
"For revenue and welfare maximization in single-dimensional Bayesian settings,
Chawla et al. (STOC10) recently showed that sequential posted-price mechanisms
(SPMs), though simple in form, can perform surprisingly well compared to the
optimal mechanisms. In this paper, we give a theoretical explanation of this
fact, based on a connection to the notion of correlation gap.
  Loosely speaking, for auction environments with matroid constraints, we can
relate the performance of a mechanism to the expectation of a monotone
submodular function over a random set. This random set corresponds to the
winner set for the optimal mechanism, which is highly correlated, and
corresponds to certain demand set for SPMs, which is independent. The notion of
correlation gap of Agrawal et al.\ (SODA10) quantifies how much we {}""lose"" in
the expectation of the function by ignoring correlation in the random set, and
hence bounds our loss in using certain SPM instead of the optimal mechanism.
Furthermore, the correlation gap of a monotone and submodular function is known
to be small, and it follows that certain SPM can approximate the optimal
mechanism by a good constant factor.
  Exploiting this connection, we give tight analysis of a greedy-based SPM of
Chawla et al.\ for several environments. In particular, we show that it gives
an $e/(e-1)$-approximation for matroid environments, gives asymptotically a
$1/(1-1/\sqrt{2\pi k})$-approximation for the important sub-case of $k$-unit
auctions, and gives a $(p+1)$-approximation for environments with
$p$-independent set system constraints."
"One major function of social networks (e.g., massive online social networks)
is the dissemination of information such as scientific knowledge, news, and
rumors. Information can be propagated by the users of the network via natural
connections in written, oral or electronic form. The information passing from a
sender to a receiver intrinsically involves both of them considering their
self-perceived knowledge, reputation, and popularity, which further determine
their decisions of whether or not to forward the information and whether or not
to provide feedback. To understand such human aspects of the information
dissemination, we propose a game theoretical model of the two-way full duplex
information forwarding and feedback mechanisms in a social network that take
into account the personalities of the communicating actors (including their
perceived knowledgeability, reputation, and desire for popularity) and the
global characteristics of the network. The model demonstrates how the emergence
of social networks can be explained in terms of maximizing game theoretical
utility."
"The revelation principle has been known in the economics society for decades.
In this paper, I will investigate it from an energy perspective, i.e.,
considering the energy consumed by agents and the designer in participating a
mechanism. The main result is that when the strategies of agents are actions
rather than messages, an additional energy condition should be added to make
the revelation principle hold in the real world."
"In this paper, we introduce a distributed dynamic routing algorithm for
secondary users (SUs) to minimize their interference with the primary users
(PUs) in multi-hop cognitive radio (CR) networks. We use the medial axis with a
relaxation factor as a reference path which is contingent on the states of the
PUs. Along the axis, we construct a hierarchical structure for multiple sources
to reach cognitive pilot channel (CPC) base stations. We use a temporal and
spatial dynamic non-cooperative game to model the interactions among SUs as
well as their influences from PUs in the multi-hop structure of the network. A
multi-stage fictitious play learning is used for distributed routing in
multi-hop CR networks. We obtain a set of mixed (behavioral) Nash equilibrium
strategies of the dynamic game in closed form by backward induction. The
proposed algorithm minimizes the overall interference and the average packet
delay along the routing path from SU nodes to CPC base stations in an optimal
and distributed manner"
"An updated version of this paper (but with a different title) can be found at
arXiv:1204.4262"
"In considering the college admissions problem, almost fifty years ago, Gale
and Shapley came up with a simple abstraction based on preferences of students
and colleges. They introduced the concept of stability and optimality; and
proposed the deferred acceptance (DA) algorithm that is proven to lead to a
stable and optimal solution. This algorithm is simple and computationally
efficient. Furthermore, in subsequent studies it is shown that the DA algorithm
is also strategy-proof, which means, when the algorithm is played out as a
mechanism for matching two sides (e.g. colleges and students), the parties
(colleges or students) have no incentives to act other than according to their
true preferences. Yet, in practical college admission systems, the DA algorithm
is often not adopted. Instead, an algorithm known as the Boston Mechanism (BM)
or its variants are widely adopted. In BM, colleges accept students without
deferral (considering other colleges' decisions), which is exactly the opposite
of Gale-Shapley's DA algorithm. To explain and rationalize this reality, we
introduce the notion of reciprocating preference to capture the influence of a
student's interest on a college's decision. This model is inspired by the
actual mechanism used to match students to universities in Hong Kong. The
notion of reciprocating preference defines a class of matching algorithms,
allowing different degrees of reciprocating preferences by the students and
colleges. DA and BM are but two extreme cases (with zero and a hundred percent
reciprocation) of this set. This model extends the notion of stability and
optimality as well. As in Gale-Shapley's original paper, we discuss how the
analogy can be carried over to the stable marriage problem, thus demonstrating
the model's general applicability."
"We revisit the problem of designing the profit-maximizing single-item
auction, solved by Myerson in his seminal paper for the case in which bidder
valuations are independently distributed. We focus on general joint
distributions, seeking the optimal deterministic incentive compatible auction.
We give a geometric characterization of the optimal auction, resulting in a
duality theorem and an efficient algorithm for finding the optimal
deterministic auction in the two-bidder case and an NP-completeness result for
three or more bidders."
"We show that every universally truthful randomized mechanism for
combinatorial auctions with submodular valuations that provides $m^{\frac 1 2
-\epsilon}$ approximation to the social welfare and uses value queries only
must use exponentially many value queries, where $m$ is the number of items. In
contrast, ignoring incentives there exist constant ratio approximation
algorithms for this problem. Our approach is based on a novel \emph{direct
hardness} approach and completely skips the notoriously hard characterization
step. The characterization step was the main obstacle for proving impossibility
results in algorithmic mechanism design so far.
  We demonstrate two additional applications of our new technique: (1) an
impossibility result for universally-truthful polynomial time flexible
combinatorial public projects and (2) an impossibility result for
truthful-in-expectation mechanisms for exact combinatorial public projects. The
latter is the first result that bounds the power of polynomial-time truthful in
expectation mechanisms in any setting."
"Building on ideas from online convex optimization, we propose a general
framework for the design of efficient securities markets over very large
outcome spaces. The challenge here is computational. In a complete market, in
which one security is offered for each outcome, the market institution can not
efficiently keep track of the transaction history or calculate security prices
when the outcome space is large. The natural solution is to restrict the space
of securities to be much smaller than the outcome space in such a way that
securities can be priced efficiently. Recent research has focused on searching
for spaces of securities that can be priced efficiently by existing mechanisms
designed for complete markets. While there have been some successes, much of
this research has led to hardness results. In this paper, we take a drastically
different approach. We start with an arbitrary space of securities with bounded
payoff, and establish a framework to design markets tailored to this space. We
prove that any market satisfying a set of intuitive conditions must price
securities via a convex potential function and that the space of reachable
prices must be precisely the convex hull of the security payoffs. We then show
how the convex potential function can be defined in terms of an optimization
over the convex hull of the security payoffs. The optimal solution to the
optimization problem gives the security prices. Using this framework, we
provide an efficient market for predicting the landing location of an object on
a sphere. In addition, we show that we can relax our ""no-arbitrage"" condition
to design a new efficient market maker for pair betting, which is known to be
#P-hard to price using existing mechanisms. This relaxation also allows the
market maker to charge transaction fees so that the depth of the market can be
dynamically increased as the number of trades increases."
"It is well known that a stable matching in a many-to-one matching market with
couples need not exist. We introduce a new matching algorithm for such markets
and show that for a general class of large random markets the algorithm will
find a stable matching with high probability. In particular we allow the number
of couples to grow at a near-linear rate. Furthermore, truth-telling is an
approximated equilibrium in the game induced by the new matching algorithm. Our
results are tight: for markets in which the number of couples grows at a linear
rate, we show that with constant probability no stable matching exists."
"We consider the problem of designing a revenue-maximizing auction for a
single item, when the values of the bidders are drawn from a correlated
distribution. We observe that there exists an algorithm that finds the optimal
randomized mechanism that runs in time polynomial in the size of the support.
We leverage this result to show that in the oracle model introduced by Ronen
and Saberi [FOCS'02], there exists a polynomial time truthful in expectation
mechanism that provides a $(\frac 3 2+\epsilon)$-approximation to the revenue
achievable by an optimal truthful-in-expectation mechanism, and a polynomial
time deterministic truthful mechanism that guarantees $\frac 5 3$ approximation
to the revenue achievable by an optimal deterministic truthful mechanism.
  We show that the $\frac 5 3$-approximation mechanism provides the same
approximation ratio also with respect to the optimal truthful-in-expectation
mechanism. This shows that the performance gap between truthful-in-expectation
and deterministic mechanisms is relatively small. En route, we solve an open
question of Mehta and Vazirani [EC'04].
  Finally, we extend some of our results to the multi-item case, and show how
to compute the optimal truthful-in-expectation mechanisms for bidders with more
complex valuations."
"This short note exhibits a truthful-in-expectation $O(\frac {\log m} {\log
\log m})$-approximation mechanism for combinatorial auctions with subadditive
bidders that uses polynomial communication."
"The Generalized Second Price auction is the primary method by which sponsered
search advertisements are sold. We study the performance of this auction under
various equilibrium concepts. In particular, we demonstrate that the Bayesian
Price of Anarchy is at most $2(1-1/e)^{-1} \approx 3.16$, significantly
improving upon previously known bounds.
  Our techniques are intuitively straightforward and extend in a number of
ways. For one, our result extends to a bound on the performance of GSP at
coarse correlated equilibria, which captures (for example) a repeated-auction
setting in which agents apply regret-minimizing bidding strategies. In
addition, our analysis is robust against the presence of byzantine agents who
cannot be assumed to participate rationally.
  Additionally, we present tight bounds for the social welfare obtained at pure
NE for the special case of an auction for 3 slots, and discuss potential
methods for extending this analysis to an arbitrary number of slots."
"This paper studies the provision of a wireless network by a monopolistic
provider who may be either benevolent (seeking to maximize social welfare) or
selfish (seeking to maximize provider profit). The paper addresses questions
that do not seem to have been studied before in the engineering literature on
wireless networks: Under what circumstances is it feasible for a provider,
either benevolent or selfish, to operate a network in such a way as to cover
costs? How is the optimal behavior of a benevolent provider different from the
optimal behavior of a selfish provider, and how does this difference affect
social welfare? And, most importantly, how does the medium access control (MAC)
technology influence the answers to these questions? To address these
questions, we build a general model, and provide analysis and simulations for
simplified but typical scenarios; the focus in these scenarios is on the
contrast between the outcomes obtained under carrier-sensing multiple access
(CSMA) and outcomes obtained under time-division multiple access (TDMA).
Simulation results demonstrate that differences in MAC technology can have a
significant effect on social welfare, on provider profit, and even on the
(financial) feasibility of a wireless network."
"In this paper, we present and analyze the properties of a new class of games
- the spatial congestion game (SCG), which is a generalization of the classical
congestion game (CG). In a classical congestion game, multiple users share the
same set of resources and a user's payoff for using any resource is a function
of the total number of users sharing it. As a potential game, this game enjoys
some very appealing properties, including the existence of a pure strategy Nash
equilibrium (NE) and that every improvement path is finite and leads to such a
NE (also called the finite improvement property or FIP). While it's tempting to
use this model to study spectrum sharing, it does not capture the spatial reuse
feature of wireless communication, where resources (interpreted as channels)
may be reused without increasing congestion provided that users are located far
away from each other. This motivates us to study an extended form of the
congestion game where a user's payoff for using a resource is a function of the
number of its interfering users sharing it. This naturally results in a spatial
congestion game (SCG), where users are placed over a network (or a conflict
graph). We study fundamental properties of a spatial congestion game; in
particular, we seek to answer under what conditions this game possesses the
finite improvement property or a Nash equilibrium. We also discuss the
implications of these results when applied to wireless spectrum sharing."
"In this paper we study stochastic dynamic games with many players; these are
a fundamental model for a wide range of economic applications. The standard
solution concept for such games is Markov perfect equilibrium (MPE), but it is
well known that MPE computation becomes intractable as the number of players
increases. We instead consider the notion of stationary equilibrium (SE), where
players optimize assuming the empirical distribution of others' states remains
constant at its long run average. We make two main contributions. First, we
provide a rigorous justification for using SE. In particular, we provide a
parsimonious collection of exogenous conditions over model primitives that
guarantee existence of SE, and ensure that an appropriate approximation
property to MPE holds, in a general model with possibly unbounded state spaces.
Second, we draw a significant connection between the validity of SE, and market
structure: under the same conditions that imply SE exist and approximates MPE
well, the market becomes fragmented in the limit of many firms. To illustrate
this connection, we study in detail a series of dynamic oligopoly examples.
These examples show that our conditions enforce a form of ""decreasing returns
to larger states""; this yields fragmented industries in the limit. By contrast,
violation of these conditions suggests ""increasing returns to larger states""
and potential market concentration. In that sense, our work uses a fully
dynamic framework to also contribute to a longstanding issue in industrial
organization: understanding the determinants of market structure in different
industries."
"We study a class of stochastic dynamic games that exhibit strategic
complementarities between players; formally, in the games we consider, the
payoff of a player has increasing differences between her own state and the
empirical distribution of the states of other players. Such games can be used
to model a diverse set of applications, including network security models,
recommender systems, and dynamic search in markets. Stochastic games are
generally difficult to analyze, and these difficulties are only exacerbated
when the number of players is large (as might be the case in the preceding
examples).
  We consider an approximation methodology called mean field equilibrium to
study these games. In such an equilibrium, each player reacts to only the long
run average state of other players. We find necessary conditions for the
existence of a mean field equilibrium in such games. Furthermore, as a simple
consequence of this existence theorem, we obtain several natural monotonicity
properties. We show that there exist a ""largest"" and a ""smallest"" equilibrium
among all those where the equilibrium strategy used by a player is
nondecreasing, and we also show that players converge to each of these
equilibria via natural myopic learning dynamics; as we argue, these dynamics
are more reasonable than the standard best response dynamics. We also provide
sensitivity results, where we quantify how the equilibria of such games move in
response to changes in parameters of the game (e.g., the introduction of
incentives to players)."
"Truthfulness is fragile and demanding. It is oftentimes computationally
harder than solving the original problem. Even worse, truthfulness can be
utterly destroyed by small uncertainties in a mechanism's outcome. One obstacle
is that truthful payments depend on outcomes other than the one realized, such
as the lengths of non-shortest-paths in a shortest-path auction. Single-call
mechanisms are a powerful tool that circumvents this obstacle --- they
implicitly charge truthful payments, guaranteeing truthfulness in expectation
using only the outcome realized by the mechanism. The cost of such truthfulness
is a trade-off between the expected quality of the outcome and the risk of
large payments.
  We largely settle when and to what extent single-call mechanisms are
possible. The first single-call construction was discovered by Babaioff,
Kleinberg, and Slivkins [BKS10] in single-parameter domains. They give a
transformation that turns any monotone, single-parameter allocation rule into a
truthful-in-expectation single-call mechanism. Our first result is a natural
complement to [BKS10]: we give a new transformation that produces a single-call
VCG mechanism from any allocation rule for which VCG payments are truthful.
Second, in both the single-parameter and VCG settings, we precisely
characterize the possible transformations, showing that that a wide variety of
transformations are possible but that all take a very simple form. Finally, we
study the inherent trade-off between the expected quality of the outcome and
the risk of large payments. We show that our construction and that of [BKS10]
simultaneously optimize a variety of metrics in their respective domains.
  As an example, we analyze pay-per-click advertising auctions, where the
truthfulness of the standard VCG-based auction is easily broken when the
auctioneer's estimated click-through-rates are imprecise."
"This is a short introduction to the subject of strategic games. We focus on
the concepts of best response, Nash equilibrium, strict and weak dominance, and
mixed strategies, and study the relation between these concepts in the context
of the iterated elimination of strategies. Also, we discuss some variants of
the original definition of a strategic game. Finally, we introduce the basics
of mechanism design and use pre-Bayesian games to explain it."
"We study the attribution problem, that is, the problem of attributing a
change in the value of a characteristic function to its independent variables.
We make three contributions. First, we propose a formalization of the problem
based on a standard cost sharing model. Second, we show that there is a unique
attribution method that satisfies Dummy, Additivity, Conditional Nonnegativity,
Affine Scale Invariance, and Anonymity for all characteristic functions that
are the sum of a multilinear function and an additive function. We term this
the Aumann-Shapley-Shubik method. Conversely, we show that such a uniqueness
result does not hold for characteristic functions outside this class. Third, we
study multilinear characteristic functions in detail; we describe a
computationally efficient implementation of the Aumann-Shapley-Shubik method
and discuss practical applications to pay-per-click advertising and portfolio
analysis."
"We propose an algorithm for computing approximate Nash equilibria of
partially observable games using Monte-Carlo tree search based on recent bandit
methods. We obtain experimental results for the game of phantom tic-tac-toe,
showing that strong strategies can be efficiently computed by our algorithm."
"We consider the use of cost sharing in the Aspnes model of network
inoculation, showing that this can improve the cost of the optimal equilibrium
by a factor of $O(\sqrt{n})$ in a network of $n$ nodes."
"The Honey-Bee game is a two-player board game that is played on a connected
hexagonal colored grid or (in a generalized setting) on a connected graph with
colored nodes. In a single move, a player calls a color and thereby conquers
all the nodes of that color that are adjacent to his own current territory.
Both players want to conquer the majority of the nodes. We show that winning
the game is PSPACE-hard in general, NP-hard on series-parallel graphs, but easy
on outerplanar graphs.
  In the solitaire version, the goal of the single player is to conquer the
entire graph with the minimum number of moves. The solitaire version is NP-hard
on trees and split graphs, but can be solved in polynomial time on
co-comparability graphs."
"We study the problem of selling a resource through an auction mechanism. The
winning buyer in turn develops this resource to generate profit. Two forms of
payment are considered: charging the winning buyer a one-time payment, or an
initial payment plus a profit sharing contract (PSC). We consider a symmetric
interdependent values model with risk averse or risk neutral buyers and a risk
neutral seller. For the second price auction and the English auction, we show
that the seller's expected total revenue from the auction where he also takes a
fraction of the positive profit is higher than the expected revenue from the
auction with only a one-time payment. Moreover, the seller can generate an even
higher expected total revenue if, in addition to taking a fraction of the
positive profit, he also takes the same fraction of any loss incurred from
developing the resource. Moving beyond simple PSCs, we show that the auction
with a PSC from a very general class generates higher expected total revenue
than the auction with only a one-time payment. Finally, we show that suitable
PSCs provide higher expected total revenue than a one-time payment even when
the incentives of the winning buyer to develop the resource must be addressed
by the seller."
"A fundamental result in mechanism design theory, the so-called revelation
principle, asserts that for many questions concerning the existence of
mechanisms with a given outcome one can restrict attention to truthful direct
revelation-mechanisms. In practice, however, many mechanism use a restricted
message space. This motivates the study of the tradeoffs involved in choosing
simplified mechanisms, which can sometimes bring benefits in precluding bad or
promoting good equilibria, and other times impose costs on welfare and revenue.
We study the simplicity-expressiveness tradeoff in two representative settings,
sponsored search auctions and combinatorial auctions, each being a canonical
example for complete information and incomplete information analysis,
respectively. We observe that the amount of information available to the agents
plays an important role for the tradeoff between simplicity and expressiveness."
"Pavlov, a well-known strategy in game theory, has been shown to have some
advantages in the Iterated Prisoner's Dilemma (IPD) game. However, this
strategy can be exploited by inveterate defectors. We modify this strategy to
mitigate the exploitation. We call the resulting strategy Rational Pavlov. This
has a parameter p which measures the ""degree of forgiveness"" of the players. We
study the evolution of cooperation in the IPD game, when n players are arranged
in a cycle, and all play this strategy. We examine the effect of varying p on
the convergence rate and prove that the convergence rate is fast, O(n log n)
time, for high values of p. We also prove that the convergence rate is
exponentially slow in n for small enough p. Our analysis leaves a gap in the
range of p, but simulations suggest that there is, in fact, a sharp phase
transition."
"In evolutionary game theory, repeated two-player games are used to study
strategy evolution in a population under natural selection. As the evolution
greatly depends on the interaction structure, there has been growing interests
in studying the games on graphs. In this setting, players occupy the vertices
of a graph and play the game only with their immediate neighbours. Various
evolutionary dynamics have been studied in this setting for different games.
Due to the complexity of the analysis, however, most of the work in this area
is experimental. This paper aims to contribute to a more complete
understanding, by providing rigorous analysis. We study the imitation dynamics
on two classes of graph: cycles and complete graphs. We focus on three well
known social dilemmas, namely the Prisoner's Dilemma, the Stag Hunt and the
Snowdrift Game. We also consider, for completeness, the so-called Harmony Game.
Our analysis shows that, on the cycle, all four games converge fast, either to
total cooperation or total defection. On the complete graph, all but the
Snowdrift game converge fast, either to cooperation or defection. The Snowdrift
game reaches a metastable state fast, where cooperators and defectors coexist.
It will converge to cooperation or defection only after spending time in this
state which is exponential in the size, n, of the graph. In exceptional cases,
it will remain in this state indefinitely. Our theoretical results are
supported by experimental investigations."
"We address a central (and classical) issue in the theory of infinite games:
the reduction of the memory size that is needed to implement winning strategies
in regular infinite games (i.e., controllers that ensure correct behavior
against actions of the environment, when the specification is a regular
omega-language). We propose an approach which attacks this problem before the
construction of a strategy, by first reducing the game graph that is obtained
from the specification. For the cases of specifications represented by
""request-response""-requirements and general ""fairness"" conditions, we show that
an exponential gain in the size of memory is possible."
"We provide a necessary and sufficient condition under which a convex set is
approachable in a game with partial monitoring, i.e.\ where players do not
observe their opponents' moves but receive random signals. This condition is an
extension of Blackwell's Criterion in the full monitoring framework, where
players observe at least their payoffs. When our condition is fulfilled, we
construct explicitly an approachability strategy, derived from a strategy
satisfying some internal consistency property in an auxiliary game. We also
provide an example of a convex set, that is neither (weakly)-approachable nor
(weakly)-excludable, a situation that cannot occur in the full monitoring case.
We finally apply our result to describe an $\epsilon$-optimal strategy of the
uninformed player in a zero-sum repeated game with incomplete information on
one side."
"We perform a simulation-based analysis of keyword auctions modeled as
one-shot games of incomplete information to study a series of mechanism design
questions. Our first question addresses the degree to which incentive
compatibility fails in generalized second-price (GSP) auctions. Our results
suggest that sincere bidding in GSP auctions is a strikingly poor strategy and
a poor predictor of equilibrium outcomes. We next show that the rank-by-revenue
mechanism is welfare optimal, corroborating past results. Finally, we analyze
profit as a function of auction mechanism under a series of alternative
settings. Our conclusions coincide with those of Lahaie and Pennock [2007] when
values and quality scores are strongly positively correlated: in such a case,
rank-by-bid rules are clearly superior. We diverge, however, in showing that
auctions that put little weight on quality scores almost universally dominate
the pure rank-by-revenue scheme."
"Strategyproof mechanisms provide robust equilibrium with minimal assumptions
about knowledge and rationality but can be unachievable in combination with
other desirable properties such as budget-balance, stability against deviations
by coalitions, and computational tractability. In the search for
maximally-strategyproof mechanisms that simultaneously satisfy other desirable
properties, we introduce a new metric to quantify the strategyproofness of a
mechanism, based on comparing the payoff distribution, given truthful reports,
against that of a strategyproof ""reference"" mechanism that solves a problem
relaxation. Focusing on combinatorial exchanges, we demonstrate that the metric
is informative about the eventual equilibrium, where simple regretbased metrics
are not, and can be used for online selection of an effective mechanism."
"A central task of artificial intelligence is the design of artificial agents
that act towards specified goals in partially observed environments. Since such
environments frequently include interaction over time with other agents with
their own goals, reasoning about such interaction relies on sequential
game-theoretic models such as extensive-form games or some of their succinct
representations such as multi-agent influence diagrams. The current algorithms
for calculating equilibria either work with inefficient representations,
possibly doubly exponential inthe number of time steps, or place strong
assumptions on the game structure. In this paper,we propose a sampling-based
approach, which calculates extensive-form correlated equilibria with small
representations without placing such strong assumptions. Thus, it is practical
in situations where the previous approaches would fail. In addition, our
algorithm allows control over characteristics of the target equilibrium, e.g.,
we can ask for an equilibrium with high social welfare. Our approach is based
on a multiplicativeweight update algorithm analogous to AdaBoost, and Markov
chain Monte Carlo sampling. We prove convergence guarantees and explore the
utility of our approach on several moderately sized multi-player games."
"Prediction markets are designed to elicit information from multiple agents in
order to predict (obtain probabilities for) future events. A good prediction
market incentivizes agents to reveal their information truthfully; such
incentive compatibility considerations are commonly studied in mechanism
design. While this relation between prediction markets and mechanism design is
well understood at a high level, the models used in prediction markets tend to
be somewhat different from those used in mechanism design. This paper considers
a model for prediction markets that fits more straightforwardly into the
mechanism design framework. We consider a number of mechanisms within this
model, all based on proper scoring rules. We discuss basic properties of these
mechanisms, such as incentive compatibility. We also draw connections between
some of these mechanisms and cooperative game theory. Finally, we speculate how
one might build a practical prediction market based on some of these ideas."
"We study cardinal auctions for selling multiple copies of a good, in which
bidders specify not only their bid or how much they are ready to pay for the
good, but also a cardinality constraint on the number of copies that will be
sold via the auction. We perform first known Price of Anarchy type analyses
with detailed comparison of the classical Vickrey-Clarke-Groves (VCG) auction
and one based on minimum pay property (MPP) which is similar to Generalized
Second Price auction commonly used in sponsored search. Without cardinality
constraints, MPP has the same efficiency (total value to bidders) and at least
as much revenue (total income to the auctioneer) as VCG; this also holds for
certain other generalizations of MPP (e.g., prefix constrained auctions, as we
show here). In contrast, our main results are that, with cardinality
constraints, (a) equilibrium efficiency of MPP is 1/2 of that of VCG and this
factor is tight, and (b) in equilibrium MPP may collect as little as 1/2 the
revenue of VCG. These aspects arise because in presence of cardinality
constraints, more strategies are available to bidders in MPP, including bidding
above their value, and this makes analyses nontrivial."
"When agents with independent priors bid for a single item, Myerson's optimal
auction maximizes expected revenue, whereas Vickrey's second-price auction
optimizes social welfare. We address the natural question of trade-offs between
the two criteria, that is, auctions that optimize, say, revenue under the
constraint that the welfare is above a given level. If one allows for
randomized mechanisms, it is easy to see that there are polynomial-time
mechanisms that achieve any point in the trade-off (the Pareto curve) between
revenue and welfare. We investigate whether one can achieve the same guarantees
using deterministic mechanisms. We provide a negative answer to this question
by showing that this is a (weakly) NP-hard problem. On the positive side, we
provide polynomial-time deterministic mechanisms that approximate with
arbitrary precision any point of the trade-off between these two fundamental
objectives for the case of two bidders, even when the valuations are correlated
arbitrarily. The major problem left open by our work is whether there is such
an algorithm for three or more bidders with independent valuation
distributions."
"We consider a setting in which a single divisible good (""cake"") needs to be
divided between n players, each with a possibly different valuation function
over pieces of the cake. For this setting, we address the problem of finding
divisions that maximize the social welfare, focusing on divisions where each
player needs to get one contiguous piece of the cake. We show that for both the
utilitarian and the egalitarian social welfare functions it is NP-hard to find
the optimal division. For the utilitarian welfare, we provide a constant factor
approximation algorithm, and prove that no FPTAS is possible unless P=NP. For
egalitarian welfare, we prove that it is NP-hard to approximate the optimum to
any factor smaller than 2. For the case where the number of players is small,
we provide an FPT (fixed parameter tractable) FPTAS for both the utilitarian
and the egalitarian welfare objectives."
"Complements between goods - where one good takes on added value in the
presence of another - have been a thorn in the side of algorithmic mechanism
designers. On the one hand, complements are common in the standard motivating
applications for combinatorial auctions, like spectrum license auctions. On the
other, welfare maximization in the presence of complements is notoriously
difficult, and this intractability has stymied theoretical progress in the
area. For example, there are no known positive results for combinatorial
auctions in which bidder valuations are multi-parameter and
non-complement-free, other than the relatively weak results known for general
valuations.
  To make inroads on the problem of combinatorial auction design in the
presence of complements, we propose a model for valuations with complements
that is parameterized by the ""size"" of the complements. A valuation in our
model is represented succinctly by a weighted hypergraph, where the size of the
hyper-edges corresponds to degree of complementarity. Our model permits a
variety of computationally efficient queries, and non-trivial
welfare-maximization algorithms and mechanisms.
  We design the following polynomial-time approximation algorithms and truthful
mechanisms for welfare maximization with bidders with hypergraph valuations.
  1- For bidders whose valuations correspond to subgraphs of a known graph that
is planar (or more generally, excludes a fixed minor), we give a truthful and
(1+epsilon)-approximate mechanism.
  2- We give a polynomial-time, r-approximation algorithm for welfare
maximization with hypergraph-r valuations. Our algorithm randomly rounds a
compact linear programming relaxation of the problem.
  3- We design a different approximation algorithm and use it to give a
polynomial-time, truthful-in-expectation mechanism that has an approximation
factor of O(log^r m)."
"In this paper, we study turn-based quantitative multiplayer non zero-sum
games played on finite graphs with both reachability and safety objectives. In
this framework a player with a reachability objective aims at reaching his own
goal as soon as possible, whereas a player with a safety objective aims at
avoiding his bad set or, if impossible, delaying its visit as long as possible.
We prove the existence of Nash equilibria with finite memory in quantitative
multiplayer reachability/safety games. Moreover, we prove the existence of
finite-memory secure equilibria for quantitative two-player reachability games."
"We propose a new class of games, called Multi-Games (MG), in which a given
number of players play a fixed number of basic games simultaneously. In each
round of the MG, each player will have a specific set of weights, one for each
basic game, which add up to one and represent the fraction of the player's
investment in each basic game. The total payoff for each player is then the
convex combination, with the corresponding weights, of the payoffs it obtains
in the basic games. The basic games in a MG can be regarded as different
environments for the players. When the players' weights for the different games
in MG are private information or types with given conditional probability
distributions, we obtain a particular class of Bayesian games. We show that for
the class of so-called completely pure regular Double Game (DG) with finite
sets of types, the Nash equilibria (NE) of the basic games can be used to
compute a Bayesian Nash equilibrium of the DG in linear time with respect to
the number of types of the players. We study a DG for the Prisoner's Dilemma
(PD) by extending the PD with a second so-called Social Game (SG), generalising
the notion of altruistic extension of a game in which players have different
altruistic levels (or social coefficients). We study two different examples of
Bayesian games in this context in which the social coefficients have a finite
set of values and each player only knows the probability distribution of the
opponent's social coefficient. In the first case we have a completely pure
regular DG for which we deduce a Bayesian NE. Finally, we use the second
example to compare various strategies in a round-robin tournament of the DG for
PD, in which the players can change their social coefficients incrementally
from one round to the next."
"The paper explores a consumer search setting where the sellers have
asymmetries. The model is an extension of the popular Stahl Model, which is
widely used in the literature. The extension introduces sellers with
heterogeneous stores number, reflecting the typical market structure. The
market consists of several sellers heterogeneous in size consumers, some of
which face a cost when sequentially searching. The paper shows that no
symmetric model exist in the extension and asymmetric NE of the Stahl model are
found for comparison. Additional results suggest that smallest sellers will be
the ones offering lowest prices, in line with several real world examples
provided in the paper. However, profits remain in most cases fixed per store,
making a larger firm more profitable, yet with lower sold quantity. The
findings suggest that on some level price dispersion will still exist, together
with some level of price stickiness, both observed in reality."
"We consider the unit-demand envy-free pricing problem, which is a unit-demand
auction where each bidder receives an item that maximizes his utility, and the
goal is to maximize the auctioneer's profit. This problem is NP-hard and
unlikely to be in APX. We present four new MIP formulations for it and
experimentally compare them to a previous one due to Shioda, Tun\c{c}el, and
Myklebust. We describe three models to generate different random instances for
general unit-demand auctions, that we designed for the computational
experiments. Each model has a nice economic interpretation. Aiming
approximation results, we consider the variant of the problem where the item
prices are restricted to be chosen from a geometric series, and prove that an
optimal solution for this variant has value that is a fraction (depending on
the series used) of the optimal value of the original problem. So this variant
is also unlikely to be in APX."
"Algorithmic Mechanism Design attempts to marry computation and incentives,
mainly by leveraging monetary transfers between designer and selfish agents
involved. This is principally because in absence of money, very little can be
done to enforce truthfulness. However, in certain applications, money is
unavailable, morally unacceptable or might simply be at odds with the objective
of the mechanism. For example, in Combinatorial Auctions (CAs), the
paradigmatic problem of the area, we aim at solutions of maximum social welfare
but still charge the society to ensure truthfulness. Additionally, truthfulness
of CAs is poorly understood already in the case in which bidders happen to be
interested in only two different sets of goods.
  We focus on the design of incentive-compatible CAs without money in the
general setting of $k$-minded bidders. We trade monetary transfers with the
observation that the mechanism can detect certain lies of the bidders: i.e., we
study truthful CAs with verification and without money. We prove a
characterization of truthful mechanisms, which makes an interesting parallel
with the well-understood case of CAs with money for single-minded bidders. We
then give a host of upper bounds on the approximation ratio obtained by either
deterministic or randomized truthful mechanisms when the sets and valuations
are private knowledge of the bidders. (Most of these mechanisms run in
polynomial time and return solutions with (nearly) best possible approximation
guarantees.) We complement these positive results with a number of lower bounds
(some of which are essentially tight) that hold in the easier case of public
sets. We thus provide an almost complete picture of truthfully approximating
CAs in this general setting with multi-dimensional bidders."
"Daily deals platforms such as Amazon Local, Google Offers, GroupOn, and
LivingSocial have provided a new channel for merchants to directly market to
consumers. In order to maximize consumer acquisition and retention, these
platforms would like to offer deals that give good value to users. Currently,
selecting such deals is done manually; however, the large number of submarkets
and localities necessitates an automatic approach to selecting good deals and
determining merchant payments.
  We approach this challenge as a market design problem. We postulate that
merchants already have a good idea of the attractiveness of their deal to
consumers as well as the amount they are willing to pay to offer their deal.
The goal is to design an auction that maximizes a combination of the revenue of
the auctioneer (platform), welfare of the bidders (merchants), and the positive
externality on a third party (the consumer), despite the asymmetry of
information about this consumer benefit. We design auctions that truthfully
elicit this information from the merchants and maximize the social welfare
objective, and we characterize the consumer welfare functions for which this
objective is truthfully implementable. We generalize this characterization to a
very broad mechanism-design setting and give examples of other applications."
"We consider Vickrey-Clarke-Groves (VCG) auctions for a very general
combinatorial structure, in an average-case setting where item costs are
independent, identically distributed uniform random variables. We prove that
the expected VCG cost is at least double the expected nominal cost, and exactly
double when the desired structure is a basis of a bridgeless matroid. In the
matroid case we further show that, conditioned upon the VCG cost, the
expectation of the nominal cost is exactly half the VCG cost, and we show
several results on variances and covariances among the nominal cost, the VCG
cost, and related quantities. As an application, we find the asymptotic
variance of the VCG cost of the minimum spanning tree in a complete graph with
random edge costs."
"Electric storage units constitute a key element in the emerging smart grid
system. In this paper, the interactions and energy trading decisions of a
number of geographically distributed storage units are studied using a novel
framework based on game theory. In particular, a noncooperative game is
formulated between storage units, such as PHEVs, or an array of batteries that
are trading their stored energy. Here, each storage unit's owner can decide on
the maximum amount of energy to sell in a local market so as to maximize a
utility that reflects the tradeoff between the revenues from energy trading and
the accompanying costs. Then in this energy exchange market between the storage
units and the smart grid elements, the price at which energy is traded is
determined via an auction mechanism. The game is shown to admit at least one
Nash equilibrium and a novel proposed algorithm that is guaranteed to reach
such an equilibrium point is proposed. Simulation results show that the
proposed approach yields significant performance improvements, in terms of the
average utility per storage unit, reaching up to 130.2% compared to a
conventional greedy approach."
"In service exchange platforms, anonymous users exchange services with each
other: clients request services and are matched to servers who provide
services. Because providing good-quality services requires effort, in any
single interaction a server will have no incentive to exert effort and will
shirk. We show that if current servers will later become clients and want
good-quality services, shirking can be eliminated by rating protocols, which
maintain ratings for each user, prescribe behavior in each client-server
interaction, and update ratings based on whether observed/reported behavior
conforms with prescribed behavior. The rating protocols proposed are the first
to achieve social optimum even when observation/reporting is imperfect (quality
is incorrectly assessed/reported or reports are lost). The proposed protocols
are remarkably simple, requiring only binary ratings and three possible
prescribed behaviors. Key to the efficacy of the proposed protocols is that
they are nonstationary, and tailor prescriptions to both current and past
rating distributions."
"This paper considers a time-varying game with $N$ players. Every time slot,
players observe their own random events and then take a control action. The
events and control actions affect the individual utilities earned by each
player. The goal is to maximize a concave function of time average utilities
subject to equilibrium constraints. Specifically, participating players are
provided access to a common source of randomness from which they can optimally
correlate their decisions. The equilibrium constraints incentivize
participation by ensuring that players cannot earn more utility if they choose
not to participate. This form of equilibrium is similar to the notions of Nash
equilibrium and correlated equilibrium, but is simpler to attain. A Lyapunov
method is developed that solves the problem in an online \emph{max-weight}
fashion by selecting actions based on a set of time-varying weights. The
algorithm does not require knowledge of the event probabilities and has
polynomial convergence time. A similar method can be used to compute a standard
correlated equilibrium, albeit with increased complexity."
"Markov decision processes (MDP) are finite-state systems with both strategic
and probabilistic choices. After fixing a strategy, an MDP produces a sequence
of probability distributions over states. The sequence is eventually
synchronizing if the probability mass accumulates in a single state, possibly
in the limit. Precisely, for 0 <= p <= 1 the sequence is p-synchronizing if a
probability distribution in the sequence assigns probability at least p to some
state, and we distinguish three synchronization modes: (i) sure winning if
there exists a strategy that produces a 1-synchronizing sequence; (ii)
almost-sure winning if there exists a strategy that produces a sequence that
is, for all epsilon > 0, a (1-epsilon)-synchronizing sequence; (iii) limit-sure
winning if for all epsilon > 0, there exists a strategy that produces a
(1-epsilon)-synchronizing sequence.
  We consider the problem of deciding whether an MDP is sure, almost-sure,
limit-sure winning, and we establish the decidability and optimal complexity
for all modes, as well as the memory requirements for winning strategies. Our
main contributions are as follows: (a) for each winning modes we present
characterizations that give a PSPACE complexity for the decision problems, and
we establish matching PSPACE lower bounds; (b) we show that for sure winning
strategies, exponential memory is sufficient and may be necessary, and that in
general infinite memory is necessary for almost-sure winning, and unbounded
memory is necessary for limit-sure winning; (c) along with our results, we
establish new complexity results for alternating finite automata over a
one-letter alphabet."
"The focus of classic mechanism design has been on truthful direct-revelation
mechanisms. In the context of combinatorial auctions the truthful
direct-revelation mechanism that maximizes social welfare is the VCG mechanism.
For many valuation spaces computing the allocation and payments of the VCG
mechanism, however, is a computationally hard problem. We thus study the
performance of the VCG mechanism when bidders are forced to choose bids from a
subspace of the valuation space for which the VCG outcome can be computed
efficiently. We prove improved upper bounds on the welfare loss for
restrictions to additive bids and upper and lower bounds for restrictions to
non-additive bids. These bounds show that the welfare loss increases in
expressiveness. All our bounds apply to equilibrium concepts that can be
computed in polynomial time as well as to learning outcomes."
"We present a model of competition between web search algorithms, and study
the impact of such competition on user welfare. In our model, search providers
compete for customers by strategically selecting which search results to
display in response to user queries. Customers, in turn, have private
preferences over search results and will tend to use search engines that are
more likely to display pages satisfying their demands.
  Our main question is whether competition between search engines increases the
overall welfare of the users (i.e., the likelihood that a user finds a page of
interest). When search engines derive utility only from customers to whom they
show relevant results, we show that they differentiate their results, and every
equilibrium of the resulting game achieves at least half of the welfare that
could be obtained by a social planner. This bound also applies whenever the
likelihood of selecting a given engine is a convex function of the probability
that a user's demand will be satisfied, which includes natural Markovian models
of user behavior.
  On the other hand, when search engines derive utility from all customers
(independent of search result relevance) and the customer demand functions are
not convex, there are instances in which the (unique) equilibrium involves no
differentiation between engines and a high degree of randomness in search
results. This can degrade social welfare by a factor of the square root of N
relative to the social optimum, where N is the number of webpages. These bad
equilibria persist even when search engines can extract only small (but
non-zero) expected revenue from dissatisfied users, and much higher revenue
from satisfied ones."
"In this note, we consider repeated play of a finite game using learning rules
whose period-by-period behavior probabilities or empirical distributions
converge to some notion of equilibria of the stage game. Our primary focus is
on uncoupled and completely uncoupled learning rules. While the former relies
on players being aware of only their own payoff functions and able to monitor
the action taken by the others, the latter assumes that players only know their
own past realized payoffs. We highlight the border between possible and
impossible results using these rules. We also overview several uncoupled and
completely uncoupled learning rules, most of which leverage notions of regret
as the solution concept to seek payoff-improving action profiles."
"Most work on manipulation assumes that all preferences are known to the
manipulators. However, in many settings elections are open and sequential, and
manipulators may know the already cast votes but may not know the future votes.
We introduce a framework, in which manipulators can see the past votes but not
the future ones, to model online coalitional manipulation of sequential
elections, and we show that in this setting manipulation can be extremely
complex even for election systems with simple winner problems. Yet we also show
that for some of the most important election systems such manipulation is
simple in certain settings. This suggests that when using sequential voting,
one should pay great attention to the details of the setting in choosing one's
voting rule.
  Among the highlights of our classifications are: We show that, depending on
the size of the manipulative coalition, the online manipulation problem can be
complete for each level of the polynomial hierarchy or even for PSPACE. We
obtain the most dramatic contrast to date between the nonunique-winner and
unique-winner models: Online weighted manipulation for plurality is in P in the
nonunique-winner model, yet is coNP-hard (constructive case) and NP-hard
(destructive case) in the unique-winner model. And we obtain what to the best
of our knowledge are the first PNP[1]-completeness and PNP-completeness results
in the field of computational social choice, in particular proving such
completeness for, respectively, the complexity of 3-candidate and 4-candidate
(and unlimited-candidate) online weighted coalition manipulation of veto
elections."
"In this paper we investigate the problem of designing a spectrum scanning
strategy to detect an intelligent Invader who wants to utilize spectrum
undetected for his/her unapproved purposes. To deal with this problem we apply
game-theoretical tools. We model the situation as a game between a Scanner and
an Invader where the Invader faces a dilemma: the more bandwidth the Invader
attempts to use leads to a larger payoff if he is not detected, but at the same
time also increases the probability of being detected and thus fined.
Similarly, the Scanner faces a dilemma: the wider the bandwidth scanned, the
higher the probability of detecting the Invader, but at the expense of
increasing the cost of building the scanning system. The equilibrium strategies
are found explicitly and reveal interesting properties. In particular, we have
found a discontinuous dependence of the equilibrium strategies on the network
parameters, fine and the type of the Invader's award. This discontinuity on
fine means that the network provider has to take into account a human factor
since some threshold values of fine could be very sensible for the Invader,
while in other situations simply increasing the fine has minimal deterrence
impact. Also we show how different reward types for the Invader (e.g. motivated
by using different type of application, say, video-streaming or downloading
files) can be incorporated into scanning strategy to increase its efficiency."
"We study the deterministic and randomized query complexity of finding
approximate equilibria in bimatrix games. We show that the deterministic query
complexity of finding an $\epsilon$-Nash equilibrium when $\epsilon <
\frac{1}{2}$ is $\Omega(k^2)$, even in zero-one constant-sum games. In
combination with previous results \cite{FGGS13}, this provides a complete
characterization of the deterministic query complexity of approximate Nash
equilibria. We also study randomized querying algorithms. We give a randomized
algorithm for finding a $(\frac{3 - \sqrt{5}}{2} + \epsilon)$-Nash equilibrium
using $O(\frac{k \cdot \log k}{\epsilon^2})$ payoff queries, which shows that
the $\frac{1}{2}$ barrier for deterministic algorithms can be broken by
randomization. For well-supported Nash equilibria (WSNE), we first give a
randomized algorithm for finding an $\epsilon$-WSNE of a zero-sum bimatrix game
using $O(\frac{k \cdot \log k}{\epsilon^4})$ payoff queries, and we then use
this to obtain a randomized algorithm for finding a $(\frac{2}{3} +
\epsilon)$-WSNE in a general bimatrix game using $O(\frac{k \cdot \log
k}{\epsilon^4})$ payoff queries. Finally, we initiate the study of lower bounds
against randomized algorithms in the context of bimatrix games, by showing that
randomized algorithms require $\Omega(k^2)$ payoff queries in order to find a
$\frac{1}{6k}$-Nash equilibrium, even in zero-one constant-sum games. In
particular, this rules out query-efficient randomized algorithms for finding
exact Nash equilibria."
"Crowd sensing is a new paradigm which leverages the pervasive smartphones to
efficiently collect sensing data, enabling numerous novel applications. To
achieve good service quality for a crowd sensing application, incentive
mechanisms are indispensable to attract more user participation. Most of
existing mechanisms only apply for the offline scenario, where the system has
full information about the users' sensing profiles, i.e., a set of locations or
mobility as well as the type of smartphones used, and their true costs. On the
contrary, we focus on a more real scenario where users with their own privacy
concerns arrive one by one online in a random order. We model the problem as a
privacy-respecting online auction in which users are willing to negotiate
access to certain private information and submit their sensing profiles
satisfying privacy concerns to the platform (the provider of crowd sensing
applications) over time, and the platform aims to the total total value of the
services provided by selected users under a budget constraint. We then design
two online mechanisms for a budgeted crowd sensing application, satisfying the
computational efficiency, individual rationality, budget feasibility,
truthfulness, consumer sovereignty, constant competitiveness and privacy
concerns. Through extensive simulations, we evaluate the performance and
validate the theoretical properties of our online mechanisms."
"We show that in any $n$-player $m$-action normal-form game, we can obtain an
approximate equilibrium by sampling any mixed-action equilibrium a small number
of times. We study three types of equilibria: Nash, correlated and coarse
correlated. For each one of them we obtain upper and lower bounds on the number
of samples required for the empirical distribution over the sampled action
profiles to form an approximate equilibrium with probability close to one.
  These bounds imply that using a small number of samples we can test whether
or not players are playing according to an approximate equilibrium, even in
games where $n$ and $m$ are large. In addition, our results substantially
improve previously known upper bounds on the support size of approximate
equilibria in games with many players. In particular, for all the three types
of equilibria we show the existence of approximate equilibrium with support
size polylogarithmic in $n$ and $m$, whereas the previously best-known upper
bounds were polynomial in $n$."
"We study Monte Carlo tree search (MCTS) in zero-sum extensive-form games with
perfect information and simultaneous moves. We present a general template of
MCTS algorithms for these games, which can be instantiated by various selection
methods. We formally prove that if a selection method is $\epsilon$-Hannan
consistent in a matrix game and satisfies additional requirements on
exploration, then the MCTS algorithm eventually converges to an approximate
Nash equilibrium (NE) of the extensive-form game. We empirically evaluate this
claim using regret matching and Exp3 as the selection methods on randomly
generated games and empirically selected worst case games. We confirm the
formal result and show that additional MCTS variants also converge to
approximate NE on the evaluated games."
"We study the problem of selecting a member of a set of agents based on
impartial nominations by agents from that set. The problem was studied
previously by Alon et al. and Holzman and Moulin and has important applications
in situations where representatives are selected from within a group or where
publishing or funding decisions are made based on a process of peer review. Our
main result concerns a randomized mechanism that in expectation awards the
prize to an agent with at least half the maximum number of nominations. Subject
to impartiality, this is best possible."
"An increasing number of businesses and organisations rely on existing users
for finding new users or spreading a message. One of the widely used
""refer-a-friend"" mechanisms offers an equal reward to both the referrer and the
invitee. This mechanism provides incentives for direct referrals and is fair to
the invitee. On the other hand, multi-level marketing and recent social
mobilisation experiments focus on mechanisms that incentivise both direct and
indirect referrals. Such mechanisms share the reward for inviting a new member
among the ancestors, usually in geometrically decreasing shares. A new member
receives nothing at the time of joining. We study fairness in multi-level
marketing mechanisms. We show how characteristic function games can be used to
model referral marketing, show how the canonical fairness concept of the
Shapley value can be applied to this setting, and establish the complexity of
finding the Shapley value in each class, and provide a comparison of the
Shapley value-based mechanism to existing referral mechanisms."
"We consider a multi-dimensional screening problem of selling a product with
multiple quality levels and design virtual value functions to derive conditions
that imply optimality of only selling highest quality. A challenge of designing
virtual values for multi-dimensional agents is that a mechanism that pointwise
optimizes virtual values resulting from a general application of integration by
parts is not incentive compatible, and no general methodology is known for
selecting the right paths for integration by parts. We resolve this issue by
first uniquely solving for paths that satisfy certain necessary conditions that
the pointwise optimality of the mechanism imposes on virtual values, and then
identifying distributions that ensure the resulting virtual surplus is indeed
pointwise optimized by the mechanism. Our method of solving for virtual values
is general, and as a second application we use it to derive conditions of
optimality for selling only the grand bundle of items to an agent with additive
preferences."
"We study combinatorial auctions where each item is sold separately but
simultaneously via a second price auction. We ask whether it is possible to
efficiently compute in this game a pure Nash equilibrium with social welfare
close to the optimal one.
  We show that when the valuations of the bidders are submodular, in many
interesting settings (e.g., constant number of bidders, budget additive
bidders) computing an equilibrium with good welfare is essentially as easy as
computing, completely ignoring incentives issues, an allocation with good
welfare. On the other hand, for subadditive valuations, we show that computing
an equilibrium requires exponential communication. Finally, for XOS (a.k.a.
fractionally subadditive) valuations, we show that if there exists an efficient
algorithm that finds an equilibrium, it must use techniques that are very
different from our current ones."
"We develop a general duality-theory framework for revenue maximization in
additive Bayesian auctions. The framework extends linear programming duality
and complementarity to constraints with partial derivatives. The dual system
reveals the geometric nature of the problem and highlights its connection with
the theory of bipartite graph matchings. We demonstrate the power of the
framework by applying it to a multiple-good monopoly setting where the buyer
has uniformly distributed valuations for the items, the canonical long-standing
open problem in the area. We propose a deterministic selling mechanism called
Straight-Jacket Auction (SJA) which we prove to be exactly optimal for up to 6
items, and conjecture its optimality for any number of goods. The duality
framework is used not only for proving optimality, but perhaps more
importantly, for deriving the optimal mechanism itself; as a result, SJA is
defined by natural geometric constraints."
"Mobile Crowd Sensing (MCS) is a new paradigm which takes advantage of
pervasive smartphones to efficiently collect data, enabling numerous novel
applications. To achieve good service quality for a MCS application, incentive
mechanisms are necessary to attract more user participation. Most of existing
mechanisms apply only for the offline scenario where all users' information are
known a priori. On the contrary, we focus on a more realistic scenario where
users arrive one by one online in a random order. Based on the online auction
model, we investigate the problem that users submit their private profiles to
the crowdsourcer when they arrive, and the crowdsourcer aims at selecting a
subset of users before a specified deadline for minimizing the total payment
while a specific number of tasks can be completed.We design three online
mechanisms, Homo-OMZ, Hetero-OMZ and Hetero-OMG, all of which can satisfy the
computational efficiency, individual rationality, cost-truthfulness, and
consumer sovereignty. The Homo-OMZ mechanism is applicable to the homogeneous
user model and can satisfy the social efficiency but not constant frugality.
The Hetero-OMZ and Hetero-OMG mechanisms are applicable to both the homogeneous
and heterogeneous user models, and can satisfy the constant frugality. Besides,
the Hetero-OMG mechanism can also satisfy the time-truthfulness. Through
extensive simulations, we evaluate the performance and validate the theoretical
properties of our online mechanisms."
"We investigate a spectrum oligopoly market where primaries lease their
channels to secondaries in lieu of financial remuneration. Transmission quality
of a channel evolves randomly. Each primary has to select the price it would
quote without knowing the transmission qualities of its competitors' channels.
Each secondary buys a channel depending on the price and the transmission
quality a channel offers. We formulate the price selection problem as a non
co-operative game with primaries as players. In the one-shot game, we show that
there exists a unique symmetric Nash Equilibrium(NE) strategy profile and
explicitly compute it. Our analysis reveals that under the NE strategy profile
a primary prices its channel to render high quality channel more preferable to
the secondary; this negates the popular belief that prices ought to be selected
to render channels equally preferable to the secondary regardless of their
qualities. We show the loss of revenue in the asymptotic limit due to the non
co-operation of primaries. In the repeated version of the game, we characterize
a subgame perfect NE where a primary can attain a payoff arbitrarily close to
the payoff it would obtain when primaries co-operate."
"We consider the {\em multi-shop ski rental} problem. This problem generalizes
the classic ski rental problem to a multi-shop setting, in which each shop has
different prices for renting and purchasing a pair of skis, and a
\emph{consumer} has to make decisions on when and where to buy. We are
interested in the {\em optimal online (competitive-ratio minimizing) mixed
strategy} from the consumer's perspective. For our problem in its basic form,
we obtain exciting closed-form solutions and a linear time algorithm for
computing them. We further demonstrate the generality of our approach by
investigating three extensions of our basic problem, namely ones that consider
costs incurred by entering a shop or switching to another shop. Our solutions
to these problems suggest that the consumer must assign positive probability in
\emph{exactly one} shop at any buying time. Our results apply to many
real-world applications, ranging from cost management in \texttt{IaaS} cloud to
scheduling in distributed computing."
"Using duality theory techniques we derive simple, closed-form formulas for
bounding the optimal revenue of a monopolist selling many heterogeneous goods,
in the case where the buyer's valuations for the items come i.i.d. from a
uniform distribution and in the case where they follow independent (but not
necessarily identical) exponential distributions. We apply this in order to get
in both these settings specific performance guarantees, as functions of the
number of items $m$, for the simple deterministic selling mechanisms studied by
Hart and Nisan [EC 2012], namely the one that sells the items separately and
the one that offers them all in a single bundle.
  We also propose and study the performance of a natural randomized mechanism
for exponential valuations, called Proportional. As an interesting corollary,
for the special case where the exponential distributions are also identical, we
can derive that offering the goods in a single full bundle is the optimal
selling mechanism for any number of items. To our knowledge, this is the first
result of its kind: finding a revenue-maximizing auction in an additive setting
with arbitrarily many goods."
"A recurring theme in recent computer science literature is that proper design
of signaling schemes is a crucial aspect of effective mechanisms aiming to
optimize social welfare or revenue. One of the research endeavors of this line
of work is understanding the algorithmic and computational complexity of
designing efficient signaling schemes. In reality, however, information is
typically not held by a central authority, but is distributed among multiple
sources (third-party ""mediators""), a fact that dramatically changes the
strategic and combinatorial nature of the signaling problem, making it a game
between information providers, as opposed to a traditional mechanism design
problem.
  In this paper we introduce {\em distributed signaling games}, while using
display advertising as a canonical example for introducing this foundational
framework. A distributed signaling game may be a pure coordination game (i.e.,
a distributed optimization task), or a non-cooperative game. In the context of
pure coordination games, we show a wide gap between the computational
complexity of the centralized and distributed signaling problems. On the other
hand, we show that if the information structure of each mediator is assumed to
be ""local"", then there is an efficient algorithm that finds a near-optimal
($5$-approximation) distributed signaling scheme.
  In the context of non-cooperative games, the outcome generated by the
mediators' signals may have different value to each (due to the auctioneer's
desire to align the incentives of the mediators with his own by relative
compensations). We design a mechanism for this problem via a novel application
of Shapley's value, and show that it possesses some interesting properties, in
particular, it always admits a pure Nash equilibrium, and it never decreases
the revenue of the auctioneer."
"When the performance of a team of agents exceeds our expectations or fall
short of them, we often explain this by saying that there was some synergy in
the team---either positive (the team exceeded our expectations) or negative
(they fell short). Our aim in this article is to develop a formal and
principled way of measuring synergies, both positive and negative. Using
characteristic function cooperative games as our underlying model, we present a
formal measure of synergy, based on the idea that a synergy is exhibited when
the performance of a team deviates from the norm. We then show that our synergy
value is the only possible such measure that satisfies certain intuitive
properties. We then investigate some alternative characterisations of this
measure."
"Generalized Second Price (GSP) auctions are widely used by search engines
today to sell their ad slots. Most search engines have supported broad match
between queries and bid keywords when executing GSP auctions, however, it has
been revealed that GSP auction with the standard broad-match mechanism they are
currently using (denoted as SBM-GSP) has several theoretical drawbacks (e.g.,
its theoretical properties are known only for the single-slot case and
full-information setting, and even in this simple setting, the corresponding
worst-case social welfare can be rather bad). To address this issue, we propose
a novel broad-match mechanism, which we call the Probabilistic Broad-Match
(PBM) mechanism. Different from SBM that puts together the ads bidding on all
the keywords matched to a given query for the GSP auction, the GSP with PBM
(denoted as PBM-GSP) randomly samples a keyword according to a predefined
probability distribution and only runs the GSP auction for the ads bidding on
this sampled keyword. We perform a comprehensive study on the theoretical
properties of the PBM-GSP. Specifically, we study its social welfare in the
worst equilibrium, in both full-information and Bayesian settings. The results
show that PBM-GSP can generate larger welfare than SBM-GSP under mild
conditions. Furthermore, we also study the revenue guarantee for PBM-GSP in
Bayesian setting. To the best of our knowledge, this is the first work on
broad-match mechanisms for GSP that goes beyond the single-slot case and the
full-information setting."
"Mobile geo-location advertising, where mobile ads are targeted based on a
user's location, has been identified as a key growth factor for the mobile
market. As with online advertising, a crucial ingredient for their success is
the development of effective economic mechanisms. An important difference is
that mobile ads are shown sequentially over time and information about the user
can be learned based on their movements. Furthermore, ads need to be shown
selectively to prevent ad fatigue. To this end, we introduce, for the first
time, a user model and suitable economic mechanisms which take these factors
into account. Specifically, we design two truthful mechanisms which produce an
advertisement plan based on the user's movements. One mechanism is allocatively
efficient, but requires exponential compute time in the worst case. The other
requires polynomial time, but is not allocatively efficient. Finally, we
experimentally evaluate the tradeoff between compute time and efficiency of our
mechanisms."
"It is well known that no reasonable voting rule is strategyproof. Moreover,
the common Plurality rule is particularly prone to strategic behavior of the
voters and empirical studies show that people often vote strategically in
practice. Multiple game-theoretic models have been proposed to better
understand and predict such behavior and the outcomes it induces. However,
these models often make unrealistic assumptions regarding voters' behavior and
the information on which they base their vote.
  We suggest a new model for strategic voting that takes into account voters'
bounded rationality, as well as their limited access to reliable information.
We introduce a simple behavioral heuristic based on \emph{local dominance},
where each voter considers a set of possible world states without assigning
probabilities to them. This set is constructed based on prospective candidates'
scores (e.g., available from an inaccurate poll). In a \emph{voting
equilibrium}, all voters vote for candidates not dominated within the set of
possible states.
  We prove that these voting equilibria exist in the Plurality rule for a broad
class of local dominance relations (that is, different ways to decide which
states are possible). Furthermore, we show that in an iterative setting where
voters may repeatedly change their vote, local dominance-based dynamics quickly
converge to an equilibrium if voters start from the truthful state. Weaker
convergence guarantees in more general settings are also provided.
  Using extensive simulations of strategic voting on generated and real
preference profiles, we show that convergence is fast and robust, that emerging
equilibria are consistent across various starting conditions, and that they
replicate widely known patterns of human voting behavior such as Duverger's
law. Further, strategic voting generally improves the quality of the winner
compared to truthful voting."
"We study techniques to incentivize self-interested agents to form socially
desirable solutions in scenarios where they benefit from mutual coordination.
Towards this end, we consider coordination games where agents have different
intrinsic preferences but they stand to gain if others choose the same strategy
as them. For non-trivial versions of our game, stable solutions like Nash
Equilibrium may not exist, or may be socially inefficient even when they do
exist. This motivates us to focus on designing efficient algorithms to compute
(almost) stable solutions like Approximate Equilibrium that can be realized if
agents are provided some additional incentives. Our results apply in many
settings like adoption of new products, project selection, and group formation,
where a central authority can direct agents towards a strategy but agents may
defect if they have better alternatives. We show that for any given instance,
we can either compute a high quality approximate equilibrium or a near-optimal
solution that can be stabilized by providing small payments to some players. We
then generalize our model to encompass situations where player relationships
may exhibit complementarities and present an algorithm to compute an
Approximate Equilibrium whose stability factor is linear in the degree of
complementarity. Our results imply that a little influence is necessary in
order to ensure that selfish players coordinate and form socially efficient
solutions."
"Constraints on agent's ability to pay play a major role in auction design for
any setting where the magnitude of financial transactions is sufficiently
large. Those constraints have been traditionally modeled in mechanism design as
\emph{hard budget}, i.e., mechanism is not allowed to charge agents more than a
certain amount. Yet, real auction systems (such as Google AdWords) allow more
sophisticated constraints on agents' ability to pay, such as \emph{average
budgets}. In this work, we investigate the design of Pareto optimal and
incentive compatible auctions for agents with \emph{constrained quasi-linear
utilities}, which captures more realistic models of liquidity constraints that
the agents may have. Our result applies to a very general class of allocation
constraints known as polymatroidal environments, encompassing many settings of
interest such as multi-unit auctions, matching markets, video-on-demand and
advertisement systems.
  Our design is based Ausubel's \emph{clinching framework}. Incentive
compatibility and feasibility with respect to ability-to-pay constraints are
direct consequences of the clinching framework. Pareto-optimality, on the other
hand, is considerably more challenging, since the no-trade condition that
characterizes it depends not only on whether agents have their budgets
exhausted or not, but also on prices {at} which the goods are allocated. In
order to get a handle on those prices, we introduce novel concepts of dropping
prices and saturation. These concepts lead to our main structural result which
is a characterization of the tight sets in the clinching auction outcome and
its relation to dropping prices."
"We examine trade-offs among stakeholders in ad auctions. Our metrics are the
revenue for the utility of the auctioneer, the number of clicks for the utility
of the users and the welfare for the utility of the advertisers. We show how to
optimize linear combinations of the stakeholder utilities, showing that these
can be tackled through a GSP auction with a per-click reserve price. We then
examine constrained optimization of stakeholder utilities.
  We use simulations and analysis of real-world sponsored search auction data
to demonstrate the feasible trade-offs, examining the effect of changing the
allowed number of ads on the utilities of the stakeholders. We investigate both
short term effects, when the players do not have the time to modify their
behavior, and long term equilibrium conditions.
  Finally, we examine a combinatorially richer constrained optimization
problem, where there are several possible allowed configurations (templates) of
ad formats. This model captures richer ad formats, which allow using the
available screen real estate in various ways. We show that two natural
generalizations of the GSP auction rules to this domain are poorly behaved,
resulting in not having a symmetric Nash equilibrium or having one with poor
welfare. We also provide positive results for restricted cases."
"We study the existence of pure Nash equilibrium (PNE) for the mechanisms used
in Internet services (e.g., online reviews and question-answer websites) to
incentivize users to generate high-quality content. Most existing work assumes
that users are homogeneous and have the same ability. However, real-world users
are heterogeneous and their abilities can be very different from each other due
to their diverse background, culture, and profession. In this work, we consider
heterogeneous users with the following framework: (1) the users are
heterogeneous and each of them has a private type indicating the best quality
of the content she can generate; (2) there is a fixed amount of reward to
allocate to the participated users. Under this framework, we study the
existence of pure Nash equilibrium of several mechanisms composed by different
allocation rules, action spaces, and information settings. We prove the
existence of PNE for some mechanisms and the non-existence of PNE for some
mechanisms. We also discuss how to find a PNE for those mechanisms with PNE
either through a constructive way or a search algorithm."
"We study the House Allocation problem (also known as the Assignment problem),
i.e., the problem of allocating a set of objects among a set of agents, where
each agent has ordinal preferences (possibly involving ties) over a subset of
the objects. We focus on truthful mechanisms without monetary transfers for
finding large Pareto optimal matchings. It is straightforward to show that no
deterministic truthful mechanism can approximate a maximum cardinality Pareto
optimal matching with ratio better than 2. We thus consider randomized
mechanisms. We give a natural and explicit extension of the classical Random
Serial Dictatorship Mechanism (RSDM) specifically for the House Allocation
problem where preference lists can include ties. We thus obtain a universally
truthful randomized mechanism for finding a Pareto optimal matching and show
that it achieves an approximation ratio of $\frac{e}{e-1}$. The same bound
holds even when agents have priorities (weights) and our goal is to find a
maximum weight (as opposed to maximum cardinality) Pareto optimal matching. On
the other hand we give a lower bound of $\frac{18}{13}$ on the approximation
ratio of any universally truthful Pareto optimal mechanism in settings with
strict preferences. In the case that the mechanism must additionally be
non-bossy, an improved lower bound of $\frac{e}{e-1}$ holds. This lower bound
is tight given that RSDM for strict preference lists is non-bossy. We moreover
interpret our problem in terms of the classical secretary problem and prove
that our mechanism provides the best randomized strategy of the administrator
who interviews the applicants."
"In the restructured electricity industry, electricity pooling markets are an
oligopoly with strategic producers possessing private information (private
production cost function). We focus on pooling markets where aggregate demand
is represented by a non-strategic agent.
  Inelasticity of demand is a main difficulty in electricity markets which can
potentially result in market failure and high prices. We consider demand to be
inelastic.
  We propose a market mechanism that has the following features. (F1) It is
individually rational. (F2) It is budget balanced. (F3) It is price efficient,
that is, at equilibrium the price of electricity is equal to the marginal cost
of production. (F4) The energy production profile corresponding to every
non-zero Nash equilibrium of the game induced by the mechanism is a solution of
the corresponding centralized problem where the objective is the maximization
of the sum of the producers' and consumers' utilities.
  We identify some open problems associated with our approach to electricity
pooling markets."
"We study two-player (zero-sum) concurrent mean-payoff games played on a
finite-state graph. We focus on the important sub-class of ergodic games where
all states are visited infinitely often with probability 1. The algorithmic
study of ergodic games was initiated in a seminal work of Hoffman and Karp in
1966, but all basic complexity questions have remained unresolved. Our main
results for ergodic games are as follows: We establish (1) an optimal
exponential bound on the patience of stationary strategies (where patience of a
distribution is the inverse of the smallest positive probability and represents
a complexity measure of a stationary strategy); (2) the approximation problem
lie in FNP; (3) the approximation problem is at least as hard as the decision
problem for simple stochastic games (for which NP intersection coNP is the
long-standing best known bound). We present a variant of the strategy-iteration
algorithm by Hoffman and Karp; show that both our algorithm and the classical
value-iteration algorithm can approximate the value in exponential time; and
identify a subclass where the value-iteration algorithm is a FPTAS. We also
show that the exact value can be expressed in the existential theory of the
reals, and establish square-root sum hardness for a related class of games."
"Priced timed games (PTGs) are two-player zero-sum games played on the
infinite graph of configurations of priced timed automata where two players
take turns to choose transitions in order to optimize cost to reach target
states. Bouyer et al. and Alur, Bernadsky, and Madhusudan independently
proposed algorithms to solve PTGs with nonnegative prices under certain
divergence restriction over prices. Brihaye, Bruyere, and Raskin later provided
a justification for such a restriction by showing the undecidability of the
optimal strategy synthesis problem in the absence of this divergence
restriction. This problem for PTGs with one clock has long been conjectured to
be in polynomial time, however the current best known algorithm, by Hansen,
Ibsen-Jensen, and Miltersen, is exponential. We extend this picture by studying
PTGs with both negative and positive prices. We refine the undecidability
results for optimal strategy synthesis problem, and show undecidability for
several variants of optimal reachability cost objectives including reachability
cost, time-bounded reachability cost, and repeated reachability cost
objectives. We also identify a subclass with bi-valued price-rates and give a
pseudo-polynomial (polynomial when prices are nonnegative) algorithm to
partially answer the conjecture on the complexity of one-clock PTGs."
"This paper develops tools for welfare and revenue analyses of Bayes-Nash
equilibria in asymmetric auctions with single-dimensional agents. We employ
these tools to derive price of anarchy results for social welfare and revenue.
Our approach separates the standard smoothness framework into two distinct
parts, isolating the analysis common to any auction from the analysis specific
to a given auction. The first part relates a bidder's contribution to welfare
in equilibrium to their contribution to welfare in the optimal auction using
the price the bidder faces for additional allocation. Intuitively, either an
agent's utility and hence contribution to welfare is high, or the price she has
to pay for additional allocation is high relative to her value. We call this
condition value covering; it holds in every Bayes-Nash equilibrium of any
auction. The second part, revenue covering, relates the prices bidders face for
additional allocation to the revenue of the auction, using an auction's rules
and feasibility constraints. Combining the two parts gives approximation
results to the optimal welfare, and, under the right conditions, the optimal
revenue. In mechanisms with reserve prices, our welfare results show
approximation with respect to the optimal mechanism with the same reserves.
  As a center-piece result, we analyze the single-item first-price auction with
individual monopoly reserves. When each distribution satisfies a regularity
condition the auction's revenue is at least a $2e/(e-1) \approx 3.16$
approximation to the revenue of the optimal auction. We also give bounds for
matroid auctions with first-price or all-pay semantics, and the generalized
first-price position auction. Finally, we give an extension theorem for
simultaneous composition, i.e., when multiple auctions are run simultaneously,
with single-valued, unit-demand agents."
"Good economic mechanisms depend on the preferences of participants in the
mechanism. For example, the revenue-optimal auction for selling an item is
parameterized by a reserve price, and the appropriate reserve price depends on
how much the bidders are willing to pay. A mechanism designer can potentially
learn about the participants' preferences by observing historical data from the
mechanism; the designer could then update the mechanism in response to learned
preferences to improve its performance. The challenge of such an approach is
that the data corresponds to the actions of the participants and not their
preferences. Preferences can potentially be inferred from actions but the
degree of inference possible depends on the mechanism. In the optimal auction
example, it is impossible to learn anything about preferences of bidders who
are not willing to pay the reserve price. These bidders will not cast bids in
the auction and, from historical bid data, the auctioneer could never learn
that lowering the reserve price would give a higher revenue (even if it would).
To address this impossibility, the auctioneer could sacrifice revenue
optimality in the initial auction to obtain better inference properties so that
the auction's parameters can be adapted to changing preferences in the future.
This paper develops the theory for optimal mechanism design subject to good
inferability."
"Assignment games represent a tractable yet versatile model of two-sided
markets with transfers. We study the likely properties of the core of randomly
generated assignment games. If the joint productivities of every firm and
worker are i.i.d bounded random variables, then with high probability all
workers are paid roughly equal wages, and all firms make similar profits. This
implies that core allocations vary significantly in balanced markets, but that
there is core convergence in even slightly unbalanced markets. For the
benchmark case of uniform distribution, we provide a tight bound for the
workers' share of the surplus under the firm-optimal core allocation. We
present simulation results suggesting that the phenomena analyzed appear even
in medium-sized markets. Finally, we briefly discuss the effects of unbounded
distributions and the ways in which they may affect wage dispersion."
"We consider reallocation problems in settings where the initial endowment of
each agent consists of a subset of the resources. The private information of
the players is their value for every possible subset of the resources. The goal
is to redistribute resources among agents to maximize efficiency. Monetary
transfers are allowed, but participation is voluntary.
  We develop incentive-compatible, individually-rational and budget balanced
mechanisms for several classic settings, including bilateral trade, partnership
dissolving, Arrow-Debreu markets, and combinatorial exchanges. All our
mechanisms (except one) provide a constant approximation to the optimal
efficiency in these settings, even in ones where the preferences of the agents
are complex multi-parameter functions."
"We consider the distributed channel selection problem in the context of
device-to-device (D2D) communication as an underlay to a cellular network.
Underlaid D2D users communicate directly by utilizing the cellular spectrum but
their decisions are not governed by any centralized controller. Selfish D2D
users that compete for access to the resources construct a distributed system,
where the transmission performance depends on channel availability and quality.
This information, however, is difficult to acquire. Moreover, the adverse
effects of D2D users on cellular transmissions should be minimized. In order to
overcome these limitations, we propose a network-assisted distributed channel
selection approach in which D2D users are only allowed to use vacant cellular
channels. This scenario is modeled as a multi-player multi-armed bandit game
with side information, for which a distributed algorithmic solution is
proposed. The solution is a combination of no-regret learning and calibrated
forecasting, and can be applied to a broad class of multi-player stochastic
learning problems, in addition to the formulated channel selection problem.
Analytically, it is established that this approach not only yields vanishing
regret (in comparison to the global optimal solution), but also guarantees that
the empirical joint frequencies of the game converge to the set of correlated
equilibria."
"We investigate the problem of a principal looking to contract an expert to
provide a probability forecast for a categorical event. We assume all experts
have a common public prior on the event's probability, but can form more
accurate opinions by engaging in research. Various experts' research costs are
unknown to the principal. We present a truthful and efficient mechanism for the
principal's problem of contracting an expert. This results in the principal
contracting the best expert to do the work, and the principal's expected
utility is equivalent to having the second best expert in-house. Our mechanism
connects scoring rules with auctions, a connection that is useful when
obtaining new information requires costly research."
"We study multidimensional mechanism design in a common scenario where players
have private information about their willingness to pay and their ability to
pay. We provide a complete characterization of dominant-strategy
incentive-compatible direct mechanisms where over-reporting the budget is not
possible. In several settings, reporting larger budgets can be made suboptimal
with a small randomized modification to the payments. We then derive a closely
related partial characterization for the general case where players can
arbitrarily misreport their private budgets. Immediate applications of these
results include simple characterizations for mechanisms with publicly-known
budgets and for mechanisms without monetary transfers.
  The celebrated revenue equivalence theorem states that the seller""s revenue
for a broad class of standard auction formats and settings will be the same in
equilibrium. Our main application is a revenue equivalence theorem for
financially constrained bidders."
"Players (people, firms, states, etc.) have privacy concerns that may affect
their choice of actions in strategic settings. We use a variant of signaling
games to model this effect and study its relation to pooling behavior,
misrepresentation of information, and inefficiency. We discuss these issues and
show that common intuitions may lead to inaccurate conclusions about the
implications of privacy concerns."
"A prevalent market structure in the Internet economy consists of buyers and
sellers connected by a platform (such as Amazon or eBay) that acts as an
intermediary and keeps a share of the revenue of each transaction. While the
optimal mechanism that maximizes the intermediary's profit in such a setting
may be quite complicated, the mechanisms observed in reality are generally much
simpler, e.g., applying an affine function to the price of the transaction as
the intermediary's fee. Loertscher and Niedermayer [2007] initiated the study
of such fee-setting mechanisms in two-sided markets, and we continue this
investigation by addressing the question of when an affine fee schedule is
approximately optimal for worst-case seller distribution. On one hand our work
supplies non-trivial sufficient conditions on the buyer side (i.e. linearity of
marginal revenue function, or MHR property of value and value minus cost
distributions) under which an affine fee schedule can obtain a constant
fraction of the intermediary's optimal profit for all seller distributions. On
the other hand we complement our result by showing that proper affine
fee-setting mechanisms (e.g. those used in eBay and Amazon selling plans) are
unable to extract a constant fraction of optimal profit in the worst-case
seller distribution. As subsidiary results we also show there exists a constant
gap between maximum surplus and maximum revenue under the aforementioned
conditions. Most of the mechanisms that we propose are also prior-independent
with respect to the seller, which signifies the practical implications of our
result."
"In an $\epsilon$-Nash equilibrium, a player can gain at most $\epsilon$ by
unilaterally changing his behaviour. For two-player (bimatrix) games with
payoffs in $[0,1]$, the best-known$\epsilon$ achievable in polynomial time is
0.3393. In general, for $n$-player games an $\epsilon$-Nash equilibrium can be
computed in polynomial time for an $\epsilon$ that is an increasing function of
$n$ but does not depend on the number of strategies of the players. For
three-player and four-player games the corresponding values of $\epsilon$ are
0.6022 and 0.7153, respectively. Polymatrix games are a restriction of general
$n$-player games where a player's payoff is the sum of payoffs from a number of
bimatrix games. There exists a very small but constant $\epsilon$ such that
computing an $\epsilon$-Nash equilibrium of a polymatrix game is \PPAD-hard.
Our main result is that a $(0.5+\delta)$-Nash equilibrium of an $n$-player
polymatrix game can be computed in time polynomial in the input size and
$\frac{1}{\delta}$. Inspired by the algorithm of Tsaknakis and Spirakis, our
algorithm uses gradient descent on the maximum regret of the players. We also
show that this algorithm can be applied to efficiently find a
$(0.5+\delta)$-Nash equilibrium in a two-player Bayesian game."
"Mobile crowdsensing (MCS) has been intensively explored recently due to its
flexible and pervasive sensing ability. Although many incentive mechanisms have
been built to attract extensive user participation, Most of these mechanisms
focus only on independent task scenarios, where the sensing tasks are
independent of each other. On the contrary, we focus on a periodical task
scenario, where each user participates in the same type of sensing tasks
periodically. In this paper, we consider the long-term user participation
incentive in a general periodical MCS system from a frugality payment
perspective. We explore the issue under both semi-online (the intra-period
interactive process is synchronous while the inter-period interactive process
is sequential and asynchronous during each period) and online user arrival
models (the previous two interactive processes are sequential and
asynchronous). In particular, we first propose a semi-online frugal incentive
mechanism by introducing a Lyapunov method. Moreover, we also extend it to an
online frugal incentive mechanism, which satisfies the constant frugality.
Besides, the two mechanisms can also satisfy computational efficiency,
asymptotical optimality, individual rationality and truthfulness. Through
extensive simulations, we evaluate the performance and validate the theoretical
properties of our online mechanisms."
"We present a systematic study of Plurality elections with strategic voters
who, in addition to having preferences over election winners, have secondary
preferences, which govern their behavior when their vote cannot affect the
election outcome. Specifically, we study two models that have been recently
considered in the literature: lazy voters, who prefer to abstain when they are
not pivotal, and truth-biased voters, who prefer to vote truthfully when they
are not pivotal. We extend prior work by investigating the behavior of both
lazy and truth-biased voters under different tie-breaking rules (lexicographic
rule, random voter rule, random candidate rule). Two of these six combinations
of secondary preferences and a tie-breaking rule have been studied in prior
work. In order to understand the impact of different secondary preferences and
tie-breaking rules on the election outcomes, we study the remaining four
combinations. We characterize pure Nash equilibria (PNE) of the resulting
strategic games and study the complexity of related computational problems. Our
results extend to settings where some of the voters may be non-strategic."
"We provide a duality-based framework for revenue maximization in a
multiple-good monopoly. Our framework shows that every optimal mechanism has a
certificate of optimality, taking the form of an optimal transportation map
between measures. Using our framework, we prove that grand-bundling mechanisms
are optimal if and only if two stochastic dominance conditions hold between
specific measures induced by the buyer's type distribution. This result
strengthens several results in the literature, where only sufficient conditions
for grand-bundling optimality have been provided. As a corollary of our tight
characterization of grand-bundling optimality, we show that the optimal
mechanism for $n$ independent uniform items each supported on $[c,c+1]$ is a
grand-bundling mechanism, as long as $c$ is sufficiently large, extending
Pavlov's result for 2 items [Pavlov11]. In contrast, our characterization also
implies that, for all $c$ and for all sufficiently large $n$, the optimal
mechanism for $n$ independent uniform items supported on $[c,c+1]$ is not a
grand bundling mechanism. The necessary and sufficient condition for grand
bundling optimality is a special case of our more general characterization
result that provides necessary and sufficient conditions for the optimality of
an arbitrary mechanism (with a finite menu size) for an arbitrary type
distribution."
"We study uncoordinated matching markets with additional local constraints
that capture, e.g., restricted information, visibility, or externalities in
markets. Each agent is a node in a fixed matching network and strives to be
matched to another agent. Each agent has a complete preference list over all
other agents it can be matched with. However, depending on the constraints and
the current state of the game, not all possible partners are available for
matching at all times. For correlated preferences, we propose and study a
general class of hedonic coalition formation games that we call coalition
formation games with constraints. This class includes and extends many recently
studied variants of stable matching, such as locally stable matching, socially
stable matching, or friendship matching. Perhaps surprisingly, we show that all
these variants are encompassed in a class of ""consistent"" instances that always
allow a polynomial improvement sequence to a stable state. In addition, we show
that for consistent instances there always exists a polynomial sequence to
every reachable state. Our characterization is tight in the sense that we
provide exponential lower bounds when each of the requirements for consistency
is violated. We also analyze matching with uncorrelated preferences, where we
obtain a larger variety of results. While socially stable matching always
allows a polynomial sequence to a stable state, for other classes different
additional assumptions are sufficient to guarantee the same results. For the
problem of reaching a given stable state, we show NP-hardness in almost all
considered classes of matching games."
"This paper presents models for predicted click-through rates in position
auctions that take into account two possibilities that are not normally
considered---that the identities of ads shown in other positions may affect the
probability that an ad in a particular position receives a click
(externalities) and that some ads may be less adversely affected by being shown
in a lower position than others (brand effects). We present a general axiomatic
methodology for how click probabilities are affected by the qualities of the
ads in the other positions, and illustrate that using these axioms will
increase revenue as long as higher quality ads tend to be ranked ahead of lower
quality ads. We also present appropriate algorithms for selecting the optimal
allocation of ads when predicted click-through rates are governed by either the
models of externalities or brand effects that we consider. Finally, we analyze
the performance of a greedy algorithm of ranking the ads by their expected
cost-per-1000-impressions bids when the true click-through rates are governed
by our model of predicted click-through rates with brand effects and illustrate
that such an algorithm will potentially cost as much as half of the total
possible social welfare."
"We propose the study of computing the Shapley value for a new class of
cooperative games that we call budgeted games, and investigate in particular
knapsack budgeted games, a version modeled after the classical knapsack
problem. In these games, the ""value"" of a set $S$ of agents is determined only
by a critical subset $T\subseteq S$ of the agents and not the entirety of $S$
due to a budget constraint that limits how large $T$ can be. We show that the
Shapley value can be computed in time faster than by the na\""ive exponential
time algorithm when there are sufficiently many agents, and also provide an
algorithm that approximates the Shapley value within an additive error. For a
related budgeted game associated with a greedy heuristic, we show that the
Shapley value can be computed in pseudo-polynomial time. Furthermore, we
generalize our proof techniques and propose what we term algorithmic
representation framework that captures a broad class of cooperative games with
the property of efficient computation of the Shapley value. The main idea is
that the problem of determining the efficient computation can be reduced to
that of finding an alternative representation of the games and an associated
algorithm for computing the underlying value function with small time and space
complexities in the representation size."
"We consider concurrent games played by two-players on a finite-state graph,
where in every round the players simultaneously choose a move, and the current
state along with the joint moves determine the successor state. We study a
fundamental objective, namely, mean-payoff objective, where a reward is
associated to each transition, and the goal of player 1 is to maximize the
long-run average of the rewards, and the objective of player 2 is strictly the
opposite. The path constraint for player 1 could be qualitative, i.e., the
mean-payoff is the maximal reward, or arbitrarily close to it; or quantitative,
i.e., a given threshold between the minimal and maximal reward. We consider the
computation of the almost-sure (resp. positive) winning sets, where player 1
can ensure that the path constraint is satisfied with probability 1 (resp.
positive probability). Our main results for qualitative path constraints are as
follows: (1) we establish qualitative determinacy results that show that for
every state either player 1 has a strategy to ensure almost-sure (resp.
positive) winning against all player-2 strategies, or player 2 has a spoiling
strategy to falsify almost-sure (resp. positive) winning against all player-1
strategies; (2) we present optimal strategy complexity results that precisely
characterize the classes of strategies required for almost-sure and positive
winning for both players; and (3) we present quadratic time algorithms to
compute the almost-sure and the positive winning sets, matching the best known
bound of algorithms for much simpler problems (such as reachability
objectives). For quantitative constraints we show that a polynomial time
solution for the almost-sure or the positive winning set would imply a solution
to a long-standing open problem (the value problem for turn-based deterministic
mean-payoff games) that is not known to be solvable in polynomial time."
"Network creation games model the creation and usage costs of networks formed
by n selfish nodes. Each node v can buy a set of edges, each for a fixed price
\alpha > 0. Its goal is to minimize its private costs, i.e., the sum (SUM-game,
Fabrikant et al., PODC 2003) or maximum (MAX-game, Demaine et al., PODC 2007)
of distances from $v$ to all other nodes plus the prices of the bought edges.
The above papers show the existence of Nash equilibria as well as upper and
lower bounds for the prices of anarchy and stability. In several subsequent
papers, these bounds were improved for a wide range of prices \alpha. In this
paper, we extend these models by incorporating quality-of-service aspects: Each
edge cannot only be bought at a fixed quality (edge length one) for a fixed
price \alpha. Instead, we assume that quality levels (i.e., edge lengths) are
varying in a fixed interval [\beta,B], 0 < \beta <= B. A node now cannot only
choose which edge to buy, but can also choose its quality x, for the price
p(x), for a given price function p. For both games and all price functions, we
show that Nash equilibria exist and that the price of stability is either
constant or depends only on the interval size of available edge lengths. Our
main results are bounds for the price of anarchy. In case of the SUM-game, we
show that they are tight if price functions decrease sufficiently fast."
"We consider a multilevel network game, where nodes can improve their
communication costs by connecting to a high-speed network. The $n$ nodes are
connected by a static network and each node can decide individually to become a
gateway to the high-speed network. The goal of a node $v$ is to minimize its
private costs, i.e., the sum (SUM-game) or maximum (MAX-game) of communication
distances from $v$ to all other nodes plus a fixed price $\alpha > 0$ if it
decides to be a gateway. Between gateways the communication distance is $0$,
and gateways also improve other nodes' distances by behaving as shortcuts. For
the SUM-game, we show that for $\alpha \leq n-1$, the price of anarchy is
$\Theta(n/\sqrt{\alpha})$ and in this range equilibria always exist. In range
$\alpha \in (n-1,n(n-1))$ the price of anarchy is $\Theta(\sqrt{\alpha})$, and
for $\alpha \geq n(n-1)$ it is constant. For the MAX-game, we show that the
price of anarchy is either $\Theta(1 + n/\sqrt{\alpha})$, for $\alpha\geq 1$,
or else $1$. Given a graph with girth of at least $4\alpha$, equilibria always
exist. Concerning the dynamics, both the SUM-game and the MAX-game are not
potential games. For the SUM-game, we even show that it is not weakly acyclic."
"Cooperative games provide a framework for fair and stable profit allocation
in multi-agent systems. \emph{Core}, \emph{least-core} and \emph{nucleolus} are
such solution concepts that characterize stability of cooperation. In this
paper, we study the algorithmic issues on the least-core and nucleolus of
threshold cardinality matching games (TCMG). A TCMG is defined on a graph
$G=(V,E)$ and a threshold $T$, in which the player set is $V$ and the profit of
a coalition $S\subseteq V$ is 1 if the size of a maximum matching in $G[S]$
meets or exceeds $T$, and 0 otherwise. We first show that for a TCMG, the
problems of computing least-core value, finding and verifying least-core payoff
are all polynomial time solvable. We also provide a general characterization of
the least core for a large class of TCMG. Next, based on Gallai-Edmonds
Decomposition in matching theory, we give a concise formulation of the
nucleolus for a typical case of TCMG which the threshold $T$ equals $1$. When
the threshold $T$ is relevant to the input size, we prove that the nucleolus
can be obtained in polynomial time in bipartite graphs and graphs with a
perfect matching."
"In the random assignment problem, objects are randomly assigned to agents
keeping in view the agents' preferences over objects. A random assignment
specifies the probability of an agent getting an object. We examine the
structural and computational aspects of ex post efficiency of random
assignments. We first show that whereas an ex post efficient assignment can be
computed easily, checking whether a given random assignment is ex post
efficient is NP-complete. Hence implementing a given random assignment via
deterministic Pareto optimal assignments is NP-hard. We then formalize another
concept of efficiency called robust ex post efficiency that is weaker than
stochastic dominance efficiency but stronger than ex post efficiency. We
present a characterization of robust ex post efficiency and show that it can be
tested in polynomial time if there are a constant number of agent types. It is
shown that the well-known random serial dictatorship rule is not robust ex post
efficient. Finally, we show that whereas robust ex post efficiency depends
solely on which entries of the assignment matrix are zero/non-zero, ex post
efficiency of an assignment depends on the actual values."
"In the framework of finite games in extensive form with perfect information
and strict preferences, this paper introduces a new equilibrium concept: the
Perfect Prediction Equilibrium (PPE).
  In the Nash paradigm, rational players consider that the opponent's strategy
is fixed while maximizing their payoff. The PPE, on the other hand, models the
behavior of agents with an alternate form of rationality that involves a
Stackelberg competition with the past.
  Agents with this form of rationality integrate in their reasoning that they
have such accurate logical and predictive skills, that the world is fully
transparent: all players share the same knowledge and know as much as an
omniscient external observer. In particular, there is common knowledge of the
solution of the game including the reached outcome and the thought process
leading to it. The PPE is stable given each player's knowledge of its actual
outcome and uses no assumptions at unreached nodes.
  This paper gives the general definition and construction of the PPE as a
fixpoint problem, proves its existence, uniqueness and Pareto optimality, and
presents two algorithms to compute it. Finally, the PPE is put in perspective
with existing literature (Newcomb's Problem, Superrationality, Nash
Equilibrium, Subgame Perfect Equilibrium, Backward Induction Paradox, Forward
Induction)."
"A real-valued game has the finite improvement property (FIP), if starting
from an arbitrary strategy profile and letting the players change strategies to
increase their individual payoffs in a sequential but non-deterministic order
always reaches a Nash equilibrium. E.g., potential games have the FIP. Many of
them have the FIP by chance nonetheless, since modifying even a single payoff
may ruin the property. This article characterises (in quadratic time) the class
of the finite games where FIP not only holds but is also preserved when
modifying all the occurrences of an arbitrary payoff. The characterisation
relies on a pattern-matching sufficient condition for games (finite or
infinite) to enjoy the FIP, and is followed by an inductive description of this
class.
  A real-valued game is weakly acyclic if the improvement described above can
reach a Nash equilibrium. This article characterises the finite such games
using Markov chains and almost sure convergence to equilibrium. It also gives
an inductive description of the two-player such games."
"We consider concurrent mean-payoff games, a very well-studied class of
two-player (player 1 vs player 2) zero-sum games on finite-state graphs where
every transition is assigned a reward between 0 and 1, and the payoff function
is the long-run average of the rewards. The value is the maximal expected
payoff that player 1 can guarantee against all strategies of player 2. We
consider the computation of the set of states with value 1 under finite-memory
strategies for player 1, and our main results for the problem are as follows:
(1) we present a polynomial-time algorithm; (2) we show that whenever there is
a finite-memory strategy, there is a stationary strategy that does not need
memory at all; and (3) we present an optimal bound (which is double
exponential) on the patience of stationary strategies (where patience of a
distribution is the inverse of the smallest positive probability and represents
a complexity measure of a stationary strategy)."
"We consider the assignment problem in which agents express ordinal
preferences over $m$ objects and the objects are allocated to the agents based
on the preferences. In a recent paper, Brams, Kilgour, and Klamler (2014)
presented the AL method to compute an envy-free assignment for two agents. The
AL method crucially depends on the assumption that agents have strict
preferences over objects. We generalize the AL method to the case where agents
may express indifferences and prove the axiomatic properties satisfied by the
algorithm. As a result of the generalization, we also get a $O(m)$ speedup on
previous algorithms to check whether a complete envy-free assignment exists or
not. Finally, we show that unless P=NP, there can be no polynomial-time
extension of GAL to the case of arbitrary number of agents."
"We provide a new, much simplified and straightforward proof to a result of
Pavlov [2011] regarding the revenue maximizing mechanism for selling two goods
with uniformly i.i.d. valuations over intervals $[c,c+1]$, to an additive
buyer. This is done by explicitly defining optimal dual solutions to a relaxed
version of the problem, where the convexity requirement for the bidder's
utility has been dropped. Their optimality comes directly from their structure,
through the use of exact complementarity. For $c=0$ and $c\geq 0.092$ it turns
out that the corresponding optimal primal solution is a feasible selling
mechanism, thus the initial relaxation comes without a loss, and revenue
maximality follows. However, for $0<c<0.092$ that's not the case, providing the
first clear example where relaxing convexity provably does not come for free,
even in a two-item regularly i.i.d. setting."
"We revisit in this paper the relation between evolution of species and the
mathematical tool of evolutionary games, which has been used to model and
predict it. We indicate known shortcoming of this model that restricts the
capacity of evolutionary games to model groups of individuals that share a
common gene or a common fitness function. In this paper we provide a new
concept to remedy this shortcoming in the standard evolutionary games in order
to cover this kind of behavior. Further, we explore the relationship between
this new concept and Nash equilibrium or ESS. We indicate through the study of
some example in the biology as Hawk and Dove game, Stag Hunt Game and Prisoner
Dilemma, that when taking into account a utility that is common to a group of
individuals, the equilibrium structure may change dramatically. We also study
the multiple access control in slotted Aloha based wireless networks. We
analyze the impact of the altruism behavior on the performance at the
equilibrium."
"We study procurement games where each seller supplies multiple units of his
item, with a cost per unit known only to him. The buyer can purchase any number
of units from each seller, values different combinations of the items
differently, and has a budget for his total payment.
  For a special class of procurement games, the {\em bounded knapsack} problem,
we show that no universally truthful budget-feasible mechanism can approximate
the optimal value of the buyer within $\ln n$, where $n$ is the total number of
units of all items available. We then construct a polynomial-time mechanism
that gives a $4(1+\ln n)$-approximation for procurement games with {\em concave
additive valuations}, which include bounded knapsack as a special case. Our
mechanism is thus optimal up to a constant factor. Moreover, for the bounded
knapsack problem, given the well-known FPTAS, our results imply there is a
provable gap between the optimization domain and the mechanism design domain.
  Finally, for procurement games with {\em sub-additive valuations}, we
construct a universally truthful budget-feasible mechanism that gives an
$O(\frac{\log^2 n}{\log \log n})$-approximation in polynomial time with a
demand oracle."
"A durable good is a long-lasting good that can be consumed repeatedly over
time, and a duropolist is a monopolist in the market of a durable good. In
1972, Ronald Coase conjectured that a duropolist who lacks commitment power
cannot sell the good above the competitive price if the time between periods
approaches zero. Coase's counterintuitive conjecture was later proven by Gul et
al. (1986) under an infinite time horizon model with non-atomic consumers.
Remarkably, the situation changes dramatically for atomic consumers and an
infinite time horizon. Bagnoli et al. (1989) showed the existence of a
subgame-perfect Nash equilibrium where the duropolist extracts all the consumer
surplus. Observe that, in these cases, duropoly profits are either arbitrarily
smaller or arbitrarily larger than the corresponding static monopoly profits --
the profit a monopolist for an equivalent consumable good could generate. In
this paper we show that the result of Bagnoli et al. (1989) is in fact driven
by the infinite time horizon. Indeed, we prove that for finite time horizons
and atomic agents, in any equilibrium satisfying the standard skimming
property, duropoly profits are at most an additive factor more than static
monopoly profits. In particular, duropoly profits are always at least static
monopoly profits but never exceed twice the static monopoly profits.
  Finally we show that, for atomic consumers, equilibria may exist that do not
satisfy the skimming property. For two time periods, we prove that amongst all
equilibria that maximize duropoly profits, at least one of them satisfies the
skimming property. We conjecture that this is true for any number of time
period."
"Several notions of game enjoy a Nash-like notion of equilibrium without
guarantee of existence. There are different ways of weakening a definition of
Nash-like equilibrium in order to guarantee the existence of a weakened
equilibrium. Nash's approach to the problem for strategic games is
probabilistic, \textit{i.e.} continuous, and static. CP and BR approaches for
CP and BR games are discrete and dynamic. This paper proposes an approach that
lies between those two different approaches: a discrete and static approach.
multi strategic games are introduced as a formalism that is able to express
both sequential and simultaneous decision-making, which promises a good
modelling power. multi strategic games are a generalisation of strategic games
and sequential graph games that still enjoys a Cartesian product structure,
\textit{i.e.} where agent actually choose their strategies. A pre-fixed point
result allows guaranteeing existence of discrete and non deterministic
equilibria. On the one hand, these equilibria can be computed with polynomial
(low) complexity. On the other hand, they are effective in terms of
recommendation, as shown by a numerical example."
"The quest for optimal/stable paths in graphs has gained attention in a few
practical or theoretical areas. To take part in this quest this chapter adopts
an equilibrium-oriented approach that is abstract and general: it works with
(quasi-arbitrary) arc-labelled digraphs, and it assumes very little about the
structure of the sought paths and the definition of equilibrium, \textit{i.e.}
optimality/stability. In this setting, this chapter presents a sufficient
condition for equilibrium existence for every graph; it also presents a
necessary condition for equilibrium existence for every graph. The necessary
condition does not imply the sufficient condition a priori. However, the
chapter pinpoints their logical difference and thus identifies what work
remains to be done. Moreover, the necessary and the sufficient conditions
coincide when the definition of optimality relates to a total order, which
provides a full-equivalence property. These results are applied to network
routing."
"Simple stochastic games are two-player zero-sum stochastic games with
turn-based moves, perfect information, and reachability winning conditions. We
present two new algorithms computing the values of simple stochastic games.
Both of them rely on the existence of optimal permutation strategies, a class
of positional strategies derived from permutations of the random vertices. The
""permutation-enumeration"" algorithm performs an exhaustive search among these
strategies, while the ""permutation-improvement"" algorithm is based on
successive improvements, \`a la Hoffman-Karp. Our algorithms improve previously
known algorithms in several aspects. First they run in polynomial time when the
number of random vertices is fixed, so the problem of solving simple stochastic
games is fixed-parameter tractable when the parameter is the number of random
vertices. Furthermore, our algorithms do not require the input game to be
transformed into a stopping game. Finally, the permutation-enumeration
algorithm does not use linear programming, while the permutation-improvement
algorithm may run in polynomial time."
"Common Knowledge Logic is meant to describe situations of the real world
where a group of agents is involved. These agents share knowledge and make
strong statements on the knowledge of the other agents (the so called
\emph{common knowledge}). But as we know, the real world changes and overall
information on what is known about the world changes as well. The changes are
described by dynamic logic. To describe knowledge changes, dynamic logic should
be combined with logic of common knowledge. In this paper we describe
experiments which we have made about the integration in a unique framework of
common knowledge logic and dynamic logic in the proof assistant \Coq. This
results in a set of fully checked proofs for readable statements. We describe
the framework and how a proof can be"
"Imitating successful behavior is a natural and frequently applied approach to
trust in when facing scenarios for which we have little or no experience upon
which we can base our decision. In this paper, we consider such behavior in
atomic congestion games. We propose to study concurrent imitation dynamics that
emerge when each player samples another player and possibly imitates this
agents' strategy if the anticipated latency gain is sufficiently large. Our
main focus is on convergence properties. Using a potential function argument,
we show that our dynamics converge in a monotonic fashion to stable states. In
such a state none of the players can improve its latency by imitating somebody
else. As our main result, we show rapid convergence to approximate equilibria.
At an approximate equilibrium only a small fraction of agents sustains a
latency significantly above or below average. In particular, imitation dynamics
behave like fully polynomial time approximation schemes (FPTAS). Fixing all
other parameters, the convergence time depends only in a logarithmic fashion on
the number of agents. Since imitation processes are not innovative they cannot
discover unused strategies. Furthermore, strategies may become extinct with
non-zero probability. For the case of singleton games, we show that the
probability of this event occurring is negligible. Additionally, we prove that
the social cost of a stable state reached by our dynamics is not much worse
than an optimal state in singleton congestion games with linear latency
function. Finally, we discuss how the protocol can be extended such that, in
the long run, dynamics converge to a Nash equilibrium."
"We show that there is a polynomial-time approximation scheme for computing
Nash equilibria in anonymous games with any fixed number of strategies (a very
broad and important class of games), extending the two-strategy result of
Daskalakis and Papadimitriou 2007. The approximation guarantee follows from a
probabilistic result of more general interest: The distribution of the sum of n
independent unit vectors with values ranging over {e1, e2, ...,ek}, where ei is
the unit vector along dimension i of the k-dimensional Euclidean space, can be
approximated by the distribution of the sum of another set of independent unit
vectors whose probabilities of obtaining each value are multiples of 1/z for
some integer z, and so that the variational distance of the two distributions
is at most eps, where eps is bounded by an inverse polynomial in z and a
function of k, but with no dependence on n. Our probabilistic result specifies
the construction of a surprisingly sparse eps-cover -- under the total
variation distance -- of the set of distributions of sums of independent unit
vectors, which is of interest on its own right."
"Coordination games describe social or economic interactions in which the
adoption of a common strategy has a higher payoff. They are classically used to
model the spread of conventions, behaviors, and technologies in societies. Here
we consider a two-strategies coordination game played asynchronously between
the nodes of a network. Agents behave according to a noisy best-response
dynamics.
  It is known that noise removes the degeneracy among equilibria: In the long
run, the ``risk-dominant'' behavior spreads throughout the network. Here we
consider the problem of computing the typical time scale for the spread of this
behavior. In particular, we study its dependence on the network structure and
derive a dichotomy between highly-connected, non-local graphs that show slow
convergence, and poorly connected, low dimensional graphs that show fast
convergence."
"We consider some well-known families of two-player, zero-sum, perfect
information games that can be viewed as special cases of Shapley's stochastic
games. We show that the following tasks are polynomial time equivalent:
  - Solving simple stochastic games.
  - Solving stochastic mean-payoff games with rewards and probabilities given
in unary. - Solving stochastic mean-payoff games with rewards and probabilities
given in binary."
"It is NP-hard to decide if a given pure-strategy Nash equilibrium of a given
three-player game in strategic form with integer payoffs is trembling hand
perfect."
"Cooperative multihop communication can greatly increase network throughput,
yet packet forwarding for other nodes involves opportunity and energy cost for
relays. Thus one of the pre-requisite problems in the successful implementation
of multihop transmission is how to foster cooperation among selfish nodes.
Existing researches mainly adopt monetary stimulating. In this manuscript, we
propose instead a simple and self-enforcing forwarding incentive scheme free of
indirect monetary remunerating for asymmetric (uplink multihop, downlink
single-hop) cellar network based on coalitional game theory, which comprises
double compensation, namely, Inter- BEA, global stimulating policy allotting
resources among relaying coalitions according to group size, and Intra-BEA,
local compensating and allocating rule within coalitions. Firstly, given the
global allotting policy, we introduce a fair allocation estimating approach
which includes remunerating for relaying cost using Myerson value for partition
function game, to enlighten the design of local allocating rules. Secondly,
given the inter- and intra-BEA relay fostering approach, we check stability of
coalition structures in terms of internal and external stability as well as
inductive core. Theoretic analysis and numerical simulation show that our
measure can provide communication opportunities for outer ring nodes and
enlarge system coverage, while at the same time provide enough motivation with
respect to resource allocation and energy saving for nodes in inner and middle
ring to relay for own profits."
"We propose and analyze a dynamic implementation of the property-rights model
of cognitive radio. A primary link has the possibility to lease the owned
spectrum to a MAC network of secondary nodes, in exchange for cooperation in
the form of distributed space-time coding (DSTC). The cooperation and
competition between the primary and secondary network are cast in the framework
of sequential game. On one hand, the primary link attempts to maximize its
quality of service in terms of signal-to-interference-plus-noise ratio (SINR);
on the other hand, nodes in the secondary network compete for transmission
within the leased time-slot following a power control mechanism. We consider
both a baseline model with complete information and a more practical version
with incomplete information, using the backward induction approach for the
former and providing approximate algorithm for the latter. Analysis and
numerical results show that our models and algorithms provide a promising
framework for fair and effective spectrum sharing, both between primary and
secondary networks and among secondary nodes."
"The issue of group-blind multiuser detection in MAC channel among wireless
nodes in the environment of multiple networks coexisting and sharing spectrum
is addressed under the Framework of coalitional game. We investigate the
performance and stability of multiple access channel (MAC) with linear
decorrelating multiuser detection under varying SNR, channel gains and
coalitional structures, in which both single BS and multiple BSs cases were
considered. The main results and conclusion are as follows: (1) the grand
coalition is payoff maximizing under loose SNR; (2) it is in conformity with
group and coalitional rationality forming coalition among nodes that have
comparative channel gains."
"EcoTRADE is a multi player network game of a virtual biodiversity credit
market. Each player controls the land use of a certain amount of parcels on a
virtual landscape. The biodiversity credits of a particular parcel depend on
neighboring parcels, which may be owned by other players. The game can be used
to study the strategies of players in experiments or classroom games and also
as a communication tool for stakeholders participating in credit markets that
include spatially interdependent credits."
"We present a novel polynomial time approximation scheme for two-strategy
anonymous games, in which the players' utility functions, although potentially
different, do not differentiate among the identities of the other players. Our
algorithm computes an $eps$-approximate Nash equilibrium of an $n$-player
2-strategy anonymous game in time $poly(n) (1/eps)^{O(1/eps^2)}$, which
significantly improves upon the running time $n^{O(1/eps^2)}$ required by the
algorithm of Daskalakis & Papadimitriou, 2007. The improved running time is
based on a new structural understanding of approximate Nash equilibria: We show
that, for any $eps$, there exists an $eps$-approximate Nash equilibrium in
which either only $O(1/eps^3)$ players randomize, or all players who randomize
use the same mixed strategy. To show this result we employ tools from the
literature on Stein's Method."
"In this paper, we consider the Shapley network design game on undirected
networks. In this game, we have an edge weighted undirected network $G(V,E)$
and $n$ selfish players where player $i$ wants to choose a path from source
vertex $s_i$ to destination vertex $t_i$. The cost of each edge is equally
split among players who pass it. The price of stability is defined as the ratio
of the cost of the best Nash equilibrium to that of the optimal solution. We
present an $O(\log n/\log\log n)$ upper bound on price of stability for the
single sink case, i.e, $t_i=t$ for all $i$."
"In a distributed system with {\it attacks} and {\it defenses,} both {\it
attackers} and {\it defenders} are self-interested entities. We assume a {\it
reward-sharing} scheme among {\it interdependent} defenders; each defender
wishes to (locally) maximize her own total {\it fair share} to the attackers
extinguished due to her involvement (and possibly due to those of others). What
is the {\em maximum} amount of protection achievable by a number of such
defenders against a number of attackers while the system is in a {\it Nash
equilibrium}? As a measure of system protection, we adopt the {\it
Defense-Ratio} \cite{MPPS05a}, which provides the expected (inverse) proportion
of attackers caught by the defenders. In a {\it Defense-Optimal} Nash
equilibrium, the Defense-Ratio is optimized.
  We discover that the possibility of optimizing the Defense-Ratio (in a Nash
equilibrium) depends in a subtle way on how the number of defenders compares to
two natural graph-theoretic thresholds we identify. In this vein, we obtain,
through a combinatorial analysis of Nash equilibria, a collection of trade-off
results:
  - When the number of defenders is either sufficiently small or sufficiently
large, there are cases where the Defense-Ratio can be optimized. The
optimization problem is computationally tractable for a large number of
defenders; the problem becomes ${\cal NP}$-complete for a small number of
defenders and the intractability is inherited from a previously unconsidered
combinatorial problem in {\em Fractional Graph Theory}.
  - Perhaps paradoxically, there is a middle range of values for the number of
defenders where optimizing the Defense-Ratio is never possible."
"We present several new characterizations of correlated equilibria in games
with continuous utility functions. These have the advantage of being more
computationally and analytically tractable than the standard definition in
terms of departure functions. We use these characterizations to construct
effective algorithms for approximating a single correlated equilibrium or the
entire set of correlated equilibria of a game with polynomial utility
functions."
"There are p heterogeneous objects to be assigned to n competing agents (n >
p) each with unit demand. It is required to design a Groves mechanism for this
assignment problem satisfying weak budget balance, individual rationality, and
minimizing the budget imbalance. This calls for designing an appropriate rebate
function. Our main result is an impossibility theorem which rules out linear
rebate functions with non-zero efficiency in heterogeneous object assignment.
Motivated by this theorem, we explore two approaches to get around this
impossibility. In the first approach, we show that linear rebate functions with
non-zero are possible when the valuations for the objects are correlated. In
the second approach, we show that rebate functions with non-zero efficiency are
possible if linearity is relaxed."
"This paper considers a conjecture-based distributed learning approach that
enables autonomous nodes to independently optimize their transmission
probabilities in random access networks. We model the interaction among
multiple self-interested nodes as a game. It is well-known that the Nash
equilibria in this game result in zero throughput for all the nodes if they
take myopic best-response, thereby leading to a network collapse. This paper
enables nodes to behave as intelligent entities which can proactively gather
information, form internal conjectures on how their competitors would react to
their actions, and update their beliefs according to their local observations.
In this way, nodes are capable to autonomously ""learn"" the behavior of their
competitors, optimize their own actions, and eventually cultivate reciprocity
in the random access network. To characterize the steady-state outcome, the
conjectural equilibrium is introduced. Inspired by the biological phenomena of
""derivative action"" and ""gradient dynamics"", two distributed conjecture-based
action update mechanisms are proposed to stabilize the random access network.
The sufficient conditions that guarantee the proposed conjecture-based learning
algorithms to converge are derived. Moreover, it is shown that all the
achievable operating points in the throughput region are essentially stable
conjectural equilibria corresponding to different conjectures. We investigate
how the conjectural equilibrium can be selected in heterogeneous networks and
how the proposed methods can be extended to ad-hoc networks. Simulations verify
that the system performance significantly outperforms existing protocols, such
as IEEE 802.11 DCF protocol and the PMAC protocol, in terms of throughput,
fairness, convergence, and stability."
"With deeper study of the Game Theory, some conditions of Prisoner's Dilemma
is no longer suitable of games in real life. So we try to develop a new
model-Villager's Dilemma which has more realistic conditions to stimulate the
process of game. It is emphasize that Prisoner's Dilemma is an exception which
is lack of universality and the importance of rules in the game. And it puts
forward that to let the rule maker take part in the game and specifies game
players can stop the game as they like. This essay describes the basic model,
the villager's dilemma (VD) and put some extended use of it, and points out the
importance of rules and the effect it has on the result of the game. It briefly
describes the disadvantage of Prisoner's Dilemma and advantage Villager's
Dilemma has. It summarizes the premise and scope of application of Villager's
Dilemma, and provides theory foundation for making rules for game and forecast
of the future of the game."
"Motivated by sponsored search auctions, we study multi-unit auctions with
budget constraints. In the mechanism we propose, Sort-Cut, understating budgets
or values is weakly dominated. Since Sort-Cut's revenue is increasing in
budgets and values, all kinds of equilibrium deviations from true valuations
turn out to be beneficial to the auctioneer. We show that the revenue of
Sort-Cut can be an order of magnitude greater than that of the natural Market
Clearing Price mechanism, and we discuss the efficiency properties of its
ex-post Nash equilibrium."
"Game-theoretic analyses of distributed and peer-to-peer systems typically use
the Nash equilibrium solution concept, but this explicitly excludes the
possibility of strategic behavior involving more than one agent. We examine the
effects of two types of strategic behavior involving more than one agent,
sybils and collusion, in the context of scrip systems where agents provide each
other with service in exchange for scrip. Sybils make an agent more likely to
be chosen to provide service, which generally makes it harder for agents
without sybils to earn money and decreases social welfare. Surprisingly, in
certain circumstances it is possible for sybils to make all agents better off.
While collusion is generally bad, in the context of scrip systems it actually
tends to make all agents better off, not merely those who collude. These
results also provide insight into the effects of allowing agents to advertise
and loan money. While many extensions of Nash equilibrium have been proposed
that address collusion and other issues relevant to distributed and
peer-to-peer systems, our results show that none of them adequately address the
issues raised by sybils and collusion in scrip systems."
"In this work we introduce hierarchy in wireless networks that can be modeled
by a decentralized multiple access channel and for which energy-efficiency is
the main performance index. In these networks users are free to choose their
power control strategy to selfishly maximize their energy-efficiency.
Specifically, we introduce hierarchy in two different ways: 1. Assuming
single-user decoding at the receiver, we investigate a Stackelberg formulation
of the game where one user is the leader whereas the other users are assumed to
be able to react to the leader's decisions; 2. Assuming neither leader nor
followers among the users, we introduce hierarchy by assuming successive
interference cancellation at the receiver. It is shown that introducing a
certain degree of hierarchy in non-cooperative power control games not only
improves the individual energy efficiency of all the users but can also be a
way of insuring the existence of a non-saturated equilibrium and reaching a
desired trade-off between the global network performance at the equilibrium and
the requested amount of signaling. In this respect, the way of measuring the
global performance of an energy-efficient network is shown to be a critical
issue."
"We study the envy-free cake-cutting problem for $d+1$ players with $d$ cuts,
for both the oracle function model and the polynomial time function model. For
the former, we derive a $\theta(({1\over\epsilon})^{d-1})$ time matching bound
for the query complexity of $d+1$ player cake cutting with Lipschitz utilities
for any $d> 1$. When the utility functions are given by a polynomial time
algorithm, we prove the problem to be PPAD-complete.
  For measurable utility functions, we find a fully polynomial-time algorithm
for finding an approximate envy-free allocation of a cake among three people
using two cuts."
"We investigate the degree of discontinuity of several solution concepts from
non-cooperative game theory. While the consideration of Nash equilibria forms
the core of our work, also pure and correlated equilibria are dealt with.
Formally, we restrict the treatment to two player games, but results and proofs
extend to the n-player case. As a side result, the degree of discontinuity of
solving systems of linear inequalities is settled."
"If a two-player social welfare maximization problem does not admit a PTAS, we
prove that any maximal-in-range truthful mechanism that runs in polynomial time
cannot achieve an approximation factor better than 1/2. Moreover, for the
k-player version of the same problem, the hardness of approximation improves to
1/k under the same two-player hardness assumption. (We note that 1/k is
achievable by a trivial deterministic maximal-in-range mechanism.) This
hardness result encompasses not only deterministic maximal-in-range mechanisms,
but also all universally-truthful randomized maximal in range algorithms, as
well as a class of strictly more powerful truthful-in-expectation randomized
mechanisms recently introduced by Dobzinski and Dughmi. Our result applies to
any class of valuation functions that satisfies some minimal closure
properties. These properties are satisfied by the valuation functions in all
well-studied APX-hard social welfare maximization problems, such as coverage,
submodular, and subadditive valuations.
  We also prove a stronger result for universally-truthful maximal-in-range
mechanisms. Namely, even for the class of budgeted additive valuations, which
admits an FPTAS, no such mechanism can achieve an approximation factor better
than 1/k in polynomial time."
"This brief paper describes the single-player card game called ""Perpetual
Motion"" and reports on a computational analysis of the game's outcome. The
analysis follows a Monte Carlo methodology based on a sample of 10,000 randomly
generated games. The key result is that 54.55% +/- 0.89% of games can be
completed (by a patient player!) but that the remaining 45.45% result in
non-terminating cycles. The lengths of these non-terminating cycles leave some
outstanding questions."
"We consider the classical mathematical economics problem of {\em Bayesian
optimal mechanism design} where a principal aims to optimize expected revenue
when allocating resources to self-interested agents with preferences drawn from
a known distribution. In single-parameter settings (i.e., where each agent's
preference is given by a single private value for being served and zero for not
being served) this problem is solved [Myerson '81]. Unfortunately, these single
parameter optimal mechanisms are impractical and rarely employed [Ausubel and
Milgrom '06], and furthermore the underlying economic theory fails to
generalize to the important, relevant, and unsolved multi-dimensional setting
(i.e., where each agent's preference is given by multiple values for each of
the multiple services available) [Manelli and Vincent '07]. In contrast to the
theory of optimal mechanisms we develop a theory of sequential posted price
mechanisms, where agents in sequence are offered take-it-or-leave-it prices.
These mechanisms are approximately optimal in single-dimensional settings, and
avoid many of the properties that make optimal mechanisms impractical.
Furthermore, these mechanisms generalize naturally to give the first known
approximations to the elusive optimal multi-dimensional mechanism design
problem. In particular, we solve multi-dimensional multi-unit auction problems
and generalizations to matroid feasibility constraints. The constant
approximations we obtain range from 1.5 to 8. For all but one case, our posted
price sequences can be computed in polynomial time."
"Scheduling on related machines ($Q||C_{\max}$) is one of the most important
problems in the field of Algorithmic Mechanism Design. Each machine is
controlled by a selfish agent and her valuation can be expressed via a single
parameter, her {\em speed}. In contrast to other similar problems, Archer and
Tardos \cite{AT01} showed that an algorithm that minimizes the makespan can be
truthfully implemented, although in exponential time. On the other hand, if we
leave out the game-theoretic issues, the complexity of the problem has been
completely settled -- the problem is strongly NP-hard, while there exists a
PTAS \cite{HS88,ES04}.
  This problem is the most well studied in single-parameter algorithmic
mechanism design. It gives an excellent ground to explore the boundary between
truthfulness and efficient computation. Since the work of Archer and Tardos,
quite a lot of deterministic and randomized mechanisms have been suggested.
Recently, a breakthrough result \cite{DDDR08} showed that a randomized truthful
PTAS exists. On the other hand, for the deterministic case, the best known
approximation factor is 2.8 \cite{Kov05,Kov07}.
  It has been a major open question whether there exists a deterministic
truthful PTAS, or whether truthfulness has an essential, negative impact on the
computational complexity of the problem. In this paper we give a definitive
answer to this important question by providing a truthful {\em deterministic}
PTAS."
"We study the power of polynomial-time truthful mechanisms comparing to
polynomial time (non-truthful) algorithms. We show that there is a setting in
which deterministic polynomial-time truthful mechanisms cannot guarantee a
bounded approximation ratio, but a non-truthful FPTAS exists. We also show that
in the same setting there is a universally truthful mechanism that provides an
approximation ratio of 2. This shows that the cost of truthfulness is
unbounded. The proofs are almost standard in the field and follow from known
results."
"An abstraction of normal form games is proposed, called
Feasibility/Desirability Games (or FD Games in short). FD Games can be seen
from three points of view: as a new presentation of games in which Nash
equilibria can be found, as choice models in microeconomics or as a model of
evolution in games."
"In this paper, we establish a connection between ranking theory and general
equilibrium theory. First of all, we show that the ranking vector of PageRank
or Invariant method is precisely the equilibrium of a special Cobb-Douglas
market. This gives a natural economic interpretation for the PageRank or
Invariant method. Furthermore, we propose a new ranking method, the CES
ranking, which is minimally fair, strictly monotone and invariant to reference
intensity, but not uniform or weakly additive."
"For most people, social contacts play an integral part in finding a new job.
As observed by Granovetter's seminal study, the proportion of jobs obtained
through social contacts is usually large compared to those obtained through
postings or agencies. At the same time, job markets are a natural example of
two-sided matching markets. An important solution concept in such markets is
that of stable matchings, and the use of the celebrated Gale-Shapley algorithm
to compute them. So far, the literature has evolved separately, either focusing
on the implications of information flowing through a social network, or on
developing a mathematical theory of job markets through the use of two-sided
matching techniques.
  In this paper we provide a model of the job market that brings both aspects
of job markets together. To model the social scientists' observations, we
assume that workers learn only about positions in firms through social
contacts. Given that information structure, we study both static properties of
what we call locally stable matchings (i.e., stable matchings subject to
informational constraints given by a social network) and dynamic properties
through a reinterpretation of Gale-Shapley's algorithm as myopic best response
dynamics.
  We prove that, in general, the set of locally stable matching strictly
contains that of stable matchings and it is in fact NP-complete to determine if
they are identical. We also show that the lattice structure of stable matchings
is in general absent. Finally, we focus on myopic best response dynamics
inspired by the Gale-Shapley algorithm. We study the efficiency loss due to the
informational constraints, providing both lower and upper bounds."
"Internet and graphs are very much related. The graphical structure of
internet has been studied extensively to provide efficient solutions to routing
and other problems. But most of these studies assume a central authority which
controls and manages the internet. In the recent years game theoretic models
have been proposed which do not require a central authority and the users are
assumed to be routing their flows selfishly. The existence of Nash Equilibria,
congestion and the amount of inefficiency caused by this selfish routing is a
major concern in this field. A type of paradox in the selfish routing networks,
Braess' Paradox, first discovered by Braess, is a major contributor to
inefficiency. Several pricing mechanisms have also been provided which give a
game theoretical model between users(consumers) and ISPs ({Internet Service
Providers} or sellers) for the internet.
  We propose a novel pricing mechanism, based on real world Internet network
architecture, which reduces the severity of Braess' Paradox in selfish routing
game theoretic networks. It's a pricing mechanism between combinatorial users
and ISPs. We prove that Nash equilibria exists in this network and provide
bounds on inefficiency . We use graphical properties of internet to prove our
result. Several interesting extensions and future work have also been
discussed."
"In this note we consider the following problem to study the effect of
malicious players on the social optimum in load balancing games: Consider two
players SOC and MAL controlling (1-f) and f fraction of the flow in a load
balancing game. SOC tries to minimize the total cost faced by her players while
MAL tries to maximize the same.
  If the latencies are linear, we show that this 2-player zero-sum game has a
pure strategy Nash equilibrium. Moreover, we show that one of the optimal
strategies for MAL is to play selfishly: let the f fraction of the flow be sent
as when the flow was controlled by infinitesimal players playing selfishly and
reaching a Nash equilibrium. This shows that a malicious player cannot cause
more harm in this game than a set of selfish agents.
  We also introduce the notion of Cost of Malice - the ratio of the cost faced
by SOC at equilibrium to (1-f)OPT, where OPT is the social optimum minimizing
the cost of all the players. In linear load balancing games we bound the cost
of malice by (1+f/2)."
"We consider massively dense ad hoc networks and study their continuum limits
as the node density increases and as the graph providing the available routes
becomes a continuous area with location and congestion dependent costs. We
study both the global optimal solution as well as the non-cooperative routing
problem among a large population of users where each user seeks a path from its
origin to its destination so as to minimize its individual cost. Finally, we
seek for a (continuum version of the) Wardrop equilibrium. We first show how to
derive meaningful cost models as a function of the scaling properties of the
capacity of the network and of the density of nodes. We present various
solution methodologies for the problem: (1) the viscosity solution of the
Hamilton-Jacobi-Bellman equation, for the global optimization problem, (2) a
method based on Green's Theorem for the least cost problem of an individual,
and (3) a solution of the Wardrop equilibrium problem using a transformation
into an equivalent global optimization problem."
"In this paper we study the optimal placement and optimal number of active
relay nodes through the traffic density in mobile sensor ad-hoc networks. We
consider a setting in which a set of mobile sensor sources is creating data and
a set of mobile sensor destinations receiving that data. We make the assumption
that the network is massively dense, i.e., there are so many sources,
destinations, and relay nodes, that it is best to describe the network in terms
of macroscopic parameters, such as their spatial density, rather than in terms
of microscopic parameters, such as their individual placements.
  We focus on a particular physical layer model that is characterized by the
following assumptions: i) the nodes must only transport the data from the
sources to the destinations, and do not need to sense the data at the sources,
or deliver them at the destinations once the data arrive at their physical
locations, and ii) the nodes have limited bandwidth available to them, but they
use it optimally to locally achieve the network capacity.
  In this setting, the optimal distribution of nodes induces a traffic density
that resembles the electric displacement that will be created if we substitute
the sources and destinations with positive and negative charges respectively.
The analogy between the two settings is very tight and have a direct
interpretation in wireless sensor networks."
"We study the design of truthful mechanisms that do not use payments for the
generalized assignment problem (GAP) and its variants. An instance of the GAP
consists of a bipartite graph with jobs on one side and machines on the other.
Machines have capacities and edges have values and sizes; the goal is to
construct a welfare maximizing feasible assignment. In our model of private
valuations, motivated by impossibility results, the value and sizes on all
job-machine pairs are public information; however, whether an edge exists or
not in the bipartite graph is a job's private information.
  We study several variants of the GAP starting with matching. For the
unweighted version, we give an optimal strategyproof mechanism; for maximum
weight bipartite matching, however, we show give a 2-approximate strategyproof
mechanism and show by a matching lowerbound that this is optimal. Next we study
knapsack-like problems, which are APX-hard. For these problems, we develop a
general LP-based technique that extends the ideas of Lavi and Swamy to reduce
designing a truthful mechanism without money to designing such a mechanism for
the fractional version of the problem, at a loss of a factor equal to the
integrality gap in the approximation ratio. We use this technique to obtain
strategyproof mechanisms with constant approximation ratios for these problems.
We then design an O(log n)-approximate strategyproof mechanism for the GAP by
reducing, with logarithmic loss in the approximation, to our solution for the
value-invariant GAP. Our technique may be of independent interest for designing
truthful mechanisms without money for other LP-based problems."
"Innovative auction methods can be exploited to increase profits, with
Shubik's famous ""dollar auction"" perhaps being the most widely known example.
Recently, some mainstream e-commerce web sites have apparently achieved the
same end on a much broader scale, by using ""pay-per-bid"" auctions to sell
items, from video games to bars of gold. In these auctions, bidders incur a
cost for placing each bid in addition to (or sometimes in lieu of) the winner's
final purchase cost. Thus even when a winner's purchase cost is a small
fraction of the item's intrinsic value, the auctioneer can still profit
handsomely from the bid fees. Our work provides novel analyses for these
auctions, based on both modeling and datasets derived from auctions at
Swoopo.com, the leading pay-per-bid auction site. While previous modeling work
predicts profit-free equilibria, we analyze the impact of information asymmetry
broadly, as well as Swoopo features such as bidpacks and the Swoop It Now
option specifically, to quantify the effects of imperfect information in these
auctions. We find that even small asymmetries across players (cheaper bids,
better estimates of other players' intent, different valuations of items,
committed players willing to play ""chicken"") can increase the auction duration
well beyond that predicted by previous work and thus skew the auctioneer's
profit disproportionately. Finally, we discuss our findings in the context of a
dataset of thousands of live auctions we observed on Swoopo, which enables us
also to examine behavioral factors, such as the power of aggressive bidding.
Ultimately, our findings show that even with fully rational players, if players
overlook or are unaware any of these factors, the result is outsized profits
for pay-per-bid auctioneers."
"In pay-per click sponsored search auctions which are currently extensively
used by search engines, the auction for a keyword involves a certain number of
advertisers (say k) competing for available slots (say m) to display their ads.
This auction is typically conducted for a number of rounds (say T). There are
click probabilities mu_ij associated with each agent-slot pairs. The goal of
the search engine is to maximize social welfare of the advertisers, that is,
the sum of values of the advertisers. The search engine does not know the true
values advertisers have for a click to their respective ads and also does not
know the click probabilities mu_ij s. A key problem for the search engine
therefore is to learn these click probabilities during the T rounds of the
auction and also to ensure that the auction mechanism is truthful. Mechanisms
for addressing such learning and incentives issues have recently been
introduced and are aptly referred to as multi-armed-bandit (MAB) mechanisms.
When m = 1, characterizations for truthful MAB mechanisms are available in the
literature and it has been shown that the regret for such mechanisms will be
O(T^{2/3}). In this paper, we seek to derive a characterization in the
realistic but non-trivial general case when m > 1 and obtain several
interesting results."
"We study the problem of designing group-strategyproof cost-sharing
mechanisms. The players report their bids for getting serviced and the
mechanism decides which players are going to be serviced and how much each one
of them is going to pay. We determine three conditions: \emph{Fence
Monotonicity}, \emph{Stability} of the allocation and \emph{Validity} of the
tie-breaking rule that are necessary and sufficient for
group-strategyproofness, regardless of the cost function. Fence Monotonicity
puts restrictions only on the payments of the mechanism and stability only on
the allocation. Consequently Fence Monotonicity characterizes
group-strategyproof cost-sharing schemes. Finally, we use our results to prove
that there exist families of cost functions, where any group-strategyproof
mechanism has unbounded approximation ratio."
"We consider the problem of revenue-optimal dynamic mechanism design in
settings where agents' types evolve over time as a function of their (both
public and private) experience with items that are auctioned repeatedly over an
infinite horizon. A central question here is understanding what natural
restrictions on the environment permit the design of optimal mechanisms (note
that even in the simpler static setting, optimal mechanisms are characterized
only under certain restrictions). We provide a {\em structural
characterization} of a natural ""separable: multi-armed bandit environment
(where the evolution and incentive structure of the a-priori type is decoupled
from the subsequent experience in a precise sense) where dynamic optimal
mechanism design is possible. Here, we present the Virtual Index Mechanism, an
optimal dynamic mechanism, which maximizes the (long term) {\em virtual
surplus} using the classical Gittins algorithm. The mechanism optimally
balances exploration and exploitation, taking incentives into account."
"In the traditional voting manipulation literature, it is assumed that a group
of manipulators jointly misrepresent their preferences to get a certain
candidate elected, while the remaining voters are truthful. In this paper, we
depart from this assumption, and consider the setting where all voters are
strategic. In this case, the election can be viewed as a game, and the election
outcomes correspond to Nash equilibria of this game. We use this framework to
analyze two variants of Plurality voting, namely, simultaneous voting, where
all voters submit their ballots at the same time, and sequential voting, where
the voters express their preferences one by one. For simultaneous voting, we
characterize the preference profiles that admit a pure Nash equilibrium, but
show that it is computationally hard to check if a given profile fits our
criterion. For sequential voting, we provide a complete analysis of the setting
with two candidates, and show that for three or more candidates the equilibria
of sequential voting may behave in a counterintuitive manner."
"This paper focuses on a one person game called Indian policeman's dilemma
(IPD). It represents the internal conflict between emotion and profession of a
typical Indian police officer. We have 'split' the game to be played
independently by different personality modules of the same player. Each module
then appears as an independent individual player of the game. None of the
players knows the exact payoff values of any of the others. Only greater than
or less than type of inequalities among the payoff values across the players
are to be inferred probabilistically. There are two Nash equilibrium (NE)
points in this game signifying two completely opposing behavior by the
policeman involved. With the help of the probabilistic inequalities probable
propensities of the different behaviors have been determined. The model
underscores the need for new surveys and data generation. A design of one such
survey to measure professionalism of the police personnel has been outlined."
"We exhibit incentive compatible multi-unit auctions that are not affine
maximizers (i.e., are not of the VCG family) and yet approximate the social
welfare to within a factor of $1+\epsilon$. For the case of two-item two-bidder
auctions we show that these auctions, termed Triage auctions, are the only
scalable ones that give an approximation factor better than 2. ""Scalable"" means
that the allocation does not depend on the units in which the valuations are
measured. We deduce from this that any scalable computationally-efficient
incentive-compatible auction for $m$ items and $n \ge 2$ bidders cannot
approximate the social welfare to within a factor better than 2. This is in
contrast to arbitrarily good approximations that can be reached under
computational constraints alone, and in contrast to the fact that the optimal
social welfare can be obtained under incentive constraints alone."
"We introduce a new class of games called the networked common goods game
(NCGG), which generalizes the well-known common goods game. We focus on a
fairly general subclass of the game where each agent's utility functions are
the same across all goods the agent is entitled to and satisfy certain natural
properties (diminishing return and smoothness). We give a comprehensive set of
technical results listed as follows.
  * We show the optimization problem faced by a single agent can be solved
efficiently in this subclass. The discrete version of the problem is however
NP-hard but admits an fully polynomial time approximation scheme (FPTAS).
  * We show uniqueness results of pure strategy Nash equilibrium of NCGG, and
that the equilibrium is fully characterized by the structure of the network and
independent of the choices and combinations of agent utility functions.
  * We show NCGG is a potential game, and give an implementation of best/better
response Nash dynamics that lead to fast convergence to an
$\epsilon$-approximate pure strategy Nash equilibrium.
  * Lastly, we show the price of anarchy of NCGG can be as large as
$\Omega(n^{1-\epsilon})$ (for any $\epsilon>0$), which means selfish behavior
in NCGG can lead to extremely inefficient social outcomes."
"We study bargaining games between suppliers and manufacturers in a network
context. Agents wish to enter into contracts in order to generate surplus which
then must be divided among the participants. Potential contracts and their
surplus are represented by weighted edges in our bipartite network. Each agent
in the market is additionally limited by a capacity representing the number of
contracts which he or she may undertake. When all agents are limited to just
one contract each, prior research applied natural generalizations of the Nash
bargaining solution to the networked setting, defined the new solution concepts
of stable and balanced, and characterized the resulting bargaining outcomes. We
simplify and generalize these results to a setting in which participants in
only one side of the market are limited to one contract each. The heart of our
results uses a linear-programming formulation to establish a novel connection
between well-studied cooperative game theory concepts (such as core and
prekernel) and the solution concepts of stable and balanced defined for the
bargaining games. This immediately implies one can take advantage of the
results and algorithms in cooperative game theory to reproduce results such as
those of Azar et al. [1] and Kleinberg and Tardos [29] and also generalize them
to our setting. The cooperative-game-theoretic connection also inspires us to
refine our solution space using standard solution concepts from that literature
such as nucleolus and lexicographic kernel. The nucleolus is particularly
attractive as it is unique, always exists, and is supported by experimental
data in the network bargaining literature. Guided by algorithms from
cooperative game theory, we show how to compute the nucleolus by pruning and
iteratively solving a natural linear-programming formulation."
"We establish a generic result concerning order independence of a dominance
relation on finite games. It allows us to draw conclusions about order
independence of various dominance relations in a direct and simple way."
"This paper studies a class of incentive schemes based on intervention, where
there exists an intervention device that is able to monitor the actions of
users and to take an action that affects the payoffs of users. We consider the
case of perfect monitoring, where the intervention device can immediately
observe the actions of users without errors. We also assume that there exist
actions of the intervention device that are most and least preferred by all the
users and the intervention device, regardless of the actions of users. We
derive analytical results about the outcomes achievable with intervention, and
illustrate our results with an example based on the Cournot model."
"We initiate the study of congestion games with variable demands where the
(variable) demand has to be assigned to exactly one subset of resources. The
players' incentives to use higher demands are stimulated by non-decreasing and
concave utility functions. The payoff for a player is defined as the difference
between the utility of the demand and the associated cost on the used
resources. Although this class of non-cooperative games captures many elements
of real-world applications, it has not been studied in this generality, to our
knowledge, in the past. We study the fundamental problem of the existence of
pure Nash equilibria (PNE for short) in congestion games with variable demands.
We call a set of cost functions C consistent if every congestion game with
variable demands and cost functions in C possesses a PNE. We say that C is FIP
consistent if every such game possesses the alpha-Finite Improvement Property
for every alpha>0. Our main results are structural characterizations of
consistency and FIP consistency for twice continuously differentiable cost
functions. Specifically, we show 1. C is consistent if and only if C contains
either only affine functions or only homogeneously exponential functions (c(x)
= a exp(p x)). 2. C is FIP consistent if and only if C contains only affine
functions. Our results provide a complete characterization of consistency of
cost functions revealing structural differences to congestion games with fixed
demands (weighted congestion games), where in the latter even inhomogeneously
exponential functions are FIP consistent. Finally, we study consistency and FIP
consistency of cost functions in a slightly different class of games, where
every player experiences the same cost on a resource (uniform cost model). We
give a characterization of consistency and FIP consistency showing that only
homogeneously exponential functions are consistent."
"This paper shows that in suitable markets, even with out-of-equilibrium trade
allowed, a simple price update rule leads to rapid convergence toward the
equilibrium. In particular, this paper considers a Fisher market repeated over
an unbounded number of time steps, with the addition of finite sized warehouses
to enable non-equilibrium trade. The main result is that suitable tatonnement
style price updates lead to convergence in a significant subset of markets
satisfying the Weak Gross Substitutes property. Throughout this process the
warehouse are always able to store or meet demand imbalances (the needed
capacity depends on the initial imbalances). Finally, our price update rule is
robust in a variety of regards: 1. The updates for each good depend only on
information about that good (its current price, its excess demand since its
last update) and occur asynchronously from updates to other prices. 2. The
process is resilient to error in the excess demand data. 3. Likewise, the
process is resilient to discreteness, i.e. a limit to divisibility, both of
goods and money."
"Incentives play an important role in (security and IT) risk management of a
large-scale organization with multiple autonomous divisions. This paper
presents an incentive mechanism design framework for risk management based on a
game-theoretic approach. The risk manager acts as a mechanism designer
providing rules and incentive factors such as assistance or subsidies to
divisions or units, which are modeled as selfish players of a strategic
(noncooperative) game. Based on this model, incentive mechanisms with various
objectives are developed that satisfy efficiency, preference-compatibility, and
strategy-proofness criteria. In addition, iterative and distributed algorithms
are presented, which can be implemented under information limitations such as
the risk manager not knowing the individual units' preferences. An example
scenario illustrates the framework and results numerically. The incentive
mechanism design approach presented is useful for not only deriving guidelines
but also developing computer-assistance systems for large-scale risk
management."
"Due to the lack of coordination, it is unlikely that the selfish players of a
strategic game reach a socially good state. A possible way to cope with
selfishness is to compute a desired outcome (if it is tractable) and impose it.
However this answer is often inappropriate because compelling an agent can be
costly, unpopular or just hard to implement. Since both situations (no
coordination and full coordination) show opposite advantages and drawbacks, it
is natural to study possible tradeoffs. In this paper we study a strategic game
where the nodes of a simple graph G are independent agents who try to form
pairs: e.g. jobs and applicants, tennis players for a match, etc. In many
instances of the game, a Nash equilibrium significantly deviates from a social
optimum. We analyze a scenario where we fix the strategy of some players; the
other players are free to make their choice. The goal is to compel a minimum
number of players and guarantee that any possible equilibrium of the modified
game is a social optimum, i.e. created pairs must form a maximum matching of G.
We mainly show that this intriguing problem is NP-hard and propose an
approximation algorithm with a constant ratio."
"In [Van Benthem 2007] the concept of a public announcement is used to study
the effect of the iterated elimination of strictly dominated strategies. We
offer a simple generalisation of this approach to cover arbitrary strategic
games and many optimality notions. We distinguish between announcements of
optimality and announcements of rationality."
"We study the performance of Fictitious Play, when used as a heuristic for
finding an approximate Nash equilibrium of a 2-player game. We exhibit a class
of 2-player games having payoffs in the range [0,1] that show that Fictitious
Play fails to find a solution having an additive approximation guarantee
significantly better than 1/2. Our construction shows that for n times n games,
in the worst case both players may perpetually have mixed strategies whose
payoffs fall short of the best response by an additive quantity 1/2 -
O(1/n^(1-delta)) for arbitrarily small delta. We also show an essentially
matching upper bound of 1/2 - O(1/n)."
"We consider perfect-information reachability stochastic games for 2 players
on infinite graphs. We identify a subclass of such games, and prove two
interesting properties of it: first, Player Max always has optimal strategies
in games from this subclass, and second, these games are strongly determined.
The subclass is defined by the property that the set of all values can only
have one accumulation point -- 0. Our results nicely mirror recent results for
finitely-branching games, where, on the contrary, Player Min always has optimal
strategies. However, our proof methods are substantially different, because the
roles of the players are not symmetric. We also do not restrict the branching
of the games. Finally, we apply our results in the context of recently studied
One-Counter stochastic games."
"This paper considers a generalized framework to study OSNR optimization-based
end-to-end link level power control problems in optical networks. We combine
favorable features of game-theoretical approach and central cost approach to
allow different service groups within the network. We develop solutions
concepts for both cases of empty and nonempty feasible sets. In addition, we
derive and prove the convergence of a distributed iterative algorithm for
different classes of users. In the end, we use numerical examples to illustrate
the novel framework."
"We consider a task of scheduling with a common deadline on a single machine.
Every player reports to a scheduler the length of his job and the scheduler
needs to finish as many jobs as possible by the deadline. For this simple
problem, there is a truthful mechanism that achieves maximum welfare in
dominant strategies. The new aspect of our work is that in our setting players
are uncertain about their own job lengths, and hence are incapable of providing
truthful reports (in the strict sense of the word). For a probabilistic model
for uncertainty our main results are as follows.
  1) Even with relatively little uncertainty, no mechanism can guarantee a
constant fraction of the maximum welfare.
  2) To remedy this situation, we introduce a new measure of economic
efficiency, based on a notion of a {\em fair share} of a player, and design
mechanisms that are $\Omega(1)$-fair. In addition to its intrinsic appeal, our
notion of fairness implies good approximation of maximum welfare in several
cases of interest.
  3) In our mechanisms the machine is sometimes left idle even though there are
jobs that want to use it. We show that this unfavorable aspect is unavoidable,
unless one gives up other favorable aspects (e.g., give up
$\Omega(1)$-fairness).
  We also consider a qualitative approach to uncertainty as an alternative to
the probabilistic quantitative model. In the qualitative approach we break away
from solution concepts such as dominant strategies (they are no longer well
defined), and instead suggest an axiomatic approach, which amounts to listing
desirable properties for mechanisms. We provide a mechanism that satisfies
these properties."
"We present a general framework to model strategic aspects and stable and fair
resource allocations in networks via variants and generalizations of path
coalitional games. In these games, a coalition of edges or vertices is
successful if it can enable an s-t path. We present polynomial-time algorithms
to compute and verify least core payoffs of cost-based generalizations of path
coalitional games and their duals, thereby settling a number of open problems.
The least core payoffs of path coalitional games are completely characterized
and a polynomial-time algorithm for computing the nucleolus of edge path
coalitional games on undirected series-parallel graphs is presented."
"Distributed power control for parallel Gaussian interference channels
recently draws great interests. However, all existing works only studied this
problem under deterministic communication channels and required certain perfect
information to carry out their proposed algorithms. In this paper, we study
this problem for stochastic parallel Gaussian interference channels. In
particular, we take into account the randomness of the communication
environment and the estimation errors of the desired information, and thus
formulate a stochastic noncooperative power control game. We then propose a
stochastic distributed learning algorithm SDLA-I to help communication pairs
learn the Nash equilibrium. A careful convergence analysis on SDLA-I is
provided based on stochastic approximation theory and projected dynamic systems
approach. We further propose another learning algorithm SDLA-II by including a
simple iterate averaging idea into SDLA-I to improve algorithmic convergence
performance. Numerical results are also presented to demonstrate the
performance of our algorithms and theoretical results."
"We study markets of indivisible items in which price-based (Walrasian)
equilibria often do not exist due to the discrete non-convex setting. Instead
we consider Nash equilibria of the market viewed as a game, where players bid
for items, and where the highest bidder on an item wins it and pays his bid. We
first observe that pure Nash-equilibria of this game excatly correspond to
price-based equilibiria (and thus need not exist), but that mixed-Nash
equilibria always do exist, and we analyze their structure in several simple
cases where no price-based equilibrium exists. We also undertake an analysis of
the welfare properties of these equilibria showing that while pure equilibria
are always perfectly efficient (""first welfare theorem""), mixed equilibria need
not be, and we provide upper and lower bounds on their amount of inefficiency."
"In this paper, we study the Nash dynamics of strategic interplays of n buyers
in a matching market setup by a seller, the market maker. Taking the standard
market equilibrium approach, upon receiving submitted bid vectors from the
buyers, the market maker will decide on a price vector to clear the market in
such a way that each buyer is allocated an item for which he desires the most
(a.k.a., a market equilibrium solution). While such equilibrium outcomes are
not unique, the market maker chooses one (maxeq) that optimizes its own
objective --- revenue maximization. The buyers in turn change bids to their
best interests in order to obtain higher utilities in the next round's market
equilibrium solution.
  This is an (n+1)-person game where buyers place strategic bids to gain the
most from the market maker's equilibrium mechanism. The incentives of buyers in
deciding their bids and the market maker's choice of using the maxeq mechanism
create a wave of Nash dynamics involved in the market. We characterize Nash
equilibria in the dynamics in terms of the relationship between maxeq and mineq
(i.e., minimum revenue equilibrium), and develop convergence results for Nash
dynamics from the maxeq policy to a mineq solution, resulting an outcome
equivalent to the truthful VCG mechanism.
  Our results imply revenue equivalence between maxeq and mineq, and address
the question that why short-term revenue maximization is a poor long run
strategy, in a deterministic and dynamic setting."
"We study the robust Nash equilibrium (RNE) for a class of games in
communications systems and networks where the impact of users on each other is
an additive function of their strategies. Each user measures this impact, which
may be corrupted by uncertainty in feedback delays, estimation errors,
movements of users, etc. To study the outcome of the game in which such
uncertainties are encountered, we utilize the worst-case robust optimization
theory. The existence and uniqueness conditions of RNE are derived using
finite-dimensions variational inequalities. To describe the effect of
uncertainty on the performance of the system, we use two criteria measured at
the RNE and at the equilibrium of the game without uncertainty. The first is
the difference between the respective social utility of users and, the second
is the differences between the strategies of users at their respective
equilibria. These differences are obtained for the case of a unique NE and
multiple NEs. To reach the RNE, we propose a distributed algorithm based on the
proximal response map and derive the conditions for its convergence.
Simulations of the power control game in interference channels, and Jackson
networks validate our analysis."
"In this work we study of competitive situations among users of a set of
global resources. More precisely we study the effect of cost policies used by
these resources in the convergence time to a pure Nash equilibrium. The work is
divided in two parts. In the theoretical part we prove lower and upper bounds
on the convergence time for various cost policies. We then implement all the
models we study and provide some experimental results. These results follows
the theoretical with one exception which is the most interesting among the
experiments. In the case of coalitional users the theoretical upper bound is
pseudo-polynomial to the number of users but the experimental results shows
that the convergence time is polynomial."
"We show how to combine Bayes nets and game theory to predict the behavior of
hybrid systems involving both humans and automated components. We call this
novel framework ""Semi Network-Form Games,"" and illustrate it by predicting
aircraft pilot behavior in potential near mid-air collisions. At present, at
the beginning of such potential collisions, a collision avoidance system in the
aircraft cockpit advises the pilots what to do to avoid the collision. However
studies of mid-air encounters have found wide variability in pilot responses to
avoidance system advisories. In particular, pilots rarely perfectly execute the
recommended maneuvers, despite the fact that the collision avoidance system's
effectiveness relies on their doing so. Rather pilots decide their actions
based on all information available to them (advisory, instrument readings,
visual observations). We show how to build this aspect into a semi network-form
game model of the encounter and then present computational simulations of the
resultant model."
"Modeling the purposeful behavior of imperfect agents from a small number of
observations is a challenging task. When restricted to the single-agent
decision-theoretic setting, inverse optimal control techniques assume that
observed behavior is an approximately optimal solution to an unknown decision
problem. These techniques learn a utility function that explains the example
behavior and can then be used to accurately predict or imitate future behavior
in similar observed or unobserved situations.
  In this work, we consider similar tasks in competitive and cooperative
multi-agent domains. Here, unlike single-agent settings, a player cannot
myopically maximize its reward --- it must speculate on how the other agents
may act to influence the game's outcome. Employing the game-theoretic notion of
regret and the principle of maximum entropy, we introduce a technique for
predicting and generalizing behavior, as well as recovering a reward function
in these domains."
"We study Bayesian mechanism design problems in settings where agents have
budgets. Specifically, an agent's utility for an outcome is given by his value
for the outcome minus any payment he makes to the mechanism, as long as the
payment is below his budget, and is negative infinity otherwise. This
discontinuity in the utility function presents a significant challenge in the
design of good mechanisms, and classical ""unconstrained"" mechanisms fail to
work in settings with budgets. The goal of this paper is to develop general
reductions from budget-constrained Bayesian MD to unconstrained Bayesian MD
with small loss in performance. We consider this question in the context of the
two most well-studied objectives in mechanism design---social welfare and
revenue---and present constant factor approximations in a number of settings.
Some of our results extend to settings where budgets are private and agents
need to be incentivized to reveal them truthfully."
"This paper presents the research on the interdisciplinary research
infrastructure for understanding human reasoning in game-theoretic terms.
Strategic reasoning is considered to impact human decision making in social,
economical and competitive interactions. The provided introduction explains and
connects concepts from AI, game theory and psychology. First result is a
concept of interdisciplinary game description language as a part of the focused
interdisciplinary research infrastructure. The need of this domain-specific
language is motivated and is aimed to accelerate the current developments. As
second result, the paper provides a summary of ongoing research and its
significance."
"We consider coalition formation games in which each player has preferences
over the other players and his preferences over coalitions are based on the
best player ($\mathcal{B}$-/B-hedonic games) or the worst player
($\mathcal{W}$/W-hedonic games) in the coalition. We show that for
$\mathcal{B}$-hedonic games, an individually stable partition is guaranteed to
exist and can be computed efficiently. Similarly, there exists a
polynomial-time algorithm which returns a Nash stable partition (if one exists)
for $\mathcal{B}$-hedonic games with strict preferences. Both $\mathcal{W}$-
and W-hedonic games are equivalent if individual rationality is assumed. It is
also shown that for B- or $\mathcal{W}$-hedonic games, checking whether a Nash
stable partition or an individually stable partition exists is NP-complete even
in some cases for strict preferences. We identify a key source of
intractability in compact coalition formation games in which preferences over
players are extended to preferences over coalitions."
"The Shapley value is one of the most important solution concepts in
cooperative game theory. In coalitional games without externalities, it allows
to compute a unique payoff division that meets certain desirable fairness
axioms. However, in many realistic applications where externalities are
present, Shapley's axioms fail to indicate such a unique division.
Consequently, there are many extensions of Shapley value to the environment
with externalities proposed in the literature built upon additional axioms. Two
important such extensions are ""externality-free"" value by Pham Do and Norde and
value that ""absorbed all externalities"" by McQuillin. They are good reference
points in a space of potential payoff divisions for coalitional games with
externalities as they limit the space at two opposite extremes. In a recent,
important publication, De Clippel and Serrano presented a marginality-based
axiomatization of the value by Pham Do Norde. In this paper, we propose a dual
approach to marginality which allows us to derive the value of McQuillin. Thus,
we close the picture outlined by De Clippel and Serrano."
"We introduce a new measure of the discrepancy in strategic games between the
social welfare in a Nash equilibrium and in a social optimum, that we call
selfishness level. It is the smallest fraction of the social welfare that needs
to be offered to each player to achieve that a social optimum is realized in a
pure Nash equilibrium. The selfishness level is unrelated to the price of
stability and the price of anarchy and is invariant under positive linear
transformations of the payoff functions. Also, it naturally applies to other
solution concepts and other forms of games.
  We study the selfishness level of several well-known strategic games. This
allows us to quantify the implicit tension within a game between players'
individual interests and the impact of their decisions on the society as a
whole. Our analyses reveal that the selfishness level often provides a deeper
understanding of the characteristics of the underlying game that influence the
players' willingness to cooperate.
  In particular, the selfishness level of finite ordinal potential games is
finite, while that of weakly acyclic games can be infinite. We derive explicit
bounds on the selfishness level of fair cost sharing games and linear
congestion games, which depend on specific parameters of the underlying game
but are independent of the number of players. Further, we show that the
selfishness level of the $n$-players Prisoner's Dilemma is $c/(b(n-1)-c)$,
where $b$ and $c$ are the benefit and cost for cooperation, respectively, that
of the $n$-players public goods game is $(1-\frac{c}{n})/(c-1)$, where $c$ is
the public good multiplier, and that of the Traveler's Dilemma game is
$\frac{1}{2}(b-1)$, where $b$ is the bonus. Finally, the selfishness level of
Cournot competition (an example of an infinite ordinal potential game, Tragedy
of the Commons, and Bertrand competition is infinite."
"We introduce a measure for the level of stability against coalitional
deviations, called \emph{stability scores}, which generalizes widely used
notions of stability in non-cooperative games. We use the proposed measure to
compare various Nash equilibria in congestion games, and to quantify the effect
of game parameters on coalitional stability. For our main results, we apply
stability scores to analyze and compare the Generalized Second Price (GSP) and
Vickrey-Clarke-Groves (VCG) ad auctions. We show that while a central result of
the ad auctions literature is that the GSP and VCG auctions implement the same
outcome in one of the equilibria of GSP, the GSP outcome is far more stable.
Finally, a modified version of VCG is introduced, which is group
strategy-proof, and thereby achieves the highest possible stability score."
"With the recent technological feasibility of electronic commerce over the
Internet, much attention has been given to the design of electronic markets for
various types of electronically-tradable goods. Such markets, however, will
normally need to function in some relationship with markets for other related
goods, usually those downstream or upstream in the supply chain. Thus, for
example, an electronic market for rubber tires for trucks will likely need to
be strongly influenced by the rubber market as well as by the truck market. In
this paper we design protocols for exchange of information between a sequence
of markets along a single supply chain. These protocols allow each of these
markets to function separately, while the information exchanged ensures
efficient global behavior across the supply chain. Each market that forms a
link in the supply chain operates as a double auction, where the bids on one
side of the double auction come from bidders in the corresponding segment of
the industry, and the bids on the other side are synthetically generated by the
protocol to express the combined information from all other links in the chain.
The double auctions in each of the markets can be of several types, and we
study several variants of incentive compatible double auctions, comparing them
in terms of their efficiency and of the market revenue."
"The planner wants to give k identical, indivisible objects to the top k
valuation agents at zero costs. Each agent knows her own valuation of the
object and whether it is among the top k. Modify the (k+1)st-price sealed-bid
auction by introducing a small participation fee and the option not to
participate in it. This simple mechanism implements the desired outcome in
iteratively undominated strategies. Moreover, no pair of agents can profitably
deviate from the equilibrium by coordinating their strategies or bribing each
other."
"We present new coordination mechanisms for scheduling selfish jobs on $m$
unrelated machines. A coordination mechanism aims to mitigate the impact of
selfishness of jobs on the efficiency of schedules by defining a local
scheduling policy on each machine. The scheduling policies induce a game among
the jobs and each job prefers to be scheduled on a machine so that its
completion time is minimum given the assignments of the other jobs. We consider
the maximum completion time among all jobs as the measure of the efficiency of
schedules. The approximation ratio of a coordination mechanism quantifies the
efficiency of pure Nash equilibria (price of anarchy) of the induced game.
  Our mechanisms are deterministic, local, and preemptive. Our first
coordination mechanism has approximation ratio $\Theta(\log m)$ and guarantees
that the induced game has pure Nash equilibria. This result improves a bound of
$O(\log^2 m)$ due to Azar, Jain, and Mirrokni and uses a global ordering of the
jobs according to their distinct IDs. Our second mechanism handles anonymous
jobs and has approximation ratio $O(\frac{\log m}{\log \log m})$ although the
game induced is not a potential game and, hence, the existence of pure Nash
equilibria is not guaranteed by potential function arguments. However, it
provides evidence that the known lower bounds for non-preemptive coordination
mechanisms could be beaten using preemptive scheduling policies. Our third
coordination mechanism also handles anonymous jobs and has a nice
cost-revealing potential function. We use this potential function in order to
prove the existence of equilibria and to upper-bound the price of anarchy of
the induced game by $O(\log^2m)$. Our third coordination mechanism is the first
that handles anonymous jobs and simultaneously guarantees that the induced game
is a potential game and has bounded price of anarchy."
"We consider two-player stochastic games played on a finite state space for an
infinite number of rounds. The games are concurrent: in each round, the two
players (player 1 and player 2) choose their moves independently and
simultaneously; the current state and the two moves determine a probability
distribution over the successor states. We also consider the important special
case of turn-based stochastic games where players make moves in turns, rather
than concurrently. We study concurrent games with \omega-regular winning
conditions specified as parity objectives. The value for player 1 for a parity
objective is the maximal probability with which the player can guarantee the
satisfaction of the objective against all strategies of the opponent. We study
the problem of continuity and robustness of the value function in concurrent
and turn-based stochastic parity gameswith respect to imprecision in the
transition probabilities. We present quantitative bounds on the difference of
the value function (in terms of the imprecision of the transition
probabilities) and show the value continuity for structurally equivalent
concurrent games (two games are structurally equivalent if the support of the
transition function is same and the probabilities differ). We also show
robustness of optimal strategies for structurally equivalent turn-based
stochastic parity games. Finally we show that the value continuity property
breaks without the structurally equivalent assumption (even for Markov chains)
and show that our quantitative bound is asymptotically optimal. Hence our
results are tight (the assumption is both necessary and sufficient) and optimal
(our quantitative bound is asymptotically optimal)."
"Turn-based stochastic games and its important subclass Markov decision
processes (MDPs) provide models for systems with both probabilistic and
nondeterministic behaviors. We consider turn-based stochastic games with two
classical quantitative objectives: discounted-sum and long-run average
objectives. The game models and the quantitative objectives are widely used in
probabilistic verification, planning, optimal inventory control, network
protocol and performance analysis. Games and MDPs that model realistic systems
often have very large state spaces, and probabilistic abstraction techniques
are necessary to handle the state-space explosion. The commonly used
full-abstraction techniques do not yield space-savings for systems that have
many states with similar value, but does not necessarily have similar
transition structure. A semi-abstraction technique, namely Magnifying-lens
abstractions (MLA), that clusters states based on value only, disregarding
differences in their transition relation was proposed for qualitative
objectives (reachability and safety objectives). In this paper we extend the
MLA technique to solve stochastic games with discounted-sum and long-run
average objectives. We present the MLA technique based abstraction-refinement
algorithm for stochastic games and MDPs with discounted-sum objectives. For
long-run average objectives, our solution works for all MDPs and a sub-class of
stochastic games where every state has the same value."
"In two-player finite-state stochastic games of partial observation on graphs,
in every state of the graph, the players simultaneously choose an action, and
their joint actions determine a probability distribution over the successor
states. We consider reachability objectives where player 1 tries to ensure a
target state to be visited almost-surely or positively. On the basis of
information, the game can be one-sided with either (a)player 1 or (b)player 2
having partial observation, or two-sided with both players having partial
observation. On the basis of randomization (a)players may not be allowed to use
randomization (pure strategies), or (b)may choose a probability distribution
over actions but the actual random choice is not visible (actions invisible),
or (c)may use full randomization. Our results for pure strategies are as
follows: (1)For one-sided games with player 2 perfect observation we show that
belief-based strategies are not sufficient, and present an exponential upper
bound on memory both for almost-sure and positive winning strategies; we show
that the problem of deciding the existence of almost-sure and positive winning
strategies for player 1 is EXPTIME-complete and present symbolic algorithms
that avoid the explicit exponential construction. (2)For one-sided games with
player 1 perfect observation we show that non-elementary memory is both
necessary and sufficient for both almost-sure and positive winning strategies.
(3)We show that for the two-sided case finite memory strategies are sufficient
for both positive and almost-sure winning. We establish the equivalence of the
almost-sure winning problem for pure strategies with randomized strategies with
actions invisible. Our equivalence result exhibit serious flaws in previous
results in the literature: we show a non-elementary memory lower bound for
almost-sure winning whereas an exponential upper bound was claimed."
"We consider 2-player games played on a finite state space for infinite
rounds. The games are concurrent: in each round, the two players choose their
moves simultaneously; the current state and the moves determine the successor.
We consider omega-regular winning conditions given as parity objectives. We
consider the qualitative analysis problems: the computation of the almost-sure
and limit-sure winning set of states, where player 1 can ensure to win with
probability 1 and with probability arbitrarily close to 1, respectively. In
general the almost-sure and limit-sure winning strategies require both
infinite-memory and infinite-precision. We study the bounded-rationality
problem for qualitative analysis of concurrent parity games, where the strategy
set player 1 is restricted to bounded-resource strategies. In terms of
precision, strategies can be deterministic, uniform, finite-precision or
infinite-precision; and in terms of memory, strategies can be memoryless,
finite-memory or infinite-memory. We present a precise and complete
characterization of the qualitative winning sets for all combinations of
classes of strategies. In particular, we show that uniform memoryless
strategies are as powerful as finite-precision infinite-memory strategies, and
infinite-precision memoryless strategies are as powerful as infinite-precision
finite-memory strategies. We show that the winning sets can be computed in
O(n^{2d+3}) time, where n is the size of the game and 2d is the number of
priorities, and our algorithms are symbolic. The membership problem of whether
a state belongs to a winning set can be decided in NP cap coNP. While this
complexity is the same as for the simpler class of turn-based games, where in
each state only one of the players has a choice of moves, our algorithms, that
are obtained by characterization of the winning sets as mu-calculus formulas,
are considerably more involved."
"We consider structural and algorithmic questions related to the Nash dynamics
of weighted congestion games. In weighted congestion games with linear latency
functions, the existence of (pure Nash) equilibria is guaranteed by potential
function arguments. Unfortunately, this proof of existence is inefficient and
computing equilibria is such games is a {\sf PLS}-hard problem. The situation
gets worse when superlinear latency functions come into play; in this case, the
Nash dynamics of the game may contain cycles and equilibria may not even exist.
Given these obstacles, we consider approximate equilibria as alternative
solution concepts. Do such equilibria exist? And if so, can we compute them
efficiently?
  We provide positive answers to both questions for weighted congestion games
with polynomial latency functions by exploiting an ""approximation"" of such
games by a new class of potential games that we call $\Psi$-games. This allows
us to show that these games have $d!$-approximate equilibria, where $d$ is the
maximum degree of the latency functions. Our main technical contribution is an
efficient algorithm for computing O(1)-approximate equilibria when $d$ is a
constant. For games with linear latency functions, the approximation guarantee
is $\frac{3+\sqrt{5}}{2}+O(\gamma)$ for arbitrarily small $\gamma>0$; for
latency functions with maximum degree $d\geq 2$, it is $d^{2d+o(d)}$. The
running time is polynomial in the number of bits in the representation of the
game and $1/\gamma$. As a byproduct of our techniques, we also show the
following structural statement for weighted congestion games with polynomial
latency functions of maximum degree $d\geq 2$: polynomially-long sequences of
best-response moves from any initial state to a $d^{O(d^2)}$-approximate
equilibrium exist and can be efficiently identified in such games as long as
$d$ is constant."
"We study problems of scheduling jobs on related machines so as to minimize
the makespan in the setting where machines are strategic agents. In this
problem, each job $j$ has a length $l_{j}$ and each machine $i$ has a private
speed $t_{i}$. The running time of job $j$ on machine $i$ is $t_{i}l_{j}$. We
seek a mechanism that obtains speed bids of machines and then assign jobs and
payments to machines so that the machines have incentive to report true speeds
and the allocation and payments are also envy-free. We show that
  1. A deterministic envy-free, truthful, individually rational, and anonymous
mechanism cannot approximate the makespan strictly better than $2-1/m$, where
$m$ is the number of machines. This result contrasts with prior work giving a
deterministic PTAS for envy-free anonymous assignment and a distinct
deterministic PTAS for truthful anonymous mechanism.
  2. For two machines of different speeds, the unique deterministic scalable
allocation of any envy-free, truthful, individually rational, and anonymous
mechanism is to allocate all jobs to the quickest machine. This allocation is
the same as that of the VCG mechanism, yielding a 2-approximation to the
minimum makespan.
  3. No payments can make any of the prior published monotone and locally
efficient allocations that yield better than an $m$-approximation for $\qcmax$
\cite{aas, at,ck10, dddr, kovacs} a truthful, envy-free, individually rational,
and anonymous mechanism."
"Budget feasible mechanism considers algorithmic mechanism design questions
where there is a budget constraint on the total payment of the mechanism. An
important question in the field is that under which valuation domains there
exist budget feasible mechanisms that admit `small' approximations (compared to
a socially optimal solution). Singer \cite{PS10} showed that additive and
submodular functions admit a constant approximation mechanism. Recently,
Dobzinski, Papadimitriou, and Singer \cite{DPS11} gave an $O(\log^2n)$
approximation mechanism for subadditive functions and remarked that: ""A
fundamental question is whether, regardless of computational constraints, a
constant-factor budget feasible mechanism exists for subadditive function.""
  In this paper, we give the first attempt to this question. We give a
polynomial time $O(\frac{\log n}{\log\log n})$ sub-logarithmic approximation
ratio mechanism for subadditive functions, improving the best known ratio
$O(\log^2 n)$. Further, we connect budget feasible mechanism design to the
concept of approximate core in cooperative game theory, and show that there is
a mechanism for subadditive functions whose approximation is, via a
characterization of the integrality gap of a linear program, linear to the
largest value to which an approximate core exists. Our result implies in
particular that the class of XOS functions, which is a superclass of submodular
functions, admits a constant approximation mechanism. We believe that our work
could be a solid step towards solving the above fundamental problem eventually,
and possibly, with an affirmative answer."
"We analyze the core of a cooperative Cournot game. We assume that when
contemplating a deviation, the members of a coalition assign positive
probability over all possible coalition structures that the non-members can
form. We show that when the number of firms in the market is sufficiently large
then the core of the underlying cooperative game is non-empty. Moreover, we
show that the core of our game is a subset of the \gamma - core."
"We analyze strategic delegation in a Stackelberg model with an arbitrary
number, n, of firms. We show that the n-1 last movers delegate their production
decisions to managers whereas the first mover does not. Equilibrium incentive
rates are increasing in the order with which managers select quantities.
Letting u_i^* denote the equilibrium payoff of the firm whose manager moves in
the i-th place, we show that u_n^*>u_{n-1}^*>...>u_2^*>u_1^*. We also compare
the delegation outcome of our game with that of a Cournot oligopoly and show
that the late (early) moving firms choose higher (lower) incentive rates than
the Cournot firms."
"We modify the concept of quantum strategic game to make it useful for
extensive form games. We prove that our modification allows to consider the
normal representation of any finite extensive game using the fundamental
concepts of quantum information. The Selten's Horse game and the general form
of two-stage extensive game with perfect information are studied to illustrate
a potential application of our idea. In both examples we use
Eisert-Wilkens-Lewenstein approach as well as Marinatto-Weber approach to
quantization of games."
"Frequency non-selective time-selective multiple access channels in which
transmitters can freely choose their power control policy are considered. The
individual objective of the transmitters is to maximize their averaged
energy-efficiency. For this purpose, a transmitter has to choose a power
control policy that is, a sequence of power levels adapted to the channel
variations. This problem can be formulated as a stochastic game with
discounting for which there exists a theorem characterizing all the equilibrium
utilities (equilibrium utility region). As in its general formulation, this
theorem relies on global channel state information (CSI), it is shown that some
points of the utility region can be reached with individual CSI. Interestingly,
time-sharing based solutions, which are usually considered for centralized
policies, appear to be part of the equilibrium solutions. This analysis is
illustrated by numerical results providing further insights to the problem
under investigation."
"Except for special classes of games, there is no systematic framework for
analyzing the dynamical properties of multi-agent strategic interactions.
Potential games are one such special but restrictive class of games that allow
for tractable dynamic analysis. Intuitively, games that are ""close"" to a
potential game should share similar properties. In this paper, we formalize and
develop this idea by quantifying to what extent the dynamic features of
potential games extend to ""near-potential"" games. We study convergence of three
commonly studied classes of adaptive dynamics: discrete-time better/best
response, logit response, and discrete-time fictitious play dynamics. For
better/best response dynamics, we focus on the evolution of the sequence of
pure strategy profiles and show that this sequence converges to a (pure)
approximate equilibrium set, whose size is a function of the ""distance"" from a
close potential game. We then study logit response dynamics and provide a
characterization of the stationary distribution of this update rule in terms of
the distance of the game from a close potential game and the corresponding
potential function. We further show that the stochastically stable strategy
profiles are pure approximate equilibria. Finally, we turn attention to
fictitious play, and establish that the sequence of empirical frequencies of
player actions converges to a neighborhood of (mixed) equilibria of the game,
where the size of the neighborhood increases with distance of the game to a
potential game. Thus, our results suggest that games that are close to a
potential game inherit the dynamical properties of potential games. Since a
close potential game to a given game can be found by solving a convex
optimization problem, our approach also provides a systematic framework for
studying convergence behavior of adaptive learning dynamics in arbitrary finite
strategic form games."
"Logit Dynamics [Blume, Games and Economic Behavior, 1993] are randomized best
response dynamics for strategic games: at every time step a player is selected
uniformly at random and she chooses a new strategy according to a probability
distribution biased toward strategies promising higher payoffs. This process
defines an ergodic Markov chain, over the set of strategy profiles of the game,
whose unique stationary distribution is the long-term equilibrium concept for
the game. However, when the mixing time of the chain is large (e.g.,
exponential in the number of players), the stationary distribution loses its
appeal as equilibrium concept, and the transient phase of the Markov chain
becomes important. It can happen that the chain is ""metastable"", i.e., on a
time-scale shorter than the mixing time, it stays close to some probability
distribution over the state space, while in a time-scale multiple of the mixing
time it jumps from one distribution to another.
  In this paper we give a quantitative definition of ""metastable probability
distributions"" for a Markov chain and we study the metastability of the logit
dynamics for some classes of coordination games. We first consider a pure
$n$-player coordination game that highlights the distinctive features of our
metastability notion based on distributions. Then, we study coordination games
on the clique without a risk-dominant strategy (which are equivalent to the
well-known Glauber dynamics for the Curie-Weiss model) and coordination games
on a ring (both with and without risk-dominant strategy)."
"The key contribution of the paper is a comprehensive study of the egalitarian
mechanism with respect to manipulation by a coalition of agents. Our main
result is that the egalitarian mechanism is, in fact, peak group strategyproof
: no coalition of agents can (weakly) benefit from jointly misreporting their
peaks. Furthermore, we show that the egalitarian mechanism cannot be
manipulated by any coalition of suppliers (or any coalition of demanders) in
the model where both the suppliers and demanders are agents. Our proofs shed
light on the structure of the two models and simpify some of the earlier proofs
of strategyproofness in the earlier papers. An implication of our results is
that the well known algorithm of Megiddo to compute a lexicographically optimal
flow in a network is group strategyproof with respect to the source capacities
(or sink capacities)."
"We develop efficient algorithms to construct utility maximizing mechanisms in
the presence of risk averse players (buyers and sellers) in Bayesian settings.
We model risk aversion by a concave utility function, and players play
strategically to maximize their expected utility. Bayesian mechanism design has
usually focused on maximizing expected revenue in a {\em risk neutral}
environment, and no succinct characterization of expected utility maximizing
mechanisms is known even for single-parameter multi-unit auctions.
  We first consider the problem of designing optimal DSIC mechanism for a risk
averse seller in the case of multi-unit auctions, and we give a poly-time
computable SPM that is $(1-1/e-\eps)$-approximation to the expected utility of
the seller in an optimal DSIC mechanism. Our result is based on a novel
application of a correlation gap bound, along with {\em splitting} and {\em
merging} of random variables to redistribute probability mass across buyers.
This allows us to reduce our problem to that of checking feasibility of a small
number of distinct configurations, each of which corresponds to a covering LP.
A feasible solution to the LP gives us the distribution on prices for each
buyer to use in a randomized SPM.
  We next consider the setting when buyers as well as the seller are risk
averse, and the objective is to maximize the seller's expected utility. We
design a truthful-in-expectation mechanism whose utility is a $(1-1/e
-\eps)^3$-approximation to the optimal BIC mechanism under two mild
assumptions. Our mechanism consists of multiple rounds that processes each
buyer in a round with small probability. Lastly, we consider the problem of
revenue maximization for a risk neutral seller in presence of risk averse
buyers, and give a poly-time algorithm to design an optimal mechanism for the
seller."
"In digital goods auctions, there is an auctioneer who sells an item with
unlimited supply to a set of potential buyers, and the objective is to design
truthful auction to maximize the total profit of the auctioneer. Motivated from
an observation that the values of buyers for the item could be interconnected
through social networks, we study digital goods auctions with positive
externalities among the buyers. This defines a multi-parameter auction design
problem where the private valuation of every buyer is a function of other
winning buyers. The main contribution of this paper is a truthful competitive
mechanism for subadditive valuations. Our competitive result is with respect to
a new solution benchmark $\mathcal{F}^{(3)}$; on the other hand, we show a
surprising impossibility result if comparing to the benchmark
$\mathcal{F}^{(2)}$, where the latter has been used quite successfully in
digital goods auctions without extenalities \cite{Goldberg2006}. Our results
from $\mathcal{F}^{(2)}$ to $\mathcal{F}^{(3)}$ could be considered as the loss
of optimal profit at the cost of externalities."
"In this paper we present and evaluate a general framework for the design of
truthful auctions for matching agents in a dynamic, two-sided market. A single
commodity, such as a resource or a task, is bought and sold by multiple buyers
and sellers that arrive and depart over time. Our algorithm, Chain, provides
the first framework that allows a truthful dynamic double auction (DA) to be
constructed from a truthful, single-period (i.e. static) double-auction rule.
The pricing and matching method of the Chain construction is unique amongst
dynamic-auction rules that adopt the same building block. We examine
experimentally the allocative efficiency of Chain when instantiated on various
single-period rules, including the canonical McAfee double-auction rule. For a
baseline we also consider non-truthful double auctions populated with
zero-intelligence plus""-style learning agents. Chain-based auctions perform
well in comparison with other schemes, especially as arrival intensity falls
and agent valuations become more volatile."
"The concept of femtocell access points underlaying existing communication
infrastructure has recently emerged as a key technology that can significantly
improve the coverage and performance of next-generation wireless networks. In
this paper, we propose a framework for macrocell-femtocell cooperation under a
closed access policy, in which a femtocell user may act as a relay for
macrocell users. In return, each cooperative macrocell user grants the
femtocell user a fraction of its superframe. We formulate a coalitional game
with macrocell and femtocell users being the players, which can take individual
and distributed decisions on whether to cooperate or not, while maximizing a
utility function that captures the cooperative gains, in terms of throughput
and delay.We show that the network can selforganize into a partition composed
of disjoint coalitions which constitutes the recursive core of the game
representing a key solution concept for coalition formation games in partition
form. Simulation results show that the proposed coalition formation algorithm
yields significant gains in terms of average rate per macrocell user, reaching
up to 239%, relative to the non-cooperative case. Moreover, the proposed
approach shows an improvement in terms of femtocell users' rate of up to 21%
when compared to the traditional closed access policy."
"We consider a network creation game in which each player (vertex) has a fixed
budget to establish links to other players. In our model, each link has unit
price and each agent tries to minimize its cost, which is either its local
diameter or its total distance to other players in the (undirected) underlying
graph of the created network. Two versions of the game are studied: in the MAX
version, the cost incurred to a vertex is the maximum distance between the
vertex and other vertices, and in the SUM version, the cost incurred to a
vertex is the sum of distances between the vertex and other vertices. We prove
that in both versions pure Nash equilibria exist, but the problem of finding
the best response of a vertex is NP-hard. We take the social cost of the
created network to be its diameter, and next we study the maximum possible
diameter of an equilibrium graph with n vertices in various cases. When the sum
of players' budgets is n-1, the equilibrium graphs are always trees, and we
prove that their maximum diameter is Theta(n) and Theta(log n) in MAX and SUM
versions, respectively. When each vertex has unit budget (i.e. can establish
link to just one vertex), the diameter of any equilibrium graph in either
version is Theta(1). We give examples of equilibrium graphs in the MAX version,
such that all vertices have positive budgets and yet the diameter is
Omega(sqrt(log n)). This interesting (and perhaps counter-intuitive) result
shows that increasing the budgets may increase the diameter of equilibrium
graphs and hence deteriorate the network structure. Then we prove that every
equilibrium graph in the SUM version has diameter 2^O(sqrt(log n)). Finally, we
show that if the budget of each player is at least k, then every equilibrium
graph in the SUM version is k-connected or has diameter smaller than 4."
"DeMarzo et al. (2005) consider auctions in which bids are selected from a
completely ordered family of securities whose values are tied to the resource
being auctioned. The paper defines a notion of relative steepness of families
of securities and shows that a steeper family provides greater expected revenue
to the seller. Two assumptions are: the buyers are risk-neutral; the random
variables through which values and signals of the buyers are realized are
affiliated. We show that this revenue ranking holds for the second price
auction in the case of risk-aversion. However, it does not hold if affiliation
is relaxed to a less restrictive form of positive dependence, namely first
order stochastic dominance (FOSD). We define the relative strong steepness of
families of securities and show that it provides a necessary and sufficient
condition for comparing two families in the FOSD case. All results extend to
the English auction."
"Many large decentralized systems rely on information propagation to ensure
their proper function. We examine a common scenario in which only participants
that are aware of the information can compete for some reward, and thus
informed participants have an incentive not to propagate information to others.
One recent example in which such tension arises is the 2009 DARPA Network
Challenge (finding red balloons). We focus on another prominent example:
Bitcoin, a decentralized electronic currency system.
  Bitcoin represents a radical new approach to monetary systems. It has been
getting a large amount of public attention over the last year, both in policy
discussions and in the popular press. Its cryptographic fundamentals have
largely held up even as its usage has become increasingly widespread. We find,
however, that it exhibits a fundamental problem of a different nature, based on
how its incentives are structured. We propose a modification to the protocol
that can eliminate this problem.
  Bitcoin relies on a peer-to-peer network to track transactions that are
performed with the currency. For this purpose, every transaction a node learns
about should be transmitted to its neighbors in the network. The current
implemented protocol provides an incentive to nodes to not broadcast
transactions they are aware of. Our solution is to augment the protocol with a
scheme that rewards information propagation. Since clones are easy to create in
the Bitcoin system, an important feature of our scheme is Sybil-proofness.
  We show that our proposed scheme succeeds in setting the correct incentives,
that it is Sybil-proof, and that it requires only a small payment overhead, all
this is achieved with iterated elimination of dominated strategies. We
complement this result by showing that there are no reward schemes in which
information propagation and no self-cloning is a dominant strategy."
"Online learning algorithms that minimize regret provide strong guarantees in
situations that involve repeatedly making decisions in an uncertain
environment, e.g. a driver deciding what route to drive to work every day.
While regret minimization has been extensively studied in repeated games, we
study regret minimization for a richer class of games called bounded memory
games. In each round of a two-player bounded memory-m game, both players
simultaneously play an action, observe an outcome and receive a reward. The
reward may depend on the last m outcomes as well as the actions of the players
in the current round. The standard notion of regret for repeated games is no
longer suitable because actions and rewards can depend on the history of play.
To account for this generality, we introduce the notion of k-adaptive regret,
which compares the reward obtained by playing actions prescribed by the
algorithm against a hypothetical k-adaptive adversary with the reward obtained
by the best expert in hindsight against the same adversary. Roughly, a
hypothetical k-adaptive adversary adapts her strategy to the defender's actions
exactly as the real adversary would within each window of k rounds. Our
definition is parametrized by a set of experts, which can include both fixed
and adaptive defender strategies.
  We investigate the inherent complexity of and design algorithms for adaptive
regret minimization in bounded memory games of perfect and imperfect
information. We prove a hardness result showing that, with imperfect
information, any k-adaptive regret minimizing algorithm (with fixed strategies
as experts) must be inefficient unless NP=RP even when playing against an
oblivious adversary. In contrast, for bounded memory games of perfect and
imperfect information we present approximate 0-adaptive regret minimization
algorithms against an oblivious adversary running in time n^{O(1)}."
"We study the design and approximation of optimal crowdsourcing contests.
Crowdsourcing contests can be modeled as all-pay auctions because entrants must
exert effort up-front to enter. Unlike all-pay auctions where a usual design
objective would be to maximize revenue, in crowdsourcing contests, the
principal only benefits from the submission with the highest quality. We give a
theory for optimal crowdsourcing contests that mirrors the theory of optimal
auction design: the optimal crowdsourcing contest is a virtual valuation
optimizer (the virtual valuation function depends on the distribution of
contestant skills and the number of contestants). We also compare crowdsourcing
contests with more conventional means of procurement. In this comparison,
crowdsourcing contests are relatively disadvantaged because the effort of
losing contestants is wasted. Nonetheless, we show that crowdsourcing contests
are 2-approximations to conventional methods for a large family of ""regular""
distributions, and 4-approximations, otherwise."
"In traditional mechanism design, agents only care about the utility they
derive from the outcome of the mechanism. We look at a richer model where
agents also assign non-negative dis-utility to the information about their
private types leaked by the outcome of the mechanism.
  We present a new model for privacy-aware mechanism design, where we only
assume an upper bound on the agents' loss due to leakage, as opposed to
previous work where a full characterization of the loss was required.
  In this model, under a mild assumption on the distribution of how agents
value their privacy, we show a generic construction of privacy-aware mechanisms
and demonstrate its applicability to electronic polling and pricing of a
digital good."
"The model of congestion games is widely used to analyze games related to
traffic and communication. A central property of these games is that they are
potential games and hence posses a pure Nash equilibrium. In reality it is
often the case that some players cooperatively decide on their joint action in
order to maximize the coalition's total utility. This is by modeled by
Coalitional Congestion Games. Typical settings include truck drivers who work
for the same shipping company, or routers that belong to the same ISP. The
formation of coalitions will typically imply that the resulting coalitional
congestion game will no longer posses a pure Nash equilibrium. In this paper we
provide conditions under which such games are potential games and posses a pure
Nash equilibrium."
"Partially-ordered set games, also called poset games, are a class of
two-player combinatorial games. The playing field consists of a set of
elements, some of which are greater than other elements. Two players take turns
removing an element and all elements greater than it, and whoever takes the
last element wins. Examples of poset games include Nim and Chomp. We
investigate the complexity of computing which player of a poset game has a
winning strategy. We give an inductive procedure that modifies poset games to
change the nim- value which informally captures the winning strategies in the
game. For a generic poset game G, we describe an efficient method for
constructing a game not G such that the first player has a winning strategy if
and only if the second player has a winning strategy on G. This solves the
long-standing problem of whether this construction can be done efficiently.
This construction also allows us to reduce the class of Boolean formulas to
poset games, establishing a lower bound on the complexity of poset games."
"Recent work has constructed economic mechanisms that are both truthful and
differentially private. In these mechanisms, privacy is treated separately from
the truthfulness; it is not incorporated in players' utility functions (and
doing so has been shown to lead to non-truthfulness in some cases). In this
work, we propose a new, general way of modelling privacy in players' utility
functions. Specifically, we only assume that if an outcome $o$ has the property
that any report of player $i$ would have led to $o$ with approximately the same
probability, then $o$ has small privacy cost to player $i$. We give three
mechanisms that are truthful with respect to our modelling of privacy: for an
election between two candidates, for a discrete version of the facility
location problem, and for a general social choice problem with discrete
utilities (via a VCG-like mechanism). As the number $n$ of players increases,
the social welfare achieved by our mechanisms approaches optimal (as a fraction
of $n$)."
"Rosenthal (1973) introduced the class of congestion games and proved that
they always possess a Nash equilibrium in pure strategies. Fotakis et al.
(2005) introduce the notion of a greedy strategy tuple, where players
sequentially and irrevocably choose a strategy that is a best response to the
choice of strategies by former players. Whereas the former solution concept is
driven by strong assumptions on the rationality of the players and the common
knowledge thereof, the latter assumes very little rationality on the players'
behavior. From Fotakis \cite{fotakis10} it follows that for Tree Representable
congestion Games greedy behavior leads to a NE. In this paper we obtain
necessary and sufficient conditions for the equivalence of these two solution
concepts. Such equivalence enhances the viability of these concepts as
realistic outcomes of the environment. The conditions for such equivalence to
emerge for monotone symmetric games is that the strategy set has a tree-form,
or equivalently is a `extension-parallel graph'."
"Mastermind is a popular board game released in 1971, where a codemaker
chooses a secret pattern of colored pegs, and a codebreaker has to guess it in
several trials. After each attempt, the codebreaker gets a response from the
codemaker containing some information on the number of correctly guessed pegs.
The search space is thus reduced at each turn, and the game continues until the
codebreaker is able to find the correct code, or runs out of trials.
  In this paper we study several variations of #MSP, the problem of computing
the size of the search space resulting from a given (possibly fictitious)
sequence of guesses and responses. Our main contribution is a proof of the
#P-completeness of #MSP under parsimonious reductions, which settles an open
problem posed by Stuckman and Zhang in 2005, concerning the complexity of
deciding if the secret code is uniquely determined by the previous guesses and
responses. Similarly, #MSP stays #P-complete under Turing reductions even with
the promise that the search space has at least k elements, for any constant k.
(In a regular game of Mastermind, k=1.)
  All our hardness results hold even in the most restrictive setting, in which
there are only two available peg colors, and also if the codemaker's responses
contain less information, for instance like in the so-called single-count
(black peg) Mastermind variation."
"As we show by using notions of equilibrium in infinite sequential games,
crashes or financial escalations are rational for economic or environmental
agents, who have a vision of an infinite world. This contradicts a picture of a
self-regulating, wise and pacific economic world. In other words, in this
context, equilibrium is not synonymous of stability. We try to draw, from this
statement, methodological consequences and new ways of thinking, especially in
economic game theory. Among those new paths, coinduction is the basis of our
reasoning in infinite games."
"We solve the classical ""Game of Pure Strategy"" using linear programming. We
notice an intricate even-odd behavior in the results of our computations, that
seems to encourage odd or maximal bids."
"We consider an auction of identical digital goods to customers whose
valuations are drawn independently from known distributions. Myerson's classic
result identifies the truthful mechanism that maximizes the seller's expected
profit.
  Under the assumption that in small groups customers can learn each others'
valuations, we show how Myerson's result can be improved to yield a higher
payoff to the seller using a mechanism that offers groups of customers to buy
bundles of items."
"Emek et al. presented a model of probabilistic single-item second price
auctions where an auctioneer who is informed about the type of an item for
sale, broadcasts a signal about this type to uninformed bidders. They proved
that finding the optimal (for the purpose of generating revenue) {\em pure}
signaling scheme is strongly NP-hard. In contrast, we prove that finding the
optimal {\em mixed} signaling scheme can be done in polynomial time using
linear programming. For the proof, we show that the problem is strongly related
to a problem of optimally bundling divisible goods for auctioning. We also
prove that a mixed signaling scheme can in some cases generate twice as much
revenue as the best pure signaling scheme and we prove a generally applicable
lower bound on the revenue generated by the best mixed signaling scheme."
"Signaling is an important topic in the study of asymmetric information in
economic settings. In particular, the transparency of information available to
a seller in an auction setting is a question of major interest. We introduce
the study of signaling when conducting a second price auction of a
probabilistic good whose actual instantiation is known to the auctioneer but
not to the bidders. This framework can be used to model impressions selling in
display advertising. We study the problem of computing a signaling scheme that
maximizes the auctioneer's revenue in a Bayesian setting. While the general
case is proved to be computationally hard, several cases of interest are shown
to be polynomially solvable. In addition, we establish a tight bound on the
minimum number of signals required to implement an optimal signaling scheme and
show that at least half of the maximum social welfare can be preserved within
such a scheme."
"Hanson's market scoring rules allow us to design a prediction market that
still gives useful information even if we have an illiquid market with a
limited number of budget-constrained agents. Each agent can ""move"" the current
price of a market towards their prediction.
  While this movement still occurs in multi-outcome or multidimensional markets
we show that no market-scoring rule, under reasonable conditions, always moves
the price directly towards beliefs of the agents. We present a modified version
of a market scoring rule for budget-limited traders, and show that it does have
the property that, from any starting position, optimal trade by a
budget-limited trader will result in the market being moved towards the
trader's true belief. This mechanism also retains several attractive strategic
properties of the market scoring rule."
"One of the fundamental questions of Algorithmic Mechanism Design is whether
there exists an inherent clash between truthfulness and computational
tractability: in particular, whether polynomial-time truthful mechanisms for
combinatorial auctions are provably weaker in terms of approximation ratio than
non-truthful ones. This question was very recently answered for universally
truthful mechanisms for combinatorial auctions \cite{D11}, and even for
truthful-in-expectation mechanisms \cite{DughmiV11}. However, both of these
results are based on information-theoretic arguments for valuations given by a
value oracle, and leave open the possibility of polynomial-time truthful
mechanisms for succinctly described classes of valuations.
  This paper is the first to prove {\em computational hardness} results for
truthful mechanisms for combinatorial auctions with succinctly described
valuations. We prove that there is a class of succinctly represented submodular
valuations for which no deterministic truthful mechanism provides an
$m^{1/2-\epsilon}$-approximation for a constant $\epsilon>0$, unless $NP=RP$
($m$ denotes the number of items). Furthermore, we prove that even
truthful-in-expectation mechanisms cannot approximate combinatorial auctions
with certain succinctly described submodular valuations better than within
$n^\gamma$, where $n$ is the number of bidders and $\gamma>0$ some absolute
constant, unless $NP \subseteq P/poly$. In addition, we prove computational
hardness results for two related problems."
"We reconsider the well-studied Selfish Routing game with affine latency
functions. The Price of Anarchy for this class of games takes maximum value
4/3; this maximum is attained already for a simple network of two parallel
links, known as Pigou's network. We improve upon the value 4/3 by means of
Coordination Mechanisms.
  We increase the latency functions of the edges in the network, i.e., if
$\ell_e(x)$ is the latency function of an edge $e$, we replace it by
$\hat{\ell}_e(x)$ with $\ell_e(x) \le \hat{\ell}_e(x)$ for all $x$. Then an
adversary fixes a demand rate as input. The engineered Price of Anarchy of the
mechanism is defined as the worst-case ratio of the Nash social cost in the
modified network over the optimal social cost in the original network.
Formally, if $\CM(r)$ denotes the cost of the worst Nash flow in the modified
network for rate $r$ and $\Copt(r)$ denotes the cost of the optimal flow in the
original network for the same rate then [\ePoA = \max_{r \ge 0}
\frac{\CM(r)}{\Copt(r)}.]
  We first exhibit a simple coordination mechanism that achieves for any
network of parallel links an engineered Price of Anarchy strictly less than
4/3. For the case of two parallel links our basic mechanism gives 5/4 = 1.25.
Then, for the case of two parallel links, we describe an optimal mechanism; its
engineered Price of Anarchy lies between 1.191 and 1.192."
"In many social computing applications such as online Q&A forums, the best
contribution for each task receives some high reward, while all remaining
contributions receive an identical, lower reward irrespective of their actual
qualities. Suppose a mechanism designer (site owner) wishes to optimize an
objective that is some function of the number and qualities of received
contributions. When potential contributors are strategic agents, who decide
whether to contribute or not to selfishly maximize their own utilities, is such
a ""best contribution"" mechanism, M_B, adequate to implement an outcome that is
optimal for the mechanism designer?
  We first show that in settings where a contribution's value is determined
primarily by an agent's expertise, and agents only strategically choose whether
to contribute or not, contests can implement optimal outcomes: for any
reasonable objective, the rewards for the best and remaining contributions in
M_B can always be chosen so that the outcome in the unique symmetric
equilibrium of M_B maximizes the mechanism designer's utility. We also show how
the mechanism designer can learn these optimal rewards when she does not know
the parameters of the agents' utilities, as might be the case in practice. We
next consider settings where a contribution's value depends on both the
contributor's expertise as well as her effort, and agents endogenously choose
how much effort to exert in addition to deciding whether to contribute. Here,
we show that optimal outcomes can never be implemented by contests if the
system can rank the qualities of contributions perfectly. However, if there is
noise in the contributions' rankings, then the mechanism designer can again
induce agents to follow strategies that maximize his utility. Thus imperfect
rankings can actually help achieve implementability of optimal outcomes when
effort is endogenous and influences quality."
"We analyze cooperative Cournot games with boundedly rational firms. Due to
cogni- tive constraints, the members of a coalition cannot accurately predict
the coalitional structure of the non-members. Thus, they compute their value
using simple heuris- tics. In particular, they assign various non-equilibrium
probability distributions over the outsiders' set of partitions. We construct
the characteristic function of a coalition in such an environment and we
analyze the core of the corresponding games. We show that the core is non-empty
provided the number of firms in the market is sufficiently large. Moreover, we
show that if two distributions over the set of partitions are related via
first-order dominance, then the core of the game under the dominated
distribution is a subset of the core under the dominant distribution."
"Shapley's discounted stochastic games, Everett's recursive games and
Gillette's undiscounted stochastic games are classical models of game theory
describing two-player zero-sum games of potentially infinite duration. We
describe algorithms for exactly solving these games."
"We introduce games with probabilistic uncertainty, a natural model for
controller synthesis in which the controller observes the state of the system
through imprecise sensors that provide correct information about the current
state with a fixed probability. That is, in each step, the sensors return an
observed state, and given the observed state, there is a probability
distribution (due to the estimation error) over the actual current state. The
controller must base its decision on the observed state (rather than the actual
current state, which it does not know). On the other hand, we assume that the
environment can perfectly observe the current state. We show that our model can
be reduced in polynomial time to standard partial-observation stochastic games,
and vice-versa. As a consequence we establish the precise decidability frontier
for the new class of games, and for most of the decidable problems establish
optimal complexity results."
"Existing web infrastructures have supported the publication of a tremendous
amount of resources, and over the past few years Data Resource Usage is an
everyday task for millions of users all over the world. In this work we model
Resource Usage as a Cooperative Cournot Game in which a resource user and the
various resource services are engaged. We give quantified answers as to when it
is of interest for the user to stop using part of a resource and to switch to a
different one. Moreover, we do the same from the perspective of a resource's
provider."
"We investigate the design of mechanisms to incentivize high quality in
crowdsourcing environments with strategic agents, when entry is an endogenous,
strategic choice. Modeling endogenous entry in crowdsourcing is important
because there is a nonzero cost to making a contribution of any quality which
can be avoided by not participating, and indeed many sites based on
crowdsourced content do not have adequate participation. We use a mechanism
with monotone, rank-based, rewards in a model where agents strategically make
participation and quality choices to capture a wide variety of crowdsourcing
environments, ranging from conventional crowdsourcing contests to crowdsourced
content as in online Q&A forums.
  We first explicitly construct the unique mixed-strategy equilibrium for such
monotone rank-order mechanisms, and use these participation probabilities and
quality distribution to address the design of incentives for two kinds of
rewards that arise in crowdsourcing. We first show that for attention rewards
as in crowdsourced content, the entire equilibrium distribution improves when
the rewards for every rank but the last are as high as possible. In particular,
when producing the lowest quality content has low cost, the optimal mechanism
displays all but the worst contribution. We next investigate settings where
there is a total reward that can be arbitrarily distributed amongst all
participants, as in crowdsourcing contests. Unlike with exogenous entry, here
the expected number of participants can be increased by subsidizing entry,
which could potentially improve the expected quality of the best contribution.
However, we show that free entry is dominated by taxing entry- making all
entrants pay a small fee, which is rebated to the winner along with whatever
rewards were already assigned, can improve the quality of the best contribution
over a winner-take-all contest with no taxes."
"We study network formation with n players and link cost \alpha > 0. After the
network is built, an adversary randomly deletes one link according to a certain
probability distribution. Cost for player v incorporates the expected number of
players to which v will become disconnected. We show existence of equilibria
and a price of stability of 1+o(1) under moderate assumptions on the adversary
and n \geq 9.
  As the main result, we prove bounds on the price of anarchy for two special
adversaries: one removes a link chosen uniformly at random, while the other
removes a link that causes a maximum number of player pairs to be separated.
For unilateral link formation we show a bound of O(1) on the price of anarchy
for both adversaries, the constant being bounded by 10+o(1) and 8+o(1),
respectively. For bilateral link formation we show O(1+\sqrt{n/\alpha}) for one
adversary (if \alpha > 1/2), and \Theta(n) for the other (if \alpha > 2
considered constant and n \geq 9). The latter is the worst that can happen for
any adversary in this model (if \alpha = \Omega(1)). This points out
substantial differences between unilateral and bilateral link formation."
"We consider a game in which a strategic defender classifies an intruder as
spy or spammer. The classification is based on the number of file server and
mail server attacks observed during a fixed window. The spammer naively attacks
(with a known distribution) his main target: the mail server. The spy
strategically selects the number of attacks on his main target: the file
server. The defender strategically selects his classification policy: a
threshold on the number of file server attacks. We model the interaction of the
two players (spy and defender) as a nonzero-sum game: The defender needs to
balance missed detections and false alarms in his objective function, while the
spy has a tradeoff between attacking the file server more aggressively and
increasing the chances of getting caught. We give a characterization of the
Nash equilibria in mixed strategies, and demonstrate how the Nash equilibria
can be computed in polynomial time. Our characterization gives interesting and
non-intuitive insights on the players' strategies at equilibrium: The defender
uniformly randomizes between a set of thresholds that includes very large
values. The strategy of the spy is a truncated version of the spammer's
distribution. We present numerical simulations that validate and illustrate our
theoretical results."
"We consider K-Facility Location games, where n strategic agents report their
locations in a metric space, and a mechanism maps them to K facilities. Our
main result is an elegant characterization of deterministic strategyproof
mechanisms with a bounded approximation ratio for 2-Facility Location on the
line. In particular, we show that for instances with n \geq 5 agents, any such
mechanism either admits a unique dictator, or always places the facilities at
the leftmost and the rightmost location of the instance. As a corollary, we
obtain that the best approximation ratio achievable by deterministic
strategyproof mechanisms for the problem of locating 2 facilities on the line
to minimize the total connection cost is precisely n-2. Another rather
surprising consequence is that the Two-Extremes mechanism of (Procaccia and
Tennenholtz, EC 2009) is the only deterministic anonymous strategyproof
mechanism with a bounded approximation ratio for 2-Facility Location on the
line.
  The proof of the characterization employs several new ideas and technical
tools, which provide new insights into the behavior of deterministic
strategyproof mechanisms for K-Facility Location games, and may be of
independent interest. Employing one of these tools, we show that for every K
\geq 3, there do not exist any deterministic anonymous strategyproof mechanisms
with a bounded approximation ratio for K-Facility Location on the line, even
for simple instances with K+1 agents. Moreover, building on the
characterization for the line, we show that there do not exist any
deterministic strategyproof mechanisms with a bounded approximation ratio for
2-Facility Location on more general metric spaces, which is true even for
simple instances with 3 agents located in a star."
"Online double auctions (DAs) model a dynamic two-sided matching problem with
private information and self-interest, and are relevant for dynamic resource
and task allocation problems. We present a general method to design truthful
DAs, such that no agent can benefit from misreporting its arrival time,
duration, or value. The family of DAs is parameterized by a pricing rule, and
includes a generalization of McAfee's truthful DA to this dynamic setting. We
present an empirical study, in which we study the allocative-surplus and agent
surplus for a number of different DAs. Our results illustrate that dynamic
pricing rules are important to provide good market efficiency for markets with
high volatility or low volume."
"Correlated equilibrium (Aumann, 1974) generalizes Nash equilibrium to allow
correlation devices. Aumann showed an example of a game, and of a correlated
equilibrium in this game, in which the agents' surplus (expected sum of payo s)
is greater than their surplus in all mixed-strategy equilibria. Following the
idea initiated by the price of anarchy literature (Koutsoupias & Papadimitriou,
1999;Papadimitriou, 2001) this suggests the study of two major measures for the
value of correlation in a game with non-negative payoffs: 1. The ratio between
the maximal surplus obtained in a correlated equilibrium to the maximal surplus
obtained in a mixed-strategy equilibrium. We refer to this ratio as the
mediation value. 2. The ratio between the maximal surplus to the maximal
surplus obtained in a correlated equilibrium. We refer to this ratio as the
enforcement value. In this work we initiate the study of the mediation and
enforcement values, providing several general results on the value of
correlation as captured by these concepts. We also present a set of results for
the more specialized case of congestion games (Rosenthal,1973), a class of
games that received a lot of attention in the recent literature."
"We investigate the complexity of bounding the uncertainty of graphical games,
and we provide new insight into the intrinsic difficulty of computing Nash
equilibria. In particular, we show that, if one adds very simple and natural
additional requirements to a graphical game, the existence of Nash equilibria
is no longer guaranteed, and computing an equilibrium is an intractable
problem. Moreover, if stronger equilibrium conditions are required for the
game, we get hardness results for the second level of the polynomial hierarchy.
Our results offer a clear picture of the complexity of mixed Nash equilibria in
graphical games, and answer some open research questions posed by Conitzer and
Sandholm (2003)."
"Simultaneous ascending auctions present agents with the exposure problem:
bidding to acquire a bundle risks the possibility of obtaining an undesired
subset of the goods. Auction theory provides little guidance for dealing with
this problem. We present a new family of decisiontheoretic bidding strategies
that use probabilistic predictions of final prices. We focus on selfconfirming
price distribution predictions, which by definition turn out to be correct when
all agents bid decision-theoretically based on them. Bidding based on these is
provably not optimal in general, but our experimental evidence indicates the
strategy can be quite effective compared to other known methods."
"One of the proposed solutions to the equilibrium selection problem for agents
learning in repeated games is obtained via the notion of stochastic stability.
Learning algorithms are perturbed so that the Markov chain underlying the
learning dynamics is necessarily irreducible and yields a unique stable
distribution. The stochastically stable distribution is the limit of these
stable distributions as the perturbation rate tends to zero. We present the
first exact algorithm for computing the stochastically stable distribution of a
Markov chain. We use our algorithm to predict the long-term dynamics of simple
learning algorithms in sample repeated games."
"Two-sided matchings are an important theoretical tool used to model markets
and social interactions. In many real life problems the utility of an agent is
influenced not only by their own choices, but also by the choices that other
agents make. Such an influence is called an externality. Whereas fully
expressive representations of externalities in matchings require exponential
space, in this paper we propose a compact model of externalities, in which the
influence of a match on each agent is computed additively. In this framework,
we analyze many-to-many and one-to-one matchings under neutral, optimistic, and
pessimistic behaviour, and provide both computational hardness results and
polynomial-time algorithms for computing stable outcomes."
"This paper describes a study of agent bidding strategies, assuming
combinatorial valuations for complementary and substitutable goods, in three
auction environments: sequential auctions, simultaneous auctions, and the
Trading Agent Competition (TAC) Classic hotel auction design, a hybrid of
sequential and simultaneous auctions. The problem of bidding in sequential
auctions is formulated as an MDP, and it is argued that expected marginal
utility bidding is the optimal bidding policy. The problem of bidding in
simultaneous auctions is formulated as a stochastic program, and it is shown by
example that marginal utility bidding is not an optimal bidding policy, even in
deterministic settings. Two alternative methods of approximating a solution to
this stochastic program are presented: the first method, which relies on
expected values, is optimal in deterministic environments; the second method,
which samples the nondeterministic environment, is asymptotically optimal as
the number of samples tends to infinity. Finally, experiments with these
various bidding policies are described in the TAC Classic setting."
"Action-graph games (AGGs) are a fully expressive game representation which
can compactly express both strict and context-specific independence between
players' utility functions. Actions are represented as nodes in a graph G, and
the payoff to an agent who chose the action s depends only on the numbers of
other agents who chose actions connected to s. We present algorithms for
computing both symmetric and arbitrary equilibria of AGGs using a continuation
method. We analyze the worst-case cost of computing the Jacobian of the payoff
function, the exponential-time bottleneck step, and in all cases achieve
exponential speedup. When the indegree of G is bounded by a constant and the
game is symmetric, the Jacobian can be computed in polynomial time."
"Mechanism design has found considerable application to the construction of
agent-interaction protocols. In the standard setting, the type (e.g., utility
function) of an agent is not known by other agents, nor is it known by the
mechanism designer. When this uncertainty is quantified probabilistically, a
mechanism induces a game of incomplete information among the agents. However,
in many settings, uncertainty over utility functions cannot easily be
quantified. We consider the problem of incomplete information games in which
type uncertainty is strict or unquantified. We propose the use of minimax
regret as a decision criterion in such games, a robust approach for dealing
with type uncertainty. We define minimax-regret equilibria and prove that these
exist in mixed strategies for finite games. We also consider the problem of
mechanism design in this framework by adopting minimax regret as an
optimization criterion for the designer itself, and study automated
optimization of such mechanisms."
"Reasoning about agent preferences on a set of alternatives, and the
aggregation of such preferences into some social ranking is a fundamental issue
in reasoning about uncertainty and multi-agent systems. When the set of agents
and the set of alternatives coincide, we get the so-called reputation systems
setting. Famous types of reputation systems include page ranking in the context
of search engines and traders ranking in the context of e-commerce. In this
paper we present the first axiomatic study of reputation systems. We present
three basic postulates that the desired/aggregated social ranking should
satisfy and prove an impossibility theorem showing that no appropriate social
ranking, satisfying all requirements, exists. Then we show that by relaxing any
of these requirements an appropriate social ranking can be found. We first
study reputation systems with (only) positive feedbacks. This setting refers to
systems where agents' votes are interpreted as indications for the importance
of other agents, as is the case in page ranking. Following this, we discuss the
case of negative feedbacks, a most common situation in e-commerce settings,
where traders may complain about the behavior of others. Finally, we discuss
the case where both positive and negative feedbacks are available."
"We introduce the study of sequential information elicitation in strategic
multi-agent systems. In an information elicitation setup a center attempts to
compute the value of a function based on private information (a-k-a secrets)
accessible to a set of agents. We consider the classical multi-party
computation setup where each agent is interested in knowing the result of the
function. However, in our setting each agent is strategic,and since acquiring
information is costly, an agent may be tempted not spending the efforts of
obtaining the information, free-riding on other agents' computations. A
mechanism which elicits agents' secrets and performs the desired computation
defines a game. A mechanism is 'appropriate' if there exists an equilibrium in
which it is able to elicit (sufficiently many) agents' secrets and perform the
computation, for all possible secret vectors.We characterize a general
efficient procedure for determining an appropriate mechanism, if such mechanism
exists. Moreover, we also address the existence problem, providing a polynomial
algorithm for verifying the existence of an appropriate mechanism."
"We describe an algorithm for computing best response strategies in a class of
two-player infinite games of incomplete information, defined by payoffs
piecewise linear in agents' types and actions, conditional on linear
comparisons of agents' actions. We show that this class includes many
well-known games including a variety of auctions and a novel allocation game.
In some cases, the best-response algorithm can be iterated to compute
Bayes-Nash equilibria. We demonstrate the efficiency of our approach on
existing and new games."
"Wireless local area networks (WLANs) based on IEEE 802.11 standards are
becoming ubiquitous today and typically support multiple data rates. In such
multi-rate WLANs, distributed medium access and rate adaptation are two key
elements to achieve efficient radio resource utilization, especially in
non-cooperative environments. In this paper, we present an analytical study on
the non-cooperative multi-rate WLANs composed of selfish users jointly
adjusting their data rate and contention window size at the medium access level
to maximize their own throughput, irrespective of the impact of their selfish
behaviors on overall system performance. Specifically, we develop an adapted
Tit-For-Tat (TFT) strategy to guide the system to an efficient equilibrium in
non-cooperative environments. We model the interactions among selfish users
under the adapted TFT framework as a non-cooperative joint medium access and
rate adaptation game. A systematic analysis is conducted on the structural
properties of the game to provide insights on the interaction between rate
adaptation and 802.11 medium access control in a competitive setting. We show
that the game has multiple equilibria, which, after the equilibrium refinement
process that we develop, reduce to a unique efficient equilibrium. We further
develop a distributed algorithm to achieve this equilibrium and demonstrate
that the equilibrium achieves the performance very close to the system optimum
in a social perspective."
"In routing games, the network performance at equilibrium can be significantly
improved if we remove some edges from the network. This counterintuitive fact,
widely known as Braess's paradox, gives rise to the (selfish) network design
problem, where we seek to recognize routing games suffering from the paradox,
and to improve the equilibrium performance by edge removal. In this work, we
investigate the computational complexity and the approximability of the network
design problem for non-atomic bottleneck routing games, where the individual
cost of each player is the bottleneck cost of her path, and the social cost is
the bottleneck cost of the network. We first show that bottleneck routing games
do not suffer from Braess's paradox either if the network is series-parallel,
or if we consider only subpath-optimal Nash flows. On the negative side, we
prove that even for games with strictly increasing linear latencies, it is
NP-hard not only to recognize instances suffering from the paradox, but also to
distinguish between instances for which the Price of Anarchy (PoA) can decrease
to 1 and instances for which the PoA is as large as \Omega(n^{0.121}) and
cannot improve by edge removal. Thus, the network design problem for such games
is NP-hard to approximate within a factor of O(n^{0.121-\eps}), for any
constant \eps > 0. On the positive side, we show how to compute an almost
optimal subnetwork w.r.t. the bottleneck cost of its worst Nash flow, when the
worst Nash flow in the best subnetwork routes a non-negligible amount of flow
on all used edges. The running time is determined by the total number of paths,
and is quasipolynomial when the number of paths is quasipolynomial."
"Network creation games model the creation and usage costs of networks formed
by a set of selfish peers. Each peer has the ability to change the network in a
limited way, e.g., by creating or deleting incident links. In doing so, a peer
can reduce its individual communication cost. Typically, these costs are
modeled by the maximum or average distance in the network. We introduce a
generalized version of the basic network creation game (BNCG). In the BNCG (by
Alon et al., SPAA 2010), each peer may replace one of its incident links by a
link to an arbitrary peer. This is done in a selfish way in order to minimize
either the maximum or average distance to all other peers. That is, each peer
works towards a network structure that allows himself to communicate
efficiently with all other peers. However, participants of large networks are
seldom interested in all peers. Rather, they want to communicate efficiently
only with a small subset of peers. Our model incorporates these (communication)
interests explicitly. In the MAX-version, each node tries to minimize its
maximum distance to nodes it is interested in.
  Given peers with interests and a communication network forming a tree, we
prove several results on the structure and quality of equilibria in our model.
For the MAX-version, we give an upper worst case bound of O(\sqrt{n}) for the
private costs in an equilibrium of n peers. Moreover, we give an equilibrium
for a circular interest graph where a node has private cost \Omega(\sqrt{n}),
showing that our bound is tight. This example can be extended such that we get
a tight bound of \Theta(\sqrt{n}) for the price of anarchy. For the case of
general communication networks we show the price of anarchy to be \Theta(n).
Additionally, we prove an interesting connection between a maximum independent
set in the interest graph and the private costs of the peers."
"In this paper we design an incentive mechanism for heterogeneous Delay
Tolerant Networks (DTNs). The proposed mechanism tackles a core problem of such
systems: how to induce coordination of DTN relays in order to achieve a target
performance figure, e.g., delivery probability or end-to-end delay, under a
given constraint in term of network resources, e.g., number of active nodes or
energy consumption. Also, we account for the realistic case when the cost for
taking part in the forwarding process varies with the devices' technology or
the users' habits. Finally, the scheme is truly applicable to DTNs since it
works with no need for end-to-end connectivity.
  In this context, we first introduce the basic coordination mechanism
leveraging the notion of a Minority Game. In this game, relays compete to be in
the population minority and their utility is defined in combination with a
rewarding mechanism. The rewards in turn configure as a control by which the
network operator controls the desired operating point for the DTN. To this aim,
we provide a full characterization of the equilibria of the game in the case of
heterogeneous DTNs. Finally, a learning algorithm based on stochastic
approximations provably drives the system to the equilibrium solution without
requiring perfect state information at relay nodes or at the source node and
without using end-to-end communications to implement the rewarding scheme. We
provide extensive numerical results to validate the proposed scheme."
"A single advertisement often benefits many parties, for example, an ad for a
Samsung laptop benefits Microsoft. We study this phenomenon in search
advertising auctions and show that standard solutions, including the status quo
ignorance of mutual benefit and a benefit-aware Vickrey-Clarke-Groves
mechanism, perform poorly. In contrast, we show that an appropriate first-price
auction has nice equilibria in a single-slot ad auction --- all equilibria that
satisfy a natural cooperative envy-freeness condition select the
welfare-maximizing ad and satisfy an intuitive lower-bound on revenue."
"This article discusses the possibility of predicting human behavior in a
mechanism. Such a mechanism will have certain properties, which are defined and
discussed here. Here it is shown that, unfortunately, certain property
combinations are not possible. The impossibility result implies that either the
such mechanism will not be finite or it would never be fully known. In both
cases such a mechanism is inapplicable to fully predict human behavior."
"In this paper, we study sequential auctions with two budget constrained
bidders and any number of identical items. All prior results on such auctions
consider only two items. We construct a canonical outcome of the auction that
is the only natural equilibrium and is unique under a refinement of subgame
perfect equilibria. We show certain interesting properties of this equilibrium;
for instance, we show that the prices decrease as the auction progresses. This
phenomenon has been observed in many experiments and previous theoretic work
attributed it to features such as uncertainty in the supply or risk averse
bidders. We show that such features are not needed for this phenomenon and that
it arises purely from the most essential features: budget constraints and the
sequential nature of the auction. A little surprisingly we also show that in
this equilibrium one agent wins all his items in the beginning and then the
other agent wins the rest. The major difficulty in analyzing such sequential
auctions has been in understanding how the selling prices of the first few
rounds affect the utilities of the agents in the later rounds. We tackle this
difficulty by identifying certain key properties of the auction and the proof
is via a joint induction on all of them."
"Recent spectrum auctions in the United Kingdom, and some proposals for future
auctions of spectrum in the United States, are based on preliminary price
discovery rounds, followed by calculation of final prices for the winning
buyers. For example, the prices could be the projection of Vikrey prices onto
the core of reported prices. The use of Vikrey prices should lead to more
straightforward bidding, but the projection reverses some of the incentive for
bidders to report truthfully. Still, we conjecture that the price paid by a
winning buyer increases no faster than the bid, as in a first price auction. It
would be rather disturbing if the conjecture is false. The conjecture is
established for a buyer interacting with disjoint groups of other buyers in a
star network setting. It is also shown that for any core-selecting payment rule
and any integer w greater than or equal to two, there is a market setting with
w winning buyers such that the price paid by some winning buyer increases at
least (1-1/w) times as fast as the price bid."
"In mean-payoff games, the objective of the protagonist is to ensure that the
limit average of an infinite sequence of numeric weights is nonnegative. In
energy games, the objective is to ensure that the running sum of weights is
always nonnegative. Multi-mean-payoff and multi-energy games replace individual
weights by tuples, and the limit average (resp. running sum) of each coordinate
must be (resp. remain) nonnegative. These games have applications in the
synthesis of resource-bounded processes with multiple resources.
  We prove the finite-memory determinacy of multi-energy games and show the
inter-reducibility of multimean-payoff and multi-energy games for finite-memory
strategies. We also improve the computational complexity for solving both
classes of games with finite-memory strategies: while the previously best known
upper bound was EXPSPACE, and no lower bound was known, we give an optimal
coNP-complete bound. For memoryless strategies, we show that the problem of
deciding the existence of a winning strategy for the protagonist is
NP-complete. Finally we present the first solution of multi-meanpayoff games
with infinite-memory strategies. We show that multi-mean-payoff games with
mean-payoff-sup objectives can be decided in NP and coNP, whereas
multi-mean-payoff games with mean-payoff-inf objectives are coNP-complete."
"Mechanism design is addressed in the context of fair allocations of
indivisible goods with monetary compensation. Motivated by a real-world social
choice problem, mechanisms with verification are considered in a setting where
(i) agents' declarations on allocated goods can be fully verified before
payments are performed, and where (ii) verification is not used to punish
agents whose declarations resulted in incorrect ones. Within this setting, a
mechanism is designed that is shown to be truthful, efficient, and
budget-balanced, and where agents' utilities are fairly determined by the
Shapley value of suitable coalitional games. The proposed mechanism is however
shown to be #P-complete. Thus, to deal with applications with many agents
involved, two polynomial-time randomized variants are also proposed: one that
is still truthful and efficient, and which is approximately budget-balanced
with high probability, and another one that is truthful in expectation, while
still budget-balanced and efficient."
"Markov decision processes (MDPs) and simple stochastic games (SSGs) provide a
rich mathematical framework to study many important problems related to
probabilistic systems. MDPs and SSGs with finite-horizon objectives, where the
goal is to maximize the probability to reach a target state in a given finite
time, is a classical and well-studied problem. In this work we consider the
strategy complexity of finite-horizon MDPs and SSGs. We show that for all
$\epsilon>0$, the natural class of counter-based strategies require at most
$\log \log (\frac{1}{\epsilon}) + n+1$ memory states, and memory of size
$\Omega(\log \log (\frac{1}{\epsilon}) + n)$ is required. Thus our bounds are
asymptotically optimal. We then study the periodic property of optimal
strategies, and show a sub-exponential lower bound on the period for optimal
strategies."
"Simultaneous item auctions are simple procedures for allocating items to
bidders with potentially complex preferences over different item sets. In a
simultaneous auction, every bidder submits bids on all items simultaneously.
The allocation and prices are then resolved for each item separately, based
solely on the bids submitted on that item. Such procedures occur in practice
(e.g. eBay) but are not truthful. We study the efficiency of Bayesian Nash
equilibrium (BNE) outcomes of simultaneous first- and second-price auctions
when bidders have complement-free (a.k.a. subadditive) valuations. We show that
the expected social welfare of any BNE is at least 1/2 of the optimal social
welfare in the case of first-price auctions, and at least 1/4 in the case of
second-price auctions. These results improve upon the previously-known
logarithmic bounds, which were established by [Hassidim, Kaplan, Mansour and
Nisan '11] for first-price auctions and by [Bhawalkar and Roughgarden '11] for
second-price auctions."
"We study the optimal mechanism design problem faced by a market intermediary
who makes revenue by connecting buyers and sellers. We first show that the
optimal intermediation protocol has substantial structure: it is the solution
to an algorithmic pricing problem in which seller's costs are replaced with
virtual costs, and the sellers' payments need only depend on the buyer's
behavior and not the buyer's actual valuation function.
  Since the underlying algorithmic pricing problem may be difficult to solve
optimally, we study specific models of buyer behavior and give mechanisms with
provable approximation guarantees. We show that offering only the single most
profitable item for sale guarantees an $\Omega(\frac1{\log n})$ fraction of the
optimal revenue when item value distributions are independent and have monotone
hazard rates. We also give constant factor approximations when the buyer
considers all items at once, $k$ items at once, or items in sequence."
"We study individual rational, Pareto optimal, and incentive compatible
mechanisms for auctions with heterogeneous items and budget limits. For
multi-dimensional valuations we show that there can be no deterministic
mechanism with these properties for divisible items. We use this to show that
there can also be no randomized mechanism that achieves this for either
divisible or indivisible items. For single-dimensional valuations we show that
there can be no deterministic mechanism with these properties for indivisible
items, but that there is a randomized mechanism that achieves this for either
divisible or indivisible items. The impossibility results hold for public
budgets, while the mechanism allows private budgets, which is in both cases the
harder variant to show. While all positive results are polynomial-time
algorithms, all negative results hold independent of complexity considerations."
"We characterize methods of dividing a cake between two bidders in a way that
is incentive-compatible and Pareto-efficient. In our cake cutting model, each
bidder desires a subset of the cake (with a uniform value over this subset),
and is allocated some subset. Our characterization proceeds via reducing to a
simple one-dimensional version of the problem, and yields, for example, a tight
bound on the social welfare achievable."
"We consider markets consisting of a set of indivisible items, and buyers that
have {\em sharp} multi-unit demand. This means that each buyer $i$ wants a
specific number $d_i$ of items; a bundle of size less than $d_i$ has no value,
while a bundle of size greater than $d_i$ is worth no more than the most valued
$d_i$ items (valuations being additive). We consider the objective of setting
prices and allocations in order to maximize the total revenue of the market
maker. The pricing problem with sharp multi-unit demand buyers has a number of
properties that the unit-demand model does not possess, and is an important
question in algorithmic pricing. We consider the problem of computing a revenue
maximizing solution for two solution concepts: competitive equilibrium and
envy-free pricing.
  For unrestricted valuations, these problems are NP-complete; we focus on a
realistic special case of ""correlated values"" where each buyer $i$ has a
valuation $v_i\qual_j$ for item $j$, where $v_i$ and $\qual_j$ are positive
quantities associated with buyer $i$ and item $j$ respectively. We present a
polynomial time algorithm to solve the revenue-maximizing competitive
equilibrium problem. For envy-free pricing, if the demand of each buyer is
bounded by a constant, a revenue maximizing solution can be found efficiently;
the general demand case is shown to be NP-hard."
"We analyze the network congestion game with atomic players, asymmetric
strategies, and the maximum latency among all players as social cost. This
important social cost function is much less understood than the average
latency. We show that the price of anarchy is at most two, when the network is
a ring and the link latencies are linear. Our bound is tight. This is the first
sharp bound for the maximum latency objective."
"We obtain revenue guarantees for the simple pricing mechanism of a single
posted price, in terms of a natural parameter of the distribution of buyers'
valuations. Our revenue guarantee applies to the single item n buyers setting,
with values drawn from an arbitrary joint distribution. Specifically, we show
that a single price drawn from the distribution of the maximum valuation Vmax =
max {V_1, V_2, ...,V_n} achieves a revenue of at least a 1/e fraction of the
geometric expecation of Vmax. This generic bound is a measure of how revenue
improves/degrades as a function of the concentration/spread of Vmax.
  We further show that in absence of buyers' valuation distributions,
recruiting an additional set of identical bidders will yield a similar
guarantee on revenue. Finally, our bound also gives a measure of the extent to
which one can simultaneously approximate welfare and revenue in terms of the
concentration/spread of Vmax."
"Auctions for perishable goods such as internet ad inventory need to make
real-time allocation and pricing decisions as the supply of the good arrives in
an online manner, without knowing the entire supply in advance. These
allocation and pricing decisions get complicated when buyers have some global
constraints. In this work, we consider a multi-unit model where buyers have
global {\em budget} constraints, and the supply arrives in an online manner.
Our main contribution is to show that for this setting there is an
individually-rational, incentive-compatible and Pareto-optimal auction that
allocates these units and calculates prices on the fly, without knowledge of
the total supply. We do so by showing that the Adaptive Clinching Auction
satisfies a {\em supply-monotonicity} property.
  We also analyze and discuss, using examples, how the insights gained by the
allocation and payment rule can be applied to design better ad allocation
heuristics in practice. Finally, while our main technical result concerns
multi-unit supply, we propose a formal model of online supply that captures
scenarios beyond multi-unit supply and has applications to sponsored search. We
conjecture that our results for multi-unit auctions can be extended to these
more general models."
"Flow scheduling tends to be one of the oldest and most stubborn problems in
networking. It becomes more crucial in the next generation network, due to fast
changing link states and tremendous cost to explore the global structure. In
such situation, distributed algorithms often dominate. In this paper, we design
a distributed virtual game to solve the flow scheduling problem and then
generalize it to situations of unknown environment, where online learning
schemes are utilized. In the virtual game, we use incentives to stimulate
selfish users to reach a Nash Equilibrium Point which is valid based on the
analysis of the `Price of Anarchy'. In the unknown-environment generalization,
our ultimate goal is the minimization of cost in the long run. In order to
achieve balance between exploration of routing cost and exploitation based on
limited information, we model this problem based on Multi-armed Bandit Scenario
and combined newly proposed DSEE with the virtual game design. Armed with these
powerful tools, we find a totally distributed algorithm to ensure the
logarithmic growing of regret with time, which is optimum in classic
Multi-armed Bandit Problem. Theoretical proof and simulation results both
affirm this claim. To our knowledge, this is the first research to combine
multi-armed bandit with distributed flow scheduling."
"We consider a simple simultaneous first price auction for multiple items in a
complete information setting. Our goal is to completely characterize the mixed
equilibria in this setting, for a simple, yet highly interesting, {\tt
AND}-{\tt OR} game, where one agent is single minded and the other is unit
demand."
"We continue the investigation of finite-duration variants of
infinite-duration games by extending known results for games played on finite
graphs to those played on infinite ones. In particular, we establish an
equivalence between pushdown parity games and a finite-duration variant. This
allows us to determine the winner of a pushdown parity game by solving a
reachability game on a finite tree."
"We provide a Polynomial Time Approximation Scheme (PTAS) for the Bayesian
optimal multi-item multi-bidder auction problem under two conditions. First,
bidders are independent, have additive valuations and are from the same
population. Second, every bidder's value distributions of items are independent
but not necessarily identical monotone hazard rate (MHR) distributions. For
non-i.i.d. bidders, we also provide a PTAS when the number of bidders is small.
Prior to our work, even for a single bidder, only constant factor
approximations are known.
  Another appealing feature of our mechanism is the simple allocation rule.
Indeed, the mechanism we use is either the second-price auction with reserve
price on every item individually, or VCG allocation with a few outlying items
that requires additional treatments. It is surprising that such simple
allocation rules suffice to obtain nearly optimal revenue."
"We present a novel coalgebraic formulation of infinite extensive games. We
define both the game trees and the strategy profiles by possibly infinite
systems of corecursive equations. Certain strategy profiles are proved to be
subgame perfect equilibria using a novel proof principle of predicate
coinduction. We characterize all subgame perfect equilibria for the dollar
auction game. The economically interesting feature is that in order to prove
these results we do not need to rely on continuity assumptions on the payoffs
which amount to discounting the future. In particular, we prove a form of
one-deviation principle without any such assumptions. This suggests that
coalgebra supports a more adequate treatment of infinite-horizon models in game
theory and economics."
"We consider the budget optimization problem faced by an advertiser
participating in repeated sponsored search auctions, seeking to maximize the
number of clicks attained under that budget. We cast the budget optimization
problem as a Markov Decision Process (MDP) with censored observations, and
propose a learning algorithm based on the wellknown Kaplan-Meier or
product-limit estimator. We validate the performance of this algorithm by
comparing it to several others on a large set of search auction data from
Microsoft adCenter, demonstrating fast convergence to optimal performance."
"Computing a Nash equilibrium (NE) is a central task in computer science. An
NE is a particularly appropriate solution concept for two-agent settings
because coalitional deviations are not an issue. However, even in this case,
finding an NE is PPAD-complete. In this paper, we combine path following
algorithms with local search techniques to design new algorithms for finding
exact and approximate NEs. We show that our algorithms largely outperform the
state of the art and that almost all the known benchmark game classes are
easily solvable or approximable (except for the GAMUT CovariantGameRand class)."
"Most analyses of manipulation of voting schemes have adopted two assumptions
that greatly diminish their practical import. First, it is usually assumed that
the manipulators have full knowledge of the votes of the nonmanipulating
agents. Second, analysis tends to focus on the probability of manipulation
rather than its impact on the social choice objective (e.g., social welfare).
We relax both of these assumptions by analyzing optimal Bayesian manipulation
strategies when the manipulators have only partial probabilistic information
about nonmanipulator votes, and assessing the expected loss in social welfare
(in the broad sense of the term). We present a general optimization framework
for the derivation of optimal manipulation strategies given arbitrary voting
rules and distributions over preferences. We theoretically and empirically
analyze the optimal manipulability of some popular voting rules using
distributions and real data sets that go well beyond the common, but
unrealistic, impartial culture assumption. We also shed light on the stark
difference between the loss in social welfare and the probability of
manipulation by showing that even when manipulation is likely, impact to social
welfare is slight (and often negligible)."
"Bidding in simultaneous auctions is challenging because an agent's value for
a good in one auction may depend on the uncertain outcome of other auctions:
the so-called exposure problem. Given the gap in understanding of general
simultaneous auction games, previous works have tackled this problem with
heuristic strategies that employ probabilistic price predictions. We define a
concept of self-confirming prices, and show that within an independent private
value model, Bayes-Nash equilibrium can be fully characterized as a profile of
optimal price prediction strategies with self-confirming predictions. We
exhibit practical procedures to compute approximately optimal bids given a
probabilistic price prediction, and near self-confirming price predictions
given a price-prediction strategy. An extensive empirical game-theoretic
analysis demonstrates that self-confirming price prediction strategies are
effective in simultaneous auction games with both complementary and
substitutable preference structures."
"A double auction game with an infinite number of buyers and sellers is
introduced. All sellers posses one unit of a good, all buyers desire to buy one
unit. Each seller and each buyer has a private valuation of the good. The
distribution of the valuations define supply and demand functions. One unit of
the good is auctioned. At successive, discrete time instances, a player is
randomly selected to make a bid (buyer) or an ask (seller). When the maximum of
the bids becomes larger than the minimum of the asks, a transaction occurs and
the auction is closed. The players have to choose the value of their bid or ask
before the auction starts and use this value when they are selected. Assuming
that the supply and demand functions are known, expected profits as functions
of the strategies are derived, as well as expected transaction prices. It is
shown that for linear supply and demand functions, there exists at most one
Bayesian Nash equilibrium. Competitive behaviour is not an equilibrium of the
game. For linear supply and demand functions, the sum of the expected profit of
the sellers and the buyers is the same for the Bayesian Nash equilibrium and
the market where players behave competitively. Connections are made with the
ZI-C traders model and the $k$-double auction."
"In auction theory, cryptography has been used to achieve anonymity of the
participants, security and privacy of the bids, secure computation and to
simulate mediator (auctioneer). Auction theory focuses on revenue and
Cryptography focuses on security and privacy. Involving Cryptography at base
level, to enhance revenue gives entirely new perspective and insight to Auction
theory, thereby achieving the core goals of auction theory. In this report, we
try to investigate an interesting field of study in Auction Theory using
Cryptographic primitives."
"We consider turn-based game arenas for which we investigate uniformity
properties of strategies. These properties involve bundles of plays, that arise
from some semantical motive. Typically, we can represent constraints on allowed
strategies, such as being observation-based. We propose a formal language to
specify uniformity properties and demonstrate its relevance by rephrasing
various known problems from the literature. Note that the ability to correlate
different plays cannot be achieved by any branching-time logic if not equipped
with an additional modality, so-called R in this contribution. We also study an
automated procedure to synthesize strategies subject to a uniformity property,
which strictly extends existing results based on, say standard temporal logics.
We exhibit a generic solution for the synthesis problem provided the bundles of
plays rely on any binary relation definable by a finite state transducer. This
solution yields a non-elementary procedure."
"We consider a classical problem in choice theory -- vote aggregation -- using
novel distance measures between permutations that arise in several practical
applications. The distance measures are derived through an axiomatic approach,
taking into account various issues arising in voting with side constraints. The
side constraints of interest include non-uniform relevance of the top and the
bottom of rankings (or equivalently, eliminating negative outliers in votes)
and similarities between candidates (or equivalently, introducing diversity in
the voting process). The proposed distance functions may be seen as weighted
versions of the Kendall $\tau$ distance and weighted versions of the Cayley
distance. In addition to proposing the distance measures and providing the
theoretical underpinnings for their applications, we also consider algorithmic
aspects associated with distance-based aggregation processes. We focus on two
methods. One method is based on approximating weighted distance measures by a
generalized version of Spearman's footrule distance, and it has provable
constant approximation guarantees. The second class of algorithms is based on a
non-uniform Markov chain method inspired by PageRank, for which currently only
heuristic guarantees are known. We illustrate the performance of the proposed
algorithms for a number of distance measures for which the optimal solution may
be easily computed."
"Decentralized resource allocation is a key problem for large-scale autonomic
(or self-managing) computing systems. Motivated by a data center scenario, we
explore efficient techniques for resolving resource conflicts via cooperative
negotiation. Rather than computing in advance the functional dependence of each
element's utility upon the amount of resource it receives, which could be
prohibitively expensive, each element's utility is elicited incrementally. Such
incremental utility elicitation strategies require the evaluation of only a
small set of sampled utility function points, yet they find near-optimal
allocations with respect to a minimax regret criterion. We describe preliminary
computational experiments that illustrate the benefit of our approach."
"We construct prior-free auctions with constant-factor approximation
guarantees with ordered bidders, in both unlimited and limited supply settings.
We compare the expected revenue of our auctions on a bid vector to the monotone
price benchmark, the maximum revenue that can be obtained from a bid vector
using supply-respecting prices that are nonincreasing in the bidder ordering
and bounded above by the second-highest bid. As a consequence, our auctions are
simultaneously near-optimal in a wide range of Bayesian multi-unit
environments."
"We consider the provision of an abstract service to single-dimensional
agents. Our model includes position auctions, single-minded combinatorial
auctions, and constrained matching markets. When the agents' values are drawn
from a distribution, the Bayesian optimal mechanism is given by Myerson (1981)
as a virtual-surplus optimizer. We develop a framework for prior-free mechanism
design and analysis. A good mechanism in our framework approximates the optimal
mechanism for the distribution if there is a distribution; moreover, when there
is no distribution this mechanism still performs well.
  We define and characterize optimal envy-free outcomes in symmetric
single-dimensional environments. Our characterization mirrors Myerson's theory.
Furthermore, unlike in mechanism design where there is no point-wise optimal
mechanism, there is always a point-wise optimal envy-free outcome.
  Envy-free outcomes and incentive-compatible mechanisms are similar in
structure and performance. We therefore use the optimal envy-free revenue as a
benchmark for measuring the performance of a prior-free mechanism. A good
mechanism is one that approximates the envy free benchmark on any profile of
agent values. We show that good mechanisms exist, and in particular, a natural
generalization of the random sampling auction of Goldberg et al. (2001) is a
constant approximation."
"We consider prior-free auctions for revenue and welfare maximization when
agents have a common budget. The abstract environments we consider are ones
where there is a downward-closed and symmetric feasibility constraint on the
probabilities of service of the agents. These environments include position
auctions where slots with decreasing click-through rates are auctioned to
advertisers. We generalize and characterize the envy-free benchmark from
Hartline and Yan (2011) to settings with budgets and characterize the optimal
envy-free outcomes for both welfare and revenue. We give prior-free mechanisms
that approximate these benchmarks. A building block in our mechanism is a
clinching auction for position auction environments. This auction is a
generalization of the multi-unit clinching auction of Dobzinski et al. (2008)
and a special case of the polyhedral clinching auction of Goel et al. (2012).
For welfare maximization, we show that this clinching auction is a good
approximation to the envy-free optimal welfare for position auction
environments. For profit maximization, we generalize the random sampling profit
extraction auction from Fiat et al. (2002) for digital goods to give a
10.0-approximation to the envy-free optimal revenue in symmetric,
downward-closed environments. The profit maximization question is of interest
even without budgets and our mechanism is a 7.5-approximation which improving
on the 30.4 bound of Ha and Hartline (2012)."
"Exploiting the algebraic structure of the set of bimatrix games, a
divide-and-conquer algorithm for finding Nash equilibria is proposed. The
algorithm is fixed-parameter tractable with the size of the largest irreducible
component of a game as parameter. An implementation of the algorithm is shown
to yield a significant performance increase on inputs with small parameters."
"We study a simple scheduling game for the speed scaling model. Players want
their job to complete early, which however generates a big energy consumption.
We address the game from the mechanism design side, and by charging the energy
usage to the players we seek for a good compromize between quality of service
and energy usage."
"We consider infinite duration alternating move games. These games were
previously studied by Roth, Balcan, Kalai and Mansour. They presented an FPTAS
for computing an approximated equilibrium, and conjectured that there is a
polynomial algorithm for finding an exact equilibrium. We extend their study in
two directions: (1) We show that finding an exact equilibrium, even for
two-player zero-sum games, is polynomial time equivalent to finding a winning
strategy for a (two-player) mean-payoff game on graphs. The existence of a
polynomial algorithm for the latter is a long standing open question in
computer science. Our hardness result for two-player games suggests that
two-player alternating move games are harder to solve than two-player
simultaneous move games, while the work of Roth et al., suggests that for
$k\geq 3$, $k$-player games are easier to analyze in the alternating move
setting. (2) We show that optimal equilibriums (with respect to the social
welfare metric) can be obtained by pure strategies, and we present an FPTAS for
computing a pure approximated equilibrium that is $\delta$-optimal with respect
to the social welfare metric. This result extends the previous work by
presenting an FPTAS that finds a much more desirable approximated equilibrium.
We also show that if there is a polynomial algorithm for mean-payoff games on
graphs, then there is a polynomial algorithm that computes an optimal exact
equilibrium, and hence, (two-player) mean-payoff games on graphs are
inter-reducible with $k$-player alternating move games, for any $k\geq 2$."
"We use Hotelling's spatial model of competition to investigate the
position-taking behaviour of political candidates under a class of electoral
systems known as scoring rules. In a scoring rule election, voters rank all the
candidates running for office, following which the candidates are assigned
points according to a vector of nonincreasing scores. Convergent Nash
equilibria in which all candidates adopt the same policy were characterised by
Cox (1987). Here, we investigate nonconvergent equilibria, where candidates
adopt divergent policies. We identify a number of classes of scoring rules
exhibiting a range of different equilibrium properties. For some of these,
nonconvergent equilibria do not exist. For others, nonconvergent equilibria in
which candidates cluster at positions spread across the issue space are
observed. In particular, we prove that the class of convex rules does not have
Nash equilibria (convergent or nonconvergent) with the exception of some
derivatives of Borda rule. Finally, we examine the special cases of four-,
five- and six- candidate elections. In the former two cases, we provide a
complete characterisation of nonconvergent equilibria."
"We study simple and approximately optimal auctions for agents with a
particular form of risk-averse preferences. We show that, for symmetric agents,
the optimal revenue (given a prior distribution over the agent preferences) can
be approximated by the first-price auction (which is prior independent), and,
for asymmetric agents, the optimal revenue can be approximated by an auction
with simple form. These results are based on two technical methods. The first
is for upper-bounding the revenue from a risk-averse agent. The second gives a
payment identity for mechanisms with pay-your-bid semantics."
"We introduce the notion of fault tolerant mechanism design, which extends the
standard game theoretic framework of mechanism design to allow for uncertainty
about execution. Specifically, we define the problem of task allocation in
which the private information of the agents is not only their costs to attempt
the tasks, but also their probabilities of failure. For several different
instances of this setting we present technical results, including positive ones
in the form of mechanisms that are incentive compatible, individually rational
and efficient, and negative ones in the form of impossibility theorems."
"We present a family of submodular valuation classes that generalizes gross
substitute. We show that Walrasian equilibrium always exist for one class in
this family, and there is a natural ascending auction which finds it. We prove
some new structural properties on gross-substitute auctions which, in turn,
show that the known ascending auctions for this class (Gul-Stacchetti and
Ausbel) are, in fact, identical. We generalize these two auctions, and provide
a simple proof that they terminate in a Walrasian equilibrium."
"There are many situations in which mis-coordinated strategic voting can leave
strategic voters worse off than they would have been had they not tried to
strategize. We analyse the simplest of such scenarios, in which the set of
strategic voters all have the same sincere preferences and all cast the same
strategic vote, while all other voters vote sincerely. Most mis-coordinations
in this framework can be classified as instances of either strategic
overshooting (too many voted strategically) or strategic undershooting (too
few). If mis-coordination can result in strategic voters ending up worse off
than they would have been had they all just voted sincerely, we call the
relevant strategic vote unsafe. We show that under every onto and
non-dictatorial social choice rule there exist circumstances where a voter has
an incentive to cast a safe strategic vote. We extend the Gibbard-Satterthwaite
Theorem by proving that every onto and non-dictatorial social choice rule can
be individually manipulated by a voter casting a safe strategic vote."
"In this paper, we develop a new method for finding an optimal biddingstrategy
in sequential auctions, using a dynamic programming technique. Theexisting
method assumes that the utility of a user is represented in anadditive form.
Thus, the remaining endowment of money must be explicitlyrepresented in each
state, and the calculation of the optimal biddingstrategy becomes
time-consuming when the initial endowment of money mbecomes large.In this
paper, we develop a new problem formalization that avoids
explicitlyrepresenting the remaining endowment, by assuming the utility of a
user canbe represented in a quasi-linear form, and representing the payment as
astate-transition cost. Experimental evaluations show that we can obtainmore
than an m-fold speed-up in the computation time. Furthermore, we havedeveloped
a method for obtaining a semi-optimal bidding strategy underbudget constraints,
and have experimentally confirmed the efficacy of thismethod."
"We study two-player zero-sum games over infinite-state graphs with
boundedness conditions. Our first contribution is about the strategy
complexity, i.e the memory required for winning strategies: we prove that over
general infinite-state graphs, memoryless strategies are sufficient for
finitary B\""uchi games, and finite-memory suffices for finitary parity games.
We then study pushdown boundedness games, with two contributions. First we
prove a collapse result for pushdown omega B games, implying the decidability
of solving these games. Second we consider pushdown games with finitary parity
along with stack boundedness conditions, and show that solving these games is
EXPTIME-complete."
"In two player bi-matrix games with partial monitoring, actions played are not
observed, only some messages are received. Those games satisfy a crucial
property of usual bi-matrix games: there are only a finite number of required
(mixed) best replies. This is very helpful while investigating sets of Nash
equilibria: for instance, in some cases, it allows to relate it to the set of
equilibria of some auxiliary game with full monitoring. In the general case,
the Lemke-Howson algorithm is extended and, under some genericity assumption,
its output are Nash equilibria of the original game. As a by product, we obtain
an oddness property on their number."
"Blackwell approachability, regret minimization and calibration are three
criteria evaluating a strategy (or an algorithm) in different sequential
decision problems, or repeated games between a player and Nature. Although they
have at first sight nothing in common, links between have been discovered: both
consistent and calibrated strategies can be constructed by following, in some
auxiliary game, an approachability strategy. We gathered famous or recent
results and provide new ones in order to develop and generalize Blackwell's
elegant theory. The final goal is to show how it can be used as a basic
powerful tool to exhibit a new class of intuitive algorithms, based on simple
geometric properties. In order to be complete, we also prove that
approachability can be seen as a byproduct of the very existence of consistent
or calibrated strategies."
"Multi-agent games are becoming an increasing prevalent formalism for the
study of electronic commerce and auctions. The speed at which transactions can
take place and the growing complexity of electronic marketplaces makes the
study of computationally simple agents an appealing direction. In this work, we
analyze the behavior of agents that incrementally adapt their strategy through
gradient ascent on expected payoff, in the simple setting of two-player,
two-action, iterated general-sum games, and present a surprising result. We
show that either the agents will converge to Nash equilibrium, or if the
strategies themselves do not converge, then their average payoffs will
nevertheless converge to the payoffs of a Nash equilibrium."
"In a matroid secretary problem, one is presented with a sequence of objects
of various weights in a random order, and must choose irrevocably to accept or
reject each item. There is a further constraint that the set of items selected
must form an independent set of an associated matroid. Constant-competitive
algorithms (algorithms whose expected solution weight is within a constant
factor of the optimal) are known for many types of matroid secretary problems.
We examine the laminar matroid and show an algorithm achieving provably 0.053
competitive ratio."
"Organizations consist of individuals connected by their responsibilities,
incentives, and reporting structure. These connections are aptly represented by
a network, hierarchical or other, which is often used to divide tasks. A
primary goal of the organization as a whole is to maximize the net productive
output. Individuals in these networks trade off between their productive and
managing efforts to perform these tasks and the trade-off is influenced by
their positions and share of rewards in the network. Efforts of the agents here
are substitutable, e.g., the increase in the productive effort by an individual
in effect reduces the same of some other individual in the network, who now
puts their efforts into management. The management effort of an agent improves
the productivity of certain other agents in the network.
  In this paper, we carry out a detailed game-theoretic analysis of
individual's equilibrium split of efforts into multiple components when
connected over a network. We provide a design recipe of the reward sharing
scheme that maximizes the net productive output. Our results show that under
the strategic behavior of the agents, it may not always be possible to achieve
the optimal output using an idea from game theory called the price of anarchy."
"A recent body of experimental literature has studied empirical
game-theoretical analysis, in which we have partial knowledge of a game,
consisting of observations of a subset of the pure-strategy profiles and their
associated payoffs to players. The aim is to find an exact or approximate Nash
equilibrium of the game, based on these observations. It is usually assumed
that the strategy profiles may be chosen in an on-line manner by the algorithm.
We study a corresponding computational learning model, and the query complexity
of learning equilibria for various classes of games. We give basic results for
bimatrix and graphical games. Our focus is on symmetric network congestion
games. For directed acyclic networks, we can learn the cost functions (and
hence compute an equilibrium) while querying just a small fraction of
pure-strategy profiles. For the special case of parallel links, we have the
stronger result that an equilibrium can be identified while only learning a
small fraction of the cost values."
"In two-sided matching markets, the agents are partitioned into two sets. Each
agent wishes to be matched to an agent in the other set and has a strict
preference over these potential matches. A matching is stable if there are no
blocking pairs, i.e., no pair of agents that prefer each other to their
assigned matches. In this paper we study a variant of stable matching motivated
by the fact that, in most centralized markets, many agents do not have direct
communication with each other. Hence even if some blocking pairs exist, the
agents involved in those pairs may not be able to coordinate a deviation. We
model communication channels with a bipartite graph between the two sets of
agents which we call the social graph, and we study socially stable matchings.
A matching is socially stable if there are no blocking pairs that are connected
by an edge in the social graph. Socially stable matchings vary in size and so
we look for a maximum socially stable matching. We prove that this problem is
NP-hard and, assuming the unique games conjecture, hard to approximate within a
factor of 3/2-{\epsilon}, for any constant {\epsilon}>0. We complement the
hardness results with a 3/2-approximation algorithm."
"We study the problem of computing approximate Nash equilibria of bimatrix
games, in a setting where players initially know their own payoffs but not the
payoffs of the other player. In order for a solution of reasonable quality to
be found, some amount of communication needs to take place between the players.
We are interested in algorithms where the communication is substantially less
than the contents of a payoff matrix, for example logarithmic in the size of
the matrix. When the communication is polylogarithmic in the number of
strategies n, we show how to obtain epsilon-approximate Nash equilibria for
epsilon approximately 0.438, and for well-supported approximate equilibria we
obtain epsilon approximately 0.732. For one-way communication we show that
epsilon=1/2 is achievable, but no constant improvement over 1/2 is possible,
even with unlimited one-way communication. For well-supported equilibria, no
value of epsilon less than 1 is achievable with one-way communication. When the
players do not communicate at all, epsilon-Nash equilibria can be obtained for
epsilon=3/4, and we also give a lower bound of slightly more than 1/2 on the
lowest constant epsilon achievable."
"Schelling's model of segregation looks to explain the way in which particles
or agents of two types may come to arrange themselves spatially into
configurations consisting of large homogeneous clusters, i.e.\ connected
regions consisting of only one type. As one of the earliest agent based models
studied by economists and perhaps the most famous model of self-organising
behaviour, it also has direct links to areas at the interface between computer
science and statistical mechanics, such as the Ising model and the study of
contagion and cascading phenomena in networks.
  While the model has been extensively studied it has largely resisted rigorous
analysis, prior results from the literature generally pertaining to variants of
the model which are tweaked so as to be amenable to standard techniques from
statistical mechanics or stochastic evolutionary game theory. In \cite{BK},
Brandt, Immorlica, Kamath and Kleinberg provided the first rigorous analysis of
the unperturbed model, for a specific set of input parameters. Here we provide
a rigorous analysis of the model's behaviour much more generally and establish
some surprising forms of threshold behaviour, notably the existence of
situations where an \emph{increased} level of intolerance for neighbouring
agents of opposite type leads almost certainly to \emph{decreased} segregation."
"In this paper we show that payment computation essentially does not present
any obstacle in designing truthful mechanisms, even for multi-parameter
domains, and even when we can only call the allocation rule once. We present a
general reduction that takes any allocation rule which satisfies ""cyclic
monotonicity"" (a known necessary and sufficient condition for truthfulness) and
converts it to a truthful mechanism using a single call to the allocation rule,
with arbitrarily small loss to the expected social welfare.
  A prominent example for a multi-parameter setting in which an allocation rule
can only be called once arises in sponsored search auctions. These are
multi-parameter domains when each advertiser has multiple possible ads he may
display, each with a different value per click. Moreover, the mechanism
typically does not have complete knowledge of the click-realization or the
click-through rates (CTRs); it can only call the allocation rule a single time
and observe the click information for ads that were presented. % are not known.
On the negative side, we show that an allocation that is truthful for any
realization essentially cannot depend on the bids, and hence cannot do better
than random selection for one agent. We then consider a relaxed requirement of
truthfulness, only in expectation over the CTRs. Even for that relaxed version,
making any progress is challenging as standard techniques for construction of
truthful mechanisms (as using VCG or an MIDR allocation rule) cannot be used in
this setting. We design an allocation rule with non-trivial performance and
directly prove it is cyclic-monotone, and thus it can be used to create a
truthful mechanism using our general reduction."
"We study probabilistic single-item second-price auctions where the item is
characterized by a set of attributes. The auctioneer knows the actual
instantiation of all the attributes, but he may choose to reveal only a subset
of these attributes to the bidders. Our model is an abstraction of the
following Ad auction scenario. The website (auctioneer) knows the demographic
information of its impressions, and this information is in terms of a list of
attributes (e.g., age, gender, country of location). The website may hide
certain attributes from its advertisers (bidders) in order to create thicker
market, which may lead to higher revenue. We study how to hide attributes in an
optimal way. We show that it is NP-hard to solve for the optimal attribute
hiding scheme. We then derive a polynomial-time solvable upper bound on the
optimal revenue. Finally, we propose two heuristic-based attribute hiding
schemes. Experiments show that revenue achieved by these schemes is close to
the upper bound."
"In the classical experimental design setting, an experimenter E has access to
a population of $n$ potential experiment subjects $i\in \{1,...,n\}$, each
associated with a vector of features $x_i\in R^d$. Conducting an experiment
with subject $i$ reveals an unknown value $y_i\in R$ to E. E typically assumes
some hypothetical relationship between $x_i$'s and $y_i$'s, e.g., $y_i \approx
\beta x_i$, and estimates $\beta$ from experiments, e.g., through linear
regression. As a proxy for various practical constraints, E may select only a
subset of subjects on which to conduct the experiment.
  We initiate the study of budgeted mechanisms for experimental design. In this
setting, E has a budget $B$. Each subject $i$ declares an associated cost $c_i
>0$ to be part of the experiment, and must be paid at least her cost. In
particular, the Experimental Design Problem (EDP) is to find a set $S$ of
subjects for the experiment that maximizes $V(S) = \log\det(I_d+\sum_{i\in
S}x_i\T{x_i})$ under the constraint $\sum_{i\in S}c_i\leq B$; our objective
function corresponds to the information gain in parameter $\beta$ that is
learned through linear regression methods, and is related to the so-called
$D$-optimality criterion. Further, the subjects are strategic and may lie about
their costs.
  We present a deterministic, polynomial time, budget feasible mechanism
scheme, that is approximately truthful and yields a constant factor
approximation to EDP. In particular, for any small $\delta > 0$ and $\epsilon >
0$, we can construct a (12.98, $\epsilon$)-approximate mechanism that is
$\delta$-truthful and runs in polynomial time in both $n$ and
$\log\log\frac{B}{\epsilon\delta}$. We also establish that no truthful,
budget-feasible algorithms is possible within a factor 2 approximation, and
show how to generalize our approach to a wide class of learning problems,
beyond linear regression."
"We study how standard auction objectives in sponsored search markets change
with refinements in the prediction of the relevance (click-through rates) of
ads. We study mechanisms that optimize for a convex combination of efficiency
and revenue. We show that the objective function of such a mechanism can only
improve with refined (improved) relevance predictions, i.e., the search engine
has no disincentive to perform these refinements. More interestingly, we show
that under assumptions, refinements to relevance predictions can only improve
the efficiency of any such mechanism. Our main technical contribution is to
study how relevance refinements affect the similarity between ranking by
virtual-value (revenue ranking) and ranking by value (efficiency ranking).
Finally, we discuss implications of our results to the literature on signaling."
"Distributed decision-makers are modeled as players in a game with two levels.
High level decisions concern the game environment and determine the willingness
of the players to form a coalition (or group). Low level decisions involve the
actions to be implemented within the chosen environment. Coalition and action
strategies are determined by probability distributions, which are updated using
learning automata schemes. The payoffs are also probabilistic and there is
uncertainty in the state vector since information is delayed. The goal is to
reach equilibrium in both levels of decision making; the results show the
conditions for instability, based on the age of information."
"In the field of computational game theory, games are often compared in terms
of their size. This can be measured in several ways, including the number of
unique game states, the number of decision points, and the total number of
legal actions over all decision points. These numbers are either known or
estimated for a wide range of classic games such as chess and checkers. In the
stochastic and imperfect information game of poker, these sizes are easily
computed in ""limit"" games which restrict the players' available actions, but
until now had only been estimated for the more complicated ""no-limit"" variants.
In this paper, we describe a simple algorithm for quickly computing the size of
two-player no-limit poker games, provide an implementation of this algorithm,
and present for the first time precise counts of the number of game states,
information sets, actions and terminal nodes in the no-limit poker games played
in the Annual Computer Poker Competition."
"Cooperative transmission in vehicular networks is studied by using
coalitional game and pricing in this paper. There are several vehicles and
roadside units (RSUs) in the networks. Each vehicle has a desire to transmit
with a certain probability, which represents its data burtiness. The RSUs can
enhance the vehicles' transmissions by cooperatively relaying the vehicles'
data. We consider two kinds of cooperations: cooperation among the vehicles and
cooperation between the vehicle and RSU. First, vehicles cooperate to avoid
interfering transmissions by scheduling the transmissions of the vehicles in
each coalition. Second, a RSU can join some coalition to cooperate the
transmissions of the vehicles in that coalition. Moreover, due to the mobility
of the vehicles, we introduce the notion of encounter between the vehicle and
RSU to indicate the availability of the relay in space. To stimulate the RSU's
cooperative relaying for the vehicles, the pricing mechanism is applied. A
non-transferable utility (NTU) game is developed to analyze the behaviors of
the vehicles and RSUs. The stability of the formulated game is studied.
Finally, we present and discuss the numerical results for the 2-vehicle and
2-RSU scenario, and the numerical results verify the theoretical analysis."
"Crowdsourcing is now widely used to replace judgement by an expert authority
with an aggregate evaluation from a number of non-experts, in applications
ranging from rating and categorizing online content to evaluation of student
assignments in massively open online courses via peer grading. A key issue in
these settings, where direct monitoring is infeasible, is incentivizing agents
in the `crowd' to put in effort to make good evaluations, as well as to
truthfully report their evaluations. This leads to a new family of information
elicitation problems with unobservable ground truth, where an agent's
proficiency- the probability with which she correctly evaluates the underlying
ground truth- is endogenously determined by her strategic choice of how much
effort to put into the task.
  Our main contribution is a simple, new, mechanism for binary information
elicitation for multiple tasks when agents have endogenous proficiencies, with
the following properties: (i) Exerting maximum effort followed by truthful
reporting of observations is a Nash equilibrium. (ii) This is the equilibrium
with maximum payoff to all agents, even when agents have different maximum
proficiencies, can use mixed strategies, and can choose a different strategy
for each of their tasks. Our information elicitation mechanism requires only
minimal bounds on the priors, asks agents to only report their own evaluations,
and does not require any conditions on a diverging number of agent reports per
task to achieve its incentive properties. The main idea behind our mechanism is
to use the presence of multiple tasks and ratings to identify and penalize
low-effort agreement: the mechanism rewards agents for agreeing with a
`reference' rater on a task but also penalizes for blind agreement by
subtracting out a statistic term designed so that agents obtain reward only
when they put effort into their observations."
"Interdependent-choice equilibrium is defined as an extension of correlated
equilibrium in which the mediator is able to choose the timing of her signals,
and observe the actions taken by the players. The set of interdependent-choice
equilibria is a nonempty, closed and convex polytope. It characterizes all the
outcomes that can be implemented in single shot interactions without
repetition, side payments, binding contracts or any other form of delegation."
"This paper addresses the matter of inequality in network formation games. We
employ a quantity that we are calling the Nash Inequality Ratio (NIR), defined
as the maximal ratio between the highest and lowest costs incurred to
individual agents in a Nash equilibrium strategy, to characterize the extent to
which inequality is possible in equilibrium. We give tight upper bounds on the
NIR for the network formation games of Fabrikant et al. (PODC '03) and Ehsani
et al. (SPAA '11). With respect to the relationship between equality and social
efficiency, we show that, contrary to common expectations, efficiency does not
necessarily come at the expense of increased inequality."
"We study two standard multi-unit auction formats for allocating multiple
units of a single good to multi-demand bidders. The first one is the
Discriminatory Auction, which charges every winner his winning bids. The second
is the Uniform Price Auction, which determines a uniform price to be paid per
unit. Variants of both formats find applications ranging from the allocation of
state bonds to investors, to online sales over the internet, facilitated by
popular online brokers. For these formats, we consider two bidding interfaces:
(i) standard bidding, which is most prevalent in the scientific literature, and
(ii) uniform bidding, which is more popular in practice. In this work, we
evaluate the economic inefficiency of both multi-unit auction formats for both
bidding interfaces, by means of upper and lower bounds on the Price of Anarchy
for pure Nash equilibria and mixed Bayes-Nash equilibria. Our developments
improve significantly upon bounds that have been obtained recently in
[Markakis, Telelis, ToCS 2014] and [Syrgkanis, Tardos, STOC 2013] for
submodular valuation functions. Moreover, we consider for the first time
bidders with subadditive valuation functions for these auction formats. Our
results signify that these auctions are nearly efficient, which provides
further justification for their use in practice."
"In the Hospitals/Residents (HR) problem, agents are partitioned into
hospitals and residents. Each agent wishes to be matched to an agent in the
other set and has a strict preference over these potential matches. A matching
is stable if there are no blocking pairs, i.e., no pair of agents that prefer
each other to their assigned matches. Such a situation is undesirable as it
could lead to a deviation in which the blocking pair form a private arrangement
outside the matching. This however assumes that the blocking pair have social
ties or communication channels to facilitate the deviation. Relaxing the
stability definition to take account of the potential lack of social ties
between agents can yield larger stable matchings.
  In this paper, we define the Hospitals/Residents problem under Social
Stability (HRSS) which takes into account social ties between agents by
introducing a social network graph to the HR problem. Edges in the social
network graph correspond to resident-hospital pairs in the HR instance that
know one another. Pairs that do not have corresponding edges in the social
network graph can belong to a matching M but they can never block M. Relative
to a relaxed stability definition for HRSS, called social stability, we show
that socially stable matchings can have different sizes and the problem of
finding a maximum socially stable matching is NP-hard, though approximable
within 3/2. Furthermore we give polynomial time algorithms for three special
cases of the problem."
"Severe impossibility results restrict the design of strategyproof random
assignment mechanisms, and trade-offs are necessary when aiming for more
demanding efficiency requirements, such as ordinal or rank efficiency. We
introduce hybrid mechanisms, which are convex combinations of two component
mechanisms. We give a set of conditions under which such hybrids facilitate a
non-degenerate trade-off between strategyproofness (in terms of partial
strategyproofness) and efficiency (in terms of dominance). This set of
conditions is tight in the sense that trade-offs may become degenerate if any
of the conditions are dropped. Moreover, we give an algorithm for the mechanism
designer's problem of determining a maximal mixing factor. Finally, we prove
that our construction can be applied to mix Random Serial Dictatorship with
Probabilistic Serial, as well as with the adaptive Boston mechanism, and we
illustrate the efficiency gains numerically."
"One of the central issues in the debate on network neutrality has been
whether one should allow or prevent preferential treatment by an internet
service provider (ISP) of traffic according to its origin. This raised the
question of whether to allow an ISP to have exclusive agreement with a content
provider (CP). In this paper we consider discrimination in the opposite
direction. We study the impact that a CP can have on the benefits of several
competing ISPs by sharing private information concerning the demand for its
content. More precisely, we consider ISPs that compete over access to one
common CP. Each ISP selects the price that it charges its subscribers for
accessing the content. The CP is assumed to have private information about
demand for its content, and in particular, about the inverse demand function
corresponding to the content. The competing ISPs are assumed to have knowledge
on only the statistical distribution of these functions. We derive in this
paper models for studying the impact that the CP can have on the utilities of
the ISPs by favoring one of them by exclusively revealing its private
information. We also consider the case where CP can charge ISPs for providing
such information. We propose two mechanisms based on {\em weighted proportional
fairness} for payment between ISPs and CP. Finally, we compare the social
utility resulting from these mechanisms with the optimal social utility by
introducing a performance metric termed as {\em price of partial bargaining}"
"In the context of auctions for digital goods, an interesting random sampling
auction has been proposed by Goldberg, Hartline, and Wright [2001]. This
auction has been analyzed by Feige, Flaxman, Hartline, and Kleinberg [2005],
who have shown that it is 15-competitive in the worst case {which is
substantially better than the previously proven constant bounds but still far
from the conjectured competitive ratio of 4. In this paper, we prove that the
aforementioned random sampling auction is indeed 4-competitive for a large
class of instances where the number of bids above (or equal to) the optimal
sale price is at least 6. We also show that it is 4:68-competitive for the
small class of remaining instances thus leaving a negligible gap between the
lower and upper bound. We employ a mix of probabilistic techniques and dynamic
programming to compute these bounds."
"Decomposition, i.e. independently analyzing possible subgames, has proven to
be an essential principle for effective decision-making in perfect information
games. However, in imperfect information games, decomposition has proven to be
problematic. To date, all proposed techniques for decomposition in imperfect
information games have abandoned theoretical guarantees. This work presents the
first technique for decomposing an imperfect information game into subgames
that can be solved independently, while retaining optimality guarantees on the
full-game solution. We can use this technique to construct theoretically
justified algorithms that make better use of information available at run-time,
overcome memory or disk limitations at run-time, or make a time/space trade-off
to overcome memory or disk limitations while solving a game. In particular, we
present an algorithm for subgame solving which guarantees performance in the
whole game, in contrast to existing methods which may have unbounded error. In
addition, we present an offline game solving algorithm, CFR-D, which can
produce a Nash equilibrium for a game that is larger than available storage."
"As the communication network is in transition towards a commercial one
controlled by service providers (SP), the present paper considers a pricing
game in a communication market covered by several wireless access points
sharing the same spectrum and analyzes two business models: monopoly (APs
controlled by one SP) and oligopoly (APs controlled by different SPs). We use a
Stackelberg game to model the problem: SPs are the leader(s) and end users are
the followers. We prove, under certain conditions, the existence and uniqueness
of Nash equilibrium for both models and derive their expressions. In order to
compare the impact of different business models on social welfare and SPs'
profits, we define two metrics: PoCS (price of competition on social welfare)
and PoCP (price of competition on profits). For symmetric cross-AP
interferences, the tight lower bound of PoCS is 3/4, and that of PoCP is 1."
"This paper explores an idea of demand-supply balance for smart grids in which
consumers are expected to play a significant role. The main objective is to
motivate the consumer, by maximizing their benefit both as a seller and a
buyer, to trade their surplus energy with the grid so as to balance the demand
at the peak hour. To that end, a Stackelberg game is proposed to capture the
interactions between the grid and consumers, and it is shown analytically that
optimal energy trading parameters that maximize customers utilities are
obtained at the solution of the game. A novel distributed algorithm is proposed
to reach the optimal solution of the game, and numerical examples are used to
assess the properties and effectiveness of the proposed approach."
"The computational characterization of game-theoretic solution concepts is a
central topic in artificial intelligence, with the aim of developing
computationally efficient tools for finding optimal ways to behave in strategic
interactions. The central solution concept in game theory is Nash equilibrium
(NE). However, it fails to capture the possibility that agents can form
coalitions (even in the 2-agent case). Strong Nash equilibrium (SNE) refines NE
to this setting. It is known that finding an SNE is NP-complete when the number
of agents is constant. This hardness is solely due to the existence of
mixed-strategy SNEs, given that the problem of enumerating all pure-strategy
SNEs is trivially in P. Our central result is that, in order for a game to have
at least one non-pure-strategy SNE, the agents' payoffs restricted to the
agents' supports must, in the case of 2 agents, lie on the same line, and, in
the case of n agents, lie on an (n - 1)-dimensional hyperplane. Leveraging this
result, we provide two contributions. First, we develop worst-case instances
for support-enumeration algorithms. These instances have only one SNE and the
support size can be chosen to be of any size-in particular, arbitrarily large.
Second, we prove that, unlike NE, finding an SNE is in smoothed polynomial
time: generic game instances (i.e., all instances except knife-edge cases) have
only pure-strategy SNEs."
"Evolutionary game theory combines game theory and dynamical systems and is
customarily adopted to describe evolutionary dynamics in multi-agent systems.
In particular, it has been proven to be a successful tool to describe
multi-agent learning dynamics. To the best of our knowledge, we provide in this
paper the first replicator dynamics applicable to the sequence form of an
extensive-form game, allowing an exponential reduction of time and space w.r.t.
the currently adopted replicator dynamics for normal form. Furthermore, our
replicator dynamics is realization equivalent to the standard replicator
dynamics for normal form. We prove our results for both discrete-time and
continuous-time cases. Finally, we extend standard tools to study the stability
of a strategy profile to our replicator dynamics."
"Iterated admissibility is a well-known and important concept in classical
game theory, e.g. to determine rational behaviors in multi-player matrix games.
As recently shown by Berwanger, this concept can be soundly extended to
infinite games played on graphs with omega-regular objectives. In this paper,
we study the algorithmic properties of this concept for such games. We settle
the exact complexity of natural decision problems on the set of strategies that
survive iterated elimination of dominated strategies. As a byproduct of our
construction, we obtain automata which recognize all the possible outcomes of
such strategies."
"We study the problem of solving discounted, two player, turn based,
stochastic games (2TBSGs). Jurdzinski and Savani showed that 2TBSGs with
deterministic transitions can be reduced to solving $P$-matrix linear
complementarity problems (LCPs). We show that the same reduction works for
general 2TBSGs. This implies that a number of interior point methods for
solving $P$-matrix LCPs can be used to solve 2TBSGs. We consider two such
algorithms. First, we consider the unified interior point method of Kojima,
Megiddo, Noma, and Yoshise, which runs in time $O((1+\kappa)n^{3.5}L)$, where
$\kappa$ is a parameter that depends on the $n \times n$ matrix $M$ defining
the LCP, and $L$ is the number of bits in the representation of $M$. Second, we
consider the interior point potential reduction algorithm of Kojima, Megiddo,
and Ye, which runs in time $O(\frac{-\delta}{\theta}n^4\log \epsilon^{-1})$,
where $\delta$ and $\theta$ are parameters that depend on $M$, and $\epsilon$
describes the quality of the solution. For 2TBSGs with $n$ states and discount
factor $\gamma$ we prove that in the worst case $\kappa =
\Theta(n/(1-\gamma)^2)$, $-\delta = \Theta(\sqrt{n}/(1-\gamma))$, and $1/\theta
= \Theta(n/(1-\gamma)^2)$. The lower bounds for $\kappa$, $-\delta$, and
$1/\theta$ are obtained using the same family of deterministic games."
"We consider an extension of strategic normal form games with a phase before
the actual play of the game, where players can make binding offers for transfer
of utilities to other players after the play of the game, contingent on the
recipient playing the strategy indicated in the offer. Such offers transform
the payoff matrix of the original game but preserve its non-cooperative nature.
The type of offers we focus on here are conditional on a suggested 'matching
offer' of the same kind made in return by the receiver. Players can exchange a
series of such offers, thus engaging in a bargaining process before a strategic
normal form game is played.
  In this paper we study and analyze solution concepts for two-player normal
form games with such preplay negotiation phase, under several assumptions for
the bargaining power of the players, such as the possibility of withdrawing
previously made offers and opting out from the negotiation process, as well as
the value of time for the players in such negotiations. We obtain results
describing the possible solutions of such bargaining games and analyze the
degrees of efficiency and fairness that can be achieved in such negotiation
process."
"We study a combinatorial market design problem, where a collection of
indivisible objects is to be priced and sold to potential buyers subject to
equilibrium constraints.The classic solution concept for such problems is
Walrasian Equilibrium (WE), which provides a simple and transparent pricing
structure that achieves optimal social welfare. The main weakness of the WE
notion is that it exists only in very restrictive cases. To overcome this
limitation, we introduce the notion of a Combinatorial Walrasian equilibium
(CWE), a natural relaxation of WE. The difference between a CWE and a
(non-combinatorial) WE is that the seller can package the items into
indivisible bundles prior to sale, and the market does not necessarily clear.
  We show that every valuation profile admits a CWE that obtains at least half
of the optimal (unconstrained) social welfare. Moreover, we devise a poly-time
algorithm that, given an arbitrary allocation X, computes a CWE that achieves
at least half of the welfare of X. Thus, the economic problem of finding a CWE
with high social welfare reduces to the algorithmic problem of social-welfare
approximation. In addition, we show that every valuation profile admits a CWE
that extracts a logarithmic fraction of the optimal welfare as revenue.
Finally, these results are complemented by strong lower bounds when the seller
is restricted to using item prices only, which motivates the use of bundles.
The strength of our results derives partly from their generality - our results
hold for arbitrary valuations that may exhibit complex combinations of
substitutes and complements."
"In the mobile communication services, users wish to subscribe to high quality
service with a low price level, which leads to competition between mobile
network operators (MNOs). The MNOs compete with each other by service prices
after deciding the extent of investment to improve quality of service (QoS).
Unfortunately, the theoretic backgrounds of price dynamics are not known to us,
and as a result, effective network planning and regulative actions are hard to
make in the competitive market. To explain this competition more detail, we
formulate and solve an optimization problem applying the two-stage Cournot and
Bertrand competition model. Consequently, we derive a price dynamics that the
MNOs increase and decrease their service prices periodically, which completely
explains the subsidy dynamics in the real world. Moving forward, to avoid this
instability and inefficiency, we suggest a simple regulation rule which leads
to a Pareto-optimal equilibrium point. Moreover, we suggest regulator's optimal
actions corresponding to user welfare and the regulator's revenue."
"We investigate the complexity of $r$-Approval control problems in $k$-peaked
elections, where at most $k$ peaks are allowed in each vote with respect to an
order of the candidates. We show that most NP-hardness results in general
elections also hold in k-peaked elections even for $k=2,3$. On the other hand,
we derive polynomial-time algorithms for some problems for $k=2$. All our
NP-hardness results apply to Approval and sincere-strategy preference-based
Approval as well. Our study leads to many dichotomy results for the problems
considered in this paper, with respect to the values of $k$ and $r$. In
addition, we study $r$-Approval control problems from the viewpoint of
parameterized complexity and achieve both fixed-parameter tractability results
and W-hardness results, with respect to the solution size. Along the way
exploring the complexity of control problems, we obtain two byproducts which
are of independent interest. First, we prove that every graph of maximum degree
3 admits a specific 2-interval representation where every 2-interval
corresponding to a vertex contains a trivial interval and, moreover,
2-intervals may only intersect at the endpoints of the intervals. Second, we
develop a fixed-parameter tractable algorithm for a generalized $r$-Set Packing
problem with respect to the solution size, where each element in the given
universal set is allowed to occur in more than one r-subset in the solution."
"We show an auction-based algorithm to compute market equilibrium prices in a
production model, where consumers purchase items under separable nonlinear
utility concave functions which satisfy W.G.S(Weak Gross Substitutes);
producers produce items with multiple linear production constraints. Our
algorithm differs from previous approaches in that the prices are allowed to
both increase and decrease to handle changes in the production. This provides a
t^atonnement style algorithm which converges and provides a PTAS. The algorithm
can also be extended to arbitrary convex production regions and the
Arrow-Debreu model. The convergence is dependent on the behavior of the
marginal utility of the concave function."
"We study assignment games in which jobs select machines, and in which certain
pairs of jobs may conflict, which is to say they may incur an additional cost
when they are both assigned to the same machine, beyond that associated with
the increase in load. Questions regarding such interactions apply beyond
allocating jobs to machines: when people in a social network choose to align
themselves with a group or party, they typically do so based upon not only the
inherent quality of that group, but also who amongst their friends (or enemies)
choose that group as well. We show how semi-smoothness, a recently introduced
generalization of smoothness, is necessary to find tight or near-tight bounds
on the price of total anarchy, and thus on the quality of correlated and Nash
equilibria, for several natural job-assignment games with interacting jobs. For
most cases, our bounds on the price of total anarchy are either exactly 2 or
approach 2. We also prove new convergence results implied by semi-smoothness
for our games. Finally we consider coalitional deviations, and prove results
about the existence and quality of Strong equilibrium."
"This paper introduces Monte Carlo *-Minimax Search (MCMS), a Monte Carlo
search algorithm for turned-based, stochastic, two-player, zero-sum games of
perfect information. The algorithm is designed for the class of of densely
stochastic games; that is, games where one would rarely expect to sample the
same successor state multiple times at any particular chance node. Our approach
combines sparse sampling techniques from MDP planning with classic pruning
techniques developed for adversarial expectimax planning. We compare and
contrast our algorithm to the traditional *-Minimax approaches, as well as MCTS
enhanced with the Double Progressive Widening, on four games: Pig, EinStein
W\""urfelt Nicht!, Can't Stop, and Ra. Our results show that MCMS can be
competitive with enhanced MCTS variants in some domains, while consistently
outperforming the equivalent classic approaches given the same amount of
thinking time."
"We consider the menu size of auctions as a measure of auction complexity and
study how it affects revenue. Our setting has a single revenue-maximizing
seller selling two or more heterogeneous items to a single buyer whose private
values for the items are drawn from a (possibly correlated) known distribution,
and whose valuation is additive over the items. We show that the revenue may
increase arbitrarily with menu size and that a bounded menu size can not ensure
any positive fraction of the optimal revenue. The menu size turns out to ""nail
down"" the revenue properties of deterministic auctions: their menu size may be
at most exponential in the number of items and indeed their revenue may be
larger than that achievable by the simplest types of auctions by a factor that
is exponential in the number of items but no larger. Our model is related to a
previously studied ""unit-demand"" model and our results also answer an open
problem in that model."
"We study scenarios where multiple sellers of a homogeneous good compete on
prices, where each seller can only sell to some subset of the buyers.
Crucially, sellers cannot price-discriminate between buyers. We model the
structure of the competition by a graph (or hyper-graph), with nodes
representing the sellers and edges representing populations of buyers. We study
equilibria in the game between the sellers, prove that they always exist, and
present various structural, quantitative, and computational results about them.
We also analyze the equilibria completely for a few cases. Many questions are
left open."
"In settings where players have a limited access to liquidity, represented in
the form of budget constraints, efficiency maximization has proven to be a
challenging goal. In particular, the social welfare cannot be approximated by a
better factor then the number of players. Therefore, the literature has mainly
resorted to Pareto-efficiency as a way to achieve efficiency in such settings.
While successful in some important scenarios, in many settings it is known that
either exactly one incentive-compatible auction that always outputs a
Pareto-efficient solution, or that no truthful mechanism can always guarantee a
Pareto-efficient outcome. Traditionally, impossibility results can be avoided
by considering approximations. However, Pareto-efficiency is a binary property
(is either satisfied or not), which does not allow for approximations.
  In this paper we propose a new notion of efficiency, called \emph{liquid
welfare}. This is the maximum amount of revenue an omniscient seller would be
able to extract from a certain instance. We explain the intuition behind this
objective function and show that it can be 2-approximated by two different
auctions. Moreover, we show that no truthful algorithm can guarantee an
approximation factor better than 4/3 with respect to the liquid welfare, and
provide a truthful auction that attains this bound in a special case.
  Importantly, the liquid welfare benchmark also overcomes impossibilities for
some settings. While it is impossible to design Pareto-efficient auctions for
multi-unit auctions where players have decreasing marginal values, we give a
deterministic $O(\log n)$-approximation for the liquid welfare in this setting."
"In a sponsored search auction, decisions about how to rank ads impose
tradeoffs between objectives such as revenue and welfare. In this paper, we
examine how these tradeoffs should be made. We begin by arguing that the most
natural solution concept to evaluate these tradeoffs is the lowest symmetric
Nash equilibrium (SNE). As part of this argument, we generalise the well known
connection between the lowest SNE and the VCG outcome. We then propose a new
ranking algorithm, loosely based on the revenue-optimal auction, that uses a
reserve price to order the ads (not just to filter them) and give conditions
under which it raises more revenue than simply applying that reserve price.
Finally, we conduct extensive simulations examining the tradeoffs enabled by
different ranking algorithms and show that our proposed algorithm enables
superior operating points by a variety of metrics."
"The first-price auction is popular in practice for its simplicity and
transparency. Moreover, its potential virtues grow in complex settings where
incentive compatible auctions may generate little or no revenue. Unfortunately,
the first-price auction is poorly understood in theory because equilibrium is
not {\em a priori} a credible predictor of bidder behavior.
  We take a dynamic approach to studying first-price auctions: rather than
basing performance guarantees solely on static equilibria, we study the
repeated setting and show that robust performance guarantees may be derived
from simple axioms of bidder behavior. For example, as long as a loser raises
her bid quickly, a standard first-price auction will generate at least as much
revenue as a second-price auction. We generalize this dynamic technique to
complex pay-your-bid auction settings and show that progressively stronger
assumptions about bidder behavior imply progressively stronger guarantees about
the auction's performance.
  Along the way, we find that the auctioneer's choice of bidding language is
critical when generalizing beyond the single-item setting, and we propose a
specific construction called the {\em utility-target auction} that performs
well. The utility-target auction includes a bidder's final utility as an
additional parameter, identifying the single dimension along which she wishes
to compete. This auction is closely related to profit-target bidding in
first-price and ascending proxy package auctions and gives strong revenue
guarantees for a variety of complex auction environments. Of particular
interest, the guaranteed existence of a pure-strategy equilibrium in the
utility-target auction shows how Overture might have eliminated the cyclic
behavior in their generalized first-price sponsored search auction if bidders
could have placed more sophisticated bids."
"We consider the pricing problem faced by a seller who assigns a price to a
good that confers its benefits not only to its buyers, but also to other
individuals around them. For example, a snow-blower is potentially useful not
only to the household that buys it, but also to others on the same street.
Given that the seller is constrained to selling such a (locally) public good
via individual private sales, how should he set his prices given the
distribution of values held by the agents?
  We study this problem as a two-stage game. In the first stage, the seller
chooses and announces a price for the product. In the second stage, the agents
(each having a private value for the good) decide simultaneously whether or not
they will buy the product. In the resulting game, which can exhibit a
multiplicity of equilibria, agents must strategize about whether they will
themselves purchase the good to receive its benefits.
  In the case of a fully public good (where all agents benefit whenever any
agent purchases), we describe a pricing mechanism that is approximately
revenue-optimal (up to a constant factor) when values are drawn from a regular
distribution. We then study settings in which the good is only ""locally""
public: agents are arranged in a network and share benefits only with their
neighbors. We describe a pricing method that approximately maximizes revenue,
in the worst case over equilibria of agent behavior, for any $d$-regular
network. Finally, we show that approximately optimal prices can be found for
general networks in the special case that private values are drawn from a
uniform distribution. We also discuss some barriers to extending these results
to general networks and regular distributions."
"What fraction of the potential social surplus in an environment can be
extracted by a revenue-maximizing monopolist? We investigate this problem in
Bayesian single-parameter environments with independent private values. The
precise answer to the question obviously depends on the particulars of the
environment: the feasibility constraint and the distributions from which the
bidders' private values are sampled. Rather than solving the problem in
particular special cases, our work aims to provide universal lower bounds on
the revenue-to-welfare ratio that hold under the most general hypotheses that
allow for non-trivial such bounds.
  Our results can be summarized as follows. For general feasibility
constraints, the revenue-to-welfare ratio is at least a constant times the
inverse-square-root of the number of agents, and this is tight up to constant
factors. For downward-closed feasibility constraints, the revenue-to-welfare
ratio is bounded below by a constant. Both results require the bidders'
distributions to satisfy hypotheses somewhat stronger than regularity; we show
that the latter result cannot avoid this requirement."
"We study the design of Bayesian incentive compatible mechanisms in single
parameter domains, for the objective of optimizing social efficiency as
measured by social cost. In the problems we consider, a group of participants
compete to receive service from a mechanism that can provide such services at a
cost. The mechanism wishes to choose which agents to serve in order to maximize
social efficiency, but is not willing to suffer an expected loss: the agents'
payments should cover the cost of service in expectation.
  We develop a general method for converting arbitrary approximation algorithms
for the underlying optimization problem into Bayesian incentive compatible
mechanisms that are cost-recovering in expectation. In particular, we give
polynomial time black-box reductions from the mechanism design problem to the
problem of designing a social cost minimization algorithm without incentive
constraints. Our reduction increases the expected social cost of the given
algorithm by a factor of O(log(min{n, h})), where n is the number of agents and
h is the ratio between the highest and lowest nonzero valuations in the
support. We also provide a lower bound illustrating that this inflation of the
social cost is essential: no BIC cost-recovering mechanism can achieve an
approximation factor better than \Omega(log(n)) or \Omega(log(h)) in general.
  Our techniques extend to show that a certain class of truthful algorithms can
be made cost-recovering in the non-Bayesian setting, in such a way that the
approximation factor degrades by at most O(log(min{n, h})). This is an
improvement over previously-known constructions with inflation factor O(log n)."
"This paper presents an optimal strategy for solving the 4 peg-7 color
Mastermind MM(4,7) in the expected case (4.676) along with optimal strategies
or upper bounds for other values. The program developed is using a depth-first
branch and bound algorithm relying on tight upper bound, dynamic lower bound
evaluation and guess equivalence to prune symmetric tree branches."
"We present algorithms for implementing local spectrum redistribution in
wireless networks using a mechanism design approach. For example, in single-hop
request scheduling, secondary users are modeled as rational agents that have
private utility when getting assigned a channel for successful transmission. We
present a rather simple algorithmic technique that allows to turn existing and
future approximation algorithms and heuristics into truthful mechanisms for a
large variety of networking problems. In contrast to previous work, our
approach works for virtually all known interference models in the literature,
including the physical model of interference based on SINR. It allows to
address single-hop and multi-hop scheduling, routing, and even more general
assignment and allocation problems. Our mechanisms are randomized and represent
the first universally-truthful mechanisms for these problems with rigorous
worst-case guarantees on the solution quality. In this way, our mechanisms can
be used to obtain guaranteed solution quality even with risk-averse or
risk-seeking bidders, for which existing approaches fail."
"In this note we provide a new proof for the results of Lipton et al. on the
existence of an approximate Nash equilibrium with logarithmic support size.
Besides its simplicity, the new proof leads to the following contributions:
  1. For n-player games, we improve the bound on the size of the support of an
approximate Nash equilibrium.
  2. We generalize the result of Daskalakis and Papadimitriou on small
probability games from the two-player case to the general n-player case.
  3. We provide a logarithmic bound on the size of the support of an
approximate Nash equilibrium in the case of graphical games."
"We consider the problem of locating a single facility on the real line. This
facility serves a set of agents, each of whom is located on the line, and
incurs a cost equal to his distance from the facility. An agent's location is
private information that is known only to him. Agents report their location to
a central planner who decides where to locate the facility. The planner's
objective is to minimize a ""social"" cost function that depends on the
agent-costs. However, agents might not report truthfully; to address this
issue, the planner must restrict himself to {\em strategyproof} mechanisms, in
which truthful reporting is a dominant strategy for each agent. A mechanism
that simply chooses the optimal solution is generally not strategyproof, and so
the planner aspires to use a mechanism that effectively {\em approximates} his
objective function. In our paper, we study the problem described above with the
social cost function being the $L_p$ norm of the vector of agent-costs. We show
that the median mechanism (which is known to be strategyproof) provides a
$2^{1-\frac{1}{p}}$ approximation ratio, and that is the optimal approximation
ratio among all deterministic strategyproof mechanisms. For randomized
mechanisms, we present two results. First, we present a negative result: we
show that for integer $\infty>p>2$, no mechanism---from a rather large class of
randomized mechanisms--- has an approximation ratio better than that of the
median mechanism. This is in contrast to the case of $p=2$ and $p=\infty$ where
a randomized mechanism provably helps improve the worst case approximation
ratio. Second, for the case of 2 agents, we show that a mechanism called LRM,
first designed by Procaccia and Tennenholtz for the special case of
$L_{\infty}$, provides the optimal approximation ratio among all randomized
mechanisms."
"We present a market-based approach to the Air Traffic Flow Management (ATFM)
problem. The goods in our market are delays and buyers are airline companies;
the latter pay money to the FAA to buy away the desired amount of delay on a
per flight basis. We give a notion of equilibrium for this market and an LP
whose solution gives an equilibrium allocation of flights to landing slots as
well as equilibrium prices for the landing slots. Via a reduction to matching,
we show that this equilibrium can be computed combinatorially in strongly
polynomial time. Moreover, there is a special set of equilibrium prices, which
can be computed easily, that is identical to the VCG solution, and therefore
the market is incentive compatible in dominant strategy."
"We consider k-Facility Location games, where n strategic agents report their
locations on the real line, and a mechanism maps them to k facilities. Each
agent seeks to minimize his connection cost, given by a nonnegative increasing
function of his distance to the nearest facility. Departing from previous work,
that mostly considers the identity cost function, we are interested in
mechanisms without payments that are (group) strategyproof for any given cost
function, and achieve a good approximation ratio for the social cost and/or the
maximum cost of the agents.
  We present a randomized mechanism, called Equal Cost, which is group
strategyproof and achieves a bounded approximation ratio for all k and n, for
any given concave cost function. The approximation ratio is at most 2 for Max
Cost and at most n for Social Cost. To the best of our knowledge, this is the
first mechanism with a bounded approximation ratio for instances with k > 2
facilities and any number of agents. Our result implies an interesting
separation between deterministic mechanisms, whose approximation ratio for Max
Cost jumps from 2 to unbounded when k increases from 2 to 3, and randomized
mechanisms, whose approximation ratio remains at most 2 for all k. On the
negative side, we exclude the possibility of a mechanism with the properties of
Equal Cost for strictly convex cost functions. We also present a randomized
mechanism, called Pick the Loser, which applies to instances with k facilities
and n = k+1 agents, and for any given concave cost function, is strongly group
strategyproof and achieves an approximation ratio of 2 for Social Cost."
"We study the structural complexity of bimatrix games, formalized via rank,
from an empirical perspective. We consider a setting where we have data on
player behavior in diverse strategic situations, but where we do not observe
the relevant payoff functions. We prove that high complexity (high rank) has
empirical consequences when arbitrary data is considered. Additionally, we
prove that, in more restrictive classes of data (termed laminar), any
observation is rationalizable using a low-rank game: specifically a zero-sum
game. Hence complexity as a structural property of a game is not always
testable. Finally, we prove a general result connecting the structure of the
feasible data sets with the highest rank that may be needed to rationalize a
set of observations."
"We investigate a spectrum oligopoly where primary users allow secondary
access in lieu of financial remuneration. Transmission qualities of the
licensed bands fluctuate randomly. Each primary needs to select the price of
its channel with the knowledge of its own channel state but not that of its
competitors. Secondaries choose among the channels available on sale based on
their states and prices. We formulate the price selection as a non-cooperative
game and prove that a symmetric Nash equilibrium (NE) strategy profile exists
uniquely. We explicitly compute this strategy profile and analytically and
numerically evaluate its efficiency. Our structural results provide certain key
insights about the unique symmetric NE."
"Recently, Apt and Markakis introduced a model for product adoption in social
networks with multiple products, where the agents, influenced by their
neighbours, can adopt one out of several alternatives (products). To analyze
these networks we introduce social network games in which product adoption is
obligatory.
  We show that when the underlying graph is a simple cycle, there is a
polynomial time algorithm allowing us to determine whether the game has a Nash
equilibrium. In contrast, in the arbitrary case this problem is NP-complete. We
also show that the problem of determining whether the game is weakly acyclic is
co-NP hard.
  Using these games we analyze various types of paradoxes that can arise in the
considered networks. One of them corresponds to the well-known Braess paradox
in congestion games. In particular, we show that social networks exist with the
property that by adding an additional product to a specific node, the choices
of the nodes will unavoidably evolve in such a way that everybody is strictly
worse off."
"Classical results in voting theory show that strategic manipulation by voters
is inevitable if a voting rule simultaneously satisfy certain desirable
properties. Motivated by this, we study the relevant question of how often a
voting rule is manipulable. It is well known that elections with a large number
of voters are rarely manipulable under impartial culture (IC) assumption.
However, the manipulability of voting rules when the number of candidates is
large has hardly been addressed in the literature and our paper focuses on this
problem. First, we propose two properties (1) asymptotic strategy-proofness and
(2) asymptotic collusion-proofness, with respect to new voters, which makes the
two notions more relevant from the perspective of computational problem of
manipulation. In addition to IC, we explore a new culture of society where all
score vectors of the candidates are equally likely. This new notion has its
motivation in computational social choice and we call it impartial scores
culture (ISC) assumption. We study asymptotic strategy-proofness and asymptotic
collusion-proofness for plurality, veto, $k$-approval, and Borda voting rules
under IC as well as ISC assumptions. Specifically, we prove bounds for the
fraction of manipulable profiles when the number of candidates is large. Our
results show that the size of the coalition and the tie-breaking rule play a
crucial role in determining whether or not a voting rule satisfies the above
two properties."
"We study a problem where a group of agents has to decide how a joint reward
should be shared among them. We focus on settings where the share that each
agent receives depends on the subjective opinions of its peers concerning that
agent's contribution to the group. To this end, we introduce a mechanism to
elicit and aggregate subjective opinions as well as for determining agents'
shares. The intuition behind the proposed mechanism is that each agent who
believes that the others are telling the truth has its expected share maximized
to the extent that it is well-evaluated by its peers and that it is truthfully
reporting its opinions. Under the assumptions that agents are Bayesian
decision-makers and that the underlying population is sufficiently large, we
show that our mechanism is incentive-compatible, budget-balanced, and
tractable. We also present strategies to make this mechanism individually
rational and fair."
"The Generalized Second Price auction (GSP) has been widely used by search
engines to sell ad slots. Previous studies have shown that the pure Price Of
Anarchy (POA) of GSP is 1.25 when there are two ad slots and 1.259 when three
ad slots. For the cases with more than three ad slots, however, only some
untight upper bounds of the pure POA were obtained. In this work, we improve
previous results in two aspects: (1) We prove that the pure POA for GSP is
1.259 when there are four ad slots, and (2) We show that the pure POA for GSP
with more than four ad slots is also 1.259 given the bidders are ranked
according to a particular permutation."
"Assuming that cards are dealt with replacement from a single deck and that
each of Player and Banker sees the total of his own two-card hand but not its
composition, baccara is a 2 x 2^88 matrix game, which was solved by Kemeny and
Snell in 1957. Assuming that cards are dealt without replacement from a d-deck
shoe and that Banker sees the composition of his own two-card hand while Player
sees only his own total, baccara is a 2 x 2^484 matrix game, which was solved
by Downton and Lockwood in 1975 for d=1,2,...,8. Assuming that cards are dealt
without replacement from a d-deck shoe and that each of Player and Banker sees
the composition of his own two-card hand, baccara is a 2^5 x 2^484 matrix game,
which is solved herein for every positive integer d."
"While microtask crowdsourcing provides a new way to solve large volumes of
small tasks at a much lower price compared with traditional in-house solutions,
it suffers from quality problems due to the lack of incentives. On the other
hand, providing incentives for microtask crowdsourcing is challenging since
verifying the quality of submitted solutions is so expensive that will negate
the advantage of microtask crowdsourcing. We study cost-effective incentive
mechanisms for microtask crowdsourcing in this paper. In particular, we
consider a model with strategic workers, where the primary objective of a
worker is to maximize his own utility. Based on this model, we analyze two
basic mechanisms widely adopted in existing microtask crowdsourcing
applications and show that, to obtain high quality solutions from workers,
their costs are constrained by some lower bounds. We then propose a
cost-effective mechanism that employs quality-aware worker training as a tool
to stimulate workers to provide high quality solutions. We prove theoretically
that the proposed mechanism, when properly designed, can obtain high quality
solutions with an arbitrarily low cost. Beyond its theoretical guarantees, we
further demonstrate the effectiveness of our proposed mechanisms through a set
of behavioral experiments."
"We consider a distributed multi-user system where individual entities possess
observations or perceptions of one another, while the truth is only known to
themselves, and they might have an interest in withholding or distorting the
truth. We ask the question whether it is possible for the system as a whole to
arrive at the correct perceptions or assessment of all users, referred to as
their reputation, by encouraging or incentivizing the users to participate in a
collective effort without violating private information and self-interest. Two
specific applications, online shopping and network reputation, are provided to
motivate our study and interpret the results. In this paper we investigate this
problem using a mechanism design theoretic approach. We introduce a number of
utility models representing users' strategic behavior, each consisting of one
or both of a truth element and an image element, reflecting the user's desire
to obtain an accurate view of the other and an inflated image of itself. For
each model, we either design a mechanism that achieves the optimal performance
(solution to the corresponding centralized problem), or present individually
rational sub-optimal solutions. In the latter case, we demonstrate that even
when the centralized solution is not achievable, by using a simple
punish-reward mechanism, not only a user has the incentive to participate and
provide information, but also that this information can improve the system
performance."
"We study a problem where a group of agents has to decide how some fixed value
should be shared among them. We are interested in settings where the share that
each agent receives is based on how that agent is evaluated by other members of
the group, where highly regarded agents receive a greater share compared to
agents that are not well regarded. We introduce two mechanisms for determining
agents' shares: the peer-evaluation mechanism, where each agent gives a direct
evaluation for every other member of the group, and the peer-prediction
mechanism, where each agent is asked to report how they believe group members
will evaluate a particular agent. The sharing is based on the provided
information. While both mechanisms are individually rational, the first
mechanism is strategy-proof and budget-balanced, but it can be collusion-prone.
Further, the second mechanism is collusion-resistant and incentive-compatible."
"It is common to assume that agents will adopt Nash equilibrium strategies;
however, experimental studies have demonstrated that Nash equilibrium is often
a poor description of human players' behavior in unrepeated normal-form games.
In this paper, we analyze five widely studied models (Quantal Response
Equilibrium, Level-$k$, Cognitive Hierarchy, QLk, and Noisy Introspection) that
aim to describe actual, rather than idealized, human behavior in such games. We
performed what we believe is the most comprehensive meta-analysis of these
models, leveraging ten different data sets from the literature recording human
play of two-player games. We began by evaluating the models' generalization or
predictive performance, asking how well a model fits unseen test data after
having had its parameters calibrated based on separate training data.
Surprisingly, we found that what we dub the QLk model of Stahl & Wilson (1994)
consistently achieved the best performance. Motivated by this finding, we
describe methods for analyzing the posterior distributions over a model's
parameters. We found that QLk's parameters were being set to values that were
not consistent with their intended economic interpretations. We thus explored
variations of QLk, ultimately identifying a new model family that has fewer
parameters, gives rise to more parsimonious parameter values, and achieves
better predictive performance."
"We study network connection games where the nodes of a network perform edge
swaps in order to improve their communication costs. For the model proposed by
Alon et al. (2010), in which the selfish cost of a node is the sum of all
shortest path distances to the other nodes, we use the probabilistic method to
provide a new, structural characterization of equilibrium graphs. We show how
to use this characterization in order to prove upper bounds on the diameter of
equilibrium graphs in terms of the size of the largest $k$-vicinity (defined as
the the set of vertices within distance $k$ from a vertex), for any $k \geq 1$
and in terms of the number of edges, thus settling positively a conjecture of
Alon et al. in the cases of graphs of large $k$-vicinity size (including graphs
of large maximum degree) and of graphs which are dense enough.
  Next, we present a new swap-based network creation game, in which selfish
costs depend on the immediate neighborhood of each node; in particular, the
profit of a node is defined as the sum of the degrees of its neighbors. We
prove that, in contrast to the previous model, this network creation game
admits an exact potential, and also that any equilibrium graph contains an
induced star. The existence of the potential function is exploited in order to
show that an equilibrium can be reached in expected polynomial time even in the
case where nodes can only acquire limited knowledge concerning non-neighboring
nodes."
"The AB game is a two-player game, where the codemaker has to choose a secret
code and the codebreaker has to guess it in as few questions as possible. It is
a variant of the famous Mastermind game, with the only difference that all pegs
in both, the secret and the questions must have distinct colors. In this work,
we consider the Generalized AB game, where for given arbitrary numbers $p$, $c$
with $p \le c$ the secret code consists of $p$ pegs each having one of $c$
colors and the answer consists only of a number of black and white pegs. There
the number of black pegs equals the number of pegs matching in the
corresponding question and the secret in position and color, and the number of
white pegs equals the additional number of pegs matching in the corresponding
question and the secret only in color. We consider also a variant of the
Generalized AB game, where the information of white pegs is omitted. This
variant is called Generalized Black-peg AB game. Let $\ab(p,c)$ and $\abb(p,c)$
be the worst case number of questions for Generalized AB game and Generalized
Black-peg AB game, respectively. Combining a computer program with theoretical
considerations, we confirm known exact values of $\ab(2,c)$ and $\ab(3,c)$ and
prove tight bounds for $\ab(4,c)$. Furthermore, we present exact values for
$\abb(2,c)$ and $\abb(3,c)$ and tight bounds for $\abb(4,c)$."
"We study lower bounds on the query complexity of determining correlated
equilibrium. In particular, we consider a query model in which an n-player game
is specified via a black box that returns players' utilities at pure action
profiles. In this model we establish that in order to compute a correlated
equilibrium any deterministic algorithm must query the black box an exponential
(in n) number of times."
"We study a problem of trust in a distributed system in which a common
resource is shared by multiple parties. In such naturally information-limited
settings, parties abide by a behavioral protocol that leads to fair sharing of
the resource. However, greedy players may defect from a cooperative protocol
and achieve a greater than fair share of resources, often without significant
adverse consequences to themselves. In this paper, we study the role of a few
vigilante players who also defect from a cooperative resource-sharing protocol
but only in response to perceived greedy behavior. For a simple model of
engagement, we demonstrate surprisingly complex dynamics among greedy and
vigilante players. We show that the best response function for the
greedy-player under our formulation has a jump discontinuity, which leads to
conditions under which there is no Nash equilibrium. To study this property, we
formulate an exact representation for the greedy player best response function
in the case when there is one greedy player, one vigilante player and $N-2$
cooperative players. We use this formulation to show conditions under which a
Nash equilibrium exists. We also illustrate that in the case when there is no
Nash equilibrium, then the discrete dynamic system generated from fictitious
play will not converge, but will oscillate indefinitely as a result of the jump
discontinuity. The case of multiple vigilante and greedy players is studied
numerically. Finally, we explore the relationship between fictitious play and
the better response dynamics (gradient descent) and illustrate that this
dynamical system can have a fixed point even when the discrete dynamical system
arising from fictitious play does not."
"We design a Copula-based generic randomized truthful mechanism for scheduling
on two unrelated machines with approximation ratio within $[1.5852, 1.58606]$,
offering an improved upper bound for the two-machine case. Moreover, we provide
an upper bound 1.5067711 for the two-machine two-task case, which is almost
tight in view of the lower bound of 1.506 for the scale-free truthful
mechanisms [4]. Of independent interest is the explicit incorporation of the
concept of Copula in the design and analysis of the proposed approximation
algorithm. We hope that techniques like this one will also prove useful in
solving other problems in the future."
"The classic result of Bulow and Klemperer \cite{BK96} says that in a
single-item auction recruiting one more bidder and running the Vickrey auction
achieves a higher revenue than the optimal auction's revenue on the original
set of bidders, when values are drawn i.i.d. from a regular distribution. We
give a version of Bulow and Klemperer's result in settings where bidders'
values are drawn from non-i.i.d. irregular distributions. We do this by
modeling irregular distributions as some convex combination of regular
distributions. The regular distributions that constitute the irregular
distribution correspond to different population groups in the bidder
population. Drawing a bidder from this collection of population groups is
equivalent to drawing from some convex combination of these regular
distributions. We show that recruiting one extra bidder from each underlying
population group and running the Vickrey auction gives at least half of the
optimal auction's revenue on the original set of bidders."
"The Shapley value is arguably the most central normative solution concept in
cooperative game theory. It specifies a unique way in which the reward from
cooperation can be ""fairly"" divided among players. While it has a wide range of
real world applications, its use is in many cases hampered by the hardness of
its computation. A number of researchers have tackled this problem by (i)
focusing on classes of games where the Shapley value can be computed
efficiently, or (ii) proposing representation formalisms that facilitate such
efficient computation, or (iii) approximating the Shapley value in certain
classes of games. For the classical \textit{characteristic function}
representation, the only attempt to approximate the Shapley value for the
general class of games is due to Castro \textit{et al.} \cite{castro}. While
this algorithm provides a bound on the approximation error, this bound is
\textit{asymptotic}, meaning that it only holds when the number of samples
increases to infinity. On the other hand, when a finite number of samples is
drawn, an unquantifiable error is introduced, meaning that the bound no longer
holds. With this in mind, we provide non-asymptotic bounds on the estimation
error for two cases: where (i) the \textit{variance}, and (ii) the
\textit{range}, of the players' marginal contributions is known. Furthermore,
for the second case, we show that when the range is significantly large
relative to the Shapley value, the bound can be improved (from $O(\frac{r}{m})$
to $O(\sqrt{\frac{r}{m}})$). Finally, we propose, and demonstrate the
effectiveness of using stratified sampling for improving the bounds further."
"We study balanced solutions for network bargaining games with general
capacities, where agents can participate in a fixed but arbitrary number of
contracts. We provide the first polynomial time algorithm for computing
balanced solutions for these games. In addition, we prove that an instance has
a balanced solution if and only if it has a stable one. Our methods use a new
idea of reducing an instance with general capacities to a network bargaining
game with unit capacities defined on an auxiliary graph. This represents a
departure from previous approaches, which rely on computing an allocation in
the intersection of the core and prekernel of a corresponding cooperative game,
and then proving that the solution corresponding to this allocation is
balanced. In fact, we show that such cooperative game methods do not extend to
general capacity games, since contrary to the case of unit capacities, there
exist allocations in the intersection of the core and prekernel with no
corresponding balanced solution. Finally, we identify two sufficient conditions
under which the set of balanced solutions corresponds to the intersection of
the core and prekernel, thereby extending the class of games for which this
result was previously known."
"This paper presents a new partial two-player game, called the \emph{cannibal
animal game}, which is a variant of Tic-Tac-Toe. The game is played on the
infinite grid, where in each round a player chooses and occupies free cells.
The first player Alice can occupy a cell in each turn and wins if she occupies
a set of cells, the union of a subset of which is a translated, reflected
and/or rotated copy of a previously agreed upon polyomino $P$ (called an
\emph{animal}). The objective of the second player Bob is to prevent Alice from
creating her animal by occupying in each round a translated, reflected and/or
rotated copy of $P$. An animal is a \emph{cannibal} if Bob has a winning
strategy, and a \emph{non-cannibal} otherwise. This paper presents some new
tools, such as the \emph{bounding strategy} and the \emph{punching lemma}, to
classify animals into cannibals or non-cannibals. We also show that the
\emph{pairing strategy} works for this problem."
"We study competition between firms in labor markets, following a
combinatorial model suggested by Kelso and Crawford [1982]. In this model, each
firm is trying to recruit workers by offering a higher salary than its
competitors, and its production function defines the utility generated from any
actual set of recruited workers. We define two natural classes of production
functions for firms, where the first one is based on additive capacities
(weights), and the second on the influence of workers in a social network. We
then analyze the existence of pure subgame perfect equilibrium (PSPE) in the
labor market and its properties. While neither class holds the gross
substitutes condition, we show that in both classes the existence of PSPE is
guaranteed under certain restrictions, and in particular when there are only
two competing firms. As a corollary, there exists a Walrasian equilibrium in a
corresponding combinatorial auction, where bidders' valuation functions belong
to these classes.
  While a PSPE may not exist when there are more than two firms, we perform an
empirical study of equilibrium outcomes for the case of weight-based games with
three firms, which extend our analytical results. We then show that stability
can in some cases be extended to coalitional stability, and study the
distribution of profit between firms and their workers in weight-based games."
"We investigate an alternative concept of Nash equilibrium, m-equilibrium,
which slightly resembles Harsanyi-Selten risk dominant equilibrium although it
is a different notion. M-equilibria provide nontrivial solutions of normal form
games as shown by comparison of the Prisoner's Dilemma with the Traveler's
Dilemma. They are also resistant on the deep iterated elimination of dominated
strategies."
"We study the query complexity of approximate notions of Nash equilibrium in
games with a large number of players $n$. Our main result states that for
$n$-player binary-action games and for constant $\varepsilon$, the query
complexity of an $\varepsilon$-well-supported Nash equilibrium is exponential
in $n$. One of the consequences of this result is an exponential lower bound on
the rate of convergence of adaptive dynamics to approxiamte Nash equilibrium."
"When comparing new wireless technologies, it is common to consider the effect
that they have on the capacity of the network (defined as the maximum number of
simultaneously satisfiable links). For example, it has been shown that giving
receivers the ability to do interference cancellation, or allowing transmitters
to use power control, never decreases the capacity and can in certain cases
increase it by $\Omega(\log (\Delta \cdot P_{\max}))$, where $\Delta$ is the
ratio of the longest link length to the smallest transmitter-receiver distance
and $P_{\max}$ is the maximum transmission power. But there is no reason to
expect the optimal capacity to be realized in practice, particularly since
maximizing the capacity is known to be NP-hard. In reality, we would expect
links to behave as self-interested agents, and thus when introducing a new
technology it makes more sense to compare the values reached at game-theoretic
equilibria than the optimum values.
  In this paper we initiate this line of work by comparing various notions of
equilibria (particularly Nash equilibria and no-regret behavior) when using a
supposedly ""better"" technology. We show a version of Braess's Paradox for all
of them: in certain networks, upgrading technology can actually make the
equilibria \emph{worse}, despite an increase in the capacity. We construct
instances where this decrease is a constant factor for power control,
interference cancellation, and improvements in the SINR threshold ($\beta$),
and is $\Omega(\log \Delta)$ when power control is combined with interference
cancellation. However, we show that these examples are basically tight: the
decrease is at most O(1) for power control, interference cancellation, and
improved $\beta$, and is at most $O(\log \Delta)$ when power control is
combined with interference cancellation."
"We consider collaborative systems where users make contributions across
multiple available projects and are rewarded for their contributions in
individual projects according to a local sharing of the value produced. This
serves as a model of online social computing systems such as online Q&A forums
and of credit sharing in scientific co-authorship settings. We show that the
maximum feasible produced value can be well approximated by simple local
sharing rules where users are approximately rewarded in proportion to their
marginal contributions and that this holds even under incomplete information
about the player's abilities and effort constraints. For natural instances we
show almost 95% optimality at equilibrium. When players incur a cost for their
effort, we identify a threshold phenomenon: the efficiency is a constant
fraction of the optimal when the cost is strictly convex and decreases with the
number of players if the cost is linear."
"We consider the optimal pricing problem for a model of the rich media
advertisement market, as well as other related applications. In this market,
there are multiple buyers (advertisers), and items (slots) that are arranged in
a line such as a banner on a website. Each buyer desires a particular number of
{\em consecutive} slots and has a per-unit-quality value $v_i$ (dependent on
the ad only) while each slot $j$ has a quality $q_j$ (dependent on the position
only such as click-through rate in position auctions). Hence, the valuation of
the buyer $i$ for item $j$ is $v_iq_j$. We want to decide the allocations and
the prices in order to maximize the total revenue of the market maker.
  A key difference from the traditional position auction is the advertiser's
requirement of a fixed number of consecutive slots. Consecutive slots may be
needed for a large size rich media ad. We study three major pricing mechanisms,
the Bayesian pricing model, the maximum revenue market equilibrium model and an
envy-free solution model. Under the Bayesian model, we design a polynomial time
computable truthful mechanism which is optimum in revenue. For the market
equilibrium paradigm, we find a polynomial time algorithm to obtain the maximum
revenue market equilibrium solution. In envy-free settings, an optimal solution
is presented when the buyers have the same demand for the number of consecutive
slots. We conduct a simulation that compares the revenues from the above
schemes and gives convincing results."
"We study network formation with the bilateral link formation rule (Jackson
and Wolinsky 1996) with $n$ players and link cost $\alpha>0$. After the network
is built, an adversary randomly destroys one link according to a certain
probability distribution. Cost for player $v$ incorporates the expected number
of players to which $v$ will become disconnected. This model was previously
studied for unilateral link formation (K. 2011).
  We prove existence of pairwise Nash equilibria under moderate assumptions on
the adversary and $n\geq 9$. As the main result, we prove bounds on the price
of anarchy for two special adversaries: one destroys a link chosen uniformly at
random, while the other destroys a link that causes a maximum number of player
pairs to be separated. We prove bounds tight up to constants, namely $O(1)$ for
one adversary (if $\alpha>1/2$), and $\Theta(n)$ for the other (if $\alpha>2$
considered constant and $n \geq 9$). The latter is the worst that can happen
for any adversary in this model (if $\alpha=\Omega(1)$)."
"We introduce a new class of games, called social contribution games (SCGs),
where each player's individual cost is equal to the cost he induces on society
because of his presence. Our results reveal that SCGs constitute useful
abstractions of altruistic games when it comes to the analysis of the robust
price of anarchy. We first show that SCGs are altruism-independently smooth,
i.e., the robust price of anarchy of these games remains the same under
arbitrary altruistic extensions. We then devise a general reduction technique
that enables us to reduce the problem of establishing smoothness for an
altruistic extension of a base game to a corresponding SCG. Our reduction
applies whenever the base game relates to a canonical SCG by satisfying a
simple social contribution boundedness property. As it turns out, several
well-known games satisfy this property and are thus amenable to our reduction
technique. Examples include min-sum scheduling games, congestion games, second
price auctions and valid utility games. Using our technique, we derive mostly
tight bounds on the robust price of anarchy of their altruistic extensions. For
the majority of the mentioned game classes, the results extend to the more
differentiated friendship setting. As we show, our reduction technique covers
this model if the base game satisfies three additional natural properties."
"We study the issues of existence and inefficiency of pure Nash equilibria in
linear congestion games with altruistic social context, in the spirit of the
model recently proposed by de Keijzer {\em et al.} \cite{DSAB13}. In such a
framework, given a real matrix $\Gamma=(\gamma_{ij})$ specifying a particular
social context, each player $i$ aims at optimizing a linear combination of the
payoffs of all the players in the game, where, for each player $j$, the
multiplicative coefficient is given by the value $\gamma_{ij}$. We give a broad
characterization of the social contexts for which pure Nash equilibria are
always guaranteed to exist and provide tight or almost tight bounds on their
prices of anarchy and stability. In some of the considered cases, our
achievements either improve or extend results previously known in the
literature."
"A traditional assumption in game theory is that players are opaque to one
another---if a player changes strategies, then this change in strategies does
not affect the choice of other players' strategies. In many situations this is
an unrealistic assumption. We develop a framework for reasoning about games
where the players may be translucent to one another; in particular, a player
may believe that if she were to change strategies, then the other player would
also change strategies. Translucent players may achieve significantly more
efficient outcomes than opaque ones.
  Our main result is a characterization of strategies consistent with
appropriate analogues of common belief of rationality. Common Counterfactual
Belief of Rationality (CCBR) holds if (1) everyone is rational, (2) everyone
counterfactually believes that everyone else is rational (i.e., all players i
believe that everyone else would still be rational even if $i$ were to switch
strategies), (3) everyone counterfactually believes that everyone else is
rational, and counterfactually believes that everyone else is rational, and so
on. CCBR characterizes the set of strategies surviving iterated removal of
minimax dominated strategies, where a strategy s for player i is minimax
dominated by s' if the worst-case payoff for i using s' is better than the best
possible payoff using s."
"We consider non-cooperative unsplittable congestion games where players share
resources, and each player's strategy is pure and consists of a subset of the
resources on which it applies a fixed weight. Such games represent unsplittable
routing flow games and also job allocation games. The congestion of a resource
is the sum of the weights of the players that use it and the player's cost
function is the sum of the utilities of the resources on its strategy. The
social cost is the total weighted sum of the player's costs. The quality of
Nash equilibria is determined by the price of anarchy ($PoA$) which expresses
how much worse is the social outcome in the worst equilibrium versus the
optimal coordinated solution. In the literature the predominant work has only
been on games with polynomial utility costs, where it has been proven that the
price of anarchy is bounded by the degree of the polynomial. However, no
results exist on general bounds for non-polynomial utility functions.
  Here, we consider general versions of these games in which the utility of
each resource is an arbitrary non-decreasing function of the congestion. In
particular, we consider a large family of superpolynomial utility functions
which are asymptotically larger than any polynomial. We demonstrate that for
every such function there exist games for which the price of anarchy is
unbounded and increasing with the number of players (even if they have
infinitesimal weights) while network resources remain fixed. We give tight
lower and upper bounds which show this dependence on the number of players.
Furthermore we provide an exact characterization of the $PoA$ of all congestion
games whose utility costs are bounded above by a polynomial function.
Heretofore such results existed only for games with polynomial cost functions."
"Although production is an integral part of the Arrow-Debreu market model,
most of the work in theoretical computer science has so far concentrated on
markets without production, i.e., the exchange economy. This paper takes a
significant step towards understanding computational aspects of markets with
production.
  We first define the notion of separable, piecewise-linear concave (SPLC)
production by analogy with SPLC utility functions. We then obtain a linear
complementarity problem (LCP) formulation that captures exactly the set of
equilibria for Arrow-Debreu markets with SPLC utilities and SPLC production,
and we give a complementary pivot algorithm for finding an equilibrium. This
settles a question asked by Eaves in 1975 of extending his complementary pivot
algorithm to markets with production.
  Since this is a path-following algorithm, we obtain a proof of membership of
this problem in PPAD, using Todd, 1976. We also obtain an elementary proof of
existence of equilibrium (i.e., without using a fixed point theorem),
rationality, and oddness of the number of equilibria. We further give a proof
of PPAD-hardness for this problem and also for its restriction to markets with
linear utilities and SPLC production. Experiments show that our algorithm runs
fast on randomly chosen examples, and unlike previous approaches, it does not
suffer from issues of numerical instability. Additionally, it is strongly
polynomial when the number of goods or the number of agents and firms is
constant. This extends the result of Devanur and Kannan (2008) to markets with
production.
  Finally, we show that an LCP-based approach cannot be extended to PLC
(non-separable) production, by constructing an example which has only
irrational equilibria."
"We prove the existence of approximate correlated equilibrium of support size
polylogarithmic in the number of players and the number of actions per player.
In particular, using the probabilistic method, we show that there exists a
multiset of polylogarithmic size such that the uniform distribution over this
multiset forms an approximate correlated equilibrium. Along similar lines, we
establish the existence of approximate coarse correlated equilibrium with
logarithmic support.
  We complement these results by considering the computational complexity of
determining small-support approximate equilibria. We show that random sampling
can be used to efficiently determine an approximate coarse correlated
equilibrium with logarithmic support. But, such a tight result does not hold
for correlated equilibrium, i.e., sampling might generate an approximate
correlated equilibrium of support size \Omega(m) where m is the number of
actions per player. Finally, we show that finding an exact correlated
equilibrium with smallest possible support is NP-hard under Cook reductions,
even in the case of two-player zero-sum games."
"One of the long-debated issues in coalitional game theory is how to extend
the Shapley value to games with externalities (partition-function games). When
externalities are present, not only can a player's marginal contribution - a
central notion to the Shapley value - be defined in a variety of ways, but it
is also not obvious which axiomatization should be used. Consequently, a number
of authors extended the Shapley value using complex and often unintuitive
axiomatizations. Furthermore, no algorithm to approximate any extension of the
Shapley value to partition-function games has been proposed to date. Given this
background, we prove in this paper that, for any well-defined measure of
marginal contribution, Shapley's original four axioms imply a unique value for
games with externalities. As an consequence of this general theorem, we show
that values proposed by Macho-Stadler et al., McQuillin and Bolger can be
derived from Shapley's axioms. Building upon our analysis of marginal
contribution, we develop a general algorithm to approximate extensions of the
Shapley value to games with externalities using a Monte Carlo simulation
technique."
"In this paper, we study online double auctions, where multiple sellers and
multiple buyers arrive and depart dynamically to exchange one commodity. We
show that there is no deterministic online double auction that is truthful and
competitive for maximising social welfare in an adversarial model. However,
given the prior information that sellers are patient and the demand is not more
than the supply, a deterministic and truthful greedy mechanism is actually
2-competitive, i.e. it guarantees that the social welfare of its allocation is
at least half of the optimal one achievable offline. Moreover, if the number of
incoming buyers is predictable, we demonstrate that an online double auction
can be reduced to an online one-sided auction, and the truthfulness and
competitiveness of the reduced online double auction follow that of the online
one-sided auction. Notably, by using the reduction, we find a truthful
mechanism that is almost 1-competitive, when buyers arrive randomly. Finally,
we argue that these mechanisms also have a promising applicability in more
general settings without assuming that sellers are patient, by decomposing a
market into multiple sub-markets."
"There is a common belief that humans and many animals follow transitive
inference (choosing A over C on the basis of knowing that A is better than B
and B is better than C). Transitivity seems to be the essence of rational
choice. We present a theoretical model of a repeated game in which the players
make a choice between three goods (e.g. food). The rules of the game refer to
the simple procedure of fair division among two players, known as the ""I cut,
you choose"" mechanism which has been widely discussed in the literature. In
this game one of the players has to make intransitive choices in order to
achieve the optimal result (for him/her and his/her co-player). The point is
that an intransitive choice can be rational. Previously, an increase in the
significance of intransitive strategies was achieved by referring to models of
quantum games. We show that \textit{relevant intransitive strategies} also
appear in the classic description of decision algorithms."
"Bidding games are extensive form games, where in each turn players bid in
order to determine who will play next. Zero-sum bidding games (also known as
Richman games) have been extensively studied, focusing on the fraction of the
initial budget that can guaranty the victory of each player [Lazarus et al.'99,
Develin and Payne'10].
  We extend the theory of bidding games to general-sum two player games,
showing the existence of pure subgame-perfect Nash equilibria (PSPE), and
studying their properties.
  We show that if the underlying game has the form of a binary tree (only two
actions available to the players in each node), then there exists a natural
PSPE with the following highly desirable properties: (a) players' utility is
weakly monotone in their budget; (b) a Pareto-efficient outcome is reached for
any initial budget; and (c) for any Pareto-efficient outcome there is an
initial budget s.t. this outcome is attained. In particular, we can assign the
budget so as to implement the outcome with maximum social welfare, maximum
Egalitarian welfare, etc.
  We show implications of this result for combinatorial bargaining. In
particular, we show that the PSPE above is fair, in the sense that a player
with a fraction of X% of the total budget prefers her allocation to X% of the
possible allocations.
  In addition, we discuss the computational challenges of bidding games, and
provide a polynomial-time algorithm to compute the PSPE."
"Central results in economics guarantee the existence of efficient equilibria
for various classes of markets. An underlying assumption in early work is that
agents are price-takers, i.e., agents honestly report their true demand in
response to prices. A line of research in economics, initiated by Hurwicz
(1972), is devoted to understanding how such markets perform when agents are
strategic about their demands. This is captured by the \emph{Walrasian
Mechanism} that proceeds by collecting reported demands, finding clearing
prices in the \emph{reported} market via an ascending price t\^{a}tonnement
procedure, and returns the resulting allocation. Similar mechanisms are used,
for example, in the daily opening of the New York Stock Exchange and the call
market for copper and gold in London.
  In practice, it is commonly observed that agents in such markets reduce their
demand leading to behaviors resembling bargaining and to inefficient outcomes.
We ask how inefficient the equilibria can be. Our main result is that the
welfare of every pure Nash equilibrium of the Walrasian mechanism is at least
one quarter of the optimal welfare, when players have gross substitute
valuations and do not overbid. Previous analysis of the Walrasian mechanism
have resorted to large market assumptions to show convergence to efficiency in
the limit. Our result shows that approximate efficiency is guaranteed
regardless of the size of the market."
"Game theory studies situations in which strategic players can modify the
state of a given system, due to the absence of a central authority. Solution
concepts, such as Nash equilibrium, are defined to predict the outcome of such
situations. In multi-player settings, it has been pointed out that to be
realistic, a solution concept should be obtainable via processes that are
decentralized and reasonably simple. Accordingly we look at the computation of
solution concepts by means of decentralized dynamics. These are algorithms in
which players move in turns to improve their own utility and the hope is that
the system reaches an ""equilibrium"" quickly.
  We study these dynamics for the class of opinion games, recently introduced
by Bindel et al. [Bindel et al., FOCS2011]. These are games, important in
economics and sociology, that model the formation of an opinion in a social
network. We study best-response dynamics and show upper and lower bounds on the
convergence to Nash equilibria. We also study a noisy version of best-response
dynamics, called logit dynamics, and prove a host of results about its
convergence rate as the noise in the system varies. To get these results, we
use a variety of techniques developed to bound the mixing time of Markov
chains, including coupling, spectral characterizations and bottleneck ratio."
"Nash equilibrium is used as a model to explain the observed behavior of
players in strategic settings. For example, in many empirical applications we
observe player behavior, and the problem is to determine if there exist payoffs
for the players for which the equilibrium corresponds to observed player
behavior. Computational complexity of Nash equilibria is an important
consideration in this framework. If the instance of the model that explains
observed player behavior requires players to have solved a computationally hard
problem, then the explanation provided is questionable. In this paper we
provide conditions under which Nash equilibrium is a reasonable explanation for
strategic behavior, i.e., conditions under which observed behavior of players
can be explained by games in which Nash equilibria are easy to compute. We
identify three structural conditions and show that if the data set of observed
behavior satisfies any of these conditions, then it is consistent with payoff
matrices for which the observed Nash equilibria could have been computed
efficiently. Our conditions admit large and structurally complex data sets of
observed behavior, showing that even with complexity considerations, Nash
equilibrium is often a reasonable model."
"We introduce draft auctions, which is a sequential auction format where at
each iteration players bid for the right to buy items at a fixed price. We show
that draft auctions offer an exponential improvement in social welfare at
equilibrium over sequential item auctions where predetermined items are
auctioned at each time step. Specifically, we show that for any subadditive
valuation the social welfare at equilibrium is an $O(\log^2(m))$-approximation
to the optimal social welfare, where $m$ is the number of items. We also
provide tighter approximation results for several subclasses. Our welfare
guarantees hold for Bayes-Nash equilibria and for no-regret learning outcomes,
via the smooth-mechanism framework. Of independent interest, our techniques
show that in a combinatorial auction setting, efficiency guarantees of a
mechanism via smoothness for a very restricted class of cardinality valuations,
extend with a small degradation, to subadditive valuations, the largest
complement-free class of valuations. Variants of draft auctions have been used
in practice and have been experimentally shown to outperform other auctions.
Our results provide a theoretical justification."
"Artificial agents are typically oriented to the realization of an externally
assigned task and try to optimize over secondary aspects of plan execution such
time lapse or power consumption, technically displaying a quasi-dichotomous
preference relation. Boolean games have been developed as a paradigm for
modelling societies of agents with this type of preference. In boolean games
agents exercise control over propositional variables and strive to achieve a
goal formula whose realization might require the opponents' cooperation.
Recently, a theory of incentive engineering for such games has been devised,
where an external authority steers the outcome of the game towards certain
desirable properties consistent with players' goals, by imposing a taxation
mechanism on the players that makes the outcomes that do not comply with those
properties less appealing to them. The present contribution stems from a
complementary perspective and studies, instead, how games with
quasi-dichotomous preferences can be transformed from inside, rather than from
outside, by endowing players with the possibility of sacrificing a part of
their payoff received at a certain outcome in order to convince other players
to play a certain strategy. Concretely we explore the properties of endogenous
games with goals, obtained coupling strategic games with goals, a
generalization of boolean games, with the machinery of endogenous games coming
from game theory. We analyze equilibria in those structures, showing the
preconditions needed for desirable outcomes to be achieved without external
intervention. What our results show is that endogenous games with goals display
specific irreducible features - with respect to what already known for
endogenous games - which makes them worth studying in their own sake."
"Two-player games on graphs provide the theoretical frame- work for many
important problems such as reactive synthesis. While the traditional study of
two-player zero-sum games has been extended to multi-player games with several
notions of equilibria, they are decidable only for perfect-information games,
whereas several applications require imperfect-information games. In this paper
we propose a new notion of equilibria, called doomsday equilibria, which is a
strategy profile such that all players satisfy their own objective, and if any
coalition of players deviates and violates even one of the players objective,
then the objective of every player is violated. We present algorithms and
complexity results for deciding the existence of doomsday equilibria for
various classes of omega-regular objectives, both for imperfect-information
games, and for perfect-information games. We provide optimal complexity bounds
for imperfect-information games, and in most cases for perfect-information
games."
"We introduce the notion of Local Computation Mechanism Design - designing
game theoretic mechanisms which run in polylogarithmic time and space. Local
computation mechanisms reply to each query in polylogarithmic time and space,
and the replies to different queries are consistent with the same global
feasible solution. In addition, the computation of the payments is also done in
polylogarithmic time and space. Furthermore, the mechanisms need to maintain
incentive compatibility with respect to the allocation and payments.
  We present local computation mechanisms for a variety of classical
game-theoretical problems: 1. stable matching, 2. job scheduling, 3.
combinatorial auctions for unit-demand and k-minded bidders, and 4. the housing
allocation problem.
  For stable matching, some of our techniques may have general implications.
Specifically, we show that when the men's preference lists are bounded, we can
achieve an arbitrarily good approximation to the stable matching within a fixed
number of iterations of the Gale-Shapley algorithm."
"An overview of different variants of the submodular welfare maximization
problem in combinatorial auctions. In particular, I studied the existing
algorithmic and game theoretic results for submodular welfare maximization
problem and its applications in other areas such as social networks."
"We introduce the prediction value (PV) as a measure of players' informational
importance in probabilistic TU games. The latter combine a standard TU game and
a probability distribution over the set of coalitions. Player $i$'s prediction
value equals the difference between the conditional expectations of $v(S)$ when
$i$ cooperates or not. We characterize the prediction value as a special member
of the class of (extended) values which satisfy anonymity, linearity and a
consistency property. Every $n$-player binomial semivalue coincides with the PV
for a particular family of probability distributions over coalitions. The PV
can thus be regarded as a power index in specific cases. Conversely, some
semivalues -- including the Banzhaf but not the Shapley value -- can be
interpreted in terms of informational importance."
"One of the earliest agent-based economical models, Schelling's spacial
proximity model illustrated how global segregation can emerge, often unwanted,
from the actions of agents of two races acting in accordance with their
individual local preferences. Here a 1-dimensional unperturbed variant of the
model is studied, which is additionally open in the sense that agents may enter
and exit the model. Following the authors' previous work in [1] and that of
Brandt, Immorlica, Kamath, and Kleinberg in [2], rigorous results are
established, whose statements are asymptotic in both the model and
neighbourhood sizes.
  The current model's openness allows one race or the other to take over almost
everywhere in a measure-theoretic sense. Tipping points are identified between
the two regions of takeover and the region of staticity, in terms of the
parameters of the model. In a significant generalization from previous work,
the parameters comprise the initial proportions of the two races, along with
independent values of the tolerance for each race."
"We consider optimal mechanism design for the case with one buyer and two
items. The buyer's valuations towards the two items are independent and
additive. In this setting, optimal mechanism is unknown for general valuation
distributions. We obtain two categories of structural results that shed light
on the optimal mechanisms.
  The first category of results state that, under certain mild condition, the
optimal mechanism has a monotone menu. In other words, in the menu that
represents the optimal mechanism, as payment increases, the allocation
probabilities for both items increase simultaneously. Applying this theorem, we
derive a version of revenue monotonicity theorem that states stochastically
superior distributions yield more revenue. Moreover, our theorem subsumes a
previous result regarding sufficient conditions under which bundling is
optimal. The second category of results state that, under certain conditions,
the optimal mechanisms have few menu items. Our first result in this category
says, for certain distributions, the optimal menu contains at most 4 items. The
condition admits power (including uniform) density functions. Based on a
similar proof of this result, we are able to obtain a wide class of
distributions where bundling is optimal. Our second result in this category
works for a weaker condition, under which the optimal menu contains at most 6
items. This condition includes exponential density functions. Our last result
in this category works for unit-demand setting. It states that, for uniform
distributions, the optimal menu contains at most 5 items. All these results are
in sharp contrast to Hart and Nisan's recent result that finite-sized menu
cannot guarantee any positive fraction of optimal revenue for correlated
valuation distributions."
"Recently, a novel class of incentive mechanisms is proposed to attract
extensive users to truthfully participate in crowd sensing applications with a
given budget constraint. The class mechanisms also bring good service quality
for the requesters in crowd sensing applications. Although it is so important,
there still exists many verification and privacy challenges, including users'
bids and subtask information privacy and identification privacy, winners' set
privacy of the platform, and the security of the payment outcomes. In this
paper, we present a privacy-preserving verifiable incentive mechanism for crowd
sensing applications with the budget constraint, not only to explore how to
protect the privacies of users and the platform, but also to make the
verifiable payment correct between the platform and users for crowd sensing
applications. Results indicate that our privacy-preserving verifiable incentive
mechanism achieves the same results as the generic one without privacy
preservation."
"We incorporate signaling scheme into Ad Auction setting, to achieve better
welfare and revenue while protect users' privacy. We propose a new
\emph{$K$-anonymous signaling scheme setting}, prove the hardness of the
corresponding welfare/revenue maximization problem, and finally propose the
algorithms to approximate the optimal revenue or welfare."
"We study the problem of finding robust equilibria in multiplayer concurrent
games with mean payoff objectives. A $(k,t)$-robust equilibrium is a strategy
profile such that no coalition of size $k$ can improve the payoff of one its
member by deviating, and no coalition of size $t$ can decrease the payoff of
other players. We are interested in pure equilibria, that is, solutions that
can be implemented using non-randomized strategies. We suggest a general
transformation from multiplayer games to two-player games such that pure
equilibria in the first game correspond to winning strategies in the second
one. We then devise from this transformation, an algorithm which computes
equilibria in mean-payoff games. Robust equilibria in mean-payoff games reduce
to winning strategies in multidimensional mean-payoff games for some threshold
satisfying some constraints. We then show that the existence of such equilibria
can be decided in polynomial space, and that the decision problem is
PSPACE-complete."
"We consider {\em profit-maximization} problems for {\em combinatorial
auctions} with {\em non-single minded valuation functions} and {\em limited
supply}.
  We obtain fairly general results that relate the approximability of the
profit-maximization problem to that of the corresponding {\em
social-welfare-maximization} (SWM) problem, which is the problem of finding an
allocation $(S_1,\ldots,S_n)$ satisfying the capacity constraints that has
maximum total value $\sum_j v_j(S_j)$. For {\em subadditive valuations} (and
hence {\em submodular, XOS valuations}), we obtain a solution with profit
$\OPT_\swm/O(\log c_{\max})$, where $\OPT_\swm$ is the optimum social welfare
and $c_{\max}$ is the maximum item-supply; thus, this yields an $O(\log
c_{\max})$-approximation for the profit-maximization problem. Furthermore,
given {\em any} class of valuation functions, if the SWM problem for this
valuation class has an LP-relaxation (of a certain form) and an algorithm
""verifying"" an {\em integrality gap} of $\al$ for this LP, then we obtain a
solution with profit $\OPT_\swm/O(\al\log c_{\max})$, thus obtaining an
$O(\al\log c_{\max})$-approximation.
  For the special case, when the tree is a path, we also obtain an incomparable
$O(\log m)$-approximation (via a different approach) for subadditive
valuations, and arbitrary valuations with unlimited supply. Our approach for
the latter problem also gives an $\frac{e}{e-1}$-approximation algorithm for
the multi-product pricing problem in the Max-Buy model, with limited supply,
improving on the previously known approximation factor of 2."
"We study the problem of computing an $\epsilon$-Nash equilibrium in repeated
games. Earlier work by Borgs et al. [2010] suggests that this problem is
intractable. We show that if we make a slight change to their model---modeling
the players as polynomial-time Turing machines that maintain state ---and make
some standard cryptographic hardness assumptions (the existence of public-key
encryption), the problem can actually be solved in polynomial time. Our
algorithm works not only for games with a finite number of players, but also
for constant-degree graphical games.
  As Nash equilibrium is a weak solution concept for extensive form games, we
additionally define and study an appropriate notion of a subgame-perfect
equilibrium for computationally bounded players, and show how to efficiently
find such an equilibrium in repeated games (again, making standard
cryptographic hardness assumptions)."
"We investigate computational and mechanism design aspects of scarce resource
allocation, where the primary rationing mechanism is through waiting times.
Specifically we consider allocating medical treatments to a population of
patients. Each patient needs exactly one treatment, and can choose from $k$
hospitals. Hospitals have different costs, which are fully paid by a third
party ---the ""payer"". The payer has a fixed budget $B$, and each hospital will
have its own waiting time. At equilibrium, each patient will choose his most
preferred hospital given his intrinsic preferences and the waiting times. The
payer thus computes the waiting times so that at equilibrium the budget
constraint is satisfied and the social welfare is maximized.
  We first show that the optimization problem is NP-hard, yet if the budget can
be relaxed to $(1+\epsilon)B$ for an arbitrarily small $\epsilon$, then the
optimum under budget $B$ can be approximated efficiently. Next, we study the
endogenous emergence of waiting time from the dynamics between hospitals and
patients, and show that there is no need for the payer to explicitly enforce
the optimal waiting times. Under certain conditions, all he need is to enforce
the amount of money he wants to pay to each hospital. The dynamics will always
converge to the desired waiting times in finite time.
  We then go beyond equilibrium solutions and investigate the optimization
problem over a much larger class of mechanisms containing the equilibrium ones
as special cases. With two hospitals, we show that under a natural assumption
on the patients' preference profiles, optimal welfare is in fact attained by
the randomized assignment mechanism, which allocates patients to hospitals at
random subject to the budget constraint, but avoids waiting times.
  Finally, we discuss potential policy implications of our results, as well as
follow-up directions and open problems."
"Many websites rely on user-generated content to provide value to consumers.
These websites typically incentivize participation by awarding users badges
based on their contributions. While these badges typically have no explicit
value, they act as symbols of social status within a community. In this paper,
we consider the design of badge mechanisms for the objective of maximizing the
total contributions made to a website. Users exert costly effort to make
contributions and, in return, are awarded with badges. A badge is only valued
to the extent that it signals social status and thus badge valuations are
determined endogenously by the number of users who earn each badge. The goal of
this paper is to study the design of optimal and approximately badge mechanisms
under these status valuations. We characterize badge mechanisms by whether they
use a coarse partitioning scheme, i.e. awarding the same badge to many users,
or use a fine partitioning scheme, i.e. awarding a unique badge to most users.
We find that the optimal mechanism uses both fine partitioning and coarse
partitioning. When status valuations exhibit a decreasing marginal value
property, we prove that coarse partitioning is a necessary feature of any
approximately optimal mechanism. Conversely, when status valuations exhibit an
increasing marginal value property, we prove that fine partitioning is
necessary for approximate optimality."
"We study the Price of Anarchy of simultaneous first-price auctions for buyers
with submodular and subadditive valuations. The current best upper bounds for
the Bayesian Price of Anarchy of these auctions are e/(e-1) [Syrgkanis and
Tardos 2013] and 2 [Feldman et al. 2013], respectively. We provide matching
lower bounds for both cases even for the case of full information and for mixed
Nash equilibria via an explicit construction.
  We present an alternative proof of the upper bound of e/(e-1) for first-price
auctions with fractionally subadditive valuations which reveals the worst-case
price distribution, that is used as a building block for the matching lower
bound construction.
  We generalize our results to a general class of item bidding auctions that we
call bid-dependent auctions (including first-price auctions and all-pay
auctions) where the winner is always the highest bidder and each bidder's
payment depends only on his own bid.
  Finally, we apply our techniques to discriminatory price multi-unit auctions.
We complement the results of [de Keijzer et al. 2013] for the case of
subadditive valuations, by providing a matching lower bound of 2. For the case
of submodular valuations, we provide a lower bound of 1.109. For the same class
of valuations, we were able to reproduce the upper bound of e/(e-1) using our
non-smooth approach."
"Network interdiction can be viewed as a game between two players, an
""interdictor"" and a ""flow player"". The flow player wishes to send as much
material as possible through a network, while the interdictor attempts to
minimize the amount of transported material by removing a certain number of
arcs, say $\Gamma$ arcs. We introduce the randomized network interdiction
problem that allows the interdictor to use randomness to select arcs to be
removed. We model the problem in two different ways: arc-based and path-based
formulations, depending on whether flows are defined on arcs or paths,
respectively. We present insights into the modeling power, complexity, and
approximability of both formulations. In particular, we prove that
$Z_{\text{NI}}/Z_{\text{RNI}}\leq \Gamma+1$,
$Z_{\text{NI}}/Z_{\text{RNI}}^{\text{Path}}\leq \Gamma+1$,
$Z_{\text{RNI}}/Z_{\text{RNI}}^{\text{Path}}\leq \Gamma$, where
$Z_{\text{NI}}$, $Z_{\text{RNI}}$, and $Z_{\text{RNI}}^{\text{Path}}$ are the
optimal values of the network interdiction problem and its randomized versions
in arc-based and path-based formulations, respectively. We also show that these
bounds are tight. We show that it is NP-hard to compute the values
$Z_{\text{RNI}}$ and $Z_{\text{RNI}}^{\text{Path}}$ for a general $\Gamma$, but
they are computable in polynomial time when $\Gamma=1$. Further, we provide a
$(\Gamma+1)$-approximation for $Z_{\text{NI}}$, a $\Gamma$-approximation for
$Z_{\text{RNI}}$, and a $\big(1+\lfloor \Gamma/2\rfloor \cdot \lceil
\Gamma/2\rceil/(\Gamma+1)\big)$-approximation for
$Z_{\text{RNI}}^{\text{Path}}$."
"Boolean games are an expressive and natural formalism through which to
investigate problems of strategic interaction in multiagent systems. Although
they have been widely studied, almost all previous work on Nash equilibria in
Boolean games has focused on the restricted setting of pure strategies. This is
a shortcoming as finite games are guaranteed to have at least one equilibrium
in mixed strategies, but many simple games fail to have pure strategy
equilibria at all. We address this by showing that a natural decision problem
about mixed equilibria: determining whether a Boolean game has a mixed strategy
equilibrium that guarantees every player a given payoff, is NEXP-hard.
Accordingly, the $\epsilon$ variety of the problem is NEXP-complete. The proof
can be adapted to show coNEXP-hardness of a similar question: whether all Nash
equilibria of a Boolean game guarantee every player at least the given payoff."
"Motivated by cost of computation in game theory, we explore how changing the
utilities of players (changing their complexity costs) affects the outcome of a
game. We show that even if we improve a player's utility in every action
profile, his payoff in equilibrium might be lower than in the equilibrium
before the change. We provide some conditions on games that are sufficient to
ensure this does not occur. We then show how this counter-intuitive phenomenon
can explain real life phenomena such as free riding, and why this might cause
people to give signals indicating that they are not as good as they really are."
"Classical power index analysis considers the individual's ability to
influence the aggregated group decision by changing its own vote, where all
decisions and votes are assumed to be binary. In many practical applications we
have more options than either ""yes"" or ""no"". Here we generalize three important
power indices to continuous convex policy spaces. This allows the analysis of a
collection of economic problems like e.g. tax rates or spending that otherwise
would not be covered in binary models."
"Competitive equilibrium with equal incomes (CEEI) is a well known fair
allocation mechanism; however, for indivisible resources a CEEI may not exist.
It was shown in [Budish '11] that in the case of indivisible resources there is
always an allocation, called A-CEEI, that is approximately fair, approximately
truthful, and approximately efficient, for some favorable approximation
parameters. This approximation is used in practice to assign students to
classes. In this paper we show that finding the A-CEEI allocation guaranteed to
exist by Budish's theorem is PPAD-complete. We further show that finding an
approximate equilibrium with better approximation guarantees is even harder:
NP-complete."
"The undercut procedure was presented by Brams et al. [2] as a procedure for
identifying an envy-free allocation when agents have preferences over sets of
objects. They assumed that agents have strict preferences over objects and
their preferences are extended over to sets of objects via the responsive set
extension. We point out some shortcomings of the undercut procedure. We then
simplify the undercut procedure of Brams et al. [2] and show that it works
under a more general condition where agents may express indifference between
objects and they may not necessarily have responsive preferences over sets of
objects. Finally, we show that the procedure works even if agents have unequal
claims."
"We consider a multiagent system consisting of selfish and heterogeneous
agents. Its behavior is modeled by multipopulation replicator dynamics, where
payoff functions of populations are different from each other. In general,
there exist several equilibrium points in the replicator dynamics. In order to
stabilize a desirable equilibrium point, we introduce a controller called a
government which controls the behaviors of agents by offering them subsidies.
In previous work, it is assumed that the government determines the subsidies
based on the populations the agents belong to. In general, however, the
government cannot identify the members of each population. In this paper, we
assume that the government observes the action of each agent and determines the
subsidies based on the observed action profile. Then, we model the controlled
behaviors of the agents using replicator dynamics with feedback. We derive a
stabilization condition of the target equilibrium point in the replicator
dynamics."
"This paper investigates the effects of a low bound price. To do so, a popular
and empirically proven model (Stahl (89') \cite{Stahl89}) is used. The model is
extended to include an exogenously given bound on prices sellers can offer,
excluding prices below such bound. The finding are rather surprising - when the
bound is set sufficiently high expected price offered (EPO) by sellers drops
significantly. The result seem to be robust in the parameters of the model, and
driven by the information provided to consumers by such legislation step: when
the limitation is set at sufficiently high levels all consumers anticipate the
bound price, and searchers reject any price above it. As a result sellers offer
the bound price as a pure strategy."
"The Shapley value---probably the most important normative payoff division
scheme in coalitional games---has recently been advocated as a useful measure
of centrality in networks. However, although this approach has a variety of
real-world applications (including social and organisational networks,
biological networks and communication networks), its computational properties
have not been widely studied. To date, the only practicable approach to compute
Shapley value-based centrality has been via Monte Carlo simulations which are
computationally expensive and not guaranteed to give an exact answer. Against
this background, this paper presents the first study of the computational
aspects of the Shapley value for network centralities. Specifically, we develop
exact analytical formulae for Shapley value-based centrality in both weighted
and unweighted networks and develop efficient (polynomial time) and exact
algorithms based on them. We empirically evaluate these algorithms on two
real-life examples (an infrastructure network representing the topology of the
Western States Power Grid and a collaboration network from the field of
astrophysics) and demonstrate that they deliver significant speedups over the
Monte Carlo approach. For instance, in the case of unweighted networks our
algorithms are able to return the exact solution about 1600 times faster than
the Monte Carlo approximation, even if we allow for a generous 10% error margin
for the latter method."
"We consider how selfish agents are likely to share revenues derived from
maintaining connectivity between important network servers. We model a network
where a failure of one node may disrupt communication between other nodes as a
cooperative game called the vertex Connectivity Game (CG). In this game, each
agent owns a vertex, and controls all the edges going to and from that vertex.
A coalition of agents wins if it fully connects a certain subset of vertices in
the graph, called the primary vertices. Power indices measure an agents ability
to affect the outcome of the game. We show that in our domain, such indices can
be used to both determine the fair share of the revenues an agent is entitled
to, and identify significant possible points of failure affecting the
reliability of communication in the network. We show that in general graphs,
calculating the Shapley and Banzhaf power indices is #P-complete, but suggest a
polynomial algorithm for calculating them in trees. We also investigate finding
stable payoff divisions of the revenues in CGs, captured by the game theoretic
solution of the core, and its relaxations, the epsilon-core and least core. We
show a polynomial algorithm for computing the core of a CG, but show that
testing whether an imputation is in the epsilon-core is coNP-complete. Finally,
we show that for trees, it is possible to test for epsilon-core imputations in
polynomial time."
"Several power indices have been introduced in the literature in order to
measure the influence of individual committee members on the aggregated
decision. Here we ask the inverse question and aim to design voting rules for a
committee such that a given desired power distribution is met as closely as
possible. We present an exact algorithm for a large class of different power
indices based on integer linear programming. With respect to negative
approximation results we generalize the approach of Alon and Edelman who
studied power distributions for the Banzhaf index, where most of the power is
concentrated on few coordinates. It turned out that each Banzhaf vector of an
n-member committee that is near to such a desired power distribution, has to be
also near to the Banzhaf vector of a k-member committee. We show that such
Alon-Edelman type results are possible for other power indices like e.g. the
Public Good index or the Coleman index to prevent actions, while they are
principally impossible for e.g. the Johnston index."
"To successfully complete a complex project, be it a construction of an
airport or of a backbone IT system, agents (companies or individuals) must form
a team having required competences and resources. A team can be formed either
by the project issuer based on individual agents' offers (centralized
formation); or by the agents themselves (decentralized formation) bidding for a
project as a consortium---in that case many feasible teams compete for the
contract. We investigate rational strategies of the agents (what salary should
they ask? with whom should they team up?). We propose concepts to characterize
the stability of the winning teams and study their computational complexity."
"The rank of a bimatrix game (A,B) is defined as rank(A+B). Computing a Nash
equilibrium (NE) of a rank-$0$, i.e., zero-sum game is equivalent to linear
programming (von Neumann'28, Dantzig'51). In 2005, Kannan and Theobald gave an
FPTAS for constant rank games, and asked if there exists a polynomial time
algorithm to compute an exact NE. Adsul et al. (2011) answered this question
affirmatively for rank-$1$ games, leaving rank-2 and beyond unresolved.
  In this paper we show that NE computation in games with rank $\ge 3$, is
PPAD-hard, settling a decade long open problem. Interestingly, this is the
first instance that a problem with an FPTAS turns out to be PPAD-hard. Our
reduction bypasses graphical games and game gadgets, and provides a simpler
proof of PPAD-hardness for NE computation in bimatrix games. In addition, we
get:
  * An equivalence between 2D-Linear-FIXP and PPAD, improving a result by
Etessami and Yannakakis (2007) on equivalence between Linear-FIXP and PPAD.
  * NE computation in a bimatrix game with convex set of Nash equilibria is as
hard as solving a simple stochastic game.
  * Computing a symmetric NE of a symmetric bimatrix game with rank $\ge 6$ is
PPAD-hard.
  * Computing a (1/poly(n))-approximate fixed-point of a (Linear-FIXP)
piecewise-linear function is PPAD-hard.
  The status of rank-$2$ games remains unresolved."
"According to the proportional allocation mechanism from the network
optimization literature, users compete for a divisible resource -- such as
bandwidth -- by submitting bids. The mechanism allocates to each user a
fraction of the resource that is proportional to her bid and collects an amount
equal to her bid as payment. Since users act as utility-maximizers, this
naturally defines a proportional allocation game. Recently, Syrgkanis and
Tardos (STOC 2013) quantified the inefficiency of equilibria in this game with
respect to the social welfare and presented a lower bound of 26.8% on the price
of anarchy over coarse-correlated and Bayes-Nash equilibria in the full and
incomplete information settings, respectively. In this paper, we improve this
bound to 50% over both equilibrium concepts. Our analysis is simpler and,
furthermore, we argue that it cannot be improved by arguments that do not take
the equilibrium structure into account. We also extend it to settings with
budget constraints where we show the first constant bound (between 36% and 50%)
on the price of anarchy of the corresponding game with respect to an effective
welfare benchmark that takes budgets into account."
"In settings with incomplete information, players can find it difficult to
coordinate to find states with good social welfare. For example, in financial
settings, if a collection of financial firms have limited information about
each other's strategies, some large number of them may choose the same
high-risk investment in hopes of high returns. While this might be acceptable
in some cases, the economy can be hurt badly if many firms make investments in
the same risky market segment and it fails. One reason why many firms might end
up choosing the same segment is that they do not have information about other
firms' investments (imperfect information may lead to `bad' game states).
Directly reporting all players' investments, however, raises confidentiality
concerns for both individuals and institutions.
  In this paper, we explore whether information about the game-state can be
publicly announced in a manner that maintains the privacy of the actions of the
players, and still suffices to deter players from reaching bad game-states. We
show that in many games of interest, it is possible for players to avoid these
bad states with the help of privacy-preserving, publicly-announced information.
We model behavior of players in this imperfect information setting in two ways
-- greedy and undominated strategic behaviours, and we prove guarantees on
social welfare that certain kinds of privacy-preserving information can help
attain. Furthermore, we design a counter with improved privacy guarantees under
continual observation."
"We consider (approximate) revenue maximization in auctions where the
distribution on input valuations is given via ""black box"" access to samples
from the distribution. We observe that the number of samples required -- the
sample complexity -- is tightly related to the representation complexity of an
approximately revenue-maximizing auction. Our main results are upper bounds and
an exponential lower bound on these complexities."
"Traditional game theory assumes that the players in the game are aware of the
rules of the game. However, in practice, often the players are unaware or have
only partial knowledge about the game they are playing. They may also have
knowledge that other players have only partial knowledge of the game they are
playing, which they can try to exploit. We present a novel mathematical
formulation of such games. We make use of Kripke semantics, which are a way to
keep track of what different players know and do not know about the world. We
propose a notion of equilibrium for such games, and show that equilibrium
always exists."
"We consider solutions of normal form games that are invariant under strategic
equivalence. We consider additional properties that can be expected (or be
desired) from a solution of a game, and we observe the following:
  - Even the weakest notion of individual rationality restricts the set of
solutions to be equilibria. This observation holds for all types of solutions:
in pure-strategies, in mixed strategies, and in correlated strategies where the
corresponding notions of equilibria are pure-Nash, Nash and coarse-correlated.
  An action profile is (strict) simultaneous maximizer if it simultaneously
globally (strictly) maximizes the payoffs of all players.
  - If we require that a simultaneous maximizer (if it exists) will be a
solution, then the solution contains the set of pure Nash equilibria.
  - There is no solution for which a strict simultaneous maximizer (if it
exists) is the unique solution."
"Tian Ji's horse racing strategy, a famous Chinese legend, constitutes a
promising concept to be applied to important issues in today's competitive
environment; this strategy is elaborated on and analyzed by examining the
general case. The mathematical formulation concerning the calculation of
winning, drawing or losing combinations and probabilities is presented to
illustrate the interesting insights on how ancient philosophies could promote
thinking in business competitiveness, in particular, the wisdom behind
sacrificing the part for the benefit of the whole or sacrificing the short-term
objectives in order to gain the long-term goal."
"Two-person bargaining problem is considered as to allocate a number of goods
between two players. This paper suggests that any non-trivial division of goods
cause a non-zero change on the solution of bargaining. So, a axiom of sharing
division is presented, as an alternative axiom to Nash axiom of independence of
irrelevant alternatives and Kalai-Smorodinsky axiom of monotonicity. This
solution is targeted at the partialities of Nash and Kalai-Smorodinsky solution
on some specific issues, but not to say it is better than others."
"Voting and assignment are two of the most fundamental settings in social
choice theory. For both settings, random serial dictatorship (RSD) is a
well-known rule that satisfies anonymity, ex post efficiency, and
strategyproofness. Recently, it was shown that computing the resulting
probabilities is #P-complete both in the voting and assignment setting. In this
paper, we study RSD from a parametrized complexity perspective. More
specifically, we present efficient algorithms to compute the RSD probabilities
under the condition that the number of agent types, alternatives, or objects is
bounded."
"We study the problem of approximate social welfare maximization (without
money) in one-sided matching problems when agents have unrestricted cardinal
preferences over a finite set of items. Random priority is a very well-known
truthful-in-expectation mechanism for the problem. We prove that the
approximation ratio of random priority is Theta(n^{-1/2}) while no
truthful-in-expectation mechanism can achieve an approximation ratio better
than O(n^{-1/2}), where n is the number of agents and items. Furthermore, we
prove that the approximation ratio of all ordinal (not necessarily
truthful-in-expectation) mechanisms is upper bounded by O(n^{-1/2}), indicating
that random priority is asymptotically the best truthful-in-expectation
mechanism and the best ordinal mechanism for the problem."
"In this work, we study the problem of online mechanism design for resources
allocation and pricing in cloud computing (RAPCC). We show that in general the
allocation problems in RAPCC are NP-hard, and therefore we focus on designing
dominant-strategy incentive compatible (DSIC) mechanisms with good competitive
ratios compared to the offline optimal allocation (with the prior knowledge
about the future jobs). We propose two kinds of DSIC online mechanisms. The
first mechanism, which is based on a greedy allocation rule and leverages a
priority function for allocation, is very fast and has a tight competitive
bound. We discuss several priority functions including exponential and linear
priority functions, and show that the former one has a better competitive
ratio. The second mechanism, which is based on a dynamic program for
allocation, also has a tight competitive ratio and performs better than the
first one when the maximum demand of cloud customers is close to the capacity
of the cloud provider."
"Prisoner's dilemma has been heavily studied. In classical model, each player
chooses to either ""Cooperate"" or ""Defect"". In this paper, we generalize the
prisoner's dilemma with a new alternative which is neither defect or
cooperation. The classical model is the special case under the condition that
the third state is not taken into consideration."
"This paper presents the ""Game Theory Explorer"" software tool to create and
analyze games as models of strategic interaction. A game in extensive or
strategic form is created and nicely displayed with a graphical user interface
in a web browser. State-of-the-art algorithms then compute all Nash equilibria
of the game after a mouseclick. In tutorial fashion, we present how the program
is used, and the ideas behind its main algorithms. We report on experiences
with the architecture of the software and its development as an open-source
project."
"The Parrondo's paradox is a counterintuitive phenomenon where
individually-losing strategies can be combined in producing a winning
expectation. In this paper, the issues surrounding the Parrondo's paradox are
investigated. The focus is lying on testifying whether the same paradoxical
effect can be reproduced by using a simple capital dependent game. The
paradoxical effect generated by the Parrondo's paradox can be explained by
placing all the parameters in one probability space. Based on this framework,
it is able to generate other possible paradoxical effects by manipulating the
parameters in the probability space."
"We relate the strategy sets that a player ends up with after refining his own
strategies according to two very different models of rationality: namely,
utility maximization and regret minimization."
"We consider auctions in which the players have very limited knowledge about
their own valuations. Specifically, the only information that a Knightian
player $i$ has about the profile of true valuations, $\theta^*$, consists of a
set of distributions, from one of which $\theta_i^*$ has been drawn.
  We analyze the social-welfare performance of the VCG mechanism, for
unrestricted combinatorial auctions, when Knightian players that either (a)
choose a regret-minimizing strategy, or (b) resort to regret minimization only
to refine further their own sets of undominated strategies, if needed. We prove
that this performance is very good."
"We consider auctions in which the players have very limited knowledge about
their own valuations. Specifically, the only information that a Knightian
player $i$ has about the profile of true valuations, $\theta^*$, consists of a
set of distributions, from one of which $\theta_i^*$ has been drawn.
  The VCG mechanism guarantees very high social welfare both in single- and
multi-good auctions, so long as Knightian players do not select strategies that
are dominated. With such Knightian players, however, we prove that the VCG
mechanism guarantees very poor social welfare in unrestricted combinatorial
auctions."
"We consider players that have very limited knowledge about their own
valuations. Specifically, the only information that a Knightian player $i$ has
about the profile of true valuations, $\theta^*$, consists of a set of
distributions, from one of which $\theta_i^*$ has been drawn.
  We prove a ``robustness'' theorem for Knightian players in single-parameter
domains: every mechanism that is weakly dominant-strategy truthful for
classical players continues to be well-behaved for Knightian players that
choose undominated strategies."
"We analyze the Vickrey mechanism for auctions of multiple identical goods
when the players have both Knightian uncertainty over their own valuations and
incomplete preferences. In this model, the Vickrey mechanism is no longer
dominant-strategy, and we prove that all dominant-strategy mechanisms are
inadequate. However, we also prove that, in undominated strategies, the social
welfare produced by the Vickrey mechanism in the worst case is not only very
good, but also essentially optimal."
"Designing fair compensation mechanisms for demand response (DR) is
challenging. This paper models the problem in a game theoretic setting and
designs a payment distribution mechanism based on the Shapley Value. As exact
computation of the Shapley Value is in general intractable, we propose
estimating it using a reinforcement learning algorithm that approximates
optimal stratified sampling. We apply this algorithm to two DR programs that
utilize the Shapley Value for payments and quantify the accuracy of the
resulting estimates."
"We study the role of a market maker (or market operator) in a transmission
constrained electricity market. We model the market as a one-shot networked
Cournot competition where generators supply quantity bids and load serving
entities provide downward sloping inverse demand functions. This mimics the
operation of a spot market in a deregulated market structure. In this paper, we
focus on possible mechanisms employed by the market maker to balance demand and
supply. In particular, we consider three candidate objective functions that the
market maker optimizes - social welfare, residual social welfare, and consumer
surplus. We characterize the existence of Generalized Nash Equilibrium (GNE) in
this setting and demonstrate that market outcomes at equilibrium can be very
different under the candidate objective functions."
"Can noncooperative behaviour of merchants lead to a market split that prima
facie seems anticompetitive? We introduce a model in which service providers,
with ISPs being the main example, aim at optimizing the number of customers
using their services, while customers aim at choosing service providers with
low customer load (high bandwidth per subscriber, for ISPs). Each service
provider chooses between a variety of levels of service (latencies, for ISPs),
and as long as it does not lose customers, aims at minimizing its level of
service; the minimum level of service required to satisfy a customer varies
across customers. We consider a two-stage competition: in the first stage,
service providers select their levels of service; in the second stage,
customers choose between service providers. In the two-stage game, we show that
the competition among service providers possesses a unique Nash equilibrium,
which is moreover super-strong; we also show that sequential better-response
dynamics of service providers reach this equilibrium, with best-response
dynamics doing so surprisingly fast. If service providers choose their levels
of service according to this equilibrium, then the unique Nash equilibrium
among customers in the second phase is a split of the market between the
service providers, based on the customers' minimum acceptable quality of
service; moreover, each service provider's chosen level of service is the
lowest acceptable by the entirety of its market slice, seemingly making no
attempt to attract other customers. Our results show that this prima facie
market allocation (collusive split of the market) arises as the unique and
highly robust outcome of noncooperative, even myopic, service-provider
behaviour. These results are applicable to a wide variety of scenarios, from
explaining phenomena observable in food markets, to shedding a surprising light
on aspects of location theory."
"Drawing intuition from a (physical) hydraulic system, we present a novel
framework, constructively showing the existence of a strong Nash equilibrium in
resource selection games (i.e., asymmetric singleton congestion games) with
nonatomic players, the coincidence of strong equilibria and Nash equilibria in
such games, and the uniqueness of the cost of each given resource across all
Nash equilibria. Our proofs allow for explicit calculation of Nash equilibrium
and for explicit and direct calculation of the resulting (unique) costs of
resources, and do not hinge on any fixed-point theorem, on the Minimax theorem
or any equivalent result, on linear programming, or on the existence of a
potential (though our analysis does provide powerful insights into the
potential, via a natural concrete physical interpretation). A generalization of
resource selection games, called resource selection games with I.D.-dependent
weighting, is defined, and the results are extended to this family, showing the
existence of strong equilibria, and showing that while resource costs are no
longer unique across Nash equilibria in games of this family, they are
nonetheless unique across all strong Nash equilibria, drawing a novel
fundamental connection between group deviation and I.D.-congestion. A natural
application of the resulting machinery to a large class of
constraint-satisfaction problems is also described."
"Top monotonicity is a relaxation of various well-known domain restrictions
such as single-peaked and single-crossing for which negative impossibility
results are circumvented and for which the median-voter theorem still holds. We
examine the problem of testing top monotonicity and present a characterization
of top monotonicity with respect to non-betweenness constraints. We then extend
the definition of top monotonicity to partial orders and show that testing top
monotonicity of partial orders is NP-complete."
"We study the price competition in a duopoly with an arbitrary number of
buyers. Each seller can offer multiple units of a commodity depending on the
availability of the commodity which is random and may be different for
different sellers. Sellers seek to select a price that will be attractive to
the buyers and also fetch adequate profits. The selection will in general
depend on the number of units available with the seller and also that of its
competitor - the seller may only know the statistics of the latter. The setting
captures a secondary spectrum access network, a non-neutral Internet, or a
microgrid network in which unused spectrum bands, resources of ISPs, and excess
power units constitute the respective commodities of sale. We analyze this
price competition as a game, and identify a set of necessary and sufficient
properties for the Nash Equilibrium (NE). The properties reveal that sellers
randomize their price using probability distributions whose support sets are
mutually disjoint and in decreasing order of the number of availability. We
prove the uniqueness of a symmetric NE in a symmetric market, and explicitly
compute the price distribution in the symmetric NE."
"When there is a dispute between players on how to divide multiple divisible
assets, how should it be resolved? In this paper we introduce a multi-asset
game model that enables cooperation between multiple agents who bargain on
sharing K assets, when each player has a different value for each asset. It
thus extends the sequential discrete Raiffa solution and the Talmud rule
solution to multi-asset cases. keyword: resource allocation, game theory,
Raiffa Bargaining Solution, Aumann Bankruptcy, non-transferable commodities"
"An heuristic approach to compute strong Nash (Aumann) equilibria is
presented. The method is based on differential evolution and three variants of
a generative relation for strong Nash equilibria characterization. Numerical
experiments performed on the minimum effort game for up to 150 players
illustrate the efficiency of the approach. The advantages and disadvantages of
each variant is discussed in terms of precision and running time."
"Berge equilibrium in the sense of Zhukovskii (Berge-Zhukovskii) is an
alternate solution concept in non-cooperative game theory that formalizes
cooperation in a noncooperative setting. In this paper the
epsilon-Berge-Zhukovskii equilibrium is introduced and characterized by using a
generative relation. A computational method for detecting
epsilon-Berge-Zhukovskii equilibrium based on evolutionary multiobjective
optimization algorithms is presented. Numerical examples are used to illustrate
the results obtained."
"We prove that finding an epsilon-Nash equilibrium in a succinctly
representable game with many players is PPAD-hard for constant epsilon. Our
proof uses succinct games, i.e. games whose payoff function is represented by a
circuit. Our techniques build on a recent query complexity lower bound by
Babichenko."
"For the classical power indices there is a disproportion between power and
relative weights, in general. We introduce two new indices, based on weighted
representations, which are proportional to suitable relative weights and which
also share several important properties of the classical power indices.
Imposing further restrictions on the set of representations may lead to a whole
family of such indices."
"Power index research has been a very active field in the last decades. Will
this continue or are all the important questions solved? We argue that there
are still many opportunities to conduct useful research with and on power
indices. Positive and normative questions keep calling for theoretical and
empirical attention. Technical and technological improvements are likely to
boost applicability."
"Secure equilibrium is a refinement of Nash equilibrium, which provides some
security to the players against deviations when a player changes his strategy
to another best response strategy. The concept of secure equilibrium is
specifically developed for assume-guarantee synthesis and has already been
applied in this context. Yet, not much is known about its existence in games
with more than two players. In this paper, we establish the existence of secure
equilibrium in two classes of multi-player perfect information turn-based
games: (1) in games with possibly probabilistic transitions, having countable
state and finite action spaces and bounded and continuous payoff functions, and
(2) in games with only deterministic transitions, having arbitrary state and
action spaces and Borel payoff functions with a finite range (in particular,
qualitative Borel payoff functions). We show that these results apply to
several types of games studied in the literature."
"Cournot competition is a fundamental economic model that represents firms
competing in a single market of a homogeneous good. Each firm tries to maximize
its utility---a function of the production cost as well as market price of the
product---by deciding on the amount of production. In today's dynamic and
diverse economy, many firms often compete in more than one market
simultaneously, i.e., each market might be shared among a subset of these
firms. In this situation, a bipartite graph models the access restriction where
firms are on one side, markets are on the other side, and edges demonstrate
whether a firm has access to a market or not. We call this game \emph{Network
Cournot Competition} (NCC). In this paper, we propose algorithms for finding
pure Nash equilibria of NCC games in different situations. First, we carefully
design a potential function for NCC, when the price functions for markets are
linear functions of the production in that market. However, for nonlinear price
functions, this approach is not feasible. We model the problem as a nonlinear
complementarity problem in this case, and design a polynomial-time algorithm
that finds an equilibrium of the game for strongly convex cost functions and
strongly monotone revenue functions. We also explore the class of price
functions that ensures strong monotonicity of the revenue function, and show it
consists of a broad class of functions. Moreover, we discuss the uniqueness of
equilibria in both of these cases which means our algorithms find the unique
equilibria of the games. Last but not least, when the cost of production in one
market is independent from the cost of production in other markets for all
firms, the problem can be separated into several independent classical
\emph{Cournot Oligopoly} problems. We give the first combinatorial algorithm
for this widely studied problem."
"In this paper we consider a mechanism design problem in the context of
large-scale crowdsourcing markets such as Amazon's Mechanical Turk,
ClickWorker, CrowdFlower. In these markets, there is a requester who wants to
hire workers to accomplish some tasks. Each worker is assumed to give some
utility to the requester. Moreover each worker has a minimum cost that he wants
to get paid for getting hired. This minimum cost is assumed to be private
information of the workers. The question then is - if the requester has a
limited budget, how to design a direct revelation mechanism that picks the
right set of workers to hire in order to maximize the requester's utility.
  We note that although the previous work has studied this problem, a crucial
difference in which we deviate from earlier work is the notion of large-scale
markets that we introduce in our model. Without the large market assumption, it
is known that no mechanism can achieve an approximation factor better than
0.414 and 0.5 for deterministic and randomized mechanisms respectively (while
the best known deterministic and randomized mechanisms achieve an approximation
ratio of 0.292 and 0.33 respectively). In this paper, we design a
budget-feasible mechanism for large markets that achieves an approximation
factor of 1-1/e (i.e. almost 0.63). Our mechanism can be seen as a
generalization of an alternate way to look at the proportional share mechanism
which is used in all the previous works so far on this problem. Interestingly,
we also show that our mechanism is optimal by showing that no truthful
mechanism can achieve a factor better than 1-1/e; thus, fully resolving this
setting. Finally we consider the more general case of submodular utility
functions and give new and improved mechanisms for the case when the markets
are large."
"Sponsored search auctions constitute one of the most successful applications
of microeconomic mechanisms. In mechanism design, auctions are usually designed
to incentivize advertisers to bid their truthful valuations and to assure both
the advertisers and the auctioneer a non-negative utility. Nonetheless, in
sponsored search auctions, the click-through-rates (CTRs) of the advertisers
are often unknown to the auctioneer and thus standard truthful mechanisms
cannot be directly applied and must be paired with an effective learning
algorithm for the estimation of the CTRs. This introduces the critical problem
of designing a learning mechanism able to estimate the CTRs at the same time as
implementing a truthful mechanism with a revenue loss as small as possible
compared to an optimal mechanism designed with the true CTRs. Previous work
showed that, when dominant-strategy truthfulness is adopted, in single-slot
auctions the problem can be solved using suitable exploration-exploitation
mechanisms able to achieve a per-step regret (over the auctioneer's revenue) of
order $O(T^{-1/3})$ (where T is the number of times the auction is repeated).
It is also known that, when truthfulness in expectation is adopted, a per-step
regret (over the social welfare) of order $O(T^{-1/2})$ can be obtained. In
this paper we extend the results known in the literature to the case of
multi-slot auctions. In this case, a model of the user is needed to
characterize how the advertisers' valuations change over the slots. We adopt
the cascade model that is the most famous model in the literature for sponsored
search auctions. We prove a number of novel upper bounds and lower bounds both
on the auctioneer's revenue loss and social welfare w.r.t. to the VCG auction
and we report numerical simulations investigating the accuracy of the bounds in
predicting the dependency of the regret on the auction parameters."
"This paper introduces algorithm instance games (AIGs) as a conceptual
classification applying to games in which outcomes are resolved from joint
strategies algorithmically. For such games, a fundamental question asks: How do
the details of the algorithm's description influence agents' strategic
behavior?
  We analyze two versions of an AIG based on the set-cover optimization
problem. In these games, joint strategies correspond to instances of the
set-cover problem, with each subset (of a given universe of elements)
representing the strategy of a single agent. Outcomes are covers computed from
the joint strategies by a set-cover algorithm. In one variant of this game,
outcomes are computed by a deterministic greedy algorithm, and the other
variant utilizes a non-deterministic form of the greedy algorithm. We
characterize Nash equilibrium strategies for both versions of the game, finding
that agents' strategies can vary considerably between the two settings. In
particular, we find that the version of the game based on the deterministic
algorithm only admits Nash equilibrium in which agents choose strategies (i.e.,
subsets) containing at most one element, with no two agents picking the same
element. On the other hand, in the version of the game based on the
non-deterministic algorithm, Nash equilibrium strategies can include agents
with zero, one, or every element, and the same element can appear in the
strategies of multiple agents."
"We prove that finding an $\epsilon$-approximate Nash equilibrium is
PPAD-complete for constant $\epsilon$ and a particularly simple class of games:
polymatrix, degree 3 graphical games, in which each player has only two
actions.
  As corollaries, we also prove similar inapproximability results for Bayesian
Nash equilibrium in a two-player incomplete information game with a constant
number of actions, for relative $\epsilon$-Well Supported Nash Equilibrium in a
two-player game, for market equilibrium in a non-monotone market, for the
generalized circuit problem defined by Chen, Deng, and Teng [CDT'09], and for
approximate competitive equilibrium from equal incomes with indivisible goods."
"This paper studies the Nash stability in hedonic coalition formation games.
We address the following issue: for a general problem formulation, is there any
utility allocation method ensuring a Nash-stable partition? We propose the
definition of the Nash-stable core and we analyze the conditions for having a
non-empty Nash-stable core. More precisely, we prove that using relaxed
efficiency in utility sharing allows to ensure a non empty Nash-stable core.
Then, a decentralized algorithm called Nash stability establisher is proposed
for finding the Nash stability in a game whenever at least one exists. The
problem of finding the Nash stability is formulated as a non-cooperative game.
In the proposed approach, during each round, each player determines its
strategy in its turn according to a random round-robin scheduler. We prove that
the algorithm converges to an equilibrium if it exists, which is the indicator
of the Nash stability."
"In this paper we propose the opinion leader-follower through mediators
systems (OLFM systems) a multiple-action collective choice model for societies.
In those societies three kind of actors are considered: opinion leaders that
can exert certain influence over the decision of other actors, followers that
can be convinced to modify their original decisions, and independent actors
that neither are influenced nor can influence; mediators are actors that both
are influenced and influence other actors. This is a generalization of the
opinion leader-follower systems (OLF systems) proposed by van den Brink R, et
al. (2011).
  The satisfaction score is defined on the set of actors. For each actor it
measures the number of society initial decisions in which the final collective
decision coincides with the one that the actor initially selected. We
generalize in OLFM systems some properties that the satisfaction score meets
for OLF systems. By using these properties, we provide an axiomatization of the
satisfaction score for the case in which followers maintain their own initial
decisions unless all their opinion leaders share an opposite inclination. This
new axiomatization generalizes the one given by van den Brink R, et al. (2012)
for OLF systems under the same restrictions."
"In the Possible Winner problem in computational social choice theory, we are
given a set of partial preferences and the question is whether a distinguished
candidate could be made winner by extending the partial preferences to linear
preferences. Previous work has provided, for many common voting rules, fixed
parameter tractable algorithms for the Possible Winner problem, with number of
candidates as the parameter. However, the corresponding kernelization question
is still open and in fact, has been mentioned as a key research challenge. In
this paper, we settle this open question for many common voting rules.
  We show that the Possible Winner problem for maximin, Copeland, Bucklin,
ranked pairs, and a class of scoring rules that include the Borda voting rule
do not admit a polynomial kernel with the number of candidates as the
parameter. We show however that the Coalitional Manipulation problem which is
an important special case of the Possible Winner problem does admit a
polynomial kernel for maximin, Copeland, ranked pairs, and a class of scoring
rules that includes the Borda voting rule, when the number of manipulators is
polynomial in the number of candidates. A significant conclusion of our work is
that the Possible Winner problem is harder than the Coalitional Manipulation
problem since the Coalitional Manipulation problem admits a polynomial kernel
whereas the Possible Winner problem does not admit a polynomial kernel."
"There are two major ways of selling impressions in display advertising. They
are either sold in spot through auction mechanisms or in advance via guaranteed
contracts. The former has achieved a significant automation via real-time
bidding (RTB); however, the latter is still mainly done over the counter
through direct sales. This paper proposes a mathematical model that allocates
and prices the future impressions between real-time auctions and guaranteed
contracts. Under conventional economic assumptions, our model shows that the
two ways can be seamless combined programmatically and the publisher's revenue
can be maximized via price discrimination and optimal allocation. We consider
advertisers are risk-averse, and they would be willing to purchase guaranteed
impressions if the total costs are less than their private values. We also
consider that an advertiser's purchase behavior can be affected by both the
guaranteed price and the time interval between the purchase time and the
impression delivery date. Our solution suggests an optimal percentage of future
impressions to sell in advance and provides an explicit formula to calculate at
what prices to sell. We find that the optimal guaranteed prices are dynamic and
are non-decreasing over time. We evaluate our method with RTB datasets and find
that the model adopts different strategies in allocation and pricing according
to the level of competition. From the experiments we find that, in a less
competitive market, lower prices of the guaranteed contracts will encourage the
purchase in advance and the revenue gain is mainly contributed by the increased
competition in future RTB. In a highly competitive market, advertisers are more
willing to purchase the guaranteed contracts and thus higher prices are
expected. The revenue gain is largely contributed by the guaranteed selling."
"Shapleys impossibility result indicates that the two-person bargaining
problem has no non-trivial ordinal solution with the traditional game-theoretic
bargaining model. Although the result is no longer true for bargaining problems
with more than two agents, none of the well known bargaining solutions are
ordinal. Searching for meaningful ordinal solutions, especially for the
bilateral bargaining problem, has been a challenging issue in bargaining theory
for more than three decades. This paper proposes a logic-based ordinal solution
to the bilateral bargaining problem. We argue that if a bargaining problem is
modeled in terms of the logical relation of players physical negotiation items,
a meaningful bargaining solution can be constructed based on the ordinal
structure of bargainers preferences. We represent bargainers demands in
propositional logic and bargainers preferences over their demands in total
preorder. We show that the solution satisfies most desirable logical
properties, such as individual rationality (logical version), consistency,
collective rationality as well as a few typical game-theoretic properties, such
as weak Pareto optimality and contraction invariance. In addition, if all
players demand sets are logically closed, the solution satisfies a fixed-point
condition, which says that the outcome of a negotiation is the result of mutual
belief revision. Finally, we define various decision problems in relation to
our bargaining model and study their computational complexity."
"The classic Gibbard-Satterthwaite theorem says that every strategy-proof
voting rule with at least three possible candidates must be dictatorial.
Similar impossibility results hold even if we consider a weaker notion of
strategy-proofness where voters believe that the other voters' preferences are
i.i.d.~(independent and identically distributed). In this paper, we take a
bounded-rationality approach to this problem and consider a setting where
voters have ""coarse"" beliefs (a notion that has gained popularity in the
behavioral economics literature). In particular, we construct good voting rules
that satisfy a notion of strategy-proofness with respect to coarse
i.i.d.~beliefs, thus circumventing the above impossibility results."
"We provide polynomial-time approximately optimal Bayesian mechanisms for
makespan minimization on unrelated machines as well as for max-min fair
allocations of indivisible goods, with approximation factors of $2$ and
$\min\{m-k+1, \tilde{O}(\sqrt{k})\}$ respectively, matching the approximation
ratios of best known polynomial-time \emph{algorithms} (for max-min fairness,
the latter claim is true for certain ratios of the number of goods $m$ to
people $k$). Our mechanisms are obtained by establishing a polynomial-time
approximation-sensitive reduction from the problem of designing approximately
optimal {\em mechanisms} for some arbitrary objective ${\cal O}$ to that of
designing bi-criterion approximation {\em algorithms} for the same objective
${\cal O}$ plus a linear allocation cost term. Our reduction is itself enabled
by extending the celebrated ""equivalence of separation and
optimization""[GLSS81,KP80] to also accommodate bi-criterion approximations.
Moreover, to apply the reduction to the specific problems of makespan and
max-min fairness we develop polynomial-time bi-criterion approximation
algorithms for makespan minimization with costs and max-min fairness with
costs, adapting the algorithms of [ST93], [BD05] and [AS07] to the type of
bi-criterion approximation that is required by the reduction."
"We consider a monopolist seller with $n$ heterogeneous items, facing a single
buyer. The buyer has a value for each item drawn independently according to
(non-identical) distributions, and his value for a set of items is additive.
The seller aims to maximize his revenue. It is known that an optimal mechanism
in this setting may be quite complex, requiring randomization [HR12] and menus
of infinite size [DDT13]. Hart and Nisan [HN12] have initiated a study of two
very simple pricing schemes for this setting: item pricing, in which each item
is priced at its monopoly reserve; and bundle pricing, in which the entire set
of items is priced and sold as one bundle. Hart and Nisan [HN12] have shown
that neither scheme can guarantee more than a vanishingly small fraction of the
optimal revenue. In sharp contrast, we show that for any distributions, the
better of item and bundle pricing is a constant-factor approximation to the
optimal revenue. We further discuss extensions to multiple buyers and to
valuations that are correlated across items."
"We consider the situation in which an organizer is trying to convene an
event, and needs to choose a subset of agents to be invited. Agents have
preferences over how many attendees should be at the event and possibly also
who the attendees should be. This induces a stability requirement: All invited
agents should prefer attending to not attending, and all the other agents
should not regret being not invited. The organizer's objective is to find the
invitation of maximum size subject to the stability requirement. We investigate
the computational complexity of finding the maximum stable invitation when all
agents are truthful, as well as the mechanism design problem when agents may
strategically misreport their preferences."
"Cr\'emer and McLean [1985] showed that, when buyers' valuations are drawn
from a correlated distribution, an auction with full knowledge on the
distribution can extract the full social surplus. We study whether this
phenomenon persists when the auctioneer has only incomplete knowledge of the
distribution, represented by a finite family of candidate distributions, and
has sample access to the real distribution. We show that the naive approach
which uses samples to distinguish candidate distributions may fail, whereas an
extended version of the Cr\'emer-McLean auction simultaneously extracts full
social surplus under each candidate distribution. With an algebraic argument,
we give a tight bound on the number of samples needed by this auction, which is
the difference between the number of candidate distributions and the dimension
of the linear space they span."
"Incentives are more likely to elicit desired outcomes when they are designed
based on accurate models of agents' strategic behavior. A growing literature,
however, suggests that people do not quite behave like standard economic agents
in a variety of environments, both online and offline. What consequences might
such differences have for the optimal design of mechanisms in these
environments? In this paper, we explore this question in the context of optimal
contest design for simple agents---agents who strategically reason about
whether or not to participate in a system, but not about the input they provide
to it. Specifically, consider a contest where $n$ potential contestants with
types $(q_i,c_i)$ each choose between participating and producing a submission
of quality $q_i$ at cost $c_i$, versus not participating at all, to maximize
their utilities. How should a principal distribute a total prize $V$ amongst
the $n$ ranks to maximize some increasing function of the qualities of elicited
submissions in a contest with such simple agents?
  We first solve the optimal contest design problem for settings with
homogenous participation costs $c_i = c$. Here, the optimal contest is always a
simple contest, awarding equal prizes to the top $j^*$ contestants for a
suitable choice of $j^*$. (In comparable models with strategic effort choices,
the optimal contest is either a winner-take-all contest or awards possibly
unequal prizes, depending on the curvature of agents' effort cost functions.)
We next address the general case with heterogeneous costs where agents' types
are inherently two-dimensional, significantly complicating equilibrium
analysis. Our main result here is that the winner-take-all contest is a
3-approximation of the optimal contest when the principal's objective is to
maximize the quality of the best elicited contribution."
"In this paper, we introduce a novel approach for reducing the $k$-item
$n$-bidder auction with additive valuation to $k$-item $1$-bidder auctions.
This approach, called the \emph{Best-Guess} reduction, can be applied to
address several central questions in optimal revenue auction theory such as the
power of randomization, and Bayesian versus dominant-strategy implementations.
First, when the items have independent valuation distributions, we present a
deterministic mechanism called {\it Deterministic Best-Guess} that yields at
least a constant fraction of the optimal revenue by any randomized mechanism.
Second, if all the $nk$ valuation random variables are independent, the optimal
revenue achievable in {\it dominant strategy incentive compatibility} (DSIC) is
shown to be at least a constant fraction of that achievable in {\it Bayesian
incentive compatibility} (BIC). Third, when all the $nk$ values are identically
distributed according to a common one-dimensional distribution $F$, the optimal
revenue is shown to be expressible in the closed form $\Theta(k(r+\int_0^{mr}
(1-F(x)^n) \ud x))$ where $r= sup_{x\geq 0} \, x(1 - F(x)^n)$ and $m=\lceil
k/n\rceil$; this revenue is achievable by a simple mechanism called
\emph{2nd-Price Bundling}. All our results apply to arbitrary distributions,
regular or irregular."
"The three most common school choice mechanisms are the Deferred Acceptance
mechanism (DA), the classic Boston mechanism (BM), and a variant of the Boston
mechanism where students automatically skip exhausted schools, which we call
the adaptive Boston mechanism (ABM). Assuming truthful reporting, we compare
student welfare under these mechanisms both from a conceptual and from a
quantitative perspective: We first show that, BM rank dominates DA whenever
they are comparable; and via limit arguments and simulations we show that ABM
yields intermediate student welfare between BM and DA. Second, we perform
computational experiments with preference data from the high school match in
Mexico City. We find that student welfare (in terms of rank transitions) is
highest under BM, intermediate under ABM, and lowest under DA. BM, ABM, and DA
can thus be understood to form a hierarchy in terms of student welfare. In
contrast, in (Mennle and Seuken, 2017), we have found that the same mechanisms
also form a hierarchy in terms of incentives for truthtelling that points in
the opposite direction. A decision between them therefore involves an implicit
trade-off between incentives and student welfare."
"In Cooperative Games with Externalities when the members of a set S \subset N
of agents wish to deviate they need to calculate their worth. This worth
depends on what the non-members (outsiders) N \setminus S will do, which in
turn depends on which coalition structure the outsiders will form. Since this
coalition formation problem is NP-hard, various approaches have been adopted.
In this paper using an evolutionary game theoretic approach we provide a set of
equations that can help agents in S reason about the coalition structures the
outsiders may form in terms of minimum distances on an n-s dimensional space,
where n=|N|, s=|S|."
"In the network design game with $n$ players, every player chooses a path in
an edge-weighted graph to connect her pair of terminals, sharing costs of the
edges on her path with all other players fairly. We study the price of
stability of the game, i.e., the ratio of the social costs of a best Nash
equilibrium (with respect to the social cost) and of an optimal play. It has
been shown that the price of stability of any network design game is at most
$H_n$, the $n$-th harmonic number. This bound is tight for directed graphs. For
undirected graphs, the situation is dramatically different, and tight bounds
are not known. It has only recently been shown that the price of stability is
at most $H_n \left(1-\frac{1}{\Theta(n^4)} \right)$, while the worst-case known
example has price of stability around 2.25. In this paper we improve the upper
bound considerably by showing that the price of stability is at most $H_{n/2} +
\epsilon$ for any $\epsilon$ starting from some suitable $n \geq n(\epsilon)$."
"We report the results of a computational study of repacking in the FCC
Incentive Auctions. Our interest lies in the structure and constraints of the
solution space of feasible repackings. Our analyses are ""mechanism-free"", in
the sense that they identify constraints that must hold regardless of the
reverse auction mechanism chosen or the prices offered for broadcaster
clearing. We examine topics such as the amount of spectrum that can be cleared
nationwide, the geographic distribution of broadcaster clearings required to
reach a clearing target, and the likelihood of reaching clearing targets under
various models for broadcaster participation. Our study uses FCC interference
data and a satisfiability-checking approach, and elucidates both the
unavoidable mathematical constraints on solutions imposed by interference, as
well as additional constraints imposed by assumptions on the participation
decisions of broadcasters."
"We describe a new coordination mechanism for non-atomic congestion games that
leads to a (selfish) social cost which is arbitrarily close to the non-selfish
optimal. This mechanism does not incur any additional extra cost, like tolls,
which are usually differentiated from the social cost as expressed in terms of
delays only."
"Incorporating fairness criteria in optimization problems comes at a certain
cost, which is measured by the so-called price of fairness. Here we consider
the allocation of indivisible goods. For envy-freeness as fairness criterion it
is known from literature that the price of fairness can increase linearly in
terms of the number of agents. For the constructive lower bound a quadratic
number of items was used. In practice this might be inadequately large. So we
introduce the price of fairness in terms of both the number of agents and
items, i.e., key parameters which generally may be considered as common and
available knowledge. It turned out that the price of fairness increases
sublinear if the number of items is not too much larger than the number of
agents. For the special case of coincide of both counts exact asymptotics could
be determined. Additionally an efficient integer programming formulation is
given."
"We prove existence of envy-free allocations in markets with heterogenous
indivisible goods and money, when a given quantity is supplied from each of the
goods and agents have unit demands. We depart from most of the previous
literature by allowing agents' preferences over the goods to depend on the
entire vector of prices. Our proof uses Shapley's K-K-M-S theorem and Hall's
marriage lemma. We then show how our theorem may be applied in two related
problems: Existence of envy-free allocations in a version of the cake-cutting
problem, and existence of equilibrium in an exchange economy with indivisible
goods and money."
"Prediction markets have demonstrated their value for aggregating collective
expertise. Combinatorial prediction markets allow forecasts not only on base
events, but also on conditional and/or Boolean combinations of events. We
describe a trade-based combinatorial prediction market asset management system,
called Dynamic Asset Cluster (DAC), that improves both time and space
efficiency over the method of, which maintains parallel junction trees for
assets and probabilities. The basic data structure is the asset block, which
compactly represents a set of trades made by a user. A user's asset model
consists of a set of asset blocks representing the user's entire trade history.
A junction tree is created dynamically from the asset blocks to compute a
user's minimum and expected assets."
"We present a mechanism design, coupling an online collaboration software and
a prediction market, which allows tracking down the very roots of individual
incentives, actions and how these behaviors influence collective intelligence
in terms of knowledge production as a public good. We show that the incentive
mechanism efficiently engages users without further governance structure, and
doesn't crowd out intrinsic motivation. Furthermore, it enables a powerful and
robust creative destruction process, which helps quickly filter out irrelevant
knowledge. While still at an early stage, this mechanism design can not only
bring insights for knowledge production organization design, but also has the
potential to illuminate the fundamental mechanisms underlying the emergence of
collective intelligence."
"In many multiagent scenarios, agents distribute resources, such as time or
energy, among several tasks. Having completed their tasks and generated
profits, task payoffs must be divided among the agents in some reasonable
manner. Cooperative games with overlapping coalitions (OCF games) are a recent
framework proposed by Chalkiadakis et al. (2010), generalizing classic
cooperative games to the case where agents may belong to more than one
coalition. Having formed overlapping coalitions and divided profits, some
agents may feel dissatisfied with their share of the profits, and would like to
deviate from the given outcome. However, deviation in OCF games is a
complicated matter: agents may decide to withdraw only some of their weight
from some of the coalitions they belong to; that is, even after deviation, it
is possible that agents will still be involved in tasks with non-deviators.
This means that the desirability of a deviation, and the stability of formed
coalitions, is to a great extent determined by the reaction of non-deviators.
In this work, we explore algorithmic aspects of OCF games, focusing on the core
in OCF games. We study the problem of deciding if the core of an OCF game is
not empty, and whether a core payoff division can be found in polynomial time;
moreover, we identify conditions that ensure that the problem admits polynomial
time algorithms. Finally, we introduce and study a natural class of OCF games,
Linear Bottleneck Games. Interestingly, we show that such games always have a
non-empty core, even assuming a highly lenient reaction to deviations."
"In this paper we study a generalization of the classic \emph{network creation
game} in the scenario in which the $n$ players sit on a given arbitrary
\emph{host graph}, which constrains the set of edges a player can activate at a
cost of $\alpha \geq 0$ each. This finds its motivations in the physical
limitations one can have in constructing links in practice, and it has been
studied in the past only when the routing cost component of a player is given
by the sum of distances to all the other nodes. Here, we focus on another
popular routing cost, namely that which takes into account for each player its
\emph{maximum} distance to any other player. For this version of the game, we
first analyze some of its computational and dynamic aspects, and then we
address the problem of understanding the structure of associated pure Nash
equilibria. In this respect, we show that the corresponding price of anarchy
(PoA) is fairly bad, even for several basic classes of host graphs. More
precisely, we first exhibit a lower bound of $\Omega (\sqrt{ n / (1+\alpha)})$
for any $\alpha = o(n)$. Notice that this implies a counter-intuitive lower
bound of $\Omega(\sqrt{n})$ for very small values of $\alpha$ (i.e., edges can
be activated almost for free). Then, we show that when the host graph is
restricted to be either $k$-regular (for any constant $k \geq 3$), or a
2-dimensional grid, the PoA is still $\Omega(1+\min\{\alpha,
\frac{n}{\alpha}\})$, which is proven to be tight for
$\alpha=\Omega(\sqrt{n})$. On the positive side, if $\alpha \geq n$, we show
the PoA is $O(1)$. Finally, in the case in which the host graph is very sparse
(i.e., $|E(H)|=n-1+k$, with $k=O(1)$), we prove that the PoA is $O(1)$, for any
$\alpha$."
"We propose a generalization of the classical stable marriage problem. In our
model, the preferences on one side of the partition are given in terms of
arbitrary binary relations, which need not be transitive nor acyclic. This
generalization is practically well-motivated, and as we show, encompasses the
well studied hard variant of stable marriage where preferences are allowed to
have ties and to be incomplete. As a result, we prove that deciding the
existence of a stable matching in our model is NP-complete. Complementing this
negative result we present a polynomial-time algorithm for the above decision
problem in a significant class of instances where the preferences are
asymmetric. We also present a linear programming formulation whose feasibility
fully characterizes the existence of stable matchings in this special case.
Finally, we use our model to study a long standing open problem regarding the
existence of cyclic 3D stable matchings. In particular, we prove that the
problem of deciding whether a fixed 2D perfect matching can be extended to a 3D
stable matching is NP-complete, showing this way that a natural attempt to
resolve the existence (or not) of 3D stable matchings is bound to fail."
"Assignment markets involve matching with transfers, as in labor markets and
housing markets. We consider a two-sided assignment market with agent types and
stochastic structure similar to models used in empirical studies, and
characterize the size of the core in such markets. Each agent has a randomly
drawn productivity with respect to each type of agent on the other side. The
value generated from a match between a pair of agents is the sum of the two
productivity terms, each of which depends only on the type but not the identity
of one of the agents, and a third deterministic term driven by the pair of
types. We allow the number of agents to grow, keeping the number of agent types
fixed. Let $n$ be the number of agents and $K$ be the number of types on the
side of the market with more types. We find, under reasonable assumptions, that
the relative variation in utility per agent over core outcomes is bounded as
$O^*(1/n^{1/K})$, where polylogarithmic factors have been suppressed. Further,
we show that this bound is tight in worst case. We also provide a tighter bound
under more restrictive assumptions. Our results provide partial justification
for the typical assumption of a unique core outcome in empirical studies."
"We present a mechanism for computing asymptotically stable school optimal
matchings, while guaranteeing that it is an asymptotic dominant strategy for
every student to report their true preferences to the mechanism. Our main tool
in this endeavor is differential privacy: we give an algorithm that coordinates
a stable matching using differentially private signals, which lead to our
truthfulness guarantee. This is the first setting in which it is known how to
achieve nontrivial truthfulness guarantees for students when computing school
optimal matchings, assuming worst- case preferences (for schools and students)
in large markets."
"We consider the problem of implementing an individually rational,
asymptotically Pareto optimal allocation in a barter-exchange economy where
agents are endowed with goods and have preferences over the goods of others,
but may not use money as a medium of exchange. Because one of the most
important instantiations of such economies is kidney exchange -- where the
""input""to the problem consists of sensitive patient medical records -- we ask
to what extent such exchanges can be carried out while providing formal privacy
guarantees to the participants. We show that individually rational allocations
cannot achieve any non-trivial approximation to Pareto optimality if carried
out under the constraint of differential privacy -- or even the relaxation of
\emph{joint} differential privacy, under which it is known that asymptotically
optimal allocations can be computed in two-sided markets, where there is a
distinction between buyers and sellers and we are concerned only with privacy
of the buyers~\citep{Matching}. We therefore consider a further relaxation that
we call \emph{marginal} differential privacy -- which promises, informally,
that the privacy of every agent $i$ is protected from every other agent $j \neq
i$ so long as $j$ does not collude or share allocation information with other
agents. We show that, under marginal differential privacy, it is possible to
compute an individually rational and asymptotically Pareto optimal allocation
in such exchange economies."
"Auction theory traditionally assumes that bidders' valuation distributions
are known to the auctioneer, such as in the celebrated, revenue-optimal Myerson
auction. However, this theory does not describe how the auctioneer comes to
possess this information. Recently, Cole and Roughgarden [2014] showed that an
approximation based on a finite sample of independent draws from each bidder's
distribution is sufficient to produce a near-optimal auction. In this work, we
consider the problem of learning bidders' valuation distributions from much
weaker forms of observations. Specifically, we consider a setting where there
is a repeated, sealed-bid auction with $n$ bidders, but all we observe for each
round is who won, but not how much they bid or paid. We can also participate
(i.e., submit a bid) ourselves, and observe when we win. From this information,
our goal is to (approximately) recover the inherently recoverable part of the
underlying bid distributions. We also consider extensions where different
subsets of bidders participate in each round, and where bidders' valuations
have a common-value component added to their independent private values."
"In this paper we consider the price of anarchy (PoA) in multi-commodity flows
where the latency or delay function on an edge has a heterogeneous dependency
on the flow commodities, i.e. when the delay on each link is dependent on the
flow of individual commodities, rather than on the aggregate flow. An
application of this study is the performance analysis of a network with
differentiated traffic that may arise when traffic is prioritized according to
some type classification. This study has implications in the debate on
net-neutrality. We provide price of anarchy bounds for networks with $k$ (types
of) commodities where each link is associated with heterogeneous polynomial
delays, i.e. commodity $i$ on edge $e$ faces delay specified by
$g_{i1}(e)f^{\theta}_1(e) + g_{i2}(e)f^{\theta}_2(e) + \ldots +
g_{ik}(e)f^{\theta}_k(e) + c_i(e), $ where $f_i(e)$ is the flow of the $i$th
commodity through edge $e$, $\theta \in {\cal N}$, $g_{i1}(e), g_{i2}(e),
\ldots, g_{ik}(e)$ and $c_i(e)$ are nonnegative constants. We consider both
atomic and non-atomic flows.
  For networks with decomposable delay functions where the delay induced by a
particular commodity is the same, i.e. delays on edge $e$ are defined by
$a_1(e)f_1^\theta(e) + a_2(e)f_2^\theta(e) + \ldots + c(e)$ where $\forall j ,
\forall e: g_{1j}(e) = g_{2j}(e) = \ldots = a_j(e)$, we show an improved bound
on the price of anarchy.
  Further, we show bounds on the price of anarchy for uniform latency functions
where each edge of the network has the same delay function."
"The $\varepsilon$-well-supported Nash equilibrium is a strong notion of
approximation of a Nash equilibrium, where no player has an incentive greater
than $\varepsilon$ to deviate from any of the pure strategies that she uses in
her mixed strategy. The smallest constant $\varepsilon$ currently known for
which there is a polynomial-time algorithm that computes an
$\varepsilon$-well-supported Nash equilibrium in bimatrix games is slightly
below $2/3$. In this paper we study this problem for symmetric bimatrix games
and we provide a polynomial-time algorithm that gives a
$(1/2+\delta)$-well-supported Nash equilibrium, for an arbitrarily small
positive constant $\delta$."
"We introduce the concept of budget games. Players choose a set of tasks and
each task has a certain demand on every resource in the game. Each resource has
a budget. If the budget is not enough to satisfy the sum of all demands, it has
to be shared between the tasks. We study strategic budget games, where the
budget is shared proportionally. We also consider a variant in which the order
of the strategic decisions influences the distribution of the budgets. The
complexity of the optimal solution as well as existence, complexity and quality
of equilibria are analyzed. Finally, we show that the time an ordered budget
game needs to convergence towards an equilibrium may be exponential."
"We undertake a formal study of the value of targeting data to an advertiser.
As expected, this value is increasing in the utility difference between
realizations of the targeting data and the accuracy of the data, and depends on
the distribution of competing bids. However, this value may vary
non-monotonically with an advertiser's budget. Similarly, modeling the values
as either private or correlated, or allowing other advertisers to also make use
of the data, leads to unpredictable changes in the value of data. We address
questions related to multiple data sources, show that utility of additional
data may be non-monotonic, and provide tradeoffs between the quality and the
price of data sources. In a game-theoretic setting, we show that advertisers
may be worse off than if the data had not been available at all. We also ask
whether a publisher can infer the value an advertiser would place on targeting
data from the advertiser's bidding behavior and illustrate that this is
impossible."
"Market share and quality, or customer satisfaction, go together. Yet
inferring one from the other appears difficult. Indeed, such an inference would
need detailed information about customer behavior, and might be clouded by
modes of behavior such as herding (following popularity) or elitism, where
customers avoid popular products. We investigate a fixed-price model where
customers are informed about their history with products and about market share
data. We find that it is in fact correct to make a Bayesian inference that the
product with the higher market share has the better quality under few and
unrestrictive assumptions on customer behavior."
"We study the efficiency (in terms of social welfare) of truthful and
symmetric mechanisms in one-sided matching problems with {\em dichotomous
preferences} and {\em normalized von Neumann-Morgenstern preferences}. We are
particularly interested in the well-known {\em Random Serial Dictatorship}
mechanism. For dichotomous preferences, we first show that truthful, symmetric
and optimal mechanisms exist if intractable mechanisms are allowed. We then
provide a connection to online bipartite matching. Using this connection, it is
possible to design truthful, symmetric and tractable mechanisms that extract
0.69 of the maximum social welfare, which works under assumption that agents
are not adversarial. Without this assumption, we show that Random Serial
Dictatorship always returns an assignment in which the expected social welfare
is at least a third of the maximum social welfare. For normalized von
Neumann-Morgenstern preferences, we show that Random Serial Dictatorship always
returns an assignment in which the expected social welfare is at least
$\frac{1}{e}\frac{\nu(\opt)^2}{n}$, where $\nu(\opt)$ is the maximum social
welfare and $n$ is the number of both agents and items. On the hardness side,
we show that no truthful mechanism can achieve a social welfare better than
$\frac{\nu(\opt)^2}{n}$."
"Quantitative games are two-player zero-sum games played on directed weighted
graphs. Total-payoff games (that can be seen as a refinement of the
well-studied mean-payoff games) are the variant where the payoff of a play is
computed as the sum of the weights. Our aim is to describe the first
pseudo-polynomial time algorithm for total-payoff games in the presence of
arbitrary weights. It consists of a non-trivial application of the value
iteration paradigm. Indeed, it requires to study, as a milestone, a refinement
of these games, called min-cost reachability games, where we add a reachability
objective to one of the players. For these games, we give an efficient value
iteration algorithm to compute the values and optimal strategies (when they
exist), that runs in pseudo-polynomial time. We also propose heuristics
allowing one to possibly speed up the computations in both cases."
"Counterfactual Regret Minimization and variants (e.g. Public Chance Sampling
CFR and Pure CFR) have been known as the best approaches for creating
approximate Nash equilibrium solutions for imperfect information games such as
poker. This paper introduces CFR$^+$, a new algorithm that typically
outperforms the previously known algorithms by an order of magnitude or more in
terms of computation time while also potentially requiring less memory."
"We introduce a dynamic mechanism design problem in which the designer wants
to offer for sale an item to an agent, and another item to the same agent at
some point in the future. The agent's joint distribution of valuations for the
two items is known, and the agent knows the valuation for the current item (but
not for the one in the future). The designer seeks to maximize expected
revenue, and the auction must be deterministic, truthful, and ex post
individually rational. The optimum mechanism involves a protocol whereby the
seller elicits the buyer's current valuation, and based on the bid makes two
take-it-or-leave-it offers, one for now and one for the future. We show that
finding the optimum deterministic mechanism in this situation - arguably the
simplest meaningful dynamic mechanism design problem imaginable - is NP-hard.
We also prove several positive results, among them a polynomial linear
programming-based algorithm for the optimum randomized auction (even for many
bidders and periods), and we show strong separations in revenue between
non-adaptive, adaptive, and randomized auctions, even when the valuations in
the two periods are uncorrelated. Finally, for the same problem in an
environment in which contracts cannot be enforced, and thus perfection of
equilibrium is necessary, we show that the optimum randomized mechanism
requires multiple rounds of cheap talk-like interactions."
"This paper deals with the problem of efficient resource allocation in dynamic
infrastructureless wireless networks. Assuming a reactive interference-limited
scenario, each transmitter is allowed to select one frequency channel (from a
common pool) together with a power level at each transmission trial; hence, for
all transmitters, not only the fading gain, but also the number of interfering
transmissions and their transmit powers are varying over time. Due to the
absence of a central controller and time-varying network characteristics, it is
highly inefficient for transmitters to acquire global channel and network
knowledge. Therefore a reasonable assumption is that transmitters have no
knowledge of fading gains, interference, and network topology. Each
transmitting node selfishly aims at maximizing its average reward (or
minimizing its average cost), which is a function of the action of that
specific transmitter as well as those of all other transmitters. This scenario
is modeled as a multi-player multi-armed adversarial bandit game, in which
multiple players receive an a priori unknown reward with an arbitrarily
time-varying distribution by sequentially pulling an arm, selected from a known
and finite set of arms. Since players do not know the arm with the highest
average reward in advance, they attempt to minimize their so-called regret,
determined by the set of players' actions, while attempting to achieve
equilibrium in some sense. To this end, we design in this paper two joint power
level and channel selection strategies. We prove that the gap between the
average reward achieved by our approaches and that based on the best fixed
strategy converges to zero asymptotically. Moreover, the empirical joint
frequencies of the game converge to the set of correlated equilibria. We
further characterize this set for two special cases of our designed game."
"The discrete sell or hold problem (DSHP), which is introduced in \cite{H12},
is studied under the constraint that each asset can only take a constant number
of different values. We show that if each asset can take only two values, the
problem becomes polynomial-time solvable. However, even if each asset can take
three different values, DSHP is still NP-hard. An approximation algorithm is
also given under this setting."
"We present a quantum approach to a signaling game; a special kind of
extensive games of incomplete information. Our model is based on quantum
schemes for games in strategic form where players perform unitary operators on
their own qubits of some fixed initial state and the payoff function is given
by a measurement on the resulting final state. We show that the quantum game
induced by our scheme coincides with a signaling game as a special case and
outputs nonclassical results in general. As an example, we consider a quantum
extension of the signaling game in which the chance move is a three-parameter
unitary operator whereas the players' actions are equivalent to classical ones.
In this case, we study the game in terms of Nash equilibria and refine the pure
Nash equilibria adapting to the quantum game the notion of a weak perfect
Bayesian equilibrium."
"Prediction markets are often used as mechanisms to aggregate information
about a future event, for example, whether a candidate will win an election.
The event is typically assumed to be exogenous. In reality, participants may
influence the outcome, and therefore (1) running the prediction market could
change the incentives of participants in the process that creates the outcome
(for example, agents may want to change their vote in an election), and (2)
simple results such as the myopic incentive compatibility of proper scoring
rules no longer hold in the prediction market itself. We introduce a model of
games of this kind, where agents first trade in a prediction market and then
take an action that influences the market outcome. Our two-stage two-player
model, despite its simplicity, captures two aspects of real-world prediction
markets: (1) agents may directly influence the outcome, (2) some of the agents
instrumental in deciding the outcome may not take part in the prediction
market. We show that this game has two different types of perfect Bayesian
equilibria, which we term LPP and HPP, depending on the values of the belief
parameters: in the LPP domain, equilibrium prices reveal expected market
outcomes conditional on the participants' private information, whereas HPP
equilibria are collusive -- participants effectively coordinate in an
uninformative and untruthful way."
"There have been great efforts in studying the cascading behavior in social
networks such as the innovation diffusion, etc. Game theoretically, in a social
network where individuals choose from two strategies: A (the innovation) and B
(the status quo) and get payoff from their neighbors for coordination, it has
long been known that the Price of Anarchy (PoA) of this game is not 1, since
the Nash equilibrium (NE) where all players take B (B Nash) is inferior to the
one all players taking A (A Nash). However, no quantitative analysis has been
performed to give an accurate upper bound of PoA in this game.
  In this paper, we adopt a widely used networked coordination game setting [3]
to study how bad a Nash equilibrium can be and give a tight upper bound of the
PoA of such games. We show that there is an NE that is slightly worse than the
B Nash. On the other hand, the PoA is bounded and the worst NE cannot be much
worse than the B Nash. In addition, we discuss how the PoA upper bound would
change when compatibility between A and B is introduced, and show an intuitive
result that the upper bound strictly decreases as the compatibility is
increased."
"Situations where a group of agents come together to jointly buy a resource
that they individually cannot afford to buy are commonly observed in markets.
For example in the US market for radio spectrum, a recent proposal invited
small firms who would benefit from gaining additional access to spectrum to
jointly submit bids for blocks of spectrum with the idea that its utilization
could be shared. In such a scenario, the problem is to design a mechanism that
truthfully elicits and aggregates the privately held preferences of these
agents, and enables them to act as a single decision-making body in order to
participate in the market. In this paper, we design a class of mechanisms
called monotonic aggregation mechanisms that achieves this under a specific
setting. We assume that the resource is being sold in a sealed-bid second-price
auction that solicits bids for the entire resource. Our mechanism truthfully
elicits utility functions from the buyers, prescribes a joint bid, and
prescribes a division of the payment and the resource in the event that they
win the resource in the auction. This mechanism further satisfies a popular
notion of collusion-resistance known as coalition-strategyproofness. We give
two explicit examples of this generic class for the case where the utility
functions of the buyers are non-decreasing and concave."
"We study the efficiency guarantees in the simple auction environment where
the auctioneer has one unit of divisible good to be distributed among a number
of budget constrained agents. With budget constraints, the social welfare
cannot be approximated by a better factor than the number of agents by any
truthful mechanism. Thus, we follow a recent work by Dobzinski and Leme (ICALP
2014) to approximate the liquid welfare, which is the welfare of the agents
each capped by her/his own budget. We design a new truthful auction with an
approximation ratio of $\frac{\sqrt{5}+1}{2} \approx 1.618$, improving the best
previous ratio of $2$ when the budgets for agents are public knowledge and
their valuation is linear (additive). In private budget setting, we propose the
first constant approximation auction with approximation ratio of $34$.
Moreover, this auction works for any valuation function. Previously, only
$O(\log n)$ approximation was known for linear and decreasing marginal
(concave) valuations, and $O(\log^2 n)$ approximation was known for
sub-additive valuations."
"Equilibrium in Economics has been seldom addressed in a situation where some
variables are discrete. This work introduces a problem related to lot-sizing
with several players, and analyses some strategies which are likely to be found
in real world games. An illustration with a simple example is presented, with
concerns about the difficulty of the problem and computation possibilities."
"We examine strategy-proof elections to select a winner amongst a set of
agents, each of whom cares only about winning. This impartial selection problem
was introduced independently by Holzman and Moulin and Alon et al. Fisher and
Klimm showed that the permutation mechanism is impartial and $1/2$-optimal,
that is, it selects an agent who gains, in expectation, at least half the
number of votes of most popular agent. Furthermore, they showed the mechanism
is $7/12$-optimal if agents cannot abstain in the election. We show that a
better guarantee is possible, provided the most popular agent receives at least
a large enough, but constant, number of votes. Specifically, we prove that, for
any $\epsilon>0$, there is a constant $N_{\epsilon}$ (independent of the number
$n$ of voters) such that, if the maximum number of votes of the most popular
agent is at least $N_{\epsilon}$ then the permutation mechanism is
$(\frac{3}{4}-\epsilon)$-optimal. This result is tight.
  Furthermore, in our main result, we prove that near-optimal impartial
mechanisms exist. In particular, there is an impartial mechanism that is
$(1-\epsilon)$-optimal, for any $\epsilon>0$, provided that the maximum number
of votes of the most popular agent is at least a constant $M_{\epsilon}$."
"We study the Shapley value in weighted voting games. The Shapley value has
been used as an index for measuring the power of individual agents in
decision-making bodies and political organizations, where decisions are made by
a majority vote process. We characterize the impact of changing the quota
(i.e., the minimum number of seats in the parliament that are required to form
a coalition) on the Shapley values of the agents. Contrary to previous studies,
which assumed that the agent weights (corresponding to the size of a caucus or
a political party) are fixed, we analyze new domains in which the weights are
stochastically generated, modelling, for example, elections processes.
  We examine a natural weight generation process: the Balls and Bins model,
with uniform as well as exponentially decaying probabilities. We also analyze
weights that admit a super-increasing sequence, answering several open
questions pertaining to the Shapley values in such games."
"The aggregation of conflicting preferences is a central problem in multiagent
systems. The key difficulty is that the agents may report their preferences
insincerely. Mechanism design is the art of designing the rules of the game so
that the agents are motivated to report their preferences truthfully and a
(socially) desirable outcome is chosen. We propose an approach where a
mechanism is automatically created for the preference aggregation setting at
hand. This has several advantages, but the downside is that the mechanism
design optimization problem needs to be solved anew each time. Focusing on
settings where side payments are not possible, we show that the mechanism
design problem is NP-complete for deterministic mechanisms. This holds both for
dominant-strategy implementation and for Bayes-Nash implementation. We then
show that if we allow randomized mechanisms, the mechanism design problem
becomes tractable. In other words, the coordinator can tackle the computational
complexity introduced by its uncertainty about the agents preferences BY making
the agents face additional uncertainty.This comes at no loss, AND IN SOME cases
at a gain, IN the(social) objective."
"We design a protocol for dynamic prioritization of data on shared routers
such as untethered 3G/4G devices. The mechanism prioritizes bandwidth in favor
of users with the highest value, and is incentive compatible, so that users can
simply report their true values for network access. A revenue pooling mechanism
also aligns incentives for sellers, so that they will choose to use
prioritization methods that retain the incentive properties on the buy-side. In
this way, the design allows for an open architecture. In addition to revenue
pooling, the technical contribution is to identify a class of stochastic demand
models and a prioritization scheme that provides allocation monotonicity.
Simulation results confirm efficiency gains from dynamic prioritization
relative to prior methods, as well as the effectiveness of revenue pooling."
"Approximating the optimal social welfare while preserving truthfulness is a
well studied problem in algorithmic mechanism design. Assuming that the social
welfare of a given mechanism design problem can be optimized by an integer
program whose integrality gap is at most $\alpha$, Lavi and Swamy~\cite{Lavi11}
propose a general approach to designing a randomized $\alpha$-approximation
mechanism which is truthful in expectation. Their method is based on
decomposing an optimal solution for the relaxed linear program into a convex
combination of integer solutions. Unfortunately, Lavi and Swamy's decomposition
technique relies heavily on the ellipsoid method, which is notorious for its
poor practical performance. To overcome this problem, we present an alternative
decomposition technique which yields an $\alpha(1 + \epsilon)$ approximation
and only requires a quadratic number of calls to an integrality gap verifier."
"Popular content distribution is one of the key services provided by vehicular
ad hoc networks (VANETs), in which a popular file is broadcasted by roadside
units (RSUs) to the on-board units (OBUs) driving through a particular area.
Due to fast speed and deep fading, some file packets might be lost during the
vehicle-to-roadside broadcasting stage. In this paper, we propose a
peer-to-peer (P2P) approach to allow the OBUs to exchange data and complement
the missing packets. Specifically, we introduce a coalitional graph game to
model the cooperation among OBUs and propose a coalition formation algorithm to
implement the P2P approach. Moreover, cognitive radio is utilized for
vehicle-to-vehicle transmissions so that the P2P approach does not require
additional bandwidth. Simulation results show that the proposed approach
performs better in various conditions, relative to the non-cooperative
approach, in which the OBUs share no information and simply response to any
data request from other OBUs."
"We model and study the game mechanisms and human behavior of the anarchy mode
in Twitch Plays Pokemon with a pure-jump continuous-time Markov process. We
computed the winning probability and expected game time for $1$ player and $N$
players and identified when collaboration helps. A numerical plug-in example is
also provided."
"In this work we investigate the inefficiency of the electricity system with
strategic agents. Specifically, we prove that without a proper control the
total demand of an inefficient system is at most twice the total demand of the
optimal outcome. We propose an incentives scheme that promotes optimal outcomes
in the inefficient electricity market. The economic incentives can be seen as
an indirect revelation mechanism that allocates resources using a
one-dimensional message space per resource to be allocated. The mechanism does
not request private information from users and is valid for any concave
customer's valuation function. We propose a distributed implementation of the
mechanism using population games and evaluate the performance of four popular
dynamics methods in terms of the cost to implement the mechanism. We find that
the achievement of efficiency in strategic environments might be achieved at a
cost, which is dependent on both the users' preferences and the dynamic
evolution of the system. Some simulation results illustrate the ideas presented
throughout the paper."
"Voting problems are central in the area of social choice. In this article, we
investigate various voting systems and types of control of elections. We
present integer linear programming (ILP) formulations for a wide range of
NP-hard control problems. Our ILP formulations are flexible in the sense that
they can work with an arbitrary number of candidates and voters. Using the
off-the-shelf solver Cplex, we show that our approaches can manipulate
elections with a large number of voters and candidates efficiently."
"Given a batch of human computation tasks, a commonly ignored aspect is how
the price (i.e., the reward paid to human workers) of these tasks must be set
or varied in order to meet latency or cost constraints. Often, the price is set
up-front and not modified, leading to either a much higher monetary cost than
needed (if the price is set too high), or to a much larger latency than
expected (if the price is set too low). Leveraging a pricing model from prior
work, we develop algorithms to optimally set and then vary price over time in
order to meet a (a) user-specified deadline while minimizing total monetary
cost (b) user-specified monetary budget constraint while minimizing total
elapsed time. We leverage techniques from decision theory (specifically, Markov
Decision Processes) for both these problems, and demonstrate that our
techniques lead to upto 30\% reduction in cost over schemes proposed in prior
work. Furthermore, we develop techniques to speed-up the computation, enabling
users to leverage the price setting algorithms on-the-fly."
"We consider a setting where $n$ buyers, with combinatorial preferences over
$m$ items, and a seller, running a priority-based allocation mechanism,
repeatedly interact. Our goal, from observing limited information about the
results of these interactions, is to reconstruct both the preferences of the
buyers and the mechanism of the seller. More specifically, we consider an
online setting where at each stage, a subset of the buyers arrive and are
allocated items, according to some unknown priority that the seller has among
the buyers. Our learning algorithm observes only which buyers arrive and the
allocation produced (or some function of the allocation, such as just which
buyers received positive utility and which did not), and its goal is to predict
the outcome for future subsets of buyers. For this task, the learning algorithm
needs to reconstruct both the priority among the buyers and the preferences of
each buyer. We derive mistake bound algorithms for additive, unit-demand and
single minded buyers. We also consider the case where buyers' utilities for a
fixed bundle can change between stages due to different (observed) prices. Our
algorithms are efficient both in computation time and in the maximum number of
mistakes (both polynomial in the number of buyers and items)."
"For a simple model of price-responsive demand, we consider a deregulated
electricity marketplace wherein the grid (ISO, retailer-distributor) accepts
bids per-unit supply from generators (simplified herein neither to consider
start-up/ramp-up expenses nor day-ahead or shorter-term load following) which
are then averaged (by supply allocations via an economic dispatch) to a common
""clearing"" price borne by customers (irrespective of variations in
transmission/distribution or generation prices), i.e., the ISO does not
compensate generators based on their marginal costs. Rather, the ISO provides
sufficient information for generators to sensibly adjust their bids.
Notwithstanding our idealizations, the dispatch dynamics are complex. For a
simple benchmark power system, we find a price-symmetric Nash equilibrium
through numerical experiments."
"We develop Integer Programming (IP) solutions for some special college
admission problems arising from the Hungarian higher education admission
scheme. We focus on four special features, namely the solution concept of
stable score-limits, the presence of lower and common quotas, and paired
applications. We note that each of the latter three special feature makes the
college admissions problem NP-hard to solve. Currently, a heuristic based on
the Gale-Shapley algorithm is being used in the application. The IP methods
that we propose are not only interesting theoretically, but may also serve as
an alternative solution concept for this practical application, and also for
other ones."
"We study the problem of eliciting and aggregating probabilistic information
from multiple agents. In order to successfully aggregate the predictions of
agents, the principal needs to elicit some notion of confidence from agents,
capturing how much experience or knowledge led to their predictions. To
formalize this, we consider a principal who wishes to elicit predictions about
a random variable from a group of Bayesian agents, each of whom have privately
observed some independent samples of the random variable, and hopes to
aggregate the predictions as if she had directly observed the samples of all
agents. Leveraging techniques from Bayesian statistics, we represent confidence
as the number of samples an agent has observed, which is quantified by a
hyperparameter from a conjugate family of prior distributions. This then allows
us to show that if the principal has access to a few samples, she can achieve
her aggregation goal by eliciting predictions from agents using proper scoring
rules. In particular, if she has access to one sample, she can successfully
aggregate the agents' predictions if and only if every posterior predictive
distribution corresponds to a unique value of the hyperparameter. Furthermore,
this uniqueness holds for many common distributions of interest. When this
uniqueness property does not hold, we construct a novel and intuitive mechanism
where a principal with two samples can elicit and optimally aggregate the
agents' predictions."
"We study the efficiency of allocations in large markets with a network
structure where every seller owns an edge in a graph and every buyer desires a
path connecting some nodes. While it is known that stable allocations in such
settings can be very inefficient, the exact properties of equilibria in markets
with multiple sellers are not fully understood even in single-source
single-sink networks. In this work, we show that for a large class of natural
buyer demand functions, we are guaranteed the existence of an equilibrium with
several desirable properties. The crucial insight that we gain into the
equilibrium structure allows us to obtain tight bounds on efficiency in terms
of the various parameters governing the market, especially the number of
monopolies M. All of our efficiency results extend to markets without the
network structure.
  While it is known that monopolies can cause large inefficiencies in general,
our main results for single-source single-sink networks indicate that for
several natural demand functions the efficiency only drops linearly with M. For
example, for concave demand we prove that the efficiency loss is at most a
factor 1+M/2 from the optimum, for demand with monotone hazard rate it is at
most 1+M, and for polynomial demand the efficiency decreases logarithmically
with M. In contrast to previous work that showed that monopolies may adversely
affect welfare, our main contribution is showing that monopolies may not be as
`evil' as they are made out to be; the loss in efficiency is bounded in many
natural markets. Finally, we consider more general, multiple-source networks
and show that in the absence of monopolies, mild assumptions on the network
topology guarantee an equilibrium that maximizes social welfare."
"The problem of analyzing the effect of privacy concerns on the behavior of
selfish utility-maximizing agents has received much attention lately. Privacy
concerns are often modeled by altering the utility functions of agents to
consider also their privacy loss. Such privacy aware agents prefer to take a
randomized strategy even in very simple games in which non-privacy aware agents
play pure strategies. In some cases, the behavior of privacy aware agents
follows the framework of Randomized Response, a well-known mechanism that
preserves differential privacy.
  Our work is aimed at better understanding the behavior of agents in settings
where their privacy concerns are explicitly given. We consider a toy setting
where agent A, in an attempt to discover the secret type of agent B, offers B a
gift that one type of B agent likes and the other type dislikes. As opposed to
previous works, B's incentive to keep her type a secret isn't the result of
""hardwiring"" B's utility function to consider privacy, but rather takes the
form of a payment between B and A. We investigate three different types of
payment functions and analyze B's behavior in each of the resulting games. As
we show, under some payments, B's behavior is very different than the behavior
of agents with hardwired privacy concerns and might even be deterministic.
Under a different payment we show that B's BNE strategy does fall into the
framework of Randomized Response."
"We generalize the classical single-crossing property to single-crossing
property on trees and obtain new ways to construct Condorcet domains which are
sets of linear orders which possess the property that every profile composed
from those orders have transitive majority relation. We prove that for any tree
there exist profiles that are single-crossing on that tree; moreover, that tree
is minimal in this respect for at least one such profile. Finally, we provide a
polynomial-time algorithm to recognize whether or not a given profile is
single-crossing with respect to some tree. We also show that finding winners
for Chamberlin-Courant rule is polynomial for profiles that are single-crossing
on trees."
"Strategic interactions often take place in an environment rife with
uncertainty. As a result, the equilibrium of a game is intimately related to
the information available to its players. The \emph{signaling problem}
abstracts the task faced by an informed ""market maker"", who must choose how to
reveal information in order to effect a desirable equilibrium.
  In this paper, we consider two fundamental signaling problems: one for
abstract normal form games, and the other for single item auctions. For the
former, we consider an abstract class of objective functions which includes the
social welfare and weighted combinations of players' utilities, and for the
latter we restrict our attention to the social welfare objective and to
signaling schemes which are constrained in the number of signals used. For both
problems, we design approximation algorithms for the signaling problem which
run in quasi-polynomial time under various conditions, extending and
complementing the results of various recent works on the topic.
  Underlying each of our results is a ""meshing scheme"" which effectively
overcomes the ""curse of dimensionality"" and discretizes the space of
""essentially different"" posterior beliefs -- in the sense of inducing
""essentially different"" equilibria. This is combined with an algorithm for
optimally assembling a signaling scheme as a convex combination of such
beliefs. For the normal form game setting, the meshing scheme leads to a convex
partition of the space of posterior beliefs and this assembly procedure is
reduced to a linear program, and in the auction setting the assembly procedure
is reduced to submodular function maximization."
"The popular generalized second price (GSP) auction for sponsored search is
built upon a separable model of click-through-rates that decomposes the
likelihood of a click into the product of a ""slot effect"" and an ""advertiser
effect"" --- if the first slot is twice as good as the second for some bidder,
then it is twice as good for everyone. Though appealing in its simplicity, this
model is quite suspect in practice. A wide variety of factors including
externalities and budgets have been studied that can and do cause it to be
violated. In this paper we adopt a view of GSP as an iterated second price
auction (see, e.g., Milgrom 2010) and study how the most basic violation of
separability --- position dependent, arbitrary public click-through-rates that
do not decompose --- affects results from the foundational analysis of GSP
(Varian 2007, Edelman et al. 2007). For the two-slot setting we prove that for
arbitrary click-through-rates, for arbitrary bidder values, an efficient
pure-strategy equilibrium always exists; however, without separability there
always exist values such that the VCG outcome and payments cannot be realized
by any bids, in equilibrium or otherwise. The separability assumption is
therefore necessary in the two-slot case to match the payments of VCG but not
for efficiency. We moreover show that without separability, generic existence
of efficient equilibria is sensitive to the choice of tie-breaking rule, and
when there are more than two slots, no (bid-independent) tie-breaking rule
yields the positive result. In light of this we suggest alternative mechanisms
that trade the simplicity of GSP for better equilibrium properties when there
are three or more slots."
"In this paper, we establish the existence of optimal bounded memory strategy
profiles in multi-player discounted sum games. We introduce a non-deterministic
approach to compute optimal strategy profiles with bounded memory. Our approach
can be used to obtain optimal rewards in a setting where a powerful player
selects the strategies of all players for Nash and leader equilibria, where in
leader equilibria the Nash condition is waived for the strategy of this
powerful player. The resulting strategy profiles are optimal for this player
among all strategy profiles that respect the given memory bound, and the
related decision problem is NP-complete. We also provide simple examples, which
show that having more memory will improve the optimal strategy profile, and
that sufficient memory to obtain optimal strategy profiles cannot be inferred
from the structure of the game."
"In a smart grid environment, we study coalition formation of prosumers that
aim at entering the energy market. It is paramount for the grid operation that
the energy producers are able to sustain the grid demand in terms of stability
and minimum production requirement. We design an algorithm that seeks to form
coalitions that will meet both of these requirements: a minimum energy level
for the coalitions and a steady production level which leads to finding
uncorrelated sources of energy to form a coalition. We propose an algorithm
that uses graph tools such as correlation graphs or clique percolation to form
coalitions that meet such complex constraints. We validate the algorithm
against a random procedure and show that, it not only performs better in term
of social welfare for the power grid, but also that it is more robust against
unforeseen production variations due to changing weather conditions for
instance."
"Considering congestion games with uncertain delays, we compute the
inefficiency introduced in network routing by risk-averse agents. At
equilibrium, agents may select paths that do not minimize the expected latency
so as to obtain lower variability. A social planner, who is likely to be more
risk neutral than agents because it operates at a longer time-scale, quantifies
social cost with the total expected delay along routes. From that perspective,
agents may make suboptimal decisions that degrade long-term quality. We define
the {\em price of risk aversion} (PRA) as the worst-case ratio of the social
cost at a risk-averse Wardrop equilibrium to that where agents are
risk-neutral. For networks with general delay functions and a single
source-sink pair, we show that the PRA depends linearly on the agents' risk
tolerance and on the degree of variability present in the network. In contrast
to the {\em price of anarchy}, in general the PRA increases when the network
gets larger but it does not depend on the shape of the delay functions. To get
this result we rely on a combinatorial proof that employs alternating paths
that are reminiscent of those used in max-flow algorithms. For {\em
series-parallel} (SP) graphs, the PRA becomes independent of the network
topology and its size. As a result of independent interest, we prove that for
SP networks with deterministic delays, Wardrop equilibria {\em maximize} the
shortest-path objective among all feasible flows."
"Media publisher platforms often face an effectiveness-nuisance tradeoff: more
annoying ads can be more effective for some advertisers because of their
ability to attract attention, but after attracting viewers' attention, their
nuisance to viewers can decrease engagement with the platform over time. With
the rise of mobile technology and ad blockers, many platforms are becoming
increasingly concerned about how to improve monetization through digital ads
while improving viewer experience.
  We study an online ad auction mechanism that incorporates a charge for ad
impact on user experience as a criterion for ad selection and pricing. Like a
Pigovian tax, the charge causes advertisers to internalize the hidden cost of
foregone future platform revenue due to ad impact on user experience. Over
time, the mechanism provides an incentive for advertisers to develop ads that
are effective while offering viewers a more pleasant experience. We show that
adopting the mechanism can simultaneously benefit the publisher, advertisers,
and viewers, even in the short term.
  Incorporating a charge for ad impact can increase expected advertiser profits
if enough advertisers compete. A stronger effectiveness-nuisance tradeoff,
meaning that ad effectiveness is more strongly associated with negative impact
on user experience, increases the amount of competition required for the
mechanism to benefit advertisers. The findings suggest that the mechanism can
benefit the marketplace for ad slots that consistently attract many
advertisers."
"We consider a community of users who must make periodic decisions about
whether to interact with one another. We propose a protocol which allows honest
users to reliably interact with each other, while limiting the damage done by
each malicious or incompetent user. The worst-case cost per user is sublinear
in the average number of interactions per user and is independent of the number
of users. Our guarantee holds simultaneously for every group of honest users.
For example, multiple groups of users with incompatible tastes or preferences
can coexist.
  As a motivating example, we consider a game where players have periodic
opportunities to do one another favors but minimal ability to determine when a
favor was done. In this setting, our protocol achieves nearly optimal
collective welfare while remaining resistant to exploitation.
  Our results also apply to a collaborative filtering setting where users must
make periodic decisions about whether to interact with resources such as movies
or restaurants. In this setting, we guarantee that any set of honest users
achieves a payoff nearly as good as if they had identified the optimal set of
items in advance and then chosen to interact only with resources from that set."
"We consider the problem of making a collective choice by means of
approval-preferential voting. The existing proposals are briefly overviewed so
as to point out several issues that leave to be desired. In particular, and
following Condorcet's last views on elections, we pay a special attention to
making sure that a good option is chosen rather than aiming for the best option
but not being so sure about it. We show that this goal is fulfilled in a
well-defined sense by a method that we introduced in a previous paper and whose
study is deepened here. This procedure, that we call path-revised approval
choice, is based on interpreting the approval and paired-comparison scores as
degrees of collective belief, revising them in the light of the existing
implications by means of the so-called Theophrastus rule, and deciding about
every option on the basis of the balance of revised degrees of belief for and
against its approval. The computations rely on the path scores, which are used
also in a method that was introduced by Markus Schulze in the spirit of looking
for the best option. Besides dealing with the confidence in the respective
results of both methods, we also establish several other properties of them,
including a property of upper semicontinuity of the choice set with respect to
the profile and a property of Pareto consistency (in a certain weak sense)."
"Many auction settings implicitly or explicitly require that bidders are
treated equally ex-ante. This may be because discrimination is philosophically
or legally impermissible, or because it is practically difficult to implement
or impossible to enforce. We study so-called {\em anonymous} auctions to
understand the revenue tradeoffs and to develop simple anonymous auctions that
are approximately optimal.
  We consider digital goods settings and show that the optimal anonymous,
dominant strategy incentive compatible auction has an intuitive structure ---
imagine that bidders are randomly permuted before the auction, then infer a
posterior belief about bidder i's valuation from the values of other bidders
and set a posted price that maximizes revenue given this posterior.
  We prove that no anonymous mechanism can guarantee an approximation better
than O(n) to the optimal revenue in the worst case (or O(log n) for regular
distributions) and that even posted price mechanisms match those guarantees.
Understanding that the real power of anonymous mechanisms comes when the
auctioneer can infer the bidder identities accurately, we show a tight O(k)
approximation guarantee when each bidder can be confused with at most k ""higher
types"". Moreover, we introduce a simple mechanism based on n target prices that
is asymptotically optimal and build on this mechanism to extend our results to
m-unit auctions and sponsored search."
"We consider pricing in settings where a consumer discovers his value for a
good only as he uses it, and the value evolves with each use. We explore simple
and natural pricing strategies for a seller in this setting, under the
assumption that the seller knows the distribution from which the consumer's
initial value is drawn, as well as the stochastic process that governs the
evolution of the value with each use.
  We consider the differences between up-front or ""buy-it-now"" pricing (BIN),
and ""pay-per-play"" (PPP) pricing, where the consumer is charged per use. Our
results show that PPP pricing can be a very effective mechanism for price
discrimination, and thereby can increase seller revenue. But it can also be
advantageous to the buyers, as a way of mitigating risk. Indeed, this
mitigation of risk can yield a larger pool of buyers. We also show that the
practice of offering free trials is largely beneficial.
  We consider two different stochastic processes for how the buyer's value
evolves: In the first, the key random variable is how long the consumer remains
interested in the product. In the second process, the consumer's value evolves
according to a random walk or Brownian motion with reflection at 1, and
absorption at 0."
"In various markets where sellers compete in price, price oscillations are
observed rather than convergence to equilibrium. Such fluctuations have been
empirically observed in the retail market for gasoline, in airline pricing and
in the online sale of consumer goods. Motivated by this, we study a model of
price competition in which an equilibrium rarely exists. We seek to analyze the
welfare, despite the nonexistence of an equilibrium, and present welfare
guarantees as a function of the market power of the sellers.
  We first study best response dynamics in markets with sellers that provide a
homogeneous good, and show that except for a modest number of initial rounds,
the welfare is guaranteed to be high. We consider two variations: in the first
the sellers have full information about the valuation of the buyer. Here we
show that if there are $n$ items available across all sellers and $n_{\max}$ is
the maximum number of items controlled by any given seller, the ratio of the
optimal welfare to the achieved welfare will be at most
$\log(\frac{n}{n-n_{\max}+1})+1$. As the market power of the largest seller
diminishes, the welfare becomes closer to optimal. In the second variation we
consider an extended model where sellers have uncertainty about the buyer's
valuation. Here we similarly show that the welfare improves as the market power
of the largest seller decreases, yet with a worse ratio of
$\frac{n}{n-n_{\max}+1}$. The exponential gap in welfare between the two
variations quantifies the value of accurately learning the buyer valuation.
  Finally, we show that extending our results to heterogeneous goods in general
is not possible. Even for the simple class of $k$-additive valuations, there
exists a setting where the welfare approximates the optimal welfare within any
non-zero factor only for $O(1/s)$ fraction of the time, where $s$ is the number
of sellers."
"We propose a uniform approach for the design and analysis of prior-free
competitive auctions and online auctions. Our philosophy is to view the
benchmark function as a variable parameter of the model and study a broad class
of functions instead of a individual target benchmark. We consider a multitude
of well-studied auction settings, and improve upon a few previous results.
  (1) Multi-unit auctions. Given a $\beta$-competitive unlimited supply
auction, the best previously known multi-unit auction is $2\beta$-competitive.
We design a $(1+\beta)$-competitive auction reducing the ratio from $4.84$ to
$3.24$. These results carry over to matroid and position auctions.
  (2) General downward-closed environments. We design a $6.5$-competitive
auction improving upon the ratio of $7.5$. Our auction is noticeably simpler
than the previous best one.
  (3) Unlimited supply online auctions. Our analysis yields an auction with a
competitive ratio of $4.12$, which significantly narrows the margin of
$[4,4.84]$ previously known for this problem.
  A particularly important tool in our analysis is a simple decomposition
lemma, which allows us to bound the competitive ratio against a sum of
benchmark functions. We use this lemma in a ""divide and conquer"" fashion by
dividing the target benchmark into the sum of simpler functions."
"Peer review (e.g., grading assignments in Massive Open Online Courses
(MOOCs), academic paper review) is an effective and scalable method to evaluate
the products (e.g., assignments, papers) of a large number of agents when the
number of dedicated reviewing experts (e.g., teaching assistants, editors) is
limited. Peer review poses two key challenges: 1) identifying the reviewers'
intrinsic capabilities (i.e., adverse selection) and 2) incentivizing the
reviewers to exert high effort (i.e., moral hazard). Some works in mechanism
design address pure adverse selection using one-shot matching rules, and pure
moral hazard was addressed in repeated games with exogenously given and fixed
matching rules. However, in peer review systems exhibiting both adverse
selection and moral hazard, one-shot or exogenous matching rules do not link
agents' current behavior with future matches and future payoffs, and as we
prove, will induce myopic behavior (i.e., exerting the lowest effort) resulting
in the lowest review quality.
  In this paper, we propose for the first time a solution that simultaneously
solves adverse selection and moral hazard. Our solution exploits the repeated
interactions of agents, utilizes ratings to summarize agents' past review
quality, and designs matching rules that endogenously depend on agents'
ratings. Our proposed matching rules are easy to implement and require no
knowledge about agents' private information (e.g., their benefit and cost
functions). Yet, they are effective in guiding the system to an equilibrium
where the agents are incentivized to exert high effort and receive ratings that
precisely reflect their review quality. Using several illustrative examples, we
quantify the significant performance gains obtained by our proposed mechanism
as compared to existing one-shot or exogenous matching rules."
"In this work we apply methods from cryptography to enable any number of
mutually distrusting players to implement broad classes of mediated equilibria
of strategic games without the need for trusted mediation.
  Our implementation makes use of a (standard) pre-play ""cheap talk"" phase, in
which players engage in free and non-binding communication prior to playing in
the original game. In our cheap talk phase, the players execute a secure
multi-party computation protocol to sample an action profile from an
equilibrium of a ""cryptographically blinded"" version of the original game, in
which actions are encrypted. The essence of our approach is to exploit the
power of encryption to selectively restrict the information available to
players about sampled action profiles, such that these desirable equilibria can
be stably achieved. In contrast to previous applications of cryptography to
game theory, this work is the first to employ the paradigm of using encryption
to allow players to benefit from hiding information \emph{from themselves},
rather than from others; and we stress that rational players would
\emph{choose} to hide the information from themselves."
"We study anonymous posted price mechanisms for combinatorial auctions in a
Bayesian framework. In a posted price mechanism, item prices are posted, then
the consumers approach the seller sequentially in an arbitrary order, each
purchasing her favorite bundle from among the unsold items at the posted
prices. These mechanisms are simple, transparent and trivially dominant
strategy incentive compatible (DSIC).
  We show that when agent preferences are fractionally subadditive (which
includes all submodular functions), there always exist prices that, in
expectation, obtain at least half of the optimal welfare. Our result is
constructive: given black-box access to a combinatorial auction algorithm A,
sample access to the prior distribution, and appropriate query access to the
sampled valuations, one can compute, in polytime, prices that guarantee at
least half of the expected welfare of A. As a corollary, we obtain the first
polytime (in n and m) constant-factor DSIC mechanism for Bayesian submodular
combinatorial auctions, given access to demand query oracles. Our results also
extend to valuations with complements, where the approximation factor degrades
linearly with the level of complementarity."
"We put forward a new model of congestion games where agents have uncertainty
over the routes used by other agents. We take a non-probabilistic approach,
assuming that each agent knows that the number of agents using an edge is
within a certain range. Given this uncertainty, we model agents who either
minimize their worst-case cost (WCC) or their worst-case regret (WCR), and
study implications on equilibrium existence, convergence through adaptive play,
and efficiency. Under the WCC behavior the game reduces to a modified
congestion game, and welfare improves when agents have moderate uncertainty.
Under WCR behavior the game is not, in general, a congestion game, but we show
convergence and efficiency bounds for a simple class of games."
"We study strong equilibria in symmetric capacitated cost-sharing games. In
these games, a graph with designated source $s$ and sink $t$ is given, and each
edge is associated with some cost. Each agent chooses strategically an $s$-$t$
path, knowing that the cost of each edge is shared equally between all agents
using it. Two variants of cost-sharing games have been previously studied: (i)
games where coalitions can form, and (ii) games where edges are associated with
capacities; both variants are inspired by real-life scenarios. In this work we
combine these variants and analyze strong equilibria (profiles where no
coalition can deviate) in capacitated games. This combination gives rise to new
phenomena that do not occur in the previous variants. Our contribution is
two-fold. First, we provide a topological characterization of networks that
always admit a strong equilibrium. Second, we establish tight bounds on the
efficiency loss that may be incurred due to strategic behavior, as quantified
by the strong price of anarchy (and stability) measures. Interestingly, our
results are qualitatively different than those obtained in the analysis of each
variant alone, and the combination of coalitions and capacities entails the
introduction of more refined topology classes than previously studied."
"Infinite games where several players seek to coordinate under imperfect
information are deemed to be undecidable, unless the information is
hierarchically ordered among the players.
  We identify a class of games for which joint winning strategies can be
constructed effectively without restricting the direction of information flow.
Instead, our condition requires that the players attain common knowledge about
the actual state of the game over and over again along every play.
  We show that it is decidable whether a given game satisfies the condition,
and prove tight complexity bounds for the strategy synthesis problem under
$\omega$-regular winning conditions given by parity automata."
"Recently, there has been a number of papers relating mechanism design and
privacy (e.g., see \cite{MT07,Xia11,CCKMV11,NST12,NOS12,HK12}). All of these
papers consider a worst-case setting where there is no probabilistic
information about the players' types. In this paper, we investigate mechanism
design and privacy in the \emph{Bayesian} setting, where the players' types are
drawn from some common distribution. We adapt the notion of \emph{differential
privacy} to the Bayesian mechanism design setting, obtaining \emph{Bayesian
differential privacy}. We also define a robust notion of approximate
truthfulness for Bayesian mechanisms, which we call \emph{persistent
approximate truthfulness}. We give several classes of mechanisms (e.g., social
welfare mechanisms and histogram mechanisms) that achieve both Bayesian
differential privacy and persistent approximate truthfulness. These classes of
mechanisms can achieve optimal (economic) efficiency, and do not use any
payments. We also demonstrate that by considering the above mechanisms in a
modified mechanism design model, the above mechanisms can achieve actual
truthfulness."
"It is typically expected that if a mechanism is truthful, then the agents
would, indeed, truthfully report their private information. But why would an
agent believe that the mechanism is truthful? We wish to design truthful
mechanisms, whose truthfulness can be verified efficiently (in the
computational sense). Our approach involves three steps: (i) specifying the
structure of mechanisms, (ii) constructing a verification algorithm, and (iii)
measuring the quality of verifiably truthful mechanisms. We demonstrate this
approach using a case study: approximate mechanism design without money for
facility location."
"We consider a sequential inspection game where an inspector uses a limited
number of inspections over a larger number of time periods to detect a
violation (an illegal act) of an inspectee. Compared with earlier models, we
allow varying rewards to the inspectee for successful violations. As one
possible example, the most valuable reward may be the completion of a sequence
of thefts of nuclear material needed to build a nuclear bomb. The inspectee can
observe the inspector, but the inspector can only determine if a violation
happens during a stage where he inspects, which terminates the game; otherwise
the game continues. Under reasonable assumptions for the payoffs, the
inspector's strategy is independent of the number of successful violations.
This allows to apply a recursive description of the game, even though this
normally assumes fully informed players after each stage. The resulting
recursive equation in three variables for the equilibrium payoff of the game,
which generalizes several other known equations of this kind, is solved
explicitly in terms of sums of binomial coefficients. We also extend this
approach to non-zero-sum games and, similar to Maschler (1966), ""inspector
leadership"" where the inspector commits to (the same) randomized inspection
schedule, but the inspectee acts legally (rather than mixes as in the
simultaneous game) as long as inspections remain."
"One of the main results shown through Roughgarden's notions of smooth games
and robust price of anarchy is that, for any sum-bounded utilitarian social
function, the worst-case price of anarchy of coarse correlated equilibria
coincides with that of pure Nash equilibria in the class of weighted congestion
games with non-negative and non-decreasing latency functions and that such a
value can always be derived through the, so called, smoothness argument. We
significantly extend this result by proving that, for a variety of (even
non-sum-bounded) utilitarian and egalitarian social functions and for a broad
generalization of the class of weighted congestion games with non-negative (and
possibly decreasing) latency functions, the worst-case price of anarchy of
$\epsilon$-approximate coarse correlated equilibria still coincides with that
of $\epsilon$-approximate pure Nash equilibria, for any $\epsilon\geq 0$. As a
byproduct of our proof, it also follows that such a value can always be
determined by making use of the primal-dual method we introduced in a previous
work. It is important to note that our scenario of investigation is beyond the
scope of application of the robust price of anarchy (for as it is currently
defined), so that our result seems unlikely to be alternatively proved via the
smoothness framework."
"A fundamental decision faced by a firm hiring employees - and a familiar one
to anyone who has dealt with the academic job market, for example - is deciding
what caliber of candidates to pursue. Should the firm try to increase its
reputation by making offers to higher-quality candidates, despite the risk that
the candidates might reject the offers and leave the firm empty-handed? Or
should it concentrate on weaker candidates who are more likely to accept the
offer? The question acquires an added level of complexity once we take into
account the effect one hiring cycle has on the next: hiring better employees in
the current cycle increases the firm's reputation, which in turn increases its
attractiveness for higher-quality candidates in the next hiring cycle. These
considerations introduce an interesting temporal dynamic aspect to the rich
line of research on matching models for job markets, in which long-range
planning and evolving reputational effects enter into the strategic decisions
made by competing firms.
  We develop a model based on two competing firms to try capturing as cleanly
as possible the elements that we believe constitute the strategic tension at
the core of the problem: the trade-off between short-term recruiting success
and long-range reputation-building; the inefficiency that results from
underemployment of people who are not ranked highest; and the influence of
earlier accidental outcomes on long-term reputations.
  Our model exhibits all these phenomena in a stylized setting, governed by a
parameter q that captures the difference in strength between the two top
candidates in each hiring cycle. We show that when q is relatively low the
efficiency of the job market is improved by long-range reputational effects,
but when q is relatively high, taking future reputations into account can
sometimes reduce the efficiency."
"We study competitive diffusion games on graphs introduced by Alon et al. [1]
to model the spread of influence in social networks. Extending results of
Roshanbin [8] for two players, we investigate the existence of pure Nash
equilibria for at least three players on different classes of graphs including
paths, cycles, grid graphs and hypercubes; as a main contribution, we answer an
open question proving that there is no Nash equilibrium for three players on (m
x n) grids with min(m, n) >= 5. Further, extending results of Etesami and Basar
[3] for two players, we prove the existence of pure Nash equilibria for four
players on every d-dimensional hypercube."
"We develop a general game-theoretic framework for reasoning about strategic
agents performing possibly costly computation. In this framework, many
traditional game-theoretic results (such as the existence of a Nash
equilibrium) no longer hold. Nevertheless, we can use the framework to provide
psychologically appealing explanations of observed behavior in well-studied
games (such as finitely repeated prisoner's dilemma and rock-paper-scissors).
Furthermore, we provide natural conditions on games sufficient to guarantee
that equilibria exist."
"We study the problem of selling $n$ items to a single buyer with an additive
valuation function. We consider the valuation of the items to be correlated,
i.e., desirabilities of the buyer for the items are not drawn independently.
Ideally, the goal is to design a mechanism to maximize the revenue. However, it
has been shown that a revenue optimal mechanism might be very complicated and
as a result inapplicable to real-world auctions. Therefore, our focus is on
designing a simple mechanism that achieves a constant fraction of the optimal
revenue. Babaioff et al. propose a simple mechanism that achieves a constant
fraction of the optimal revenue for independent setting with a single additive
buyer. However, they leave the following problem as an open question: ""Is there
a simple, approximately optimal mechanism for a single additive buyer whose
value for $n$ items is sampled from a common base-value distribution?""
  Babaioff et al. show a constant approximation factor of the optimal revenue
can be achieved by either selling the items separately or as a whole bundle in
the independent setting. We show a similar result for the correlated setting
when the desirabilities of the buyer are drawn from a common base-value
distribution. It is worth mentioning that the core decomposition lemma which is
mainly the heart of the proofs for efficiency of the mechanisms does not hold
for correlated settings. Therefore we propose a modified version of this lemma
which is applicable to the correlated settings as well. Although we apply this
technique to show the proposed mechanism can guarantee a constant fraction of
the optimal revenue in a very weak correlation, this method alone can not
directly show the efficiency of the mechanism in stronger correlations."
"We consider a strategic variant of the facility location problem. We would
like to locate a facility on a closed interval. There are n agents located on
that interval, divided into two types: type 1 agents, who wish for the facility
to be as far from them as possible, and type 2 agents, who wish for the
facility to be as close to them as possible. Our goal is to maximize a form of
aggregated social benefit: maxisum- the sum of the agents' utilities, or the
egalitarian objective- the minimal agent utility. The strategic aspect of the
problem is that the agents' locations are not known to us, but rather reported
to us by the agents- an agent might misreport his location in an attempt to
move the facility away from or towards to his true location. We therefore
require the facility-locating mechanism to be strategyproof, namely that
reporting truthfully is a dominant strategy for each agent. As simply
maximizing the social benefit is generally not strategyproof, our goal is to
design strategyproof mechanisms with good approximation ratios.
  For the maxisum objective, in the deterministic setting, we provide a
best-possible 3- approximate strategyproof mechanism; in the randomized
setting, we provide a 23/13- approximate strategyproof mechanism and a lower
bound of \frac{2}{\sqrt{3}}. For the egalitarian objective, we provide a lower
bound of 3/2 in the randomized setting, and show that no bounded approximation
ratio is attainable in the deterministic setting. To obtain our deterministic
lower bounds, we characterize all deterministic strategyproof mechanisms when
all agents are of type 1. Finally, we consider a generalized model that allows
an agent to control more than one location, and provide best-possible 3- and
3/2- approximate strategyproof mechanisms for maxisum, in the deterministic and
randomized settings respectively, when only type 1 agents are present."
"In the early 1950s Lloyd Shapley proposed an ordinal and set-valued solution
concept for zero-sum games called \emph{weak saddle}. We show that all weak
saddles of a given zero-sum game are interchangeable and equivalent. As a
consequence, every such game possesses a unique set-based value."
"In cost sharing games, the existence and efficiency of pure Nash equilibria
fundamentally depends on the method that is used to share the resources' costs.
We consider a general class of resource allocation problems in which a set of
resources is used by a heterogeneous set of selfish users. The cost of a
resource is a (non-decreasing) function of the set of its users. Under the
assumption that the costs of the resources are shared by uniform cost sharing
protocols, i.e., protocols that use only local information of the resource's
cost structure and its users to determine the cost shares, we exactly quantify
the inefficiency of the resulting pure Nash equilibria. Specifically, we show
tight bounds on prices of stability and anarchy for games with only submodular
and only supermodular cost functions, respectively, and an asymptotically tight
bound for games with arbitrary set-functions. While all our upper bounds are
attained for the well-known Shapley cost sharing protocol, our lower bounds
hold for arbitrary uniform cost sharing protocols and are even valid for games
with anonymous costs, i.e., games in which the cost of each resource only
depends on the cardinality of the set of its users."
"Fictitious play is a natural dynamic for equilibrium play in zero-sum games,
proposed by [Brown 1949], and shown to converge by [Robinson 1951]. Samuel
Karlin conjectured in 1959 that fictitious play converges at rate
$O(1/\sqrt{t})$ with the number of steps $t$. We disprove this conjecture
showing that, when the payoff matrix of the row player is the $n \times n$
identity matrix, fictitious play may converge with rate as slow as
$\Omega(t^{-1/n})$."
"We show that the problem of finding an {\epsilon}-approximate Nash
equilibrium in an anonymous game with seven pure strategies is complete in
PPAD, when the approximation parameter {\epsilon} is exponentially small in the
number of players."
"Motivated by a problem of scheduling unit-length jobs with weak preferences
over time-slots, the random assignment problem (also called the house
allocation problem) is considered on a uniform preference domain. For the
subdomain in which preferences are strict except possibly for the class of
unacceptable objects, Bogomolnaia and Moulin characterized the probabilistic
serial mechanism as the only mechanism satisfying equal treatment of equals,
strategyproofness, and ordinal efficiency. The main result in this paper is
that the natural extension of the probabilistic serial mechanism to the domain
of weak, but uniform, preferences fails strategyproofness, but so does every
other mechanism that is ordinally efficient and treats equals equally. If
envy-free assignments are required, then any (probabilistic or deterministic)
mechanism that guarantees an ex post efficient outcome must fail even a weak
form of strategyproofness."
"We examine sequential equilibrium in the context of computational games,
where agents are charged for computation. In such games, an agent can
rationally choose to forget, so issues of imperfect recall arise. In this
setting, we consider two notions of sequential equilibrium. One is an ex ante
notion, where a player chooses his strategy before the game starts and is
committed to it, but chooses it in such a way that it remains optimal even off
the equilibrium path. The second is an interim notion, where a player can
reconsider at each information set whether he is doing the ""right"" thing, and
if not, can change his strategy. The two notions agree in games of perfect
recall, but not in games of imperfect recall. Although the interim notion seems
more appealing, \fullv{Halpern and Pass [2011] argue that there are some deep
conceptual problems with it in standard games of imperfect recall. We show that
the conceptual problems largely disappear in the computational setting.
Moreover, in this setting, under natural assumptions, the two notions coincide."
"We study the computation of equilibria of anonymous games, via algorithms
that may proceed via a sequence of adaptive queries to the game's payoff
function, assumed to be unknown initially. The general topic we consider is
\emph{query complexity}, that is, how many queries are necessary or sufficient
to compute an exact or approximate Nash equilibrium.
  We show that exact equilibria cannot be found via query-efficient algorithms.
We also give an example of a 2-strategy, 3-player anonymous game that does not
have any exact Nash equilibrium in rational numbers. However, more positive
query-complexity bounds are attainable if either further symmetries of the
utility functions are assumed or we focus on approximate equilibria. We
investigate four sub-classes of anonymous games previously considered by
\cite{bfh09, dp14}.
  Our main result is a new randomized query-efficient algorithm that finds a
$O(n^{-1/4})$-approximate Nash equilibrium querying $\tilde{O}(n^{3/2})$
payoffs and runs in time $\tilde{O}(n^{3/2})$. This improves on the running
time of pre-existing algorithms for approximate equilibria of anonymous games,
and is the first one to obtain an inverse polynomial approximation in
poly-time. We also show how this can be utilized as an efficient
polynomial-time approximation scheme (PTAS). Furthermore, we prove that
$\Omega(n \log{n})$ payoffs must be queried in order to find any
$\epsilon$-well-supported Nash equilibrium, even by randomized algorithms."
"Nash equilibrium (NE) assumes that players always make a best response.
However, this is not always true; sometimes people cooperate even it is not a
best response to do so. For example, in the Prisoner's Dilemma, people often
cooperate. Are there rules underlying cooperative behavior? In an effort to
answer this question, we propose a new equilibrium concept: perfect cooperative
equilibrium (PCE), and two related variants: max-PCE and cooperative
equilibrium. PCE may help explain players' behavior in games where cooperation
is observed in practice. A player's payoff in a PCE is at least as high as in
any NE. However, a PCE does not always exist. We thus consider {\alpha}-PCE,
where {\alpha} takes into account the degree of cooperation; a PCE is a 0-PCE.
Every game has a Pareto-optimal max-PCE (M-PCE); that is, an {\alpha}-PCE for a
maximum {\alpha}. We show that M-PCE does well at predicting behavior in quite
a few games of interest. We also consider cooperative equilibrium (CE), another
generalization of PCE that takes punishment into account. Interestingly, all
Pareto-optimal M-PCE are CE. We prove that, in 2-player games, a PCE (if it
exists), a M-PCE, and a CE can all be found in polynomial time using bilinear
programming. This is a contrast to Nash equilibrium, which is PPAD complete
even in 2-player games [Chen, Deng, and Teng 2009]. We compare M-PCE to the
coco value [Kalai and Kalai 2009], another solution concept that tries to
capture cooperation, both axiomatically and in terms of an algebraic
characterization, and show that the two are closely related, despite their very
different definitions."
"Auctions have been proposed as a way to provide economic incentives for
primary users to dynamically allocate unused spectrum to other users in need of
it. Previously proposed schemes do not take into account the fact that the
power constraints of users might prevent them from transmitting their bid
prices to the auctioneer with high precision and that transmitted bid prices
must travel through a noisy channel. These schemes also have very high
overheads which cannot be accommodated in wireless standards. We propose
auction schemes where a central clearing authority auctions spectrum to users
who bid for it, while taking into account quantization of prices, overheads in
bid revelation, and noise in the channel explicitly. Our schemes are closely
related to channel output feedback problems and, specifically, to the technique
of posterior matching. We consider several scenarios where the objective of the
clearing authority is to award spectrum to the bidders who value spectrum the
most. We prove theoretically that this objective is asymptotically attained by
our scheme when the bidders are non-strategic with constant bids. We propose
separate schemes to make strategic users reveal their private values
truthfully, to auction multiple sub-channels among strategic users, and to
track slowly time-varying bid prices. Our simulations illustrate the optimality
of our schemes for constant bid prices, and also demonstrate the effectiveness
of our tracking algorithm for slowly time-varying bids."
"Approval-like voting rules, such as Sincere-Strategy Preference-Based
Approval voting (SP-AV), the Bucklin rule (an adaptive variant of $k$-Approval
voting), and the Fallback rule (an adaptive variant of SP-AV) have many
desirable properties: for example, they are easy to understand and encourage
the candidates to choose electoral platforms that have a broad appeal. In this
paper, we investigate both classic and parameterized computational complexity
of electoral campaign management under such rules. We focus on two methods that
can be used to promote a given candidate: asking voters to move this candidate
upwards in their preference order or asking them to change the number of
candidates they approve of. We show that finding an optimal campaign management
strategy of the first type is easy for both Bucklin and Fallback. In contrast,
the second method is computationally hard even if the degree to which we need
to affect the votes is small. Nevertheless, we identify a large class of
scenarios that admit fixed-parameter tractable algorithms."
"In congested urban areas, it remains a pressing challenge to reduce
unnecessary vehicle circling for parking while at the same time maximize
parking space utilization. In observance of new information technologies that
have become readily accessible to drivers and parking agencies, we develop a
dynamic non-cooperative bi-level model (i.e. Stackelberg leader-follower game)
to set parking prices in real-time for effective parking access and space
utilization. The model is expected to fit into an integrated parking pricing
and management system, where parking reservations and transactions are
facilitated by sensing and informatics infrastructures, that ensures the
availability of convenient spaces at equilibrium market prices. It is shown
with numerical examples that the proposed dynamic parking pricing model has the
potential to virtually eliminate vehicle circling for parking, which results in
significant reduction in adverse socioeconomic externalities such as traffic
congestion and emissions."
"Team formation is a core problem in AI. Remarkably, little prior work has
addressed the problem of mechanism design for team formation, accounting for
the need to elicit agents' preferences over potential teammates. Coalition
formation in the related hedonic games has received much attention, but only
from the perspective of coalition stability, with little emphasis on the
mechanism design objectives of true preference elicitation, social welfare, and
equity. We present the first formal mechanism design framework for team
formation, building on recent combinatorial matching market design literature.
We exhibit four mechanisms for this problem, two novel, two simple extensions
of known mechanisms from other domains. Two of these (one new, one known) have
desirable theoretical properties. However, we use extensive experiments to show
our second novel mechanism, despite having no theoretical guarantees,
empirically achieves good incentive compatibility, welfare, and fairness."
"McLennan and Tourky (2010) showed that ""imitation games"" provide a new view
of the computation of Nash equilibria of bimatrix games with the Lemke-Howson
algorithm. In an imitation game, the payoff matrix of one of the players is the
identity matrix. We study the more general ""unit vector games"", which are
already known, where the payoff matrix of one player is composed of unit
vectors. Our main application is a simplification of the construction by Savani
and von Stengel (2006) of bimatrix games where two basic equilibrium-finding
algorithms take exponentially many steps: the Lemke-Howson algorithm, and
support enumeration."
"We propose a hybrid spectrum and information market for a database-assisted
TV white space network, where the geo-location database serves as both a
spectrum market platform and an information market platform. We study the
inter- actions among the database operator, the spectrum licensee, and
unlicensed users systematically, using a three-layer hierarchical model. In
Layer I, the database and the licensee negotiate the commission fee that the
licensee pays for using the spectrum market platform. In Layer II, the database
and the licensee compete for selling information or channels to unlicensed
users. In Layer III, unlicensed users determine whether they should buy the
exclusive usage right of licensed channels from the licensee, or the
information regarding unlicensed channels from the database. Analyzing such a
three-layer model is challenging due to the co-existence of both positive and
negative network externalities in the information market. We characterize how
the network externalities affect the equilibrium behaviours of all parties
involved. Our numerical results show that the proposed hybrid market can
improve the network profit up to 87%, compared with a pure information market.
Meanwhile, the achieved network profit is very close to the coordinated
benchmark solution (the gap is less than 4% in our simulation)."
"Evolutionarily stable strategy (ESS) is a key concept in evolutionary game
theory. ESS provides an evolutionary stability criterion for biological, social
and economical behaviors. In this paper, we develop a new approach to evaluate
ESS in symmetric two player games with fuzzy payoffs. Particularly, every
strategy is assigned a fuzzy membership that describes to what degree it is an
ESS in presence of uncertainty. The fuzzy set of ESS characterize the nature of
ESS. The proposed approach avoids loss of any information that happens by the
defuzzification method in games and handles uncertainty of payoffs through all
steps of finding an ESS. We use the satisfaction function to compare fuzzy
payoffs, and adopts the fuzzy decision rule to obtain the membership function
of the fuzzy set of ESS. The theorem shows the relation between fuzzy ESS and
fuzzy Nash quilibrium. The numerical results illustrate the proposed method is
an appropriate generalization of ESS to fuzzy payoff games."
"We consider the problem of designing mechanisms that interact with strategic
agents through strategic intermediaries (or mediators), and investigate the
cost to society due to the mediators' strategic behavior. Selfish agents with
private information are each associated with exactly one strategic mediator,
and can interact with the mechanism exclusively through that mediator. Each
mediator aims to optimize the combined utility of his agents, while the
mechanism aims to optimize the combined utility of all agents. We focus on the
problem of facility location on a metric induced by a publicly known tree. With
non-strategic mediators, there is a dominant strategy mechanism that is
optimal. We show that when both agents and mediators act strategically, there
is no dominant strategy mechanism that achieves any approximation. We, thus,
slightly relax the incentive constraints, and define the notion of a two-sided
incentive compatible mechanism. We show that the $3$-competitive deterministic
mechanism suggested by Procaccia and Tennenholtz (2013) and Dekel et al. (2010)
for lines extends naturally to trees, and is still $3$-competitive as well as
two-sided incentive compatible. This is essentially the best possible. We then
show that by allowing randomization one can construct a $2$-competitive
randomized mechanism that is two-sided incentive compatible, and this is also
essentially tight. This result also closes a gap left in the work of Procaccia
and Tennenholtz (2013) and Lu et al. (2009) for the simpler problem of
designing strategy-proof mechanisms for weighted agents with no mediators on a
line, while extending to the more general model of trees. We also investigate a
further generalization of the above setting where there are multiple levels of
mediators."
"Symmetry is inherent in the definition of most of the two-player zero-sum
games, including parity, mean-payoff, and discounted-payoff games. It is
therefore quite surprising that no symmetric analysis techniques for these
games exist. We develop a novel symmetric strategy improvement algorithm where,
in each iteration, the strategies of both players are improved simultaneously.
We show that symmetric strategy improvement defies Friedmann's traps, which
shook the belief in the potential of classic strategy improvement to be
polynomial."
"In AAMAS 2014, Bouveret and Lemaitre (2014) presented a hierarchy of fairness
concepts for allocation of indivisible objects. Among them CEEI (Competitive
Equilibrium with Equal Incomes) was the strongest. In this note, we settle the
complexity of computing a discrete CEEI assignment by showing it is strongly
NP-hard. We then highlight a fairness notion (CEEI-FRAC) that is even stronger
than CEEI for discrete assignments, is always Pareto optimal, and can be
verified in polynomial time. We also show that computing a CEEI-FRAC discrete
assignment is strongly NP-hard in general but polynomial-time computable if the
utilities are zero or one."
"We introduce natural strategic games on graphs, which capture the idea of
coordination in a local setting. We study the existence of equilibria that are
resilient to coalitional deviations of unbounded and bounded size (i.e., strong
equilibria and k-equilibria respectively). We show that pure Nash equilibria
and 2-equilibria exist, and give an example in which no 3-equilibrium exists.
Moreover, we prove that strong equilibria exist for various special cases.
  We also study the price of anarchy (PoA) and price of stability (PoS) for
these solution concepts. We show that the PoS for strong equilibria is 1 in
almost all of the special cases for which we have proven strong equilibria to
exist. The PoA for pure Nash equilbria turns out to be unbounded, even when we
fix the graph on which the coordination game is to be played. For the PoA for
k-equilibria, we show that the price of anarchy is between 2(n-1)/(k-1) - 1 and
2(n-1)/(k-1). The latter upper bound is tight for $k=n$ (i.e., strong
equilibria).
  Finally, we consider the problems of computing strong equilibria and of
determining whether a joint strategy is a k-equilibrium or strong equilibrium.
We prove that, given a coordination game, a joint strategy s, and a number k as
input, it is co-NP complete to determine whether s is a k-equilibrium. On the
positive side, we give polynomial time algorithms to compute strong equilibria
for various special cases."
"We study the revenue maximization problem of a seller with n heterogeneous
items for sale to a single buyer whose valuation function for sets of items is
unknown and drawn from some distribution D. We show that if D is a distribution
over subadditive valuations with independent items, then the better of pricing
each item separately or pricing only the grand bundle achieves a
constant-factor approximation to the revenue of the optimal mechanism. This
includes buyers who are k-demand, additive up to a matroid constraint, or
additive up to constraints of any downwards-closed set system (and whose values
for the individual items are sampled independently), as well as buyers who are
fractionally subadditive with item multipliers drawn independently. Our proof
makes use of the core-tail decomposition framework developed in prior work
showing similar results for the significantly simpler class of additive buyers
[LY13, BILW14].
  In the second part of the paper, we develop a connection between
approximately optimal simple mechanisms and approximate revenue monotonicity
with respect to buyers' valuations. Revenue non-monotonicity is the phenomenon
that sometimes strictly increasing buyers' values for every set can strictly
decrease the revenue of the optimal mechanism [HR12]. Using our main result, we
derive a bound on how bad this degradation can be (and dub such a bound a proof
of approximate revenue monotonicity); we further show that better bounds on
approximate monotonicity imply a better analysis of our simple mechanisms."
"In this work we consider selling items using a sequential first price auction
mechanism. We generalize the assumption of conservative bidding to extensive
form games (henceforth optimistic conservative bidding), and show that for both
linear and unit demand valuations, the only pure subgame perfect equilibrium
where buyers are bidding in an optimistic conservative manner is the minimal
Walrasian equilibrium.
  In addition, we show examples where without the requirement of conservative
bidding, subgame perfect equilibria can admit a variety of unlikely
predictions, including high price of anarchy and low revenue in markets
composed of additive bidders, equilibria which elicit all the surplus as
revenue, and more. We also show that the order in which the items are sold can
influence the outcome."
"In the design and analysis of revenue-maximizing auctions, auction
performance is typically measured with respect to a prior distribution over
inputs. The most obvious source for such a distribution is past data. The goal
is to understand how much data is necessary and sufficient to guarantee
near-optimal expected revenue.
  Our basic model is a single-item auction in which bidders' valuations are
drawn independently from unknown and non-identical distributions. The seller is
given $m$ samples from each of these distributions ""for free"" and chooses an
auction to run on a fresh sample. How large does m need to be, as a function of
the number k of bidders and eps > 0, so that a (1 - eps)-approximation of the
optimal revenue is achievable?
  We prove that, under standard tail conditions on the underlying
distributions, m = poly(k, 1/eps) samples are necessary and sufficient. Our
lower bound stands in contrast to many recent results on simple and
prior-independent auctions and fundamentally involves the interplay between
bidder competition, non-identical distributions, and a very close (but still
constant) approximation of the optimal revenue. It effectively shows that the
only way to achieve a sufficiently good constant approximation of the optimal
revenue is through a detailed understanding of bidders' valuation
distributions. Our upper bound is constructive and applies in particular to a
variant of the empirical Myerson auction, the natural auction that runs the
revenue-maximizing auction with respect to the empirical distributions of the
samples.
  Our sample complexity lower bound depends on the set of allowable
distributions, and to capture this we introduce alpha-strongly regular
distributions, which interpolate between the well-studied classes of regular
(alpha = 0) and MHR (alpha = 1) distributions. We give evidence that this
definition is of independent interest."
"A new game-theoretic approach for combining multiple classifiers is proposed.
A short introduction in Game Theory and coalitions illustrate the way any
collective decision scheme can be viewed as a competitive game of coalitions
that are formed naturally when players state their preferences. The winning
conditions and the voting power of each player are studied under the scope of
voting power indices, as well and the collective competence of the group.
Coalitions and power indices are presented in relation to the Condorcet
criterion of optimality in voting systems, and weighted Borda count models are
asserted as a way to implement them in practice. A special case of coalition
games, the weighted majority games (WMG) are presented as a restricted
realization in dichotomy choice situations. As a result, the weighted majority
rules (WMR), an extended version of the simple majority rules, are asserted as
the theoretically optimal and complete solution to this type of coalition
gaming. Subsequently, a generalized version of WMRs is suggested as the means
to design a voting system that is optimal in the sense of both the correct
classification criterion and the Condorcet efficiency criterion. In the scope
of Pattern Recognition, a generalized risk-based approach is proposed as the
framework upon which any classifier combination scheme can be applied. A new
fully adaptive version of WMRs is proposed as a statistically invariant way of
adjusting the design process of the optimal WMR to the arbitrary
non-symmetrical properties of the underlying feature space. SVM theory is
associated with properties and conclusions that emerge from the game-theoretic
approach of the classification in general, while the theoretical and practical
implications of employing SVM experts in WMR combination schemes are briefly
discussed. Finally, a summary of the most important issues for further research
is presented."
"We design a mechanism for Fair and Efficient Distribution of Resources
(FEDoR) in the presence of strategic agents. We consider a multiple-instances,
Bayesian setting, where in each round the preference of an agent over the set
of resources is a private information. We assume that in each of r rounds n
agents are competing for k non-identical indivisible goods, (n > k). In each
round the strategic agents declare how much they value receiving any of the
goods in the specific round. The agent declaring the highest valuation receives
the good with the highest value, the agent with the second highest valuation
receives the second highest valued good, etc. Hence we assume a decision
function that assigns goods to agents based on their valuations. The novelty of
the mechanism is that no payment scheme is required to achieve truthfulness in
a setting with rational/strategic agents. The FEDoR mechanism takes advantage
of the repeated nature of the framework, and through a statistical test is able
to punish the misreporting agents and be fair, truthful, and socially
efficient. FEDoR is fair in the sense that, in expectation over the course of
the rounds, all agents will receive the same good the same amount of times.
FEDoR is an eligible candidate for applications that require fair distribution
of resources over time. For example, equal share of bandwidth for nodes through
the same point of access. But further on, FEDoR can be applied in less trivial
settings like sponsored search, where payment is necessary and can be given in
the form of a flat participation fee. To this extent we perform a comparison
with traditional mechanisms applied to sponsored search, presenting the
advantage of FEDoR."
"Extensive-form games constitute the standard representation scheme for games
with a temporal component. But do all extensive-form games correspond to
protocols that we can implement in the real world? We often rule out games with
imperfect recall, which prescribe that an agent forget something that she knew
before. In this paper, we show that even some games with perfect recall can be
problematic to implement. Specifically, we show that if the agents have a sense
of time passing (say, access to a clock), then some extensive-form games can no
longer be implemented; no matter how we attempt to time the game, some
information will leak to the agents that they are not supposed to have. We say
such a game is not exactly timeable. We provide easy-to-check necessary and
sufficient conditions for a game to be exactly timeable. Most of the technical
depth of the paper concerns how to approximately time games, which we show can
always be done, though it may require large amounts of time. Specifically, we
show that for some games the time required to approximately implement the game
grows as a power tower of height proportional to the number of players and with
a parameter that measures the precision of the approximation at the top of the
power tower. In practice, that makes the games untimeable. Besides the
conceptual contribution to game theory, we believe our methodology can have
applications to preventing information leakage in security protocols."
"We study the Price of Anarchy of mechanisms for the well-known problem of
one-sided matching, or house allocation, with respect to the social welfare
objective. We consider both ordinal mechanisms, where agents submit preference
lists over the items, and cardinal mechanisms, where agents may submit
numerical values for the items being allocated. We present a general lower
bound of $\Omega(\sqrt{n})$ on the Price of Anarchy, which applies to all
mechanisms. We show that two well-known mechanisms, Probabilistic Serial, and
Random Priority, achieve a matching upper bound. We extend our lower bound to
the Price of Stability of a large class of mechanisms that satisfy a common
proportionality property, and show stronger bounds on the Price of Anarchy of
all deterministic mechanisms."
"We revisit a classic coordination problem from the perspective of mechanism
design: how can we coordinate a social welfare maximizing flow in a network
congestion game with selfish players? The classical approach, which computes
tolls as a function of known demands, fails when the demands are unknown to the
mechanism designer, and naively eliciting them does not necessarily yield a
truthful mechanism. Instead, we introduce a weak mediator that can provide
suggested routes to players and set tolls as a function of reported demands.
However, players can choose to ignore or misreport their type to this mediator.
Using techniques from differential privacy, we show how to design a weak
mediator such that it is an asymptotic ex-post Nash equilibrium for all players
to truthfully report their types to the mediator and faithfully follow its
suggestion, and that when they do, they end up playing a nearly optimal flow.
Notably, our solution works in settings of incomplete information even in the
absence of a prior distribution on player types. Along the way, we develop new
techniques for privately solving convex programs which may be of independent
interest."
"Individual decision-makers consume information revealed by the previous
decision makers, and produce information that may help in future decisions.
This phenomenon is common in a wide range of scenarios in the Internet economy,
as well as in other domains such as medical decisions. Each decision-maker
would individually prefer to ""exploit"": select an action with the highest
expected reward given her current information. At the same time, each
decision-maker would prefer previous decision-makers to ""explore"", producing
information about the rewards of various actions. A social planner, by means of
carefully designed information disclosure, can incentivize the agents to
balance the exploration and exploitation so as to maximize social welfare.
  We formulate this problem as a multi-armed bandit problem (and various
generalizations thereof) under incentive-compatibility constraints induced by
the agents' Bayesian priors. We design an incentive-compatible bandit algorithm
for the social planner whose regret is asymptotically optimal among all bandit
algorithms (incentive-compatible or not). Further, we provide a black-box
reduction from an arbitrary multi-arm bandit algorithm to an
incentive-compatible one, with only a constant multiplicative increase in
regret. This reduction works for very general bandit setting that incorporate
contexts and arbitrary auxiliary feedback."
"In this paper we investigate the problem of designing a spectrum scanning
strategy to detect an intelligent Invader who wants to utilize spectrum
undetected for his/her unapproved purposes. To deal with this problem we model
the situation as two games, between a Scanner and an Invader, and solve them
sequentially. The first game is formulated to design the optimal (in maxmin
sense) scanning algorithm, while the second one allows one to find the optimal
values of the parameters for the algorithm depending on parameters of the
network. These games provide solutions for two dilemmas that the rivals face.
The Invader's dilemma consists of the following: the more bandwidth the Invader
attempts to use leads to a larger payoff if he is not detected, but at the same
time also increases the probability of being detected and thus fined.
Similarly, the Scanner faces a dilemma: the wider the bandwidth scanned, the
higher the probability of detecting the Invader, but at the expense of
increasing the cost of building the scanning system. The equilibrium strategies
are found explicitly and reveal interesting properties. In particular, we have
found a discontinuous dependence of the equilibrium strategies on the network
parameters, fine and the type of the Invader's award. This discontinuity of the
fine means that the network provider has to take into account a human/social
factor since some threshold values of fine could be very sensible for the
Invader, while in other situations simply increasing the fine has minimal
deterrence impact. Also we show how incomplete information about the Invader's
technical characteristics and reward (e.g. motivated by using different type of
application, say, video-streaming or downloading files) can be incorporated
into scanning strategy to increase its efficiency."
"The probabilistic serial (PS) rule is a prominent randomized rule for
assigning indivisible goods to agents. Although it is well known for its good
fairness and welfare properties, it is not strategyproof. In view of this, we
address several fundamental questions regarding equilibria under PS. Firstly,
we show that Nash deviations under the PS rule can cycle. Despite the
possibilities of cycles, we prove that a pure Nash equilibrium is guaranteed to
exist under the PS rule. We then show that verifying whether a given profile is
a pure Nash equilibrium is coNP-complete, and computing a pure Nash equilibrium
is NP-hard. For two agents, we present a linear-time algorithm to compute a
pure Nash equilibrium which yields the same assignment as the truthful profile.
Finally, we conduct experiments to evaluate the quality of the equilibria that
exist under the PS rule, finding that the vast majority of pure Nash equilibria
yield social welfare that is at least that of the truthful profile."
"While there have been a number of studies about the efficacy of methods to
find exact Nash equilibria in bimatrix games, there has been little empirical
work on finding approximate Nash equilibria. Here we provide such a study that
compares a number of approximation methods and exact methods. In particular, we
explore the trade-off between the quality of approximate equilibrium and the
required running time to find one. We found that the existing library GAMUT,
which has been the de facto standard that has been used to test exact methods,
is insufficient as a test bed for approximation methods since many of its games
have pure equilibria or other easy-to-find good approximate equilibria. We
extend the breadth and depth of our study by including new interesting families
of bimatrix games, and studying bimatrix games upto size $2000 \times 2000$.
Finally, we provide new close-to-worst-case examples for the best-performing
algorithms for finding approximate Nash equilibria."
"We study the problem of locating a single facility on a real line based on
the reports of self-interested agents, when agents have double-peaked
preferences, with the peaks being on opposite sides of their locations. We
observe that double-peaked preferences capture real-life scenarios and thus
complement the well-studied notion of single-peaked preferences. We mainly
focus on the case where peaks are equidistant from the agents' locations and
discuss how our results extend to more general settings. We show that most of
the results for single-peaked preferences do not directly apply to this
setting; this makes the problem essentially more challenging. As our main
contribution, we present a simple truthful-in-expectation mechanism that
achieves an approximation ratio of 1+b/c for both the social and the maximum
cost, where b is the distance of the agent from the peak and c is the minimum
cost of an agent. For the latter case, we provide a 3/2 lower bound on the
approximation ratio of any truthful-in-expectation mechanism. We also study
deterministic mechanisms under some natural conditions, proving lower bounds
and approximation guarantees. We prove that among a large class of reasonable
mechanisms, there is no deterministic mechanism that outperforms our
truthful-in-expectation mechanism."
"We study the trade-offs between strategyproofness and other desiderata, such
as efficiency or fairness, that often arise in the design of random ordinal
mechanisms. We use approximate strategyproofness to define manipulability, a
measure to quantify the incentive properties of non-strategyproof mechanisms,
and we introduce the deficit, a measure to quantify the performance of
mechanisms with respect to another desideratum. When this desideratum is
incompatible with strategyproofness, mechanisms that trade off manipulability
and deficit optimally form the Pareto frontier. Our main contribution is a
structural characterization of this Pareto frontier, and we present algorithms
that exploit this structure to compute it. To illustrate its shape, we apply
our results for two different desiderata, namely Plurality and Veto scoring, in
settings with 3 alternatives and up to 18 agents."
"The Deferred Acceptance Algorithm (DAA) is the most widely accepted and used
algorithm to match students, workers, or residents to colleges, firms or
hospitals respectively. In this paper, we consider for the first time, the
complexity of manipulating DAA by agents such as colleges that have capacity
more than one. For such agents, truncation is not an exhaustive strategy. We
present efficient algorithms to compute a manipulation for the colleges when
the colleges are proposing or being proposed to. We then conduct detailed
experiments on the frequency of manipulable instances in order to get better
insight into strategic aspects of two-sided matching markets. Our results bear
somewhat negative news: assuming that agents have information other agents'
preference, they not only often have an incentive to misreport but there exist
efficient algorithms to find such a misreport."
"We study the problem of a buyer (aka auctioneer) who gains stochastic rewards
by procuring multiple units of a service or item from a pool of heterogeneous
strategic agents. The reward obtained for a single unit from an allocated agent
depends on the inherent quality of the agent; the agent's quality is fixed but
unknown. Each agent can only supply a limited number of units (capacity of the
agent). The costs incurred per unit and capacities are private information of
the agents. The auctioneer is required to elicit costs as well as capacities
(making the mechanism design bidimensional) and further, learn the qualities of
the agents as well, with a view to maximize her utility. Motivated by this, we
design a bidimensional multi-armed bandit procurement auction that seeks to
maximize the expected utility of the auctioneer subject to incentive
compatibility and individual rationality while simultaneously learning the
unknown qualities of the agents. We first assume that the qualities are known
and propose an optimal, truthful mechanism 2D-OPT for the auctioneer to elicit
costs and capacities. Next, in order to learn the qualities of the agents in
addition, we provide sufficient conditions for a learning algorithm to be
Bayesian incentive compatible and individually rational. We finally design a
novel learning mechanism, 2D-UCB that is stochastic Bayesian incentive
compatible and individually rational."
"We are interested in the problem of optimal commitments in rank-and-bid based
auctions, a general class of auctions that include first price and all-pay
auctions as special cases. Our main contribution is a novel approach to solve
for optimal commitment in this class of auctions, for any continuous type
distributions. Applying our approach, we are able to solve optimal commitments
for first-price and all-pay auctions in closed-form for fairly general
distribution settings. The optimal commitments functions in these auctions
reveal two surprisingly opposite insights: in the optimal commitment, the
leader bids passively when he has a low type. We interpret this as a credible
way to alleviate competition and to collude. In sharp contrast, when his type
is high enough, the leader sometimes would go so far as to bid above his own
value. We interpret this as a credible way to threat. Combing both insights, we
show via concrete examples that the leader is indeed willing to do so to secure
more utility when his type is in the middle. Our main approach consists of a
series of nontrivial innovations. In particular we put forward a concept called
equal-bid function that connects both players' strategies, as well as a concept
called equal-utility curve that smooths any leader strategy into a continuous
and differentiable strategy. We believe these techniques and insights are
general and can be applied to similar problems."
"Participatory sensing is a powerful paradigm which takes advantage of
smartphones to collect and analyze data beyond the scale of what was previously
possible. Given that participatory sensing systems rely completely on the
users' willingness to submit up-to-date and accurate information, it is
paramount to effectively incentivize users' active and reliable participation.
In this paper, we survey existing literature on incentive mechanisms for
participatory sensing systems. In particular, we present a taxonomy of existing
incentive mechanisms for participatory sensing systems, which are subsequently
discussed in depth by comparing and contrasting different approaches. Finally,
we discuss an agenda of open research challenges in incentivizing users in
participatory sensing."
"In this paper, we consider permutation manipulations by any subset of women
in the Gale-Shapley algorithm. This paper is motivated by the college
admissions process in China. Our results also answer Gusfield and Irving's open
problem on what can be achieved by permutation manipulations. We present an
efficient algorithm to find a strategy profile such that the induced matching
is stable and Pareto-optimal while the strategy profile itself is
inconspicuous. Surprisingly, we show that such a strategy profile actually
forms a Nash equilibrium of the manipulation game. We also show that a strong
Nash equilibrium or a super-strong Nash equilibrium does not always exist in
general and it is NP-hard to check the existence of these equilibria. We
consider an alternative notion of strong Nash equilibria and super-strong Nash
equilibrium. Under such notions, we characterize the super-strong Nash
equilibrium by Pareto-optimal strategy profiles.
  In the end, we show that it is NP-complete to find a manipulation that is
strictly better for all members of the coalition. This result demonstrates a
sharp contrast between weakly better-off outcomes and strictly better-off
outcomes."
"We study the classic setting of envy-free pricing, in which a single seller
chooses prices for its many items, with the goal of maximizing revenue once the
items are allocated. Despite the large body of work addressing such settings,
most versions of this problem have resisted good approximation factors for
maximizing revenue; this is true even for the classic unit-demand case. In this
paper we study envy-free pricing with unit-demand buyers, but unlike previous
work we focus on large markets: ones in which the demand of each buyer is
infinitesimally small compared to the size of the overall market. We assume
that the buyer valuations for the items they desire have a nice (although
reasonable) structure, i.e., that the aggregate buyer demand has a monotone
hazard rate and that the values of every buyer type come from the same support.
  For such large markets, our main contribution is a 1.88 approximation
algorithm for maximizing revenue, showing that good pricing schemes can be
computed when the number of buyers is large. We also give a (e,2)-bicriteria
algorithm that simultaneously approximates both maximum revenue and welfare,
thus showing that it is possible to obtain both good revenue and welfare at the
same time. We further generalize our results by relaxing some of our
assumptions, and quantify the necessary tradeoffs between revenue and welfare
in our setting. Our results are the first known approximations for large
markets, and crucially rely on new lower bounds which we prove for the
revenue-maximizing prices."
"We study the problem of computing maximin share guarantees, a recently
introduced fairness notion. Given a set of $n$ agents and a set of goods, the
maximin share of a single agent is the best that she can guarantee to herself,
if she would be allowed to partition the goods in any way she prefers, into $n$
bundles, and then receive her least desirable bundle. The objective then in our
problem is to find a partition, so that each agent is guaranteed her maximin
share. In settings with indivisible goods, such allocations are not guaranteed
to exist, so we resort to approximation algorithms. Our main result is a
$2/3$-approximation, that runs in polynomial time for any number of agents.
This improves upon the algorithm of Procaccia and Wang, which also produces a
$2/3$-approximation but runs in polynomial time only for a constant number of
agents. To achieve this, we redesign certain parts of their algorithm.
Furthermore, motivated by the apparent difficulty, both theoretically and
experimentally, in finding lower bounds on the existence of approximate
solutions, we undertake a probabilistic analysis. We prove that in randomly
generated instances, with high probability there exists a maximin share
allocation. This can be seen as a justification of the experimental evidence
reported in relevant works.
  Finally, we provide further positive results for two special cases that arise
from previous works. The first one is the intriguing case of $3$ agents, for
which it is already known that exact maximin share allocations do not always
exist (contrary to the case of $2$ agents). We provide a $7/8$-approximation
algorithm, improving the previously known result of $3/4$. The second case is
when all item values belong to $\{0, 1, 2\}$, extending the $\{0, 1\}$ setting
studied in Bouveret and Lema\^itre. We obtain an exact algorithm for any number
of agents in this case."
"Optimal mechanisms have been provided in quite general multi-item settings,
as long as each bidder's type distribution is given explicitly by listing every
type in the support along with its associated probability. In the implicit
setting, e.g. when the bidders have additive valuations with independent and/or
continuous values for the items, these results do not apply, and it was
recently shown that exact revenue optimization is intractable, even when there
is only one bidder. Even for item distributions with special structure, optimal
mechanisms have been surprisingly rare and the problem is challenging even in
the two-item case. In this paper, we provide a framework for designing optimal
mechanisms using optimal transport theory and duality theory. We instantiate
our framework to obtain conditions under which only pricing the grand bundle is
optimal in multi-item settings (complementing the work of [Manelli and Vincent
2006], as well as to characterize optimal two-item mechanisms. We use our
results to derive closed-form descriptions of the optimal mechanism in several
two-item settings, exhibiting also a setting where a continuum of lotteries is
necessary for revenue optimization but a closed-form representation of the
mechanism can still be found efficiently using our framework."
"We study the revenue performance of sequential posted price mechanisms and
some natural extensions, for a general setting where the valuations of the
buyers are drawn from a correlated distribution. Sequential posted price
mechanisms are conceptually simple mechanisms that work by proposing a
take-it-or-leave-it offer to each buyer. We apply sequential posted price
mechanisms to single-parameter multi-unit settings in which each buyer demands
only one item and the mechanism can assign the service to at most k of the
buyers. For standard sequential posted price mechanisms, we prove that with the
valuation distribution having finite support, no sequential posted price
mechanism can extract a constant fraction of the optimal expected revenue, even
with unlimited supply. We extend this result to the the case of a continuous
valuation distribution when various standard assumptions hold simultaneously.
In fact, it turns out that the best fraction of the optimal revenue that is
extractable by a sequential posted price mechanism is proportional to ratio of
the highest and lowest possible valuation. We prove that for two simple
generalizations of these mechanisms, a better revenue performance can be
achieved: if the sequential posted price mechanism has for each buyer the
option of either proposing an offer or asking the buyer for its valuation, then
a Omega(1/max{1,d}) fraction of the optimal revenue can be extracted, where d
denotes the degree of dependence of the valuations, ranging from complete
independence (d=0) to arbitrary dependence (d=n-1). Moreover, when we
generalize the sequential posted price mechanisms further, such that the
mechanism has the ability to make a take-it-or-leave-it offer to the i-th buyer
that depends on the valuations of all buyers except i's, we prove that a
constant fraction (2-sqrt{e})/4~0.088 of the optimal revenue can be always be
extracted."
"Social utility maximization refers to the process of allocating resources in
such a way that the sum of agents' utilities is maximized under the system
constraints. Such allocation arises in several problems in the general area of
communications, including unicast (and multicast multi-rate) service on the
Internet, as well as in applications with (local) public goods, such as power
allocation in wireless networks, spectrum allocation, etc. Mechanisms that
implement such allocations in Nash equilibrium have also been studied but
either they do not possess full implementation property, or are given in a
case-by-case fashion, thus obscuring fundamental understanding of these
problems.
  In this paper we propose a unified methodology for creating mechanisms that
fully implement, in Nash equilibria, social utility maximizing functions
arising in various contexts where the constraints are convex. The construction
of the mechanism is done in a systematic way by considering the dual
optimization problem. In addition to the required properties of efficiency and
individual rationality that such mechanisms ought to satisfy, three additional
design goals are the focus of this paper: a) the size of the message space
scaling linearly with the number of agents (even if agents' types are entire
valuation functions), b) allocation being feasible on and off equilibrium, and
c) strong budget balance at equilibrium and also off equilibrium whenever
demand is feasible."
"We show that computing the revenue-optimal deterministic auction in
unit-demand single-buyer Bayesian settings, i.e. the optimal item-pricing, is
computationally hard even in single-item settings where the buyer's value
distribution is a sum of independently distributed attributes, or multi-item
settings where the buyer's values for the items are independent. We also show
that it is intractable to optimally price the grand bundle of multiple items
for an additive bidder whose values for the items are independent. These
difficulties stem from implicit definitions of a value distribution. We provide
three instances of how different properties of implicit distributions can lead
to intractability: the first is a #P-hardness proof, while the remaining two
are reductions from the SQRT-SUM problem of Garey, Graham, and Johnson. While
simple pricing schemes can oftentimes approximate the best scheme in revenue,
they can have drastically different underlying structure. We argue therefore
that either the specification of the input distribution must be highly
restricted in format, or it is necessary for the goal to be mere approximation
to the optimal scheme's revenue instead of computing properties of the scheme
itself."
"Performing some task among a set of agents requires the use of some protocol
that regulates the interactions between them. If those agents are rational,
they may try to subvert the protocol for their own benefit, in an attempt to
reach an outcome that provides greater utility. We revisit the traditional
notion of self-enforcing protocols implemented using existing game-theoretic
solution concepts, we describe its shortcomings in real-world applications, and
we propose a new notion of self-enforcing protocols, namely co-utile protocols.
The latter represent a solution concept that can be implemented without a
coordination mechanism in situations when traditional self-enforcing protocols
need a coordination mechanism. Co-utile protocols are preferable in
decentralized systems of rational agents because of their efficiency and
fairness. We illustrate the application of co-utile protocols to information
technology, specifically to preserving the privacy of query profiles of
database/search engine users."
"We consider the general problem of resource sharing in societal networks,
consisting of interconnected communication, transportation, energy and other
networks important to the functioning of society. Participants in such network
need to take decisions daily, both on the quantity of resources to use as well
as the periods of usage. With this in mind, we discuss the problem of
incentivizing users to behave in such a way that society as a whole benefits.
In order to perceive societal level impact, such incentives may take the form
of rewarding users with lottery tickets based on good behavior, and
periodically conducting a lottery to translate these tickets into real rewards.
We will pose the user decision problem as a mean field game (MFG), and the
incentives question as one of trying to select a good mean field equilibrium
(MFE). In such a framework, each agent (a participant in the societal network)
takes a decision based on an assumed distribution of actions of his/her
competitors, and the incentives provided by the social planner. The system is
said to be at MFE if the agent's action is a sample drawn from the assumed
distribution. We will show the existence of such an MFE under different
settings, and also illustrate how to choose an attractive equilibrium using as
an example demand-response in energy networks."
"In the game of Matching Pennies, Alice and Bob each hold a penny, and at
every tick of the clock they simultaneously display the head or the tail sides
of their coins. If they both display the same side, then Alice wins Bob's
penny; if they display different sides, then Bob wins Alice's penny. To avoid
giving the opponent a chance to win, both players seem to have nothing else to
do but to randomly play heads and tails with equal frequencies. However, while
not losing in this game is easy, not missing an opportunity to win is not.
Randomizing your own moves can be made easy. Recognizing when the opponent's
moves are not random can be arbitrarily hard.
  The notion of randomness is central in game theory, but it is usually taken
for granted. The notion of outsmarting is not central in game theory, but it is
central in the practice of gaming. We pursue the idea that these two notions
can be usefully viewed as two sides of the same coin."
"We consider the problem of designing network cost-sharing protocols with good
equilibria under uncertainty. The underlying game is a multicast game in a
rooted undirected graph with nonnegative edge costs. A set of k terminal
vertices or players need to establish connectivity with the root. The social
optimum is the Minimum Steiner Tree. We are interested in situations where the
designer has incomplete information about the input. We propose two different
models, the adversarial and the stochastic. In both models, the designer has
prior knowledge of the underlying metric but the requested subset of the
players is not known and is activated either in an adversarial manner
(adversarial model) or is drawn from a known probability distribution
(stochastic model).
  In the adversarial model, the designer's goal is to choose a single,
universal protocol that has low Price of Anarchy (PoA) for all possible
requested subsets of players. The main question we address is: to what extent
can prior knowledge of the underlying metric help in the design? We first
demonstrate that there exist graphs (outerplanar) where knowledge of the
underlying metric can dramatically improve the performance of good network
design. Then, in our main technical result, we show that there exist graph
metrics, for which knowing the underlying metric does not help and any
universal protocol has PoA of $\Omega(\log k)$, which is tight. We attack this
problem by developing new techniques that employ powerful tools from extremal
combinatorics, and more specifically Ramsey Theory in high dimensional
hypercubes.
  Then we switch to the stochastic model, where each player is independently
activated. We show that there exists a randomized ordered protocol that
achieves constant PoA. By using standard derandomization techniques, we produce
a deterministic ordered protocol with constant PoA."
"We address the question of whether price of stability results (existence of
equilibria with low social cost) are robust to incomplete information. We show
that this is the case in potential games, if the underlying algorithmic social
cost minimization problem admits a constant factor approximation algorithm via
strict cost-sharing schemes. Roughly, if the existence of an
$\alpha$-approximate equilibrium in the complete information setting was proven
via the potential method, then there also exists a $\alpha\cdot
\beta$-approximate Bayes-Nash equilibrium in the incomplete information
setting, where $\beta$ is the approximation factor of the strict-cost sharing
scheme algorithm. We apply our approach to Bayesian versions of the archetypal,
in the price of stability analysis, network design models and show the
existence of $O(\log(n))$-approximate Bayes-Nash equilibria in several games
whose complete information counterparts have been well-studied, such as
undirected network design games, multi-cast games and covering games."
"Cooperative games provide an appropriate framework for fair and stable profit
distribution in multiagent systems. In this paper, we study the algorithmic
issues on path cooperative games that arise from the situations where some
commodity flows through a network. In these games, a coalition of edges or
vertices is successful if it enables a path from the source to the sink in the
network, and lose otherwise. Based on dual theory of linear programming and the
relationship with flow games, we provide the characterizations on the CS-core,
least-core and nucleolus of path cooperative games. Furthermore, we show that
the least-core and nucleolus are polynomially solvable for path cooperative
games defined on both directed and undirected network."
"Game-theoretic models relevant for computer science applications usually
feature a large number of players. The goal of this paper is to develop an
analytical framework for bounding the price of anarchy in such models. We
demonstrate the wide applicability of our framework through instantiations for
several well-studied models, including simultaneous single-item auctions,
greedy combinatorial auctions, and routing games. In all cases, we identify
conditions under which the POA of large games is much better than that of
worst-case instances. Our results also give new senses in which simple auctions
can perform almost as well as optimal ones in realistic settings."
"We study mechanisms that use greedy allocation rules and pay-your-bid pricing
to allocate resources subject to a matroid constraint. We show that all such
mechanisms obtain a constant fraction of the optimal welfare at any equilibrium
of bidder behavior, via a smoothness argument. This unifies numerous recent
results on the price of anarchy of simple auctions. Our results extend to
polymatroid and matching constraints, and we discuss extensions to more general
matroid intersections."
"We study the causal effects of financial incentives on the quality of
crowdwork. We focus on performance-based payments (PBPs), bonus payments
awarded to workers for producing high quality work. We design and run
randomized behavioral experiments on the popular crowdsourcing platform Amazon
Mechanical Turk with the goal of understanding when, where, and why PBPs help,
identifying properties of the payment, payment structure, and the task itself
that make them most effective. We provide examples of tasks for which PBPs do
improve quality. For such tasks, the effectiveness of PBPs is not too sensitive
to the threshold for quality required to receive the bonus, while the magnitude
of the bonus must be large enough to make the reward salient. We also present
examples of tasks for which PBPs do not improve quality. Our results suggest
that for PBPs to improve quality, the task must be effort-responsive: the task
must allow workers to produce higher quality work by exerting more effort. We
also give a simple method to determine if a task is effort-responsive a priori.
Furthermore, our experiments suggest that all payments on Mechanical Turk are,
to some degree, implicitly performance-based in that workers believe their work
may be rejected if their performance is sufficiently poor. Finally, we propose
a new model of worker behavior that extends the standard principal-agent model
from economics to include a worker's subjective beliefs about his likelihood of
being paid, and show that the predictions of this model are in line with our
experimental findings. This model may be useful as a foundation for theoretical
studies of incentives in crowdsourcing markets."
"The achievement of common goals through voluntary efforts of members of a
group can be challenged by the high temptation of individual defection. Here,
two-person one-goal assurance games are generalized to N-person, M-goal
achievement games in which group members can have different motivations with
respect to the achievement of the different goals. The theoretical performance
of groups faced with the challenge of multiple simultaneous goals is analyzed
mathematically and computationally. For two-goal scenarios one finds that
""polarized"" as well as ""biased"" groups perform well in the presence of
defectors. A special case, called individual purpose games (N-person, N-goal
achievements games where there is a one-to-one mapping between actors and goals
for which they have a high achievement motivation) is analyzed in more detail
in form of the ""importance of being different theorem"". It is shown that in
some individual purpose games, groups of size N can successfully accomplish N
goals, such that each group member is highly motivated towards the achievement
of one unique goal. The game-theoretic results suggest that multiple goals as
well as differences in motivations can, in some cases, correspond to highly
effective groups."
"Persuasion, defined as the act of exploiting an informational advantage in
order to effect the decisions of others, is ubiquitous. Indeed, persuasive
communication has been estimated to account for almost a third of all economic
activity in the US. This paper examines persuasion through a computational
lens, focusing on what is perhaps the most basic and fundamental model in this
space: the celebrated Bayesian persuasion model of Kamenica and Gentzkow. Here
there are two players, a sender and a receiver. The receiver must take one of a
number of actions with a-priori unknown payoff, and the sender has access to
additional information regarding the payoffs. The sender can commit to
revealing a noisy signal regarding the realization of the payoffs of various
actions, and would like to do so as to maximize her own payoff assuming a
perfectly rational receiver.
  We examine the sender's optimization task in three of the most natural input
models for this problem, and essentially pin down its computational complexity
in each. When the payoff distributions of the different actions are i.i.d. and
given explicitly, we exhibit a polynomial-time (exact) algorithm, and a
""simple"" $(1-1/e)$-approximation algorithm. Our optimal scheme for the i.i.d.
setting involves an analogy to auction theory, and makes use of Border's
characterization of the space of reduced-forms for single-item auctions. When
action payoffs are independent but non-identical with marginal distributions
given explicitly, we show that it is #P-hard to compute the optimal expected
sender utility. Finally, we consider a general (possibly correlated) joint
distribution of action payoffs presented by a black box sampling oracle, and
exhibit a fully polynomial-time approximation scheme (FPTAS) with a bi-criteria
guarantee. We show that this result is the best possible in the black-box model
for information-theoretic reasons."
"We analyze the computational complexity of the problem of deciding whether,
for a given simple game, there exists the possibility of rearranging the
participants in a set of $j$ given losing coalitions into a set of $j$ winning
coalitions. We also look at the problem of turning winning coalitions into
losing coalitions. We analyze the problem when the simple game is represented
by a list of wining, losing, minimal winning or maximal loosing coalitions."
"In settings where full incentive-compatibility is not available, such as
core-constraint combinatorial auctions and budget-balanced combinatorial
exchanges, we may wish to design mechanisms that are as incentive-compatible as
possible. This paper offers a new characterization of approximate
incentive-compatibility by casting the pricing problem as a meta-game between
the center and the participating agents. Through a suitable set of
simplifications, we describe the equilibrium of this game as a variational
problem. We use this to characterize the space of optimal prices, enabling
closed-form solutions in restricted cases, and numerically-determined prices in
the general case. We offer theory motivating this approach, and numerical
experiments showing its application."
"The Adjusted Winner procedure is an important fair division mechanism
proposed by Brams and Taylor for allocating goods between two parties. It has
been used in practice for divorce settlements and analyzing political disputes.
Assuming truthful declaration of the valuations, it computes an allocation that
is envy-free, equitable and Pareto optimal.
  We show that Adjusted Winner admits several elegant characterizations, which
further shed light on the outcomes reached with strategic agents. We find that
the procedure may not admit pure Nash equilibria in either the discrete or
continuous variants, but is guaranteed to have $\epsilon$-Nash equilibria for
each $\epsilon$ > 0. Moreover, under informed tie-breaking, exact pure Nash
equilibria always exist, are Pareto optimal, and their social welfare is at
least 3/4 of the optimal."
"Energy storage can absorb variability from the rising number of wind and
solar power producers. Storage is different from the conventional generators
that have traditionally balanced supply and demand on fast time scales due to
its hard energy capacity constraints, dynamic coupling, and low marginal costs.
These differences are leading system operators to propose new mechanisms for
enabling storage to participate in reserve and real-time energy markets. The
persistence of market power and gaming in electricity markets suggests that
these changes will expose new vulnerabilities.
  We develop a new model of strategic behavior among storages in energy
balancing markets. Our model is a two-stage game that generalizes a classic
model of capacity followed by Bertrand-Edgeworth price competition by
explicitly modeling storage dynamics and uncertainty in the pricing stage. By
applying the model to balancing markets with storage, we are able to compare
capacity and energy-based pricing schemes, and to analyze the dynamic effects
of the market horizon and energy losses due to leakage. Our first key finding
is that capacity pricing leads to higher prices and higher capacity
commitments, and that energy pricing leads to lower, randomized prices and
lower capacity commitments. Second, we find that a longer market horizon and
higher physical efficiencies lead to lower prices by inducing the storage to
compete to have their states of charge cycled more frequently."
"We consider the problem of allocating indivisible goods in a way that is
fair, using one of the leading market mechanisms in economics: the competitive
equilibrium from equal incomes. Focusing on two major classes of valuations,
namely perfect substitutes and perfect complements, we establish the
computational properties of algorithms operating in this framework. For the
class of valuations with perfect complements, our algorithm yields a
surprisingly succinct characterization of instances that admit a competitive
equilibrium from equal incomes."
"In a system of interdependent users, the security of an entity is affected
not only by that user's investment in security measures, but also by the
positive externality of the security decisions of (some of) the other users.
The provision of security in such system is therefore modeled as a public good
provision problem, and is referred to as a security game. In this paper, we
compare two well-known incentive mechanisms in this context for incentivizing
optimal security investments among users, namely the Pivotal and the
Externality mechanisms. The taxes in a Pivotal mechanism are designed to ensure
users' voluntary participation, while those in an Externality mechanism are
devised to maintain a balanced budget. We first show the more general result
that, due to the non-excludable nature of security, no mechanism can
incentivize the socially optimal investment profile, while at the same time
ensuring voluntary participation and maintaining a balanced budget for all
instances of security games. To further illustrate, we apply the Pivotal and
Externality mechanisms to the special case of weighted total effort
interdependence models, and identify some of the effects of varying
interdependency between users on the budget deficit in the Pivotal mechanism,
as well as on the participation incentives in the Externality mechanism."
"In this paper, we derive bounds for profit maximizing prior-free procurement
auctions where a buyer wishes to procure multiple units of a homogeneous item
from n sellers who are strategic about their per unit valuation. The buyer
earns the profit by reselling these units in an external consumer market. The
paper looks at three scenarios of increasing complexity. First, we look at unit
capacity sellers where per unit valuation is private information of each seller
and the revenue curve is concave. For this setting, we define two benchmarks.
We show that no randomized prior free auction can be constant competitive
against any of these two benchmarks. However, for a lightly constrained
benchmark we design a prior-free auction PEPA (Profit Extracting Procurement
Auction) which is 4-competitive and we show this bound is tight. Second, we
study a setting where the sellers have non-unit capacities that are common
knowledge and derive similar results. In particular, we propose a prior free
auction PEPAC (Profit Extracting Procurement Auction with Capacity) which is
truthful for any concave revenue curve. Third, we obtain results in the
inherently harder bi-dimensional case where per unit valuation as well as
capacities are private information of the sellers. We show that PEPAC is
truthful and constant competitive for the specific case of linear revenue
curves. We believe that this paper represents the first set of results on
single dimensional and bi-dimensional profit maximizing prior-free multi-unit
procurement auctions."
"We study $n$-player turn-based games played on a finite directed graph. For
each play, the players have to pay a cost that they want to minimize. Instead
of the well-known notion of Nash equilibrium (NE), we focus on the notion of
subgame perfect equilibrium (SPE), a refinement of NE well-suited in the
framework of games played on graphs. We also study natural variants of SPE,
named weak (resp. very weak) SPE, where players who deviate cannot use the full
class of strategies but only a subclass with a finite number of (resp. a
unique) deviation step(s).
  Our results are threefold. Firstly, we characterize in the form of a Folk
theorem the set of all plays that are the outcome of a weak SPE. Secondly, for
the class of quantitative reachability games, we prove the existence of a
finite-memory SPE and provide an algorithm for computing it (only existence was
known with no information regarding the memory). Moreover, we show that the
existence of a constrained SPE, i.e. an SPE such that each player pays a cost
less than a given constant, can be decided. The proofs rely on our Folk theorem
for weak SPEs (which coincide with SPEs in the case of quantitative
reachability games) and on the decidability of MSO logic on infinite words.
Finally with similar techniques, we provide a second general class of games for
which the existence of a (constrained) weak SPE is decidable."
"We continue the study of welfare maximization in unit-demand (matching)
markets, in a distributed information model where agent's valuations are
unknown to the central planner, and therefore communication is required to
determine an efficient allocation. Dobzinski, Nisan and Oren (STOC'14) showed
that if the market size is $n$, then $r$ rounds of interaction (with
logarithmic bandwidth) suffice to obtain an $n^{1/(r+1)}$-approximation to the
optimal social welfare. In particular, this implies that such markets converge
to a stable state (constant approximation) in time logarithmic in the market
size.
  We obtain the first multi-round lower bound for this setup. We show that even
if the allowable per-round bandwidth of each agent is $n^{\epsilon(r)}$, the
approximation ratio of any $r$-round (randomized) protocol is no better than
$\Omega(n^{1/5^{r+1}})$, implying an $\Omega(\log \log n)$ lower bound on the
rate of convergence of the market to equilibrium.
  Our construction and technique may be of interest to round-communication
tradeoffs in the more general setting of combinatorial auctions, for which the
only known lower bound is for simultaneous ($r=1$) protocols [DNO14]."
"Exchange of services and resources in, or over, networks is attracting
nowadays renewed interest. However, despite the broad applicability and the
extensive study of such models, e.g., in the context of P2P networks, many
fundamental questions regarding their properties and efficiency remain
unanswered. We consider such a service exchange model and analyze the users'
interactions under three different approaches. First, we study a centrally
designed service allocation policy that yields the fair total service each user
should receive based on the service it others to the others. Accordingly, we
consider a competitive market where each user determines selfishly its
allocation policy so as to maximize the service it receives in return, and a
coalitional game model where users are allowed to coordinate their policies. We
prove that there is a unique equilibrium exchange allocation for both game
theoretic formulations, which also coincides with the central fair service
allocation. Furthermore, we characterize its properties in terms of the
coalitions that emerge and the equilibrium allocations, and analyze its
dependency on the underlying network graph. That servicing policy is the
natural reference point to the various mechanisms that are currently proposed
to incentivize user participation and improve the efficiency of such networked
service (or, resource) exchange markets."
"The classic Gibbard-Satterthwaite theorem says that every strategy-proof
voting rule with at least three possible candidates must be dictatorial. In
\cite{McL11}, McLennan showed that a similar impossibility result holds even if
we consider a weaker notion of strategy-proofness where voters believe that the
other voters' preferences are i.i.d.~(independent and identically distributed):
If an anonymous voting rule (with at least three candidates) is strategy-proof
w.r.t.~all i.i.d.~beliefs and is also Pareto efficient, then the voting rule
must be a random dictatorship. In this paper, we strengthen McLennan's result
by relaxing Pareto efficiency to $\epsilon$-Pareto efficiency where Pareto
efficiency can be violated with probability $\epsilon$, and we further relax
$\epsilon$-Pareto efficiency to a very weak notion of efficiency which we call
$\epsilon$-super-weak unanimity. We then show the following: If an anonymous
voting rule (with at least three candidates) is strategy-proof w.r.t.~all
i.i.d.~beliefs and also satisfies $\epsilon$-super-weak unanimity, then the
voting rule must be $O(\epsilon)$-close to random dictatorship."
"In an all-pay auction, only one bidder wins but all bidders must pay the
auctioneer. All-pay bidding games arise from attaching a similar bidding
structure to traditional combinatorial games to determine which player moves
next. In contrast to the established theory of single-pay bidding games,
optimal play involves choosing bids from some probability distribution that
will guarantee a minimum probability of winning. In this manner, all-pay
bidding games wed the underlying concepts of economic and combinatorial games.
We present several results on the structures of optimal strategies in these
games. We then give a fast algorithm for computing such strategies for a large
class of all-pay bidding games. The methods presented provide a framework for
further development of the theory of all-pay bidding games."
"We consider two-sided matching markets, and study the incentives of agents to
circumvent a centralized clearing house by signing binding contracts with one
another. It is well-known that if the clearing house implements a stable match
and preferences are known, then no group of agents can profitably deviate in
this manner.
  We ask whether this property holds even when agents have incomplete
information about their own preferences or the preferences of others. We find
that it does not. In particular, when agents are uncertain about the
preferences of others, every mechanism is susceptible to deviations by groups
of agents. When, in addition, agents are uncertain about their own preferences,
every mechanism is susceptible to deviations in which a single pair of agents
agrees in advance to match to each other."
"We consider a strategic game, where players submit jobs to a machine that
executes all jobs in a way that minimizes energy while respecting the given
deadlines. The energy consumption is then charged to the players in some way.
Each player wants to minimize the sum of that charge and of their job's
deadline multiplied by a priority weight. Two charging schemes are studied, the
proportional cost share which does not always admit pure Nash equilibria, and
the marginal cost share, which does always admit pure Nash equilibria, at the
price of overcharging by a constant factor."
"We study online auction settings in which agents arrive and depart
dynamically in a random (secretary) order, and each agent's private type
consists of the agent's arrival and departure times, value and budget. We
consider multi-unit auctions with additive agents for the allocation of both
divisible and indivisible items. For both settings, we devise truthful
mechanisms that give a constant approximation with respect to the auctioneer's
revenue, under a large market assumption. For divisible items, we devise in
addition a truthful mechanism that gives a constant approximation with respect
to the liquid welfare --- a natural efficiency measure for budgeted settings
introduced by Dobzinski and Paes Leme [ICALP'14]. Our techniques provide
high-level principles for transforming offline truthful mechanisms into online
ones, with or without budget constraints. To the best of our knowledge, this is
the first work that addresses the non-trivial challenge of combining online
settings with budgeted agents."
"We discuss and solve a model for a game with many players, where a subset of
truely deciding players is embedded into a hierarchy of dependent agents.
  These interdependencies modify the game matrix and the Nash equilibria for
the deciding players. In a concrete example, we recognize the partition
function of the Ising model and for high dependency we observe a phase
transition to a new Nash equilibrium, which is the Pareto-efficient outcome.
  An example we have in mind is the game theory for major shareholders in a
stock market, where intermediate companies decide according to a majority vote
of their owners and compete for the final profit. In our model, these
interdependency eventually forces cooperation."
"Mechanism design for a social utility being the sum of agents' utilities
(SoU) is a well-studied problem. There are, however, a number of problems of
theoretical and practical interest where a designer may have a different
objective than maximization of the SoU. One motivation for this is the desire
for more equitable allocation of resources among agents. A second, more subtle,
motivation is the fact that a fairer allocation indirectly implies less
variation in taxes which can be desirable in a situation where (implicit)
individual agent budgetary constraints make payment of large taxes unrealistic.
In this paper we study a family of social utilities that provide fair
allocation (with SoU being subsumed as an extreme case) and derive conditions
under which Bayesian and Dominant strategy implementation is possible.
Furthermore, it is shown how a simple modification of the above mechanism can
guarantee full Bayesian implementation. Through a numerical example it is shown
that the proposed method can result in significant gains both in allocation
fairness and tax reduction."
"One of the most appealing aspects of the (coarse) correlated equilibrium
concept is that natural dynamics quickly arrive at approximations of such
equilibria, even in games with many players. In addition, there exist
polynomial-time algorithms that compute exact (coarse) correlated equilibria.
In light of these results, a natural question is how good are the (coarse)
correlated equilibria that can arise from any efficient algorithm or dynamics.
  In this paper we address this question, and establish strong negative
results. In particular, we show that in multiplayer games that have a succinct
representation, it is NP-hard to compute any coarse correlated equilibrium (or
approximate coarse correlated equilibrium) with welfare strictly better than
the worst possible. The focus on succinct games ensures that the underlying
complexity question is interesting; many multiplayer games of interest are in
fact succinct. Our results imply that, while one can efficiently compute a
coarse correlated equilibrium, one cannot provide any nontrivial welfare
guarantee for the resulting equilibrium, unless P=NP. We show that analogous
hardness results hold for correlated equilibria, and persist under the
egalitarian objective or Pareto optimality.
  To complement the hardness results, we develop an algorithmic framework that
identifies settings in which we can efficiently compute an approximate
correlated equilibrium with near-optimal welfare. We use this framework to
develop an efficient algorithm for computing an approximate correlated
equilibrium with near-optimal welfare in aggregative games."
"We study a generalisation of sabotage games, a model of dynamic network games
introduced by van Benthem. The original definition of the game is inherently
finite and therefore does not allow one to model infinite processes. We propose
an extension of the sabotage games in which the first player (Runner) traverses
an arena with dynamic weights determined by the second player (Saboteur). In
our model of quantitative sabotage games, Saboteur is now given a budget that
he can distribute amongst the edges of the graph, whilst Runner attempts to
minimise the quantity of budget witnessed while completing his task. We show
that, on the one hand, for most of the classical cost functions considered in
the literature, the problem of determining if Runner has a strategy to ensure a
cost below some threshold is EXPTIME-complete. On the other hand, if the budget
of Saboteur is fixed a priori, then the problem is in PTIME for most cost
functions. Finally, we show that restricting the dynamics of the game also
leads to better complexity."
"In this paper we present optimization problems with biconvex objective
function and linear constraints such that the set of global minima of the
optimization problems is the same as the set of Nash equilibria of a n-player
general-sum normal form game. We further show that the objective function is an
invex function and consider a projected gradient descent algorithm. We prove
that the projected gradient descent scheme converges to a partial optimum of
the objective function. We also present simulation results on certain test
cases showing convergence to a Nash equilibrium strategy."
"Data centers have emerged as promising resources for demand response,
particularly for emergency demand response (EDR), which saves the power grid
from incurring blackouts during emergency situations. However, currently, data
centers typically participate in EDR by turning on backup (diesel) generators,
which is both expensive and environmentally unfriendly. In this paper, we focus
on ""greening"" demand response in multi-tenant data centers, i.e., colocation
data centers, by designing a pricing mechanism through which the data center
operator can efficiently extract load reductions from tenants during emergency
periods to fulfill energy reduction requirement for EDR. In particular, we
propose a pricing mechanism for both mandatory and voluntary EDR programs,
ColoEDR, that is based on parameterized supply function bidding and provides
provably near-optimal efficiency guarantees, both when tenants are price-taking
and when they are price-anticipating. In addition to analytic results, we
extend the literature on supply function mechanism design, and evaluate ColoEDR
using trace-based simulation studies. These validate the efficiency analysis
and conclude that the pricing mechanism is both beneficial to the environment
and to the data center operator (by decreasing the need for backup diesel
generation), while also aiding tenants (by providing payments for load
reductions)."
"In this paper, some new criteria for detecting whether a finite game is
potential are proposed by solving potential equations. The verification
equations with the minimal number for checking a potential game are obtained
for the first time. Some connections between the potential equations and the
existing characterizations of potential games are established. It is revealed
that a finite game is potential if and only if its every bi-matrix sub-game is
potential."
"We consider polymatrix coordination games with individual preferences where
every player corresponds to a node in a graph who plays with each neighbor a
separate bimatrix game with non-negative symmetric payoffs. In this paper, we
study $\alpha$-approximate $k$-equilibria of these games, i.e., outcomes where
no group of at most $k$ players can deviate such that each member increases his
payoff by at least a factor $\alpha$. We prove that for $\alpha \ge 2$ these
games have the finite coalitional improvement property (and thus
$\alpha$-approximate $k$-equilibria exist), while for $\alpha < 2$ this
property does not hold. Further, we derive an almost tight bound of
$2\alpha(n-1)/(k-1)$ on the price of anarchy, where $n$ is the number of
players; in particular, it scales from unbounded for pure Nash equilibria ($k =
1)$ to $2\alpha$ for strong equilibria ($k = n$). We also settle the complexity
of several problems related to the verification and existence of these
equilibria. Finally, we investigate natural means to reduce the inefficiency of
Nash equilibria. Most promisingly, we show that by fixing the strategies of $k$
players the price of anarchy can be reduced to $n/k$ (and this bound is tight)."
"In quasi-proportional auctions, each bidder receives a fraction of the
allocation equal to the weight of their bid divided by the sum of weights of
all bids, where each bid's weight is determined by a weight function. We study
the relationship between the weight function, bidders' private values, number
of bidders, and the seller's revenue in equilibrium. It has been shown that if
one bidder has a much higher private value than the others, then a nearly flat
weight function maximizes revenue. Essentially, threatening the bidder who has
the highest valuation with having to share the allocation maximizes the
revenue. We show that as bidder private values approach parity, steeper weight
functions maximize revenue by making the quasi-proportional auction more like a
winner-take-all auction. We also show that steeper weight functions maximize
revenue as the number of bidders increases. For flatter weight functions, there
is known to be a unique pure-strategy Nash equilibrium. We show that a
pure-strategy Nash equilibrium also exists for steeper weight functions, and we
give lower bounds for bids at an equilibrium. For a special case that includes
the two-bidder auction, we show that the pure-strategy Nash equilibrium is
unique, and we show how to compute the revenue at equilibrium. We also show
that selecting a weight function based on private value ratios and number of
bidders is necessary for a quasi-proportional auction to produce more revenue
than a second-price auction."
"A dataset has been classified by some unknown classifier into two types of
points. What were the most important factors in determining the classification
outcome? In this work, we employ an axiomatic approach in order to uniquely
characterize an influence measure: a function that, given a set of classified
points, outputs a value for each feature corresponding to its influence in
determining the classification outcome. We show that our influence measure
takes on an intuitive form when the unknown classifier is linear. Finally, we
employ our influence measure in order to analyze the effects of user profiling
on Google's online display advertising."
"This paper explores a PAC (probably approximately correct) learning model in
cooperative games. Specifically, we are given $m$ random samples of coalitions
and their values, taken from some unknown cooperative game; can we predict the
values of unseen coalitions? We study the PAC learnability of several
well-known classes of cooperative games, such as network flow games, threshold
task games, and induced subgraph games. We also establish a novel connection
between PAC learnability and core stability: for games that are efficiently
learnable, it is possible to find payoff divisions that are likely to be stable
using a polynomial number of samples."
"Many hard computational social choice problems are known to become tractable
when voters' preferences belong to a restricted domain, such as those of
single-peaked or single-crossing preferences. However, to date, all algorithmic
results of this type have been obtained for the setting where each voter's
preference list is a total order of candidates. The goal of this paper is to
extend this line of research to the setting where voters' preferences are
dichotomous, i.e., each voter approves a subset of candidates and disapproves
the remaining candidates. We propose several analogues of the notions of
single-peaked and single-crossing preferences for dichotomous profiles and
investigate the relationships among them. We then demonstrate that for some of
these notions the respective restricted domains admit efficient algorithms for
computationally hard approval-based multi-winner rules."
"We study the quality of outcomes in repeated games when the population of
players is dynamically changing and participants use learning algorithms to
adapt to the changing environment. Game theory classically considers Nash
equilibria of one-shot games, while in practice many games are players
repeatedly, and in such games players often use algorithmic tools to learn to
play in the given environment. Learning in repeated games has only been studied
when the population playing the game is stable over time.
  We analyze efficiency of repeated games in dynamically changing environments,
motivated by application domains such as packet routing and Internet
ad-auctions. We prove that, in many classes of games, if players choose their
strategies in a way that guarantees low adaptive regret, then high social
welfare is ensured, even under very frequent changes. This result extends
previous work, which showed high welfare for learning outcomes in stable
environments. A main technical tool for our analysis is the existence of a
solution to the welfare maximization problem that is both close to optimal and
relatively stable over time. Such a solution serves as a benchmark in the
efficiency analysis of learning outcomes. We show that such stable and
near-optimal solutions exist for many problems, even in cases when the exact
optimal solution can be very unstable. We develop direct techniques to show the
existence of a stable solution in some classes of games. Further, we show that
a sufficient condition for the existence of stable solutions is the existence
of a differentially private algorithm for the welfare maximization problem. We
demonstrate our techniques by focusing on three classes of games as examples:
simultaneous item auctions, bandwidth allocation mechanisms and congestion
games."
"Analysis of efficiency of outcomes in game theoretic settings has been a main
item of study at the intersection of economics and computer science. The notion
of the price of anarchy takes a worst-case stance to efficiency analysis,
considering instance independent guarantees of efficiency. We propose a
data-dependent analog of the price of anarchy that refines this worst-case
assuming access to samples of strategic behavior. We focus on auction settings,
where the latter is non-trivial due to the private information held by
participants. Our approach to bounding the efficiency from data is robust to
statistical errors and mis-specification. Unlike traditional econometrics,
which seek to learn the private information of players from observed behavior
and then analyze properties of the outcome, we directly quantify the
inefficiency without going through the private information. We apply our
approach to datasets from a sponsored search auction system and find empirical
results that are a significant improvement over bounds from worst-case
analysis."
"Revealed preference techniques are used to test whether a data set is
compatible with rational behaviour. They are also incorporated as constraints
in mechanism design to encourage truthful behaviour in applications such as
combinatorial auctions. In the auction setting, we present an efficient
combinatorial algorithm to find a virtual valuation function with the optimal
(additive) rationality guarantee. Moreover, we show that there exists such a
valuation function that both is individually rational and is minimum (that is,
it is component-wise dominated by any other individually rational, virtual
valuation function that approximately fits the data). Similarly, given upper
bound constraints on the valuation function, we show how to fit the maximum
virtual valuation function with the optimal additive rationality guarantee. In
practice, revealed preference bidding constraints are very demanding. We
explain how approximate rationality can be used to create relaxed revealed
preference constraints in an auction. We then show how combinatorial methods
can be used to implement these relaxed constraints. Worst/best-case welfare
guarantees that result from the use of such mechanisms can be quantified via
the minimum/maximum virtual valuation function."
"The main goal of this paper is to develop a theory of inference of player
valuations from observed data in the generalized second price auction without
relying on the Nash equilibrium assumption. Existing work in Economics on
inferring agent values from data relies on the assumption that all participant
strategies are best responses of the observed play of other players, i.e. they
constitute a Nash equilibrium. In this paper, we show how to perform inference
relying on a weaker assumption instead: assuming that players are using some
form of no-regret learning. Learning outcomes emerged in recent years as an
attractive alternative to Nash equilibrium in analyzing game outcomes, modeling
players who haven't reached a stable equilibrium, but rather use algorithmic
learning, aiming to learn the best way to play from previous observations. In
this paper we show how to infer values of players who use algorithmic learning
strategies. Such inference is an important first step before we move to testing
any learning theoretic behavioral model on auction data. We apply our
techniques to a dataset from Microsoft's sponsored search ad auction system."
"We consider a ridesharing problem where there is uncertainty about the
completion of trips from both drivers and riders. Specifically, we study
ridesharing mechanisms that aim to incentivize commuters to reveal their
valuation for trips and their probability of undertaking their trips. Due to
the interdependence created by the uncertainty on commuters' valuations, we
show that the Groves mechanisms are not ex-post truthful even if there is only
one commuter whose valuation depends on the other commuters' uncertainty of
undertaking their trips. To circumvent this impossibility, we propose an
ex-post truthful mechanism, the best incentive we can design without
sacrificing social welfare in this setting. Our mechanism pays a commuter if
she undertakes her trip, otherwise she is penalized for not undertaking her
trip. Furthermore, we identify a sufficient and necessary condition under which
our mechanism is ex-post truthful."
"There is a class of models for pol/mil/econ bargaining and conflict that is
loosely based on the Median Voter Theorem which has been used with great
success for about 30 years. However, there are fundamental mathematical
limitations to these models. They apply to issues which can be represented on a
single one-dimensional continuum. They represent fundamental group decision
process by a deterministic Condorcet Election: deterministic voting by all
actors, and deterministic outcomes of each vote. This work provides a
methodology for addressing a broader class of problems. The first extension is
to continuous issue sets where the consequences of policies are not
well-described by a distance measure or utility is not monotonic in distance.
The second fundamental extension is to inherently discrete issue sets. Because
the options cannot easily be mapped into a multidimensional space so that the
utility depends on distance, we refer to it as a non-spatial issue set. The
third, but most fundamental, extension is to represent the negotiating process
as a probabilistic Condorcet election (PCE). The actors' generalized voting is
deterministic, but the outcomes of the votes is probabilistic. The PCE provides
the flexibility to make the first two extensions possible; this flexibility
comes at the cost of less precise predictions and more complex validation. The
methodology has been implemented in two proof-of-concept prototypes which
address the subset selection problem of forming a parliament, and strategy
optimization for one-dimensional issues."
"Dependence on the parameter is continuous when perturbations of the parameter
preserves strict preference for one alternative over another. We characterise
this property via a utility function over alternatives that depends
continuously on the parameter. The class of parameter spaces where such a
representation is guaranteed to exist is also identified. When the parameter is
the type or belief of a player, these results have implications for Bayesian
and psychological games. When alternatives are discrete, the representation is
jointly continuous and an extension of Berge's theorem of the maximum yields a
continuous value function. We apply this result to generalise a standard
consumer choice problem where parameters are price-wealth vectors. When the
parameter space is lexicographically ordered, a novel application to
reference-dependent preferences is possible."
"Celebrity games, a new model of network creation games is introduced. The
specific features of this model are that players have different celebrity
weights and that a critical distance is taken into consideration. The aim of
any player is to be close (at distance less than critical) to the others,
mainly to those with high celebrity weights. The cost of each player depends on
the cost of establishing direct links to other players and on the sum of the
weights of those players at a distance greater than the critical distance. We
show that celebrity games always have pure Nash equilibria and we characterize
the family of subgames having connected Nash equilibria, the so called star
celebrity games. We provide exact bounds for the PoA of celebrity games.
  The PoA can be tightened when restricted to particular classes of Nash
equilibria graphs, in particular for trees."
"We consider the following question: in a nonatomic routing game, can the
tolls that induce the minimum latency flow be computed without knowing the
latency functions? Since the latency functions are unknown, we assume we are
given an oracle that has access to the underlying routing game. A query to the
oracle consists of tolls on edges, and the response is the resulting
equilibrium flow. We show that in this model, it is impossible to obtain
optimal tolls. However, if we augment the oracle so that it returns the total
latency of the equilibrium flow induced by the tolls in addition to the flow
itself, then the required tolls can be computed with a polynomial number of
queries."
"Betting strategies are often expressed formally as martingales. A martingale
is called integer-valued if each bet must be an integer value. Integer-valued
strategies correspond to the fact that in most betting situations, there is a
minimum amount that a player can bet. According to a well known paradigm,
algorithmic randomness can be founded on the notion of betting strategies. A
real X is called integer-valued random if no effective integer-valued
martingale succeeds on X. It turns out that this notion of randomness has
interesting interactions with genericity and the computably enumerable degrees.
We investigate the computational power of the integer-valued random reals in
terms of standard notions from computability theory."
"Evolutionary game theory classically investigates which behavioral patterns
are evolutionarily successful in a single game. More recently, a number of
contributions have studied the evolution of preferences instead: which
subjective conceptualizations of a game's payoffs give rise to evolutionarily
successful behavior in a single game. Here, we want to extend this existing
approach even further by asking: which general patterns of subjective
conceptualizations of payoff functions are evolutionarily successful across a
class of games. In other words, we will look at evolutionary competition of
payoff transformations in ""meta-games"", obtained from averaging over payoffs of
single games. Focusing for a start on the class of 2x2 symmetric games, we show
that regret minimization can outperform payoff maximization if agents resort to
a security strategy in case of radical uncertainty."
"Escalation is the fact that in a game (for instance in an auction), the
agents play forever. The $0,1$-game is an extremely simple infinite game with
intelligent agents in which escalation arises. It shows at the light of
research on cognitive psychology the difference between intelligence
(algorithmic mind) and rationality (algorithmic and reflective mind) in
decision processes. It also shows that depending on the point of view (inside
or outside) the rationality of the agent may change which is proposed to be
called the principle of relativity."
"One of the major drawbacks of the celebrated VCG auction is its low (or zero)
revenue even when the agents have high value for the goods and a {\em
competitive} outcome could have generated a significant revenue. A competitive
outcome is one for which it is impossible for the seller and a subset of buyers
to `block' the auction by defecting and negotiating an outcome with higher
payoffs for themselves. This corresponds to the well-known concept of {\em
core} in cooperative game theory.
  In particular, VCG revenue is known to be not competitive when the goods
being sold have complementarities. A bottleneck here is an impossibility result
showing that there is no auction that simultaneously achieves competitive
prices (a core outcome) and incentive-compatibility.
  In this paper we try to overcome the above impossibility result by asking the
following natural question: is it possible to design an incentive-compatible
auction whose revenue is comparable (even if less) to a competitive outcome?
Towards this, we define a notion of {\em core-competitive} auctions. We say
that an incentive-compatible auction is $\alpha$-core-competitive if its
revenue is at least $1/\alpha$ fraction of the minimum revenue of a
core-outcome. We study the Text-and-Image setting. In this setting, there is an
ad slot which can be filled with either a single image ad or $k$ text ads. We
design an $O(\ln \ln k)$ core-competitive randomized auction and an
$O(\sqrt{\ln(k)})$ competitive deterministic auction for the Text-and-Image
setting. We also show that both factors are tight."
"Here we present a ground-breaking new postulate for game theory. The first
part of this postulate contains the axiomatic observation that all games are
created by a designer, whether they are: e.g., (dynamic/static) or
(stationary/non-stationary) or (sequential/one-shot) non-cooperative games, and
importantly, whether or not they are intended to represent a non-cooperative
Stackelberg game, they can be mapped to a Stackelberg game. I.e., the game
designer is the leader who is totally rational and honest, and the followers
are mapped to the players of the designed game. If now the game designer, or
""the leader"" in the Stackelberg context, adopts a pure strategy, we postulate
the following second part following from axiomatic observation of ultimate game
leadership, where empirical insight leads to the second part of this postulate.
Importantly, implementing a non-cooperative Stackelberg game, with a very
honest and rational leader results in social optimality for all players
(followers), assuming pure strategy across all followers and leader, and that
the leader is totally rational, honest, and is able to achieve a minimum amount
of competency in leading this game, with any finite number of iterations of
leading this finite game."
"We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable."
"In applied game theory the motivation of players is a key element. It is
encoded in the payoffs of the game form and often based on utility functions.
But there are cases were formal descriptions in the form of a utility function
do not exist. In this paper we introduce a representation of games where
players' goals are modeled based on so-called higher-order functions. Our
representation provides a general and powerful way to mathematically summarize
players' intentions. In our framework utility functions as well as preference
relations are special cases to describe players' goals. We show that in
higher-order functions formal descriptions of players may still exist where
utility functions do not using a classical example, a variant of Keynes' beauty
contest. We also show that equilibrium conditions based on Nash can be easily
adapted to our framework. Lastly, this framework serves as a stepping stone to
powerful tools from computer science that can be usefully applied to economic
game theory in the future such as computational and computability aspects."
"Classical decision theory models behaviour in terms of utility maximisation
where utilities represent rational preference relations over outcomes. However,
empirical evidence and theoretical considerations suggest that we need to go
beyond this framework. We propose to represent goals by higher-order functions
or operators that take other functions as arguments where the max and argmax
operators are special cases. Our higher-order functions take a context function
as their argument where a context represents a process from actions to
outcomes. By that we can define goals being dependent on the actions and the
process in addition to outcomes only. This formulation generalises outcome
based preferences to context-dependent goals. We show how to uniformly
represent within our higher-order framework classical utility maximisation but
also various other extensions that have been debated in economics."
"We investigate a non-cooperative game-theoretic model for the formation of
communication networks by selfish agents. Each agent aims for a central
position at minimum cost for creating edges. In particular, the general model
(Fabrikant et al., PODC'03) became popular for studying the structure of the
Internet or social networks. Despite its significance, locality in this game
was first studied only recently (Bil\`o et al., SPAA'14), where a worst case
locality model was presented, which came with a high efficiency loss in terms
of quality of equilibria. Our main contribution is a new and more optimistic
view on locality: agents are limited in their knowledge and actions to their
local view ranges, but can probe different strategies and finally choose the
best. We study the influence of our locality notion on the hardness of
computing best responses, convergence to equilibria, and quality of equilibria.
Moreover, we compare the strength of local versus non-local strategy-changes.
Our results address the gap between the original model and the worst case
locality variant. On the bright side, our efficiency results are in line with
observations from the original model, yet we have a non-constant lower bound on
the price of anarchy."
"The goal of this paper is to propose and study properties of multiwinner
voting rules which can be consider as generalisations of single-winner scoring
voting rules. We consider SNTV, Bloc, k-Borda, STV, and several variants of
Chamberlin--Courant's and Monroe's rules and their approximations. We identify
two broad natural classes of multiwinner score-based rules, and show that many
of the existing rules can be captured by one or both of these approaches. We
then formulate a number of desirable properties of multiwinner rules, and
evaluate the rules we consider with respect to these properties."