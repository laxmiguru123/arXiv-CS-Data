summary
"This paper examines the four main types of Evolutionary Design by computers:
Evolutionary Design Optimisation, Evolutionary Art, Evolutionary Artificial
Life Forms and Creative Evolutionary Design. Definitions for all four areas are
provided. A review of current work in each of these areas is given, with
examples of the types of applications that have been tackled. The different
properties and requirements of each are examined. Descriptions of typical
representations and evolutionary algorithms are provided and examples of
designs evolved using these techniques are shown. The paper then discusses how
the boundaries of these areas are beginning to merge, resulting in four new
'overlapping' types of Evolutionary Design: Integral Evolutionary Design,
Artificial Life Based Evolutionary Design, Aesthetic Evolutionary AL and
Aesthetic Evolutionary Design. Finally, the last part of the paper discusses
some common problems faced by creators of Evolutionary Design systems,
including: interdependent elements in designs, epistasis, and constraint
handling."
"A new training algorithm is presented for delayed reinforcement learning
problems that does not assume the existence of a critic model and employs the
polytope optimization algorithm to adjust the weights of the action network so
that a simple direct measure of the training performance is maximized.
Experimental results from the application of the method to the pole balancing
problem indicate improved training performance compared with critic-based and
genetic reinforcement approaches."
"A mean field feedback artificial neural network algorithm is developed and
explored for the set covering problem. A convenient encoding of the inequality
constraints is achieved by means of a multilinear penalty function. An
approximate energy minimum is obtained by iterating a set of mean field
equations, in combination with annealing. The approach is numerically tested
against a set of publicly available test problems with sizes ranging up to
5x10^3 rows and 10^6 columns. When comparing the performance with exact results
for sizes where these are available, the approach yields results within a few
percent from the optimal solutions. Comparisons with other approximate methods
also come out well, in particular given the very low CPU consumption required
-- typically a few seconds. Arbitrary problems can be processed using the
algorithm via a public domain server."
"The process of evolutionary emergence of purposeful adaptive behavior is
investigated by means of computer simulations. The model proposed implies that
there is an evolving population of simple agents, which have two natural needs:
energy and reproduction. Any need is characterized quantitatively by a
corresponding motivation. Motivations determine goal-directed behavior of
agents. The model demonstrates that purposeful behavior does emerge in the
simulated evolutionary processes. Emergence of purposefulness is accompanied by
origin of a simple hierarchy in the control system of agents."
"In general, we can not use algebraic or enumerative methods to optimize a
quality control (QC) procedure so as to detect the critical random and
systematic analytical errors with stated probabilities, while the probability
for false rejection is minimum. Genetic algorithms (GAs) offer an alternative,
as they do not require knowledge of the objective function to be optimized and
search through large parameter spaces quickly. To explore the application of
GAs in statistical QC, we have developed an interactive GAs based computer
program that designs a novel near optimal QC procedure, given an analytical
process. The program uses the deterministic crowding algorithm. An illustrative
application of the program suggests that it has the potential to design QC
procedures that are significantly better than 45 alternative ones that are used
in the clinical laboratories."
"A new general procedure for a priori selection of more predictable events
from a time series of observed variable is proposed. The procedure is
applicable to time series which contains different types of events that feature
significantly different predictability, or, in other words, to heteroskedastic
time series. A priori selection of future events in accordance to expected
uncertainty of their forecasts may be helpful for making practical decisions.
The procedure first implies creation of two neural network based forecasting
models, one of which is aimed at prediction of conditional mean and other -
conditional dispersion, and then elaboration of the rule for future event
selection into groups of more and less predictable events. The method is
demonstrated and tested by the example of the computer generated time series,
and then applied to the real world time series, Dow Jones Industrial Average
index."
"Ever increasing computational power will require methods for automatic
programming. We present an alternative to genetic programming, based on a
general model of thinking and learning. The advantage is that evolution takes
place in the space of constructs and can thus exploit the mathematical
structures of this space. The model is formalized, and a macro language is
presented which allows for a formal yet intuitive description of the problem
under consideration. A prototype has been developed to implement the scheme in
PERL. This method will lead to a concentration on the analysis of problems, to
a more rapid prototyping, to the treatment of new problem classes, and to the
investigation of philosophical problems. We see fields of application in
nonlinear differential equations, pattern recognition, robotics, model
building, and animated pictures."
"GoTools is a program which solves life & death problems in the game of Go.
This paper describes experiments using a Genetic Algorithm to optimize
heuristic weights used by GoTools' tree-search. The complete set of heuristic
weights is composed of different subgroups, each of which can be optimized with
a suitable fitness function. As a useful side product, an MPI interface for
FreePascal was implemented to allow the use of a parallelized fitness function
running on a Beowulf cluster. The aim of this exercise is to optimize the
current version of GoTools, and to make tools available in preparation of an
extension of GoTools for solving open boundary life & death problems, which
will introduce more heuristic parameters to be fine tuned."
"In the present paper a newer application of Artificial Neural Network (ANN)
has been developed i.e., predicting response-function results of
electrical-mechanical system through ANN. This method is specially useful to
complex systems for which it is not possible to find the response-function
because of complexity of the system. The proposed approach suggests that how
even without knowing the response-function, the response-function results can
be predicted with the use of ANN to the system. The steps used are: (i)
Depending on the system, the ANN-architecture and the input & output parameters
are decided, (ii) Training & test data are generated from simplified circuits
and through tactic-superposition of it for complex circuits, (iii) Training the
ANN with training data through many cycles and (iv) Test-data are used for
predicting the response-function results. It is found that the proposed novel
method for response prediction works satisfactorily. Thus this method could be
used specially for complex systems where other methods are unable to tackle it.
In this paper the application of ANN is particularly demonstrated to
electrical-circuit system but can be applied to other systems too."
"We introduce a novel evolutionary formulation of the problem of finding a
maximum independent set of a graph. The new formulation is based on the
relationship that exists between a graph's independence number and its acyclic
orientations. It views such orientations as individuals and evolves them with
the aid of evolutionary operators that are very heavily based on the structure
of the graph and its acyclic orientations. The resulting heuristic has been
tested on some of the Second DIMACS Implementation Challenge benchmark graphs,
and has been found to be competitive when compared to several of the other
heuristics that have also been tested on those graphs."
"We introduce two novel evolutionary formulations of the problem of coloring
the nodes of a graph. The first formulation is based on the relationship that
exists between a graph's chromatic number and its acyclic orientations. It
views such orientations as individuals and evolves them with the aid of
evolutionary operators that are very heavily based on the structure of the
graph and its acyclic orientations. The second formulation, unlike the first
one, does not tackle one graph at a time, but rather aims at evolving a
`program' to color all graphs belonging to a class whose members all have the
same number of nodes and other common attributes. The heuristics that result
from these formulations have been tested on some of the Second DIMACS
Implementation Challenge benchmark graphs, and have been found to be
competitive when compared to the several other heuristics that have also been
tested on those graphs."
"This paper studies how the generalization ability of neurons can be affected
by mutual processing of different signals. This study is done on the basis of a
feedforward artificial neural network. The mutual processing of signals can
possibly be a good model of patterns in a set generalized by a neural network
and in effect may improve generalization. In this paper it is discussed that
the interference may also cause a highly random generalization. Adaptive
activation functions are discussed as a way of reducing that type of
generalization. A test of a feedforward neural network is performed that shows
the discussed random generalization."
"In this paper, feedforward neural networks are presented that have nonlinear
weight functions based on look--up tables, that are specially smoothed in a
regularization called the diffusion. The idea of such a type of networks is
based on the hypothesis that the greater number of adaptive parameters per a
weight function might reduce the total number of the weight functions needed to
solve a given problem. Then, if the computational complexity of a propagation
through a single such a weight function would be kept low, then the introduced
neural networks might possibly be relatively fast.
  A number of tests is performed, showing that the presented neural networks
may indeed perform better in some cases than the classic neural networks and a
number of other learning machines."
"Websites of a particular class form increasingly complex networks, and new
tools are needed to map and understand them. A way of visualizing this complex
network is by mapping it. A map highlights which members of the community have
similar interests, and reveals the underlying social network. In this paper, we
will map a network of websites using Kohonen's self-organizing map (SOM), a
neural-net like method generally used for clustering and visualization of
complex data sets. The set of websites considered has been the Blogalia weblog
hosting site (based at http://www.blogalia.com/), a thriving community of
around 200 members, created in January 2002. In this paper we show how SOM
discovers interesting community features, its relation with other
community-discovering algorithms, and the way it highlights the set of
communities formed over the network."
"This paper presents a parameter-less optimization framework that uses the
extended compact genetic algorithm (ECGA) and iterated local search (ILS), but
is not restricted to these algorithms. The presented optimization algorithm
(ILS+ECGA) comes as an extension of the parameter-less genetic algorithm (GA),
where the parameters of a selecto-recombinative GA are eliminated. The approach
that we propose is tested on several well known problems. In the absence of
domain knowledge, it is shown that ILS+ECGA is a robust and easy-to-use
optimization method."
"This paper presents an architecture which is suitable for a massive
parallelization of the compact genetic algorithm. The resulting scheme has
three major advantages. First, it has low synchronization costs. Second, it is
fault tolerant, and third, it is scalable.
  The paper argues that the benefits that can be obtained with the proposed
approach is potentially higher than those obtained with traditional parallel
genetic algorithms. In addition, the ideas suggested in the paper may also be
relevant towards parallelizing more complex probabilistic model building
genetic algorithms."
"This paper makes a number of connections between life and various facets of
genetic and evolutionary algorithms research. Specifically, it addresses the
topics of adaptation, multiobjective optimization, decision making, deception,
and search operators, among others. It argues that human life, from birth to
death, is an adaptive or dynamic optimization problem where people are
continuously searching for happiness. More important, the paper speculates that
genetic algorithms can be used as a source of inspiration for helping people
make decisions in their everyday life."
"Recently, researchers have applied genetic algorithms (GAs) to address some
problems in quantum computation. Also, there has been some works in the
designing of genetic algorithms based on quantum theoretical concepts and
techniques. The so called Quantum Evolutionary Programming has two major
sub-areas: Quantum Inspired Genetic Algorithms (QIGAs) and Quantum Genetic
Algorithms (QGAs). The former adopts qubit chromosomes as representations and
employs quantum gates for the search of the best solution. The later tries to
solve a key question in this field: what GAs will look like as an
implementation on quantum hardware? As we shall see, there is not a complete
answer for this question. An important point for QGAs is to build a quantum
algorithm that takes advantage of both the GA and quantum computing parallelism
as well as true randomness provided by quantum computers. In the first part of
this paper we present a survey of the main works in GAs plus quantum computing
including also our works in this area. Henceforth, we review some basic
concepts in quantum computation and GAs and emphasize their inherent
parallelism. Next, we review the application of GAs for learning quantum
operators and circuit design. Then, quantum evolutionary programming is
considered. Finally, we present our current research in this field and some
perspectives."
"This paper presents two different efficiency-enhancement techniques for
probabilistic model building genetic algorithms. The first technique proposes
the use of a mutation operator which performs local search in the sub-solution
neighborhood identified through the probabilistic model. The second technique
proposes building and using an internal probabilistic model of the fitness
along with the probabilistic model of variable interactions. The fitness values
of some offspring are estimated using the probabilistic model, thereby avoiding
computationally expensive function evaluations. The scalability of the
aforementioned techniques are analyzed using facetwise models for convergence
time and population sizing. The speed-up obtained by each of the methods is
predicted and verified with empirical results. The results show that for
additively separable problems the competent mutation operator requires O(k 0.5
logm)--where k is the building-block size, and m is the number of building
blocks--less function evaluations than its selectorecombinative counterpart.
The results also show that the use of an internal probabilistic fitness model
reduces the required number of function evaluations to as low as 1-10% and
yields a speed-up of 2--50."
"This paper analyzes the relative advantages between crossover and mutation on
a class of deterministic and stochastic additively separable problems. This
study assumes that the recombination and mutation operators have the knowledge
of the building blocks (BBs) and effectively exchange or search among competing
BBs. Facetwise models of convergence time and population sizing have been used
to determine the scalability of each algorithm. The analysis shows that for
additively separable deterministic problems, the BB-wise mutation is more
efficient than crossover, while the crossover outperforms the mutation on
additively separable problems perturbed with additive Gaussian noise. The
results show that the speed-up of using BB-wise mutation on deterministic
problems is O(k^{0.5}logm), where k is the BB size, and m is the number of BBs.
Likewise, the speed-up of using crossover on stochastic problems with fixed
noise variance is O(mk^{0.5}log m)."
"This paper presents a competent selectomutative genetic algorithm (GA), that
adapts linkage and solves hard problems quickly, reliably, and accurately. A
probabilistic model building process is used to automatically identify key
building blocks (BBs) of the search problem. The mutation operator uses the
probabilistic model of linkage groups to find the best among competing building
blocks. The competent selectomutative GA successfully solves additively
separable problems of bounded difficulty, requiring only subquadratic number of
function evaluations. The results show that for additively separable problems
the probabilistic model building BB-wise mutation scales as O(2^km^{1.5}), and
requires O(k^{0.5}logm) less function evaluations than its selectorecombinative
counterpart, confirming theoretical results reported elsewhere (Sastry &
Goldberg, 2004)."
"This paper studies fitness inheritance as an efficiency enhancement technique
for a class of competent genetic algorithms called estimation distribution
algorithms. Probabilistic models of important sub-solutions are developed to
estimate the fitness of a proportion of individuals in the population, thereby
avoiding computationally expensive function evaluations. The effect of fitness
inheritance on the convergence time and population sizing are modeled and the
speed-up obtained through inheritance is predicted. The results show that a
fitness-inheritance mechanism which utilizes information on building-block
fitnesses provides significant efficiency enhancement. For additively separable
problems, fitness inheritance reduces the number of function evaluations to
about half and yields a speed-up of about 1.75--2.25."
"In this paper we apply a heuristic method based on artificial neural networks
in order to trace out the efficient frontier associated to the portfolio
selection problem. We consider a generalization of the standard Markowitz
mean-variance model which includes cardinality and bounding constraints. These
constraints ensure the investment in a given number of different assets and
limit the amount of capital to be invested in each asset. We present some
experimental results obtained with the neural network heuristic and we compare
them to those obtained with three previous heuristic methods."
"This article presents the Neo-Fuzzy-Neuron Modified by Kohonen Network
(NFN-MK), an hybrid computational model that combines fuzzy system technique
and artificial neural networks. Its main task consists in the automatic
generation of membership functions, in particular, triangle forms, aiming a
dynamic modeling of a system. The model is tested by simulating real systems,
here represented by a nonlinear mathematical function. Comparison with the
results obtained by traditional neural networks, and correlated studies of
neurofuzzy systems applied in system identification area, shows that the NFN-MK
model has a similar performance, despite its greater simplicity."
"We describe a new neural-network technique developed for an automated
recognition of solar filaments visible in the hydrogen H-alpha line full disk
spectroheliograms. This technique allows neural networks learn from a few image
fragments labelled manually to recognize the single filaments depicted on a
local background. The trained network is able to recognize filaments depicted
on the backgrounds with variations in brightness caused by atmospherics
distortions. Despite the difference in backgrounds in our experiments the
neural network has properly recognized filaments in the testing image
fragments. Using a parabolic activation function we extend this technique to
recognize multiple solar filaments which may appear in one fragment."
"The Hopfield network has been applied to solve optimization problems over
decades. However, it still has many limitations in accomplishing this task.
Most of them are inherited from the optimization algorithms it implements. The
computation of a Hopfield network, defined by a set of difference equations,
can easily be trapped into one local optimum or another, sensitive to initial
conditions, perturbations, and neuron update orders. It doesn't know how long
it will take to converge, as well as if the final solution is a global optimum,
or not. In this paper, we present a Hopfield network with a new set of
difference equations to fix those problems. The difference equations directly
implement a new powerful optimization algorithm."
"The recognition of optical characters is known to be one of the earliest
applications of Artificial Neural Networks, which partially emulate human
thinking in the domain of artificial intelligence. In this paper, a simplified
neural approach to recognition of optical or visual characters is portrayed and
discussed. The document is expected to serve as a resource for learners and
amateur investigators in pattern recognition, neural networking and related
disciplines."
"The Artificial Neural network is a functional imitation of simplified model
of the biological neurons and their goal is to construct useful computers for
real world problems. The ANN applications have increased dramatically in the
last few years fired by both theoretical and practical applications in a wide
variety of applications. A brief theory of ANN is presented and potential areas
are identified and future trends are discussed."
"This paper discusses the notion of generalization of training samples over
long distances in the input space of a feedforward neural network. Such a
generalization might occur in various ways, that differ in how great the
contribution of different training features should be.
  The structure of a neuron in a feedforward neural network is analyzed and it
is concluded, that the actual performance of the discussed generalization in
such neural networks may be problematic -- while such neural networks might be
capable for such a distant generalization, a random and spurious generalization
may occur as well.
  To illustrate the differences in generalizing of the same function by
different learning machines, results given by the support vector machines are
also presented."
"A dissipative particle swarm optimization is developed according to the
self-organization of dissipative structure. The negative entropy is introduced
to construct an opening dissipative system that is far-from-equilibrium so as
to driving the irreversible evolution process with better fitness. The testing
of two multimodal functions indicates it improves the performance effectively"
"A self-organizing particle swarm is presented. It works in dissipative state
by employing the small inertia weight, according to experimental analysis on a
simplified model, which with fast convergence. Then by recognizing and
replacing inactive particles according to the process deviation information of
device parameters, the fluctuation is introduced so as to driving the
irreversible evolution process with better fitness. The testing on benchmark
functions and an application example for device optimization with designed
fitness function indicates it improves the performance effectively."
"The adaptive constraints relaxing rule for swarm algorithms to handle with
the problems with equality constraints is presented. The feasible space of such
problems may be similiar to ridge function class, which is hard for applying
swarm algorithms. To enter the solution space more easily, the relaxed quasi
feasible space is introduced and shrinked adaptively. The experimental results
on benchmark functions are compared with the performance of other algorithms,
which show its efficiency."
"The periodic mode is analyzed together with two conventional boundary
handling modes for particle swarm. By providing an infinite space that
comprises periodic copies of original search space, it avoids possible
disorganizing of particle swarm that is induced by the undesired mutations at
the boundary. The results on benchmark functions show that particle swarm with
periodic mode is capable of improving the search performance significantly, by
compared with that of conventional modes and other algorithms."
"A swarm algorithm framework (SWAF), realized by agent-based modeling, is
presented to solve numerical optimization problems. Each agent is a bare bones
cognitive architecture, which learns knowledge by appropriately deploying a set
of simple rules in fast and frugal heuristics. Two essential categories of
rules, the generate-and-test and the problem-formulation rules, are
implemented, and both of the macro rules by simple combination and subsymbolic
deploying of multiple rules among them are also studied. Experimental results
on benchmark problems are presented, and performance comparison between SWAF
and other existing algorithms indicates that it is efficiently."
"Routing, as a basic phenomena, by itself, has got umpteen scopes to analyse,
discuss and arrive at an optimal solution for the technocrats over years.
Routing is analysed based on many factors; few key constraints that decide the
factors are communication medium, time dependency, information source nature.
Parametric routing has become the requirement of the day, with some kind of
adaptation to the underlying network environment. Satellite constellations,
particularly LEO satellite constellations have become a reality in operational
to have a non-breaking voice/data communication around the world.Routing in
these constellations has to be treated in a non conventional way, taking their
network geometry into consideration. One of the efficient methods of
optimization is putting Neural Networks to use. Few Artificial Neural Network
models are very much suitable for the adaptive control mechanism, by their
nature of network arrangement. One such efficient model is Hopfield Network
model.
  This paper is an attempt to design a framework for the Hopfield Network based
adaptive routing phenomena in satellite constellations."
"We posit a new paradigm for image information processing. For the last 25
years, this task was usually approached in the frame of Treisman's two-stage
paradigm [1]. The latter supposes an unsupervised, bottom-up directed process
of preliminary information pieces gathering at the lower processing stages and
a supervised, top-down directed process of information pieces binding and
grouping at the higher stages. It is acknowledged that these sub-processes
interact and intervene between them in a tricky and a complicated manner.
Notwithstanding the prevalence of this paradigm in biological and computer
vision, we nevertheless propose to replace it with a new one, which we would
like to designate as a two-part paradigm. In it, information contained in an
image is initially extracted in an independent top-down manner by one part of
the system, and then it is examined and interpreted by another, separate system
part. We argue that the new paradigm seems to be more plausible than its
forerunner. We provide evidence from human attention vision studies and
insights of Kolmogorov's complexity theory to support our arguments. We also
provide some reasons in favor of separate image interpretation issues."
"An operator algebra implementation of Markov chain Monte Carlo algorithms for
simulating Markov random fields is proposed. It allows the dynamics of networks
whose nodes have discrete state spaces to be specified by the action of an
update operator that is composed of creation and annihilation operators. This
formulation of discrete network dynamics has properties that are similar to
those of a quantum field theory of bosons, which allows reuse of many
conceptual and theoretical structures from QFT. The equilibrium behaviour of
one of these generalised MRFs and of the adaptive cluster expansion network
(ACEnet) are shown to be equivalent, which provides a way of unifying these two
theories."
"Traditional Support Vector Machines (SVMs) need pre-wired finite time windows
to predict and classify time series. They do not have an internal state
necessary to deal with sequences involving arbitrary long-term dependencies.
Here we introduce a new class of recurrent, truly sequential SVM-like devices
with internal adaptive states, trained by a novel method called EVOlution of
systems with KErnel-based outputs (Evoke), an instance of the recent Evolino
class of methods. Evoke evolves recurrent neural networks to detect and
represent temporal dependencies while using quadratic programming/support
vector regression to produce precise outputs. Evoke is the first SVM-based
mechanism learning to classify a context-sensitive language. It also
outperforms recent state-of-the-art gradient-based recurrent neural networks
(RNNs) on various time series prediction tasks."
"We exhibit a family of neural networks of McCulloch and Pitts of size $2nk+2$
which can be simulated by a neural networks of Caianiello of size $2n+2$ and
memory length $k$. This simulation allows us to find again one of the result of
the following article: [Cycles exponentiels des r\'{e}seaux de Caianiello et
compteurs en arithm\'{e}tique redondante, Technique et Science Informatiques
Vol. 19, pages 985-1008] on the existence of neural networks of Caianiello of
size $2n+2$ and memory length $k$ which describes a cycle of length $k \times
2^{nk}$."
"This paper looks in detail at how an evolutionary algorithm attempts to solve
instances from the multimodal problem generator. The paper shows that in order
to consistently reach the global optimum, an evolutionary algorithm requires a
population size that should grow at least linearly with the number of peaks. It
is also shown a close relationship between the supply and decision making
issues that have been identified previously in the context of population sizing
models for additively decomposable problems.
  The most important result of the paper, however, is that solving an instance
of the multimodal problem generator is like solving a peak-in-a-haystack, and
it is argued that evolutionary algorithms are not the best algorithms for such
a task. Finally, and as opposed to what several researchers have been doing, it
is our strong belief that the multimodal problem generator is not adequate for
assessing the performance of evolutionary algorithms."
"In an evolutionary algorithm, the population has a very important role as its
size has direct implications regarding solution quality, speed, and
reliability. Theoretical studies have been done in the past to investigate the
role of population sizing in evolutionary algorithms. In addition to those
studies, several self-adjusting population sizing mechanisms have been proposed
in the literature. This paper revisits the latter topic and pays special
attention to the genetic algorithm with adaptive population size (APGA), for
which several researchers have claimed to be very effective at autonomously
(re)sizing the population.
  As opposed to those previous claims, this paper suggests a complete opposite
view. Specifically, it shows that APGA is not capable of adapting the
population size at all. This claim is supported on theoretical grounds and
confirmed by computer simulations."
"Hybrid neuro-evolutionary algorithms may be inspired on Darwinian or
Lamarckian evolu- tion. In the case of Darwinian evolution, the Baldwin effect,
that is, the progressive incorporation of learned characteristics to the
genotypes, can be observed and leveraged to improve the search. The purpose of
this paper is to carry out an exper- imental study into how learning can
improve G-Prop genetic search. Two ways of combining learning and genetic
search are explored: one exploits the Baldwin effect, while the other uses a
Lamarckian strategy. Our experiments show that using a Lamarckian op- erator
makes the algorithm find networks with a low error rate, and the smallest size,
while using the Bald- win effect obtains MLPs with the smallest error rate, and
a larger size, taking longer to reach a solution. Both approaches obtain a
lower average error than other BP-based algorithms like RPROP, other evolu-
tionary methods and fuzzy logic based methods"
"The Kak family of neural networks is able to learn patterns quickly, and this
speed of learning can be a decisive advantage over other competing models in
many applications. Amongst the implementations of these networks are those
using reconfigurable networks, FPGAs and optical networks. In some
applications, it is useful to use complex data, and it is with that in mind
that this introduction to the basic Kak network with complex inputs is being
presented. The training algorithm is prescriptive and the network weights are
assigned simply upon examining the inputs. The input is mapped using quaternary
encoding for purpose of efficienty. This network family is part of a larger
hierarchy of learning schemes that include quantum models."
"A method of autonomic face recognition based on the biologically plausible
network of networks (NoN) model of information processing is presented. The NoN
model is based on locally parallel and globally coordinated transformations in
which the neurons or computational units form distributed networks, which
themselves link to form larger networks. This models the structures in the
cerebral cortex described by Mountcastle and the architecture based on that
proposed for information processing by Sutton. In the proposed implementation,
face images are processed by a nested family of locally operating networks
along with a hierarchically superior network that classifies the information
from each of the local networks. The results of the experiments yielded a
maximum of 98.5% recognition accuracy and an average of 97.4% recognition
accuracy on a benchmark database."
"Many real world data are sampled functions. As shown by Functional Data
Analysis (FDA) methods, spectra, time series, images, gesture recognition data,
etc. can be processed more efficiently if their functional nature is taken into
account during the data analysis process. This is done by extending standard
data analysis methods so that they can apply to functional inputs. A general
way to achieve this goal is to compute projections of the functional data onto
a finite dimensional sub-space of the functional space. The coordinates of the
data on a basis of this sub-space provide standard vector representations of
the functions. The obtained vectors can be processed by any standard method. In
our previous work, this general approach has been used to define projection
based Multilayer Perceptrons (MLPs) with functional inputs. We study in this
paper important theoretical properties of the proposed model. We show in
particular that MLPs with functional inputs are universal approximators: they
can approximate to arbitrary accuracy any continuous mapping from a compact
sub-space of a functional space to R. Moreover, we provide a consistency result
that shows that any mapping from a functional space to R can be learned thanks
to examples by a projection based MLP: the generalization mean square error of
the MLP decreases to the smallest possible mean square error on the data when
the number of examples goes to infinity."
"This paper introduces an objective function that seeks to minimise the
average total number of bits required to encode the joint state of all of the
layers of a Markov source. This type of encoder may be applied to the problem
of optimising the bottom-up (recognition model) and top-down (generative model)
connections in a multilayer neural network, and it unifies several previous
results on the optimisation of multilayer neural networks."
"This article investigates Kak neural networks, which can be instantaneously
trained, for complex and quaternion inputs. The performance of the basic
algorithm has been analyzed and shown how it provides a plausible model of
human perception and understanding of images. The motivation for studying
quaternion inputs is their use in representing spatial rotations that find
applications in computer graphics, robotics, global navigation, computer vision
and the spatial orientation of instruments. The problem of efficient mapping of
data in quaternion neural networks is examined. Some problems that need to be
addressed before quaternion neural networks find applications are identified."
"In this paper we present a novel tool to evaluate problem solving systems.
Instead of using a system to solve a problem, we suggest using the problem to
evaluate the system. By finding a numerical representation of a problem's
complexity, one can implement genetic algorithm to search for the most complex
problem the given system can solve. This allows a comparison between different
systems that solve the same set of problems. In this paper we implement this
approach on pattern recognition neural networks to try and find the most
complex pattern a given configuration can solve. The complexity of the pattern
is calculated using linguistic complexity. The results demonstrate the power of
the problem evolution approach in ranking different neural network
configurations according to their pattern recognition abilities. Future
research and implementations of this technique are also discussed."
"Some visual search tasks require to memorize the location of stimuli that
have been previously scanned. Considerations about the eye movements raise the
question of how we are able to maintain a coherent memory, despite the frequent
drastically changes in the perception. In this article, we present a
computational model that is able to anticipate the consequences of the eye
movements on the visual perception in order to update a spatial memory"
"We consider flocks of artificial birds and study the emergence of V-like
formations during flight. We introduce a small set of fully distributed
positioning rules to guide the birds' movements and demonstrate, by means of
simulations, that they tend to lead to stabilization into several of the
well-known V-like formations that have been observed in nature. We also provide
quantitative indicators that we believe are closely related to achieving V-like
formations, and study their behavior over a large set of independent
simulations."
"The development of the algorithm of a neural network building by the
corresponding parts of a DNA code is discussed."
"The random initialization of weights of a multilayer perceptron makes it
possible to model its training process as a Las Vegas algorithm, i.e. a
randomized algorithm which stops when some required training error is obtained,
and whose execution time is a random variable. This modeling is used to perform
a case study on a well-known pattern recognition benchmark: the UCI Thyroid
Disease Database. Empirical evidence is presented of the training time
probability distribution exhibiting a heavy tail behavior, meaning a big
probability mass of long executions. This fact is exploited to reduce the
training time cost by applying two simple restart strategies. The first assumes
full knowledge of the distribution yielding a 40% cut down in expected time
with respect to the training without restarts. The second, assumes null
knowledge, yielding a reduction ranging from 9% to 23%."
"When looking for a solution, deterministic methods have the enormous
advantage that they do find global optima. Unfortunately, they are very
CPU-intensive, and are useless on untractable NP-hard problems that would
require thousands of years for cutting-edge computers to explore. In order to
get a result, one needs to revert to stochastic algorithms, that sample the
search space without exploring it thoroughly. Such algorithms can find very
good results, without any guarantee that the global optimum has been reached;
but there is often no other choice than using them. This chapter is a short
introduction to the main methods used in stochastic optimization."
"The assessment of highly-risky situations at road intersections have been
recently revealed as an important research topic within the context of the
automotive industry. In this paper we shall introduce a novel approach to
compute risk functions by using a combination of a highly non-linear processing
model in conjunction with a powerful information encoding procedure.
Specifically, the elements of information either static or dynamic that appear
in a road intersection scene are encoded by using directed positional acyclic
labeled graphs. The risk assessment problem is then reformulated in terms of an
inductive learning task carried out by a recursive neural network. Recursive
neural networks are connectionist models capable of solving supervised and
non-supervised learning problems represented by directed ordered acyclic
graphs. The potential of this novel approach is demonstrated through well
predefined scenarios. The major difference of our approach compared to others
is expressed by the fact of learning the structure of the risk. Furthermore,
the combination of a rich information encoding procedure with a generalized
model of dynamical recurrent networks permit us, as we shall demonstrate, a
sophisticated processing of information that we believe as being a first step
for building future advanced intersection safety systems"
"The Boolean satisfiability problem (SAT) can be solved efficiently with
variants of the DPLL algorithm. For industrial SAT problems, DPLL with conflict
analysis dependent dynamic decision heuristics has proved to be particularly
efficient, e.g. in Chaff. In this work, algorithms that initialize the variable
activity values in the solver MiniSAT v1.14 by analyzing the CNF are evolved
using genetic programming (GP), with the goal to reduce the total number of
conflicts of the search and the solving time. The effect of using initial
activities other than zero is examined by initializing with random numbers. The
possibility of countering the detrimental effects of reordering the CNF with
improved initialization is investigated. The best result found (with validation
testing on further problems) was used in the solver Actin, which was submitted
to SAT-Race 2006."
"Neural network models of real-world systems, such as industrial processes,
made from sensor data must often rely on incomplete data. System states may not
all be known, sensor data may be biased or noisy, and it is not often known
which sensor data may be useful for predictive modelling. Genetic algorithms
may be used to help to address this problem by determining the near optimal
subset of sensor variables most appropriate to produce good models. This paper
describes the use of genetic search to optimize variable selection to determine
inputs into the neural network model. We discuss genetic algorithm
implementation issues including data representation types and genetic operators
such as crossover and mutation. We present the use of this technique for neural
network modelling of a typical industrial application, a liquid fed ceramic
melter, and detail the results of the genetic search to optimize the neural
network model for this application."
"Although there are some real world applications where the use of variable
length representation (VLR) in Evolutionary Algorithm is natural and suitable,
an academic framework is lacking for such representations. In this work we
propose a family of tunable fitness landscapes based on VLR of genotypes. The
fitness landscapes we propose possess a tunable degree of both neutrality and
epistasis; they are inspired, on the one hand by the Royal Road fitness
landscapes, and the other hand by the NK fitness landscapes. So these
landscapes offer a scale of continuity from Royal Road functions, with
neutrality and no epistasis, to landscapes with a large amount of epistasis and
no redundancy. To gain insight into these fitness landscapes, we first use
standard tools such as adaptive walks and correlation length. Second, we
evaluate the performances of evolutionary algorithms on these landscapes for
various values of the neutral and the epistatic parameters; the results allow
us to correlate the performances with the expected degrees of neutrality and
epistasis."
"Usually the offspring-parent fitness correlation is used to visualize and
analyze some caracteristics of fitness landscapes such as evolvability. In this
paper, we introduce a more general representation of this correlation, the
Fitness Cloud (FC). We use the bottleneck metaphor to emphasise fitness levels
in landscape that cause local search process to slow down. For a local search
heuristic such as hill-climbing or simulated annealing, FC allows to visualize
bottleneck and neutrality of landscapes. To confirm the relevance of the FC
representation we show where the bottlenecks are in the well-know NK fitness
landscape and also how to use neutrality information from the FC to combine
some neutral operator with local search heuristic."
"We proposed a new search heuristic using the scuba diving metaphor. This
approach is based on the concept of evolvability and tends to exploit
neutrality in fitness landscape. Despite the fact that natural evolution does
not directly select for evolvability, the basic idea behind the scuba search
heuristic is to explicitly push the evolvability to increase. The search
process switches between two phases: Conquest-of-the-Waters and
Invasion-of-the-Land. A comparative study of the new algorithm and standard
local search heuristics on the NKq-landscapes has shown advantage and limit of
the scuba search. To enlighten qualitative differences between neutral search
processes, the space is changed into a connected graph to visualize the
pathways that the search is likely to follow."
"We proposed a new search heuristic using the scuba diving metaphor. This
approach is based on the concept of evolvability and tends to exploit
neutrality which exists in many real-world problems. Despite the fact that
natural evolution does not directly select for evolvability, the basic idea
behind the scuba search heuristic is to explicitly push evolvability to
increase. A comparative study of the scuba algorithm and standard local search
heuristics has shown the advantage and the limitation of the scuba search. In
order to tune neutrality, we use the NKq fitness landscapes and a family of
travelling salesman problems (TSP) where cities are randomly placed on a
lattice and where travel distance between cities is computed with the Manhattan
metric. In this last problem the amount of neutrality varies with the city
concentration on the grid ; assuming the concentration below one, this TSP
reasonably remains a NP-hard problem."
"This work exposes which mechanisms and procesess in the Nature of evolution
compute a function not computable by Turing machine. The computer with
intelligence that is not higher than one bacteria population could have, but
with efficency to solve the problems that are non-computable by Turing machine
is represented. This theoretical construction is called Universal Evolutinary
Computer and it is based on the superecursive algorithms of evolvability."
"The field of algorithmic self-assembly is concerned with the design and
analysis of self-assembly systems from a computational perspective, that is,
from the perspective of mathematical problems whose study may give insight into
the natural processes through which elementary objects self-assemble into more
complex ones. One of the main problems of algorithmic self-assembly is the
minimum tile set problem (MTSP), which asks for a collection of types of
elementary objects (called tiles) to be found for the self-assembly of an
object having a pre-established shape. Such a collection is to be as concise as
possible, thus minimizing supply diversity, while satisfying a set of stringent
constraints having to do with the termination and other properties of the
self-assembly process from its tile types. We present a study of what we think
is the first practical approach to MTSP. Our study starts with the introduction
of an evolutionary heuristic to tackle MTSP and includes results from extensive
experimentation with the heuristic on the self-assembly of simple objects in
two and three dimensions. The heuristic we introduce combines classic elements
from the field of evolutionary computation with a problem-specific variant of
Pareto dominance into a multi-objective approach to MTSP."
"Cellular Simultaneous Recurrent Neural Network (SRN) has been shown to be a
function approximator more powerful than the MLP. This means that the
complexity of MLP would be prohibitively large for some problems while SRN
could realize the desired mapping with acceptable computational constraints.
The speed of training of complex recurrent networks is crucial to their
successful application. Present work improves the previous results by training
the network with extended Kalman filter (EKF). We implemented a generic
Cellular SRN and applied it for solving two challenging problems: 2D maze
navigation and a subset of the connectedness problem. The speed of convergence
has been improved by several orders of magnitude in comparison with the earlier
results in the case of maze navigation, and superior generalization has been
demonstrated in the case of connectedness. The implications of this
improvements are discussed."
"This issue discusses the fault-trajectory approach suitability for fault
diagnosis on analog networks. Recent works have shown promising results
concerning a method based on this concept for ATPG for diagnosing faults on
analog networks. Such method relies on evolutionary techniques, where a generic
algorithm (GA) is coded to generate a set of optimum frequencies capable to
disclose faults."
"State estimation is necessary in diagnosing anomalies in Water Demand Systems
(WDS). In this paper we present a neural network performing such a task. State
estimation is performed by using optimization, which tries to reconcile all the
available information. Quantification of the uncertainty of the input data
(telemetry measures and demand predictions) can be achieved by means of robust
estate estimation. Using a mathematical model of the network, fuzzy estimated
states for anomalous states of the network can be obtained. They are used to
train a neural network capable of assessing WDS anomalies associated with
particular sets of measurements."
"In some real world situations, linear models are not sufficient to represent
accurately complex relations between input variables and output variables of a
studied system. Multilayer Perceptrons are one of the most successful
non-linear regression tool but they are unfortunately restricted to inputs and
outputs that belong to a normed vector space. In this chapter, we propose a
general recoding method that allows to use symbolic data both as inputs and
outputs to Multilayer Perceptrons. The recoding is quite simple to implement
and yet provides a flexible framework that allows to deal with almost all
practical cases. The proposed method is illustrated on a real world data set."
"In this paper, a new implementation of the adaptation of Kohonen
self-organising maps (SOM) to dissimilarity matrices is proposed. This
implementation relies on the branch and bound principle to reduce the algorithm
running time. An important property of this new approach is that the obtained
algorithm produces exactly the same results as the standard algorithm."
"Prediction problems from spectra are largely encountered in chemometry. In
addition to accurate predictions, it is often needed to extract information
about which wavelengths in the spectra contribute in an effective way to the
quality of the prediction. This implies to select wavelengths (or wavelength
intervals), a problem associated to variable selection. In this paper, it is
shown how this problem may be tackled in the specific case of smooth (for
example infrared) spectra. The functional character of the spectra (their
smoothness) is taken into account through a functional variable projection
procedure. Contrarily to standard approaches, the projection is performed on a
basis that is driven by the spectra themselves, in order to best fit their
characteristics. The methodology is illustrated by two examples of functional
projection, using Independent Component Analysis and functional variable
clustering, respectively. The performances on two standard infrared spectra
benchmarks are illustrated."
"Self organizing maps (SOMs) are widely-used for unsupervised classification.
For this application, they must be combined with some partitioning scheme that
can identify boundaries between distinct regions in the maps they produce. We
discuss a novel partitioning scheme for SOMs based on the Bayesian Blocks
segmentation algorithm of Scargle [1998]. This algorithm minimizes a cost
function to identify contiguous regions over which the values of the attributes
can be represented as approximately constant. Because this cost function is
well-defined and largely independent of assumptions regarding the number and
structure of clusters in the original sample space, this partitioning scheme
offers significant advantages over many conventional methods. Sample code is
available."
"Evolutionary complexity is here measured by the number of trials/evaluations
needed for evolving a logical gate in a non-linear medium. Behavioural
complexity of the gates evolved is characterised in terms of cellular automata
behaviour. We speculate that hierarchies of behavioural and evolutionary
complexities are isomorphic up to some degree, subject to substrate specificity
of evolution and the spectrum of evolution parameters."
"We apply Agent-Based Modeling and Simulation (ABMS) to investigate a set of
problems in a retail context. Specifically, we are working to understand the
relationship between human resource management practices and retail
productivity. Despite the fact we are working within a relatively novel and
complex domain, it is clear that intelligent agents do offer potential for
developing organizational capabilities in the future. Our multi-disciplinary
research team has worked with a UK department store to collect data and capture
perceptions about operations from actors within departments. Based on this case
study work, we have built a simulator that we present in this paper. We then
use the simulator to gather empirical evidence regarding two specific
management practices: empowerment and employee development."
"Intelligent agents offer a new and exciting way of understanding the world of
work. In this paper we apply agent-based modeling and simulation to investigate
a set of problems in a retail context. Specifically, we are working to
understand the relationship between human resource management practices and
retail productivity. Despite the fact we are working within a relatively novel
and complex domain, it is clear that intelligent agents could offer potential
for fostering sustainable organizational capabilities in the future. Our
research so far has led us to conduct case study work with a top ten UK
retailer, collecting data in four departments in two stores. Based on our case
study data we have built and tested a first version of a department store
simulator. In this paper we will report on the current development of our
simulator which includes new features concerning more realistic data on the
pattern of footfall during the day and the week, a more differentiated view of
customers, and the evolution of customers over time. This allows us to
investigate more complex scenarios and to analyze the impact of various
management practices."
"The observation and modeling of natural Complex Systems (CSs) like the human
nervous system, the evolution or the weather, allows the definition of special
abilities and models reusable to solve other problems. For instance, Genetic
Algorithms or Ant Colony Optimizations are inspired from natural CSs to solve
optimization problems. This paper proposes the use of ant-based systems to
solve various problems with a non assessing approach. This means that solutions
to some problem are not evaluated. They appear as resultant structures from the
activity of the system. Problems are modeled with graphs and such structures
are observed directly on these graphs. Problems of Multiple Sequences Alignment
and Natural Language Processing are addressed with this approach."
"Crucial to an Evolutionary Algorithm's performance is its selection scheme.
We mathematically investigate the relation between polynomial rank and
probabilistic tournament methods which are (respectively) generalisations of
the popular linear ranking and tournament selection schemes. We show that every
probabilistic tournament is equivalent to a unique polynomial rank scheme. In
fact, we derived explicit operators for translating between these two types of
selection. Of particular importance is that most linear and most practical
quadratic rank schemes are probabilistic tournaments."
"We study in detail the fitness landscape of a difficult cellular automata
computational task: the majority problem. Our results show why this problem
landscape is so hard to search, and we quantify the large degree of neutrality
found in various ways. We show that a particular subspace of the solution
space, called the ""Olympus"", is where good solutions concentrate, and give
measures to quantitatively characterize this subspace."
"The application of genetic algorithms (GAs) to many optimization problems in
organizations often results in good performance and high quality solutions. For
successful and efficient use of GAs, it is not enough to simply apply simple
GAs (SGAs). In addition, it is necessary to find a proper representation for
the problem and to develop appropriate search operators that fit well to the
properties of the genotype encoding. The representation must at least be able
to encode all possible solutions of an optimization problem, and genetic
operators such as crossover and mutation should be applicable to it. In this
paper, serial alternation strategies between two codings are formulated in the
framework of dynamic change of genotype encoding in GAs for function
optimization. Likewise, a new variant of GAs for difficult optimization
problems denoted {\it Split-and-Merge} GA (SM-GA) is developed using a parallel
implementation of an SGA and evolving a dynamic exchange of individual
representation in the context of Dual Coding concept. Numerical experiments
show that the evolved SM-GA significantly outperforms an SGA with static single
coding."
"This paper presents the Anisotropic selection scheme for cellular Genetic
Algorithms (cGA). This new scheme allows to enhance diversity and to control
the selective pressure which are two important issues in Genetic Algorithms,
especially when trying to solve difficult optimization problems. Varying the
anisotropic degree of selection allows swapping from a cellular to an island
model of parallel genetic algorithm. Measures of performances and diversity
have been performed on one well-known problem: the Quadratic Assignment Problem
which is known to be difficult to optimize. Experiences show that, tuning the
anisotropic degree, we can find the accurate trade-off between cGA and island
models to optimize performances of parallel evolutionary algorithms. This
trade-off can be interpreted as the suitable degree of migration among
subpopulations in a parallel Genetic Algorithm."
"Popular computational models of visual attention tend to neglect the
influence of saccadic eye movements whereas it has been shown that the primates
perform on average three of them per seconds and that the neural substrate for
the deployment of attention and the execution of an eye movement might
considerably overlap. Here we propose a computational model in which the
deployment of attention with or without a subsequent eye movement emerges from
local, distributed and numerical computations."
"Approaches to machine intelligence based on brain models have stressed the
use of neural networks for generalization. Here we propose the use of a hybrid
neural network architecture that uses two kind of neural networks
simultaneously: (i) a surface learning agent that quickly adapt to new modes of
operation; and, (ii) a deep learning agent that is very accurate within a
specific regime of operation. The two networks of the hybrid architecture
perform complementary functions that improve the overall performance. The
performance of the hybrid architecture has been compared with that of
back-propagation perceptrons and the CC and FC networks for chaotic time-series
prediction, the CATS benchmark test, and smooth function approximation. It has
been shown that the hybrid architecture provides a superior performance based
on the RMS error criterion."
"Skepticism of the building block hypothesis (BBH) has previously been
expressed on account of the weak theoretical foundations of this hypothesis and
the anomalies in the empirical record of the simple genetic algorithm. In this
paper we hone in on a more fundamental cause for skepticism--the extraordinary
strength of some of the assumptions that undergird the BBH. Specifically, we
focus on assumptions made about the distribution of fitness over the genome
set, and argue that these assumptions are unacceptably strong. As most of these
assumptions have been embraced by the designers of so-called ""competent""
genetic algorithms, our critique is relevant to an appraisal of such algorithms
as well."
"Since the inception of genetic algorithmics the identification of
computational efficiencies of the simple genetic algorithm (SGA) has been an
important goal. In this paper we distinguish between a computational competency
of the SGA--an efficient, but narrow computational ability--and a computational
proficiency of the SGA--a computational ability that is both efficient and
broad. Till date, attempts to deduce a computational proficiency of the SGA
have been unsuccessful. It may, however, be possible to inductively infer a
computational proficiency of the SGA from a set of related computational
competencies that have been deduced. With this in mind we deduce two
computational competencies of the SGA. These competencies, when considered
together, point toward a remarkable computational proficiency of the SGA. This
proficiency is pertinent to a general problem that is closely related to a
well-known statistical problem at the cutting edge of computational genetics."
"We propose a network characterization of combinatorial fitness landscapes by
adapting the notion of inherent networks proposed for energy surfaces (Doye,
2002). We use the well-known family of $NK$ landscapes as an example. In our
case the inherent network is the graph where the vertices are all the local
maxima and edges mean basin adjacency between two maxima. We exhaustively
extract such networks on representative small NK landscape instances, and show
that they are 'small-worlds'. However, the maxima graphs are not random, since
their clustering coefficients are much larger than those of corresponding
random graphs. Furthermore, the degree distributions are close to exponential
instead of Poissonian. We also describe the nature of the basins of attraction
and their relationship with the local maxima network."
"We propose a network characterization of combinatorial fitness landscapes by
adapting the notion of inherent networks proposed for energy surfaces. We use
the well-known family of NK landscapes as an example. In our case the inherent
network is the graph where the vertices represent the local maxima in the
landscape, and the edges account for the transition probabilities between their
corresponding basins of attraction. We exhaustively extracted such networks on
representative small NK landscape instances, and performed a statistical
characterization of their properties. We found that most of these network
properties can be related to the search difficulty on the underlying NK
landscapes with varying values of K."
"EVITA, standing for Evolutionary Inventory and Transportation Algorithm, is a
two-level methodology designed to address the Inventory and Transportation
Problem (ITP) in retail chains. The top level uses an evolutionary algorithm to
obtain delivery patterns for each shop on a weekly basis so as to minimise the
inventory costs, while the bottom level solves the Vehicle Routing Problem
(VRP) for every day in order to obtain the minimum transport costs associated
to a particular set of patterns. The aim of this paper is to investigate
whether a multiobjective approach to this problem can yield any advantage over
the previously used single objective approach. The analysis performed allows us
to conclude that this is not the case and that the single objective approach is
in gene- ral preferable for the ITP in the case studied. A further conclusion
is that it is useful to employ a classical algorithm such as Clarke & Wright's
as the seed for other metaheuristics like local search or tabu search in order
to provide good results for the Vehicle Routing Problem."
"The emission rate of minority atmospheric gases is inferred by a new approach
based on neural networks. The neural network applied is the multi-layer
perceptron with backpropagation algorithm for learning. The identification of
these surface fluxes is an inverse problem. A comparison between the new
neural-inversion and regularized inverse solution id performed. The results
obtained from the neural networks are significantly better. In addition, the
inversion with the neural netwroks is fster than regularized approaches, after
training."
"This paper proposes a novel neural-network-based adaptive hybrid-reflectance
three-dimensional (3-D) surface reconstruction model. The neural network
combines the diffuse and specular components into a hybrid model. The proposed
model considers the characteristics of each point and the variant albedo to
prevent the reconstructed surface from being distorted. The neural network
inputs are the pixel values of the two-dimensional images to be reconstructed.
The normal vectors of the surface can then be obtained from the output of the
neural network after supervised learning, where the illuminant direction does
not have to be known in advance. Finally, the obtained normal vectors can be
applied to integration method when reconstructing 3-D objects. Facial images
were used for training in the proposed approach"
"In the interconnected power system network, instability problems are caused
mainly by the low frequency oscillations of 0.2 to 2.5 Hz. The supplementary
control signal in addition with AVR and high gain excitation systems are
provided by means of Power System Stabilizer (PSS). Conventional power system
stabilizers provide effective damping only on a particular operating point. But
fuzzy based PSS provides good damping for a wide range of operating points. The
bottlenecks faced in designing a fuzzy logic controller can be minimized by
using appropriate optimization techniques like Genetic Algorithm, Particle Swam
Optimization, Ant Colony Optimization etc.In this paper the membership
functions of FLC are optimized by the new breed optimization technique called
Genetic Algorithm. This design methodology is implemented on a Single Machine
Infinite Bus (SMIB) system. Simulation results on SMIB show the effectiveness
and robustness of the proposed PSS over a wide range of operating conditions
and system configurations."
"The stability and convergence of the neural networks are the fundamental
characteristics in the Hopfield type networks. Since time delay is ubiquitous
in most physical and biological systems, more attention is being made for the
delayed neural networks. The inclusion of time delay into a neural model is
natural due to the finite transmission time of the interactions. The stability
analysis of the neural networks depends on the Lyapunov function and hence it
must be constructed for the given system. In this paper we have made an attempt
to establish the logarithmic stability of the impulsive delayed neural networks
by constructing suitable Lyapunov function."
"This paper describes a new method for the synthesis of planar antenna arrays
using fuzzy genetic algorithms (FGAs) by optimizing phase excitation
coefficients to best meet a desired radiation pattern. We present the
application of a rigorous optimization technique based on fuzzy genetic
algorithms (FGAs), the optimizing algorithm is obtained by adjusting control
parameters of a standard version of genetic algorithm (SGAs) using a fuzzy
controller (FLC) depending on the best individual fitness and the population
diversity measurements (PDM). The presented optimization algorithms were
previously checked on specific mathematical test function and show their
superior capabilities with respect to the standard version (SGAs). A planar
array with rectangular cells using a probe feed is considered. Included example
using FGA demonstrates the good agreement between the desired and calculated
radiation patterns than those obtained by a SGA."
"The Application of Bio Inspired Algorithms to complicated Power System
Stability Problems has recently attracted the researchers in the field of
Artificial Intelligence. Low frequency oscillations after a disturbance in a
Power system, if not sufficiently damped, can drive the system unstable. This
paper provides a systematic procedure to damp the low frequency oscillations
based on Bio Inspired Genetic (GA) and Particle Swarm Optimization (PSO)
algorithms. The proposed controller design is based on formulating a System
Damping ratio enhancement based Optimization criterion to compute the optimal
controller parameters for better stability. The Novel and contrasting feature
of this work is the mathematical modeling and simulation of the Synchronous
generator model including the Steam Governor Turbine (GT) dynamics. To show the
robustness of the proposed controller, Non linear Time domain simulations have
been carried out under various system operating conditions. Also, a detailed
Comparative study has been done to show the superiority of the Bio inspired
algorithm based controllers over the Conventional Lead lag controller."
"Since their conception in 1975, Genetic Algorithms have been an extremely
popular approach to find exact or approximate solutions to optimization and
search problems. Over the last years there has been an enhanced interest in the
field with related techniques, such as grammatical evolution, being developed.
Unfortunately, work on developing genetic optimizations for low-end embedded
architectures hasn't embraced the same enthusiasm. This short paper tackles
that situation by demonstrating how genetic algorithms can be implemented in
Arduino Duemilanove, a 16 MHz open-source micro-controller, with limited
computation power and storage resources. As part of this short paper, the
libraries used in this implementation are released into the public domain under
a GPL license."
"Inventory management is considered to be an important field in Supply Chain
Management because the cost of inventories in a supply chain accounts for about
30 percent of the value of the product. The service provided to the customer
eventually gets enhanced once the efficient and effective management of
inventory is carried out all through the supply chain. The precise estimation
of optimal inventory is essential since shortage of inventory yields to lost
sales, while excess of inventory may result in pointless storage costs. Thus
the determination of the inventory to be held at various levels in a supply
chain becomes inevitable so as to ensure minimal cost for the supply chain. The
minimization of the total supply chain cost can only be achieved when
optimization of the base stock level is carried out at each member of the
supply chain. This paper deals with the problem of determination of base stock
levels in a ten member serial supply chain with multiple products produced by
factories using Uniform Crossover Genetic Algorithms. The complexity of the
problem increases when more distribution centers and agents and multiple
products were involved. These considerations leading to very complex inventory
management process has been resolved in this work."
"With information revolution, increased globalization and competition, supply
chain has become longer and more complicated than ever before. These
developments bring supply chain management to the forefront of the managements
attention. Inventories are very important in a supply chain. The total
investment in inventories is enormous, and the management of inventory is
crucial to avoid shortages or delivery delays for the customers and serious
drain on a companys financial resources. The supply chain cost increases
because of the influence of lead times for supplying the stocks as well as the
raw materials. Practically, the lead times will not be same through out all the
periods. Maintaining abundant stocks in order to avoid the impact of high lead
time increases the holding cost. Similarly, maintaining fewer stocks because of
ballpark lead time may lead to shortage of stocks. This also happens in the
case of lead time involved in supplying raw materials. A better optimization
methodology that utilizes the Particle Swarm Optimization algorithm, one of the
best optimization algorithms, is proposed to overcome the impasse in
maintaining the optimal stock levels in each member of the supply chain. Taking
into account the stock levels thus obtained from the proposed methodology, an
appropriate stock levels to be maintained in the approaching periods that will
minimize the supply chain inventory cost can be arrived at."
"Because of the stochastic nature of traffic requirement matrix, it is very
difficult to get the optimal traffic distribution to minimize the delay even
with adaptive routing protocol in a fixed connection network where capacity
already defined for each link. Hence there is a requirement to define such a
method, which could generate the optimal solution very quickly and efficiently.
This paper presenting a new concept to provide the adaptive optimal traffic
distribution for dynamic condition of traffic matrix using nature based
intelligence methods. With the defined load and fixed capacity of links,
average delay for packet has minimized with various variations of evolutionary
programming and particle swarm optimization. Comparative study has given over
their performance in terms of converging speed. Universal approximation
capability, the key feature of feed forward neural network has applied to
predict the flow distribution on each link to minimize the average delay for a
total load available at present on the network. For any variation in the total
load, the new flow distribution can be generated by neural network immediately,
which could generate minimum delay in the network. With the inclusion of this
information, performance of routing protocol will be improved very much."
"This piece of research belongs to the field of educational assessment issue
based upon the cognitive multimedia theory. Considering that theory; visual and
auditory material should be presented simultaneously to reinforce the retention
of a mathematical learned topic, a carefully computer-assisted learning (CAL)
module is designed for development of a multimedia tutorial for our suggested
mathematical topic. The designed CAL module is a multimedia tutorial computer
package with visual and/or auditory material. So, via suggested computer
package, Multi-Sensory associative memories and classical conditioning theories
are practically applicable at an educational field (a children classroom). It
is noticed that comparative practical results obtained are interesting for
field application of CAL package with and without associated teacher's voice.
Finally, the presented study highly recommends application of a novel teaching
trend aiming to improve quality of children mathematical learning performance."
"In this paper, researchers estimated the stock price of activated companies
in Tehran (Iran) stock exchange. It is used Linear Regression and Artificial
Neural Network methods and compared these two methods. In Artificial Neural
Network, of General Regression Neural Network method (GRNN) for architecture is
used. In this paper, first, researchers considered 10 macro economic variables
and 30 financial variables and then they obtained seven final variables
including 3 macro economic variables and 4 financial variables to estimate the
stock price using Independent components Analysis (ICA). So, we presented an
equation for two methods and compared their results which shown that artificial
neural network method is more efficient than linear regression method."
"In this paper an attempt has been made to identify most important human
resource factors and propose a diagnostic model based on the back-propagation
and connectionist model approaches of artificial neural network (ANN). The
focus of the study is on the mobile -communication industry of India. The ANN
based approach is particularly important because conventional approaches (such
as algorithmic) to the problem solving have their inherent disadvantages. The
algorithmic approach is well-suited to the problems that are well-understood
and known solution(s). On the other hand the ANNs have learning by example and
processing capabilities similar to that of a human brain. ANN has been followed
due to its inherent advantage over conversion algorithmic like approaches and
having capabilities, training and human like intuitive decision making
capabilities. Therefore, this ANN based approach is likely to help researchers
and organizations to reach a better solution to the problem of managing the
human resource. The study is particularly important as many studies have been
carried in developed countries but there is a shortage of such studies in
developing nations like India. Here, a model has been derived using
connectionist-ANN approach and improved and verified via back-propagation
algorithm. This suggested ANN based model can be used for testing the success
and failure human factors in any of the communication Industry. Results have
been obtained on the basis of connectionist model, which has been further
refined by BPNN to an accuracy of 99.99%. Any company to predict failure due to
HR factors can directly deploy this model."
"The Global Positioning Systems (GPS) and Inertial Navigation System (INS)
technology have attracted a considerable importance recently because of its
large number of solutions serving both military as well as civilian
applications. This paper aims to develop a more efficient and especially a
faster method for processing the GPS signal in case of INS signal loss without
losing the accuracy of the data. The conventional or usual method consists of
processing data through a neural network and obtaining accurate positioning
output data. The new or improved method adds selective filtering at the
low-band frequency, the mid-band frequency and the high band frquency, before
processing the GPS data through the neural network, so that the processing time
is decreased significantly while the accuracy remains the same."
"We introduce a new neural architecture and an unsupervised algorithm for
learning invariant representations from temporal sequence of images. The system
uses two groups of complex cells whose outputs are combined multiplicatively:
one that represents the content of the image, constrained to be constant over
several consecutive frames, and one that represents the precise location of
features, which is allowed to vary over time but constrained to be sparse. The
architecture uses an encoder to extract features, and a decoder to reconstruct
the input from the features. The method was applied to patches extracted from
consecutive movie frames and produces orientation and frequency selective units
analogous to the complex cells in V1. An extension of the method is proposed to
train a network composed of units with local receptive field spread over a
large image of arbitrary size. A layer of complex cells, subject to sparsity
constraints, pool feature units over overlapping local neighborhoods, which
causes the feature units to organize themselves into pinwheel patterns of
orientation-selective receptive fields, similar to those observed in the
mammalian visual cortex. A feed-forward encoder efficiently computes the
feature representation of full images."
"We address the problem of finding patterns from multi-neuronal spike trains
that give us insights into the multi-neuronal codes used in the brain and help
us design better brain computer interfaces. We focus on the synchronous firings
of groups of neurons as these have been shown to play a major role in coding
and communication. With large electrode arrays, it is now possible to
simultaneously record the spiking activity of hundreds of neurons over large
periods of time. Recently, techniques have been developed to efficiently count
the frequency of synchronous firing patterns. However, when the number of
neurons being observed grows they suffer from the combinatorial explosion in
the number of possible patterns and do not scale well. In this paper, we
present a temporal data mining scheme that overcomes many of these problems. It
generates a set of candidate patterns from frequent patterns of smaller size;
all possible patterns are not counted. Also we count only a certain well
defined subset of occurrences and this makes the process more efficient. We
highlight the computational advantage that this approach offers over the
existing methods through simulations.
  We also propose methods for assessing the statistical significance of the
discovered patterns. We detect only those patterns that repeat often enough to
be significant and thus be able to automatically fix the threshold for the
data-mining application. Finally we discuss the usefulness of these methods for
brain computer interfaces."
"This paper continues on the work of the B-Matrix approach in hebbian learning
proposed by Dr. Kak. It reports the results on methods of improving the memory
retrieval capacity of the hebbian neural network which implements the B-Matrix
approach. Previously, the approach to retrieving the memories from the network
was to clamp all the individual neurons separately and verify the integrity of
these memories. Here we present a network with the capability to identify the
""active sites"" in the network during the training phase and use these ""active
sites"" to generate the memories retrieved from these neurons. Three methods are
proposed for obtaining the update order of the network from the proximity
matrix when multiple neurons are to be clamped. We then present a comparison
between the new methods to the classical case and also among the methods
themselves."
"A series of results of evolution supervised by genetic algorithms with
interest to agricultural and horticultural fields are reviewed. New obtained
original results from the use of genetic algorithms on structure-activity
relationships are reported."
"Neuroevolution is an active and growing research field, especially in times
of increasingly parallel computing architectures. Learning methods for
Artificial Neural Networks (ANN) can be divided into two groups. Neuroevolution
is mainly based on Monte-Carlo techniques and belongs to the group of global
search methods, whereas other methods such as backpropagation belong to the
group of local search methods. ANN's comprise important symmetry properties,
which can influence Monte-Carlo methods. On the other hand, local search
methods are generally unaffected by these symmetries. In the literature,
dealing with the symmetries is generally reported as being not effective or
even yielding inferior results. In this paper, we introduce the so called
Minimum Global Optimum Proximity principle derived from theoretical
considerations for effective symmetry breaking, applied to offline supervised
learning. Using Differential Evolution (DE), which is a popular and robust
evolutionary global optimization method, we experimentally show significant
global search efficiency improvements by symmetry breaking."
"In this paper, Estimation of Distribution Algorithm (EDA) is used for Zone
Routing Protocol (ZRP) in Mobile Ad-hoc Network (MANET) instead of Genetic
Algorithm (GA). It is an evolutionary approach, and used when the network size
grows and the search space increases. When the destination is outside the zone,
EDA is applied to find the route with minimum cost and time. The implementation
of proposed method is compared with Genetic ZRP, i.e., GZRP and the result
demonstrates better performance for the proposed method. Since the method
provides a set of paths to the destination, it results in load balance to the
network. As both EDA and GA use random search method to reach the optimal
point, the searching cost reduced significantly, especially when the number of
data is large."
"This paper presents some properties of unary coding of significance for
biological learning and instantaneously trained neural networks."
"This research is to search for alternatives to the resolution of complex
medical diagnosis where human knowledge should be apprehended in a general
fashion. Successful application examples show that human diagnostic
capabilities are significantly worse than the neural diagnostic system. Our
research describes a constructive neural network algorithm with
backpropagation; offer an approach for the incremental construction of
nearminimal neural network architectures for pattern classification. The
algorithm starts with minimal number of hidden units in the single hidden
layer; additional units are added to the hidden layer one at a time to improve
the accuracy of the network and to get an optimal size of a neural network. Our
algorithm was tested on several benchmarking classification problems including
Cancer1, Heart, and Diabetes with good generalization ability."
"Artificial neural networks (ANNs) have been successfully applied to solve a
variety of classification and function approximation problems. Although ANNs
can generally predict better than decision trees for pattern classification
problems, ANNs are often regarded as black boxes since their predictions cannot
be explained clearly like those of decision trees. This paper presents a new
algorithm, called rule extraction from ANNs (REANN), to extract rules from
trained ANNs for medical diagnosis problems. A standard three-layer feedforward
ANN with four-phase training is the basis of the proposed algorithm. In the
first phase, the number of hidden nodes in ANNs is determined automatically by
a constructive algorithm. In the second phase, irrelevant connections and input
nodes are removed from trained ANNs without sacrificing the predictive accuracy
of ANNs. The continuous activation values of the hidden nodes are discretized
by using an efficient heuristic clustering algorithm in the third phase.
Finally, rules are extracted from compact ANNs by examining the discretized
activation values of the hidden nodes. Extensive experimental studies on three
benchmark classification problems, i.e. breast cancer, diabetes and lenses,
demonstrate that REANN can generate high quality rules from ANNs, which are
comparable with other methods in terms of number of rules, average number of
conditions for a rule, and predictive accuracy."
"Although backpropagation ANNs generally predict better than decision trees do
for pattern classification problems, they are often regarded as black boxes,
i.e., their predictions cannot be explained as those of decision trees. In many
applications, it is desirable to extract knowledge from trained ANNs for the
users to gain a better understanding of how the networks solve the problems. A
new rule extraction algorithm, called rule extraction from artificial neural
networks (REANN) is proposed and implemented to extract symbolic rules from
ANNs. A standard three-layer feedforward ANN is the basis of the algorithm. A
four-phase training algorithm is proposed for backpropagation learning.
Explicitness of the extracted rules is supported by comparing them to the
symbolic rules generated by other methods. Extracted rules are comparable with
other methods in terms of number of rules, average number of conditions for a
rule, and predictive accuracy. Extensive experimental studies on several
benchmarks classification problems, such as breast cancer, iris, diabetes, and
season classification problems, demonstrate the effectiveness of the proposed
approach with good generalization ability."
"This research is to search for alternatives to the resolution of complex
medical diagnosis where human knowledge should be apprehended in a general
fashion. Successful application examples show that human diagnostic
capabilities are significantly worse than the neural diagnostic system. This
paper describes a modified feedforward neural network constructive algorithm
(MFNNCA), a new algorithm for medical diagnosis. The new constructive algorithm
with backpropagation; offer an approach for the incremental construction of
near-minimal neural network architectures for pattern classification. The
algorithm starts with minimal number of hidden units in the single hidden
layer; additional units are added to the hidden layer one at a time to improve
the accuracy of the network and to get an optimal size of a neural network. The
MFNNCA was tested on several benchmarking classification problems including the
cancer, heart disease and diabetes. Experimental results show that the MFNNCA
can produce optimal neural network architecture with good generalization
ability."
"This paper describes an efficient rule generation algorithm, called rule
generation from artificial neural networks (RGANN) to generate symbolic rules
from ANNs. Classification rules are sought in many areas from automatic
knowledge acquisition to data mining and ANN rule extraction. This is because
classification rules possess some attractive features. They are explicit,
understandable and verifiable by domain experts, and can be modified, extended
and passed on as modular knowledge. A standard three-layer feedforward ANN is
the basis of the algorithm. A four-phase training algorithm is proposed for
backpropagation learning. Comparing them to the symbolic rules generated by
other methods supports explicitness of the generated rules. Generated rules are
comparable with other methods in terms of number of rules, average number of
conditions for a rule, and predictive accuracy. Extensive experimental studies
on several benchmarks classification problems, including breast cancer, wine,
season, golf-playing, and lenses classification demonstrate the effectiveness
of the proposed approach with good generalization ability."
"Neural networks (NNs) have been successfully applied to solve a variety of
application problems involving classification and function approximation.
Although backpropagation NNs generally predict better than decision trees do
for pattern classification problems, they are often regarded as black boxes,
i.e., their predictions cannot be explained as those of decision trees. In many
applications, it is desirable to extract knowledge from trained NNs for the
users to gain a better understanding of how the networks solve the problems. An
algorithm is proposed and implemented to extract symbolic rules for medical
diagnosis problem. Empirical study on three benchmarks classification problems,
such as breast cancer, diabetes, and lenses demonstrates that the proposed
algorithm generates high quality rules from NNs comparable with other methods
in terms of number of rules, average number of conditions for a rule, and
predictive accuracy."
"In recent years, many neural network models have been proposed for pattern
classification, function approximation and regression problems. This paper
presents an approach for classifying patterns from simplified NNs. Although the
predictive accuracy of ANNs is often higher than that of other methods or human
experts, it is often said that ANNs are practically ""black boxes"", due to the
complexity of the networks. In this paper, we have an attempted to open up
these black boxes by reducing the complexity of the network. The factor makes
this possible is the pruning algorithm. By eliminating redundant weights,
redundant input and hidden units are identified and removed from the network.
Using the pruning algorithm, we have been able to prune networks such that only
a few input units, hidden units and connections left yield a simplified
network. Experimental results on several benchmarks problems in neural networks
show the effectiveness of the proposed approach with good generalization
ability."
"Artificial neural networks have been successfully applied to a variety of
business application problems involving classification and regression. Although
backpropagation neural networks generally predict better than decision trees do
for pattern classification problems, they are often regarded as black boxes,
i.e., their predictions are not as interpretable as those of decision trees. In
many applications, it is desirable to extract knowledge from trained neural
networks so that the users can gain a better understanding of the solution.
This paper presents an efficient algorithm to extract rules from artificial
neural networks. We use two-phase training algorithm for backpropagation
learning. In the first phase, the number of hidden nodes of the network is
determined automatically in a constructive fashion by adding nodes one after
another based on the performance of the network on training data. In the second
phase, the number of relevant input units of the network is determined using
pruning algorithm. The pruning process attempts to eliminate as many
connections as possible from the network. Relevant and irrelevant attributes of
the data are distinguished during the training process. Those that are relevant
will be kept and others will be automatically discarded. From the simplified
networks having small number of connections and nodes we may easily able to
extract symbolic rules using the proposed algorithm. Extensive experimental
results on several benchmarks problems in neural networks demonstrate the
effectiveness of the proposed approach with good generalization ability."
"This paper describes an efficient algorithm REx for generating symbolic rules
from artificial neural network (ANN). Classification rules are sought in many
areas from automatic knowledge acquisition to data mining and ANN rule
extraction. This is because classification rules possess some attractive
features. They are explicit, understandable and verifiable by domain experts,
and can be modified, extended and passed on as modular knowledge. REx exploits
the first order information in the data and finds shortest sufficient
conditions for a rule of a class that can differentiate it from patterns of
other classes. It can generate concise and perfect rules in the sense that the
error rate of the rules is not worse than the inconsistency rate found in the
original data. An important feature of rule extraction algorithm, REx, is its
recursive nature. They are concise, comprehensible, order insensitive and do
not involve any weight values. Extensive experimental studies on several
benchmark classification problems, such as breast cancer, iris, season, and
golf-playing, demonstrate the effectiveness of the proposed approach with good
generalization ability."
"In This paper we present a genetic algorithm for the multi-pickup and
delivery problem with time windows (m-PDPTW). The m-PDPTW is an optimization
vehicles routing problem which must meet requests for transport between
suppliers and customers satisfying precedence, capacity and time constraints.
This paper purposes a brief literature review of the PDPTW, present our
approach based on genetic algorithms to minimizing the total travel distance
and thereafter the total travel cost, by showing that an encoding represents
the parameters of each individual."
"In This paper we present a genetic algorithm for mulicriteria optimization of
a multipickup and delivery problem with time windows (m-PDPTW). The m-PDPTW is
an optimization vehicles routing problem which must meet requests for transport
between suppliers and customers satisfying precedence, capacity and time
constraints. This paper purposes a brief literature review of the PDPTW,
present an approach based on genetic algorithms and Pareto dominance method to
give a set of satisfying solutions to the m-PDPTW minimizing total travel cost,
total tardiness time and the vehicles number."
"The PDPTW is an optimization vehicles routing problem which must meet
requests for transport between suppliers and customers satisfying precedence,
capacity and time constraints. We present, in this paper, a genetic algorithm
for optimization of a multi pickup and delivery problem with time windows
(m-PDPTW). We purposes a brief literature review of the PDPTW, present an
approach based on genetic algorithms to give a satisfying solution to the
m-PDPTW minimizing the total travel cost."
"Nowadays, the transport goods problem occupies an important place in the
economic life of modern societies. The pickup and delivery problem with time
windows (PDPTW) is one of the problems which a large part of the research was
interested. In this paper, we present a a brief literature review of the VRP
and the PDPTW, propose our multicriteria approach based on genetic algorithms
which allows minimize the compromise between the vehicles number, the total
tardiness time and the total travel cost. And this, by treating the case where
a customer can have multiple suppliers and one supplier can have multiple
customers"
"Extending previous analyses on function classes like linear functions, we
analyze how the simple (1+1) evolutionary algorithm optimizes pseudo-Boolean
functions that are strictly monotone. Contrary to what one would expect, not
all of these functions are easy to optimize. The choice of the constant $c$ in
the mutation probability $p(n) = c/n$ can make a decisive difference.
  We show that if $c < 1$, then the (1+1) evolutionary algorithm finds the
optimum of every such function in $\Theta(n \log n)$ iterations. For $c=1$, we
can still prove an upper bound of $O(n^{3/2})$. However, for $c > 33$, we
present a strictly monotone function such that the (1+1) evolutionary algorithm
with overwhelming probability does not find the optimum within $2^{\Omega(n)}$
iterations. This is the first time that we observe that a constant factor
change of the mutation probability changes the run-time by more than constant
factors."
"Sparse coding algorithms are about finding a linear basis in which signals
can be represented by a small number of active (non-zero) coefficients. Such
coding has many applications in science and engineering and is believed to play
an important role in neural information processing. However, due to the
computational complexity of the task, only approximate solutions provide the
required efficiency (in terms of time). As new results show, under particular
conditions there exist efficient solutions by minimizing the magnitude of the
coefficients (`$l_1$-norm') instead of minimizing the size of the active subset
of features (`$l_0$-norm'). Straightforward neural implementation of these
solutions is not likely, as they require \emph{a priori} knowledge of the
number of active features. Furthermore, these methods utilize iterative
re-evaluation of the reconstruction error, which in turn implies that final
sparse forms (featuring `population sparseness') can only be reached through
the formation of a series of non-sparse representations, which is in contrast
with the overall sparse functioning of the neural systems (`lifetime
sparseness'). In this article we present a novel algorithm which integrates our
previous `$l_0$-norm' model on spike based probabilistic optimization for
sparse coding with ideas coming from novel `$l_1$-norm' solutions.
  The resulting algorithm allows neurally plausible implementation and does not
require an exactly defined sparseness level thus it is suitable for
representing natural stimuli with a varying number of features. We also
demonstrate that the combined method significantly extends the domain where
optimal solutions can be found by `$l_1$-norm' based algorithms."
"Differential evolution (DE) is a population based evolutionary algorithm
widely used for solving multidimensional global optimization problems over
continuous spaces. However, the design of its operators makes it unsuitable for
many real-life constrained combinatorial optimization problems which operate on
binary space. On the other hand, the quantum inspired evolutionary algorithm
(QEA) is very well suitable for handling such problems by applying several
quantum computing techniques such as Q-bit representation and rotation gate
operator, etc. This paper extends the concept of differential operators with
adaptive parameter control to the quantum paradigm and proposes the adaptive
quantum-inspired differential evolution algorithm (AQDE). The performance of
AQDE is found to be significantly superior as compared to QEA and a discrete
version of DE on the standard 0-1 knapsack problem for all the considered test
cases."
"In this work, we introduce multiplicative drift analysis as a suitable way to
analyze the runtime of randomized search heuristics such as evolutionary
algorithms.
  We give a multiplicative version of the classical drift theorem. This allows
easier analyses in those settings where the optimization progress is roughly
proportional to the current distance to the optimum.
  To display the strength of this tool, we regard the classical problem how the
(1+1) Evolutionary Algorithm optimizes an arbitrary linear pseudo-Boolean
function. Here, we first give a relatively simple proof for the fact that any
linear function is optimized in expected time $O(n \log n)$, where $n$ is the
length of the bit string. Afterwards, we show that in fact any such function is
optimized in expected time at most ${(1+o(1)) 1.39 \euler n\ln (n)}$, again
using multiplicative drift analysis. We also prove a corresponding lower bound
of ${(1-o(1))e n\ln(n)}$ which actually holds for all functions with a unique
global optimum.
  We further demonstrate how our drift theorem immediately gives natural proofs
(with better constants) for the best known runtime bounds for the (1+1)
Evolutionary Algorithm on combinatorial problems like finding minimum spanning
trees, shortest paths, or Euler tours."
"Spectrum scarceness is one of the major challenges that the present world is
facing. The efficient use of existing licensed spectrum is becoming most
critical as growing demand of the radio spectrum. Different researches show
that the use of licensed are not utilized inefficiently. It has been also shown
that primary user does not use more than 70% of the licensed frequency band
most of the time. Many researchers are trying to found the techniques that
efficiently utilize the under-utilized licensed spectrum. One of the approaches
is the use of ""Cognitive Radio"". This allows the radio to learn from its
environment, changing certain parameters. Based on this knowledge the radio can
dynamically exploit the spectrum holes in the licensed band of the spectrum.
This paper w i l l focus on the performance of spectrum allocation technique,
based on popular meta-heuristics Genetics Algorithm and analyzing the
performance of this technique using Mat Lab."
"Multi-Objective Evolutionary Algorithms (MOEAs) have been proved efficient to
deal with Multi-objective Optimization Problems (MOPs). Until now tens of MOEAs
have been proposed. The unified mode would provide a more systematic approach
to build new MOEAs. Here a new model is proposed which includes two sub-models
based on two classes of different schemas of MOEAs. According to the new model,
some representatives algorithms are decomposed and some interesting issues are
discussed."
"A genetic programming system is created. A first fitness function f1 is used
to evolve a program that implements a first feature. Then the fitness function
is switched to a second function f2, which is used to evolve a program that
implements a second feature while still maintaining the first feature. The
median number of generations G1 and G2 needed to evolve programs that work as
defined by f1 and f2 are measured. The behavior of G1 and G2 are observed as
the difficulty of the problem is increased.
  In these systems, the density D1 of programs that work (for fitness function
f1) is measured in the general population of programs. The relationship
G1~1/sqrt(D1) is observed to approximately hold. Also, the density D2 of
programs that work (for fitness function f2) is measured in the general
population of programs. The relationship G2~1/sqrt(D2) is observed to
approximately hold."
"Team pursuit track cycling is a bicycle racing sport held on velodromes and
is part of the Summer Olympics. It involves the use of strategies to minimize
the overall time that a team of cyclists needs to complete a race. We present
an optimisation framework for team pursuit track cycling and show how to evolve
strategies using metaheuristics for this interesting real-world problem. Our
experimental results show that these heuristics lead to significantly better
strategies than state-of-art strategies that are currently used by teams of
cyclists."
"This paper explores an idealized dynamic population sizing strategy for
solving additive decomposable problems of uniform scale. The method is designed
on top of the foundations of existing population sizing theory for this class
of problems, and is carefully compared with an optimal fixed population sized
genetic algorithm. The resulting strategy should be close to a lower bound in
terms of what can be achieved, performance-wise, by self-adjusting population
sizing algorithms for this class of problems."
"Evolution is one of the major omnipresent powers in the universe that has
been studied for about two centuries. Recent scientific and technical
developments make it possible to make the transition from passively
understanding to actively mastering evolution. As of today, the only area where
human experimenters can design and manipulate evolutionary processes in full is
that of Evolutionary Computing, where evolutionary processes are carried out in
a digital space, inside computers, in simulation. We argue that in the near
future it will be possible to move evolutionary computing outside such
imaginary spaces and make it physically embodied. In other words, we envision
the ""Evolution of Things"", rather than just the evolution of code, leading to a
new field of Embodied Artificial Evolution (EAE). The main objective of the
present paper is to offer an umbrella term and vision in order to aid the
development of this high potential research area. To this end, we introduce the
notion of EAE, discuss a few examples and applications, and elaborate on the
expected benefits as well as the grand challenges this developing field will
have to address."
"Estimating of the overhead costs of building construction projects is an
important task in the management of these projects. The quality of construction
management depends heavily on their accurate cost estimation. Construction
costs prediction is a very difficult and sophisticated task especially when
using manual calculation methods. This paper uses Artificial Neural Network
(ANN) approach to develop a parametric cost-estimating model for site overhead
cost in Egypt. Fifty-two actual real-life cases of building projects
constructed in Egypt during the seven year period 2002-2009 were used as
training materials. The neural network architecture is presented for the
estimation of the site overhead costs as a percentage from the total project
price."
"Although the traditional permute matrix coming along with Hopfield is able to
describe many common problems, it seems to have limitation in solving more
complicated problem with more constrains, like resource leveling which is
actually a NP problem. This paper tries to find a better solution for it by
using neural network. In order to give the neural network description of
resource leveling problem, a new description method called Augmented permute
matrix is proposed by expending the ability of the traditional one. An Embedded
Hybrid Model combining Hopfield model and SA are put forward to improve the
optimization in essence in which Hopfield servers as State Generator for the
SA. The experiment results show that Augmented permute matrix is able to
completely and appropriately describe the application. The energy function and
hybrid model given in this study are also highly efficient in solving resource
leveling problem."
"In this paper, we present a novel computational framework for nonlinear
dimensionality reduction which is specifically suited to process large data
sets: the Exploratory Inspection Machine (XIM). XIM introduces a conceptual
cross-link between hitherto separate domains of machine learning, namely
topographic vector quantization and divergence-based neighbor embedding
approaches. There are three ways to conceptualize XIM, namely (i) as the
inversion of the Exploratory Observation Machine (XOM) and its variants, such
as Neighbor Embedding XOM (NE-XOM), (ii) as a powerful optimization scheme for
divergence-based neighbor embedding cost functions inspired by Stochastic
Neighbor Embedding (SNE) and its variants, such as t-distributed SNE (t-SNE),
and (iii) as an extension of topographic vector quantization methods, such as
the Self-Organizing Map (SOM). By preserving both global and local data
structure, XIM combines the virtues of classical and advanced recent embedding
methods. It permits direct visualization of large data collections without the
need for prior data reduction. Finally, XIM can contribute to many application
domains of data analysis and visualization important throughout the sciences
and engineering, such as pattern matching, constrained incremental learning,
data clustering, and the analysis of non-metric dissimilarity data."
"Biclustering is a two way clustering approach involving simultaneous
clustering along two dimensions of the data matrix. Finding biclusters of web
objects (i.e. web users and web pages) is an emerging topic in the context of
web usage mining. It overcomes the problem associated with traditional
clustering methods by allowing automatic discovery of browsing pattern based on
a subset of attributes. A coherent bicluster of clickstream data is a local
browsing pattern such that users in bicluster exhibit correlated browsing
pattern through a subset of pages of a web site. This paper proposed a new
application of biclustering to web data using a combination of heuristics and
meta-heuristics such as K-means, Greedy Search Procedure and Genetic Algorithms
to identify the coherent browsing pattern. Experiment is conducted on the
benchmark clickstream msnbc dataset from UCI repository. Results demonstrate
the efficiency and beneficial outcome of the proposed method by correlating the
users and pages of a web site in high degree.This approach shows excellent
performance at finding high degree of overlapped coherent biclusters from web
data."
"Why 'GSA: A Gravitational Search Algorithm' Is Not Genuinely Based on the Law
of Gravity"
"Black-box complexity is a complexity theoretic measure for how difficult a
problem is to be optimized by a general purpose optimization algorithm. It is
thus one of the few means trying to understand which problems are tractable for
genetic algorithms and other randomized search heuristics.
  Most previous work on black-box complexity is on artificial test functions.
In this paper, we move a step forward and give a detailed analysis for the two
combinatorial problems minimum spanning tree and single-source shortest paths.
Besides giving interesting bounds for their black-box complexities, our work
reveals that the choice of how to model the optimization problem is non-trivial
here. This in particular comes true where the search space does not consist of
bit strings and where a reasonable definition of unbiasedness has to be agreed
on."
"We investigate Turing's notion of an A-type artificial neural network. We
study a refinement of Turing's original idea, motivated by work of Teuscher,
Bull, Preen and Copeland. Our A-types can process binary data by accepting and
outputting sequences of binary vectors; hence we can associate a function to an
A-type, and we say the A-type {\em represents} the function. There are two
modes of data processing: clamped and sequential. We describe an evolutionary
algorithm, involving graph-theoretic manipulations of A-types, which searches
for A-types representing a given function. The algorithm uses both mutation and
crossover operators. We implemented the algorithm and applied it to three
benchmark tasks. We found that the algorithm performed much better than a
random search. For two out of the three tasks, the algorithm with crossover
performed better than a mutation-only version."
"In this paper, a novel mutation operator of differential evolution algorithm
is proposed. A new algorithm called divergence differential evolution algorithm
(DDEA) is developed by combining the new mutation operator with divergence
operator and assimilation operator (divergence operator divides population,
and, assimilation operator combines population), which can detect multiple
solutions and robustness in noisy environment. The new algorithm is applied to
optimize Michalewicz Function and to track changing of rain-induced-attenuation
process. The results based on DDEA are compared with those based on
Differential Evolution Algorithm (DEA). It shows that DDEA algorithm gets
better results than DEA does in the same premise. The new algorithm is
significant for optimizing and tracking the characteristics of MIMO (Multiple
Input Multiple Output) channel at millimeter waves."
"We present a number of bounds on convergence time for two elitist
population-based Evolutionary Algorithms using a recombination operator
k-Bit-Swap and a mainstream Randomized Local Search algorithm. We study the
effect of distribution of elite species and population size."
"We present an analysis of the performance of an elitist Evolutionary
algorithm using a recombination operator known as 1-Bit-Swap on the Royal Roads
test function based on a population. We derive complete, approximate and
asymptotic convergence rates for the algorithm. The complete model shows the
benefit of the size of the population and re- combination pool."
"The analysis of randomized search heuristics on classes of functions is
fundamental for the understanding of the underlying stochastic process and the
development of suitable proof techniques. Recently, remarkable progress has
been made in bounding the expected optimization time of the simple (1+1) EA on
the class of linear functions. We improve the best known bound in this setting
from $(1.39+o(1))en\ln n$ to $en\ln n+O(n)$ in expectation and with high
probability, which is tight up to lower-order terms. Moreover, upper and lower
bounds for arbitrary mutations probabilities $p$ are derived, which imply
expected polynomial optimization time as long as $p=O((\ln n)/n)$ and which are
tight if $p=c/n$ for a constant $c$. As a consequence, the standard mutation
probability $p=1/n$ is optimal for all linear functions, and the (1+1) EA is
found to be an optimal mutation-based algorithm. The proofs are based on
adaptive drift functions and the recent multiplicative drift theorem."
"Population-based evolutionary algorithms (EAs) have been widely applied to
solve various optimization problems. The question of how the performance of a
population-based EA depends on the population size arises naturally. The
performance of an EA may be evaluated by different measures, such as the
average convergence rate to the optimal set per generation or the expected
number of generations to encounter an optimal solution for the first time.
Population scalability is the performance ratio between a benchmark EA and
another EA using identical genetic operators but a larger population size.
Although intuitively the performance of an EA may improve if its population
size increases, currently there exist only a few case studies for simple
fitness functions. This paper aims at providing a general study for discrete
optimisation. A novel approach is introduced to analyse population scalability
using the fundamental matrix. The following two contributions summarize the
major results of the current article. (1) We demonstrate rigorously that for
elitist EAs with identical global mutation, using a lager population size
always increases the average rate of convergence to the optimal set; and yet,
sometimes, the expected number of generations needed to find an optimal
solution (measured by either the maximal value or the average value) may
increase, rather than decrease. (2) We establish sufficient and/or necessary
conditions for the superlinear scalability, that is, when the average
convergence rate of a $(\mu+\mu)$ EA (where $\mu\ge2$) is bigger than $\mu$
times that of a $(1+1)$ EA."
"Most transformer failures are attributed to bushings failures. Hence it is
necessary to monitor the condition of bushings. In this paper three methods are
developed to monitor the condition of oil filled bushing. Multi-layer
perceptron (MLP), Radial basis function (RBF) and Rough Set (RS) models are
developed and combined through majority voting to form a committee. The MLP
performs better that the RBF and the RS is terms of classification accuracy.
The RBF is the fasted to train. The committee performs better than the
individual models. The diversity of models is measured to evaluate their
similarity when used in the committee."
"Most transformer failures are attributed to bushings failures. Hence it is
necessary to monitor the condition of bushings. In this paper three methods are
developed to monitor the condition of oil filled bushing. Multi-layer
perceptron (MLP), Radial basis function (RBF) and Rough Set (RS) models are
developed and combined through majority voting to form a committee. The MLP
performs better that the RBF and the RS is terms of classification accuracy.
The RBF is the fasted to train. The committee performs better than the
individual models. The diversity of models is measured to evaluate their
similarity when used in the committee."
"One of the problems in applying Genetic Algorithm is that there is some
situation where the evolutionary process converges too fast to a solution which
causes it to be trapped in local optima. To overcome this problem, a proper
diversity in the candidate solutions must be determined. Most existing
diversity-maintenance mechanisms require a problem specific knowledge to setup
parameters properly. This work proposes a method to control diversity of the
population without explicit parameter setting. A self-adaptation mechanism is
proposed based on the competition of preference characteristic in mating. It
can adapt the population toward proper diversity for the problems. The
experiments are carried out to measure the effectiveness of the proposed method
based on nine well-known test problems. The performance of the adaptive method
is comparable to traditional Genetic Algorithm with the best parameter setting."
"In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks."
"In this paper, a complete preprocessing methodology for discovering patterns
in web usage mining process to improve the quality of data by reducing the
quantity of data has been proposed. A dynamic ART1 neural network clustering
algorithm to group users according to their Web access patterns with its neat
architecture is also proposed. Several experiments are conducted and the
results show the proposed methodology reduces the size of Web log files down to
73-82% of the initial size and the proposed ART1 algorithm is dynamic and
learns relatively stable quality clusters."
"We present a new method for proving lower bounds on the expected running time
of evolutionary algorithms. It is based on fitness-level partitions and an
additional condition on transition probabilities between fitness levels. The
method is versatile, intuitive, elegant, and very powerful. It yields exact or
near-exact lower bounds for LO, OneMax, long k-paths, and all functions with a
unique optimum. Most lower bounds are very general: they hold for all
evolutionary algorithms that only use bit-flip mutation as variation
operator---i.e. for all selection operators and population models. The lower
bounds are stated with their dependence on the mutation rate.
  These results have very strong implications. They allow to determine the
optimal mutation-based algorithm for LO and OneMax, i.e., which algorithm
minimizes the expected number of fitness evaluations. This includes the choice
of the optimal mutation rate."
"Evolutionary algorithms are popular heuristics for solving various
combinatorial problems as they are easy to apply and often produce good
results. Island models parallelize evolution by using different populations,
called islands, which are connected by a graph structure as communication
topology. Each island periodically communicates copies of good solutions to
neighboring islands in a process called migration.
  We consider the speedup gained by island models in terms of the parallel
running time for problems from combinatorial optimization: sorting (as
maximization of sortedness), shortest paths, and Eulerian cycles. Different
search operators are considered. The results show in which settings and up to
what degree evolutionary algorithms can be parallelized efficiently. Along the
way, we also investigate how island models deal with plateaus. In particular,
we show that natural settings lead to exponential vs. logarithmic speedups,
depending on the frequency of migration."
"In this paper we propose a crossover operator for evolutionary algorithms
with real values that is based on the statistical theory of population
distributions. The operator is based on the theoretical distribution of the
values of the genes of the best individuals in the population. The proposed
operator takes into account the localization and dispersion features of the
best individuals of the population with the objective that these features would
be inherited by the offspring. Our aim is the optimization of the balance
between exploration and exploitation in the search process. In order to test
the efficiency and robustness of this crossover, we have used a set of
functions to be optimized with regard to different criteria, such as,
multimodality, separability, regularity and epistasis. With this set of
functions we can extract conclusions in function of the problem at hand. We
analyze the results using ANOVA and multiple comparison statistical tests. As
an example of how our crossover can be used to solve artificial intelligence
problems, we have applied the proposed model to the problem of obtaining the
weight of each network in a ensemble of neural networks. The results obtained
are above the performance of standard methods."
"Spiking neural networks have been referred to as the third generation of
artificial neural networks where the information is coded as time of the
spikes. There are a number of different spiking neuron models available and
they are categorized based on their level of abstraction. In addition, there
are two known learning methods, unsupervised and supervised learning. This
thesis focuses on supervised learning where a new algorithm is proposed, based
on genetic algorithms. The proposed algorithm is able to train both synaptic
weights and delays and also allow each neuron to emit multiple spikes thus
taking full advantage of the spatial-temporal coding power of the spiking
neurons. In addition, limited synaptic precision is applied; only six bits are
used to describe and train a synapse, three bits for the weights and three bits
for the delays. Two limited precision schemes are investigated. The proposed
algorithm is tested on the XOR classification problem where it produces better
results for even smaller network architectures than the proposed ones.
Furthermore, the algorithm is benchmarked on the Fisher iris classification
problem where it produces higher classification accuracies compared to
SpikeProp, QuickProp and Rprop. Finally, a hardware implementation on a
microcontroller is done for the XOR problem as a proof of concept. Keywords:
Spiking neural networks, supervised learning, limited synaptic precision,
genetic algorithms, hardware implementation."
"This is a preprint of a book chapter from the Handbook of Memetic Algorithms,
Studies in Computational Intelligence, Vol. 379, ISBN 978-3-642-23246-6,
Springer, edited by F. Neri, C. Cotta, and P. Moscato. It is devoted to the
parametrization of memetic algorithms and how to find a good balance between
global and local search."
"Evolvable hardware (EHW) is a set of techniques that are based on the idea of
combining reconfiguration hardware systems with evolutionary algorithms. In
other word, EHW has two sections; the reconfigurable hardware and evolutionary
algorithm where the configurations are under the control of an evolutionary
algorithm. This paper, suggests a method to design and optimize the synchronous
sequential circuits. Genetic algorithm (GA) was applied as evolutionary
algorithm. In this approach, for building input combinational logic circuit of
each DFF, and also output combinational logic circuit, the cell arrays have
been used. The obtained results show that our method can reduce the average
number of generations by limitation the search space."
"Mixed strategy EAs aim to integrate several mutation operators into a single
algorithm. However few theoretical analysis has been made to answer the
question whether and when the performance of mixed strategy EAs is better than
that of pure strategy EAs. In theory, the performance of EAs can be measured by
asymptotic convergence rate and asymptotic hitting time. In this paper, it is
proven that given a mixed strategy (1+1) EAs consisting of several mutation
operators, its performance (asymptotic convergence rate and asymptotic hitting
time)is not worse than that of the worst pure strategy (1+1) EA using one
mutation operator; if these mutation operators are mutually complementary, then
it is possible to design a mixed strategy (1+1) EA whose performance is better
than that of any pure strategy (1+1) EA using one mutation operator."
"Customer Relationship Management becomes a leading business strategy in
highly competitive business environment. It aims to enhance the performance of
the businesses by improving the customer satisfaction and loyalty. The
objective of this paper is to improve customer satisfaction on product's colors
and design with the help of the expert system developed by using Artificial
Neural Networks. The expert system's role is to capture the knowledge of the
experts and the data from the customer requirements, and then, process the
collected data and form the appropriate rules for choosing product's colors and
design. In order to identify the hidden pattern of the customer's needs, the
Artificial Neural Networks technique has been applied to classify the colors
and design based upon a list of selected information. Moreover, the expert
system has the capability to make decisions in ranking the scores of the colors
and design presented in the selection. In addition, the expert system has been
validated with a different customer types."
"The rapid advances in the field of optimization methods in many pure and
applied science pose the difficulty of keeping track of the developments as
well as selecting an appropriate technique that best suits the problem in-hand.
From a practitioner point of view is rightful to wander ""which optimization
method is the best for my problem?"". Looking at the optimization process as a
""system"" of intercon- nected parts, in this paper are collected some ideas
about how to tackle an optimization problem using a class of tools from
evolutionary computations called Genetic Algorithms. Despite the number of
optimization techniques available nowadays the author of this paper thinks that
Genetic Algorithms still play a central role for their versatility, robustness,
theoretical framework and simplicity of use. The paper can be considered a
""collection of tips"" (from literature and personal experience) for the
non-computer-scientist that has to deal with optimization problems both in the
science and engineering practice. No original methods or algorithms are
proposed."
"Two complementary techniques for analyzing search spaces are proposed: (i) an
algorithm to detect search points with potential to be local optima; and (ii) a
slightly adjusted Wang-Landau sampling algorithm to explore larger search
spaces. The detection algorithm assumes that local optima are points which are
easier to reach and harder to leave by a slow adaptive walker. A slow adaptive
walker moves to a nearest fitter point. Thus, points with larger outgoing step
sizes relative to incoming step sizes are marked using the local optima score
formulae as potential local optima points (PLOPs). Defining local optima in
these more general terms allows their detection within the closure of a subset
of a search space, and the sampling of a search space unshackled by a
particular move set. Tests are done with NK and HIFF problems to confirm that
PLOPs detected in the manner proposed retain characteristics of local optima,
and that the adjusted Wang-Landau samples are more representative of the search
space than samples produced by choosing points uniformly at random. While our
approach shows promise, more needs to be done to reduce its computation cost
that it may pave a way toward analyzing larger search spaces of practical
meaning."
"Robot design complexity is increasing day by day especially in automated
industries. In this paper we propose biologically inspired design framework for
robots in dynamic world on the basis of Co-Evolution, Virtual Ecology, Life
time learning which are derived from biological creatures. We have created a
virtual khepera robot in Framsticks and tested its operational credibility in
terms hardware and software components by applying the above suggested
techniques. Monitoring complex and non complex behaviors in different
environments and obtaining the parameters that influence software and hardware
design of the robot that influence anticipated and unanticipated failures,
control programs of robot generation are the major concerns of our techniques."
"This short paper introduces a new way by which to design production system
rules. An indirect encoding scheme is presented which views such rules as
protein complexes produced by the temporal behaviour of an artificial genetic
regulatory network. This initial study begins by using a simple Boolean
regulatory network to produce traditional ternary-encoded rules before moving
to a fuzzy variant to produce real-valued rules. Competitive performance is
shown with related genetic regulatory networks and rule-based systems on
benchmark problems."
"We investigate the self-organising behaviour of Digital Ecosystems, because a
primary motivation for our research is to exploit the self-organising
properties of biological ecosystems. We extended a definition for the
complexity, grounded in the biological sciences, providing a measure of the
information in an organism's genome. Next, we extended a definition for the
stability, originating from the computer sciences, based upon convergence to an
equilibrium distribution. Finally, we investigated a definition for the
diversity, relative to the selection pressures provided by the user requests.
We conclude with a summary and discussion of the achievements, including the
experimental results."
"Applications of ACO algorithms to obtain better solutions for combinatorial
optimization problems have become very popular in recent years. In ACO
algorithms, group of agents repeatedly perform well defined actions and
collaborate with other ants in order to accomplish the defined task. In this
paper, we introduce new mechanisms for selecting the Elite ants dynamically
based on simple statistical tools. We also investigate the performance of newly
proposed mechanisms."
"Handwritten Numeral recognition plays a vital role in postal automation
services especially in countries like India where multiple languages and
scripts are used Discrete Hidden Markov Model (HMM) and hybrid of Neural
Network (NN) and HMM are popular methods in handwritten word recognition
system. The hybrid system gives better recognition result due to better
discrimination capability of the NN. A major problem in handwriting recognition
is the huge variability and distortions of patterns. Elastic models based on
local observations and dynamic programming such HMM are not efficient to absorb
this variability. But their vision is local. But they cannot face to length
variability and they are very sensitive to distortions. Then the SVM is used to
estimate global correlations and classify the pattern. Support Vector Machine
(SVM) is an alternative to NN. In Handwritten recognition, SVM gives a better
recognition result. The aim of this paper is to develop an approach which
improve the efficiency of handwritten recognition using artificial neural
network"
"We show that for all $1<k \leq \log n$ the $k$-ary unbiased black-box
complexity of the $n$-dimensional $\onemax$ function class is $O(n/k)$. This
indicates that the power of higher arity operators is much stronger than what
the previous $O(n/\log k)$ bound by Doerr et al. (Faster black-box algorithms
through higher arity operators, Proc. of FOGA 2011, pp. 163--172, ACM, 2011)
suggests.
  The key to this result is an encoding strategy, which might be of independent
interest. We show that, using $k$-ary unbiased variation operators only, we may
simulate an unrestricted memory of size $O(2^k)$ bits."
"The computational complexity analysis of genetic programming (GP) has been
started recently by analyzing simple (1+1) GP algorithms for the problems ORDER
and MAJORITY. In this paper, we study how taking the complexity as an
additional criteria influences the runtime behavior. We consider
generalizations of ORDER and MAJORITY and present a computational complexity
analysis of (1+1) GP using multi-criteria fitness functions that take into
account the original objective and the complexity of a syntax tree as a
secondary measure. Furthermore, we study the expected time until
population-based multi-objective genetic programming algorithms have computed
the Pareto front when taking the complexity of a syntax tree as an equally
important objective."
"In this paper, we present a new mutation operator, Hybrid Mutation (HPRM),
for a genetic algorithm that generates high quality solutions to the Traveling
Salesman Problem (TSP). The Hybrid Mutation operator constructs an offspring
from a pair of parents by hybridizing two mutation operators, PSM and RSM. The
efficiency of the HPRM is compared as against some existing mutation operators;
namely, Reverse Sequence Mutation (RSM) and Partial Shuffle Mutation (PSM) for
BERLIN52 as instance of TSPLIB. Experimental results show that the new mutation
operator is better than the RSM and PSM."
"The hardness of fitness functions is an important research topic in the field
of evolutionary computation. In theory, the study can help understanding the
ability of evolutionary algorithms. In practice, the study may provide a
guideline to the design of benchmarks. The aim of this paper is to answer the
following research questions: Given a fitness function class, which functions
are the easiest with respect to an evolutionary algorithm? Which are the
hardest? How are these functions constructed? The paper provides theoretical
answers to these questions. The easiest and hardest fitness functions are
constructed for an elitist (1+1) evolutionary algorithm to maximise a class of
fitness functions with the same optima. It is demonstrated that the unimodal
functions are the easiest and deceptive functions are the hardest in terms of
the time-fitness landscape. The paper also reveals that the easiest fitness
function to one algorithm may become the hardest to another algorithm, and vice
versa."
"Today, robotics is an auspicious and fast-growing branch of technology that
involves the manufacturing, design, and maintenance of robot machines that can
operate in an autonomous fashion and can be used in a wide variety of
applications including space exploration, weaponry, household, and
transportation. More particularly, in space applications, a common type of
robots has been of widespread use in the recent years. It is called planetary
rover which is a robot vehicle that moves across the surface of a planet and
conducts detailed geological studies pertaining to the properties of the
landing cosmic environment. However, rovers are always impeded by obstacles
along the traveling path which can destabilize the rover's body and prevent it
from reaching its goal destination. This paper proposes an ANN model that
allows rover systems to carry out autonomous path-planning to successfully
navigate through challenging planetary terrains and follow their goal location
while avoiding dangerous obstacles. The proposed ANN is a multilayer network
made out of three layers: an input, a hidden, and an output layer. The network
is trained in offline mode using back-propagation supervised learning
algorithm. A software-simulated rover was experimented and it revealed that it
was able to follow the safest trajectory despite existing obstacles. As future
work, the proposed ANN is to be parallelized so as to speed-up the execution
time of the training process."
"Today, a wide variety of probabilistic and expert AI systems used to analyze
real world inputs such as unstructured text, sounds, images, and statistical
data. However, all these systems exist on different platforms, with different
implementations, and with very different, often very specific goals in mind.
This paper introduces a concept for a mediator framework for such systems and
seeks to show several architectures which would support it, potential benefits
in combining the signals of disparate networks for formalized, high level logic
and signal processing, and its possible academic and industrial uses."
"Spike-Timing Dependent Plasticity (STDP) is believed to play an important
role in learning and the formation of computational function in the brain. The
classical model of STDP which considers the timing between pairs of
pre-synaptic and post-synaptic spikes (p-STDP) is incapable of reproducing
synaptic weight changes similar to those seen in biological experiments which
investigate the effect of either higher order spike trains (e.g. triplet and
quadruplet of spikes), or, simultaneous effect of the rate and timing of spike
pairs on synaptic plasticity. In this paper, we firstly investigate synaptic
weight changes using a p-STDP circuit and show how it fails to reproduce the
mentioned complex biological experiments. We then present a new STDP VLSI
circuit which acts based on the timing among triplets of spikes (t-STDP) that
is able to reproduce all the mentioned experimental results. We believe that
our new STDP VLSI circuit improves upon previous circuits, whose learning
capacity exceeds current designs due to its capability of mimicking the
outcomes of biological experiments more closely; thus plays a significant role
in future VLSI implementation of neuromorphic systems."
"This paper describes the application of a real coded genetic algorithm (GA)
to align two or more 2-D images by means of image registration. The proposed
search strategy is a transformation parameters-based approach involving the
affine transform. The real coded GA uses Simulated Binary Crossover (SBX), a
parent-centric recombination operator that has shown to deliver a good
performance in many optimization problems in the continuous domain. In
addition, we propose a new technique for matching points between a warped and
static images by using a randomized ordering when visiting the points during
the matching procedure. This new technique makes the evaluation of the
objective function somewhat noisy, but GAs and other population-based search
algorithms have been shown to cope well with noisy fitness evaluations. The
results obtained are competitive to those obtained by state-of-the-art
classical methods in image registration, confirming the usefulness of the
proposed noisy objective function and the suitability of SBX as a recombination
operator for this type of problem."
"This paper presents a novel mechanism to adapt surrogate-assisted
population-based algorithms. This mechanism is applied to ACM-ES, a recently
proposed surrogate-assisted variant of CMA-ES. The resulting algorithm,
saACM-ES, adjusts online the lifelength of the current surrogate model (the
number of CMA-ES generations before learning a new surrogate) and the surrogate
hyper-parameters. Both heuristics significantly improve the quality of the
surrogate model, yielding a significant speed-up of saACM-ES compared to the
ACM-ES and CMA-ES baselines. The empirical validation of saACM-ES on the
BBOB-2012 noiseless testbed demonstrates the efficiency and the scalability
w.r.t the problem dimension and the population size of the proposed approach,
that reaches new best results on some of the benchmark problems."
"The placement of wind turbines on a given area of land such that the wind
farm produces a maximum amount of energy is a challenging optimization problem.
In this article, we tackle this problem, taking into account wake effects that
are produced by the different turbines on the wind farm. We significantly
improve upon existing results for the minimization of wake effects by
developing a new problem-specific local search algorithm. One key step in the
speed-up of our algorithm is the reduction in computation time needed to assess
a given wind farm layout compared to previous approaches. Our new method allows
the optimization of large real-world scenarios within a single night on a
standard computer, whereas weeks on specialized computing servers were required
for previous approaches."
"This paper explores the theoretical basis of the covariance matrix adaptation
evolution strategy (CMA-ES) from the information geometry viewpoint.
  To establish a theoretical foundation for the CMA-ES, we focus on a geometric
structure of a Riemannian manifold of probability distributions equipped with
the Fisher metric. We define a function on the manifold which is the
expectation of fitness over the sampling distribution, and regard the goal of
update of the parameters of sampling distribution in the CMA-ES as maximization
of the expected fitness. We investigate the steepest ascent learning for the
expected fitness maximization, where the steepest ascent direction is given by
the natural gradient, which is the product of the inverse of the Fisher
information matrix and the conventional gradient of the function.
  Our first result is that we can obtain under some types of parameterization
of multivariate normal distribution the natural gradient of the expected
fitness without the need for inversion of the Fisher information matrix. We
find that the update of the distribution parameters in the CMA-ES is the same
as natural gradient learning for expected fitness maximization. Our second
result is that we derive the range of learning rates such that a step in the
direction of the exact natural gradient improves the parameters in the expected
fitness. We see from the close relation between the CMA-ES and natural gradient
learning that the default setting of learning rates in the CMA-ES seems
suitable in terms of monotone improvement in expected fitness. Then, we discuss
the relation to the expectation-maximization framework and provide an
information geometric interpretation of the CMA-ES."
"In this paper, we study the performance of IPOP-saACM-ES, recently proposed
self-adaptive surrogate-assisted Covariance Matrix Adaptation Evolution
Strategy. The algorithm was tested using restarts till a total number of
function evaluations of $10^6D$ was reached, where $D$ is the dimension of the
function search space. The experiments show that the surrogate model control
allows IPOP-saACM-ES to be as robust as the original IPOP-aCMA-ES and
outperforms the latter by a factor from 2 to 3 on 6 benchmark problems with
moderate noise. On 15 out of 30 benchmark problems in dimension 20,
IPOP-saACM-ES exceeds the records observed during BBOB-2009 and BBOB-2010."
"The Artificial Bee Colony (ABC) is the name of an optimization algorithm that
was inspired by the intelligent behavior of a honey bee swarm. It is widely
recognized as a quick, reliable, and efficient methods for solving optimization
problems. This paper proposes a hybrid ABC (HABC) algorithm for graph
3-coloring, which is a well-known discrete optimization problem. The results of
HABC are compared with results of the well-known graph coloring algorithms of
today, i.e. the Tabucol and Hybrid Evolutionary algorithm (HEA) and results of
the traditional evolutionary algorithm with SAW method (EA-SAW). Extensive
experimentations has shown that the HABC matched the competitive results of the
best graph coloring algorithms, and did better than the traditional heuristics
EA-SAW when solving equi-partite, flat, and random generated medium-sized
graphs."
"Prior studies have generally suggested that Artificial Neural Networks (ANNs)
are superior to conventional statistical models in predicting consumer buying
behavior. There are, however, contradicting findings which raise question over
usefulness of ANNs. This paper discusses development of three neural networks
for modeling consumer e-commerce behavior and compares the findings to
equivalent logistic regression models. The results showed that ANNs predict
e-commerce adoption slightly more accurately than logistic models but this is
hardly justifiable given the added complexity. Further, ANNs seem to be highly
adaptive, particularly when a small sample is coupled with a large number of
nodes in hidden layers which, in turn, limits the neural networks'
generalisability."
"The paper introduces a connectionist network approach to find numerical
solutions of Diophantine equations as an attempt to address the famous
Hilbert's tenth problem. The proposed methodology uses a three layer feed
forward neural network with back propagation as sequential learning procedure
to find numerical solutions of a class of Diophantine equations. It uses a
dynamically constructed network architecture where number of nodes in the input
layer is chosen based on the number of variables in the equation. The powers of
the given Diophantine equation are taken as input to the input layer. The
training of the network starts with initial random integral weights. The
weights are updated based on the back propagation of the error values at the
output layer. The optimization of weights is augmented by adding a momentum
factor into the network. The optimized weights of the connection between the
input layer and the hidden layer are taken as numerical solution of the given
Diophantine equation. The procedure is validated using different Diophantine
Equations of different number of variables and different powers."
"We present a new method for analyzing the running time of parallel
evolutionary algorithms with spatially structured populations. Based on the
fitness-level method, it yields upper bounds on the expected parallel running
time. This allows to rigorously estimate the speedup gained by parallelization.
Tailored results are given for common migration topologies: ring graphs, torus
graphs, hypercubes, and the complete graph. Example applications for
pseudo-Boolean optimization show that our method is easy to apply and that it
gives powerful results. In our examples the possible speedup increases with the
density of the topology. Surprisingly, even sparse topologies like ring graphs
lead to a significant speedup for many functions while not increasing the total
number of function evaluations by more than a constant factor. We also identify
which number of processors yield asymptotically optimal speedups, thus giving
hints on how to parametrize parallel evolutionary algorithms."
"An algorithm (bliss) is proposed to speed up the construction of slow
adaptive walks. Slow adaptive walks are adaptive walks biased towards closer
points or smaller move steps. They were previously introduced to explore a
search space, e.g. to detect potential local optima or to assess the ruggedness
of a fitness landscape. To avoid the quadratic cost of computing Hamming
distance (HD) for all-pairs of strings in a set in order to find the set of
closest strings for each string, strings are sorted and clustered by bliss such
that similar strings are more likely to get paired off for HD computation. To
efficiently arrange the strings by similarity, bliss employs the idea of shared
non-overlapping position specific subsequences between strings which is
inspired by an alignment-free protein sequence comparison algorithm. Tests are
performed to evaluate the quality of b-walks, i.e. slow adaptive walks
constructed from the output of bliss, on enumerated search spaces. Finally,
b-walks are applied to explore larger search spaces with the help of
Wang-Landau sampling."
"In this research paper, the problem of optimization of quadratic forms
associated with the dynamics of Hopfield-Amari neural network is considered. An
elegant (and short) proof of the states at which local/global minima of
quadratic form are attained is provided. A theorem associated with local/global
minimization of quadratic energy function using the Hopfield-Amari neural
network is discussed. The results are generalized to a ""Complex Hopfield neural
network"" dynamics over the complex hypercube (using a ""complex signum
function""). It is also reasoned through two theorems that there is no loss of
generality in assuming the threshold vector to be a zero vector in the case of
real as well as a ""Complex Hopfield neural network"". Some structured quadratic
forms like Toeplitz form and Complex Toeplitz form are discussed."
"In this paper, we study the performance of IPOP-saACM-ES and BIPOP-saACM-ES,
recently proposed self-adaptive surrogate-assisted Covariance Matrix Adaptation
Evolution Strategies. Both algorithms were tested using restarts till a total
number of function evaluations of $10^6D$ was reached, where $D$ is the
dimension of the function search space. We compared surrogate-assisted
algorithms with their surrogate-less versions IPOP-saACM-ES and BIPOP-saACM-ES,
two algorithms with one of the best overall performance observed during the
BBOB-2009 and BBOB-2010. The comparison shows that the surrogate-assisted
versions outperform the original CMA-ES algorithms by a factor from 2 to 4 on 8
out of 24 noiseless benchmark problems, showing the best results among all
algorithms of the BBOB-2009 and BBOB-2010 on Ellipsoid, Discus, Bent Cigar,
Sharp Ridge and Sum of different powers functions."
"Designing a fast and efficient optimization method with local optima
avoidance capability on a variety of optimization problems is still an open
problem for many researchers. In this work, the concept of a new global
optimization method with an open implementation area is introduced as a Curved
Space Optimization (CSO) method, which is a simple probabilistic optimization
method enhanced by concepts of general relativity theory. To address global
optimization challenges such as performance and convergence, this new method is
designed based on transformation of a random search space into a new search
space based on concepts of space-time curvature in general relativity theory.
In order to evaluate the performance of our proposed method, an implementation
of CSO is deployed and its results are compared on benchmark functions with
state-of-the art optimization methods. The results show that the performance of
CSO is promising on unimodal and multimodal benchmark functions with different
search space dimension sizes."
"The utilization of populations is one of the most important features of
evolutionary algorithms (EAs). There have been many studies analyzing the
impact of different population sizes on the performance of EAs. However, most
of such studies are based computational experiments, except for a few cases.
The common wisdom so far appears to be that a large population would increase
the population diversity and thus help an EA. Indeed, increasing the population
size has been a commonly used strategy in tuning an EA when it did not perform
as well as expected for a given problem. He and Yao (2002) showed theoretically
that for some problem instance classes, a population can help to reduce the
runtime of an EA from exponential to polynomial time. This paper analyzes the
role of population further in EAs and shows rigorously that large populations
may not always be useful. Conditions, under which large populations can be
harmful, are discussed in this paper. Although the theoretical analysis was
carried out on one multi-modal problem using a specific type of EAs, it has
much wider implications. The analysis has revealed certain problem
characteristics, which can be either the problem considered here or other
problems, that lead to the disadvantages of large population sizes. The
analytical approach developed in this paper can also be applied to analyzing
EAs on other problems."
"Very recently new genetic operators, called geometric semantic operators,
have been defined for genetic programming. Contrarily to standard genetic
operators, which are uniquely based on the syntax of the individuals, these new
operators are based on their semantics, meaning with it the set of input-output
pairs on training data. Furthermore, these operators present the interesting
property of inducing a unimodal fitness landscape for every problem that
consists in finding a match between given input and output data (for instance
regression and classification). Nevertheless, the current definition of these
operators has a serious limitation: they impose an exponential growth in the
size of the individuals in the population, so their use is impossible in
practice. This paper is intended to overcome this limitation, presenting a new
genetic programming system that implements geometric semantic operators in an
extremely efficient way. To demonstrate the power of the proposed system, we
use it to solve a complex real-life application in the field of
pharmacokinetic: the prediction of the human oral bioavailability of potential
new drugs. Besides the excellent performances on training data, which were
expected because the fitness landscape is unimodal, we also report an excellent
generalization ability of the proposed system, at least for the studied
application. In fact, it outperforms standard genetic programming and a wide
set of other well-known machine learning methods."
"An extension to a recently introduced binary neural network is proposed in
order to allow the learning of sparse messages, in large numbers and with high
memory efficiency. This new network is justified both in biological and
informational terms. The learning and retrieval rules are detailed and
illustrated by various simulation results."
"This paper explores fast, polynomial time heuristic approximate solutions to
the NP-hard problem of scheduling jobs on N identical machines. The jobs are
independent and are allowed to be stopped and restarted on another machine at a
later time. They have well-defined deadlines, and relative priorities
quantified by non-negative real weights. The objective is to find schedules
which minimize the total weighted tardiness (TWT) of all jobs. We show how this
problem can be mapped into quadratic form and present a polynomial time
heuristic solution based on the Hopfield Neural Network (HNN) approach. It is
demonstrated, through the results of extensive numerical simulations, that this
solution outperforms other popular heuristic methods. The proposed heuristic is
both theoretically and empirically shown to be scalable to large problem sizes
(over 100 jobs to be scheduled), which makes it applicable to grid computing
scheduling, arising in fields such as computational biology, chemistry and
finance."
"The global market for textile industry is highly competitive nowadays.
Quality control in production process in textile industry has been a key factor
for retaining existence in such competitive market. Automated textile
inspection systems are very useful in this respect, because manual inspection
is time consuming and not accurate enough. Hence, automated textile inspection
systems have been drawing plenty of attention of the researchers of different
countries in order to replace manual inspection. Defect detection and defect
classification are the two major problems that are posed by the research of
automated textile inspection systems. In this paper, we perform an extensive
investigation on the applicability of genetic algorithm (GA) in the context of
textile defect classification using neural network (NN). We observe the effect
of tuning different network parameters and explain the reasons. We empirically
find a suitable NN model in the context of textile defect classification. We
compare the performance of this model with that of the classification models
implemented by others."
"This short paper presents a work on the design of low noise microwave
amplifiers using particle swarm optimization (PSO) technique. Particle Swarm
Optimization is used as a method that is applied to a single stage amplifier
circuit to meet two criteria: desired gain and desired low noise. The aim is to
get the best optimized design using the predefined constraints for gain and low
noise values. The code is written to apply the algorithm to meet the desired
goals and the obtained results are verified using different simulators. The
results obtained show that PSO can be applied very efficiently for this kind of
design problems with multiple constraints."
"The problem of implementing a class of functions with particular conditions
by using monotonic multilayer functions is considered. A genetic algorithm is
used to create monotonic functions of a certain class, and these are
implemented with two-layer monotonic functions. The existence of a solution to
the given problem suggests that from two monotone functions, a monotonic
function with the same dimensions can be created. A new algorithm based on the
genetic algorithm is proposed, which easily implemented two-layer monotonic
functions of a specific class for up to six variables."
"This paper presents a new intelligent algorithm that can solve the problems
of finding the optimum solution in the state space among which the desired
solution resides. The algorithm mimics the principles of bat sonar in finding
its targets. The algorithm introduces three search approaches. The first search
approach considers a single sonar unit (SSU) with a fixed beam length and a
single starting point. In this approach, although the results converge toward
the optimum fitness, it is not guaranteed to find the global optimum solution
especially for complex problems; it is satisfied with finding 'acceptably good'
solutions to these problems. The second approach considers multisonar units
(MSU) working in parallel in the same state space. Each unit has its own
starting point and tries to find the optimum solution. In this approach the
probability that the algorithm converges toward the optimum solution is
significantly increased. It is found that this approach is suitable for complex
functions and for problems of wide state space. In the third approach, a single
sonar unit with a moment (SSM) is used in order to handle the problem of
convergence toward a local optimum rather than a global optimum. The momentum
term is added to the length of the transmitted beams. This will give the chance
to find the best fitness in a wider range within the state space. In this paper
a comparison between the proposed algorithm and genetic algorithm (GA) has been
made. It showed that both of the algorithms can catch approximately the optimum
solutions for all of the testbed functions except for the function that has a
local minimum, in which the proposed algorithm's result is much better than
that of the GA algorithm. On the other hand, the comparison showed that the
required execution time to obtain the optimum solution using the proposed
algorithm is much less than that of the GA algorithm."
"In the field of empirical modeling using Genetic Programming (GP), it is
important to evolve solution with good generalization ability. Generalization
ability of GP solutions get affected by two important issues: bloat and
over-fitting. We surveyed and classified existing literature related to
different techniques used by GP research community to deal with these issues.
We also point out limitation of these techniques, if any. Moreover, the
classification of different bloat control approaches and measures for bloat and
over-fitting are also discussed. We believe that this work will be useful to GP
practitioners in following ways: (i) to better understand concepts of
generalization in GP (ii) comparing existing bloat and over-fitting control
techniques and (iii) selecting appropriate approach to improve generalization
ability of GP evolved solutions."
"In this paper, the goal is to achieve an ultra low sidelobe level (SLL) and
sideband levels (SBL) of a time modulated linear antenna array. The approach
followed here is not to give fixed level of excitation to the elements of an
array, but to change it dynamically with time. The excitation levels of the
different array elements over time are varied to get the low sidelobe and
sideband levels. The mathematics of getting the SLL and SBL furnished in detail
and simulation is done using the mathematical results. The excitation pattern
over time is optimized using Genetic Algorithm (GA). Since, the amplitudes of
the excitations of the elements are varied within a finite limit, results show
it gives better sidelobe and sideband suppression compared to previous time
modulated arrays with uniform amplitude excitations."
"In this paper, we present a heuristic for designing facility layouts that are
convenient for designing a unidirectional loop for material handling. We use
genetic algorithm where the objective function and crossover and mutation
operators have all been designed specifically for this purpose. Our design is
made under flexible bay structure and comparisons are made with other layouts
from the literature that were designed under flexible bay structure."
"This paper considers the problem of information capacity of a random neural
network. The network is represented by matrices that are square and
symmetrical. The matrices have a weight which determines the highest and lowest
possible value found in the matrix. The examined matrices are randomly
generated and analyzed by a computer program. We find the surprising result
that the capacity of the network is a maximum for the binary random neural
network and it does not change as the number of quantization levels associated
with the weights increases."
"Particle swarm optimization is a popular method for solving difficult
optimization problems. There have been attempts to formulate the method in
formal probabilistic or stochastic terms (e.g. bare bones particle swarm) with
the aim to achieve more generality and explain the practical behavior of the
method. Here we present a Bayesian interpretation of the particle swarm
optimization. This interpretation provides a formal framework for incorporation
of prior knowledge about the problem that is being solved. Furthermore, it also
allows to extend the particle optimization method through the use of kernel
functions that represent the intermediary transformation of the data into a
different space where the optimization problem is expected to be easier to be
resolved, such transformation can be seen as a form of prior knowledge about
the nature of the optimization problem. We derive from the general Bayesian
formulation the commonly used particle swarm methods as particular cases."
"Cyclic patterns of neuronal activity are ubiquitous in animal nervous
systems, and partially responsible for generating and controlling rhythmic
movements such as locomotion, respiration, swallowing and so on. Clarifying the
role of the network connectivities for generating cyclic patterns is
fundamental for understanding the generation of rhythmic movements. In this
paper, the storage of binary cycles in neural networks is investigated. We call
a cycle $\Sigma$ admissible if a connectivity matrix satisfying the cycle's
transition conditions exists, and construct it using the pseudoinverse learning
rule. Our main focus is on the structural features of admissible cycles and
corresponding network topology. We show that $\Sigma$ is admissible if and only
if its discrete Fourier transform contains exactly $r={rank}(\Sigma)$ nonzero
columns. Based on the decomposition of the rows of $\Sigma$ into loops, where a
loop is the set of all cyclic permutations of a row, cycles are classified as
simple cycles, separable or inseparable composite cycles. Simple cycles contain
rows from one loop only, and the network topology is a feedforward chain with
feedback to one neuron if the loop-vectors in $\Sigma$ are cyclic permutations
of each other. Composite cycles contain rows from at least two disjoint loops,
and the neurons corresponding to the rows in $\Sigma$ from the same loop are
identified with a cluster. Networks constructed from separable composite cycles
decompose into completely isolated clusters. For inseparable composite cycles
at least two clusters are connected, and the cluster-connectivity is related to
the intersections of the spaces spanned by the loop-vectors of the clusters.
Simulations showing successfully retrieved cycles in continuous-time
Hopfield-type networks and in networks of spiking neurons are presented."
"Bio-Inspired computing is the subset of Nature-Inspired computing. Job Shop
Scheduling Problem is categorized under popular scheduling problems. In this
research work, Bacterial Foraging Optimization was hybridized with Ant Colony
Optimization and a new technique Hybrid Bacterial Foraging Optimization for
solving Job Shop Scheduling Problem was proposed. The optimal solutions
obtained by proposed Hybrid Bacterial Foraging Optimization algorithms are much
better when compared with the solutions obtained by Bacterial Foraging
Optimization algorithm for well-known test problems of different sizes. From
the implementation of this research work, it could be observed that the
proposed Hybrid Bacterial Foraging Optimization was effective than Bacterial
Foraging Optimization algorithm in solving Job Shop Scheduling Problems. Hybrid
Bacterial Foraging Optimization is used to implement real world Job Shop
Scheduling Problems."
"We create a novel optimisation technique inspired by natural ecosystems,
where the optimisation works at two levels: a first optimisation, migration of
genes which are distributed in a peer-to-peer network, operating continuously
in time; this process feeds a second optimisation based on evolutionary
computing that operates locally on single peers and is aimed at finding
solutions to satisfy locally relevant constraints. We consider from the domain
of computer science distributed evolutionary computing, with the relevant
theory from the domain of theoretical biology, including the fields of
evolutionary and ecological theory, the topological structure of ecosystems,
and evolutionary processes within distributed environments. We then define
ecosystem- oriented distributed evolutionary computing, imbibed with the
properties of self-organisation, scalability and sustainability from natural
ecosystems, including a novel form of distributed evolu- tionary computing.
Finally, we conclude with a discussion of the apparent compromises resulting
from the hybrid model created, such as the network topology."
"This erratum points out an error in the simplified drift theorem (SDT)
[Algorithmica 59(3), 369-386, 2011]. It is also shown that a minor modification
of one of its conditions is sufficient to establish a valid result. In many
respects, the new theorem is more general than before. We no longer assume a
Markov process nor a finite search space. Furthermore, the proof of the theorem
is more compact than the previous ones. Finally, previous applications of the
SDT are revisited. It turns out that all of these either meet the modified
condition directly or by means of few additional arguments."
"Differential evolution possesses a multitude of various strategies for
generating new trial solutions. Unfortunately, the best strategy is not known
in advance. Moreover, this strategy usually depends on the problem to be
solved. This paper suggests using various regression methods (like random
forest, extremely randomized trees, gradient boosting, decision trees, and a
generalized linear model) on ensemble strategies in differential evolution
algorithm by predicting the best differential evolution strategy during the
run. Comparing the preliminary results of this algorithm by optimizing a suite
of five well-known functions from literature, it was shown that using the
random forest regression method substantially outperformed the results of the
other regression methods."
"Drift analysis is one of the state-of-the-art techniques for the runtime
analysis of randomized search heuristics (RSHs) such as evolutionary algorithms
(EAs), simulated annealing etc. The vast majority of existing drift theorems
yield bounds on the expected value of the hitting time for a target state,
e.g., the set of optimal solutions, without making additional statements on the
distribution of this time. We address this lack by providing a general drift
theorem that includes bounds on the upper and lower tail of the hitting time
distribution. The new tail bounds are applied to prove very precise
sharp-concentration results on the running time of a simple EA on standard
benchmark problems, including the class of general linear functions.
Surprisingly, the probability of deviating by an $r$-factor in lower order
terms of the expected time decreases exponentially with $r$ on all these
problems. The usefulness of the theorem outside the theory of RSHs is
demonstrated by deriving tail bounds on the number of cycles in random
permutations. All these results handle a position-dependent (variable) drift
that was not covered by previous drift theorems with tail bounds. Moreover, our
theorem can be specialized into virtually all existing drift theorems with
drift towards the target from the literature. Finally, user-friendly
specializations of the general drift theorem are given."
"Sufficient conditions are found under which the iterated non-elitist genetic
algorithm with tournament selection first visits a local optimum in
polynomially bounded time on average. It is shown that these conditions are
satisfied on a class of problems with guaranteed local optima (GLO) if
appropriate parameters of the algorithm are chosen."
"Swarm intelligence and bio-inspired algorithms form a hot topic in the
developments of new algorithms inspired by nature. These nature-inspired
metaheuristic algorithms can be based on swarm intelligence, biological
systems, physical and chemical systems. Therefore, these algorithms can be
called swarm-intelligence-based, bio-inspired, physics-based and
chemistry-based, depending on the sources of inspiration. Though not all of
them are efficient, a few algorithms have proved to be very efficient and thus
have become popular tools for solving real-world problems. Some algorithms are
insufficiently studied. The purpose of this review is to present a relatively
comprehensive list of all the algorithms in the literature, so as to inspire
further research."
"The fitness-level method, also called the method of f-based partitions, is an
intuitive and widely used technique for the running time analysis of randomized
search heuristics. It was originally defined to prove upper and lower bounds on
the expected running time. Recently, upper tail bounds were added to the
technique; however, these tail bounds only apply to running times that are at
least twice as large as the expectation.
  We remove this restriction and supplement the fitness-level method with sharp
tail bounds, including lower tails. As an exemplary application, we prove that
the running time of randomized local search on OneMax is sharply concentrated
around n ln n - 0.1159 n."
"Nowadays swarm intelligence-based algorithms are being used widely to
optimize the dynamic traveling salesman problem (DTSP). In this paper, we have
used mixed method of Ant Colony Optimization (AOC)and gradient descent to
optimize DTSP which differs with ACO algorithm in evaporation rate and
innovative data. This approach prevents premature convergence and scape from
local optimum spots and also makes it possible to find better solutions for
algorithm. In this paper, we are going to offer gradient descent and ACO
algorithm which in comparison to some former methods it shows that algorithm
has significantly improved routes optimization."
"This paper examines the memory capacity of generalized neural networks.
Hopfield networks trained with a variety of learning techniques are
investigated for their capacity both for binary and non-binary alphabets. It is
shown that the capacity can be much increased when multilevel inputs are used.
New learning strategies are proposed to increase Hopfield network capacity, and
the scalability of these methods is also examined in respect to size of the
network. The ability to recall entire patterns from stimulation of a single
neuron is examined for the increased capacity networks."
"We investigate fundamental decisions in the design of instruction set
architectures for linear genetic programs that are used as both model systems
in evolutionary biology and underlying solution representations in evolutionary
computation. We subjected digital organisms with each tested architecture to
seven different computational environments designed to present a range of
evolutionary challenges. Our goal was to engineer a general purpose
architecture that would be effective under a broad range of evolutionary
conditions. We evaluated six different types of architectural features for the
virtual CPUs: (1) genetic flexibility: we allowed digital organisms to more
precisely modify the function of genetic instructions, (2) memory: we provided
an increased number of registers in the virtual CPUs, (3) decoupled sensors and
actuators: we separated input and output operations to enable greater control
over data flow. We also tested a variety of methods to regulate expression: (4)
explicit labels that allow programs to dynamically refer to specific genome
positions, (5) position-relative search instructions, and (6) multiple new flow
control instructions, including conditionals and jumps. Each of these features
also adds complication to the instruction set and risks slowing evolution due
to epistatic interactions. Two features (multiple argument specification and
separated I/O) demonstrated substantial improvements int the majority of test
environments. Some of the remaining tested modifications were detrimental,
thought most exhibit no systematic effects on evolutionary potential,
highlighting the robustness of digital evolution. Combined, these observations
enhance our understanding of how instruction architecture impacts evolutionary
potential, enabling the creation of architectures that support more rapid
evolution of complex solutions to a broad range of challenges."
"In Class-D Power Amplifiers (CDPAs), the power supply noise can intermodulate
with the input signal, manifesting into power-supply induced intermodulation
distortion (PS-IMD) and due to the memory effects of the system, there exist
asymmetries in the PS-IMDs. In this paper, a new behavioral modeling based on
the Elman Wavelet Neural Network (EWNN) is proposed to study the nonlinear
distortion of the CDPAs. In EWNN model, the Morlet wavelet functions are
employed as the activation function and there is a normalized operation in the
hidden layer, the modification of the scale factor and translation factor in
the wavelet functions are ignored to avoid the fluctuations of the error
curves. When there are 30 neurons in the hidden layer, to achieve the same
square sum error (SSE) $\epsilon_{min}=10^{-3}$, EWNN needs 31 iteration steps,
while the basic Elman neural network (BENN) model needs 86 steps. The
Volterra-Laguerre model has 605 parameters to be estimated but still can't
achieve the same magnitude accuracy of EWNN. Simulation results show that the
proposed approach of EWNN model has fewer parameters and higher accuracy than
the Volterra-Laguerre model and its convergence rate is much faster than the
BENN model."
"Many optimization problems arising in applications have to consider several
objective functions at the same time. Evolutionary algorithms seem to be a very
natural choice for dealing with multi-objective problems as the population of
such an algorithm can be used to represent the trade-offs with respect to the
given objective functions. In this paper, we contribute to the theoretical
understanding of evolutionary algorithms for multi-objective problems. We
consider indicator-based algorithms whose goal is to maximize the hypervolume
for a given problem by distributing {\mu} points on the Pareto front. To gain
new theoretical insights into the behavior of hypervolume-based algorithms we
compare their optimization goal to the goal of achieving an optimal
multiplicative approximation ratio. Our studies are carried out for different
Pareto front shapes of bi-objective problems. For the class of linear fronts
and a class of convex fronts, we prove that maximizing the hypervolume gives
the best possible approximation ratio when assuming that the extreme points
have to be included in both distributions of the points on the Pareto front.
Furthermore, we investigate the choice of the reference point on the
approximation behavior of hypervolume-based approaches and examine Pareto
fronts of different shapes by numerical calculations."
"Inspired by the importance of both communication and feedback on errors in
human learning, our main goal was to implement a similar mechanism in
supervised learning of artificial neural networks. The starting point in our
study was the observation that words should accompany the input vectors
included in the training set, thus extending the ANN input space. This had as
consequence the necessity to take into consideration a modified sigmoid
activation function for neurons in the first hidden layer (in agreement with a
specific MLP apartment structure), and also a modified version of the
Backpropagation algorithm, which allows using of unspecified (null) desired
output components. Following the belief that basic concepts should be tested on
simple examples, the previous mentioned mechanism was applied on both the XOR
problem and a didactic color case study. In this context, we noticed the
interesting fact that the ANN was capable to categorize all desired input
vectors in the absence of their corresponding words, even though the training
set included only word accompanied inputs, in both positive and negative
examples. Further analysis along applying this approach to more complex
scenarios is currently in progress, as we consider the proposed language-driven
algorithm might contribute to a better understanding of learning in humans,
opening as well the possibility to create a specific category of artificial
neural networks, with abstraction capabilities."
"Genetic programming is a powerful heuristic search technique that is used for
a number of real world applications to solve among others regression,
classification, and time-series forecasting problems. A lot of progress towards
a theoretic description of genetic programming in form of schema theorems has
been made, but the internal dynamics and success factors of genetic programming
are still not fully understood. In particular, the effects of different
crossover operators in combination with offspring selection are largely
unknown.
  This contribution sheds light on the ability of well-known GP crossover
operators to create better offspring when applied to benchmark problems. We
conclude that standard (sub-tree swapping) crossover is a good default choice
in combination with offspring selection, and that GP with offspring selection
and random selection of crossover operators can improve the performance of the
algorithm in terms of best solution quality when no solution size constraints
are applied."
"In this paper a data mining approach for variable selection and knowledge
extraction from datasets is presented. The approach is based on unguided
symbolic regression (every variable present in the dataset is treated as the
target variable in multiple regression runs) and a novel variable relevance
metric for genetic programming. The relevance of each input variable is
calculated and a model approximating the target variable is created. The
genetic programming configurations with different target variables are executed
multiple times to reduce stochastic effects and the aggregated results are
displayed as a variable interaction network. This interaction network
highlights important system components and implicit relations between the
variables. The whole approach is tested on a blast furnace dataset, because of
the complexity of the blast furnace and the many interrelations between the
variables. Finally the achieved results are discussed with respect to existing
knowledge about the blast furnace process."
"This paper deals with a method for solving Poisson Equation (PE) based on
genetic algorithms and grammatical evolution. The method forms generations of
solutions expressed in an analytical form. Several examples of PE are tested
and in most cases the exact solution is recovered. But, when the solution
cannot be expressed in an analytical form, our method produces a satisfactory
solution with a good level of accuracy"
"Particle swam optimization (PSO) is a popular stochastic optimization method
that has found wide applications in diverse fields. However, PSO suffers from
high computational complexity and slow convergence speed. High computational
complexity hinders its use in applications that have limited power resources
while slow convergence speed makes it unsuitable for time critical
applications. In this paper, we propose two techniques to overcome these
limitations. The first technique reduces the computational complexity of PSO
while the second technique speeds up its convergence. These techniques can be
applied, either separately or in conjunction, to any existing PSO variant. The
proposed techniques are robust to the number of dimensions of the optimization
problem. Simulation results are presented for the proposed techniques applied
to the standard PSO as well as to several PSO variants. The results show that
the use of both these techniques in conjunction results in a reduction in the
number of computations required as well as faster convergence speed while
maintaining an acceptable error performance for time-critical applications."
"The swarm intelligence of animals is a natural paradigm to apply to
optimization problems. Ant colony, bee colony, firefly and bat algorithms are
amongst those that have been demonstrated to efficiently to optimize complex
constraints. This paper proposes the new Sparkling Squid Algorithm (SSA) for
multimodal optimization, inspired by the intelligent swarm behavior of its
namesake. After an introduction, formulation and discussion of its
implementation, it will be compared to other popular metaheuristics. Finally,
applications to well - known problems such as image registration and the
traveling salesperson problem will be discussed."
"The concept of cognitive radio pioneered by Mitola promises to change the
future of wireless communication especially in the area of spectrum management.
Currently, the command and control strategy employed in spectrum assignment is
too rigid and needs to be reviewed. Recent studies have shown that assigned
spectrum is underutilized spectrally and temporally. Cognitive radio provides a
viable solution whereby licensed users can share the spectrum with unlicensed
users opportunistically without causing interference. Unlicensed users must be
able to sense weather the channel is busy or idle, failure to do so will lead
to interference to the licensed user. In this paper, a neural network based
prediction model for predicting the channel status using historical data
obtained during a spectrum occupancy measurement is presented. Genetic
algorithm is combined with LM BP for increasing the probability of obtaining
the best weights thus optimizing the network. The results obtained indicate
high prediction accuracy over all bands considered"
"Although real-coded differential evolution (DE) algorithms can perform well
on continuous optimization problems (CoOPs), it is still a challenging task to
design an efficient binary-coded DE algorithm. Inspired by the learning
mechanism of particle swarm optimization (PSO) algorithms, we propose a binary
learning differential evolution (BLDE) algorithm that can efficiently locate
the global optimal solutions by learning from the last population. Then, we
theoretically prove the global convergence of BLDE, and compare it with some
existing binary-coded evolutionary algorithms (EAs) via numerical experiments.
Numerical results show that BLDE is competitive to the compared EAs, and
meanwhile, further study is performed via the change curves of a renewal metric
and a refinement metric to investigate why BLDE cannot outperform some compared
EAs for several selected benchmark problems. Finally, we employ BLDE solving
the unit commitment problem (UCP) in power systems to show its applicability in
practical problems."
"Recent studies have shown the classification and prediction power of the
Neural Networks. It has been demonstrated that a NN can approximate any
continuous function. Neural networks have been successfully used for
forecasting of financial data series. The classical methods used for time
series prediction like Box-Jenkins or ARIMA assumes that there is a linear
relationship between inputs and outputs. Neural Networks have the advantage
that can approximate nonlinear functions. In this paper we compared the
performances of different feed forward and recurrent neural networks and
training algorithms for predicting the exchange rate EUR/RON and USD/RON. We
used data series with daily exchange rates starting from 2005 until 2013."
"Bi-level optimisation problems have gained increasing interest in the field
of combinatorial optimisation in recent years. With this paper, we start the
runtime analysis of evolutionary algorithms for bi-level optimisation problems.
We examine two NP-hard problems, the generalised minimum spanning tree problem
(GMST), and the generalised travelling salesman problem (GTSP) in the context
of parameterised complexity.
  For the generalised minimum spanning tree problem, we analyse the two
approaches presented by Hu and Raidl (2012) with respect to the number of
clusters that distinguish each other by the chosen representation of possible
solutions. Our results show that a (1+1) EA working with the spanning nodes
representation is not a fixed-parameter evolutionary algorithm for the problem,
whereas the global structure representation enables to solve the problem in
fixed-parameter time. We present hard instances for each approach and show that
the two approaches are highly complementary by proving that they solve each
other's hard instances very efficiently.
  For the generalised travelling salesman problem, we analyse the problem with
respect to the number of clusters in the problem instance. Our results show
that a (1+1) EA working with the global structure representation is a
fixed-parameter evolutionary algorithm for the problem."
"We present the N2Sky system, which provides a framework for the exchange of
neural network specific knowledge, as neural network paradigms and objects, by
a virtual organization environment. It follows the sky computing paradigm
delivering ample resources by the usage of federated Clouds. N2Sky is a novel
Cloud-based neural network simulation environment, which follows a pure service
oriented approach. The system implements a transparent environment aiming to
enable both novice and experienced users to do neural network research easily
and comfortably. N2Sky is built using the RAVO reference architecture of
virtual organizations which allows itself naturally integrating into the Cloud
service stack (SaaS, PaaS, and IaaS) of service oriented architectures."
"The purpose of this paper is to give an introduction to the field of Schema
Theory written by a mathematician and for mathematicians. In particular, we
endeavor to to highlight areas of the field which might be of interest to a
mathematician, to point out some related open problems, and to suggest some
large-scale projects. Schema theory seeks to give a theoretical justification
for the efficacy of the field of genetic algorithms, so readers who have
studied genetic algorithms stand to gain the most from this paper. However,
nothing beyond basic probability theory is assumed of the reader, and for this
reason we write in a fairly informal style.
  Because the mathematics behind the theorems in schema theory is relatively
elementary, we focus more on the motivation and philosophy. Many of these
results have been proven elsewhere, so this paper is designed to serve a
primarily expository role. We attempt to cast known results in a new light,
which makes the suggested future directions natural. This involves devoting a
substantial amount of time to the history of the field.
  We hope that this exposition will entice some mathematicians to do research
in this area, that it will serve as a road map for researchers new to the
field, and that it will help explain how schema theory developed. Furthermore,
we hope that the results collected in this document will serve as a useful
reference. Finally, as far as the author knows, the questions raised in the
final section are new."
"Population-based search algorithms (PBSAs), including swarm intelligence
algorithms (SIAs) and evolutionary algorithms (EAs), are competitive
alternatives for solving complex optimization problems and they have been
widely applied to real-world optimization problems in different fields. In this
study, a novel population-based across neighbourhood search (ANS) is proposed
for numerical optimization. ANS is motivated by two straightforward assumptions
and three important issues raised in improving and designing efficient PBSAs.
In ANS, a group of individuals collaboratively search the solution space for an
optimal solution of the optimization problem considered. A collection of
superior solutions found by individuals so far is maintained and updated
dynamically. At each generation, an individual directly searches across the
neighbourhoods of multiple superior solutions with the guidance of a Gaussian
distribution. This search manner is referred to as across neighbourhood search.
The characteristics of ANS are discussed and the concept comparisons with other
PBSAs are given. The principle behind ANS is simple. Moreover, ANS is easy for
implementation and application with three parameters being required to tune.
Extensive experiments on 18 benchmark optimization functions of different types
show that ANS has well balanced exploration and exploitation capabilities and
performs competitively compared with many efficient PBSAs (Related Matlab codes
used in the experiments are available from
http://guohuawunudt.gotoip2.com/publications.html)."
"The current work describes an empirical study conducted in order to
investigate the behavior of an optimization method in a fuzzy environment.
MAX-MIN Ant System, an efficient implementation of a heuristic method is used
for solving an optimization problem derived from the Traveling Salesman Problem
(TSP). Several publicly-available symmetric TSP instances and their fuzzy
variants are tested in order to extract some general features. The entry data
was adapted by introducing a two-dimensional systematic degree of fuzziness,
proportional with the number of nodes, the dimension of the instance and also
with the distances between nodes, the scale of the instance. The results show
that our proposed method can handle the data uncertainty, showing good
resilience and adaptability."
"In this paper, we apply genetic algorithms to the field of electoral studies.
Forecasting election results is one of the most exciting and demanding tasks in
the area of market research, especially due to the fact that decisions have to
be made within seconds on live television. We show that the proposed method
outperforms currently applied approaches and thereby provide an argument to
tighten the intersection between computer science and social science,
especially political science, further. We scrutinize the performance of our
algorithm's runtime behavior to evaluate its applicability in the field.
Numerical results with real data from a local election in the Austrian province
of Styria from 2010 substantiate the applicability of the proposed approach."
"Optimizing decision problems under uncertainty can be done using a variety of
solution methods. Soft computing and heuristic approaches tend to be powerful
for solving such problems. In this overview article, we survey Evolutionary
Optimization techniques to solve Stochastic Programming problems - both for the
single-stage and multi-stage case."
"The optimization of dynamic problems is both widespread and difficult. When
conducting dynamic optimization, a balance between reinitialization and
computational expense has to be found. There are multiple approaches to this.
In parallel genetic algorithms, multiple sub-populations concurrently try to
optimize a potentially dynamic problem. But as the number of sub-population
increases, their efficiency decreases. Cultural algorithms provide a framework
that has the potential to make optimizations more efficient. But they adapt
slowly to changing environments. We thus suggest a confluence of these
approaches: revolutionary algorithms. These algorithms seek to extend the
evolutionary and cultural aspects of the former to approaches with a notion of
the political. By modeling how belief systems are changed by means of
revolution, these algorithms provide a framework to model and optimize dynamic
problems in an efficient fashion."
"In land surveying, the generation of maps was greatly simplified with the
introduction of orthophotos and at a later stage with airborne LiDAR laser
scanning systems. While the original purpose of LiDAR systems was to determine
the altitude of ground elevations, newer full wave systems provide additional
information that can be used on classifying the type of ground cover and the
generation of maps. The LiDAR resulting point clouds are huge, multidimensional
data sets that need to be grouped in classes of ground cover. We propose a
genetic algorithm that aids in classifying these data sets and thus make them
usable for map generation. A key feature are tailor-made genetic operators and
fitness functions for the subject. The algorithm is compared to a traditional
k-means clustering."
"Genetic algorithms are considered as one of the most efficient search
techniques. Although they do not offer an optimal solution, their ability to
reach a suitable solution in considerably short time gives them their
respectable role in many AI techniques. This work introduces genetic algorithms
and describes their characteristics. Then a novel method using genetic
algorithm in best training set generation and selection for a back-propagation
network is proposed. This work also offers a new extension to the original
genetic algorithms"
"The large number of exact fitness function evaluations makes evolutionary
algorithms to have computational cost. In some real-world problems, reducing
number of these evaluations is much more valuable even by increasing
computational complexity and spending more time. To fulfill this target, we
introduce an effective factor, in spite of applied factor in Adaptive Fuzzy
Fitness Granulation with Non-dominated Sorting Genetic Algorithm-II, to filter
out worthless individuals more precisely. Our proposed approach is compared
with respect to Adaptive Fuzzy Fitness Granulation with Non-dominated Sorting
Genetic Algorithm-II, using the Hyper volume and the Inverted Generational
Distance performance measures. The proposed method is applied to 1 traditional
and 1 state-of-the-art benchmarks with considering 3 different dimensions. From
an average performance view, the results indicate that although decreasing the
number of fitness evaluations leads to have performance reduction but it is not
tangible compared to what we gain."
"Functional Data Analysis (FDA) is an extension of traditional data analysis
to functional data, for example spectra, temporal series, spatio-temporal
images, gesture recognition data, etc. Functional data are rarely known in
practice; usually a regular or irregular sampling is known. For this reason,
some processing is needed in order to benefit from the smooth character of
functional data in the analysis methods. This paper shows how to extend the
Radial-Basis Function Networks (RBFN) and Multi-Layer Perceptron (MLP) models
to functional data inputs, in particular when the latter are known through
lists of input-output pairs. Various possibilities for functional processing
are discussed, including the projection on smooth bases, Functional Principal
Component Analysis, functional centering and reduction, and the use of
differential operators. It is shown how to incorporate these functional
processing into the RBFN and MLP models. The functional approach is illustrated
on a benchmark of spectrometric data analysis."
"In this paper, we study a natural extension of Multi-Layer Perceptrons (MLP)
to functional inputs. We show that fundamental results for classical MLP can be
extended to functional MLP. We obtain universal approximation results that show
the expressive power of functional MLP is comparable to that of numerical MLP.
We obtain consistency results which imply that the estimation of optimal
parameters for functional MLP is statistically well defined. We finally show on
simulated and real world data that the proposed model performs in a very
satisfactory way."
"This effort examines the intersection of the emerging field of quantum
computing and the more established field of evolutionary computation. The goal
is to understand what benefits quantum computing might offer to computational
intelligence and how computational intelligence paradigms might be implemented
as quantum programs to be run on a future quantum computer. We critically
examine proposed algorithms and methods for implementing computational
intelligence paradigms, primarily focused on heuristic optimization methods
including and related to evolutionary computation, with particular regard for
their potential for eventual implementation on quantum computing hardware."
"Recently, a new method for encoding data sets in the form of ""Density Codes""
was proposed in the literature (Courrieu, 2006). This method allows to compare
sets of points belonging to every multidimensional space, and to build shape
spaces invariant to a wide variety of affine and non-affine transformations.
However, this general method does not take advantage of the special properties
of image data, resulting in a quite slow encoding process that makes this tool
practically unusable for processing large image databases with conventional
computers. This paper proposes a very simple variant of the density code method
that directly works on the image function, which is thousands times faster than
the original Parzen window based method, without loss of its useful properties."
"The solving of least square systems is a useful operation in
neurocomputational modeling of learning, pattern matching, and pattern
recognition. In these last two cases, the solution must be obtained on-line,
thus the time required to solve a system in a plausible neural architecture is
critical. This paper presents a recurrent network of Sigma-Pi neurons, whose
solving time increases at most like the logarithm of the system size, and of
its condition number, which provides plausible computation times for biological
systems."
"Many neural learning algorithms require to solve large least square systems
in order to obtain synaptic weights. Moore-Penrose inverse matrices allow for
solving such systems, even with rank deficiency, and they provide minimum-norm
vectors of synaptic weights, which contribute to the regularization of the
input-output mapping. It is thus of interest to develop fast and accurate
algorithms for computing Moore-Penrose inverse matrices. In this paper, an
algorithm based on a full rank Cholesky factorization is proposed. The
resulting pseudoinverse matrices are similar to those provided by other
algorithms. However the computation time is substantially shorter, particularly
for large systems."
"We combine a refined version of two-point step-size adaptation with the
covariance matrix adaptation evolution strategy (CMA-ES). Additionally, we
suggest polished formulae for the learning rate of the covariance matrix and
the recombination weights. In contrast to cumulative step-size adaptation or to
the 1/5-th success rule, the refined two-point adaptation (TPA) does not rely
on any internal model of optimality. In contrast to conventional
self-adaptation, the TPA will achieve a better target step-size in particular
with large populations. The disadvantage of TPA is that it relies on two
additional objective function"
"In this paper the Sudoku problem is solved using stochastic search techniques
and these are: Cultural Genetic Algorithm (CGA), Repulsive Particle Swarm
Optimization (RPSO), Quantum Simulated Annealing (QSA) and the Hybrid method
that combines Genetic Algorithm with Simulated Annealing (HGASA). The results
obtained show that the CGA, QSA and HGASA are able to solve the Sudoku puzzle
with CGA finding a solution in 28 seconds, while QSA finding a solution in 65
seconds and HGASA in 1.447 seconds. This is mainly because HGASA combines the
parallel searching of GA with the flexibility of SA. The RPSO was found to be
unable to solve the puzzle."
"Although there are many neural network (NN) algorithms for prediction and for
control, and although methods for optimal estimation (including filtering and
prediction) and for optimal control in linear systems were provided by Kalman
in 1960 (with nonlinear extensions since then), there has been, to my
knowledge, no NN algorithm that learns either Kalman prediction or Kalman
control (apart from the special case of stationary control). Here we show how
optimal Kalman prediction and control (KPC), as well as system identification,
can be learned and executed by a recurrent neural network composed of
linear-response nodes, using as input only a stream of noisy measurement data.
  The requirements of KPC appear to impose significant constraints on the
allowed NN circuitry and signal flows. The NN architecture implied by these
constraints bears certain resemblances to the local-circuit architecture of
mammalian cerebral cortex. We discuss these resemblances, as well as caveats
that limit our current ability to draw inferences for biological function. It
has been suggested that the local cortical circuit (LCC) architecture may
perform core functions (as yet unknown) that underlie sensory, motor,and other
cortical processing. It is reasonable to conjecture that such functions may
include prediction, the estimation or inference of missing or noisy sensory
data, and the goal-driven generation of control signals. The resemblances found
between the KPC NN architecture and that of the LCC are consistent with this
conjecture."
"A computationally method on damage detection problems in structures was
conducted using neural networks. The problem that is considered in this works
consists of estimating the existence, location and extent of stiffness
reduction in structure which is indicated by the changes of the structural
static parameters such as deflection and strain. The neural network was trained
to recognize the behaviour of static parameter of the undamaged structure as
well as of the structure with various possible damage extent and location which
were modelled as random states. The proposed techniques were applied to detect
damage in a simply supported beam. The structure was analyzed using
finite-element-method (FEM) and the damage identification was conducted by a
back-propagation neural network using the change of the structural strain and
displacement. The results showed that using proposed method the strain is more
efficient for identification of damage than the displacement."
"Traditional Genetic Algorithms (GAs) mating schemes select individuals for
crossover independently of their genotypic or phenotypic similarities. In
Nature, this behaviour is known as random mating. However, non-random schemes -
in which individuals mate according to their kinship or likeness - are more
common in natural systems. Previous studies indicate that, when applied to GAs,
negative assortative mating (a specific type of non-random mating, also known
as dissortative mating) may improve their performance (on both speed and
reliability) in a wide range of problems. Dissortative mating maintains the
genetic diversity at a higher level during the run, and that fact is frequently
observed as an explanation for dissortative GAs ability to escape local optima
traps. Dynamic problems, due to their specificities, demand special care when
tuning a GA, because diversity plays an even more crucial role than it does
when tackling static ones. This paper investigates the behaviour of
dissortative mating GAs, namely the recently proposed Adaptive Dissortative
Mating GA (ADMGA), on dynamic trap functions. ADMGA selects parents according
to their Hamming distance, via a self-adjustable threshold value. The method,
by keeping population diversity during the run, provides an effective means to
deal with dynamic problems. Tests conducted with deceptive and nearly deceptive
trap functions indicate that ADMGA is able to outperform other GAs, some
specifically designed for tracking moving extrema, on a wide range of tests,
being particularly effective when speed of change is not very fast. When
comparing the algorithm to a previously proposed dissortative GA, results show
that performance is equivalent on the majority of the experiments, but ADMGA
performs better when solving the hardest instances of the test set."
"The goal of this paper is to present the implementation of a Radial Basis
Function neural network with built-in knowledge to recognize hand-written
characters. The neural network includes in its architecture gates controlled by
an attraction/repulsion system of coefficients. These coefficients are derived
from a preprocessing stage which groups the characters according to their
ascendant, central, or descendent components. The neural network is trained
using data from invariant moment functions. Results are compared with those
obtained using a K nearest neighbor method on the same moment data."
"An immune system inspired Artificial Immune System (AIS) algorithm is
presented, and is used for the purposes of automated program verification.
Relevant immunological concepts are discussed and the field of AIS is briefly
reviewed. It is proposed to use this AIS algorithm for a specific automated
program verification task: that of predicting shape of program invariants. It
is shown that the algorithm correctly predicts program invariant shape for a
variety of benchmarked programs."
"This paper extends the treatment of single-neuron memories obtained by the
B-matrix approach. The spreading of the activity within the network is
determined by the network's proximity matrix which represents the separations
amongst the neurons through the neural pathways."
"Simulation is useful for the evaluation of a Master Production/distribution
Schedule (MPS). Also, the goal of this paper is the study of the design of a
simulation model by reducing its complexity. According to theory of
constraints, we want to build reduced models composed exclusively by
bottlenecks and a neural network. Particularly a multilayer perceptron, is
used. The structure of the network is determined by using a pruning procedure.
This work focuses on the impact of discrete data on the results and compares
different approaches to deal with these data. This approach is applied to
sawmill internal supply chain"
"The design and the implementation of a genetic algorithm are described. The
applicability domain is on structure-activity relationships expressed as
multiple linear regressions and predictor variables are from families of
structure-based molecular descriptors. An experiment to compare different
selection and survival strategies was designed and realized. The genetic
algorithm was run using the designed experiment on a set of 206 polychlorinated
biphenyls searching on structure-activity relationships having known the
measured octanol-water partition coefficients and a family of molecular
descriptors. The experiment shows that different selection and survival
strategies create different partitions on the entire population of all possible
genotypes."
"Algorithm::Evolutionary (A::E from now on) was introduced in 2002, after a
talk in YAPC::EU in Munich. 7 years later, A::E is in its 0.67 version (past
its ""number of the beast"" 0.666), and has been used extensively, to the point
of being the foundation of much of the (computer) science being done by our
research group (and, admittedly, not many others). All is not done, however;
now A::E is being integrated with POE so that evolutionary algorithms (EAs) can
be combined with all kinds of servers and used in client, servers, and anything
in between. In this companion to the talk I will explain what evolutionary
algorithms are, what they are being used for, how to do them with Perl (using
these or other fine modules found in CPAN) and what evolutionary algorithms can
do for Perl at large."
"Learning is the important property of Back Propagation Network (BPN) and
finding the suitable weights and thresholds during training in order to improve
training time as well as achieve high accuracy. Currently, data pre-processing
such as dimension reduction input values and pre-training are the contributing
factors in developing efficient techniques for reducing training time with high
accuracy and initialization of the weights is the important issue which is
random and creates paradox, and leads to low accuracy with high training time.
One good data preprocessing technique for accelerating BPN classification is
dimension reduction technique but it has problem of missing data. In this
paper, we study current pre-training techniques and new preprocessing technique
called Potential Weight Linear Analysis (PWLA) which combines normalization,
dimension reduction input values and pre-training. In PWLA, the first data
preprocessing is performed for generating normalized input values and then
applying them by pre-training technique in order to obtain the potential
weights. After these phases, dimension of input values matrix will be reduced
by using real potential weights. For experiment results XOR problem and three
datasets, which are SPECT Heart, SPECTF Heart and Liver disorders (BUPA) will
be evaluated. Our results, however, will show that the new technique of PWLA
will change BPN to new Supervised Multi Layer Feed Forward Neural Network
(SMFFNN) model with high accuracy in one epoch without training cycle. Also
PWLA will be able to have power of non linear supervised and unsupervised
dimension reduction property for applying by other supervised multi layer feed
forward neural network model in future work."
"Wong's diffusion network is a stochastic, zero-input Hopfield network with a
Gibbs stationary distribution over a bounded, connected continuum. Previously,
logarithmic thermal annealing was demonstrated for the diffusion network and
digital versions of it were studied and applied to imaging. Recently, ""quantum""
annealed Markov chains have garnered significant attention because of their
improved performance over ""pure"" thermal annealing. In this note, a joint
quantum and thermal version of Wong's diffusion network is described and its
convergence properties are studied. Different choices for ""auxiliary"" functions
are discussed, including those of the kinetic type previously associated with
quantum annealing."
"The research area of evolutionary multiobjective optimization (EMO) is
reaching better understandings of the properties and capabilities of EMO
algorithms, and accumulating much evidence of their worth in practical
scenarios. An urgent emerging issue is that the favoured EMO algorithms scale
poorly when problems have many (e.g. five or more) objectives. One of the chief
reasons for this is believed to be that, in many-objective EMO search,
populations are likely to be largely composed of nondominated solutions. In
turn, this means that the commonly-used algorithms cannot distinguish between
these for selective purposes. However, there are methods that can be used
validly to rank points in a nondominated set, and may therefore usefully
underpin selection in EMO search. Here we discuss and compare several such
methods. Our main finding is that simple variants of the often-overlooked
Average Ranking strategy usually outperform other methods tested, covering
problems with 5-20 objectives and differing amounts of inter-objective
correlation."
"This paper reports the results of an experiment on the use of Kak's B-Matrix
approach to spreading activity in a Hebbian neural network. Specifically, it
concentrates on the memory retrieval from single neurons and compares the
performance of the B-Matrix approach to that of the traditional approach."
"This paper reports the results on methods of comparing the memory retrieval
capacity of the Hebbian neural network which implements the B-Matrix approach,
by using the Widrow-Hoff rule of learning. We then, extend the recently
proposed Active Sites model by developing a delta rule to increase memory
capacity. Also, this paper extends the binary neural network to a multi-level
(non-binary) neural network."
"This paper presents an analysis of building blocks propagation in
Quantum-Inspired Genetic Algorithm, which belongs to a new class of
metaheuristics drawing their inspiration from both biological evolution and
unitary evolution of quantum systems. The expected number of quantum
chromosomes matching a schema has been analyzed and a random variable
corresponding to this issue has been introduced. The results have been compared
with Simple Genetic Algorithm. Also, it has been presented how selected binary
quantum chromosomes cover a domain of one-dimensional fitness function."
"With this paper, we contribute to the understanding of ant colony
optimization (ACO) algorithms by formally analyzing their runtime behavior. We
study simple MAX-MIN ant systems on the class of linear pseudo-Boolean
functions defined on binary strings of length 'n'. Our investigations point out
how the progress according to function values is stored in pheromone. We
provide a general upper bound of O((n^3 \log n)/ \rho) for two ACO variants on
all linear functions, where (\rho) determines the pheromone update strength.
Furthermore, we show improved bounds for two well-known linear pseudo-Boolean
functions called OneMax and BinVal and give additional insights using an
experimental study."
"In the paper, an evolutionary approach to test generation for functional BIST
is considered. The aim of the proposed scheme is to minimize the test data
volume by allowing the device's microprogram to test its logic, providing an
observation structure to the system, and generating appropriate test data for
the given architecture. Two methods of deriving a deterministic test set at
functional level are suggested. The first method is based on the classical
genetic algorithm with binary and arithmetic crossover and mutation operators.
The second one uses genetic programming, where test is represented as a
sequence of microoperations. In the latter case, we apply two-point crossover
based on exchanging test subsequences and mutation implemented as random
replacement of microoperations or operands. Experimental data of the program
realization showing the efficiency of the proposed methods are presented."
"It is now widely accepted that memristive devices are perfect candidates for
the emulation of biological synapses in neuromorphic systems. This is mainly
because of the fact that like the strength of synapse, memristance of the
memristive device can be tuned actively (e.g., by the application of volt- age
or current). In addition, it is also possible to fabricate very high density of
memristive devices (comparable to the number of synapses in real biological
system) through the nano-crossbar structures. However, in this paper we will
show that there are some problems associated with memristive synapses
(memristive devices which are playing the role of biological synapses). For
example, we show that the variation rate of the memristance of memristive
device depends completely on the current memristance of the device and
therefore it can change significantly with time during the learning phase. This
phenomenon can degrade the performance of learning methods like Spike
Timing-Dependent Plasticity (STDP) and cause the corresponding neuromorphic
systems to become unstable. Finally, at the end of this paper, we illustrate
that using two serially connected memristive devices with different polarities
as a synapse can somewhat fix the aforementioned problem."
"The recommendation to change breathing patterns from the mouth to the nose
can have a significantly positive impact upon the general well being of the
individual. We classify nasal and mouth breathing by using an acoustic sensor
and intelligent signal processing techniques. The overall purpose is to
investigate the possibility of identifying the differences in patterns between
nasal and mouth breathing in order to integrate this information into a
decision support system which will form the basis of a patient monitoring and
motivational feedback system to recommend the change from mouth to nasal
breathing. Our findings show that the breath pattern can be discriminated in
certain places of the body both by visual spectrum analysis and with a Back
Propagation neural network classifier. The sound file recoded from the sensor
placed on the hollow in the neck shows the most promising accuracy which is as
high as 90%."
"This paper details the application of a genetic programming framework for
classification of decision tree of Soil data to classify soil texture. The
database contains measurements of soil profile data. We have applied GATree for
generating classification decision tree. GATree is a decision tree builder that
is based on Genetic Algorithms (GAs). The idea behind it is rather simple but
powerful. Instead of using statistic metrics that are biased towards specific
trees we use a more flexible, global metric of tree quality that try to
optimize accuracy and size. GATree offers some unique features not to be found
in any other tree inducers while at the same time it can produce better results
for many difficult problems. Experimental results are presented which
illustrate the performance of generating best decision tree for classifying
soil texture for soil data set."
"Evolutionary algorithms (EAs) are heuristic algorithms inspired by natural
evolution. They are often used to obtain satisficing solutions in practice. In
this paper, we investigate a largely underexplored issue: the approximation
performance of EAs in terms of how close the solution obtained is to an optimal
solution. We study an EA framework named simple EA with isolated population
(SEIP) that can be implemented as a single- or multi-objective EA. We analyze
the approximation performance of SEIP using the partial ratio, which
characterizes the approximation ratio that can be guaranteed. Specifically, we
analyze SEIP using a set cover problem that is NP-hard. We find that in a
simple configuration, SEIP efficiently achieves an $H_n$-approximation ratio,
the asymptotic lower bound, for the unbounded set cover problem. We also find
that SEIP efficiently achieves an $(H_k-\frac{k-1}/{8k^9})$-approximation
ratio, the currently best-achievable result, for the k-set cover problem.
Moreover, for an instance class of the k-set cover problem, we disclose how
SEIP, using either one-bit or bit-wise mutation, can overcome the difficulty
that limits the greedy algorithm."
"This paper introduces a novel type of memetic algorithm based Topology and
Weight Evolving Artificial Neural Network (TWEANN) system called DX Neural
Network (DXNN). DXNN implements a number of interesting features, amongst which
is: a simple and database friendly tuple based encoding method, a 2 phase
neuroevolutionary approach aimed at removing the need for speciation due to its
intrinsic population diversification effects, a new ""Targeted Tuning Phase""
aimed at dealing with ""the curse of dimensionality"", and a new Random Intensity
Mutation (RIM) method that removes the need for crossover algorithms. The paper
will discuss DXNN's architecture, mutation operators, and its built in feature
selection method that allows for the evolved systems to expand and incorporate
new sensors and actuators. I then compare DXNN to other state of the art
TWEANNs on the standard double pole balancing benchmark, and demonstrate its
superior ability to evolve highly compact solutions faster than its
competitors. Then a set of oblation experiments is performed to demonstrate how
each feature of DXNN effects its performance, followed by a set of experiments
which demonstrate the platform's ability to create NN populations with
exceptionally high diversity profiles. Finally, DXNN is used to evolve
artificial robots in a set of two dimensional open-ended food gathering and
predator-prey simulations, demonstrating the system's ability to produce ever
more complex Neural Networks, and the system's applicability to the domain of
robotics, artificial life, and coevolution."
"Several genetic programming systems are created, each solving a different
problem. In these systems, the median number of generations G needed to evolve
a working program is measured. The behavior of G is observed as the difficulty
of the problem is increased. In these systems, the density D of working
programs in the universe of all possible programs is measured. The relationship
G ~ 1/sqrt(D) is observed to approximately hold for two program-like systems.
For parallel systems (systems that look like several independent programs
evolving in parallel), the relationship G ~ 1/(n ln n) is observed to
approximately hold. Finally, systems that are anti-parallel are considered."
"This work is focused on improving the character recognition capability of
feed-forward back-propagation neural network by using one, two and three hidden
layers and the modified additional momentum term. 182 English letters were
collected for this work and the equivalent binary matrix form of these
characters was applied to the neural network as training patterns. While the
network was getting trained, the connection weights were modified at each epoch
of learning. For each training sample, the error surface was examined for
minima by computing the gradient descent. We started the experiment by using
one hidden layer and the number of hidden layers was increased up to three and
it has been observed that accuracy of the network was increased with low mean
square error but at the cost of training time. The recognition accuracy was
improved further when modified additional momentum term was used."
"The capability of discretization of matrix elements in the problem of
quadratic functional minimization with linear member built on matrix in
N-dimensional configuration space with discrete coordinates is researched. It
is shown, that optimal procedure of replacement matrix elements by the integer
quantities with the limited number of gradations exist, and the efficient of
minimization does not reduce. Parameter depends on matrix properties, which
allows estimate the capability of using described procedure for given type of
matrix, is found. Computational complexities of algorithm and RAM requirements
are reduced by 16 times, correct using of integer elements allows increase
minimization algorithm speed by the orders."
"A large part of the workforce, and growing every day, is originally from
India. India one of the second largest populations in the world, they have a
lot to offer in terms of jobs. The sheer number of IT workers makes them a
formidable travelling force as well, easily picking up employment in English
speaking countries. The beginning of the economic crises since 2008 September,
many Indians have return homeland, and this has had a substantial impression on
the Indian Rupee (INR) as liken to the US Dollar (USD). We are using
numerational knowledge based techniques for forecasting has been proved highly
successful in present time. The purpose of this paper is to examine the effects
of several important neural network factors on model fitting and forecasting
the behaviours. In this paper, Artificial Neural Network has successfully been
used for exchange rate forecasting. This paper examines the effects of the
number of inputs and hidden nodes and the size of the training sample on the
in-sample and out-of-sample performance. The Indian Rupee (INR) / US Dollar
(USD) is used for detailed examinations. The number of input nodes has a
greater impact on performance than the number of hidden nodes, while a large
number of observations do reduce forecast errors."
"The performance of GA is measured and analyzed in terms of its performance
parameters against variations in its genetic operators and associated
parameters. Since last four decades huge numbers of researchers have been
working on the performance of GA and its enhancement. This earlier research
work on analyzing the performance of GA enforces the need to further
investigate the exploration and exploitation characteristics and observe its
impact on the behavior and overall performance of GA. This paper introduces the
novel approach of adaptive twin probability associated with the advanced twin
operator that enhances the performance of GA. The design of the advanced twin
operator is extrapolated from the twin offspring birth due to single ovulation
in natural genetic systems as mentioned in the earlier works. The twin
probability of this operator is adaptively varied based on the fitness of best
individual thereby relieving the GA user from statically defining its value.
This novel approach of adaptive twin probability is experimented and tested on
the standard benchmark optimization test functions. The experimental results
show the increased accuracy in terms of the best individual and reduced
convergence time."
"Many real world problems are NP-Hard problems are a very large part of them
can be represented as graph based problems. This makes graph theory a very
important and prevalent field of study. In this work a new bio-inspired
meta-heuristics called Green Heron Swarm Optimization (GHOSA) Algorithm is
being introduced which is inspired by the fishing skills of the bird. The
algorithm basically suited for graph based problems like combinatorial
optimization etc. However introduction of an adaptive mathematical variation
operator called Location Based Neighbour Influenced Variation (LBNIV) makes it
suitable for high dimensional continuous domain problems. The new algorithm is
being operated on the traditional benchmark equations and the results are
compared with Genetic Algorithm and Particle Swarm Optimization. The algorithm
is also operated on Travelling Salesman Problem, Quadratic Assignment Problem,
Knapsack Problem dataset. The procedure to operate the algorithm on the
Resource Constraint Shortest Path and road network optimization is also
discussed. The results clearly demarcates the GHOSA algorithm as an efficient
algorithm specially considering that the number of algorithms for the discrete
optimization is very low and robust and more explorative algorithm is required
in this age of social networking and mostly graph based problem scenarios."
"This paper defines and discusses Mouse Level Computational Intelligence
(MLCI) as a grand challenge for the coming century. It provides a specific
roadmap to reach that target, citing relevant work and review papers and
discussing the relation to funding priorities in two NSF funding activities:
the ongoing Energy, Power and Adaptive Systems program (EPAS) and the recent
initiative in Cognitive Optimization and Prediction (COPN). It elaborates on
the first step, vector intelligence, a challenge in the development of
universal learning systems, which itself will require considerable new research
to attain. This in turn is a crucial prerequisite to true functional
understanding of how mammal brains achieve such general learning capabilities."
"The 0-1 knapsack problem is a well-known combinatorial optimisation problem.
Approximation algorithms have been designed for solving it and they return
provably good solutions within polynomial time. On the other hand, genetic
algorithms are well suited for solving the knapsack problem and they find
reasonably good solutions quickly. A naturally arising question is whether
genetic algorithms are able to find solutions as good as approximation
algorithms do. This paper presents a novel multi-objective optimisation genetic
algorithm for solving the 0-1 knapsack problem. Experiment results show that
the new algorithm outperforms its rivals, the greedy algorithm, mixed strategy
genetic algorithm, and greedy algorithm + mixed strategy genetic algorithm."
"Evolutionary algorithms are well suited for solving the knapsack problem.
Some empirical studies claim that evolutionary algorithms can produce good
solutions to the 0-1 knapsack problem. Nonetheless, few rigorous investigations
address the quality of solutions that evolutionary algorithms may produce for
the knapsack problem. The current paper focuses on a theoretical investigation
of three types of (N+1) evolutionary algorithms that exploit bitwise mutation,
truncation selection, plus different repair methods for the 0-1 knapsack
problem. It assesses the solution quality in terms of the approximation ratio.
Our work indicates that the solution produced by pure strategy and mixed
strategy evolutionary algorithms is arbitrarily bad. Nevertheless, the
evolutionary algorithm using helper objectives may produce 1/2-approximation
solutions to the 0-1 knapsack problem."
"The supplier selection problem is based on electing the best supplier from a
group of pre-specified candidates, is identified as a Multi Criteria Decision
Making (MCDM), is proportionately significant in terms of qualitative and
quantitative attributes. It is a fundamental issue to achieve a trade-off
between such quantifiable and unquantifiable attributes with an aim to
accomplish the best solution to the abovementioned problem. This article
portrays a metaheuristic based optimization model to solve this NP-Complete
problem. Initially the Analytic Hierarchy Process (AHP) is implemented to
generate an initial feasible solution of the problem. Thereafter a Simulated
Annealing (SA) algorithm is exploited to improve the quality of the obtained
solution. The Taguchi robust design method is exploited to solve the critical
issues on the subject of the parameter selection of the SA technique. In order
to verify the proposed methodology the numerical results are demonstrated based
on tangible industry data."
"A method that allows us to give a different treatment to any neuron inside
feedforward neural networks is presented. The algorithm has been implemented
with two very different learning methods: a standard Back-propagation (BP)
procedure and an evolutionary algorithm. First, we have demonstrated that the
EA training method converges faster and gives more accurate results than BP.
Then we have made a full analysis of the effects of turning off different
combinations of neurons after the training phase. We demonstrate that EA is
much more robust than BP for all the cases under study. Even in the case when
two hidden neurons are lost, EA training is still able to give good average
results. This difference implies that we must be very careful when pruning or
redundancy effects are being studied since the network performance when losing
neurons strongly depends on the training method. Moreover, the influence of the
individual inputs will also depend on the training algorithm. Since EA keeps a
good classification performance when units are lost, this method could be a
good way to simulate biological learning systems since they must be robust
against deficient neuron performance. Although biological systems are much more
complex than the simulations shown in this article, we propose that a smart
training strategy such as the one shown here could be considered as a first
protection against the losing of a certain number of neurons."
"We propose a computationally efficient limited memory Covariance Matrix
Adaptation Evolution Strategy for large scale optimization, which we call the
LM-CMA-ES. The LM-CMA-ES is a stochastic, derivative-free algorithm for
numerical optimization of non-linear, non-convex optimization problems in
continuous domain. Inspired by the limited memory BFGS method of Liu and
Nocedal (1989), the LM-CMA-ES samples candidate solutions according to a
covariance matrix reproduced from $m$ direction vectors selected during the
optimization process. The decomposition of the covariance matrix into Cholesky
factors allows to reduce the time and memory complexity of the sampling to
$O(mn)$, where $n$ is the number of decision variables. When $n$ is large
(e.g., $n$ > 1000), even relatively small values of $m$ (e.g., $m=20,30$) are
sufficient to efficiently solve fully non-separable problems and to reduce the
overall run-time."
"Coevolutionary minimal substrates are simple and abstract models that allow
studying the relationships and codynamics between objective and subjective
fitness. Using these models an approach is presented for defining and analyzing
fitness landscapes of coevolutionary problems. We devise similarity measures of
codynamic fitness landscapes and experimentally study minimal substrates of
test--based and compositional problems for both cooperative and competitive
interaction."
"Usually, reservoir computing shows an exponential memory decay. This paper
investigates under which circumstances echo state networks can show a power law
forgetting. That means traces of earlier events can be found in the reservoir
for very long time spans. Such a setting requires critical connectivity exactly
at the limit of what is permissible according the echo state condition.
However, for general matrices the limit cannot be determined exactly from
theory. In addition, the behavior of the network is strongly influenced by the
input flow. Results are presented that use certain types of restricted
recurrent connectivity and anticipation learning with regard to the input,
where indeed power law forgetting can be achieved."
"We introduce a novel evolutionary algorithm (EA) with a semantic
network-based representation. For enabling this, we establish new formulations
of EA variation operators, crossover and mutation, that we adapt to work on
semantic networks. The algorithm employs commonsense reasoning to ensure all
operations preserve the meaningfulness of the networks, using ConceptNet and
WordNet knowledge bases. The algorithm can be interpreted as a novel memetic
algorithm (MA), given that (1) individuals represent pieces of information that
undergo evolution, as in the original sense of memetics as it was introduced by
Dawkins; and (2) this is different from existing MA, where the word ""memetic""
has been used as a synonym for local refinement after global optimization. For
evaluating the approach, we introduce an analogical similarity-based fitness
measure that is computed through structure mapping. This setup enables the
open-ended generation of networks analogous to a given base network."
"Reservoir computing is a recent trend in neural networks which uses the
dynamical perturbations on the phase space of a system to compute a desired
target function. We present how one can formulate an expectation of system
performance in a simple class of reservoir computing called echo state
networks. In contrast with previous theoretical frameworks, which only reveal
an upper bound on the total memory in the system, we analytically calculate the
entire memory curve as a function of the structure of the system and the
properties of the input and the target function. We demonstrate the precision
of our framework by validating its result for a wide range of system sizes and
spectral radii. Our analytical calculation agrees with numerical simulations.
To the best of our knowledge this work presents the first exact analytical
characterization of the memory curve in echo state networks."
"An extension to a recently introduced architecture of clique-based neural
networks is presented. This extension makes it possible to store sequences with
high efficiency. To obtain this property, network connections are provided with
orientation and with flexible redundancy carried by both spatial and temporal
redundancy, a mechanism of anticipation being introduced in the model. In
addition to the sequence storage with high efficiency, this new scheme also
offers biological plausibility. In order to achieve accurate sequence
retrieval, a double layered structure combining hetero-association and
auto-association is also proposed."
"We propose NM landscapes as a new class of tunably rugged benchmark problems.
NM landscapes are well-defined on alphabets of any arity, including both
discrete and real-valued alphabets, include epistasis in a natural and
transparent manner, are proven to have known value and location of the global
maximum and, with some additional constraints, are proven to also have a known
global minimum. Empirical studies are used to illustrate that, when
coefficients are selected from a recommended distribution, the ruggedness of NM
landscapes is smoothly tunable and correlates with several measures of search
difficulty. We discuss why these properties make NM landscapes preferable to
both NK landscapes and Walsh polynomials as benchmark landscape models with
tunable epistasis."
"The balance of exploration versus exploitation (EvE) is a key issue on
evolutionary computation. In this paper we will investigate how an adaptive
controller aimed to perform Operator Selection can be used to dynamically
manage the EvE balance required by the search, showing that the search
strategies determined by this control paradigm lead to an improvement of
solution quality found by the evolutionary algorithm."
"We present a simple regularization technique for Recurrent Neural Networks
(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful
technique for regularizing neural networks, does not work well with RNNs and
LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show
that it substantially reduces overfitting on a variety of tasks. These tasks
include language modeling, speech recognition, image caption generation, and
machine translation."
"The Particle Swarm Optimized (PSO) fuzzy controller has been proposed for
indirect vector control of induction motor. In this proposed scheme a Neutral
Point Clamped (NPC) multilevel inverter is used and hysteresis current control
technique has been adopted for switching the IGBTs. A Mamdani type fuzzy
controller is used in place of conventional PI controller. To ensure better
performance of fuzzy controller all parameters such as membership functions,
normalizing and de-normalizing parameters are optimized using PSO. The
performance of proposed controller is investigated under various load and speed
conditions. The simulation results show its stability and robustness for high
performance derives applications."
"Ant Colony Optimization (ACO) has been applied in supervised learning in
order to induce classification rules as well as decision trees, named
Ant-Miners. Although these are competitive classifiers, the stability of these
classifiers is an important concern that owes to their stochastic nature. In
this paper, to address this issue, an acclaimed machine learning technique
named, ensemble of classifiers is applied, where an ACO classifier is used as a
base classifier to prepare the ensemble. The main trade-off is, the predictions
in the new approach are determined by discovering a group of models as opposed
to the single model classification. In essence, we prepare multiple models from
the randomly replaced samples of training data from which, a unique model is
prepared by aggregating the models to test the unseen data points. The main
objective of this new approach is to increase the stability of the Ant-Miner
results there by improving the performance of ACO classification. We found that
the ensemble Ant-Miners significantly improved the stability by reducing the
classification error on unseen data."
"The Traveling salesman problem (TSP) is proved to be NP-complete in most
cases. The genetic algorithm (GA) is one of the most useful algorithms for
solving this problem. In this paper a conventional GA is compared with an
improved hybrid GA in solving TSP. The improved or hybrid GA consist of
conventional GA and two local optimization strategies. The first strategy is
extracting all sequential groups including four cities of samples and changing
the two central cities with each other. The second local optimization strategy
is similar to an extra mutation process. In this step with a low probability a
sample is selected. In this sample two random cities are defined and the path
between these cities is reversed. The computation results show that the
proposed method also finds better paths than the conventional GA within an
acceptable computation time."
"The lane reversal has proven to be a useful method to mitigate traffic
congestion during rush hour or in case of specific events that affect high
traffic volumes. In this work we propose a methodology that is placed within
optimization via Simulation, by means of which a multi-objective genetic
algorithm and simulations of traffic are used to determine the configuration of
ideal lane reversal."
"There are several training algorithms for backpropagation method in neural
network. Not all of these algorithms have the same accuracy level demonstrated
through the percentage level of suitability in recognizing patterns in the
data. In this research tested 12 training algorithms specifically in recognize
data patterns of test validity. The basic network parameters used are the
maximum allowable epoch = 1000, target error = 10-3, and learning rate = 0.05.
Of the twelve training algorithms each performed 20 times looping. The test
results obtained that the percentage rate of the great match is trainlm
algorithm with alpha 5% have adequate levels of suitability of 87.5% at the
level of significance of 0.000. This means the most appropriate training
algorithm in recognizing the the data pattern of test validity is the trainlm
algorithm."
"The author previously presented an event window segmentation (EWS) algorithm
[5] that uses purely statistical methods to learn to recognize recurring
patterns in an input stream of events. In the following discussion, the EWS
algorithm is first extended to make predictions about future events. Next, this
extended algorithm is used to construct a high-level, simplified model of a
neocortical hierarchy. An event stream enters at the bottom of the hierarchy,
and drives processing activity upward in the hierarchy. Successively higher
regions in the hierarchy learn to recognize successively deeper levels of
patterns in these events as they propagate from the bottom of the hierarchy.
The lower levels in the hierarchy use the predictions from the levels above to
strengthen their own predictions. A C++ source code listing of the model
implementation and test program is included as an appendix."
"This work studies the behavior of three elitist multi- and many-objective
evolutionary algorithms generating a high-resolution approximation of the
Pareto optimal set. Several search-assessment indicators are defined to trace
the dynamics of survival selection and measure the ability to simultaneously
keep optimal solutions and discover new ones under different population sizes,
set as a fraction of the size of the Pareto optimal set."
"In this paper we analyze and extend the neural network based associative
memory proposed by Gripon and Berrou. This associative memory resembles the
celebrated Willshaw model with an added partite cluster structure. In the
literature, two retrieving schemes have been proposed for the network dynamics,
namely sum-of-sum and sum-of-max. They both offer considerably better
performance than Willshaw and Hopfield networks, when comparable retrieval
scenarios are considered. Former discussions and experiments concentrate on the
erasure scenario, where a partial message is used as a probe to the network, in
the hope of retrieving the full message. In this regard, sum-of-max outperforms
sum-of-sum in terms of retrieval rate by a large margin. However, we observe
that when noise and errors are present and the network is queried by a corrupt
probe, sum-of-max faces a severe limitation as its stringent activation rule
prevents a neuron from reviving back into play once deactivated. In this
manuscript, we categorize and analyze different error scenarios so that both
the erasure and the corrupt scenarios can be treated consistently. We make an
amendment to the network structure to improve the retrieval rate, at the cost
of an extra scalar per neuron. Afterwards, five different approaches are
proposed to deal with corrupt probes. As a result, we extend the network
capability, and also increase the robustness of the retrieving procedure. We
then experimentally compare all these proposals and discuss pros and cons of
each approach under different types of errors. Simulation results show that if
carefully designed, the network is able to preserve both a high retrieval rate
and a low running time simultaneously, even when queried by a corrupt probe."
"Many kinds of Evolutionary Algorithms (EAs) have been described in the
literature since the last 30 years. However, though most of them share a common
structure, no existing software package allows the user to actually shift from
one model to another by simply changing a few parameters, e.g. in a single
window of a Graphical User Interface. This paper presents GUIDE, a Graphical
User Interface for DREAM Experiments that, among other user-friendly features,
unifies all kinds of EAs into a single panel, as far as evolution parameters
are concerned. Such a window can be used either to ask for one of the well
known ready-to-use algorithms, or to very easily explore new combinations that
have not yet been studied. Another advantage of grouping all necessary elements
to describe virtually all kinds of EAs is that it creates a fantastic pedagogic
tool to teach EAs to students and newcomers to the field."
"Can intelligence optimise Digital Ecosystems? How could a distributed
intelligence interact with the ecosystem dynamics? Can the software components
that are part of genetic selection be intelligent in themselves, as in an
adaptive technology? We consider the effect of a distributed intelligence
mechanism on the evolutionary and ecological dynamics of our Digital Ecosystem,
which is the digital counterpart of a biological ecosystem for evolving
software services in a distributed network. We investigate Neural Networks and
Support Vector Machine for the learning based pattern recognition functionality
of our distributed intelligence. Simulation results imply that the Digital
Ecosystem performs better with the application of a distributed intelligence,
marginally more effectively when powered by Support Vector Machine than Neural
Networks, and suggest that it can contribute to optimising the operation of our
Digital Ecosystem."
"Stability is perhaps one of the most desirable features of any engineered
system, given the importance of being able to predict its response to various
environmental conditions prior to actual deployment. Engineered systems are
becoming ever more complex, approaching the same levels of biological
ecosystems, and so their stability becomes ever more important, but taking on
more and more differential dynamics can make stability an ever more elusive
property. The Chli-DeWilde definition of stability views a Multi-Agent System
as a discrete time Markov chain with potentially unknown transition
probabilities. With a Multi-Agent System being considered stable when its
state, a stochastic process, has converged to an equilibrium distribution,
because stability of a system can be understood intuitively as exhibiting
bounded behaviour. We investigate an extension to include Multi-Agent Systems
with evolutionary dynamics, focusing on the evolving agent populations of our
Digital Ecosystem. We then built upon this to construct an entropy-based
definition for the degree of instability (entropy of the limit probabilities),
which was later used to perform a stability analysis. The Digital Ecosystem is
considered to investigate the stability of an evolving agent population through
simulations, for which the results were consistent with the original
Chli-DeWilde definition."
"We view Digital Ecosystems to be the digital counterparts of biological
ecosystems, exploiting the self-organising properties of biological ecosystems,
which are considered to be robust, self-organising and scalable architectures
that can automatically solve complex, dynamic problems. Digital Ecosystems are
a novel optimisation technique where the optimisation works at two levels: a
first optimisation, migration of agents (representing services) which are
distributed in a decentralised peer-to-peer network, operating continuously in
time; this process feeds a second optimisation based on evolutionary computing
that operates locally on single peers and is aimed at finding solutions to
satisfy locally relevant constraints. We created an Ecosystem-Oriented
Architecture of Digital Ecosystems by extending Service-Oriented Architectures
with distributed evolutionary computing, allowing services to recombine and
evolve over time, constantly seeking to improve their effectiveness for the
user base. Individuals within our Digital Ecosystem will be applications
(groups of services), created in response to user requests by using
evolutionary optimisation to aggregate the services. These individuals will
migrate through the Digital Ecosystem and adapt to find niches where they are
useful in fulfilling other user requests for applications. Simulation results
imply that the Digital Ecosystem performs better at large scales than a
comparable Service-Oriented Architecture, suggesting that incorporating ideas
from theoretical ecology can contribute to useful self-organising properties in
digital ecosystems."
"We start with a discussion of the relevant literature, including Nature
Inspired Computing as a framework in which to understand this work, and the
process of biomimicry to be used in mimicking the necessary biological
processes to create Digital Ecosystems. We then consider the relevant
theoretical ecology in creating the digital counterpart of a biological
ecosystem, including the topological structure of ecosystems, and evolutionary
processes within distributed environments. This leads to a discussion of the
relevant fields from computer science for the creation of Digital Ecosystems,
including evolutionary computing, Multi-Agent Systems, and Service-Oriented
Architectures. We then define Ecosystem-Oriented Architectures for the creation
of Digital Ecosystems, imbibed with the properties of self-organisation and
scalability from biological ecosystems, including a novel form of distributed
evolutionary computing."
"The stucture determination of a neural network for the modelisation of a
system remain the core of the problem. Within this framework, we propose a
pruning algorithm of the network based on the use of the analysis of the
sensitivity of the variance of all the parameters of the network. This
algorithm will be tested on two examples of simulation and its performances
will be compared with three other algorithms of pruning of the literature"
"Simulation is often used to evaluate the relevance of a Directing Program of
Production (PDP) or to evaluate its impact on detailed sc\'enarii of
scheduling. Within this framework, we propose to reduce the complexity of a
model of simulation by exploiting a multilayer perceptron. A main phase of the
modeling of one system using a multilayer perceptron remains the determination
of the structure of the network. We propose to compare and use various pruning
algorithms in order to determine the optimal structure of the network used to
reduce the complexity of the model of simulation of our case of application: a
sawmill."
"This paper introduces a method to generate hierarchically modular networks
with prescribed node degree list and proposes a metric to measure network
modularity based on the notion of edge distance. The generated networks are
used as test problems to explore the effect of modularity and degree
distribution on evolutionary algorithm performance. Results from the
experiments (i) confirm a previous finding that modularity increases the
performance advantage of genetic algorithms over hill climbers, and (ii)
support a new conjecture that test problems with modularized constraint
networks having heavy-tailed right-skewed degree distributions are more easily
solved than test problems with modularized constraint networks having
bell-shaped normal degree distributions."
"Artificial Neural Network is one of the most common AI application fields.
This field has direct and indirect usages most sciences. The main goal of ANN
is to imitate biological neural networks for solving scientific problems. But
the level of parallelism is the main problem of ANN systems in comparison with
biological systems. To solve this problem, we have offered a XML-based
framework for implementing ANN on the Globus Toolkit Platform. Globus Toolkit
is well known management software for multipurpose Grids. Using the Grid for
simulating the neuron network will lead to a high degree of parallelism in the
implementation of ANN. We have used the XML for improving flexibility and
scalability in our framework."
"Abbreviated Abstract: The objective of Evolutionary Computation is to solve
practical problems (e.g. optimization, data mining) by simulating the
mechanisms of natural evolution. This thesis addresses several topics related
to adaptation and self-organization in evolving systems with the overall aims
of improving the performance of Evolutionary Algorithms (EA), understanding its
relation to natural evolution, and incorporating new mechanisms for mimicking
complex biological systems."
"Aluminum extrusion die manufacturing is a critical task for productive
improvement and increasing potential of competition in aluminum extrusion
industry. It causes to meet the efficiency not only consistent quality but also
time and production cost reduction. Die manufacturing consists first of die
design and process planning in order to make a die for extruding the customer's
requirement products. The efficiency of die design and process planning are
based on the knowledge and experience of die design and die manufacturer
experts. This knowledge has been formulated into a computer system called the
knowledge-based system. It can be reused to support a new die design and
process planning. Such knowledge can be extracted directly from die geometry
which is composed of die features. These features are stored in die feature
library to be prepared for producing a new die manufacturing. Die geometry is
defined according to the characteristics of the profile so we can reuse die
features from the previous similar profile design cases. This paper presents
the CaseXpert Process Planning System for die manufacturing based on feature
based neural network technique. Die manufacturing cases in the case library
would be retrieved with searching and learning method by neural network for
reusing or revising it to build a die design and process planning when a new
case is similar with the previous die manufacturing cases. The results of the
system are dies design and machining process. The system has been successfully
tested, it has been proved that the system can reduce planning time and respond
high consistent plans."
"Digital logic forms the functional basics of most modern electronic equipment
and as such the creation of novel digital logic circuits is an active area of
computer engineering research. This study demonstrates that genetic algorithms
can be used to evolve functionally useful sets of logic gate interconnections
to create useful digital logic circuits. The efficacy of this approach is
illustrated via the evolution of AND, OR, XOR, NOR, and XNOR functionality from
sets of NAND gates, thereby illustrating that evolutionary methods have the
potential be applied to the design of digital electronics."
"A primary motivation for research in Digital Ecosystems is the desire to
exploit the self-organising properties of natural ecosystems. Ecosystems arc
thought to be robust, scalable architectures that can automatically solve
complex, dynamic problems. However, the biological processes that contribute to
these properties have not been made explicit in Digital Ecosystem research.
Here, we introduce how biological properties contribute to the self-organising
features of natural ecosystems. These properties include populations of
evolving agents, a complex dynamic environment, and spatial distributions which
generate local interactions. The potential for exploiting these properties in
artificial systems is then considered."
"The official meteorological network is poor on the island of Corsica: only
three sites being about 50 km apart are equipped with pyranometers which enable
measurements by hourly and daily step. These sites are Ajaccio (41\degree 55'N
and 8\degree 48'E, seaside), Bastia (42\degree 33'N, 9\degree 29'E, seaside)
and Corte (42\degree 30'N, 9\degree 15'E average altitude of 486 meters). This
lack of weather station makes difficult the predictability of PV power grid
performance. This work intends to study a methodology which can predict global
solar irradiation using data available from another location for daily and
hourly horizon. In order to achieve this prediction, we have used Artificial
Neural Network which is a popular artificial intelligence technique in the
forecasting domain. A simulator has been obtained using data available for the
station of Ajaccio that is the only station for which we have a lot of data: 16
years from 1972 to 1987. Then we have tested the efficiency of this simulator
in two places with different geographical features: Corte, a mountainous region
and Bastia, a coastal region. On daily horizon, the relocation has implied
fewer errors than a ""na\""ive"" prediction method based on the persistence
(RMSE=1468 Vs 1383Wh/m^2 to Bastia and 1325 Vs 1213Wh/m^2 to Corte). On hourly
case, the results were still satisfactory, and widely better than persistence
(RMSE=138.8 Vs 109.3 Wh/m^2 to Bastia and 135.1 Vs 114.7 Wh/m^2 to Corte). The
last experiment was to evaluate the accuracy of our simulator on a PV power
grid localized at 10 km from the station of Ajaccio. We got errors very
suitable (nRMSE=27.9%, RMSE=99.0 W.h) compared to those obtained with the
persistence (nRMSE=42.2%, RMSE=149.7 W.h)."
"Reactive power plays an important role in supporting the real power transfer
by maintaining voltage stability and system reliability. It is a critical
element for a transmission operator to ensure the reliability of an electric
system while minimizing the cost associated with it. The traditional objectives
of reactive power dispatch are focused on the technical side of reactive
support such as minimization of transmission losses. Reactive power cost
compensation to a generator is based on the incurred cost of its reactive power
contribution less the cost of its obligation to support the active power
delivery. In this paper an efficient Particle Swarm Optimization (PSO) based
reactive power optimization approach is presented. The optimal reactive power
dispatch problem is a nonlinear optimization problem with several constraints.
The objective of the proposed PSO is to minimize the total support cost from
generators and reactive compensators. It is achieved by maintaining the whole
system power loss as minimum thereby reducing cost allocation. The purpose of
reactive power dispatch is to determine the proper amount and location of
reactive support. Reactive Optimal Power Flow (ROPF) formulation is developed
as an analysis tool and the validity of proposed method is examined using an
IEEE-14 bus system."
"This paper reviews application of Artificial Neural Networks in Aircraft
Maintenance, Repair and Overhaul (MRO). MRO solutions are designed to
facilitate the authoring and delivery of maintenance and repair information to
the line maintenance technicians who need to improve aircraft repair turn
around time, optimize the efficiency and consistency of fleet maintenance and
ensure regulatory compliance. The technical complexity of aircraft systems,
especially in avionics, has increased to the point at which it poses a
significant troubleshotting and repair challenge for MRO personnel. As per the
existing scenario, the MRO systems in place are inefficient. In this paper, we
propose the centralization and integration of the MRO database to increase its
efficiency. Moreover the implementation of Artificial Neural Networks in this
system can rid the system of many of its deficiencies. In order to make the
system more efficient we propose to integrate all the modules so as to reduce
the efficacy of repair."
"This paper extends the analogies employed in the development of
quantum-inspired evolutionary algorithms by proposing quantum-inspired Hadamard
walks, called QHW. A novel quantum-inspired evolutionary algorithm, called
HQEA, for solving combinatorial optimization problems, is also proposed. The
novelty of HQEA lies in it's incorporation of QHW Remote Search and QHW Local
Search - the quantum equivalents of classical mutation and local search, that
this paper defines. The intuitive reasoning behind this approach, and the
exploration-exploitation balance thus occurring is explained. From the results
of the experiments carried out on the 0,1-knapsack problem, HQEA performs
significantly better than a conventional genetic algorithm, CGA, and two
quantum-inspired evolutionary algorithms - QEA and NQEA, in terms of
convergence speed and accuracy."
"This paper presents an application of evolutionary search procedures to
artificial neural networks. Here, we can distinguish among three kinds of
evolution in artificial neural networks, i.e. the evolution of connection
weights, of architectures, and of learning rules. We review each kind of
evolution in detail and analyse critical issues related to different
evolutions. This article concentrates on finding the suitable way of using
evolutionary algorithms for optimizing the artificial neural network
parameters."
"A general procedure of average-case performance evaluation for population
dynamics such as genetic algorithms (GAs) is proposed and its validity is
numerically examined. We introduce a learning algorithm of Gibbs distributions
from training sets which are gene configurations (strings) generated by GA in
order to figure out the statistical properties of GA from the view point of
thermodynamics. The learning algorithm is constructed by means of minimization
of the Kullback-Leibler information between a parametric Gibbs distribution and
the empirical distribution of gene configurations. The formulation is applied
to the solvable probabilistic models having multi-valley energy landscapes,
namely, the spin glass chain and the Sherrington-Kirkpatrick model. By using
computer simulations, we discuss the asymptotic behaviour of the effective
temperature scheduling and the residual energy induced by the GA dynamics."
"Mobility prediction allows estimating the stability of paths in a mobile
wireless Ad Hoc networks. Identifying stable paths helps to improve routing by
reducing the overhead and the number of connection interruptions. In this
paper, we introduce a neural network based method for mobility prediction in Ad
Hoc networks. This method consists of a multi-layer and recurrent neural
network using back propagation through time algorithm for training."
"We extend the work of Lehre and Witt (GECCO 2010) on the unbiased black-box
model by considering higher arity variation operators. In particular, we show
that already for binary operators the black-box complexity of \leadingones
drops from $\Theta(n^2)$ for unary operators to $O(n \log n)$. For \onemax, the
$\Omega(n \log n)$ unary black-box complexity drops to O(n) in the binary case.
For $k$-ary operators, $k \leq n$, the \onemax-complexity further decreases to
$O(n/\log k)$."
"A way to enhance the performance of a model that combines genetic algorithms
and fuzzy logic for feature selection and classification is proposed. Early
diagnosis of any disease with less cost is preferable. Diabetes is one such
disease. Diabetes has become the fourth leading cause of death in developed
countries and there is substantial evidence that it is reaching epidemic
proportions in many developing and newly industrialized nations. In medical
diagnosis, patterns consist of observable symptoms along with the results of
diagnostic tests. These tests have various associated costs and risks. In the
automated design of pattern classification, the proposed system solves the
feature subset selection problem. It is a task of identifying and selecting a
useful subset of pattern-representing features from a larger set of features.
Using fuzzy rule-based classification system, the proposed system proves to
improve the classification accuracy."
"This paper is an extension to the memory retrieval procedure of the B-Matrix
approach [6],[17] to neural network learning. The B-Matrix is a part of the
interconnection matrix generated from the Hebbian neural network, and in memory
retrieval, the B-matrix is clamped with a small fragment of the memory. The
fragment gradually enlarges by means of feedback, until the entire vector is
obtained. In this paper, we propose the use of delta learning to enhance the
retrieval rate of the stored memories."
"In this work we provide a formal model for the different time-dependent
components that can appear in dynamic multi-objective optimization problems,
along with a classification of these components. Four main classes are
identified, corresponding to the influence of the parameters, objective
functions, previous states of the dynamic system and, last, environment
changes, which in turn lead to online optimization problems. For illustration
purposes, examples are provided for each class identified - by no means
standing as the most representative ones or exhaustive in scope."
"The article presents new results on the use of variable thresholds to
increase the capacity of a feedback neural network. Non-binary networks are
also considered in this analysis."
"Genetic Programming (GP) has found various applications. Understanding this
type of algorithm from a theoretical point of view is a challenging task. The
first results on the computational complexity of GP have been obtained for
problems with isolated program semantics. With this paper, we push forward the
computational complexity analysis of GP on a problem with dependent program
semantics. We study the well-known sorting problem in this context and analyze
rigorously how GP can deal with different measures of sortedness."
"In the present paper, an effort has been made for storing and recalling
images with Hopfield Neural Network Model of auto-associative memory. Images
are stored by calculating a corresponding weight matrix. Thereafter, starting
from an arbitrary configuration, the memory will settle on exactly that stored
image, which is nearest to the starting configuration in terms of Hamming
distance. Thus given an incomplete or corrupted version of a stored image, the
network is able to recall the corresponding original image. The storing of the
objects has been performed according to the Hopfield algorithm explained below.
Once the net has completely learnt this set of input patterns, a set of testing
patterns containing degraded images will be given to the net. Then the Hopfield
net will tend to recall the closest matching pattern for the given degraded
image. The simulated results show that Hopfield model is the best for storing
and recalling images."
"The genetic algorithm (GA) is an optimization and search technique based on
the principles of genetics and natural selection. A GA allows a population
composed of many individuals to evolve under specified selection rules to a
state that maximizes the ""fitness"" function. In that process, crossover
operator plays an important role. To comprehend the GAs as a whole, it is
necessary to understand the role of a crossover operator. Today, there are a
number of different crossover operators that can be used in GAs. However, how
to decide what operator to use for solving a problem? A number of test
functions with various levels of difficulty has been selected as a test polygon
for determine the performance of crossover operators. In this paper, a novel
crossover operator called 'ring crossover' is proposed. In order to evaluate
the efficiency and feasibility of the proposed operator, a comparison between
the results of this study and results of different crossover operators used in
GAs is made through a number of test functions with various levels of
difficulty. Results of this study clearly show significant differences between
the proposed operator and the other crossover operators."
"The risk of diseases such as heart attack and high blood pressure could be
reduced by adequate physical activity. However, even though majority of general
population claims to perform some physical exercise, only a minority exercises
enough to keep a healthy living style. Thus, physical inactivity has become one
of the major concerns of public health in the past decade. Research shows that
the highest decrease in physical activity is noticed from high school to
college. Thus, it is of great importance to quickly identify college students
at health risk due to physical inactivity. Research also shows that the level
of physical activity of an individual is highly correlated to demographic
features such as race and gender, as well as self motivation and support from
family and friends. This information could be collected from each student via a
20 minute questionnaire, but the time needed to distribute and analyze each
questionnaire is infeasible on a collegiate campus. Thus, we propose an
automatic identifier of students at risk, so that these students could easier
be targeted by collegiate campuses and physical activity promotion departments.
We present in this paper preliminary results of a supervised backpropagation
multilayer neural network for classifying students into at-risk or not at-risk
group."
"In this paper, we present an empirical study on convergence nature of
Differential Evolution (DE) variants to solve unconstrained global optimization
problems. The aim is to identify the competitive nature of DE variants in
solving the problem at their hand and compare. We have chosen fourteen
benchmark functions grouped by feature: unimodal and separable, unimodal and
nonseparable, multimodal and separable, and multimodal and nonseparable.
Fourteen variants of DE were implemented and tested on fourteen benchmark
problems for dimensions of 30. The competitiveness of the variants are
identified by the Mean Objective Function value, they achieved in 100 runs. The
convergence nature of the best and worst performing variants are analyzed by
measuring their Convergence Speed (Cs) and Quality Measure (Qm)."
"Ant Colony Optimization (ACO) is a very popular metaheuristic for solving
computationally hard combinatorial optimization problems. Runtime analysis of
ACO with respect to various pseudo-boolean functions and different graph based
combinatorial optimization problems has been taken up in recent years. In this
paper, we investigate the runtime behavior of an MMAS*(Max-Min Ant System) ACO
algorithm on some well known hypergraph covering problems that are NP-Hard. In
particular, we have addressed the Minimum Edge Cover problem, the Minimum
Vertex Cover problem and the Maximum Weak- Independent Set problem. The
influence of pheromone values and heuristic information on the running time is
analysed. The results indicate that the heuristic information has greater
impact towards improving the expected optimization time as compared to
pheromone values. For certain instances of hypergraphs, we show that the MMAS*
algorithm gives a constant order expected optimization time when the dominance
of heuristic information is suitably increased."
"A schema is a naturally defined subset of the space of fixed-length binary
strings. The Holland Schema Theorem gives a lower bound on the expected
fraction of a population in a schema after one generation of a simple genetic
algorithm. This paper gives formulas for the exact expected fraction of a
population in a schema after one generation of the simple genetic algorithm.
Holland's schema theorem has three parts, one for selection, one for crossover,
and one for mutation. The selection part is exact, whereas the crossover and
mutation parts are approximations. This paper shows how the crossover and
mutation parts can be made exact. Holland's schema theorem follows naturally as
a corollary. There is a close relationship between schemata and the
representation of the population in the Walsh basis. This relationship is used
in the derivation of the results, and can also make computation of the schema
averages more efficient. This paper gives a version of the Vose infinite
population model where crossover and mutation are separated into two functions
rather than a single ""mixing"" function."
"This paper analises distributed evolutionary computation based on the
Representational State Transfer (REST) protocol, which overlays a farming model
on evolutionary computation. An approach to evolutionary distributed
optimisation of multilayer perceptrons (MLP) using REST and language Perl has
been done. In these experiments, a master-slave based evolutionary algorithm
(EA) has been implemented, where slave processes evaluate the costly fitness
function (training a MLP to solve a classification problem). Obtained results
show that the parallel version of the developed programs obtains similar or
better results using much less time than the sequential version, obtaining a
good speedup."
"In this paper, a high-level comparison of both SOAP (Simple Object Access
Protocol) and REST (Representational State Transfer) is made. These are the two
main approaches for interfacing to the web with web services. Both approaches
are different and present some advantages and disadvantages for interfacing to
web services: SOAP is conceptually more difficult (has a steeper learning
curve) and more ""heavy-weight"" than REST, although it lacks of standards
support for security. In order to test their eficiency (in time), two
experiments have been performed using both technologies: a client-server model
implementation and a master-slave based genetic algorithm (GA). The results
obtained show clear differences in time between SOAP and REST implementations.
Although both techniques are suitable for developing parallel systems, SOAP is
heavier than REST, mainly due to the verbosity of SOAP communications (XML
increases the time taken to parse the messages)."
"We reconsider stochastic convergence analyses of particle swarm optimisation,
and point out that previously obtained parameter conditions are not always
sufficient to guarantee mean square convergence to a local optimum. We show
that stagnation can in fact occur for non-trivial configurations in non-optimal
parts of the search space, even for simple functions like SPHERE. The
convergence properties of the basic PSO may in these situations be detrimental
to the goal of optimisation, to discover a sufficiently good solution within
reasonable time. To characterise optimisation ability of algorithms, we suggest
the expected first hitting time (FHT), i.e., the time until a search point in
the vicinity of the optimum is visited. It is shown that a basic PSO may have
infinite expected FHT, while an algorithm introduced here, the Noisy PSO, has
finite expected FHT on some functions."
"This work presents, the classification of user activities such as Rest, Walk
and Run, on the basis of frequency component present in the acceleration data
in a wireless sensor network environment. As the frequencies of the above
mentioned activities differ slightly for different person, so it gives a more
accurate result. The algorithm uses just one parameter i.e. the frequency of
the body acceleration data of the three axes for classifying the activities in
a set of data. The algorithm includes a normalization step and hence there is
no need to set a different value of threshold value for magnitude for different
test person. The classification is automatic and done on a block by block
basis."
"This paper presents the designing of a neural network for the classification
of Human activity. A Triaxial accelerometer sensor, housed in a chest worn
sensor unit, has been used for capturing the acceleration of the movements
associated. All the three axis acceleration data were collected at a base
station PC via a CC2420 2.4GHz ISM band radio (zigbee wireless compliant),
processed and classified using MATLAB. A neural network approach for
classification was used with an eye on theoretical and empirical facts. The
work shows a detailed description of the designing steps for the classification
of human body acceleration data. A 4-layer back propagation neural network,
with Levenberg-marquardt algorithm for training, showed best performance among
the other neural network training algorithms."
"Artificial Neural Networks (ANN) comprise important symmetry properties,
which can influence the performance of Monte Carlo methods in Neuroevolution.
The problem of the symmetries is also known as the competing conventions
problem or simply as the permutation problem. In the literature, symmetries are
mainly addressed in Genetic Algoritm based approaches. However, investigations
in this direction based on other Evolutionary Algorithms (EA) are rare or
missing. Furthermore, there are different and contradictionary reports on the
efficacy of symmetry breaking. By using a novel viewpoint, we offer a possible
explanation for this issue. As a result, we show that a strategy which is
invariant to the global optimum can only be successfull on certain problems,
whereas it must fail to improve the global convergence on others. We introduce
the \emph{Minimum Global Optimum Proximity} principle as a generalized and
adaptive strategy to symmetry breaking, which depends on the location of the
global optimum. We apply the proposed principle to Differential Evolution (DE)
and Covariance Matrix Adaptation Evolution Strategies (CMA-ES), which are two
popular and conceptually different global optimization methods. Using a wide
range of feedforward ANN problems, we experimentally illustrate significant
improvements in the global search efficiency by the proposed symmetry breaking
technique."
"Evolutionary algorithms (EAs), simulating the evolution process of natural
species, are used to solve optimization problems. Crossover (also called
recombination), originated from simulating the chromosome exchange phenomena in
zoogamy reproduction, is widely employed in EAs to generate offspring
solutions, of which the effectiveness has been examined empirically in
applications. However, due to the irregularity of crossover operators and the
complicated interactions to mutation, crossover operators are hard to analyze
and thus have few theoretical results. Therefore, analyzing crossover not only
helps in understanding EAs, but also helps in developing novel techniques for
analyzing sophisticated metaheuristic algorithms.
  In this paper, we derive the General Markov Chain Switching Theorem (GMCST)
to facilitate theoretical studies of crossover-enabled EAs. The theorem allows
us to analyze the running time of a sophisticated EA from an easy-to-analyze
EA. Using this tool, we analyze EAs with several crossover operators on the
LeadingOnes and OneMax problems, which are noticeably two well studied problems
for mutation-only EAs but with few results for crossover-enabled EAs. We first
derive the bounds of running time of the (2+2)-EA with crossover operators;
then we study the running time gap between the mutation-only (2:2)-EA and the
(2:2)-EA with crossover operators; finally, we develop strategies that apply
crossover operators only when necessary, which improve from the mutation-only
as well as the crossover-all-the-time (2:2)-EA. The theoretical results are
verified by experiments."
"The quadratic assignment problem (QAP) is one of the most difficult
combinatorial optimization problems. One of the most powerful and commonly used
heuristics to obtain approximations to the optimal solution of the QAP is
simulated annealing (SA). We present an efficient implementation of the SA
heuristic which performs more than 100 times faster then existing
implementations for large problem sizes and a large number of SA iterations."
"Power dissipation in sequential circuits is due to increased toggling count
of Circuit under Test, which depends upon test vectors applied. If successive
test vectors sequences have more toggling nature then it is sure that toggling
rate of flip flops is higher. Higher toggling for flip flops results more power
dissipation. To overcome this problem, one method is to use GA to have test
vectors of high fault coverage in short interval, followed by Hamming distance
management on test patterns. This approach is time consuming and needs more
efforts. Another method which is purposed in this paper is a PSO based Frame
Work to optimize power dissipation. Here target is to set the entire test
vector in a frame for time period 'T', so that the frame consists of all those
vectors strings which not only provide high fault coverage but also arrange
vectors in frame to produce minimum toggling."
"In the last decades, complex networks theory significantly influenced other
disciplines on the modeling of both static and dynamic aspects of systems
observed in nature. This work aims to investigate the effects of networks'
topological features on the dynamics of an evolutionary algorithm, considering
in particular the ability to find a large number of optima on multi-modal
problems. We introduce a novel spatially-structured evolutionary algorithm and
we apply it on two combinatorial problems: ONEMAX and the multi-modal NMAX.
Considering three different network models we investigate the relationships
between their features, algorithm's convergence and its ability to find
multiple optima (for the multi-modal problem). In order to perform a deeper
analysis we investigate the introduction of weighted graphs with time-varying
weights. The results show that networks with a large Average Path Length lead
to an higher number of optima and a consequent slow exploration dynamics (i.e.
low First Hitting Time). Furthermore, the introduction of weighted networks
shows the possibility to tune algorithm's dynamics during its execution with
the parameter related with weights' change. This work gives a first answer
about the effects of various graph topologies on the diversity of evolutionary
algorithms and it describes a simple but powerful algorithmic framework which
allows to investigate many aspects of ssEAs dynamics."
"Nowadays hybrid evolutionary algorithms, i.e, heuristic search algorithms
combining several mutation operators some of which are meant to implement
stochastically a well known technique designed for the specific problem in
question while some others playing the role of random search, have become
rather popular for tackling various NP-hard optimization problems. While
empirical studies demonstrate that hybrid evolutionary algorithms are
frequently successful at finding solutions having fitness sufficiently close to
the optimal, many fewer articles address the computational complexity in a
mathematically rigorous fashion. This paper is devoted to a mathematically
motivated design and analysis of a parameterized family of evolutionary
algorithms which provides a polynomial time approximation scheme for one of the
well-known NP-hard combinatorial optimization problems, namely the ""single
machine scheduling problem without precedence constraints"". The authors hope
that the techniques and ideas developed in this article may be applied in many
other situations."
"The all-pairs shortest path problem is the first non-artificial problem for
which it was shown that adding crossover can significantly speed up a
mutation-only evolutionary algorithm. Recently, the analysis of this algorithm
was refined and it was shown to have an expected optimization time (w.r.t. the
number of fitness evaluations) of $\Theta(n^{3.25}(\log n)^{0.25})$.
  In contrast to this simple algorithm, evolutionary algorithms used in
practice usually employ refined recombination strategies in order to avoid the
creation of infeasible offspring. We study extensions of the basic algorithm by
two such concepts which are central in recombination, namely \emph{repair
mechanisms} and \emph{parent selection}. We show that repairing infeasible
offspring leads to an improved expected optimization time of
$\mathord{O}(n^{3.2}(\log n)^{0.2})$. As a second part of our study we prove
that choosing parents that guarantee feasible offspring results in an even
better optimization time of $\mathord{O}(n^{3}\log n)$.
  Both results show that already simple adjustments of the recombination
operator can asymptotically improve the runtime of evolutionary algorithms."
"A significantly under-explored area of evolutionary optimization in the
literature is the study of optimization methodologies that can evolve along
with the problems solved. Particularly, present evolutionary optimization
approaches generally start their search from scratch or the ground-zero state
of knowledge, independent of how similar the given new problem of interest is
to those optimized previously. There has thus been the apparent lack of
automated knowledge transfers and reuse across problems. Taking the cue, this
paper introduces a novel Memetic Computational Paradigm for search, one that
models after how human solves problems, and embarks on a study towards
intelligent evolutionary optimization of problems through the transfers of
structured knowledge in the form of memes learned from previous problem-solving
experiences, to enhance future evolutionary searches. In particular, the
proposed memetic search paradigm is composed of four culture-inspired
operators, namely, Meme Learning, Meme Selection, Meme Variation and Meme
Imitation. The learning operator mines for memes in the form of latent
structures derived from past experiences of problem-solving. The selection
operator identifies the fit memes that replicate and transmit across problems,
while the variation operator introduces innovations into the memes. The
imitation operator, on the other hand, defines how fit memes assimilate into
the search process of newly encountered problems, thus gearing towards
efficient and effective evolutionary optimization. Finally, comprehensive
studies on two widely studied challenging well established NP-hard routing
problem domains, particularly, the capacitated vehicle routing (CVR) and
capacitated arc routing (CAR), confirm the high efficacy of the proposed
memetic computational search paradigm for intelligent evolutionary optimization
of problems."
"Partial classification popularly known as nugget discovery comes under
descriptive knowledge discovery. It involves mining rules for a target class of
interest. Classification ""If-Then"" rules are the most sought out by decision
makers since they are the most comprehensible form of knowledge mined by data
mining techniques. The rules have certain properties namely the rule metrics
which are used to evaluate them. Mining rules with user specified properties
can be considered as a multi-objective optimization problem since the rules
have to satisfy more than one property to be used by the user. Cultural
algorithm (CA) with its knowledge sources have been used in solving many
optimization problems. However research gap exists in using cultural algorithm
for multi-objective optimization of rules. In the current study a
multi-objective cultural algorithm is proposed for partial classification.
Results of experiments on benchmark data sets reveal good performance."
"The last decade has seen the parallel emergence in computational neuroscience
and machine learning of neural network structures which spread the input signal
randomly to a higher dimensional space; perform a nonlinear activation; and
then solve for a regression or classification output by means of a mathematical
pseudoinverse operation. In the field of neuromorphic engineering, these
methods are increasingly popular for synthesizing biologically plausible neural
networks, but the ""learning method"" - computation of the pseudoinverse by
singular value decomposition - is problematic both for biological plausibility
and because it is not an online or an adaptive method. We present an online or
incremental method of computing the pseudoinverse, which we argue is
biologically plausible as a learning method, and which can be made adaptable
for non-stationary data streams. The method is significantly more
memory-efficient than the conventional computation of pseudoinverses by
singular value decomposition."
"We have employed a recent implementation of genetic algorithms to study a
range of standard benchmark functions for global optimization. It turns out
that some of them are not very useful as challenging test functions, since they
neither allow for a discrimination between different variants of genetic
operators nor exhibit a dimensionality scaling resembling that of real-world
problems, for example that of global structure optimization of atomic and
molecular clusters. The latter properties seem to be simulated better by two
other types of benchmark functions. One type is designed to be deceptive,
exemplified here by Lunacek's function. The other type offers additional
advantages of markedly increased complexity and of broad tunability in search
space characteristics. For the latter type, we use an implementation based on
randomly distributed Gaussians. We advocate the use of the latter types of test
functions for algorithm development and benchmarking."
"VEGAS (Varying Evolvability-Guided Adaptive Search) is a new methodology
proposed to deal with the neutrality property of some optimization problems. ts
main feature is to consider the whole neutral network rather than an arbitrary
solution. Moreover, VEGAS is designed to escape from plateaus based on the
evolvability of solution and a multi-armed bandit. Experiments are conducted on
NK-landscapes with neutrality. Results show the importance of considering the
whole neutral network and of guiding the search cleverly. The impact of the
level of neutrality and of the exploration-exploitation trade-off are deeply
analyzed."
"The field of evolutionary computation is inspired by the achievements of
natural evolution, in which there is no final objective. Yet the pursuit of
objectives is ubiquitous in simulated evolution. A significant problem is that
objective approaches assume that intermediate stepping stones will increasingly
resemble the final objective when in fact they often do not. The consequence is
that while solutions may exist, searching for such objectives may not discover
them. This paper highlights the importance of leveraging human insight during
search as an alternative to articulating explicit objectives. In particular, a
new approach called novelty-assisted interactive evolutionary computation
(NA-IEC) combines human intuition with novelty search for the first time to
facilitate the serendipitous discovery of agent behaviors. In this approach,
the human user directs evolution by selecting what is interesting from the
on-screen population of behaviors. However, unlike in typical IEC, the user can
now request that the next generation be filled with novel descendants. The
experimental results demonstrate that combining human insight with novelty
search finds solutions significantly faster and at lower genomic complexities
than fully-automated processes, including pure novelty search, suggesting an
important role for human users in the search for solutions."
"Background: In recent years automated data analysis techniques have drawn
great attention and are used in almost every field of research including
biomedical. Artificial Neural Networks (ANNs) are one of the Computer- Aided-
Diagnosis tools which are used extensively by advances in computer hardware
technology. The application of these techniques for disease diagnosis has made
great progress and is widely used by physicians. An Electrocardiogram carries
vital information about heart activity and physicians use this signal for
cardiac disease diagnosis which was the great motivation towards our study.
Methods: In this study we are using Probabilistic Neural Networks (PNN) as an
automatic technique for ECG signal analysis along with a Genetic Algorithm
(GA). As every real signal recorded by the equipment can have different
artifacts, we need to do some preprocessing steps before feeding it to the ANN.
Wavelet transform is used for extracting the morphological parameters and
median filter for data reduction of the ECG signal. The subset of morphological
parameters are chosen and optimized using GA. We had two approaches in our
investigation, the first one uses the whole signal with 289 normalized and
de-noised data points as input to the ANN. In the second approach after
applying all the preprocessing steps the signal is reduced to 29 data points
and also their important parameters extracted to form the ANN input with 35
data points. Results: The outcome of the two approaches for 8 types of
arrhythmia shows that the second approach is superior than the first one with
an average accuracy of %99.42."
"Two metaheuristic algorithms namely Artificial Immune Systems (AIS) and
Genetic Algorithms are classified as computational systems inspired by
theoretical immunology and genetics mechanisms. In this work we examine the
comparative performances of two algorithms. A special selection algorithm,
Clonal Selection Algorithm (CLONALG), which is a subset of Artificial Immune
Systems, and Genetic Algorithms are tested with certain benchmark functions. It
is shown that depending on type of a function Clonal Selection Algorithm and
Genetic Algorithm have better performance over each other."
"The paper describes some recent developments in neural networks and discusses
the applicability of neural networks in the development of a machine that
mimics the human brain. The paper mentions a new architecture, the pulsed
neural network that is being considered as the next generation of neural
networks. The paper also explores the use of memristors in the development of a
brain-like computer called the MoNETA. A new model, multi/infinite dimensional
neural networks, are a recent development in the area of advanced neural
networks. The paper concludes that the need of neural networks in the
development of human-like technology is essential and may be non-expendable for
it."
"A Neural Network, in general, is not considered to be a good solver of
mathematical and binary arithmetic problems. However, networks have been
developed for such problems as the XOR circuit. This paper presents a technique
for the implementation of the Half-adder circuit using the CoActive Neuro-Fuzzy
Inference System (CANFIS) Model and attempts to solve the problem using the
NeuroSolutions 5 Simulator. The paper gives the experimental results along with
the interpretations and possible applications of the technique."
"In equality-constrained optimization, a standard regularity assumption is
often associated with feasible point methods, namely the gradients of
constraints are linearly independent. In practice, the regularity assumption
may be violated. To avoid such a singularity, we propose a new projection
matrix, based on which a feasible point method for the continuous-time,
equality-constrained optimization problem is developed. First, the equality
constraint is transformed into a continuous-time dynamical system with
solutions that always satisfy the equality constraint. Then, the singularity is
explained in detail and a new projection matrix is proposed to avoid
singularity. An update (or say a controller) is subsequently designed to
decrease the objective function along the solutions of the transformed system.
The invariance principle is applied to analyze the behavior of the solution. We
also propose a modified approach for addressing cases in which solutions do not
satisfy the equality constraint. Finally, the proposed optimization approaches
are applied to two examples to demonstrate its effectiveness."
"The Traveling Salesman Problem (TSP) is one of the most famous optimization
problems. Greedy crossover designed by Greffenstette et al, can be used while
Symmetric TSP (STSP) is resolved by Genetic Algorithm (GA). Researchers have
proposed several versions of greedy crossover. Here we propose improved version
of it. We compare our greedy crossover with some of recent crossovers, we use
our greedy crossover and some recent crossovers in GA then compare crossovers
on speed and accuracy."
"Self-delimiting (SLIM) programs are a central concept of theoretical computer
science, particularly algorithmic information & probability theory, and
asymptotically optimal program search (AOPS). To apply AOPS to (possibly
recurrent) neural networks (NNs), I introduce SLIM NNs. Neurons of a typical
SLIM NN have threshold activation functions. During a computational episode,
activations are spreading from input neurons through the SLIM NN until the
computation activates a special halt neuron. Weights of the NN's used
connections define its program. Halting programs form a prefix code. The reset
of the initial NN state does not cost more than the latest program execution.
Since prefixes of SLIM programs influence their suffixes (weight changes
occurring early in an episode influence which weights are considered later),
SLIM NN learning algorithms (LAs) should execute weight changes online during
activation spreading. This can be achieved by applying AOPS to growing SLIM
NNs. To efficiently teach a SLIM NN to solve many tasks, such as correctly
classifying many different patterns, or solving many different robot control
tasks, each connection keeps a list of tasks it is used for. The lists may be
efficiently updated during training. To evaluate the overall effect of
currently tested weight changes, a SLIM NN LA needs to re-test performance only
on the efficiently computable union of tasks potentially affected by the
current weight changes. Future SLIM NNs will be implemented on 3-dimensional
brain-like multi-processor hardware. Their LAs will minimize task-specific
total wire length of used connections, to encourage efficient solutions of
subtasks by subsets of neurons that are physically close. The novel class of
SLIM NN LAs is currently being probed in ongoing experiments to be reported in
separate papers."
"A significant challenge in nature-inspired algorithmics is the identification
of specific characteristics of problems that make them harder (or easier) to
solve using specific methods. The hope is that, by identifying these
characteristics, we may more easily predict which algorithms are best-suited to
problems sharing certain features. Here, we approach this problem using fitness
landscape analysis. Techniques already exist for measuring the ""difficulty"" of
specific landscapes, but these are often designed solely with evolutionary
algorithms in mind, and are generally specific to discrete optimisation. In
this paper we develop an approach for comparing a wide range of continuous
optimisation algorithms. Using a fitness landscape generation technique, we
compare six different nature-inspired algorithms and identify which methods
perform best on landscapes exhibiting specific features."
"We introduce a dynamic neural algorithm called Dynamic Neural (DN)
SARSA(\lambda) for learning a behavioral sequence from delayed reward.
DN-SARSA(\lambda) combines Dynamic Field Theory models of behavioral sequence
representation, classical reinforcement learning, and a computational
neuroscience model of working memory, called Item and Order working memory,
which serves as an eligibility trace. DN-SARSA(\lambda) is implemented on both
a simulated and real robot that must learn a specific rewarding sequence of
elementary behaviors from exploration. Results show DN-SARSA(\lambda) performs
on the level of the discrete SARSA(\lambda), validating the feasibility of
general reinforcement learning without compromising neural dynamics."
"Packing problems are in general NP-hard, even for simple cases. Since now
there are no highly efficient algorithms available for solving packing
problems. The two-dimensional bin packing problem is about packing all given
rectangular items, into a minimum size rectangular bin, without overlapping.
The restriction is that the items cannot be rotated. The current paper is
comparing a greedy algorithm with a hybrid genetic algorithm in order to see
which technique is better for the given problem. The algorithms are tested on
different sizes data."
"This paper presents an implementation of multilayer feed forward neural
networks (NN) to optimize CMOS analog circuits. For modeling and design
recently neural network computational modules have got acceptance as an
unorthodox and useful tool. To achieve high performance of active or passive
circuit component neural network can be trained accordingly. A well trained
neural network can produce more accurate outcome depending on its learning
capability. Neural network model can replace empirical modeling solutions
limited by range and accuracy.[2] Neural network models are easy to obtain for
new circuits or devices which can replace analytical methods. Numerical
modeling methods can also be replaced by neural network model due to their
computationally expansive behavior.[2][10][20]. The pro- posed implementation
is aimed at reducing resource requirement, without much compromise on the
speed. The NN ensures proper functioning by assigning the appropriate inputs,
weights, biases, and excitation function of the layer that is currently being
computed. The concept used is shown to be very effective in reducing resource
requirements and enhancing speed."
"An experimental evaluation is conducted to asses the performance of 4
different Particle Swarm Optimization neighborhood structures in solving
Max-Sat problem. The experiment has shown that none of the algorithms achieves
statistically significant performance over the others under confidence level of
0.05."
"Standard neural network based on general back propagation learning using
delta method or gradient descent method has some great faults like poor
optimization of error-weight objective function, low learning rate, instability
.This paper introduces a hybrid supervised back propagation learning algorithm
which uses trust-region method of unconstrained optimization of the error
objective function by using quasi-newton method .This optimization leads to
more accurate weight update system for minimizing the learning error during
learning phase of multi-layer perceptron.[13][14][15] In this paper augmented
line search is used for finding points which satisfies Wolfe condition. In this
paper, This hybrid back propagation algorithm has strong global convergence
properties & is robust & efficient in practice."
"This paper presents the coupling of a building thermal simulation code with
genetic algorithms (GAs). GAs are randomized search algorithms that are based
on the mechanisms of natural selection and genetics. We show that this coupling
allows the location of defective sub-models of a building thermal model i.e.
parts of model that are responsible for the disagreements between measurements
and model predictions. The method first of all is checked and validated on the
basis of a numerical model of a building taken as reference. It is then applied
to a real building case. The results show that the method could constitute an
efficient tool when checking the model validity."
"In this paper, we consider situations in which a given logical function is
realized by a multithreshold threshold function. In such situations, constant
functions can be easily obtained from multithreshold threshold functions, and
therefore, we can show that it becomes possible to optimize a class of
high-order neural networks. We begin by proposing a generating method for
threshold functions in which we use a vector that determines the boundary
between the linearly separable function and the high-order threshold function.
By applying this method to high-order threshold functions, we show that
functions with the same weight as, but a different threshold than, a threshold
function generated by the generation process can be easily obtained. We also
show that the order of the entire network can be extended while maintaining the
structure of given functions."
"Radio Spectrum is most precious and scarce resource and must be utilized
efficiently and effectively. Cognitive radio is the promising solutions for the
optimum utilization of the scared natural resource. The spectrum owned by the
primary user should be shared among the secondary user, but primary user should
not be interfered by the secondary user. In order to utilize the primary user
spectrum, secondary user must detect accurately, the existence of primary in
the band of interest. In cooperative spectrum sensing, the channel between the
secondary users and the cognitive radio base station is non stationary and
causes interference in the decision in decision fusion and in information in
information due to multipath fading. In this paper neural network based
cooperative spectrum sensing method is proposed, the performance of proposed
method is evaluated and observed that, the neural network based scheme
performance improve significantly over the AND,OR and Majority rule"
"Evolutionary algorithms are good general problem solver but suffer from a
lack of domain specific knowledge. However, the problem specific knowledge can
be added to evolutionary algorithms by hybridizing. Interestingly, all the
elements of the evolutionary algorithms can be hybridized. In this chapter, the
hybridization of the three elements of the evolutionary algorithms is
discussed: the objective function, the survivor selection operator and the
parameter settings. As an objective function, the existing heuristic function
that construct the solution of the problem in traditional way is used. However,
this function is embedded into the evolutionary algorithm that serves as a
generator of new solutions. In addition, the objective function is improved by
local search heuristics. The new neutral selection operator has been developed
that is capable to deal with neutral solutions, i.e. solutions that have the
different representation but expose the equal values of objective function. The
aim of this operator is to directs the evolutionary search into a new
undiscovered regions of the search space. To avoid of wrong setting of
parameters that control the behavior of the evolutionary algorithm, the
self-adaptation is used. Finally, such hybrid self-adaptive evolutionary
algorithm is applied to the two real-world NP-hard problems: the graph
3-coloring and the optimization of markers in the clothing industry. Extensive
experiments shown that these hybridization improves the results of the
evolutionary algorithms a lot. Furthermore, the impact of the particular
hybridizations is analyzed in details as well."
"This paper proposes a hybrid self-adaptive evolutionary algorithm for graph
coloring that is hybridized with the following novel elements: heuristic
genotype-phenotype mapping, a swap local search heuristic, and a neutral
survivor selection operator. This algorithm was compared with the evolutionary
algorithm with the SAW method of Eiben et al., the Tabucol algorithm of Hertz
and de Werra, and the hybrid evolutionary algorithm of Galinier and Hao. The
performance of these algorithms were tested on a test suite consisting of
randomly generated 3-colorable graphs of various structural features, such as
graph size, type, edge density, and variability in sizes of color classes.
Furthermore, the test graphs were generated including the phase transition
where the graphs are hard to color. The purpose of the extensive experimental
work was threefold: to investigate the behavior of the tested algorithms in the
phase transition, to identify what impact hybridization with the DSatur
traditional heuristic has on the evolutionary algorithm, and to show how graph
structural features influence the performance of the graph-coloring algorithms.
The results indicate that the performance of the hybrid self-adaptive
evolutionary algorithm is comparable with, or better than, the performance of
the hybrid evolutionary algorithm which is one of the best graph-coloring
algorithms today. Moreover, the fact that all the considered algorithms
performed poorly on flat graphs confirms that this type of graphs is really the
hardest to color."
"The recognition of unconstrained handwriting continues to be a difficult task
for computers despite active research for several decades. This is because
handwritten text offers great challenges such as character and word
segmentation, character recognition, variation between handwriting styles,
different character size and no font constraints as well as the background
clarity. In this paper primarily discussed Online Handwriting Recognition
methods for Arabic words which being often used among then across the Middle
East and North Africa people. Because of the characteristic of the whole body
of the Arabic words, namely connectivity between the characters, thereby the
segmentation of An Arabic word is very difficult. We introduced a recurrent
neural network to online handwriting Arabic word recognition. The key
innovation is a recently produce recurrent neural networks objective function
known as connectionist temporal classification. The system consists of an
advanced recurrent neural network with an output layer designed for sequence
labeling, partially combined with a probabilistic language model. Experimental
results show that unconstrained Arabic words achieve recognition rates about
79%, which is significantly higher than the about 70% using a previously
developed hidden markov model based recognition system."
"In this paper, after analyzing the reasons of poor generalization and
overfitting in neural networks, we consider some noise data as a singular value
of a continuous function - jump discontinuity point. The continuous part can be
approximated with the simplest neural networks, which have good generalization
performance and optimal network architecture, by traditional algorithms such as
constructive algorithm for feed-forward neural networks with incremental
training, BP algorithm, ELM algorithm, various constructive algorithm, RBF
approximation and SVM. At the same time, we will construct RBF neural networks
to fit the singular value with every error in, and we prove that a function
with jumping discontinuity points can be approximated by the simplest neural
networks with a decay RBF neural networks in by each error, and a function with
jumping discontinuity point can be constructively approximated by a decay RBF
neural networks in by each error and the constructive part have no
generalization influence to the whole machine learning system which will
optimize neural network architecture and generalization performance, reduce the
overfitting phenomenon by avoid fitting the noisy data."
"Memristors are used to compare three gathering techniques in an
already-mapped environment where resource locations are known. The All Site
model, which apportions gatherers based on the modeled memristance of that
path, proves to be good at increasing overall efficiency and decreasing time to
fully deplete an environment, however it only works well when the resources are
of similar quality. The Leaf Cutter method, based on Leaf Cutter Ant behaviour,
assigns all gatherers first to the best resource, and once depleted, uses the
All Site model to spread them out amongst the rest. The Leaf Cutter model is
better at increasing resource influx in the short-term and vastly out-performs
the All Site model in a more varied environments. It is demonstrated that
memristor based abstractions of gatherer models provide potential methods for
both the comparison and implementation of agent controls."
"We consider the problem of neural association for a network of non-binary
neurons. Here, the task is to first memorize a set of patterns using a network
of neurons whose states assume values from a finite number of integer levels.
Later, the same network should be able to recall previously memorized patterns
from their noisy versions. Prior work in this area consider storing a finite
number of purely random patterns, and have shown that the pattern retrieval
capacities (maximum number of patterns that can be memorized) scale only
linearly with the number of neurons in the network.
  In our formulation of the problem, we concentrate on exploiting redundancy
and internal structure of the patterns in order to improve the pattern
retrieval capacity. Our first result shows that if the given patterns have a
suitable linear-algebraic structure, i.e. comprise a sub-space of the set of
all possible patterns, then the pattern retrieval capacity is in fact
exponential in terms of the number of neurons. The second result extends the
previous finding to cases where the patterns have weak minor components, i.e.
the smallest eigenvalues of the correlation matrix tend toward zero. We will
use these minor components (or the basis vectors of the pattern null space) to
both increase the pattern retrieval capacity and error correction capabilities.
  An iterative algorithm is proposed for the learning phase, and two simple
neural update algorithms are presented for the recall phase. Using analytical
results and simulations, we show that the proposed methods can tolerate a fair
amount of errors in the input while being able to memorize an exponentially
large number of patterns."
"Simulated landscapes have been used for decades to evaluate search strategies
whose goal is to find the landscape location with maximum fitness. Applications
include modeling the capacity of enzymes to catalyze reactions and the clinical
effectiveness of medical treatments. Understanding properties of landscapes is
important for understanding search difficulty. This paper presents a novel and
transparent characterization of NK landscapes.
  We prove that NK landscapes can be represented by parametric linear
interaction models where model coefficients have meaningful interpretations. We
derive the statistical properties of the model coefficients, providing insight
into how the NK algorithm parses importance to main effects and interactions.
An important insight derived from the linear model representation is that the
rank of the linear model defined by the NK algorithm is correlated with the
number of local optima, a strong determinant of landscape complexity and search
difficulty. We show that the maximal rank for an NK landscape is achieved
through epistatic interactions that form partially balanced incomplete block
designs. Finally, an analytic expression representing the expected number of
local optima on the landscape is derived, providing a way to quickly compute
the expected number of local optima for very large landscapes."
"Industrial pollution is often considered to be one of the prime factors
contributing to air, water and soil pollution. Sectoral pollution loads
(ton/yr) into different media (i.e. air, water and land) in Lagos were
estimated using Industrial Pollution Projected System (IPPS). These were
further studied using Artificial neural Networks (ANNs), a data mining
technique that has the ability of detecting and describing patterns in large
data sets with variables that are non- linearly related. Time Lagged Recurrent
Network (TLRN) appeared as the best Neural Network model among all the neural
networks considered which includes Multilayer Perceptron (MLP) Network,
Generalized Feed Forward Neural Network (GFNN), Radial Basis Function (RBF)
Network and Recurrent Network (RN). TLRN modelled the data-sets better than the
others in terms of the mean average error (MAE) (0.14), time (39 s) and linear
correlation coefficient (0.84). The results showed that Artificial Neural
Networks (ANNs) technique (i.e., Time Lagged Recurrent Network) is also
applicable and effective in environmental assessment study. Keywords:
Artificial Neural Networks (ANNs), Data Mining Techniques, Industrial Pollution
Projection System (IPPS), Pollution load, Pollution Intensity."
"Positron Emission Tomography (PET) scan images are one of the bio medical
imaging techniques similar to that of MRI scan images but PET scan images are
helpful in finding the development of tumors.The PET scan images requires
expertise in the segmentation where clustering plays an important role in the
automation process.The segmentation of such images is manual to automate the
process clustering is used.Clustering is commonly known as unsupervised
learning process of n dimensional data sets are clustered into k groups so as
to maximize the inter cluster similarity and to minimize the intra cluster
similarity.This paper is proposed to implement the commonly used K Means and
Fuzzy CMeans (FCM) clustering algorithm.This work is implemented using MATrix
LABoratory (MATLAB) and tested with sample PET scan image. The sample data is
collected from Alzheimers Disease Neuro imaging Initiative ADNI. Medical Image
Processing and Visualization Tool (MIPAV) are used to compare the resultant
images."
"Seasonality is a distinctive characteristic which is often observed in many
practical time series. Artificial Neural Networks (ANNs) are a class of
promising models for efficiently recognizing and forecasting seasonal patterns.
In this paper, the Particle Swarm Optimization (PSO) approach is used to
enhance the forecasting strengths of feedforward ANN (FANN) as well as Elman
ANN (EANN) models for seasonal data. Three widely popular versions of the basic
PSO algorithm, viz. Trelea-I, Trelea-II and Clerc-Type1 are considered here.
The empirical analysis is conducted on three real-world seasonal time series.
Results clearly show that each version of the PSO algorithm achieves notably
better forecasting accuracies than the standard Backpropagation (BP) training
method for both FANN and EANN models. The neural network forecasting results
are also compared with those from the three traditional statistical models,
viz. Seasonal Autoregressive Integrated Moving Average (SARIMA), Holt-Winters
(HW) and Support Vector Machine (SVM). The comparison demonstrates that both
PSO and BP based neural networks outperform SARIMA, HW and SVM models for all
three time series datasets. The forecasting performances of ANNs are further
improved through combining the outputs from the three PSO based models."
"This paper examines the effect of mimicking discontinuous heredity caused by
carrying more than one chromosome in some living organisms cells in
Evolutionary Multi-Objective Optimization algorithms. In this representation,
the phenotype may not fully reflect the genotype. By doing so we are mimicking
living organisms inheritance mechanism, where traits may be silently carried
for many generations to reappear later. Representations with different number
of chromosomes in each solution vector are tested on different benchmark
problems with high number of decision variables and objectives. A comparison
with Non-Dominated Sorting Genetic Algorithm-II is done on all problems."
"This paper presents a new technique for induction motor parameter
identification. The proposed technique is based on a simple startup test using
a standard V/F inverter. The recorded startup currents are compared to that
obtained by simulation of an induction motor model. A Modified PSO optimization
is used to find out the best model parameter that minimizes the sum square
error between the measured and the simulated currents. The performance of the
modified PSO is compared with other optimization methods including line search,
conventional PSO and Genetic Algorithms. Simulation results demonstrate the
ability of the proposed technique to capture the true values of the machine
parameters and the superiority of the results obtained using the modified PSO
over other optimization techniques."
"This paper introduces a new dynamic neighborhood network for particle swarm
optimization. In the proposed Clubs-based Particle Swarm Optimization (C-PSO)
algorithm, each particle initially joins a default number of what we call
'clubs'. Each particle is affected by its own experience and the experience of
the best performing member of the clubs it is a member of. Clubs membership is
dynamic, where the worst performing particles socialize more by joining more
clubs to learn from other particles and the best performing particles are made
to socialize less by leaving clubs to reduce their strong influence on other
members. Particles return gradually to default membership level when they stop
showing extreme performance. Inertia weights of swarm members are made random
within a predefined range. This proposed dynamic neighborhood algorithm is
compared with other two algorithms having static neighborhood topologies on a
set of classic benchmark problems. The results showed superior performance for
C-PSO regarding escaping local optima and convergence speed."
"Evolutionary computation techniques have mostly been used to solve various
optimization and learning problems successfully. Evolutionary algorithm is more
effective to gain optimal solution(s) to solve complex problems than
traditional methods. In case of problems with large set of parameters,
evolutionary computation technique incurs a huge computational burden for a
single processing unit. Taking this limitation into account, this paper
presents a new distributed evolutionary computation technique, which decomposes
decision vectors into smaller components and achieves optimal solution in a
short time. In this technique, a Jacobi-based Time Variant Adaptive (JBTVA)
Hybrid Evolutionary Algorithm is distributed incorporating cluster computation.
Moreover, two new selection methods named Best All Selection (BAS) and Twin
Selection (TS) are introduced for selecting best fit solution vector.
Experimental results show that optimal solution is achieved for different kinds
of problems having huge parameters and a considerable speedup is obtained in
proposed distributed system."
"This paper presented a genetic algorithm (GA) to solve the container storage
problem in the port. This problem is studied with different container types
such as regular, open side, open top, tank, empty and refrigerated containers.
The objective of this problem is to determine an optimal containers
arrangement, which respects customers delivery deadlines, reduces the rehandle
operations of containers and minimizes the stop time of the container ship. In
this paper, an adaptation of the genetic algorithm to the container storage
problem is detailed and some experimental results are presented and discussed.
The proposed approach was compared to a Last In First Out (LIFO) algorithm
applied to the same problem and has recorded good results"
"This paper proposes a generalized Hybrid Real-coded Quantum Evolutionary
Algorithm (HRCQEA) for optimizing complex functions as well as combinatorial
optimization. The main idea of HRCQEA is to devise a new technique for mutation
and crossover operators. Using the evolutionary equation of PSO a
Single-Multiple gene Mutation (SMM) is designed and the concept of Arithmetic
Crossover (AC) is used in the new Crossover operator. In HRCQEA, each triploid
chromosome represents a particle and the position of the particle is updated
using SMM and Quantum Rotation Gate (QRG), which can make the balance between
exploration and exploitation. Crossover is employed to expand the search space,
Hill Climbing Selection (HCS) and elitism help to accelerate the convergence
speed. Simulation results on Knapsack Problem and five benchmark complex
functions with high dimension show that HRCQEA performs better in terms of
ability to discover the global optimum and convergence speed."
"Prediction of annual rice production in all the 31 districts of Tamilnadu is
an important decision for the Government of Tamilnadu. Rice production is a
complex process and non linear problem involving soil, crop, weather, pest,
disease, capital, labour and management parameters. ANN software was designed
and developed with Feed Forward Back Propagation (FFBP) network to predict rice
production. The input layer has six independent variables like area of
cultivation and rice production in three seasons like Kuruvai, Samba and Kodai.
The popular sigmoid activation function was adopted to convert input data into
sigmoid values. The hidden layer computes the summation of six sigmoid values
with six sets of weightages. The final output was converted into sigmoid values
using a sigmoid transfer function. ANN outputs are the predicted results. The
error between original data and ANN output values were computed. A threshold
value of 10-9 was used to test whether the error is greater than the threshold
level. If the error is greater than threshold then updating of weights was done
all summations were done by back propagation. This process was repeated until
error equal to zero. The predicted results were printed and it was found to be
exactly matching with the expected values. It shows that the ANN prediction was
100% accurate."
"Stochastic, iterative search methods such as Evolutionary Algorithms (EAs)
are proven to be efficient optimizers. However, they require evaluation of the
candidate solutions which may be prohibitively expensive in many real world
optimization problems. Use of approximate models or surrogates is being
explored as a way to reduce the number of such evaluations. In this paper we
investigated three such methods. The first method (DAFHEA) partially replaces
an expensive function evaluation by its approximate model. The approximation is
realized with support vector machine (SVM) regression models. The second method
(DAFHEA II) is an enhancement on DAFHEA to accommodate for uncertain
environments. The third one uses surrogate ranking with preference learning or
ordinal regression. The fitness of the candidates is estimated by modeling
their rank. The techniques' performances on some of the benchmark numerical
optimization problems have been reported. The comparative benefits and
shortcomings of both techniques have been identified."
"Surrogate assisted evolutionary algorithms (EA) are rapidly gaining
popularity where applications of EA in complex real world problem domains are
concerned. Although EAs are powerful global optimizers, finding optimal
solution to complex high dimensional, multimodal problems often require very
expensive fitness function evaluations. Needless to say, this could brand any
population-based iterative optimization technique to be the most crippling
choice to handle such problems. Use of approximate model or surrogates provides
a much cheaper option. However, naturally this cheaper option comes with its
own price. This paper discusses some of the key issues involved with use of
approximation in evolutionary algorithm, possible best practices and solutions.
Answers to the following questions have been sought: what type of fitness
approximation to be used; which approximation model to use; how to integrate
the approximation model in EA; how much approximation to use; and how to ensure
reliable approximation."
"ROC is usually used to analyze the performance of classifiers in data mining.
ROC convex hull (ROCCH) is the least convex major-ant (LCM) of the empirical
ROC curve, and covers potential optima for the given set of classifiers.
Generally, ROC performance maximization could be considered to maximize the
ROCCH, which also means to maximize the true positive rate (tpr) and minimize
the false positive rate (fpr) for each classifier in the ROC space. However,
tpr and fpr are conflicting with each other in the ROCCH optimization process.
Though ROCCH maximization problem seems like a multi-objective optimization
problem (MOP), the special characters make it different from traditional MOP.
In this work, we will discuss the difference between them and propose convex
hull-based multi-objective genetic programming (CH-MOGP) to solve ROCCH
maximization problems. Convex hull-based sort is an indicator based selection
scheme that aims to maximize the area under convex hull, which serves as a
unary indicator for the performance of a set of points. A selection procedure
is described that can be efficiently implemented and follows similar design
principles than classical hyper-volume based optimization algorithms. It is
hypothesized that by using a tailored indicator-based selection scheme CH-MOGP
gets more efficient for ROC convex hull approximation than algorithms which
compute all Pareto optimal points. To test our hypothesis we compare the new
CH-MOGP to MOGP with classical selection schemes, including NSGA-II, MOEA/D)
and SMS-EMOA. Meanwhile, CH-MOGP is also compared with traditional machine
learning algorithms such as C4.5, Naive Bayes and Prie. Experimental results
based on 22 well-known UCI data sets show that CH-MOGP outperforms
significantly traditional EMOAs."
"Hybrid optimization algorithms have gained popularity as it has become
apparent there cannot be a universal optimization strategy which is globally
more beneficial than any other. Despite their popularity, hybridization
frameworks require more detailed categorization regarding: the nature of the
problem domain, the constituent algorithms, the coupling schema and the
intended area of application. This report proposes a hybrid algorithm for
solving small to large-scale continuous global optimization problems. It
comprises evolutionary computation (EC) algorithms and a sequential quadratic
programming (SQP) algorithm; combined in a collaborative portfolio. The SQP is
a gradient based local search method. To optimize the individual contributions
of the EC and SQP algorithms for the overall success of the proposed hybrid
system, improvements were made in key features of these algorithms. The report
proposes enhancements in: i) the evolutionary algorithm, ii) a new convergence
detection mechanism was proposed; and iii) in the methods for evaluating the
search directions and step sizes for the SQP local search algorithm. The
proposed hybrid design aim was to ensure that the two algorithms complement
each other by exploring and exploiting the problem search space. Preliminary
results justify that an adept hybridization of evolutionary algorithms with a
suitable local search method, could yield a robust and efficient means of
solving wide range of global optimization problems. Finally, a discussion of
the outcomes of the initial investigation and a review of the associated
challenges and inherent limitations of the proposed method is presented to
complete the investigation. The report highlights extensive research,
particularly, some potential case studies and application areas."
"Bilevel optimization problems are a class of challenging optimization
problems, which contain two levels of optimization tasks. In these problems,
the optimal solutions to the lower level problem become possible feasible
candidates to the upper level problem. Such a requirement makes the
optimization problem difficult to solve, and has kept the researchers busy
towards devising methodologies, which can efficiently handle the problem.
Despite the efforts, there hardly exists any effective methodology, which is
capable of handling a complex bilevel problem. In this paper, we introduce
bilevel evolutionary algorithm based on quadratic approximations (BLEAQ) of
optimal lower level variables with respect to the upper level variables. The
approach is capable of handling bilevel problems with different kinds of
complexities in relatively smaller number of function evaluations. Ideas from
classical optimization have been hybridized with evolutionary methods to
generate an efficient optimization algorithm for generic bilevel problems. The
efficacy of the algorithm has been shown on two sets of test problems. The
first set is a recently proposed SMD test set, which contains problems with
controllable complexities, and the second set contains standard test problems
collected from the literature. The proposed method has been evaluated against
two benchmarks, and the performance gain is observed to be significant."
"Swarm intelligence is a very powerful technique to be used for optimization
purposes. In this paper we present a new swarm intelligence algorithm, based on
the bat algorithm. The Bat algorithm is hybridized with differential evolution
strategies. Besides showing very promising results of the standard benchmark
functions, this hybridization also significantly improves the original bat
algorithm."
"Triplet-based Spike Timing Dependent Plasticity (TSTDP) is a powerful
synaptic plasticity rule that acts beyond conventional pair-based STDP (PSTDP).
Here, the TSTDP is capable of reproducing the outcomes from a variety of
biological experiments, while the PSTDP rule fails to reproduce them.
Additionally, it has been shown that the behaviour inherent to the spike
rate-based Bienenstock-Cooper-Munro (BCM) synaptic plasticity rule can also
emerge from the TSTDP rule. This paper proposes an analog implementation of the
TSTDP rule. The proposed VLSI circuit has been designed using the AMS 0.35 um
CMOS process and has been simulated using design kits for Synopsys and Cadence
tools. Simulation results demonstrate how well the proposed circuit can alter
synaptic weights according to the timing difference amongst a set of different
patterns of spikes. Furthermore, the circuit is shown to give rise to a
BCM-like learning rule, which is a rate-based rule. To mimic implementation
environment, a 1000 run Monte Carlo (MC) analysis was conducted on the proposed
circuit. The presented MC simulation analysis and the simulation result from
fine-tuned circuits show that, it is possible to mitigate the effect of process
variations in the proof of concept circuit, however, a practical variation
aware design technique is required to promise a high circuit performance in a
large scale neural network. We believe that the proposed design can play a
significant role in future VLSI implementations of both spike timing and rate
based neuromorphic learning systems."
"This paper presents a cumulative multi-niching genetic algorithm (CMN GA),
designed to expedite optimization problems that have computationally-expensive
multimodal objective functions. By never discarding individuals from the
population, the CMN GA makes use of the information from every objective
function evaluation as it explores the design space. A fitness-related
population density control over the design space reduces unnecessary objective
function evaluations. The algorithm's novel arrangement of genetic operations
provides fast and robust convergence to multiple local optima. Benchmark tests
alongside three other multi-niching algorithms show that the CMN GA has a
greater convergence ability and provides an order-of-magnitude reduction in the
number of objective function evaluations required to achieve a given level of
convergence."
"For simple digital circuits, conventional method of designing circuits can
easily be applied. But for complex digital circuits, the conventional method of
designing circuits is not fruitfully applicable because it is time-consuming.
On the contrary, Genetic Programming is used mostly for automatic program
generation. The modern approach for designing Arithmetic circuits, commonly
digital circuits, is based on Graphs. This graph-based evolutionary design of
arithmetic circuits is a method of optimized designing of arithmetic circuits.
In this paper, a new technique for evolutionary design of digital circuits is
proposed using Genetic Programming (GP) with Subtree Mutation in place of
Graph-based design. The results obtained using this technique demonstrates the
potential capability of genetic programming in digital circuit design with
limited computer algorithms. The proposed technique, helps to simplify and
speed up the process of designing digital circuits, discovers a variation in
the field of digital circuit design where optimized digital circuits can be
successfully and effectively designed."
"Now-a-days, it is important to find out solutions of Multi-Objective
Optimization Problems (MOPs). Evolutionary Strategy helps to solve such real
world problems efficiently and quickly. But sequential Evolutionary Algorithms
(EAs) require an enormous computation power to solve such problems and it takes
much time to solve large problems. To enhance the performance for solving this
type of problems, this paper presents a new Distributed Novel Evolutionary
Strategy Algorithm (DNESA) for Multi-Objective Optimization. The proposed DNESA
applies the divide-and-conquer approach to decompose population into smaller
sub-population and involves multiple solutions in the form of cooperative
sub-populations. In DNESA, the server distributes the total computation load to
all associate clients and simulation results show that the time for solving
large problems is much less than sequential EAs. Also DNESA shows better
performance in convergence test when compared with other three well-known EAs."
"This paper proposes a self-adaptation mechanism to manage the resources
allocated to the different species comprising a cooperative coevolutionary
algorithm. The proposed approach relies on a dynamic extension to the
well-known multi-armed bandit framework. At each iteration, the dynamic
multi-armed bandit makes a decision on which species to evolve for a
generation, using the history of progress made by the different species to
guide the decisions. We show experimentally, on a benchmark and a real-world
problem, that evolving the different populations at different paces allows not
only to identify solutions more rapidly, but also improves the capacity of
cooperative coevolution to solve more complex problems."
"Novelty search is a recent artificial evolution technique that challenges
traditional evolutionary approaches. In novelty search, solutions are rewarded
based on their novelty, rather than their quality with respect to a predefined
objective. The lack of a predefined objective precludes premature convergence
caused by a deceptive fitness function. In this paper, we apply novelty search
combined with NEAT to the evolution of neural controllers for homogeneous
swarms of robots. Our empirical study is conducted in simulation, and we use a
common swarm robotics task - aggregation, and a more challenging task - sharing
of an energy recharging station. Our results show that novelty search is
unaffected by deception, is notably effective in bootstrapping the evolution,
can find solutions with lower complexity than fitness-based evolution, and can
find a broad diversity of solutions for the same task. Even in non-deceptive
setups, novelty search achieves solution qualities similar to those obtained in
traditional fitness-based evolution. Our study also encompasses variants of
novelty search that work in concert with fitness-based evolution to combine the
exploratory character of novelty search with the exploitatory character of
objective-based evolution. We show that these variants can further improve the
performance of novelty search. Overall, our study shows that novelty search is
a promising alternative for the evolution of controllers for robotic swarms."
"Novelty search has shown to be a promising approach for the evolution of
controllers for swarm robotics. In existing studies, however, the experimenter
had to craft a domain dependent behaviour similarity measure to use novelty
search in swarm robotics applications. The reliance on hand-crafted similarity
measures places an additional burden to the experimenter and introduces a bias
in the evolutionary process. In this paper, we propose and compare two
task-independent, generic behaviour similarity measures: combined state count
and sampled average state. The proposed measures use the values of sensors and
effectors recorded for each individual robot of the swarm. The characterisation
of the group-level behaviour is then obtained by combining the sensor-effector
values from all the robots. We evaluate the proposed measures in an aggregation
task and in a resource sharing task. We show that the generic measures match
the performance of domain dependent measures in terms of solution quality. Our
results indicate that the proposed generic measures operate as effective
behaviour similarity measures, and that it is possible to leverage the benefits
of novelty search without having to craft domain specific similarity measures."
"Premature convergence is one of the important issues while using Genetic
Programming for data modeling. It can be avoided by improving population
diversity. Intelligent genetic operators can help to improve the population
diversity. Crossover is an important operator in Genetic Programming. So, we
have analyzed number of intelligent crossover operators and proposed an
algorithm with the modification of soft brood crossover operator. It will help
to improve the population diversity and reduce the premature convergence. We
have performed experiments on three different symbolic regression problems.
Then we made the performance comparison of our proposed crossover (Modified
Soft Brood Crossover) with the existing soft brood crossover and subtree
crossover operators."
"This paper represents the metaheuristics proposed for solving a class of Shop
Scheduling problem. The Bacterial Foraging Optimization algorithm is featured
with Ant Colony Optimization algorithm and proposed as a natural inspired
computing approach to solve the Mixed Shop Scheduling problem. The Mixed Shop
is the combination of Job Shop, Flow Shop and Open Shop scheduling problems.
The sample instances for all mentioned Shop problems are used as test data and
Mixed Shop survive its computational complexity to minimize the makespan. The
computational results show that the proposed algorithm is gentler to solve and
performs better than the existing algorithms."
"In the field of empirical modeling using Genetic Programming (GP), it is
important to evolve solution with good generalization ability. Generalization
ability of GP solutions get affected by two important issues: bloat and
over-fitting. Bloat is uncontrolled growth of code without any gain in fitness
and important issue in GP. We surveyed and classified existing literature
related to different techniques used by GP research community to deal with the
issue of bloat. Moreover, the classifications of different bloat control
approaches and measures for bloat are discussed. Next, we tested four bloat
control methods: Tarpeian, double tournament, lexicographic parsimony pressure
with direct bucketing and ratio bucketing on six different problems and
identified where each bloat control method performs well on per problem basis.
Based on the analysis of each method, we combined two methods: double
tournament (selection method) and Tarpeian method (works before evaluation) to
avoid bloated solutions and compared with the results obtained from individual
performance of double tournament method. It was found that the results were
improved with this combination of two methods."
"Large set of linear equations, especially for sparse and structured
coefficient (matrix) equations, solutions using classical methods become
arduous. And evolutionary algorithms have mostly been used to solve various
optimization and learning problems. Recently, hybridization of classical
methods (Jacobi method and Gauss-Seidel method) with evolutionary computation
techniques have successfully been applied in linear equation solving. In the
both above hybrid evolutionary methods, uniform adaptation (UA) techniques are
used to adapt relaxation factor. In this paper, a new Jacobi Based Time-Variant
Adaptive (JBTVA) hybrid evolutionary algorithm is proposed. In this algorithm,
a Time-Variant Adaptive (TVA) technique of relaxation factor is introduced
aiming at both improving the fine local tuning and reducing the disadvantage of
uniform adaptation of relaxation factors. This algorithm integrates the Jacobi
based SR method with time variant adaptive evolutionary algorithm. The
convergence theorems of the proposed algorithm are proved theoretically. And
the performance of the proposed algorithm is compared with JBUA hybrid
evolutionary algorithm and classical methods in the experimental domain. The
proposed algorithm outperforms both the JBUA hybrid algorithm and classical
methods in terms of convergence speed and effectiveness."
"The particle swarm approach provides a low complexity solution to the
optimization problem among various existing heuristic algorithms. Recent
advances in the algorithm resulted in improved performance at the cost of
increased computational complexity, which is undesirable. Literature shows that
the particle swarm optimization algorithm based on comprehensive learning
provides the best complexity-performance trade-off. We show how to reduce the
complexity of this algorithm further, with a slight but acceptable performance
loss. This enhancement allows the application of the algorithm in time critical
applications, such as, real-time tracking, equalization etc."
"The processes occurring in climatic change evolution and their variations
play a major role in environmental engineering. Different techniques are used
to model the relationship between temperatures, dew point and relative
humidity. Gene expression programming is capable of modelling complex realities
with great accuracy, allowing, at the same time, the extraction of knowledge
from the evolved models compared to other learning algorithms. This research
aims to use Gene Expression Programming for modelling of dew point. Generally,
accuracy of the model is the only objective used by selection mechanism of GEP.
This will evolve large size models with low training error. To avoid this
situation, use of multiple objectives, like accuracy and size of the model are
preferred by Genetic Programming practitioners. Multi-objective problem finds a
set of solutions satisfying the objectives given by decision maker.
Multiobjective based GEP will be used to evolve simple models. Various
algorithms widely used for multi objective optimization like NSGA II and SPEA 2
are tested for different test cases. The results obtained thereafter gives idea
that SPEA 2 is better algorithm compared to NSGA II based on the features like
execution time, number of solutions obtained and convergence rate. Thus
compared to models obtained by GEP, multi-objective algorithms fetch better
solutions considering the dual objectives of fitness and size of the equation.
These simple models can be used to predict dew point."
"Different techniques are used to model the relationship between temperatures,
dew point and relative humidity. Gene expression programming is capable of
modelling complex realities with great accuracy, allowing at the same time, the
extraction of knowledge from the evolved models compared to other learning
algorithms. We aim to use Gene Expression Programming for modelling of dew
point. Generally, accuracy of the model is the only objective used by selection
mechanism of GEP. This will evolve large size models with low training error.
To avoid this situation, use of multiple objectives, like accuracy and size of
the model are preferred by Genetic Programming practitioners. Solution to a
multi-objective problem is a set of solutions which satisfies the objectives
given by decision maker. Multi objective based GEP will be used to evolve
simple models. Various algorithms widely used for multi objective optimization,
like NSGA II and SPEA 2, are tested on different test problems. The results
obtained thereafter gives idea that SPEA 2 is better than NSGA II based on the
features like execution time, number of solutions obtained and convergence
rate. We selected SPEA 2 for dew point prediction. The multi-objective base GEP
produces accurate and simpler (smaller) solutions compared to solutions
produced by plain GEP for dew point predictions. Thus multi objective base GEP
produces better solutions by considering the dual objectives of fitness and
size of the solution. These simple models can be used to predict future values
of dew point."
"The problem of parameterization is often central to the effective deployment
of nature-inspired algorithms. However, finding the optimal set of parameter
values for a combination of problem instance and solution method is highly
challenging, and few concrete guidelines exist on how and when such tuning may
be performed. Previous work tends to either focus on a specific algorithm or
use benchmark problems, and both of these restrictions limit the applicability
of any findings. Here, we examine a number of different algorithms, and study
them in a ""problem agnostic"" fashion (i.e., one that is not tied to specific
instances) by considering their performance on fitness landscapes with varying
characteristics. Using this approach, we make a number of observations on which
algorithms may (or may not) benefit from tuning, and in which specific
circumstances."
"Hybrid and mixed strategy EAs have become rather popular for tackling various
complex and NP-hard optimization problems. While empirical evidence suggests
that such algorithms are successful in practice, rather little theoretical
support for their success is available, not mentioning a solid mathematical
foundation that would provide guidance towards an efficient design of this type
of EAs. In the current paper we develop a rigorous mathematical framework that
suggests such designs based on generalized schema theory, fitness levels and
drift analysis. An example-application for tackling one of the classical
NP-hard problems, the ""single-machine scheduling problem"" is presented."
"The classical Geiringer theorem addresses the limiting frequency of
occurrence of various alleles after repeated application of crossover. It has
been adopted to the setting of evolutionary algorithms and, a lot more
recently, reinforcement learning and Monte-Carlo tree search methodology to
cope with a rather challenging question of action evaluation at the chance
nodes. The theorem motivates novel dynamic parallel algorithms that are
explicitly described in the current paper for the first time. The algorithms
involve independent agents traversing a dynamically constructed directed graph
that possibly has loops. A rather elegant and profound category-theoretic model
of cognition in biological neural networks developed by a well-known French
mathematician, professor Andree Ehresmann jointly with a neurosurgeon, Jan Paul
Vanbremeersch over the last thirty years provides a hint at the connection
between such algorithms and Hebbian learning."
"This paper proposes a new scheme for performance enhancement of distributed
genetic algorithm (DGA). Initial population is divided in two classes i.e.
female and male. Simple distance based clustering is used for cluster formation
around females. For reclustering self-adaptive K-means is used, which produces
well distributed and well separated clusters. The self-adaptive K-means used
for reclustering automatically locates initial position of centroids and number
of clusters. Four plans of co-evolution are applied on these clusters
independently. Clusters evolve separately. Merging of clusters takes place
depending on their performance. For experimentation unimodal and multimodal
test functions have been used. Test result show that the new scheme of
distribution of population has given better performance."
"The performance of a Multiobjective Evolutionary Algorithm (MOEA) is
crucially dependent on the parameter setting of the operators. The most desired
control of such parameters presents the characteristic of adaptiveness, i.e.,
the capacity of changing the value of the parameter, in distinct stages of the
evolutionary process, using feedbacks from the search for determining the
direction and/or magnitude of changing. Given the great popularity of the
algorithm NSGA-II, the objective of this research is to create adaptive
controls for each parameter existing in this MOEA. With these controls, we
expect to improve even more the performance of the algorithm.
  In this work, we propose an adaptive mutation operator that has an adaptive
control which uses information about the diversity of candidate solutions for
controlling the magnitude of the mutation. A number of experiments considering
different problems suggest that this mutation operator improves the ability of
the NSGA-II for reaching the Pareto optimal Front and for getting a better
diversity among the final solutions."
"This thesis investigates the use of problem-specific knowledge to enhance a
genetic algorithm approach to multiple-choice optimisation problems. It shows
that such information can significantly enhance performance, but that the
choice of information and the way it is included are important factors for
success."
"The search for patterns or motifs in data represents an area of key interest
to many researchers. In this paper we present the Motif Tracking Algorithm, a
novel immune inspired pattern identification tool that is able to identify
variable length unknown motifs which repeat within time series data. The
algorithm searches from a neutral perspective that is independent of the data
being analysed and the underlying motifs. In this paper we test the flexibility
of the motif tracking algorithm by applying it to the search for patterns in
two industrial data sets. The algorithm is able to identify a population of
meaningful motifs in both cases, and the value of these motifs is discussed."
"Recently a new metaheuristic called harmony search was developed. It mimics
the behaviors of musicians improvising to find the better state harmony. In
this paper, this algorithm is described and applied to solve the container
storage problem in the harbor. The objective of this problem is to determine a
valid containers arrangement, which meets customers delivery deadlines, reduces
the number of container rehandlings and minimizes the ship idle time. In this
paper, an adaptation of the harmony search algorithm to the container storage
problem is detailed and some experimental results are presented and discussed.
The proposed approach was compared to a genetic algorithm previously applied to
the same problem and recorded a good results."
"This paper deals with the resolution of combinatorial optimization problems,
particularly those concerning the maritime transport scheduling. We are
interested in the management platforms in a river port and more specifically in
container organisation operations with a view to minimizing the number of
container rehandlings. Subsequently, we rmeet customers delivery deadlines and
we reduce ship stoppage time In this paper, we propose a genetic algorithm to
solve this problem and we present some experiments and results."
"Atmospheric pollutants concentration forecasting is an important issue in air
quality monitoring. Qualitair Corse, the organization responsible for
monitoring air quality in Corsica (France) region, needs to develop a
short-term prediction model to lead its mission of information towards the
public. Various deterministic models exist for meso-scale or local forecasting,
but need powerful large variable sets, a good knowledge of atmospheric
processes, and can be inaccurate because of local climatical or geographical
particularities, as observed in Corsica, a mountainous island located in a
Mediterranean Sea. As a result, we focus in this study on statistical models,
and particularly Artificial Neural Networks (ANN) that have shown good results
in the prediction of ozone concentration at horizon h+1 with data measured
locally. The purpose of this study is to build a predictor to realize
predictions of ozone and PM10 at horizon d+1 in Corsica in order to be able to
anticipate pollution peak formation and to take appropriated prevention
measures. Specific meteorological conditions are known to lead to particular
pollution event in Corsica (e.g. Saharan dust event). Therefore, several ANN
models will be used, for meteorological conditions clustering and for
operational forecasting."
"This paper introduces a novel idea for representation of individuals using
quaternions in swarm intelligence and evolutionary algorithms. Quaternions are
a number system, which extends complex numbers. They are successfully applied
to problems of theoretical physics and to those areas needing fast rotation
calculations. We propose the application of quaternions in optimization, more
precisely, we have been using quaternions for representation of individuals in
Bat algorithm. The preliminary results of our experiments when optimizing a
test-suite consisting of ten standard functions showed that this new algorithm
significantly improved the results of the original Bat algorithm. Moreover, the
obtained results are comparable with other swarm intelligence and evolutionary
algorithms, like the artificial bees colony, and differential evolution. We
believe that this representation could also be successfully applied to other
swarm intelligence and evolutionary algorithms."
"An artificial Ant Colony System (ACS) algorithm to solve general-purpose
combinatorial Optimization Problems (COP) that extends previous AC models [21]
by the inclusion of a negative pheromone, is here described. Several Travelling
Salesman Problem (TSP) were used as benchmark. We show that by using two
different sets of pheromones, a second-order co-evolved compromise between
positive and negative feedbacks achieves better results than single positive
feedback systems. The algorithm was tested against known NP-complete
combinatorial Optimization Problems, running on symmetrical TSP's. We show that
the new algorithm compares favourably against these benchmarks, accordingly to
recent biological findings by Robinson [26,27], and Gruter [28] where ""No
entry"" signals and negative feedback allows a colony to quickly reallocate the
majority of its foragers to superior food patches. This is the first time an
extended ACS algorithm is implemented with these successful characteristics."
"This work focuses on development of a Offline Hand Written English Character
Recognition algorithm based on Artificial Neural Network (ANN). The ANN
implemented in this work has single output neuron which shows whether the
tested character belongs to a particular cluster or not. The implementation is
carried out completely in 'C' language. Ten sets of English alphabets
(small-26, capital-26) were used to train the ANN and 5 sets of English
alphabets were used to test the network. The characters were collected from
different persons over duration of about 25 days. The algorithm was tested with
5 capital letters and 5 small letter sets. However, the result showed that the
algorithm recognized English alphabet patterns with maximum accuracy of 92.59%
and False Rejection Rate (FRR) of 0%."
"Solving Quadratic equation is one of the intrinsic interests as it is the
simplest nonlinear equations. A novel approach for solving Quadratic Equation
based on Genetic Algorithms (GAs) is presented. Genetic Algorithms (GAs) are a
technique to solve problems which need optimization. Generation of trial
solutions have been formed by this method. Many examples have been worked out,
and in most cases we find out the exact solution. We have discussed the effect
of different parameters on the performance of the developed algorithm. The
results are concluded after rigorous testing on different equations."
"Advanced combustion technologies such as homogeneous charge compression
ignition (HCCI) engines have a narrow stable operating region defined by
complex control strategies such as exhaust gas recirculation (EGR) and variable
valve timing among others. For such systems, it is important to identify the
operating envelope or the boundary of stable operation for diagnostics and
control purposes. Obtaining a good model of the operating envelope using
physics becomes intractable owing to engine transient effects. In this paper, a
machine learning based approach is employed to identify the stable operating
boundary of HCCI combustion directly from experimental data. Owing to imbalance
in class proportions in the data, two approaches are considered. A re-sampling
(under-sampling, over-sampling) based approach is used to develop models using
existing algorithms while a cost-sensitive approach is used to modify the
learning algorithm without modifying the data set. Support vector machines and
recently developed extreme learning machines are used for model development and
results compared against linear classification methods show that cost-sensitive
versions of ELM and SVM algorithms are well suited to model the HCCI operating
envelope. The prediction results indicate that the models have the potential to
be used for predicting HCCI instability based on sensor measurement history."
"A Multi-Layer Perceptron (MLP) defines a family of artificial neural networks
often used in TS modeling and forecasting. Because of its ""black box"" aspect,
many researchers refuse to use it. Moreover, the optimization (often based on
the exhaustive approach where ""all"" configurations are tested) and learning
phases of this artificial intelligence tool (often based on the
Levenberg-Marquardt algorithm; LMA) are weaknesses of this approach
(exhaustively and local minima). These two tasks must be repeated depending on
the knowledge of each new problem studied, making the process, long, laborious
and not systematically robust. In this paper a pruning process is proposed.
This method allows, during the training phase, to carry out an inputs selecting
method activating (or not) inter-nodes connections in order to verify if
forecasting is improved. We propose to use iteratively the popular damped
least-squares method to activate inputs and neurons. A first pass is applied to
10% of the learning sample to determine weights significantly different from 0
and delete other. Then a classical batch process based on LMA is used with the
new MLP. The validation is done using 25 measured meteorological TS and
cross-comparing the prediction results of the classical LMA and the 2-stage
LMA."
"The design process of photovoltaic (PV) modules can be greatly enhanced by
using advanced and accurate models in order to predict accurately their
electrical output behavior. The main aim of this paper is to investigate the
application of an advanced neural network based model of a module to improve
the accuracy of the predicted output I--V and P--V curves and to keep in
account the change of all the parameters at different operating conditions.
Radial basis function neural networks (RBFNN) are here utilized to predict the
output characteristic of a commercial PV module, by reading only the data of
solar irradiation and temperature. A lot of available experimental data were
used for the training of the RBFNN, and a backpropagation algorithm was
employed. Simulation and experimental validation is reported."
"This paper aims to study how the population size affects the computation time
of evolutionary algorithms in a rigorous way. The computation time of an
evolutionary algorithm can be measured by either the expected number of
generations (hitting time) or the expected number of fitness evaluations
(running time) to find an optimal solution. Population scalability is the ratio
of the expected hitting time between a benchmark algorithm and an algorithm
using a larger population size. Average drift analysis is presented for
comparing the expected hitting time of two algorithms and estimating lower and
upper bounds on population scalability. Several intuitive beliefs are
rigorously analysed. It is prove that (1) using a population sometimes
increases rather than decreases the expected hitting time; (2) using a
population cannot shorten the expected running time of any elitist evolutionary
algorithm on unimodal functions in terms of the time-fitness landscape, but
this is not true in terms of the distance-based fitness landscape; (3) using a
population cannot always reduce the expected running time on fully-deceptive
functions, which depends on the benchmark algorithm using elitist selection or
random selection."
"Solar radiation prediction is an important challenge for the electrical
engineer because it is used to estimate the power developed by commercial
photovoltaic modules. This paper deals with the problem of solar radiation
prediction based on observed meteorological data. A 2-day forecast is obtained
by using novel wavelet recurrent neural networks (WRNNs). In fact, these WRNNS
are used to exploit the correlation between solar radiation and
timescale-related variations of wind speed, humidity, and temperature. The
input to the selected WRNN is provided by timescale-related bands of wavelet
coefficients obtained from meteorological time series. The experimental setup
available at the University of Catania, Italy, provided this information. The
novelty of this approach is that the proposed WRNN performs the prediction in
the wavelet domain and, in addition, also performs the inverse wavelet
transform, giving the predicted signal as output. The obtained simulation
results show a very low root-mean-square error compared to the results of the
solar radiation prediction approaches obtained by hybrid neural networks
reported in the recent literature."
"Associative memories are data structures addressed using part of the content
rather than an index. They offer good fault reliability and biological
plausibility. Among different families of associative memories, sparse ones are
known to offer the best efficiency (ratio of the amount of bits stored to that
of bits used by the network itself). Their retrieval process performance has
been shown to benefit from the use of iterations. However classical algorithms
require having prior knowledge about the data to retrieve such as the number of
nonzero symbols. We introduce several families of algorithms to enhance the
retrieval process performance in recently proposed sparse associative memories
based on binary neural networks. We show that these algorithms provide better
performance, along with better plausibility than existing techniques. We also
analyze the required number of iterations and derive corresponding curves."
"This paper explains genetic algorithm for novice in this field. Basic
philosophy of genetic algorithm and its flowchart are described. Step by step
numerical computation of genetic algorithm for solving simple mathematical
equality problem will be briefly explained"
"A hybrid evolutionary algorithm with importance sampling method is proposed
for multi-dimensional optimization problems in this paper. In order to make use
of the information provided in the search process, a set of visited solutions
is selected to give scores for intervals in each dimension, and they are
updated as algorithm proceeds. Those intervals with higher scores are regarded
as good intervals, which are used to estimate the joint distribution of optimal
solutions through an interaction between the pool of good genetics, which are
the individuals with smaller fitness values. And the sampling probabilities for
good genetics are determined through an interaction between those estimated
good intervals. It is a cross validation mechanism which determines the
sampling probabilities for good intervals and genetics, and the resulted
probabilities are used to design crossover, mutation and other stochastic
operators with importance sampling method. As the selection of genetics and
intervals is not directly dependent on the values of fitness, the resulted
offsprings may avoid the trap of local optima. And a purely random EA is also
combined into the proposed algorithm to maintain the diversity of population.
30 benchmark test functions are used to evaluate the performance of the
proposed algorithm, and it is found that the proposed hybrid algorithm is an
efficient algorithm for multi-dimensional optimization problems considered in
this paper."
"The importance of studying the brain microstructure is described and the
existing and state of the art non-invasive methods for the investigation of the
brain microstructure using Diffusion Weighted Magnetic Resonance Imaging (DWI)
is studied. In the next step, Cramer-Rao Lower Bound (CRLB) analysis is
described and utilised for assessment of the minimum estimation error and
uncertainty level of different Diffusion Weighted Magnetic Resonance (DWMR)
signal decay models. The analyses are performed considering the best scenario
through which, we assume that the models are the appropriate representation of
the measured phenomena. This includes the study of the sensitivity of the
estimations to the measurement and model parameters. It is demonstrated that
none of the existing models can achieve a reasonable minimum uncertainty level
under typical measurement setup. At the end, the practical obstacles for
achieving higher performance in clinical and experimental environments are
studied and their effects on feasibility of the methods are discussed."
"This paper presents an adaptive amoeba algorithm to address the shortest path
tree (SPT) problem in dynamic graphs. In dynamic graphs, the edge weight
updates consists of three categories: edge weight increases, edge weight
decreases, the mixture of them. Existing work on this problem solve this issue
through analyzing the nodes influenced by the edge weight updates and recompute
these affected vertices. However, when the network becomes big, the process
will become complex. The proposed method can overcome the disadvantages of the
existing approaches. The most important feature of this algorithm is its
adaptivity. When the edge weight changes, the proposed algorithm can recognize
the affected vertices and reconstruct them spontaneously. To evaluate the
proposed adaptive amoeba algorithm, we compare it with the Label Setting
algorithm and Bellman-Ford algorithm. The comparison results demonstrate the
effectiveness of the proposed method."
"We present a newly developed -Gaussian Swarm Quantum-like Particle
Optimization (q-GSQPO) algorithm to determine the global minimum of the
potential energy function. Swarm Quantum-like Particle Optimization (SQPO)
algorithms have been derived using different attractive potential fields to
represent swarm particles moving in a quantum environment, where the one which
uses a harmonic oscillator potential as attractive field is considered as an
improved version. In this paper, we propose a new SQPO that uses -Gaussian
probability density function for the attractive potential field (q-GSQPO)
rather than Gaussian one (GSQPO) which corresponds to harmonic potential. The
performance of the q-GSQPO is compared against the GSQPO. The new algorithm
outperforms the GSQPO on most of the time in convergence to the global optimum
by increasing the efficiency of sampling the phase space and avoiding the
premature convergence to local minima. Moreover, the computational efforts were
comparable for both algorithms. We tested the algorithm to determine the lowest
energy configurations of a particle moving in a 2, 5, 10, and 50 dimensional
spaces."
"Mathematical definitions of polyhedrons and perceptron networks are
discussed. The formalization of polyhedrons is done in a rather traditional
way. For networks, previously proposed systems are developed. Perceptron
networks in disjunctive normal form (DNF) and conjunctive normal forms (CNF)
are introduced. The main theme is that single output perceptron neural networks
and characteristic functions of polyhedrons are one and the same class of
functions. A rigorous formulation and proof that three layers suffice is
obtained. The various constructions and results are among several steps
required for algorithms that replace incremental and statistical learning with
more efficient, direct and exact geometric methods for calculation of
perceptron architecture and weights."
"The Santa Fe Ant model problem has been extensively used to investigate, test
and evaluate Evolutionary Computing systems and methods over the past two
decades. There is however no literature on its program structures that are
systematically used for fitness improvement, the geometries of those structures
and their dynamics during optimization. This paper analyzes the Santa Fe Ant
Problem using a new phenotypic schema and landscape analysis based on executed
instruction sequences. For the first time we detail systematic structural
features that give high fitness and the evolutionary dynamics of such
structures. The new schema avoids variances due to introns. We develop a
phenotypic variation method that tests the new understanding of the landscape.
We also develop a modified function set that tests newly identified
synchronization constraints. We obtain favorable computational efforts compared
to those in the literature, on testing the new variation and function set on
both the Santa Fe Trail, and the more computationally demanding Los Altos
Trail. Our findings suggest that for the Santa Fe Ant problem, a perspective of
program assembly from repetition of highly fit responses to trail conditions
leads to better analysis and performance."
"The present survey provides the state-of-the-art of research, copiously
devoted to Evolutionary Approach (EAs) for clustering exemplified with a
diversity of evolutionary computations. The Survey provides a nomenclature that
highlights some aspects that are very important in the context of evolutionary
data clustering. The paper missions the clustering trade-offs branched out with
wide-ranging Multi Objective Evolutionary Approaches (MOEAs) methods. Finally,
this study addresses the potential challenges of MOEA design and data
clustering, along with conclusions and recommendations for novice and
researchers by positioning most promising paths of future research. MOEAs have
substantial success across a variety of MOP applications, from pedagogical
multifunction optimization to real-world engineering design. The survey paper
noticeably organizes the developments witnessed in the past three decades for
EAs based metaheuristics to solve multiobjective optimization problems (MOP)
and to derive significant progression in ruling high quality elucidations in a
single run. Data clustering is an exigent task, whose intricacy is caused by a
lack of unique and precise definition of a cluster. The discrete optimization
problem uses the cluster space to derive a solution for Multiobjective data
clustering. Discovery of a majority or all of the clusters (of illogical
shapes) present in the data is a long-standing goal of unsupervised predictive
learning problems or exploratory pattern analysis."
"Large-scale problems are nonlinear problems that need metaheuristics, or
global optimization algorithms. This paper reviews nature-inspired
metaheuristics, then it introduces a framework named Competitive Ant Colony
Optimization inspired by the chemical communications among insects. Then a case
study is presented to investigate the proposed framework for large-scale global
optimization."
"Bio inspiration is a branch of artificial simulation science that shows
pervasive contributions to variety of engineering fields such as automate
pattern recognition, systematic fault detection and applied optimization. In
this paper, a new metaheuristic optimizing algorithm that is the simulation of
The Great Salmon Run(TGSR) is developed. The obtained results imply on the
acceptable performance of implemented method in optimization of complex non
convex, multi dimensional and multi-modal problems. To prove the superiority of
TGSR in both robustness and quality, it is also compared with most of the well
known proposed optimizing techniques such as Simulated Annealing (SA), Parallel
Migrating Genetic Algorithm (PMGA), Differential Evolutionary Algorithm (DEA),
Particle Swarm Optimization (PSO), Bee Algorithm (BA), Artificial Bee Colony
(ABC), Firefly Algorithm (FA) and Cuckoo Search (CS). The obtained results
confirm the acceptable performance of the proposed method in both robustness
and quality for different bench-mark optimizing problems and also prove the
authors claim."
"In last decades optimization and control of complex systems that possessed
various conflicted objectives simultaneously attracted an incremental interest
of scientists. This is because of the vast applications of these systems in
various fields of real life engineering phenomena that are generally multi
modal, non convex and multi criterion. Hence, many researchers utilized
versatile intelligent models such as Pareto based techniques, game theory
(cooperative and non cooperative games), neuro evolutionary systems, fuzzy
logic and advanced neural networks for handling these types of problems. In
this paper a novel method called Synchronous Self Learning Pareto Strategy
Algorithm (SSLPSA) is presented which utilizes Evolutionary Computing (EC),
Swarm Intelligence (SI) techniques and adaptive Classical Self Organizing Map
(CSOM) simultaneously incorporating with a data shuffling behavior.
Evolutionary Algorithms (EA) which attempt to simulate the phenomenon of
natural evolution are powerful numerical optimization algorithms that reach an
approximate global maximum of a complex multi variable function over a wide
search space and swarm base technique can improved the intensity and the
robustness in EA. CSOM is a neural network capable of learning and can improve
the quality of obtained optimal Pareto front. To prove the efficient
performance of proposed algorithm, authors utilized some well known benchmark
test functions. Obtained results indicate that the cited method is best suit in
the case of vector optimization."
"Recently, with the rapid development of technology, there are a lot of
applications require to achieve low-cost learning. However the computational
power of classical artificial neural networks, they are not capable to provide
low-cost learning. In contrast, quantum neural networks may be representing a
good computational alternate to classical neural network approaches, based on
the computational power of quantum bit (qubit) over the classical bit. In this
paper we present a new computational approach to the quantum perceptron neural
network can achieve learning in low-cost computation. The proposed approach has
only one neuron can construct self-adaptive activation operators capable to
accomplish the learning process in a limited number of iterations and, thereby,
reduce the overall computational cost. The proposed approach is capable to
construct its own set of activation operators to be applied widely in both
quantum and classical applications to overcome the linearity limitation of
classical perceptron. The computational power of the proposed approach is
illustrated via solving variety of problems where promising and comparable
results are given."
"Deep Learning has attracted significant attention in recent years. Here I
present a brief overview of my first Deep Learner of 1991, and its historic
context, with a timeline of Deep Learning highlights."
"K-means Fast Learning Artificial Neural Network (K-FLANN) is an unsupervised
neural network requires two parameters: tolerance and vigilance. Best
Clustering results are feasible only by finest parameters specified to the
neural network. Selecting optimal values for these parameters is a major
problem. To solve this issue, Genetic Algorithm (GA) is used to determine
optimal parameters of K-FLANN for finding groups in multidimensional data.
K-FLANN is a simple topological network, in which output nodes grows
dynamically during the clustering process on receiving input patterns. Original
K-FLANN is enhanced to select winner unit out of the matched nodes so that
stable clusters are formed with in a less number of epochs. The experimental
results show that the GA is efficient in finding optimal values of parameters
from the large search space and is tested using artificial and synthetic data
sets."
"The firefly algorithm has become an increasingly important tool of Swarm
Intelligence that has been applied in almost all areas of optimization, as well
as engineering practice. Many problems from various areas have been
successfully solved using the firefly algorithm and its variants. In order to
use the algorithm to solve diverse problems, the original firefly algorithm
needs to be modified or hybridized. This paper carries out a comprehensive
review of this living and evolving discipline of Swarm Intelligence, in order
to show that the firefly algorithm could be applied to every problem arising in
practice. On the other hand, it encourages new researchers and algorithm
developers to use this simple and yet very efficient algorithm for problem
solving. It often guarantees that the obtained results will meet the
expectations."
"Evolutionary and swarm algorithms have found many applications in design
problems since todays computing power enables these algorithms to find
solutions to complicated design problems very fast. Newly proposed hybrid
algorithm, bat algorithm, has been applied for the design of microwave
microstrip couplers for the first time. Simulation results indicate that the
bat algorithm is a very fast algorithm and it produces very reliable results."
"Associative memories are structures that store data patterns and retrieve
them given partial inputs. Sparse Clustered Networks (SCNs) are
recently-introduced binary-weighted associative memories that significantly
improve the storage and retrieval capabilities over the prior state-of-the art.
However, deleting or updating the data patterns result in a significant
increase in the data retrieval error probability. In this paper, we propose an
algorithm to address this problem by incorporating multiple-valued weights for
the interconnections used in the network. The proposed algorithm lowers the
error rate by an order of magnitude for our sample network with 60% deleted
contents. We then investigate the advantages of the proposed algorithm for
hardware implementations."
"Recently there has been many works on adaptive subspace filtering in the
signal processing literature. Most of them are concerned with tracking the
signal subspace spanned by the eigenvectors corresponding to the eigenvalues of
the covariance matrix of the signal plus noise data. Minor Component Analysis
(MCA) is important tool and has a wide application in telecommunications,
antenna array processing, statistical parametric estimation, etc. As an
important feature extraction technique, MCA is a statistical method of
extracting the eigenvector associated with the smallest eigenvalue of the
covariance matrix. In this paper, we will present a MCA learning algorithm to
extract minor component from input signals, and the learning rate parameter is
also presented, which ensures fast convergence of the algorithm, because it has
direct effect on the convergence of the weight vector and the error level is
affected by this value. MCA is performed to determine the estimated DOA.
Simulation results will be furnished to illustrate the theoretical results
achieved."
"The purpose of this paper is to present a method of solving the Shr\""odinger
Equation (SE) by Genetic Algorithms and Grammatical Evolution. The method forms
generations of trial solutions expressed in an analytical form. We illustrate
the effectiveness of this method providing, for example, the results of its
application to a quantum system minimal energy, and we compare these results
with those produced by traditional analytical methods"
"Data clustering is a recognized data analysis method in data mining whereas
K-Means is the well known partitional clustering method, possessing pleasant
features. We observed that, K-Means and other partitional clustering techniques
suffer from several limitations such as initial cluster centre selection,
preknowledge of number of clusters, dead unit problem, multiple cluster
membership and premature convergence to local optima. Several optimization
methods are proposed in the literature in order to solve clustering
limitations, but Swarm Intelligence (SI) has achieved its remarkable position
in the concerned area. Particle Swarm Optimization (PSO) is the most popular SI
technique and one of the favorite areas of researchers. In this paper, we
present a brief overview of PSO and applicability of its variants to solve
clustering challenges. Also, we propose an advanced PSO algorithm named as
Subtractive Clustering based Boundary Restricted Adaptive Particle Swarm
Optimization (SC-BR-APSO) algorithm for clustering multidimensional data. For
comparison purpose, we have studied and analyzed various algorithms such as
K-Means, PSO, K-Means-PSO, Hybrid Subtractive + PSO, BRAPSO, and proposed
algorithm on nine different datasets. The motivation behind proposing
SC-BR-APSO algorithm is to deal with multidimensional data clustering, with
minimum error rate and maximum convergence rate."
"Particle Swarm Optimisation (PSO) makes use of a dynamical system for solving
a search task. Instead of adding search biases in order to improve performance
in certain problems, we aim to remove algorithm-induced scales by controlling
the swarm with a mechanism that is scale-free except possibly for a suppression
of scales beyond the system size. In this way a very promising performance is
achieved due to the balance of large-scale exploration and local search. The
resulting algorithm shows evidence for self-organised criticality, brought
about via the intrinsic dynamics of the swarm as it interacts with the
objective function, rather than being explicitly specified. The Critical
Particle Swarm (CriPS) can be easily combined with many existing extensions
such as chaotic exploration, additional force terms or non-trivial topologies."
"This paper describes a new model for an artificial neural network processing
unit or neuron. It is slightly different to a traditional feedforward network
by the fact that it favours a mechanism of trying to match the wave-like
'shape' of the input with the shape of the output against specific value error
corrections. The expectation is then that a best fit shape can be transposed
into the desired output values more easily. This allows for notions of
reinforcement through resonance and also the construction of synapses."
"Autonomous threshold element circuit networks are used to investigate the
structure of neural networks. With these circuits, as the transition functions
are threshold functions, it is necessary to consider the existence of sequences
of state configurations that cannot be transitioned. In this study, we focus on
all logical functions of four or fewer variables, and we discuss the periodic
sequences and transient series that transition from all sequences of state
configurations. Furthermore, by using the sequences of state configurations in
the Garden of Eden, we show that it is easy to obtain functions that determine
the operation of circuit networks."
"Application of metaheuristic algorithms has been of continued interest in the
field of electrical engineering because of their powerful features. In this
work special design is done for a tapered transmission line used for matching
an arbitrary real load to a 50{\Omega} line. The problem at hand is to match
this arbitrary load to 50 {\Omega} line using three section tapered
transmission line with impedances in decreasing order from the load. So the
problem becomes optimizing an equation with three unknowns with various
conditions. The optimized values are obtained using Particle Swarm
Optimization. It can easily be shown that PSO is very strong in solving this
kind of multiobjective optimization problems."
"This paper presents results on the memory capacity of a generalized feedback
neural network using a circulant matrix. Children are capable of learning soon
after birth which indicates that the neural networks of the brain have prior
learnt capacity that is a consequence of the regular structures in the brain's
organization. Motivated by this idea, we consider the capacity of circulant
matrices as weight matrices in a feedback network."
"Recent advances in associative memory design through structured pattern sets
and graph-based inference algorithms have allowed reliable learning and recall
of an exponential number of patterns. Although these designs correct external
errors in recall, they assume neurons that compute noiselessly, in contrast to
the highly variable neurons in brain regions thought to operate associatively
such as hippocampus and olfactory cortex.
  Here we consider associative memories with noisy internal computations and
analytically characterize performance. As long as the internal noise level is
below a specified threshold, the error probability in the recall phase can be
made exceedingly small. More surprisingly, we show that internal noise actually
improves the performance of the recall phase while the pattern retrieval
capacity remains intact, i.e., the number of stored patterns does not reduce
with noise (up to a threshold). Computational experiments lend additional
support to our theoretical analysis. This work suggests a functional benefit to
noisy neurons in biological neuronal networks."
"The human brain is a dynamical system whose extremely complex sensor-driven
neural processes give rise to conceptual, logical cognition. Understanding the
interplay between nonlinear neural dynamics and concept-level cognition remains
a major scientific challenge. Here I propose a mechanism of neurodynamical
organization, called conceptors, which unites nonlinear dynamics with basic
principles of conceptual abstraction and logic. It becomes possible to learn,
store, abstract, focus, morph, generalize, de-noise and recognize a large
number of dynamical patterns within a single neural system; novel patterns can
be added without interfering with previously acquired ones; neural noise is
automatically filtered. Conceptors help explaining how conceptual-level
information processing emerges naturally and robustly in neural systems, and
remove a number of roadblocks in the theory and applications of recurrent
neural networks."
"A supply chain is a system which moves products from a supplier to customers.
The supply chains are ubiquitous. They play a key role in all economic
activities. Inspired by biological principles of nutrients' distribution in
protoplasmic networks of slime mould Physarum polycephalum we propose a novel
algorithm for a supply chain design. The algorithm handles the supply networks
where capacity investments and product flows are variables. The networks are
constrained by a need to satisfy product demands. Two features of the slime
mould are adopted in our algorithm. The first is the continuity of a flux
during the iterative process, which is used in real-time update of the costs
associated with the supply links. The second feature is adaptivity. The supply
chain can converge to an equilibrium state when costs are changed. Practicality
and flexibility of our algorithm is illustrated on numerical examples."
"In this paper we study the problem of optimal layout of an offshore wind farm
to minimize the wake effect impacts. Considering the specific requirements of
concerned offshore wind farm, we propose an adaptive genetic algorithm (AGA)
which introduces location swaps to replace random crossovers in conventional
GAs. That way the total number of turbines in the resulting layout will be
effectively kept to the initially specified value. We experiment the proposed
AGA method on three cases with free wind speed of 12 m/s, 20 m/s, and a typical
offshore wind distribution setting respectively. Numerical results verify the
effectiveness of our proposed algorithm which achieves a much faster
convergence compared to conventional GA algorithms."
"We analyze the unbiased black-box complexity of jump functions with small,
medium, and large sizes of the fitness plateau surrounding the optimal
solution.
  Among other results, we show that when the jump size is $(1/2 -
\varepsilon)n$, that is, only a small constant fraction of the fitness values
is visible, then the unbiased black-box complexities for arities $3$ and higher
are of the same order as those for the simple \textsc{OneMax} function. Even
for the extreme jump function, in which all but the two fitness values $n/2$
and $n$ are blanked out, polynomial-time mutation-based (i.e., unary unbiased)
black-box optimization algorithms exist. This is quite surprising given that
for the extreme jump function almost the whole search space (all but a
$\Theta(n^{-1/2})$ fraction) is a plateau of constant fitness.
  To prove these results, we introduce new tools for the analysis of unbiased
black-box complexities, for example, selecting the new parent individual not by
comparing the fitnesses of the competing search points, but also by taking into
account the (empirical) expected fitnesses of their offspring."
"According to conventional neural network theories, the feature of
single-hidden-layer feedforward neural networks(SLFNs) resorts to parameters of
the weighted connections and hidden nodes. SLFNs are universal approximators
when at least the parameters of the networks including hidden-node parameter
and output weight are exist. Unlike above neural network theories, this paper
indicates that in order to let SLFNs work as universal approximators, one may
simply calculate the hidden node parameter only and the output weight is not
needed at all. In other words, this proposed neural network architecture can be
considered as a standard SLFNs with fixing output weight equal to an unit
vector. Further more, this paper presents experiments which show that the
proposed learning method tends to extremely reduce network output error to a
very small number with only 1 hidden node. Simulation results demonstrate that
the proposed method can provide several to thousands of times faster than other
learning algorithm including BP, SVM/SVR and other ELM methods."
"This paper introduces an effective memetic algorithm for the linear ordering
problem with cumulative costs. The proposed algorithm combines an order-based
recombination operator with an improved forward-backward local search procedure
and employs a solution quality based replacement criterion for pool updating.
Extensive experiments on 118 well-known benchmark instances show that the
proposed algorithm achieves competitive results by identifying 46 new upper
bounds. Furthermore, some critical ingredients of our algorithm are analyzed to
understand the source of its performance."
"In this paper, we present our investigations on the use of single objective
and multiobjective genetic algorithms based optimisation algorithms to improve
the design of OFDM pulses for radar. We discuss these optimization procedures
in the scope of a waveform design intended for two different radar processing
solutions. Lastly, we show how the encoding solution is suited to permit the
optimizations of waveform for OFDM radar related challenges such as enhanced
detection."
"The Quadratic Assignment Problem (QAP) is one of the models used for the
multi-row layout problem with facilities of equal area. There are a set of n
facilities and a set of n locations. For each pair of locations, a distance is
specified and for each pair of facilities a weight or flow is specified (e.g.,
the amount of supplies transported between the two facilities). The problem is
to assign all facilities to different locations with the aim of minimizing the
sum of the distances multiplied by the corresponding flows. The QAP is among
the most difficult NP-hard combinatorial optimization problems. Because of
this, this paper presents an efficient Genetic algorithm (GA) to solve this
problem in reasonable time. For validation the proposed GA some examples are
selected from QAP library. The obtained results in reasonable time show the
efficiency of proposed GA."
"For learning problem of Radial Basis Function Process Neural Network
(RBF-PNN), an optimization training method based on GA combined with SA is
proposed in this paper. Through building generalized Fr\'echet distance to
measure similarity between time-varying function samples, the learning problem
of radial basis centre functions and connection weights is converted into the
training on corresponding discrete sequence coefficients. Network training
objective function is constructed according to the least square error
criterion, and global optimization solving of network parameters is implemented
in feasible solution space by use of global optimization feature of GA and
probabilistic jumping property of SA . The experiment results illustrate that
the training algorithm improves the network training efficiency and stability."
"The ELM method has become widely used for classification and regressions
problems as a result of its accuracy, simplicity and ease of use. The solution
of the hidden layer weights by means of a matrix pseudoinverse operation is a
significant contributor to the utility of the method; however, the conventional
calculation of the pseudoinverse by means of a singular value decomposition
(SVD) is not always practical for large data sets or for online updates to the
solution. In this paper we discuss incremental methods for solving the
pseudoinverse which are suitable for ELM. We show that careful choice of
methods allows us to optimize for accuracy, ease of computation, or
adaptability of the solution."
"Whilst most engineered systems use signals that are continuous in time, there
is a domain of systems in which signals consist of events. Events, like Dirac
delta functions, have no meaningful time duration. Many important real-world
systems are intrinsically event-based, including the mammalian brain, in which
the primary packets of data are spike events, or action potentials. In this
domain, signal processing requires responses to spatio-temporal patterns of
events. We show that some straightforward modifications to the standard ELM
topology produce networks that are able to perform spatio-temporal event
processing online with a high degree of accuracy. The modifications involve the
re-definition of hidden layer units as synaptic kernels, in which the input
delta functions are transformed into continuous-valued signals using a variety
of impulse-response functions. This permits the use of linear solution methods
in the output layer, which can produce events as output, if modeled as a
classifier; the output classes are 'event' or 'no event'. We illustrate the
method in application to a spike-processing problem."
"Particle Swarm Optimization (PSO) is a popular nature-inspired meta-heuristic
for solving continuous optimization problems. Although this technique is widely
used, the understanding of the mechanisms that make swarms so successful is
still limited. We present the first substantial experimental investigation of
the influence of the local attractor on the quality of exploration and
exploitation. We compare in detail classical PSO with the social-only variant
where local attractors are ignored. To measure the exploration capabilities, we
determine how frequently both variants return results in the neighborhood of
the global optimum. We measure the quality of exploitation by considering only
function values from runs that reached a search point sufficiently close to the
global optimum and then comparing in how many digits such values still deviate
from the global minimum value. It turns out that the local attractor
significantly improves the exploration, but sometimes reduces the quality of
the exploitation. As a compromise, we propose and evaluate a hybrid PSO which
switches off its local attractors at a certain point in time. The effects
mentioned can also be observed by measuring the potential of the swarm."
"Most multimodal optimization algorithms use the so called \textit{niching
methods}~\cite{mahfoud1995niching} in order to promote diversity during
optimization, while others, like \textit{Artificial Immune
Systems}~\cite{de2010conceptual} try to find multiple solutions as its main
objective. One of such algorithms, called
\textit{dopt-aiNet}~\cite{de2005artificial}, introduced the Line Distance that
measures the distance between two solutions regarding their basis of
attraction. In this short abstract I propose the use of the Line Distance
measure as the main objective-function in order to locate multiple optima at
once in a population."
"Studies have shown that multi-objective optimization problems are hard
problems. Such problems either require longer time to converge to an optimum
solution, or may not converge at all. Recently some researchers have claimed
that real culprit for increasing the hardness of multi-objective problems are
not the number of objectives themselves rather it is the increased size of
solution set, incompatibility of solutions, and high probability of finding
suboptimal solution due to increased number of local maxima. In this work, we
have setup a simple framework for the evaluation of hardness of multi-objective
genetic algorithms (MOGA). The algorithm is designed for a pray-predator game
where a player is to improve its lifespan, challenging level and usability of
the game arena through number of generations. A rigorous set of experiments are
performed for quantifying the hardness in terms of evolution for increasing
number of objective functions. In genetic algorithm, crossover and mutation
with equal probability are applied to create offspring in each generation.
First, each objective function is maximized individually by ranking the
competing players on the basis of the fitness (cost) function, and then a
multi-objective cost function (sum of individual cost functions) is maximized
with ranking, and also without ranking where dominated solutions are also
allowed to evolve."
"This paper has been withdrawn by the author due to a crucial accuracy error
in Fig. 5. For precise performance of ALBNN please refer to Yoon et al.'s work
in the following article. Yoon, H., Park, C. S., Kim, J. S., & Baek, J. G.
(2013). Algorithm learning based neural network integrating feature selection
and classification. Expert Systems with Applications, 40(1), 231-241.
http://www.sciencedirect.com/science/article/pii/S0957417412008731"
"Conceptors provide an elementary neuro-computational mechanism which sheds a
fresh and unifying light on a diversity of cognitive phenomena. A number of
demanding learning and processing tasks can be solved with unprecedented ease,
robustness and accuracy. Some of these tasks were impossible to solve before.
This entirely informal paper introduces the basic principles of conceptors and
highlights some of their usages."
"We present a closed form expression for initializing the input weights in a
multi-layer perceptron, which can be used as the first step in synthesis of an
Extreme Learning Ma-chine. The expression is based on the standard function for
a separating hyperplane as computed in multilayer perceptrons and linear
Support Vector Machines; that is, as a linear combination of input data
samples. In the absence of supervised training for the input weights, random
linear combinations of training data samples are used to project the input data
to a higher dimensional hidden layer. The hidden layer weights are solved in
the standard ELM fashion by computing the pseudoinverse of the hidden layer
outputs and multiplying by the desired output values. All weights for this
method can be computed in a single pass, and the resulting networks are more
accurate and more consistent on some standard problems than regular ELM
networks of the same size."
"Swarm intelligence is a research field that models the collective behavior in
swarms of insects or animals. Several algorithms arising from such models have
been proposed to solve a wide range of complex optimization problems. In this
paper, a novel swarm algorithm called the Social Spider Optimization (SSO) is
proposed for solving optimization tasks. The SSO algorithm is based on the
simulation of cooperative behavior of social-spiders. In the proposed
algorithm, individuals emulate a group of spiders which interact to each other
based on the biological laws of the cooperative colony. The algorithm considers
two different search agents (spiders): males and females. Depending on gender,
each individual is conducted by a set of different evolutionary operators which
mimic different cooperative behaviors that are typically found in the colony.
In order to illustrate the proficiency and robustness of the proposed approach,
it is compared to other well-known evolutionary methods. The comparison
examines several standard benchmark functions that are commonly considered
within the literature of evolutionary algorithms. The outcome shows a high
performance of the proposed method for searching a global optimum with several
benchmark functions."
"This Paper presents the methodology of penetration of Micro-Grids (MG) in the
radial distribution system (RDS). The aim of this paper is to minimize a total
real power loss that descends the performance of the radial distribution system
by integrating various renewable resources as Distributed Generation (DG). The
combination of different types of renewable energy resources contributes a
sustainable MG. These resources are optimally sized and located using
evolutionary approach in various penetration levels. The optimal solutions are
experimented with IEEE 33 radial distribution system using Particle Swarm
Optimization (PSO) technique. The results are quite promising and authenticate
its potential to solve problem in radial distribution system effectively."
"Initial population plays an important role in heuristic algorithms such as GA
as it help to decrease the time those algorithms need to achieve an acceptable
result. Furthermore, it may influence the quality of the final answer given by
evolutionary algorithms. In this paper, we shall introduce a heuristic method
to generate a target based initial population which possess two mentioned
characteristics. The efficiency of the proposed method has been shown by
presenting the results of our tests on the benchmarks."
"We introduce a technology stack or specification describing the multiple
levels of abstraction and specialization needed to implement a neuromorphic
processor (NPU) based on the previously-described concept of AHaH Computing and
integrate it into today's digital computing systems. The general purpose NPU
implementation described here is called Thermodynamic-RAM (kT-RAM) and is just
one of many possible architectures, each with varying advantages and trade
offs. Bringing us closer to brain-like neural computation, kT-RAM will provide
a general-purpose adaptive hardware resource to existing computing platforms
enabling fast and low-power machine learning capabilities that are currently
hampered by the separation of memory and processing, a.k.a the von Neumann
bottleneck. Because understanding such a processor based on non-traditional
principles can be difficult, by presenting the various levels of the stack from
the bottom up, layer by layer, explaining kT-RAM becomes a much easier task.
The levels of the Thermodynamic-RAM technology stack include the memristor,
synapse, AHaH node, kT-RAM, instruction set, sparse spike encoding, kT-RAM
emulator, and SENSE server."
"Interest in multimodal function optimization is expanding rapidly since real
world optimization problems often demand locating multiple optima within a
search space. This article presents a new multimodal optimization algorithm
named as the Collective Animal Behavior (CAB). Animal groups, such as schools
of fish, flocks of birds, swarms of locusts and herds of wildebeest, exhibit a
variety of behaviors including swarming about a food source, milling around a
central location or migrating over large distances in aligned groups. These
collective behaviors are often advantageous to groups, allowing them to
increase their harvesting efficiency to follow better migration routes, to
improve their aerodynamic and to avoid predation. In the proposed algorithm,
searcher agents are a group of animals which interact to each other based on
the biological laws of collective motion. Experimental results demonstrate that
the proposed algorithm is capable of finding global and local optima of
benchmark multimodal optimization problems with a higher efficiency in
comparison to other methods reported in the literature."
"Swarm dynamics is the study of collections of agents that interact with one
another without central control. In natural systems, insects, birds, fish and
other large mammals function in larger units to increase the overall fitness of
the individuals. Their behavior is coordinated through local interactions to
enhance mate selection, predator detection, migratory route identification and
so forth [Andersson and Wallander 2003; Buhl et al. 2006; Nagy et al. 2010;
Partridge 1982; Sumpter et al. 2008]. In artificial systems, swarms of
autonomous agents can augment human activities such as search and rescue, and
environmental monitoring by covering large areas with multiple nodes [Alami et
al. 2007; Caruso et al. 2008; Ogren et al. 2004; Paley et al. 2007; Sibley et
al. 2002]. In this paper, we explore the interplay between swarm dynamics,
covert leadership and theoretical information transfer. A leader is a member of
the swarm that acts upon information in addition to what is provided by local
interactions. Depending upon the leadership model, leaders can use their
external information either all the time or in response to local conditions
[Couzin et al. 2005; Sun et al. 2013]. A covert leader is a leader that is
treated no differently than others in the swarm, so leaders and followers
participate equally in whatever interaction model is used [Rossi et al. 2007].
In this study, we use theoretical information transfer as a means of analyzing
swarm interactions to explore whether or not it is possible to distinguish
between followers and leaders based on interactions within the swarm. We find
that covert leaders can be distinguished from followers in a swarm because they
receive less transfer entropy than followers."
"Navigating networked robot swarms often requires knowing where to go, sensing
the environment, and path-planning based on the destination and barriers in the
environment. Such a process is computationally intensive. Moreover, as the
network scales up, the computational load increases quadratically, or even
exponentially. Unlike these man-made systems, most biological systems scale
linearly in complexity. Furthermore, the scale of a biological swarm can even
enable collective intelligence. One example comes from observations of golden
shiner fish. Golden shiners naturally prefer darkness and school together. Each
individual golden shiner does not know where the darkness is. Neither does it
sense the light gradients in the environment. However, by moving together as a
school, they always end up in the shady area. We apply such collective
intelligence learned from golden shiner fish to navigating robot swarms. Each
individual robot's dynamic is based on the gold shiners' movement strategy---a
random walk with its speed modulated by the light intensity and its direction
affected by its neighbors. The theoretical analysis and simulation results show
that our method 1) promises to navigate a robot swarm with little situational
knowledge, 2) simplifies control and decision-making for each individual robot,
3) requires minimal or even no information exchange within the swarm, and 4) is
highly distributed, adaptive, and robust."
"One of the major motifs in collective or swarm intelligence is that, even
though individuals follow simple rules, the resulting global behavior can be
complex and intelligent. In artificial swarm systems, such as swarm robots, the
goal is to use systems that are as simple and cheap as possible, deploy many of
them, and coordinate them to conduct complex tasks that each individual cannot
accomplish. Shape formation in artificial intelligence systems is usually
required for specific task-oriented performance, including 1) forming sensing
grids, 2) exploring and mapping in space, underwater, or hazardous
environments, and 3) forming a barricade for surveillance or protecting an area
or a person. This paper presents a dynamic model of an artificial swarm system
based on a virtual spring damper model and algorithms for dispersion without a
leader and line formation with an interim leader using only the distance
estimation among the neighbors."
"Block matching (BM) motion estimation plays a very important role in video
coding. In a BM approach, image frames in a video sequence are divided into
blocks. For each block in the current frame, the best matching block is
identified inside a region of the previous frame, aiming to minimize the sum of
absolute differences (SAD). Unfortunately, the SAD evaluation is
computationally expensive and represents the most consuming operation in the BM
process. Therefore, BM motion estimation can be approached as an optimization
problem, where the goal is to find the best matching block within a search
space. The simplest available BM method is the full search algorithm (FSA)
which finds the most accurate motion vector through an exhaustive computation
of SAD values for all elements of the search window. Recently, several fast BM
algorithms have been proposed to reduce the number of SAD operations by
calculating only a fixed subset of search locations at the price of poor
accuracy. In this paper, a new algorithm based on Artificial Bee Colony (ABC)
optimization is proposed to reduce the number of search locations in the BM
process. In our algorithm, the computation of search locations is drastically
reduced by considering a fitness calculation strategy which indicates when it
is feasible to calculate or only estimate new search locations. Since the
proposed algorithm does not consider any fixed search pattern or any other
movement assumption as most of other BM approaches do, a high probability for
finding the true minimum (accurate motion vector) is expected. Conducted
simulations show that the proposed method achieves the best balance over other
fast BM algorithms, in terms of both estimation accuracy and computational
cost."
"A new supervised learning algorithm, SNN/LP, is proposed for Spiking Neural
Networks. This novel algorithm uses limited precision for both synaptic weights
and synaptic delays; 3 bits in each case. Also a genetic algorithm is used for
the supervised training. The results are comparable or better than previously
published work. The results are applicable to the realization of large scale
hardware neural networks. One of the trained networks is implemented in
programmable hardware."
"To address the difficulty of creating online collaborative evolutionary
systems, this paper presents a new prototype library called Worldwide
Infrastructure for Neuroevolution (WIN) and its accompanying site WIN Online
(http://winark.org/). The WIN library is a collection of software packages
built on top of Node.js that reduce the complexity of creating fully
persistent, online, and interactive (or automated) evolutionary platforms
around any domain. WIN Online is the public interface for WIN, providing an
online collection of domains built with the WIN library that lets novice and
expert users browse and meaningfully contribute to ongoing experiments. The
long term goal of WIN is to make it trivial to connect any platform to the
world, providing both a stream of online users, and archives of data and
discoveries for later extension by humans or computers."
"A real-coded genetic algorithm is used to schedule the charging of an energy
storage system (ESS), operated in tandem with renewable power by an electricity
consumer who is subject to time-of-use pricing and a demand charge. Simulations
based on load and generation profiles of typical residential customers show
that an ESS scheduled by our algorithm can reduce electricity costs by
approximately 17%, compared to a system without an ESS, and by 8% compared to a
scheduling algorithm based on net power."
"Evolutionary algorithms (EA) have been widely accepted as efficient solvers
for complex real world optimization problems, including engineering
optimization. However, real world optimization problems often involve uncertain
environment including noisy and/or dynamic environments, which pose major
challenges to EA-based optimization. The presence of noise interferes with the
evaluation and the selection process of EA, and thus adversely affects its
performance. In addition, as presence of noise poses challenges to the
evaluation of the fitness function, it may need to be estimated instead of
being evaluated. Several existing approaches attempt to address this problem,
such as introduction of diversity (hyper mutation, random immigrants, special
operators) or incorporation of memory of the past (diploidy, case based
memory). However, these approaches fail to adequately address the problem. In
this paper we propose a Distributed Population Switching Evolutionary Algorithm
(DPSEA) method that addresses optimization of functions with noisy fitness
using a distributed population switching architecture, to simulate a
distributed self-adaptive memory of the solution space. Local regression is
used in the pseudo-populations to estimate the fitness. Successful applications
to benchmark test problems ascertain the proposed method's superior performance
in terms of both robustness and accuracy."
"This paper studies a class of enhanced diffusion processes in which random
walkers perform L\'evy flights and apply it for global optimization. L\'evy
flights offer controlled balance between exploitation and exploration. We
develop four optimization algorithms based on such properties. We compare new
algorithms with the well-known Simulated Annealing on hard test functions and
the results are very promising."
"Artificial Bee Colony (ABC) is a distinguished optimization strategy that can
resolve nonlinear and multifaceted problems. It is comparatively a
straightforward and modern population based probabilistic approach for
comprehensive optimization. In the vein of the other population based
algorithms, ABC is moreover computationally classy due to its slow nature of
search procedure. The solution exploration equation of ABC is extensively
influenced by a arbitrary quantity which helps in exploration at the cost of
exploitation of the better search space. In the solution exploration equation
of ABC due to the outsized step size the chance of skipping the factual
solution is high. Therefore, here this paper improve onlooker bee phase with
help of a local search strategy inspired by memetic algorithm to balance the
diversity and convergence capability of the ABC. The proposed algorithm is
named as Improved Onlooker Bee Phase in ABC (IoABC). It is tested over 12 well
known un-biased test problems of diverse complexities and two engineering
optimization problems; results show that the anticipated algorithm go one
better than the basic ABC and its recent deviations in a good number of the
experiments."
"Ability of deep networks to extract high level features and of recurrent
networks to perform time-series inference have been studied. In view of
universality of one hidden layer network at approximating functions under weak
constraints, the benefit of multiple layers is to enlarge the space of
dynamical systems approximated or, given the space, reduce the number of units
required for a certain error. Traditionally shallow networks with manually
engineered features are used, back-propagation extent is limited to one and
attempt to choose a large number of hidden units to satisfy the Markov
condition is made. In case of Markov models, it has been shown that many
systems need to be modeled as higher order. In the present work, we present
deep recurrent networks with longer backpropagation through time extent as a
solution to modeling systems that are high order and to predicting ahead. We
study epileptic seizure suppression electro-stimulator. Extraction of manually
engineered complex features and prediction employing them has not allowed small
low-power implementations as, to avoid possibility of surgery, extraction of
any features that may be required has to be included. In this solution, a
recurrent neural network performs both feature extraction and prediction. We
prove analytically that adding hidden layers or increasing backpropagation
extent increases the rate of decrease of approximation error. A Dynamic
Programming (DP) training procedure employing matrix operations is derived. DP
and use of matrix operations makes the procedure efficient particularly when
using data-parallel computing. The simulation studies show the geometry of the
parameter space, that the network learns the temporal structure, that
parameters converge while model output displays same dynamic behavior as the
system and greater than .99 Average Detection Rate on all real seizure data
tried."
"Machine learning algorithms, and more in particular neural networks, arguably
experience a revolution in terms of performance. Currently, the best systems we
have for speech recognition, computer vision and similar problems are based on
neural networks, trained using the half-century old backpropagation algorithm.
Despite the fact that neural networks are a form of analog computers, they are
still implemented digitally for reasons of convenience and availability. In
this paper we demonstrate how we can design physical linear dynamic systems
with non-linear feedback as a generic platform for dynamic, neuro-inspired
analog computing. We show that a crucial advantage of this setup is that the
error backpropagation can be performed physically as well, which greatly speeds
up the optimisation process. As we show in this paper, using one experimentally
validated and one conceptual example, such systems may be the key to providing
a relatively straightforward mechanism for constructing highly scalable, fully
dynamic analog computers."
"This paper proposes a new numerical optimization algorithm inspired by the
strawberry plant for solving complicated engineering problems. Plants like
strawberry develop both runners and roots for propagation and search for water
resources and minerals. In these plants, runners and roots can be thought of as
tools for global and local searches, respectively. The proposed algorithm has
three main differences with the trivial nature-inspired optimization
algorithms: duplication-elimination of the computational agents at all
iterations, subjecting all agents to both small and large movements from the
beginning to end, and the lack of communication (information exchange) between
agents. Moreover, it has the advantage of using only three parameters to be
tuned by user. This algorithm is applied to standard test functions and the
results are compared with GA and PSO. The proposed algorithm is also used to
solve an open problem in the field of robust control theory. These simulations
show that the proposed algorithm can very effectively solve complicated
optimization problems."
"Benchmarking is key for developing and comparing optimization algorithms. In
this paper, a CUDA-based real parameter optimization benchmark (cuROB) is
introduced. Test functions of diverse properties are included within cuROB and
implemented efficiently with CUDA. Speedup of one order of magnitude can be
achieved in comparison with CPU-based benchmark of CEC'14."
"Differential Evolution (DE) is a renowned optimization stratagem that can
easily solve nonlinear and comprehensive problems. DE is a well known and
uncomplicated population based probabilistic approach for comprehensive
optimization. It has apparently outperformed a number of Evolutionary
Algorithms and further search heuristics in the vein of Particle Swarm
Optimization at what time of testing over both yardstick and actual world
problems. Nevertheless, DE, like other probabilistic optimization algorithms,
from time to time exhibits precipitate convergence and stagnates at suboptimal
position. In order to stay away from stagnation behavior while maintaining an
excellent convergence speed, an innovative search strategy is introduced, named
memetic search in DE. In the planned strategy, positions update equation
customized as per a memetic search stratagem. In this strategy a better
solution participates more times in the position modernize procedure. The
position update equation is inspired from the memetic search in artificial bee
colony algorithm. The proposed strategy is named as Memetic Search in
Differential Evolution (MSDE). To prove efficiency and efficacy of MSDE, it is
tested over 8 benchmark optimization problems and three real world optimization
problems. A comparative analysis has also been carried out among proposed MSDE
and original DE. Results show that the anticipated algorithm go one better than
the basic DE and its recent deviations in a good number of the experiments."
"Artificial Bee Colony (ABC) optimization algorithm is one of the recent
population based probabilistic approach developed for global optimization. ABC
is simple and has been showed significant improvement over other Nature
Inspired Algorithms (NIAs) when tested over some standard benchmark functions
and for some complex real world optimization problems. Memetic Algorithms also
become one of the key methodologies to solve the very large and complex
real-world optimization problems. The solution search equation of Memetic ABC
is based on Golden Section Search and an arbitrary value which tries to balance
exploration and exploitation of search space. But still there are some chances
to skip the exact solution due to its step size. In order to balance between
diversification and intensification capability of the Memetic ABC, it is
randomized the step size in Memetic ABC. The proposed algorithm is named as
Randomized Memetic ABC (RMABC). In RMABC, new solutions are generated nearby
the best so far solution and it helps to increase the exploitation capability
of Memetic ABC. The experiments on some test problems of different complexities
and one well known engineering optimization application show that the proposed
algorithm outperforms over Memetic ABC (MeABC) and some other variant of ABC
algorithm(like Gbest guided ABC (GABC),Hooke Jeeves ABC (HJABC), Best-So-Far
ABC (BSFABC) and Modified ABC (MABC) in case of almost all the problems."
"Due to that the existing traffic facilities can hardly be extended,
developing traffic signal control methods is the most important way to improve
the traffic efficiency of modern roundabouts. This paper proposes a novel
traffic signal controller with two fuzzy layers for signalizing the roundabout.
The outer layer of the controller computes urgency degrees of all the phase
subsets and then activates the most urgent subset. This mechanism helps to
instantly respond to the current traffic condition of the roundabout so as to
improve real-timeness. The inner layer of the controller computes extension
time of the current phase. If the extension value is larger than a threshold
value, the current phase is maintained; otherwise the next phase in the running
phase subset (selected by the outer layer) is activated. The inner layer adopts
well-designed phase sequences, which helps to smooth the traffic flows and to
avoid traffic jam. In general, the proposed traffic signal controller is
capable of improving real-timeness as well as reducing traffic congestion.
Moreover, an offline particle swarm optimization (PSO) algorithm is developed
to optimize the membership functions adopted in the proposed controller. By
using optimal membership functions, the performance of the controller can be
further improved. Simulation results demonstrate that the proposed controller
outperforms previous traffic signal controllers in terms of improving the
traffic efficiency of modern roundabouts."
"It is clear that the current attempts at using algorithms to create
artificial neural networks have had mixed success at best when it comes to
creating large networks and/or complex behavior. This should not be unexpected,
as creating an artificial brain is essentially a design problem. Human design
ingenuity still surpasses computational design for most tasks in most domains,
including architecture, game design, and authoring literary fiction. This leads
us to ask which the best way is to combine human and machine design capacities
when it comes to designing artificial brains. Both of them have their strengths
and weaknesses; for example, humans are much too slow to manually specify
thousands of neurons, let alone the billions of neurons that go into a human
brain, but on the other hand they can rely on a vast repository of common-sense
understanding and design heuristics that can help them perform a much better
guided search in design space than an algorithm. Therefore, in this paper we
argue for a mixed-initiative approach for collaborative online brain building
and present first results towards this goal."
"We have introduced two crossover operators, MMX-BLXexploit and
MMX-BLXexplore, for simultaneously solving multiple feature/subset selection
problems where the features may have numeric attributes and the subset sizes
are not predefined. These operators differ on the level of exploration and
exploitation they perform; one is designed to produce convergence controlled
mutation and the other exhibits a quasi-constant mutation rate. We illustrate
the characteristic of these operators by evolving pattern detectors to
distinguish alcoholics from controls using their visually evoked response
potentials (VERPs). This task encapsulates two groups of subset selection
problems; choosing a subset of EEG leads along with the lead-weights (features
with attributes) and the other that defines the temporal pattern that
characterizes the alcoholic VERPs. We observed better generalization
performance from MMX-BLXexplore. Perhaps, MMX-BLXexploit was handicapped by not
having a restart mechanism. These operators are novel and appears to hold
promise for solving simultaneous feature selection problems."
"Challenging optimisation problems are abundant in all areas of science. Since
the 1950s, scientists have developed ever-diversifying families of black box
optimisation algorithms designed to address any optimisation problem, requiring
only that quality of a candidate solution is calculated via a fitness function
specific to the problem. For such algorithms to be successful, at least three
properties are required: an effective informed sampling strategy, that guides
generation of new candidates on the basis of fitnesses and locations of
previously visited candidates; mechanisms to ensure efficiency, so that same
candidates are not repeatedly visited; absence of structural bias, which, if
present, would predispose the algorithm towards limiting its search to some
regions of solution space. The first two of these properties have been
extensively investigated, however the third is little understood. In this
article we provide theoretical and empirical analyses that contribute to the
understanding of structural bias. We prove a theorem concerning dynamics of
population variance in the case of real-valued search spaces. This reveals how
structural bias can manifest as non-uniform clustering of population over time.
Theory predicts that structural bias is exacerbated with increasing population
size and problem difficulty. These predictions reveal two previously
unrecognised aspects of structural bias. Respectively, increasing population
size, though ostensibly promoting diversity, will magnify any inherent
structural bias, and effects of structural bias are more apparent when faced
with difficult problems. Our theoretical result also suggests that two commonly
used approaches to enhancing exploration, increasing population size and
increasing disruptiveness of search operators, have quite distinct implications
in terms of structural bias."
"We introduce a novel framework of reservoir computing. Cellular automaton is
used as the reservoir of dynamical systems. Input is randomly projected onto
the initial conditions of automaton cells and nonlinear computation is
performed on the input via application of a rule in the automaton for a period
of time. The evolution of the automaton creates a space-time volume of the
automaton state space, and it is used as the reservoir. The proposed framework
is capable of long short-term memory and it requires orders of magnitude less
computation compared to Echo State Networks. Also, for additive cellular
automaton rules, reservoir features can be combined using Boolean operations,
which provides a direct way for concept building and symbolic processing, and
it is much more efficient compared to state-of-the-art approaches."
"The unit commitment (UC) problem is a nonlinear, high-dimensional, highly
constrained, mixed-integer power system optimization problem and is generally
solved in the literature considering minimizing the system operation cost as
the only objective. However, due to increasing environmental concerns, the
recent attention has shifted to incorporating emission in the problem
formulation. In this paper, a multi-objective evolutionary algorithm based on
decomposition (MOEA/D) is proposed to solve the UC problem as a multi-objective
optimization problem considering minimizing cost and emission as the multiple
objec- tives. Since, UC problem is a mixed-integer optimization problem
consisting of binary UC variables and continuous power dispatch variables, a
novel hybridization strategy is proposed within the framework of MOEA/D such
that genetic algorithm (GA) evolves the binary variables while differential
evolution (DE) evolves the continuous variables. Further, a novel non-uniform
weight vector distribution strategy is proposed and a parallel island model
based on combination of MOEA/D with uniform and non-uniform weight vector
distribution strategy is implemented to enhance the performance of the
presented algorithm. Extensive case studies are presented on different test
systems and the effectiveness of the proposed hybridization strategy, the
non-uniform weight vector distribution strategy and parallel island model is
verified through stringent simulated results. Further, exhaustive benchmarking
against the algorithms proposed in the literature is presented to demonstrate
the superiority of the proposed algorithm in obtaining significantly better
converged and uniformly distributed trade-off solutions."
"Evolutionary robotics is a promising approach to autonomously synthesize
machines with abilities that resemble those of animals, but the field suffers
from a lack of strong foundations. In particular, evolutionary systems are
currently assessed solely by the fitness score their evolved artifacts can
achieve for a specific task, whereas such fitness-based comparisons provide
limited insights about how the same system would evaluate on different tasks,
and its adaptive capabilities to respond to changes in fitness (e.g., from
damages to the machine, or in new situations). To counter these limitations, we
introduce the concept of ""evolvability signatures"", which picture the
post-mutation statistical distribution of both behavior diversity (how
different are the robot behaviors after a mutation?) and fitness values (how
different is the fitness after a mutation?). We tested the relevance of this
concept by evolving controllers for hexapod robot locomotion using five
different genotype-to-phenotype mappings (direct encoding, generative encoding
of open-loop and closed-loop central pattern generators, generative encoding of
neural networks, and single-unit pattern generators (SUPG)). We observed a
predictive relationship between the evolvability signature of each encoding and
the number of generations required by hexapods to adapt from incurred damages.
Our study also reveals that, across the five investigated encodings, the SUPG
scheme achieved the best evolvability signature, and was always foremost in
recovering an effective gait following robot damages. Overall, our evolvability
signatures neatly complement existing task-performance benchmarks, and pave the
way for stronger foundations for research in evolutionary robotics."
"We extend the capabilities of neural networks by coupling them to external
memory resources, which they can interact with by attentional processes. The
combined system is analogous to a Turing Machine or Von Neumann architecture
but is differentiable end-to-end, allowing it to be efficiently trained with
gradient descent. Preliminary results demonstrate that Neural Turing Machines
can infer simple algorithms such as copying, sorting, and associative recall
from input and output examples."
"Advanced inventory management in complex supply chains requires effective and
robust nonlinear optimization due to the stochastic nature of supply and demand
variations. Application of estimated gradients can boost up the convergence of
Particle Swarm Optimization (PSO) algorithm but classical gradient calculation
cannot be applied to stochastic and uncertain systems. In these situations
Monte-Carlo (MC) simulation can be applied to determine the gradient. We
developed a memory based algorithm where instead of generating and evaluating
new simulated samples the stored and shared former function evaluations of the
particles are sampled to estimate the gradients by local weighted least squares
regression. The performance of the resulted regional gradient-based PSO is
verified by several benchmark problems and in a complex application example
where optimal reorder points of a supply chain are determined."
"This paper surveys research on applying neuroevolution (NE) to games. In
neuroevolution, artificial neural networks are trained through evolutionary
algorithms, taking inspiration from the way biological brains evolved. We
analyse the application of NE in games along five different axes, which are the
role NE is chosen to play in a game, the different types of neural networks
used, the way these networks are evolved, how the fitness is determined and
what type of input the network receives. The article also highlights important
open research challenges in the field."
"A new hybridization of the Cuckoo Search (CS) is developed and applied to
optimize multi-cell solar systems; namely multi-junction and split spectrum
cells. The new approach consists of combining the CS with the Nelder-Mead
method. More precisely, instead of using single solutions as nests for the CS,
we use the concept of a simplex which is used in the Nelder-Mead algorithm.
This makes it possible to use the flip operation introduces in the Nelder-Mead
algorithm instead of the Levy flight which is a standard part of the CS. In
this way, the hybridized algorithm becomes more robust and less sensitive to
parameter tuning which exists in CS. The goal of our work was to optimize the
performance of multi-cell solar systems. Although the underlying problem
consists of the minimization of a function of a relatively small number of
parameters, the difficulty comes from the fact that the evaluation of the
function is complex and only a small number of evaluations is possible. In our
test, we show that the new method has a better performance when compared to
similar but more compex hybridizations of Nelder-Mead algorithm using genetic
algorithms or particle swarm optimization on standard benchmark functions.
Finally, we show that the new method outperforms some standard meta-heuristics
for the problem of interest."
"This paper research review Ant colony optimization (ACO) and Genetic
Algorithm (GA), both are two powerful meta-heuristics. This paper explains some
major defects of these two algorithm at first then proposes a new model for ACO
in which, artificial ants use a quick genetic operator and accelerate their
actions in selecting next state. Experimental results show that proposed hybrid
algorithm is effective and its performance including speed and accuracy beats
other version."
"Ant Colony Algorithm (ACA) and Genetic Local Search (GLS) are two
optimization algorithms that have been successfully applied to the Traveling
Salesman Problem (TSP). In this paper we define new crossover operator then
redefine ACAs ants as operate according to defined crossover operator then put
forward our GLS that uses these ants to solve Symmetric TSP (STSP) instances."
"The search ability of an Evolutionary Algorithm (EA) depends on the variation
among the individuals in the population. Maintaining an optimal level of
diversity in the EA population is imperative to ensure that progress of the EA
search is unhindered by premature convergence to suboptimal solutions. Clearer
understanding of the concept of population diversity, in the context of
evolutionary search and premature convergence in particular, is the key to
designing efficient EAs. To this end, this paper first presents a comprehensive
analysis of the EA population diversity issues. Next we present an
investigation on a counter-niching EA technique that introduces and maintains
constructive diversity in the population. The proposed approach uses informed
genetic operations to reach promising, but un-explored or under-explored areas
of the search space, while discouraging premature local convergence. Simulation
runs on a number of standard benchmark test functions with Genetic Algorithm
(GA) implementation shows promising results."
"This Paper will deal with a combination of Ant Colony and Genetic Programming
Algorithm to optimize Travelling Salesmen problem (NP-Hard). However, the
complexity of the algorithm requires considerable computational time and
resources. Parallel implementation can reduce the computational time. In this
paper, emphasis in the parallelizing section is given to Multi-core
architecture and Multi-Processor Systems which is developed and used almost
everywhere today and hence, multi-core parallelization to the combination of
algorithm is achieved by OpenMP library by Intel Corporation."
"The model of interaction between learning and evolutionary optimization is
designed and investigated. The evolving population of modeled organisms is
considered. The mechanism of the genetic assimilation of the acquired features
during a number of generations of Darwinian evolution is studied. It is shown
that the genetic assimilation takes place as follows: phenotypes of modeled
organisms move towards the optimum at learning; then the selection takes place;
genotypes of selected organisms also move towards the optimum. The hiding
effect is also studied; this effect means that strong learning can inhibit the
evolutionary search for the optimal genotype. The mechanism of influence of the
learning load on the interaction between learning and evolution is analyzed. It
is shown that the learning load can lead to a significant acceleration of
evolution."
"Recurrent networks with transfer functions that fulfill the Lipschitz
continuity with K=1 may be echo state networks if certain limitations on the
recurrent connectivity are applied. It has been shown that it is sufficient if
the largest singular value of the recurrent connectivity is smaller than 1. The
main achievement of this paper is a proof under which conditions the network is
an echo state network even if the largest singular value is one. It turns out
that in this critical case the exact shape of the transfer function plays a
decisive role in determining whether the network still fulfills the echo state
condition. In addition, several examples with one neuron networks are outlined
to illustrate effects of critical connectivity. Moreover, within the manuscript
a mathematical definition for a critical echo state network is suggested."
"Estimation of Distribution Algorithms (EDAs) require flexible probability
models that can be efficiently learned and sampled. Restricted Boltzmann
Machines (RBMs) are generative neural networks with these desired properties.
We integrate an RBM into an EDA and evaluate the performance of this system in
solving combinatorial optimization problems with a single objective. We assess
how the number of fitness evaluations and the CPU time scale with problem size
and with problem complexity. The results are compared to the Bayesian
Optimization Algorithm, a state-of-the-art EDA. Although RBM-EDA requires
larger population sizes and a larger number of fitness evaluations, it
outperforms BOA in terms of CPU times, in particular if the problem is large or
complex. RBM-EDA requires less time for model building than BOA. These results
highlight the potential of using generative neural networks for combinatorial
optimization."
"Most experimental studies initialize the population of evolutionary
algorithms with random genotypes. In practice, however, optimizers are
typically seeded with good candidate solutions either previously known or
created according to some problem-specific method. This ""seeding"" has been
studied extensively for single-objective problems. For multi-objective
problems, however, very little literature is available on the approaches to
seeding and their individual benefits and disadvantages. In this article, we
are trying to narrow this gap via a comprehensive computational study on common
real-valued test functions. We investigate the effect of two seeding techniques
for five algorithms on 48 optimization problems with 2, 3, 4, 6, and 8
objectives. We observe that some functions (e.g., DTLZ4 and the LZ family)
benefit significantly from seeding, while others (e.g., WFG) profit less. The
advantage of seeding also depends on the examined algorithm."
"We derive a synaptic weight update rule for learning temporally precise spike
train to spike train transformations in multilayer feedforward networks of
spiking neurons. The framework, aimed at seamlessly generalizing error
backpropagation to the deterministic spiking neuron setting, is based strictly
on spike timing and avoids invoking concepts pertaining to spike rates or
probabilistic models of spiking. The derivation is founded on two innovations.
First, an error functional is proposed that compares the spike train emitted by
the output neuron of the network to the desired spike train by way of their
putative impact on a virtual postsynaptic neuron. This formulation sidesteps
the need for spike alignment and leads to closed form solutions for all
quantities of interest. Second, virtual assignment of weights to spikes rather
than synapses enables a perturbation analysis of individual spike times and
synaptic weights of the output as well as all intermediate neurons in the
network, which yields the gradients of the error functional with respect to the
said entities. Learning proceeds via a gradient descent mechanism that
leverages these quantities. Simulation experiments demonstrate the efficacy of
the proposed learning framework. The experiments also highlight asymmetries
between synapses on excitatory and inhibitory neurons."
"We present a technique for developing a network of re-used features, where
the topology is formed using a coarse learning method, that allows
gradient-descent fine tuning, known as an Abstract Deep Network (ADN). New
features are built based on observed co-occurrences, and the network is
maintained using a selection process related to evolutionary algorithms. This
allows coarse ex- ploration of the problem space, effective for irregular
domains, while gradient descent allows pre- cise solutions. Accuracy on
standard UCI and Protein-Structure Prediction problems is comparable with
benchmark SVM and optimized GBML approaches, and shows scalability for
addressing large problems. The discrete implementation is symbolic, allowing
interpretability, while the continuous method using fine-tuning shows improved
accuracy. The binary multiplexer problem is explored, as an irregular domain
that does not support gradient descent learning, showing solution to the bench-
mark 135-bit problem. A convolutional implementation is demonstrated on image
classification, showing an error-rate of 0.79% on the MNIST problem, without a
pre-defined topology. The ADN system provides a method for developing a very
sparse, deep feature topology, based on observed relationships between
features, that is able to find solutions in irregular domains, and initialize a
network prior to gradient descent learning."
"In this paper, we perform an experimental study of optimal recombination
operator for makespan minimization problem on single machine with
sequence-dependent setup times ($1|s_{vu}|C_{\max}$). The computational
experiment on benchmark problems from TSPLIB library indicates practical
applicability of optimal recombination in crossover operator of genetic
algorithm for $1|s_{vu}|C_{\max}$."
"In this paper, we continue the efforts of the Computational Theory of
Intelligence (CTI) by extending concepts to include computational processes in
terms of Genetic Algorithms (GA's) and Turing Machines (TM's). Active, Passive,
and Hybrid Computational Intelligence processes are also introduced and
discussed. We consider the ramifications of the assumptions of CTI with regard
to the qualities of reproduction and virility. Applications to Biology,
Computer Science and Cyber Security are also discussed."
"In this study we want to connect our previously proposed context-relevant
topographical maps with the deep learning community. Our architecture is a
classifier with hidden layers that are hierarchical two-dimensional
topographical maps. These maps differ from the conventional self-organizing
maps in that their organizations are influenced by the context of the data
labels in a top-down manner. In this way bottom-up and top-down learning are
combined in a biologically relevant representational learning setting. Compared
to our previous work, we are here specifically elaborating the model in a more
challenging setting compared to our previous experiments and to advance more
hidden representation layers to bring our discussions into the context of deep
representational learning."
"To improve the problem that the parameter identification for fuzzy neural
network has many time complexities in calculating, an improved T-S fuzzy
inference method and an parameter identification method for fuzzy neural
network are proposed. It mainly includes three parts. First, improved fuzzy
inference method based on production term for T-S Fuzzy model is explained.
Then, compared with existing Sugeno fuzzy inference based on Compositional
rules and type-distance fuzzy inference method, the proposed fuzzy inference
algorithm has a less amount of complexity in calculating and the calculating
process is simple. Next, a parameter identification method for FNN based on
production inference is proposed. Finally, the proposed method is applied for
the precipitation forecast and security situation prediction. Test results
showed that the proposed method significantly improved the effectiveness of
identification, reduced the learning order, time complexity and learning error."
"This article introduces a robust hybrid method for solving supervised
learning tasks, which uses the Echo State Network (ESN) model and the Particle
Swarm Optimization (PSO) algorithm. An ESN is a Recurrent Neural Network with
the hidden-hidden weights fixed in the learning process. The recurrent part of
the network stores the input information in internal states of the network.
Another structure forms a free-memory method used as supervised learning tool.
The setting procedure for initializing the recurrent structure of the ESN model
can impact on the model performance. On the other hand, the PSO has been shown
to be a successful technique for finding optimal points in complex spaces.
Here, we present an approach to use the PSO for finding some initial
hidden-hidden weights of the ESN model. We present empirical results that
compare the canonical ESN model with this hybrid method on a wide range of
benchmark problems."
"This technical report includes the introduction and ranking results of the
ICSI 2014 Competition on Single Objective Optimization."
"Evolutionary algorithms (EAs) are very popular tools to design and evolve
artificial neural networks (ANNs), especially to train them. These methods have
advantages over the conventional backpropagation (BP) method because of their
low computational requirement when searching in a large solution space. In this
paper, we employ Chemical Reaction Optimization (CRO), a newly developed global
optimization method, to replace BP in training neural networks. CRO is a
population-based metaheuristics mimicking the transition of molecules and their
interactions in a chemical reaction. Simulation results show that CRO
outperforms many EA strategies commonly used to train neural networks."
"Chemical Reaction Optimization (CRO) is a powerful metaheuristic which mimics
the interactions of molecules in chemical reactions to search for the global
optimum. The perturbation function greatly influences the performance of CRO on
solving different continuous problems. In this paper, we study four different
probability distributions, namely, the Gaussian distribution, the Cauchy
distribution, the exponential distribution, and a modified Rayleigh
distribution, for the perturbation function of CRO. Different distributions
have different impacts on the solutions. The distributions are tested by a set
of well-known benchmark functions and simulation results show that problems
with different characteristics have different preference on the distribution
function. Our study gives guidelines to design CRO for different types of
optimization problems."
"Air pollution monitoring is a very popular research topic and many monitoring
systems have been developed. In this paper, we formulate the Bus Sensor
Deployment Problem (BSDP) to select the bus routes on which sensors are
deployed, and we use Chemical Reaction Optimization (CRO) to solve BSDP. CRO is
a recently proposed metaheuristic designed to solve a wide range of
optimization problems. Using the real world data, namely Hong Kong Island bus
route data, we perform a series of simulations and the results show that CRO is
capable of solving this optimization problem efficiently."
"An electric vehicle (EV) may be used as energy storage which allows the
bi-directional electricity flow between the vehicle's battery and the electric
power grid. In order to flatten the load profile of the electricity system, EV
scheduling has become a hot research topic in recent years. In this paper, we
propose a new formulation of the joint scheduling of EV and Unit Commitment
(UC), called EVUC. Our formulation considers the characteristics of EVs while
optimizing the system total running cost. We employ Chemical Reaction
Optimization (CRO), a general-purpose optimization algorithm to solve this
problem and the simulation results on a widely used set of instances indicate
that CRO can effectively optimize this problem."
"Optimization techniques are frequently applied in science and engineering
research and development. Evolutionary algorithms, as a kind of general-purpose
metaheuristic, have been shown to be very effective in solving a wide range of
optimization problems. A recently proposed chemical-reaction-inspired
metaheuristic, Chemical Reaction Optimization (CRO), has been applied to solve
many global optimization problems. However, the functionality of the
inter-molecular ineffective collision operator in the canonical CRO design
overlaps that of the on-wall ineffective collision operator, which can
potential impair the overall performance. In this paper we propose a new
inter-molecular ineffective collision operator for CRO for global optimization.
To fully utilize our newly proposed operator, we also design a scheme to adapt
the algorithm to optimization problems with different search space
characteristics. We analyze the performance of our proposed algorithm with a
number of widely used benchmark functions. The simulation results indicate that
the new algorithm has superior performance over the canonical CRO."
"The set covering problem (SCP) is one of the representative combinatorial
optimization problems, having many practical applications. This paper
investigates the development of an algorithm to solve SCP by employing chemical
reaction optimization (CRO), a general-purpose metaheuristic. It is tested on a
wide range of benchmark instances of SCP. The simulation results indicate that
this algorithm gives outstanding performance compared with other heuristics and
metaheuristics in solving SCP."
"Echo state networks (ESN), a type of reservoir computing (RC) architecture,
are efficient and accurate artificial neural systems for time series processing
and learning. An ESN consists of a core of recurrent neural networks, called a
reservoir, with a small number of tunable parameters to generate a
high-dimensional representation of an input, and a readout layer which is
easily trained using regression to produce a desired output from the reservoir
states. Certain computational tasks involve real-time calculation of high-order
time correlations, which requires nonlinear transformation either in the
reservoir or the readout layer. Traditional ESN employs a reservoir with
sigmoid or tanh function neurons. In contrast, some types of biological neurons
obey response curves that can be described as a product unit rather than a sum
and threshold. Inspired by this class of neurons, we introduce a RC
architecture with a reservoir of product nodes for time series computation. We
find that the product RC shows many properties of standard ESN such as
short-term memory and nonlinear capacity. On standard benchmarks for chaotic
prediction tasks, the product RC maintains the performance of a standard
nonlinear ESN while being more amenable to mathematical analysis. Our study
provides evidence that such networks are powerful in highly nonlinear tasks
owing to high-order statistics generated by the recurrent product node
reservoir."
"The growing complexity of real-world problems has motivated computer
scientists to search for efficient problem-solving methods. Metaheuristics
based on evolutionary computation and swarm intelligence are outstanding
examples of nature-inspired solution techniques. Inspired by the social
spiders, we propose a novel Social Spider Algorithm to solve global
optimization problems. This algorithm is mainly based on the foraging strategy
of social spiders, utilizing the vibrations on the spider web to determine the
positions of preys. Different from the previously proposed swarm intelligence
algorithms, we introduce a new social animal foraging strategy model to solve
optimization problems. In addition, we perform preliminary parameter
sensitivity analysis for our proposed algorithm, developing guidelines for
choosing the parameter values. The Social Spider Algorithm is evaluated by a
series of widely-used benchmark functions, and our proposed algorithm has
superior performance compared with other state-of-the-art metaheuristics."
"In this research paper novel real/complex valued recurrent Hopfield Neural
Network (RHNN) is proposed. The method of synthesizing the energy landscape of
such a network and the experimental investigation of dynamics of Recurrent
Hopfield Network is discussed. Parallel modes of operation (other than fully
parallel mode) in layered RHNN is proposed. Also, certain potential
applications are proposed."
"The benefit of sexual recombination is one of the most fundamental questions
both in population genetics and evolutionary computation. It is widely believed
that recombination helps solving difficult optimization problems. We present
the first result, which rigorously proves that it is beneficial to use sexual
recombination in an uncertain environment with a noisy fitness function. For
this, we model sexual recombination with a simple estimation of distribution
algorithm called the Compact Genetic Algorithm (cGA), which we compare with the
classical $\mu+1$ EA. For a simple noisy fitness function with additive
Gaussian posterior noise $\mathcal{N}(0,\sigma^2)$, we prove that the
mutation-only $\mu+1$ EA typically cannot handle noise in polynomial time for
$\sigma^2$ large enough while the cGA runs in polynomial time as long as the
population size is not too small. This shows that in this uncertain environment
sexual recombination is provably beneficial. We observe the same behavior in a
small empirical study."
"Multi-objective optimisation is regarded as one of the most promising ways
for dealing with constrained optimisation problems in evolutionary
optimisation. This paper presents a theoretical investigation of a
multi-objective optimisation evolutionary algorithm for solving the 0-1
knapsack problem. Two initialisation methods are considered in the algorithm:
local search initialisation and greedy search initialisation. Then the solution
quality of the algorithm is analysed in terms of the approximation ratio."
"Supralinear and sublinear pre-synaptic and dendritic integration is
considered to be responsible for nonlinear computation power of biological
neurons, emphasizing the role of nonlinear integration as opposed to nonlinear
output thresholding. How, why, and to what degree the transfer function
nonlinearity helps biologically inspired neural network models is not fully
understood. Here, we study these questions in the context of echo state
networks (ESN). ESN is a simple neural network architecture in which a fixed
recurrent network is driven with an input signal, and the output is generated
by a readout layer from the measurements of the network states. ESN
architecture enjoys efficient training and good performance on certain
signal-processing tasks, such as system identification and time series
prediction. ESN performance has been analyzed with respect to the connectivity
pattern in the network structure and the input bias. However, the effects of
the transfer function in the network have not been studied systematically.
Here, we use an approach tanh on the Taylor expansion of a frequently used
transfer function, the hyperbolic tangent function, to systematically study the
effect of increasing nonlinearity of the transfer function on the memory,
nonlinear capacity, and signal processing performance of ESN. Interestingly, we
find that a quadratic approximation is enough to capture the computational
power of ESN with tanh function. The results of this study apply to both
software and hardware implementation of ESN."
"We study the expressive power of positive neural networks. The model uses
positive connection weights and multiple input neurons. Different behaviors can
be expressed by varying the connection weights. We show that in discrete time,
and in absence of noise, the class of positive neural networks captures the
so-called monotone-regular behaviors, that are based on regular languages. A
finer picture emerges if one takes into account the delay by which a
monotone-regular behavior is implemented. Each monotone-regular behavior can be
implemented by a positive neural network with a delay of one time unit. Some
monotone-regular behaviors can be implemented with zero delay. And,
interestingly, some simple monotone-regular behaviors can not be implemented
with zero delay."
"In the biological nervous system, large neuronal populations work
collaboratively to encode sensory stimuli. These neuronal populations are
characterised by a diverse distribution of tuning curves, ensuring that the
entire range of input stimuli is encoded. Based on these principles, we have
designed a neuromorphic system called a Trainable Analogue Block (TAB), which
encodes given input stimuli using a large population of neurons with a
heterogeneous tuning curve profile. Heterogeneity of tuning curves is achieved
using random device mismatches in VLSI (Very Large Scale Integration) process
and by adding a systematic offset to each hidden neuron. Here, we present
measurement results of a single test cell fabricated in a 65nm technology to
verify the TAB framework. We have mimicked a large population of neurons by
re-using measurement results from the test cell by varying offset. We thus
demonstrate the learning capability of the system for various regression tasks.
The TAB system may pave the way to improve the design of analogue circuits for
commercial applications, by rendering circuits insensitive to random mismatch
that arises due to the manufacturing process."
"We demonstrate a genetic algorithm that employs a versatile fitness function
to optimize route selection for the Hyperloop, a proposed high speed passenger
transportation system."
"In this paper, we propose a realistic mathematical model taking into account
the mutual interference among the interacting populations. This model attempts
to describe the control (vaccination) function as a function of the number of
infective individuals, which is an improvement over the existing susceptible
infective epidemic models. Regarding the growth of the epidemic as a nonlinear
phenomenon we have developed a neural network architecture to estimate the
vital parameters associated with this model. This architecture is based on a
recently developed new class of neural networks known as co-operative and
supportive neural networks. The application of this architecture to the present
study involves preprocessing of the input data, and this renders an efficient
estimation of the rate of spread of the epidemic. It is observed that the
proposed new neural network outperforms a simple feed-forward neural network
and polynomial regression."
"Estimation of Distribution Algorithms (EDAs) require flexible probability
models that can be efficiently learned and sampled. Autoencoders (AE) are
generative stochastic networks with these desired properties. We integrate a
special type of AE, the Denoising Autoencoder (DAE), into an EDA and evaluate
the performance of DAE-EDA on several combinatorial optimization problems with
a single objective. We asses the number of fitness evaluations as well as the
required CPU times. We compare the results to the performance to the Bayesian
Optimization Algorithm (BOA) and RBM-EDA, another EDA which is based on a
generative neural network which has proven competitive with BOA. For the
considered problem instances, DAE-EDA is considerably faster than BOA and
RBM-EDA, sometimes by orders of magnitude. The number of fitness evaluations is
higher than for BOA, but competitive with RBM-EDA. These results show that DAEs
can be useful tools for problems with low but non-negligible fitness evaluation
costs."
"Financial forecasting is an estimation of future financial outcomes for a
company, industry, country using historical internal accounting and sales data.
We may predict the future outcome of BSE_SENSEX practically by some soft
computing techniques and can also optimized using PSO (Particle Swarm
Optimization), EA (Evolutionary Algorithm) or DEA (Differential Evolutionary
Algorithm) etc. PSO is a biologically inspired computational search &
optimization method developed in 1995 by Dr. Eberhart and Dr. Kennedy based on
the social behaviors of fish schooling or birds flocking. PSO is a promising
method to train Artificial Neural Network (ANN). It is easy to implement then
Genetic Algorithm except few parameters are adjusted. PSO is a random & pattern
search technique based on populating of particle. In PSO, the particles are
having some position and velocity in the search space. Two terms are used in
PSO one is Local Best and another one is Global Best. To optimize problems that
are like Irregular, Noisy, Change over time, Static etc. PSO uses a classic
optimization method such as Gradient Decent & Quasi-Newton Methods. The
observation and review of few related studies in the last few years, focusing
on function of PSO, modification of PSO and operation that have implemented
using PSO like function optimization, ANN Training & Fuzzy Control etc.
Differential Evolution is an efficient EA technique for optimization of
numerical problems, financial problems etc. PSO technique is introduced due to
the swarming behavior of animals which is the collective behavior of similar
size that aggregates together."
"This paper presents a comparative analysis of the performance of the
Incremental Ant Colony algorithm for continuous optimization
($IACO_\mathbb{R}$), with different algorithms provided in the NLopt library.
The key objective is to understand how the various algorithms in the NLopt
library perform in combination with the Multi Trajectory Local Search (Mtsls1)
technique. A hybrid approach has been introduced in the local search strategy
by the use of a parameter which allows for probabilistic selection between
Mtsls1 and a NLopt algorithm. In case of stagnation, the algorithm switch is
made based on the algorithm being used in the previous iteration. The paper
presents an exhaustive comparison on the performance of these approaches on
Soft Computing (SOCO) and Congress on Evolutionary Computation (CEC) 2014
benchmarks. For both benchmarks, we conclude that the best performing algorithm
is a hybrid variant of Mtsls1 with BFGS for local search."
"Traffic is a problem in many urban areas worldwide. Traffic flow is dictated
by certain devices such as traffic lights. The traffic lights signal when each
lane is able to pass through the intersection. Often, static schedules
interfere with ideal traffic flow. The purpose of this project was to find a
way to make intersections controlled with traffic lights more efficient. This
goal was accomplished through the creation of a genetic algorithm, which
enhances an input algorithm through genetic principles to produce the fittest
algorithm. The program was comprised of two major elements: coding in Java and
coding in Simulation of Urban Mobility (SUMO), which is an environment that
simulates real traffic. The Java code called upon the SUMO simulation via a
command prompt which ran the simulation, received the output, altered the
algorithm, and looped. The SUMO component initialized a simulation in which a 1
x 1 street layout was created, each intersection with its own traffic light.
Each loop enhanced the input algorithm by altering the scheduling string
(dictates the light changes). After the looped simulations were executed, the
data was then analyzed. This was accomplished by creating an algorithm based
upon regular practice, timed traffic lights, and comparing the output which was
comprised of the total time it took for all vehicles to exit the system and the
average time it took each individual vehicle to exit the system. These
different variables: the time it took the average vehicle to exit the system
and total time for all vehicles to exit the system, where then graphed together
to provide a visual aid. The genetic algorithm did improve traffic light and
traffic flow efficiency in comparison to traditional scheduling methods."
"IR or near-infrared (NIR) spectroscopy is a method used to identify a
compound or to analyze the composition of a material. Calibration of NIR
spectra refers to the use of the spectra as multivariate descriptors to predict
concentrations of the constituents. To build a calibration model,
state-of-the-art software predominantly uses linear regression techniques. For
nonlinear calibration problems, neural network-based models have proved to be
an interesting alternative. In this paper, we propose a novel extension of the
conventional neural network-based approach, the use of an ensemble of neural
network models. The individual neural networks are obtained by resampling the
available training data with bootstrapping or cross-validation techniques. The
results obtained for a realistic calibration example show that the
ensemble-based approach produces a significantly more accurate and robust
calibration model than conventional regression methods."
"The distribution system problems, such as planning, loss minimization, and
energy restoration, usually involve the phase balancing or network
reconfiguration procedures. The determination of an optimal phase balance is,
in general, a combinatorial optimization problem. This paper proposes optimal
reconfiguration of the phase balancing using the neural network, to switch on
and off the different switches, allowing the three phases supply by the
transformer to the end-users to be balanced. This paper presents the
application examples of the proposed method using the real and simulated test
data."
"We characterize the structure of the periods of a neuronal recurrence
equation. Firstly, we give a characterization of k-chains in 0-1 periodic
sequences. Secondly, we characterize the periods of all cycles of some neuronal
recurrence equation. Thirdly, we explain how these results can be used to
deduce the existence of the generalized period-halving bifurcation."
"Restricted Boltzmann Machines and Deep Belief Networks have been successfully
used in a wide variety of applications including image classification and
speech recognition. Inference and learning in these algorithms uses a Markov
Chain Monte Carlo procedure called Gibbs sampling. A sigmoidal function forms
the kernel of this sampler which can be realized from the firing statistics of
noisy integrate-and-fire neurons on a neuromorphic VLSI substrate. This paper
demonstrates such an implementation on an array of digital spiking neurons with
stochastic leak and threshold properties for inference tasks and presents some
key performance metrics for such a hardware-based sampler in both the
generative and discriminative contexts."
"Neural gas (NG) is a robust vector quantization algorithm with a well-known
mathematical model. According to this, the neural gas samples the underlying
data distribution following a power law with a magnification exponent that
depends on data dimensionality only. The effects of shape in the input data
distribution, however, are not entirely covered by the NG model above, due to
the technical difficulties involved. The experimental work described here shows
that shape is indeed relevant in determining the overall NG behavior; in
particular, some experiments reveal richer and complex behaviors induced by
shape that cannot be explained by the power law alone. Although a more
comprehensive analytical model remains to be defined, the evidence collected in
these experiments suggests that the NG algorithm has an interesting potential
for detecting complex shapes in noisy datasets."
"Information encoding in the nervous system is supported through the precise
spike-timings of neurons; however, an understanding of the underlying processes
by which such representations are formed in the first place remains unclear.
Here we examine how networks of spiking neurons can learn to encode for input
patterns using a fully temporal coding scheme. To this end, we introduce a
learning rule for spiking networks containing hidden neurons which optimizes
the likelihood of generating desired output spiking patterns. We show the
proposed learning rule allows for a large number of accurate input-output spike
pattern mappings to be learnt, which outperforms other existing learning rules
for spiking neural networks: both in the number of mappings that can be learnt
as well as the complexity of spike train encodings that can be utilised. The
learning rule is successful even in the presence of input noise, is
demonstrated to solve the linearly non-separable XOR computation and
generalizes well on an example dataset. We further present a biologically
plausible implementation of backpropagated learning in multilayer spiking
networks, and discuss the neural mechanisms that might underlie its function.
Our approach contributes both to a systematic understanding of how pattern
encodings might take place in the nervous system, and a learning rule that
displays strong technical capability."
"In this paper, we design a set of multi-objective constrained optimization
problems (MCOPs) and propose a new repair operator to address them. The
proposed repair operator is used to fix the solutions that violate the box
constraints. More specifically, it employs a reversed correction strategy that
can effectively avoid the population falling into local optimum. In addition,
we integrate the proposed repair operator into two classical multi-objective
evolutionary algorithms MOEA/D and NSGA-II. The proposed repair operator is
compared with other two kinds of commonly used repair operators on benchmark
problems CTPs and MCOPs. The experiment results demonstrate that our proposed
approach is very effective in terms of convergence and diversity."
"The Travelling Salesman Problem (TSP) is one of the most famous optimization
problems. The Genetic Algorithm (GA) is one of metaheuristics that have been
applied to TSP. The Crossover and mutation operators are two important elements
of GA. There are many TSP solver crossover operators. In this paper, we state
implementation of some recent TSP solver crossovers at first and then we use
each of them in GA to solve some Symmetric TSP (STSP) instances and finally
compare their effects on speed and accuracy of presented GA."
"While evolutionary algorithms are known to be very successful for a broad
range of applications, the algorithm designer is often left with many
algorithmic choices, for example, the size of the population, the mutation
rates, and the crossover rates of the algorithm. These parameters are known to
have a crucial influence on the optimization time, and thus need to be chosen
carefully, a task that often requires substantial efforts. Moreover, the
optimal parameters can change during the optimization process. It is therefore
of great interest to design mechanisms that dynamically choose best-possible
parameters. An example for such an update mechanism is the one-fifth success
rule for step-size adaption in evolutionary strategies. While in continuous
domains this principle is well understood also from a mathematical point of
view, no comparable theory is available for problems in discrete domains.
  In this work we show that the one-fifth success rule can be effective also in
discrete settings. We regard the $(1+(\lambda,\lambda))$~GA proposed in
[Doerr/Doerr/Ebel: From black-box complexity to designing new genetic
algorithms, TCS 2015]. We prove that if its population size is chosen according
to the one-fifth success rule then the expected optimization time on
\textsc{OneMax} is linear. This is better than what \emph{any} static
population size $\lambda$ can achieve and is asymptotically optimal also among
all adaptive parameter choices."
"Evolutionary Algorithms (EAs) are being routinely applied for a variety of
optimization tasks, and real-parameter optimization in the presence of
constraints is one such important area. During constrained optimization EAs
often create solutions that fall outside the feasible region; hence a viable
constraint- handling strategy is needed. This paper focuses on the class of
constraint-handling strategies that repair infeasible solutions by bringing
them back into the search space and explicitly preserve feasibility of the
solutions. Several existing constraint-handling strategies are studied, and two
new single parameter constraint-handling methodologies based on parent-centric
and inverse parabolic probability (IP) distribution are proposed. The existing
and newly proposed constraint-handling methods are first studied with PSO, DE,
GAs, and simulation results on four scalable test-problems under different
location settings of the optimum are presented. The newly proposed
constraint-handling methods exhibit robustness in terms of performance and also
succeed on search spaces comprising up-to 500 variables while locating the
optimum within an error of 10$^{-10}$. The working principle of the IP based
methods is also demonstrated on (i) some generic constrained optimization
problems, and (ii) a classic `Weld' problem from structural design and
mechanics. The successful performance of the proposed methods clearly exhibits
their efficacy as a generic constrained-handling strategy for a wide range of
applications."
"This paper presents a multi-swarm PSO algorithm for the Quadratic Assignment
Problem (QAP) implemented on OpenCL platform. Our work was motivated by results
of time efficiency tests performed for single-swarm algorithm implementation
that showed clearly that the benefits of a parallel execution platform can be
fully exploited, if the processed population is large. The described algorithm
can be executed in two modes: with independent swarms or with migration. We
discuss the algorithm construction, as well as we report results of tests
performed on several problem instances from the QAPLIB library. During the
experiments the algorithm was configured to process large populations. This
allowed us to collect statistical data related to values of goal function
reached by individual particles. We use them to demonstrate on two test cases
that although single particles seem to behave chaotically during the
optimization process, when the whole population is analyzed, the probability
that a particle will select a near-optimal solution grows."
"Swarm intelligence is all about developing collective behaviours to solve
complex, ill-structured and large-scale problems. Efficiency in collective
behaviours depends on how to harmonise the individual contributions so that a
complementary collective effort can be achieved to offer a useful solution. The
main points in organising the harmony remains as managing the diversification
and intensification actions appropriately, where the efficiency of collective
behaviours depends on blending these two actions appropriately. In this study,
two swarm intelligence algorithms inspired of natural honeybee colonies have
been overviewed with many respects and two new revisions and a hybrid version
have been studied to improve the efficiencies in solving numerical optimisation
problems, which are well-known hard benchmarks. Consequently, the revisions and
especially the hybrid algorithm proposed have outperformed the two original bee
algorithms in solving these very hard numerical optimisation benchmarks."
"Neural network algorithms simulated on standard computing platforms typically
make use of high resolution weights, with floating-point notation. However, for
dedicated hardware implementations of such algorithms, fixed-point synaptic
weights with low resolution are preferable. The basic approach of reducing the
resolution of the weights in these algorithms by standard rounding methods
incurs drastic losses in performance. To reduce the resolution further, in the
extreme case even to binary weights, more advanced techniques are necessary. To
this end, we propose two methods for mapping neural network algorithms with
high resolution weights to corresponding algorithms that work with low
resolution weights and demonstrate that their performance is substantially
better than standard rounding. We further use these methods to investigate the
performance of three common neural network algorithms under fixed memory size
of the weight matrix with different weight resolutions. We show that dedicated
hardware systems, whose technology dictates very low weight resolutions (be
they electronic or biological) could in principle implement the algorithms we
study."
"Evolutionary algorithms (EAs) form a popular optimisation paradigm inspired
by natural evolution. In recent years the field of evolutionary computation has
developed a rigorous analytical theory to analyse their runtime on many
illustrative problems. Here we apply this theory to a simple model of natural
evolution. In the Strong Selection Weak Mutation (SSWM) evolutionary regime the
time between occurrence of new mutations is much longer than the time it takes
for a new beneficial mutation to take over the population. In this situation,
the population only contains copies of one genotype and evolution can be
modelled as a (1+1)-type process where the probability of accepting a new
genotype (improvements or worsenings) depends on the change in fitness.
  We present an initial runtime analysis of SSWM, quantifying its performance
for various parameters and investigating differences to the (1+1)EA. We show
that SSWM can have a moderate advantage over the (1+1)EA at crossing fitness
valleys and study an example where SSWM outperforms the (1+1)EA by taking
advantage of information on the fitness gradient."
"It has been shown in the past that a multistart hillclimbing strategy
compares favourably to a standard genetic algorithm with respect to solving
instances of the multimodal problem generator. We extend that work and verify
if the utilization of diversity preservation techniques in the genetic
algorithm changes the outcome of the comparison. We do so under two scenarios:
(1) when the goal is to find the global optimum, (2) when the goal is to find
all optima.
  A mathematical analysis is performed for the multistart hillclimbing
algorithm and a through empirical study is conducted for solving instances of
the multimodal problem generator with increasing number of optima, both with
the hillclimbing strategy as well as with genetic algorithms with niching.
Although niching improves the performance of the genetic algorithm, it is still
inferior to the multistart hillclimbing strategy on this class of problems.
  An idealized niching strategy is also presented and it is argued that its
performance should be close to a lower bound of what any evolutionary algorithm
can do on this class of problems."
"A control theoretic approach is presented in this paper for both batch and
instantaneous updates of weights in feed-forward neural networks. The popular
Hamilton-Jacobi-Bellman (HJB) equation has been used to generate an optimal
weight update law. The remarkable contribution in this paper is that closed
form solutions for both optimal cost and weight update can be achieved for any
feed-forward network using HJB equation in a simple yet elegant manner. The
proposed approach has been compared with some of the existing best performing
learning algorithms. It is found as expected that the proposed approach is
faster in convergence in terms of computational time. Some of the benchmark
test data such as 8-bit parity, breast cancer and credit approval, as well as
2D Gabor function have been used to validate our claims. The paper also
discusses issues related to global optimization. The limitations of popular
deterministic weight update laws are critiqued and the possibility of global
optimization using HJB formulation is discussed. It is hoped that the proposed
algorithm will bring in a lot of interest in researchers working in developing
fast learning algorithms and global optimization."
"In smart power grids, keeping the synchronicity of generators and the
corresponding controls is of great importance. To do so, a simple model is
employed in terms of swing equation to represent the interactions among
dynamics of generators and feedback control. In case of having a communication
network available, the control can be done based on the transmitted
measurements by the communication network. The stability of system is denoted
by the largest eigenvalue of the weighted sum of the Laplacian matrices of the
communication infrastructure and power network. In this work, we use graph
theory to model the communication network as a graph problem. Then, Ant Colony
System (ACS) is employed for optimum design of above graph for synchronization
of power grids. Performance evaluation of the proposed method for the 39-bus
New England power system versus methods such as exhaustive search and Rayleigh
quotient approximation indicates feasibility and effectiveness of our method
for even large scale smart power grids."
"In this paper a multi-parameter A*(A- star)-ants based algorithm is proposed
in order to find the best optimized multi-parameter path between two desired
points in regions. This algorithm recognizes paths, according to user desired
parameters using electronic maps. The proposed algorithm is a combination of A*
and ants algorithm in which the proposed A* algorithm is the prologue to the
suggested ant based algorithm .In fact, this A* algorithm invigorates some
paths pheromones in ants algorithm. As one of implementations of this method,
this algorithm was applied on a part of Kerman city, Iran as a multi-parameter
vehicle navigator. It finds the best optimized multi-parameter direction
between two desired junctions based on city traveler parameters. Comparison
results between the proposed method and ants algorithm demonstrates efficiency
and lower cost function results of the proposed method versus ants algorithm."
"In evolutionary optimization, it is important to understand how fast
evolutionary algorithms converge to the optimum per generation, or their
convergence rate. This paper proposes a new measure of the convergence rate,
called average convergence rate. It is a normalised geometric mean of the
reduction ratio of the fitness difference per generation. The calculation of
the average convergence rate is very simple and it is applicable for most
evolutionary algorithms on both continuous and discrete optimization. A
theoretical study of the average convergence rate is conducted for discrete
optimization. Lower bounds on the average convergence rate are derived. The
limit of the average convergence rate is analysed and then the asymptotic
average convergence rate is proposed."
"This paper presents a cooperative framework for fireworks algorithm (CoFFWA).
A detailed analysis of existing fireworks algorithm (FWA) and its recently
developed variants has revealed that (i) the selection strategy lead to the
contribution of the firework with the best fitness (core firework) for the
optimization overwhelms the contributions of the rest of fireworks (non-core
fireworks) in the explosion operator, (ii) the Gaussian mutation operator is
not as effective as it is designed to be. To overcome these limitations, the
CoFFWA is proposed, which can greatly enhance the exploitation ability of
non-core fireworks by using independent selection operator and increase the
exploration capacity by crowdness-avoiding cooperative strategy among the
fireworks. Experimental results on the CEC2013 benchmark functions suggest that
CoFFWA outperforms the state-of-the-art FWA variants, artificial bee colony,
differential evolution, the standard particle swarm optimization (SPSO) in 2007
and the most recent SPSO in 2011 in term of convergence performance."
"The problem of optimising a network of discretely firing neurons is
addressed. An objective function is introduced which measures the average
number of bits that are needed for the network to encode its state. When this
is minimised, it is shown that this leads to a number of results, such as
topographic mappings, piecewise linear dependence on the input of the
probability of a neuron firing, and factorial encoder networks."
"Generally, when genetic programming (GP) is used for function synthesis any
valuable experience gained by the system is lost from one problem to the next,
even when the problems are closely related. With the aim of developing a system
which retains beneficial experience from problem to problem, this paper
introduces the novel Node-by-Node Growth Solver (NNGS) algorithm which features
a component, called the controller, which can be adapted and improved for use
across a set of related problems. NNGS grows a single solution tree from root
to leaves. Using semantic backpropagation and acting locally on each node in
turn, the algorithm employs the controller to assign subsequent child nodes
until a fully formed solution is generated.
  The aim of this paper is to pave a path towards the use of a neural network
as the controller component and also, separately, towards the use of meta-GP as
a mechanism for improving the controller component. A proof-of-concept
controller is discussed which demonstrates the success and potential of the
NNGS algorithm. In this case, the controller constitutes a set of hand written
rules which can be used to deterministically and greedily solve standard
Boolean function synthesis benchmarks. Even before employing machine learning
to improve the controller, the algorithm vastly outperforms other well known
recent algorithms on run times, maintains comparable solution sizes, and has a
100% success rate on all Boolean function synthesis benchmarks tested so far."
"This paper introduces the NK Echo State Network. The problem of learning in
the NK Echo State Network is reduced to the problem of optimizing a special
form of a Spin Glass Problem known as an NK Landscape. No weight adjustment is
used; all learning is accomplished by spinning up (turning on) or spinning down
(turning off) neurons in order to find a combination of neurons that work
together to achieve the desired computation. For special types of NK
Landscapes, an exact global solution can be obtained in polynomial time using
dynamic programming. The NK Echo State Network is applied to a reinforcement
learning problem requiring a recurrent network: balancing two poles on a cart
given no velocity information. Empirical results shows that the NK Echo State
Network learns very rapidly and yields very good generalization."
"We propose a sign-based online learning (SOL) algorithm for a neuromorphic
hardware framework called Trainable Analogue Block (TAB). The TAB framework
utilises the principles of neural population coding, implying that it encodes
the input stimulus using a large pool of nonlinear neurons. The SOL algorithm
is a simple weight update rule that employs the sign of the hidden layer
activation and the sign of the output error, which is the difference between
the target output and the predicted output. The SOL algorithm is easily
implementable in hardware, and can be used in any artificial neural network
framework that learns weights by minimising a convex cost function. We show
that the TAB framework can be trained for various regression tasks using the
SOL algorithm."
"Kohonen's Self-Organizing Maps (SOMs) have proven to be a successful
data-reduction method to identify the intrinsic lower-dimensional sub-manifold
of a data set that is scattered in the higher-dimensional feature space.
Motivated by the possibly non-Euclidian nature of the feature space and of the
intrinsic geometry of the data set, we extend the definition of classic SOMs to
obtain the General Riemannian SOM (GRiSOM). We additionally provide an
implementation as a proof-of-concept for geometries with constant curvature. We
furthermore perform the analytic and numerical analysis of the stability limits
of certain (GRi)SOM configurations covering the different possible regular
tessellation of the map space in each geometry. A deviation between the
numerical and analytic stability limit has been observed for the square and
hexagonal Euclidean maps for very small neighbourhoods in the map space as well
as agreement in case of longer-ranged relations between the map nodes."
"An artificial neural network is presented based on the idea of connections
between units that are only active for a specific range of input values and
zero outside that range (and so are not evaluated outside the active range).
The connection function is represented by a polynomial with compact support.
The finite range of activation allows for great activation sparsity in the
network and means that theoretically you are able to add computational power to
the network without increasing the computational time required to evaluate the
network for a given input. The polynomial order ranges from first to fifth
order. Unit dropout is used for regularization and a parameter free weight
update is used. Better performance is obtained by moving from piecewise linear
connections to piecewise quadratic, even better performance can be obtained by
moving to higher order polynomials. The algorithm is tested on the MAGIC Gamma
ray data set as well as the MNIST data set."
"Neuromorphic computing is a brainlike information processing paradigm that
requires adaptive learning mechanisms. A spiking neuro-evolutionary system is
used for this purpose; plastic resistive memories are implemented as synapses
in spiking neural networks. The evolutionary design process exploits parameter
self-adaptation and allows the topology and synaptic weights to be evolved for
each network in an autonomous manner. Variable resistive memories are the focus
of this research; each synapse has its own conductance profile which modifies
the plastic behaviour of the device and may be altered during evolution. These
variable resistive networks are evaluated on a noisy robotic dynamic-reward
scenario against two static resistive memories and a system containing standard
connections only. Results indicate that the extra behavioural degrees of
freedom available to the networks incorporating variable resistive memories
enable them to outperform the comparative synapse types."
"A strict interpretation of connectionism mandates complex networks of simple
components. The question here is, is this simplicity to be interpreted in
absolute terms? I conjecture that absolute simplicity might not be an essential
attribute of connectionism, and that it may be effectively exchanged with a
requirement for relative simplicity, namely simplicity with respect to the
current organizational level. In this paper I provide some elements to the
analysis of the above question. In particular I conjecture that fractally
organized connectionist networks may provide a convenient means to achive what
Leibniz calls an ""art of complication"", namely an effective way to encapsulate
complexity and practically extend the applicability of connectionism to domains
such as sociotechnical system modeling and design. Preliminary evidence to my
claim is brought by considering the design of the software architecture
designed for the telemonitoring service of Flemish project ""Little Sister""."
"Hierarchical organization -- the recursive composition of sub-modules -- is
ubiquitous in biological networks, including neural, metabolic, ecological, and
genetic regulatory networks, and in human-made systems, such as large
organizations and the Internet. To date, most research on hierarchy in networks
has been limited to quantifying this property. However, an open, important
question in evolutionary biology is why hierarchical organization evolves in
the first place. It has recently been shown that modularity evolves because of
the presence of a cost for network connections. Here we investigate whether
such connection costs also tend to cause a hierarchical organization of such
modules. In computational simulations, we find that networks without a
connection cost do not evolve to be hierarchical, even when the task has a
hierarchical structure. However, with a connection cost, networks evolve to be
both modular and hierarchical, and these networks exhibit higher overall
performance and evolvability (i.e. faster adaptation to new environments).
Additional analyses confirm that hierarchy independently improves adaptability
after controlling for modularity. Overall, our results suggest that the same
force--the cost of connections--promotes the evolution of both hierarchy and
modularity, and that these properties are important drivers of network
performance and adaptability. In addition to shedding light on the emergence of
hierarchy across the many domains in which it appears, these findings will also
accelerate future research into evolving more complex, intelligent
computational brains in the fields of artificial intelligence and robotics."
"Nanoscale resistive memories are expected to fuel dense integration of
electronic synapses for large-scale neuromorphic system. To realize such a
brain-inspired computing chip, a compact CMOS spiking neuron that performs
in-situ learning and computing while driving a large number of resistive
synapses is desired. This work presents a novel leaky integrate-and-fire neuron
design which implements the dual-mode operation of current integration and
synaptic drive, with a single opamp and enables in-situ learning with crossbar
resistive synapses. The proposed design was implemented in a 0.18 $\mu$m CMOS
technology. Measurements show neuron's ability to drive a thousand resistive
synapses, and demonstrate an in-situ associative learning. The neuron circuit
occupies a small area of 0.01 mm$^2$ and has an energy-efficiency of 9.3
pJ$/$spike$/$synapse."
"The spike trains are the main components of the information processing in the
brain. To model spike trains several point processes have been investigated in
the literature. And more macroscopic approaches have also been studied, using
partial differential equation models. The main aim of the present article is to
build a bridge between several point processes models (Poisson, Wold, Hawkes)
that have been proved to statistically fit real spike trains data and
age-structured partial differential equations as introduced by Pakdaman,
Perthame and Salort."
"A striking difference between brain-inspired neuromorphic processors and
current von Neumann processors architectures is the way in which memory and
processing is organized. As Information and Communication Technologies continue
to address the need for increased computational power through the increase of
cores within a digital processor, neuromorphic engineers and scientists can
complement this need by building processor architectures where memory is
distributed with the processing. In this paper we present a survey of
brain-inspired processor architectures that support models of cortical networks
and deep neural networks. These architectures range from serial clocked
implementations of multi-neuron systems to massively parallel asynchronous ones
and from purely digital systems to mixed analog/digital systems which implement
more biological-like models of neurons and synapses together with a suite of
adaptation and learning mechanisms analogous to the ones found in biological
nervous systems. We describe the advantages of the different approaches being
pursued and present the challenges that need to be addressed for building
artificial neural processing systems that can display the richness of behaviors
seen in biological systems."
"Coevolution is a powerful tool in evolutionary computing that mitigates some
of its endemic problems, namely stagnation in local optima and lack of
convergence in high dimensionality problems. Since its inception in 1990, there
are multiple articles that have contributed greatly to the development and
improvement of the coevolutionary techniques. In this report we review some of
those landmark articles dwelving in the techniques they propose and how they
fit to conform robust evolutionary algorithms"
"In this paper, a neuron with nonlinear dendrites (NNLD) and binary synapses
that is able to learn temporal features of spike input patterns is considered.
Since binary synapses are considered, learning happens through formation and
elimination of connections between the inputs and the dendritic branches to
modify the structure or ""morphology"" of the NNLD. A morphological learning
algorithm inspired by the 'Tempotron', i.e., a recently proposed temporal
learning algorithm-is presented in this work. Unlike 'Tempotron', the proposed
learning rule uses a technique to automatically adapt the NNLD threshold during
training. Experimental results indicate that our NNLD with 1-bit synapses can
obtain similar accuracy as a traditional Tempotron with 4-bit synapses in
classifying single spike random latency and pair-wise synchrony patterns.
Hence, the proposed method is better suited for robust hardware implementation
in the presence of statistical variations. We also present results of applying
this rule to real life spike classification problems from the field of tactile
sensing."
"Most research in the theory of evolutionary computation assumes that the
problem at hand has a fixed problem size. This assumption does not always apply
to real-world optimization challenges, where the length of an optimal solution
may be unknown a priori.
  Following up on previous work of Cathabard, Lehre, and Yao [FOGA 2011] we
analyze variants of the (1+1) evolutionary algorithm for problems with unknown
solution length. For their setting, in which the solution length is sampled
from a geometric distribution, we provide mutation rates that yield an expected
optimization time that is of the same order as that of the (1+1) EA knowing the
solution length.
  We then show that almost the same run times can be achieved even if \emph{no}
a priori information on the solution length is available.
  Finally, we provide mutation rates suitable for settings in which neither the
solution length nor the positions of the relevant bits are known. Again we
obtain almost optimal run times for the \textsc{OneMax} and
\textsc{LeadingOnes} test functions, thus solving an open problem from
Cathabard et al."
"Understanding how crossover works is still one of the big challenges in
evolutionary computation research, and making our understanding precise and
proven by mathematical means might be an even bigger one. As one of few
examples where crossover provably is useful, the $(1+(\lambda, \lambda))$
Genetic Algorithm (GA) was proposed recently in [Doerr, Doerr, Ebel: TCS 2015].
Using the fitness level method, the expected optimization time on general
OneMax functions was analyzed and a $O(\max\{n\log(n)/\lambda, \lambda n\})$
bound was proven for any offspring population size $\lambda \in [1..n]$.
  We improve this work in several ways, leading to sharper bounds and a better
understanding of how the use of crossover speeds up the runtime in this
algorithm. We first improve the upper bound on the runtime to
$O(\max\{n\log(n)/\lambda, n\lambda \log\log(\lambda)/\log(\lambda)\})$. This
improvement is made possible from observing that in the parallel generation of
$\lambda$ offspring via crossover (but not mutation), the best of these often
is better than the expected value, and hence several fitness levels can be
gained in one iteration.
  We then present the first lower bound for this problem. It matches our upper
bound for all values of $\lambda$. This allows to determine the asymptotically
optimal value for the population size. It is $\lambda =
\Theta(\sqrt{\log(n)\log\log(n)/\log\log\log(n)})$, which gives an optimization
time of $\Theta(n \sqrt{\log(n)\log\log\log(n)/\log\log(n)})$. Hence the
improved runtime analysis gives a better runtime guarantee along with a better
suggestion for the parameter $\lambda$.
  We finally give a tail bound for the upper tail of the runtime distribution,
which shows that the actual runtime exceeds our runtime guarantee by a factor
of $(1+\delta)$ with probability $O((n/\lambda^2)^{-\delta})$ only."
"Different types of evolutionary algorithms have been developed for
constrained continuous optimization. We carry out a feature-based analysis of
evolved constrained continuous optimization instances to understand the
characteristics of constraints that make problems hard for evolutionary
algorithm. In our study, we examine how various sets of constraints can
influence the behaviour of e-Constrained Differential Evolution. Investigating
the evolved instances, we obtain knowledge of what type of constraints and
their features make a problem difficult for the examined algorithm."
"Deep learning Networks play a crucial role in the evolution of a vast number
of current machine learning models for solving a variety of real world
non-trivial tasks. Such networks use big data which is generally unlabeled
unsupervised and multi-layered requiring no form of supervision for training
and learning data and has been used to successfully build automatic supervisory
neural networks. However the question still remains how well the learned data
represents interestingness, and their effectiveness i.e. efficiency in deep
learning models or applications. If the output of a network of deep learning
models can be beamed unto a scene of observables, we could learn the
variational frequencies of these stacked networks in a parallel and
distributive way.This paper seeks to discover and represent interesting
patterns in an efficient and less complex way by incorporating the concept of
Mode synthesizers in the deep learning process models"
"Stochastic optimization is an important task in many optimization problems
where the tasks are not expressible as convex optimization problems. In the
case of non-convex optimization problems, various different stochastic
algorithms like simulated annealing, evolutionary algorithms, and tabu search
are available. Most of these algorithms require user-defined parameters
specific to the problem in order to find out the optimal solution. Moreover, in
many situations, iterative fine-tunings are required for the user-defined
parameters, and therefore these algorithms cannot adapt if the search space and
the optima changes over time. In this paper we propose an \underline{a}daptive
parameter-free \underline{s}tochastic \underline{o}ptimization technique for
\underline{c}ontinuous random variables called ASOC."
"Numerous genotypic diversity measures (GDMs) are available in the literature
to assess the convergence status of an evolutionary algorithm (EA) or describe
its search behavior. In a recent study, the authors of this paper drew
attention to the need for a GDM validation framework. In response, this study
proposes three requirements (monotonicity in individual varieties, twinning,
and monotonicity in distance) that can clearly portray any GDMs. These
diversity requirements are analysed by means of controlled population
arrangements. In this paper four GDMs are evaluated with the proposed
validation framework. The results confirm that properly evaluating population
diversity is a rather difficult task, as none of the analysed GDMs complies
with all the diversity requirements."
"This paper describes Postfix-GP system, postfix notation based Genetic
Programming (GP), for solving symbolic regression problems. It presents an
object-oriented architecture of Postfix-GP framework. It assists the user in
understanding of the implementation details of various components of
Postfix-GP. Postfix-GP provides graphical user interface which allows user to
configure the experiment, to visualize evolved solutions, to analyze GP run,
and to perform out-of-sample predictions. The use of Postfix-GP is demonstrated
by solving the benchmark symbolic regression problem. Finally, features of
Postfix-GP framework are compared with that of other GP systems."
"The merit of evolutionary algorithms (EA) to solve convex optimization
problems is widely acknowledged. In this paper, a genetic algorithm (GA)
optimization based waveform design framework is used to improve the features of
radar pulses relying on the orthogonal frequency division multiplexing (OFDM)
structure. Our optimization techniques focus on finding optimal phase code
sequences for the OFDM signal. Several optimality criteria are used since we
consider two different radar processing solutions which call either for single
or multiple-objective optimizations. When minimization of the so-called
peak-to-mean envelope power ratio (PMEPR) single-objective is tackled, we
compare our findings with existing methods and emphasize on the merit of our
approach. In the scope of the two-objective optimization, we first address
PMEPR and peak-to-sidelobe level ratio (PSLR) and show that our approach based
on the non-dominated sorting genetic algorithm-II (NSGA-II) provides design
solutions with noticeable improvements as opposed to random sets of phase
codes. We then look at another case of interest where the objective functions
are two measures of the sidelobe level, namely PSLR and the integrated-sidelobe
level ratio (ISLR) and propose to modify the NSGA-II to include a constrain on
the PMEPR instead. In the last part, we illustrate via a case study how our
encoding solution makes it possible to minimize the single objective PMEPR
while enabling a target detection enhancement strategy, when the SNR metric
would be chosen for the detection framework."
"Social Spider Algorithm (SSA) is a recently proposed general-purpose
real-parameter metaheuristic designed to solve global numerical optimization
problems. This work systematically benchmarks SSA on a suite of 11 functions
with different control parameters. We conduct parameter sensitivity analysis of
SSA using advanced non-parametric statistical tests to generate statistically
significant conclusion on the best performing parameter settings. The
conclusion can be adopted in future work to reduce the effort in parameter
tuning. In addition, we perform a success rate test to reveal the impact of the
control parameters on the convergence speed of the algorithm."
"A newly proposed chemical-reaction-inspired metaheurisic, Chemical Reaction
Optimization (CRO), has been applied to many optimization problems in both
discrete and continuous domains. To alleviate the effort in tuning parameters,
this paper reduces the number of optimization parameters in canonical CRO and
develops an adaptive scheme to evolve them. Our proposed Adaptive CRO (ACRO)
adapts better to different optimization problems. We perform simulations with
ACRO on a widely-used benchmark of continuous problems. The simulation results
show that ACRO has superior performance over canonical CRO."
"Random device mismatch that arises as a result of scaling of the CMOS
(complementary metal-oxide semi-conductor) technology into the deep submicron
regime degrades the accuracy of analogue circuits. Methods to combat this
increase the complexity of design. We have developed a novel neuromorphic
system called a Trainable Analogue Block (TAB), which exploits device mismatch
as a means for random projections of the input to a higher dimensional space.
The TAB framework is inspired by the principles of neural population coding
operating in the biological nervous system. Three neuronal layers, namely
input, hidden, and output, constitute the TAB framework, with the number of
hidden layer neurons far exceeding the input layer neurons. Here, we present
measurement results of the first prototype TAB chip built using a 65nm process
technology and show its learning capability for various regression tasks. Our
TAB chip exploits inherent randomness and variability arising due to the
fabrication process to perform various learning tasks. Additionally, we
characterise each neuron and discuss the statistical variability of its tuning
curve that arises due to random device mismatch, a desirable property for the
learning capability of the TAB. We also discuss the effect of the number of
hidden neurons and the resolution of output weights on the accuracy of the
learning capability of the TAB."
"We present a hardware architecture that uses the Neural Engineering Framework
(NEF) to implement large-scale neural networks on Field Programmable Gate
Arrays (FPGAs) for performing pattern recognition in real time. NEF is a
framework that is capable of synthesising large-scale cognitive systems from
subnetworks. We will first present the architecture of the proposed neural
network implemented using fixed-point numbers and demonstrate a routine that
computes the decoding weights by using the online pseudoinverse update method
(OPIUM) in a parallel and distributed manner. The proposed system is
efficiently implemented on a compact digital neural core. This neural core
consists of 64 neurons that are instantiated by a single physical neuron using
a time-multiplexing approach. As a proof of concept, we combined 128 identical
neural cores together to build a handwritten digit recognition system using the
MNIST database and achieved a recognition rate of 96.55%. The system is
implemented on a state-of-the-art FPGA and can process 5.12 million digits per
second. The architecture is not limited to handwriting recognition, but is
generally applicable as an extremely fast pattern recognition processor for
various kinds of patterns such as speech and images."
"There has been significant research over the past two decades in developing
new platforms for spiking neural computation. Current neural computers are
primarily developed to mimick biology. They use neural networks which can be
trained to perform specific tasks to mainly solve pattern recognition problems.
These machines can do more than simulate biology, they allow us to re-think our
current paradigm of computation. The ultimate goal is to develop brain inspired
general purpose computation architectures that can breach the current
bottleneck introduced by the Von Neumann architecture. This work proposes a new
framework for such a machine. We show that the use of neuron like units with
precise timing representation, synaptic diversity, and temporal delays allows
us to set a complete, scalable compact computation framework. The presented
framework provides both linear and non linear operations, allowing us to
represent and solve any function. We show usability in solving real use cases
from simple differential equations to sets of non-linear differential equations
leading to chaotic attractors."
"Energy disaggregation estimates appliance-by-appliance electricity
consumption from a single meter that measures the whole home's electricity
demand. Recently, deep neural networks have driven remarkable improvements in
classification performance in neighbouring machine learning fields such as
image classification and automatic speech recognition. In this paper, we adapt
three deep neural network architectures to energy disaggregation: 1) a form of
recurrent neural network called `long short-term memory' (LSTM); 2) denoising
autoencoders; and 3) a network which regresses the start time, end time and
average power demand of each appliance activation. We use seven metrics to test
the performance of these algorithms on real aggregate power data from five
appliances. Tests are performed against a house not seen during training and
against houses seen during training. We find that all three neural nets achieve
better F1 scores (averaged over all five appliances) than either combinatorial
optimisation or factorial hidden Markov models and that our neural net
algorithms generalise well to an unseen house."
"Computational models are of increasing complexity and their behavior may in
particular emerge from the interaction of different parts. Studying such models
becomes then more and more difficult and there is a need for methods and tools
supporting this process. Multi-objective evolutionary algorithms generate a set
of trade-off solutions instead of a single optimal solution. The availability
of a set of solutions that have the specificity to be optimal relative to
carefully chosen objectives allows to perform data mining in order to better
understand model features and regularities. We review the corresponding work,
propose a unifying framework, and highlight its potential use. Typical
questions that such a methodology allows to address are the following: what are
the most critical parameters of the model? What are the relations between the
parameters and the objectives? What are the typical behaviors of the model? Two
examples are provided to illustrate the capabilities of the methodology. The
features of a flapping-wing robot are thus evaluated to find out its
speed-energy relation, together with the criticality of its parameters. A
neurocomputational model of the Basal Ganglia brain nuclei is then considered
and its most salient features according to this methodology are presented and
discussed."
"A virtual chemical spectrophotometer for the simultaneous analysis of nickel
(Ni) and cobalt (Co) was developed based on an artificial neural network (ANN).
The developed ANN correlates the respective concentrations of Co and Ni given
the absorbance profile of a Co-Ni mixture based on the Beer's Law. The virtual
chemical spectrometer was trained using a 3-layer jump connection neural
network model (NNM) with 126 input nodes corresponding to the 126 absorbance
readings from 350 nm to 600 nm, 70 nodes in the hidden layer using a logistic
activation function, and 2 nodes in the output layer with a logistic function.
Test result shows that the NNM has correlation coefficients of 0.9953 and
0.9922 when predicting [Co] and [Ni], respectively. We observed, however, that
the NNM has a duality property and that there exists a real-world practical
application in solving the dual problem: Predict the Co-Ni mixture's absorbance
profile given [Co] and [Ni]. It turns out that the dual problem is much harder
to solve because the intended output has a much bigger cardinality than that of
the input. Thus, we trained the dual ANN, a 3-layer jump connection nets with 2
input nodes corresponding to [Co] and [Ni], 70-logistic-activated nodes in the
hidden layer, and 126 output nodes corresponding to the 126 absorbance readings
from 250 nm to 600 nm. Test result shows that the dual NNM has correlation
coefficients that range from 0.9050 through 0.9980 at 356 nm through 578 nm
with the maximum coefficient observed at 480 nm. This means that the dual ANN
can be used to predict the absorbance profile given the respective Co-Ni
concentrations which can be of importance in creating academic models for a
virtual chemical spectrophotometer."
"Economic Load Dispatch (ELD) is one of the essential components in power
system control and operation. Although conventional ELD formulation can be
solved using mathematical programming techniques, modern power system
introduces new models of the power units which are non-convex,
non-differentiable, and sometimes non-continuous. In order to solve such
non-convex ELD problems, in this paper we propose a new approach based on the
Social Spider Algorithm (SSA). The classical SSA is modified and enhanced to
adapt to the unique characteristics of ELD problems, e.g., valve-point effects,
multi-fuel operations, prohibited operating zones, and line losses. To
demonstrate the superiority of our proposed approach, five widely-adopted test
systems are employed and the simulation results are compared with the
state-of-the-art algorithms. In addition, the parameter sensitivity is
illustrated by a series of simulations. The simulation results show that SSA
can solve ELD problems effectively and efficiently."
"In this paper, we consider a fitness-level model of a non-elitist
mutation-only evolutionary algorithm (EA) with tournament selection. The model
provides upper and lower bounds for the expected proportion of the individuals
with fitness above given thresholds. In the case of so-called monotone
mutation, the obtained bounds imply that increasing the tournament size
improves the EA performance. As corollaries, we obtain an exponentially
vanishing tail bound for the Randomized Local Search on unimodal functions and
polynomial upper bounds on the runtime of EAs on 2-SAT problem and on a family
of Set Cover problems proposed by E. Balas."
"The complex effect of genetic algorithm's (GA) operators and parameters to
its performance has been studied extensively by researchers in the past but
none studied their interactive effects while the GA is under different problem
sizes. In this paper, We present the use of experimental model (1)~to
investigate whether the genetic operators and their parameters interact to
affect the offline performance of GA, (2)~to find what combination of genetic
operators and parameter settings will provide the optimum performance for GA,
and (3)~to investigate whether these operator-parameter combination is
dependent on the problem size. We designed a GA to optimize a family of
traveling salesman problems (TSP), with their optimal solutions known for
convenient benchmarking. Our GA was set to use different algorithms in
simulating selection ($\Omega_s$), different algorithms ($\Omega_c$) and
parameters ($p_c$) in simulating crossover, and different parameters ($p_m$) in
simulating mutation. We used several $n$-city TSPs ($n=\{5, 7, 10, 100,
1000\}$) to represent the different problem sizes (i.e., size of the resulting
search space as represented by GA schemata). Using analysis of variance of
3-factor factorial experiments, we found out that GA performance is affected by
$\Omega_s$ at small problem size (5-city TSP) where the algorithm Partially
Matched Crossover significantly outperforms Cycle Crossover at $95\%$
confidence level."
"Deep neural networks have been demonstrated impressive results in various
cognitive tasks such as object detection and image classification. In order to
execute large networks, Von Neumann computers store the large number of weight
parameters in external memories, and processing elements are timed-shared,
which leads to power-hungry I/O operations and processing bottlenecks. This
paper describes a neuromorphic computing system that is designed from the
ground up for the energy-efficient evaluation of large-scale neural networks.
The computing system consists of a non-conventional compiler, a neuromorphic
architecture, and a space-efficient microarchitecture that leverages existing
integrated circuit design methodologies. The compiler factorizes a trained,
feedforward network into a sparsely connected network, compresses the weights
linearly, and generates a time delay neural network reducing the number of
connections. The connections and units in the simplified network are mapped to
silicon synapses and neurons. We demonstrate an implementation of the
neuromorphic computing system based on a field-programmable gate array that
performs the MNIST hand-written digit classification with 97.64% accuracy."
"LSTM (Long Short-Term Memory) recurrent neural networks have been highly
successful in a number of application areas. This technical report describes
the use of the MNIST and UW3 databases for benchmarking LSTM networks and
explores the effect of different architectural and hyperparameter choices on
performance. Significant findings include: (1) LSTM performance depends
smoothly on learning rates, (2) batching and momentum has no significant effect
on performance, (3) softmax training outperforms least square training, (4)
peephole units are not useful, (5) the standard non-linearities (tanh and
sigmoid) perform best, (6) bidirectional training combined with CTC performs
better than other methods."
"In this article we provide a comprehensive review of the different
evolutionary algorithm techniques used to address multimodal optimization
problems, classifying them according to the nature of their approach. On the
one hand there are algorithms that address the issue of the early convergence
to a local optimum by differentiating the individuals of the population into
groups and limiting their interaction, hence having each group evolve with a
high degree of independence. On the other hand other approaches are based on
directly addressing the lack of genetic diversity of the population by
introducing elements into the evolutionary dynamics that promote new niches of
the genotypical space to be explored. Finally, we study multi-objective
optimization genetic algorithms, that handle the situations where multiple
criteria have to be satisfied with no penalty for any of them. Very rich
literature has arised over the years on these topics, and we aim at offering an
overview of the most important techniques of each branch of the field."
"The recent development of multi-agent simulations brings about a need for
population synthesis. It is a task of reconstructing the entire population from
a sampling survey of limited size (1% or so), supplying the initial conditions
from which simulations begin. This paper presents a new kernel density
estimator for this task. Our method is an analogue of the classical
Breiman-Meisel-Purcell estimator, but employs novel techniques that harness the
huge degree of freedom which is required to model high-dimensional nonlinearly
correlated datasets: the crossover kernel, the k-nearest neighbor restriction
of the kernel construction set and the bagging of kernels. The performance as a
statistical estimator is examined through real and synthetic datasets. We
provide an ""optimization-free"" parameter selection rule for our method, a
theory of how our method works and a computational cost analysis. To
demonstrate the usefulness as a population synthesizer, our method is applied
to a household synthesis task for an urban micro-simulator."
"Learning Classifier Systems (LCS) are population-based reinforcement learners
that were originally designed to model various cognitive phenomena. This paper
presents an explicitly cognitive LCS by using spiking neural networks as
classifiers, providing each classifier with a measure of temporal dynamism. We
employ a constructivist model of growth of both neurons and synaptic
connections, which permits a Genetic Algorithm (GA) to automatically evolve
sufficiently-complex neural structures. The spiking classifiers are coupled
with a temporally-sensitive reinforcement learning algorithm, which allows the
system to perform temporal state decomposition by appropriately rewarding
""macro-actions,"" created by chaining together multiple atomic actions. The
combination of temporal reinforcement learning and neural information
processing is shown to outperform benchmark neural classifier systems, and
successfully solve a robotic navigation task."
"Neuromorphic computing --- brainlike computing in hardware --- typically
requires myriad CMOS spiking neurons interconnected by a dense mesh of
nanoscale plastic synapses. Memristors are frequently citepd as strong synapse
candidates due to their statefulness and potential for low-power
implementations. To date, plentiful research has focused on the bipolar
memristor synapse, which is capable of incremental weight alterations and can
provide adaptive self-organisation under a Hebbian learning scheme. In this
paper we consider the Unipolar memristor synapse --- a device capable of
non-Hebbian switching between only two states (conductive and resistive)
through application of a suitable input voltage --- and discuss its suitability
for neuromorphic systems. A self-adaptive evolutionary process is used to
autonomously find highly fit network configurations. Experimentation on a two
robotics tasks shows that unipolar memristor networks evolve task-solving
controllers faster than both bipolar memristor networks and networks containing
constant nonplastic connections whilst performing at least comparably."
"This paper proposes a new algorithm based on multi-scale stochastic local
search with binary representation for training neural networks.
  In particular, we study the effects of neighborhood evaluation strategies,
the effect of the number of bits per weight and that of the maximum weight
range used for mapping binary strings to real values. Following this
preliminary investigation, we propose a telescopic multi-scale version of local
search where the number of bits is increased in an adaptive manner, leading to
a faster search and to local minima of better quality. An analysis related to
adapting the number of bits in a dynamic way is also presented. The control on
the number of bits, which happens in a natural manner in the proposed method,
is effective to increase the generalization performance. Benchmark tasks
include a highly non-linear artificial problem, a control problem requiring
either feed-forward or recurrent architectures for feedback control, and
challenging real-world tasks in different application domains.
  The results demonstrate the effectiveness of the proposed method."
"We present an analogue Very Large Scale Integration (aVLSI) implementation
that uses first-order lowpass filters to implement a conductance-based silicon
neuron for high-speed neuromorphic systems. The aVLSI neuron consists of a soma
(cell body) and a single synapse, which is capable of linearly summing both the
excitatory and inhibitory postsynaptic potentials (EPSP and IPSP) generated by
the spikes arriving from different sources. Rather than biasing the silicon
neuron with different parameters for different spiking patterns, as is
typically done, we provide digital control signals, generated by an FPGA, to
the silicon neuron to obtain different spiking behaviours. The proposed neuron
is only ~26.5 um2 in the IBM 130nm process and thus can be integrated at very
high density. Circuit simulations show that this neuron can emulate different
spiking behaviours observed in biological neurons."
"We present a neuromorphic Analogue-to-Digital Converter (ADC), which uses
integrate-and-fire (I&F) neurons as the encoders of the analogue signal, with
modulated inhibitions to decohere the neuronal spikes trains. The architecture
consists of an analogue chip and a control module. The analogue chip comprises
two scan chains and a twodimensional integrate-and-fire neuronal array.
Individual neurons are accessed via the chains one by one without any encoder
decoder or arbiter. The control module is implemented on an FPGA (Field
Programmable Gate Array), which sends scan enable signals to the scan chains
and controls the inhibition for individual neurons. Since the control module is
implemented on an FPGA, it can be easily reconfigured. Additionally, we propose
a pulse width modulation methodology for the lateral inhibition, which makes
use of different pulse widths indicating different strengths of inhibition for
each individual neuron to decohere neuronal spikes. Software simulations in
this paper tested the robustness of the proposed ADC architecture to fixed
random noise. A circuit simulation using ten neurons shows the performance and
the feasibility of the architecture."
"This paper adapts the corner classification algorithm (CC4) to train the
neural networks using spread unary inputs. This is an important problem as
spread unary appears to be at the basis of data representation in biological
learning. The modified CC4 algorithm is tested using the pattern classification
experiment and the results are found to be good. Specifically, we show that the
number of misclassified points is not particularly sensitive to the chosen
radius of generalization."
"Artificial neural networks learn how to solve new problems through a
computationally intense and time consuming process. One way to reduce the
amount of time required is to inject preexisting knowledge into the network. To
make use of past knowledge, we can take advantage of techniques that transfer
the knowledge learned from one task, and reuse it on another (sometimes
unrelated) task. In this paper we propose a novel selective breeding technique
that extends the transfer learning with behavioural genetics approach proposed
by Kohli, Magoulas and Thomas (2013), and evaluate its performance on financial
data. Numerical evidence demonstrates the credibility of the new approach. We
provide insights on the operation of transfer learning and highlight the
benefits of using behavioural principles and selective breeding when tackling a
set of diverse financial applications problems."
"This article proposes a convenient tool for decoding the output of neural
networks trained by Connectionist Temporal Classification (CTC) for handwritten
text recognition. We use regular expressions to describe the complex structures
expected in the writing. The corresponding finite automata are employed to
build a decoder. We analyze theoretically which calculations are relevant and
which can be avoided. A great speed-up results from an approximation. We
conclude that the approximation most likely fails if the regular expression
does not match the ground truth which is not harmful for many applications
since the low probability will be even underestimated. The proposed decoder is
very efficient compared to other decoding methods. The variety of applications
reaches from information retrieval to full text recognition. We refer to
applications where we integrated the proposed decoder successfully."
"In this paper we introduce a new method which employs the concept of
""Orientation Vectors"" to train a feed forward neural network and suitable for
problems where large dimensions are involved and the clusters are
characteristically sparse. The new method is not NP hard as the problem size
increases. We `derive' the method by starting from Kolmogrov's method and then
relax some of the stringent conditions. We show for most classification
problems three layers are sufficient and the network size depends on the number
of clusters. We prove as the number of clusters increase from N to N+dN the
number of processing elements in the first layer only increases by d(logN), and
are proportional to the number of classes, and the method is not NP hard.
  Many examples are solved to demonstrate that the method of Orientation
Vectors requires much less computational effort than Radial Basis Function
methods and other techniques wherein distance computations are required, in
fact the present method increases logarithmically with problem size compared to
the Radial Basis Function method and the other methods which depend on distance
computations e.g statistical methods where probabilistic distances are
calculated. A practical method of applying the concept of Occum's razor to
choose between two architectures which solve the same classification problem
has been described. The ramifications of the above findings on the field of
Deep Learning have also been briefly investigated and we have found that it
directly leads to the existence of certain types of NN architectures which can
be used as a ""mapping engine"", which has the property of ""invertibility"", thus
improving the prospect of their deployment for solving problems involving Deep
Learning and hierarchical classification. The latter possibility has a lot of
future scope in the areas of machine learning and cloud computing."
"Estimation of Distribution Algorithms (EDAs) require flexible probability
models that can be efficiently learned and sampled. Deep Boltzmann Machines
(DBMs) are generative neural networks with these desired properties. We
integrate a DBM into an EDA and evaluate the performance of this system in
solving combinatorial optimization problems with a single objective. We compare
the results to the Bayesian Optimization Algorithm. The performance of DBM-EDA
was superior to BOA for difficult additively decomposable functions, i.e.,
concatenated deceptive traps of higher order. For most other benchmark
problems, DBM-EDA cannot clearly outperform BOA, or other neural network-based
EDAs. In particular, it often yields optimal solutions for a subset of the runs
(with fewer evaluations than BOA), but is unable to provide reliable
convergence to the global optimum competitively. At the same time, the model
building process is computationally more expensive than that of other EDAs
using probabilistic models from the neural network family, such as DAE-EDA."
"In this paper, a hybrid method for solving multi-objective problem has been
provided. The proposed method is combining the {\epsilon}-Constraint and the
Cuckoo algorithm. First the multi objective problem transfers into a
single-objective problem using $\epsilon$-Constraint, then the Cuckoo
optimization algorithm will optimize the problem in each task. At last the
optimized Pareto frontier will be drawn. The advantage of this method is the
high accuracy and the dispersion of its Pareto frontier. In order to testing
the efficiency of the suggested method, a lot of test problems have been solved
using this method. Comparing the results of this method with the results of
other similar methods shows that the Cuckoo algorithm is more suitable for
solving the multi-objective problems."
"Solving constrained optimization problems by multi-objective evolutionary
algorithms has scored tremendous achievements in the last decade. Standard
multi-objective schemes usually aim at minimizing the objective function and
also the degree of constraint violation simultaneously. This paper proposes a
new multi-objective method for solving constrained optimization problems. The
new method keeps two standard objectives: the original objective function and
the sum of degrees of constraint violation. But besides them, four more
objectives are added. One is based on the feasible rule. The other three come
from the penalty functions. This paper conducts an initial experimental study
on thirteen benchmark functions. A simplified version of CMODE is applied to
solving multi-objective optimization problems. Our initial experimental results
confirm our expectation that adding more helper functions could be useful. The
performance of SMODE with more helper functions (four or six) is better than
that with only two helper functions."
"Estimation of Distribution Algorithms (EDAs) require flexible probability
models that can be efficiently learned and sampled. Generative Adversarial
Networks (GAN) are generative neural networks which can be trained to
implicitly model the probability distribution of given data, and it is possible
to sample this distribution. We integrate a GAN into an EDA and evaluate the
performance of this system when solving combinatorial optimization problems
with a single objective. We use several standard benchmark problems and compare
the results to state-of-the-art multivariate EDAs. GAN-EDA doe not yield
competitive results - the GAN lacks the ability to quickly learn a good
approximation of the probability distribution. A key reason seems to be the
large amount of noise present in the first EDA generations."
"We present our asynchronous implementation of the LM-CMA-ES algorithm, which
is a modern evolution strategy for solving complex large-scale continuous
optimization problems. Our implementation brings the best results when the
number of cores is relatively high and the computational complexity of the
fitness function is also high. The experiments with benchmark functions show
that it is able to overcome its origin on the Sphere function, reaches certain
thresholds faster on the Rosenbrock and Ellipsoid function, and surprisingly
performs much better than the original version on the Rastrigin function."
"Differential Evolution (DE) is one of the most successful and powerful
evolutionary algorithms for global optimization problem. The most important
operator in this algorithm is mutation operator which parents are selected
randomly to participate in it. Recently, numerous papers are tried to make this
operator more intelligent by selection of parents for mutation intelligently.
The intelligent selection for mutation vectors is performed by applying design
space (also known as decision space) criterion or fitness space criterion,
however, in both cases, half of valuable information of the problem space is
disregarded. In this article, a Universal Differential Evolution (UDE) is
proposed which takes advantage of both design and fitness spaces criteria for
intelligent selection of mutation vectors. The experimental analysis on UDE are
performed on CEC2005 benchmarks and the results stated that UDE significantly
improved the performance of differential evolution in comparison with other
methods that only use one criterion for intelligent selection."
"Differential Evolution (DE) proved to be one of the most successful
evolutionary algorithms for global optimization purposes in continuous
problems. The core operator in DE is mutation which can provide the algorithm
with both exploration and exploitation. In this article, a new notation for DE
is proposed which has a formula that can be utilized for generating and
extracting novel mutations and by applying this new notation, four novel
mutations are proposed. More importantly, by combining these novel trial vector
generation strategies and four other well-known ones, we proposed Generalized
Mutation Differential Evolution (GMDE) that takes advantage of two mutation
pools that have both explorative and exploitative strategies inside them.
Results and experimental analysis are performed on CEC2005 benchmarks and the
results stated that GMDE is surprisingly competitive and significantly improved
the performance of this algorithm. Finally, GMDE is also applied to parameters
optimization, modification and improvement of a feature selection method for
cancer classification purposes over gene expression microarray profiles."
"This paper addresses the reservoir design problem in the context of
delay-based reservoir computers for multidimensional input signals, parallel
architectures, and real-time multitasking. First, an approximating reservoir
model is presented in those frameworks that provides an explicit functional
link between the reservoir parameters and architecture and its performance in
the execution of a specific task. Second, the inference properties of the ridge
regression estimator in the multivariate context is used to assess the impact
of finite sample training on the decrease of the reservoir capacity. Finally,
an empirical study is conducted that shows the adequacy of the theoretical
results with the empirical performances exhibited by various reservoir
architectures in the execution of several nonlinear tasks with multidimensional
inputs. Our results confirm the robustness properties of the parallel reservoir
architecture with respect to task misspecification and parameter choice that
had already been documented in the literature."
"Adversarial examples have raised questions regarding the robustness and
security of deep neural networks. In this work we formalize the problem of
adversarial images given a pretrained classifier, showing that even in the
linear case the resulting optimization problem is nonconvex. We generate
adversarial images using shallow and deep classifiers on the MNIST and ImageNet
datasets. We probe the pixel space of adversarial images using noise of varying
intensity and distribution. We bring novel visualizations that showcase the
phenomenon and its high variability. We show that adversarial images appear in
large regions in the pixel space, but that, for the same task, a shallow
classifier seems more robust to adversarial images than a deep convolutional
network."
"The search ability of an Evolutionary Algorithm (EA) depends on the variation
among the individuals in the population [3, 4, 8]. Maintaining an optimal level
of diversity in the EA population is imperative to ensure that progress of the
EA search is unhindered by premature convergence to suboptimal solutions.
Clearer understanding of the concept of population diversity, in the context of
evolutionary search and premature convergence in particular, is the key to
designing efficient EAs. To this end, this paper first presents a brief
analysis of the EA population diversity issues. Next we present an
investigation on a counter-niching EA technique [4] that introduces and
maintains constructive diversity in the population. The proposed approach uses
informed genetic operations to reach promising, but unexplored or
under-explored areas of the search space, while discouraging premature local
convergence. Simulation runs on a suite of standard benchmark test functions
with Genetic Algorithm (GA) implementation shows promising results."
"Since their introduction in 1994 (Sims), evolved virtual creatures (EVCs)
have employed the coevolution of morphology and control to produce high-impact
work in multiple fields, including graphics, evolutionary computation,
robotics, and artificial life. However, in contrast to fixed-morphology
creatures, there has been no clear increase in the behavioral complexity of
EVCs in those two decades. This paper describes a method for moving beyond this
limit, making use of high-level human input in the form of a syllabus of
intermediate learning tasks--along with mechanisms for preservation, reuse, and
combination of previously learned tasks. This method--named ESP for its three
components: encapsulation, syllabus, and pandemonium--is presented in two
complementary versions: Fast ESP, which constrains later morphological changes
to achieve linear growth in computation time as behavioral complexity is added,
and General ESP, which allows this restriction to be removed when sufficient
computational resources are available. Experiments demonstrate that the ESP
method allows evolved virtual creatures to reach new levels of behavioral
complexity in the co-evolution of morphology and control, approximately
doubling the previous state of the art."
"Solving constraint satisfaction problems (CSPs) is a notoriously expensive
computational task. Recently, it has been proposed that efficient stochastic
solvers can be obtained through appropriately configured spiking neural
networks performing Markov Chain Monte Carlo (MCMC) sampling. The possibility
to run such models on massively parallel, low-power neuromorphic hardware holds
great promise; however, previously proposed networks are based on
probabilistically spiking neurons, and thus rely on random number generators or
external noise sources to achieve the necessary stochasticity, leading to
significant overhead in the implementation. Here we show how stochasticity can
be achieved by implementing deterministic models of integrate and fire neurons
using subthreshold analog circuits that are affected by thermal noise. We
present an efficient implementation of spike-based CSP solvers using a
reconfigurable neural network VLSI device, and the device's intrinsic noise as
a source of randomness. To illustrate the overall concept, we implement a
generic Sudoku solver based on our approach and demonstrate its operation. We
establish a link between the neuron parameters and the system dynamics,
allowing for a simple temperature control mechanism."
"We improve the results by Siegelmann & Sontag (1995) by providing a novel and
parsimonious constructive mapping between Turing Machines and Recurrent
Artificial Neural Networks, based on recent developments of Nonlinear Dynamical
Automata. The architecture of the resulting R-ANNs is simple and elegant,
stemming from its transparent relation with the underlying NDAs. These
characteristics yield promise for developments in machine learning methods and
symbolic computation with continuous time dynamical systems. A framework is
provided to directly program the R-ANNs from Turing Machine descriptions, in
absence of network training. At the same time, the network can potentially be
trained to perform algorithmic tasks, with exciting possibilities in the
integration of approaches akin to Google DeepMind's Neural Turing Machines."
"An important question in evolutionary computation is how good solutions
evolutionary algorithms can produce. This paper aims to provide an analytic
analysis of solution quality in terms of the relative approximation error,
which is defined by the error between 1 and the approximation ratio of the
solution found by an evolutionary algorithm. Since evolutionary algorithms are
iterative methods, the relative approximation error is a function of
generations. With the help of matrix analysis, it is possible to obtain an
exact expression of such a function. In this paper, an analytic expression for
calculating the relative approximation error is presented for a class of
evolutionary algorithms, that is, (1+1) strictly elitist evolution algorithms.
Furthermore, analytic expressions of the fitness value and the average
convergence rate in each generation are also derived for this class of
evolutionary algorithms. The approach is promising, and it can be extended to
non-elitist or population-based algorithms too."
"Recent studies have shown that synaptic unreliability is a robust and
sufficient mechanism for inducing the stochasticity observed in cortex. Here,
we introduce Synaptic Sampling Machines, a class of neural network models that
uses synaptic stochasticity as a means to Monte Carlo sampling and unsupervised
learning. Similar to the original formulation of Boltzmann machines, these
models can be viewed as a stochastic counterpart of Hopfield networks, but
where stochasticity is induced by a random mask over the connections. Synaptic
stochasticity plays the dual role of an efficient mechanism for sampling, and a
regularizer during learning akin to DropConnect. A local synaptic plasticity
rule implementing an event-driven form of contrastive divergence enables the
learning of generative models in an on-line fashion. Synaptic sampling machines
perform equally well using discrete-timed artificial units (as in Hopfield
networks) or continuous-timed leaky integrate & fire neurons. The learned
representations are remarkably sparse and robust to reductions in bit precision
and synapse pruning: removal of more than 75% of the weakest connections
followed by cursory re-learning causes a negligible performance loss on
benchmark classification tasks. The spiking neuron-based synaptic sampling
machines outperform existing spike-based unsupervised learners, while
potentially offering substantial advantages in terms of power and complexity,
and are thus promising models for on-line learning in brain-inspired hardware."
"Recurrent Neural Networks (RNNs) have the ability to retain memory and learn
data sequences. Due to the recurrent nature of RNNs, it is sometimes hard to
parallelize all its computations on conventional hardware. CPUs do not
currently offer large parallelism, while GPUs offer limited parallelism due to
sequential components of RNN models. In this paper we present a hardware
implementation of Long-Short Term Memory (LSTM) recurrent network on the
programmable logic Zynq 7020 FPGA from Xilinx. We implemented a RNN with $2$
layers and $128$ hidden units in hardware and it has been tested using a
character level language model. The implementation is more than $21\times$
faster than the ARM CPU embedded on the Zynq 7020 FPGA. This work can
potentially evolve to a RNN co-processor for future mobile devices."
"Evolutionary algorithms based on modeling the statistical dependencies
(interactions) between the variables have been proposed to solve a wide range
of complex problems. These algorithms learn and sample probabilistic graphical
models able to encode and exploit the regularities of the problem. This paper
investigates the effect of using probabilistic modeling techniques as a way to
enhance the behavior of MOEA/D framework. MOEA/D is a decomposition based
evolutionary algorithm that decomposes a multi-objective optimization problem
(MOP) in a number of scalar single-objective subproblems and optimizes them in
a collaborative manner. MOEA/D framework has been widely used to solve several
MOPs. The proposed algorithm, MOEA/D using probabilistic Graphical Models
(MOEA/D-GM) is able to instantiate both univariate and multi-variate
probabilistic models for each subproblem. To validate the introduced framework
algorithm, an experimental study is conducted on a multi-objective version of
the deceptive function Trap5. The results show that the variant of the
framework (MOEA/D-Tree), where tree models are learned from the matrices of the
mutual information between the variables, is able to capture the structure of
the problem. MOEA/D-Tree is able to achieve significantly better results than
both MOEA/D using genetic operators and MOEA/D using univariate probability
models, in terms of the approximation to the true Pareto front."
"Particle swarm optimisation is a metaheuristic algorithm which finds
reasonable solutions in a wide range of applied problems if suitable parameters
are used. We study the properties of the algorithm in the framework of random
dynamical systems which, due to the quasi-linear swarm dynamics, yields
analytical results for the stability properties of the particles. Such
considerations predict a relationship between the parameters of the algorithm
that marks the edge between convergent and divergent behaviours. Comparison
with simulations indicates that the algorithm performs best near this margin of
instability."
"This manuscript contains an outline of lectures course ""Evolutionary
Algorithms"" read by the author in Omsk State University n.a. F.M.Dostoevsky.
The course covers Canonic Genetic Algorithm and various other genetic
algorithms as well as evolutioanry algorithms in general. Some facts, such as
the Rotation Property of crossover, the Schemata Theorem, GA performance as a
local search and ""almost surely"" convergence of evolutionary algorithms are
given with complete proofs. The text is in Russian."
"The rnn package provides components for implementing a wide range of
Recurrent Neural Networks. It is built withing the framework of the Torch
distribution for use with the nn package. The components have evolved from 3
iterations, each adding to the flexibility and capability of the package. All
component modules inherit either the AbstractRecurrent or AbstractSequencer
classes. Strong unit testing, continued backwards compatibility and access to
supporting material are the principles followed during its development. The
package is compared against existing implementations of two published papers."
"Today artificial neural networks are applied in various fields - engineering,
data analysis, robotics. While they represent a successful tool for a variety
of relevant applications, mathematically speaking they are still far from being
conclusive. In particular, they suffer from being unable to find the best
configuration possible during the training process (local minimum problem). In
this paper, we focus on this issue and suggest a simple, but effective,
post-learning strategy to allow the search for improved set of weights at a
relatively small extra computational cost. Therefore, we introduce a novel
technique based on analogy with quantum effects occurring in nature as a way to
improve (and sometimes overcome) this problem. Several numerical experiments
are presented to validate the approach."
"This paper proposes an optimization algorithm based on how human fight and
learn from each duelist. Since this algorithm is based on population, the
proposed algorithm starts with an initial set of duelists. The duel is to
determine the winner and loser. The loser learns from the winner, while the
winner try their new skill or technique that may improve their fighting
capabilities. A few duelists with highest fighting capabilities are called as
champion. The champion train a new duelists such as their capabilities. The new
duelist will join the tournament as a representative of each champion. All
duelist are re-evaluated, and the duelists with worst fighting capabilities is
eliminated to maintain the amount of duelists. Two optimization problem is
applied for the proposed algorithm, together with genetic algorithm, particle
swarm optimization and imperialist competitive algorithm. The results show that
the proposed algorithm is able to find the better global optimum and faster
iteration."
"Oil refinery is one of industries that require huge energy consumption. The
today technology advance requires energy saving. Heat integration is a method
used to minimize the energy comsumption though the implementation of Heat
Exchanger Network (HEN). CPT is one of types of Heat Exchanger Network (HEN)
that functions to recover the heat in the flow of product or waste. HEN
comprises a number of heat exchangers (HEs) that are serially connected.
However, the presence of fouling in the heat exchanger has caused the decline
of the performance of both heat exchangers and all heat exchanger networks.
Fouling can not be avoided. However, it can be mitigated. In industry, periodic
heat exchanger cleaning is the most effective and widely used mitigation
technique. On the other side, a very frequent cleaning of heat exchanger can be
much costly in maintenance and lost of production. In this way, an accurate
optimization technique of cleaning schedule interval of heat exchanger is very
essential. Commonly, this technique involves three elements: model to simulate
the heat exchanger network, representative fouling model to describe the
fouling behavior and suitable optimization algorithm to solve the problem of
clening schedule interval for heat exchanger network. This paper describe the
optimization of interval cleaning schedule of HEN within the 44-month period
using PSO (particle swarm optimization). The number of iteration used to
achieve the convergent is 100 iterations and the fitness value in PSO
correlated with the amount of heat recovery, cleaning cost, and additional
pumping cost. The saving after the optimization of cleaning schedule of HEN in
this research achieved at $ 1.236 millions or 23% of maximum potential savings."
"Synapse plays an important role of learning in a neural network; the learning
rules which modify the synaptic strength based on the timing difference between
the pre- and post-synaptic spike occurrence is termed as Spike Time Dependent
Plasticity (STDP). The most commonly used rule posits weight change based on
time difference between one pre- and one post spike and is hence termed doublet
STDP (DSTDP). However, D-STDP could not reproduce results of many biological
experiments; a triplet STDP (T-STDP) that considers triplets of spikes as the
fundamental unit has been proposed recently to explain these observations. This
paper describes the compact implementation of a synapse using single
floating-gate (FG) transistor that can store a weight in a nonvolatile manner
and demonstrate the triplet STDP (T-STDP) learning rule by modifying drain
voltages according to triplets of spikes. We describe a mathematical procedure
to obtain control voltages for the FG device for T-STDP and also show
measurement results from a FG synapse fabricated in TSMC 0.35um CMOS process to
support the theory. Possible VLSI implementation of drain voltage waveform
generator circuits are also presented with simulation results."
"In this article, we propose a novel Winner-Take-All (WTA) architecture
employing neurons with nonlinear dendrites and an online unsupervised
structural plasticity rule for training it. Further, to aid hardware
implementations, our network employs only binary synapses. The proposed
learning rule is inspired by spike time dependent plasticity (STDP) but differs
for each dendrite based on its activation level. It trains the WTA network
through formation and elimination of connections between inputs and synapses.
To demonstrate the performance of the proposed network and learning rule, we
employ it to solve two, four and six class classification of random Poisson
spike time inputs. The results indicate that by proper tuning of the inhibitory
time constant of the WTA, a trade-off between specificity and sensitivity of
the network can be achieved. We use the inhibitory time constant to set the
number of subpatterns per pattern we want to detect. We show that while the
percentage of successful trials are 92%, 88% and 82% for two, four and six
class classification when no pattern subdivisions are made, it increases to
100% when each pattern is subdivided into 5 or 10 subpatterns. However, the
former scenario of no pattern subdivision is more jitter resilient than the
later ones."
"The paper is devoted to upper bounds on run-time of Non-Elitist Genetic
Algorithms until some target subset of solutions is visited for the first time.
In particular, we consider the sets of optimal solutions and the sets of local
optima as the target subsets. Previously known upper bounds are improved by
means of drift analysis. Finally, we propose conditions ensuring that a
Non-Elitist Genetic Algorithm efficiently finds approximate solutions with
constant approximation ratio on the class of combinatorial optimization
problems with guaranteed local optima (GLO)."
"The application of evolution in the digital realm, with the goal of creating
artificial intelligence and artificial life, has a history as long as that of
the digital computer itself. We illustrate the intertwined history of these
ideas, starting with the early theoretical work of John von Neumann and the
pioneering experimental work of Nils Aall Barricelli. We argue that
evolutionary thinking and artificial life will continue to play an integral
role in the future development of the digital world."
"NM-landscapes have been recently introduced as a class of tunable rugged
models. They are a subset of the general interaction models where all the
interactions are of order less or equal $M$. The Boltzmann distribution has
been extensively applied in single-objective evolutionary algorithms to
implement selection and study the theoretical properties of model-building
algorithms. In this paper we propose the combination of the multi-objective
NM-landscape model and the Boltzmann distribution to obtain Pareto-front
approximations. We investigate the joint effect of the parameters of the
NM-landscapes and the probabilistic factorizations in the shape of the Pareto
front approximations."
"We review several of the most widely used techniques for training recurrent
neural networks to approximate dynamical systems, then describe a novel
algorithm for this task. The algorithm is based on an earlier theoretical
result that guarantees the quality of the network approximation. We show that a
feedforward neural network can be trained on the vector field representation of
a given dynamical system using backpropagation, then recast, using matrix
manipulations, as a recurrent network that replicates the original system's
dynamics. After detailing this algorithm and its relation to earlier
approaches, we present numerical examples that demonstrate its capabilities.
One of the distinguishing features of our approach is that both the original
dynamical systems and the recurrent networks that simulate them operate in
continuous time."
"The differential evolution (DE) algorithm suffers from high computational
time due to slow nature of evaluation. In contrast, micro-DE (MDE) algorithms
employ a very small population size, which can converge faster to a reasonable
solution. However, these algorithms are vulnerable to a premature convergence
as well as to high risk of stagnation. In this paper, MDE algorithm with
vectorized random mutation factor (MDEVM) is proposed, which utilizes the small
size population benefit while empowers the exploration ability of mutation
factor through randomizing it in the decision variable level. The idea is
supported by analyzing mutation factor using Monte-Carlo based simulations. To
facilitate the usage of MDE algorithms with very-small population sizes, new
mutation schemes for population sizes less than four are also proposed.
Furthermore, comprehensive comparative simulations and analysis on performance
of the MDE algorithms over various mutation schemes, population sizes, problem
types (i.e. uni-modal, multi-modal, and composite), problem dimensionalities,
and mutation factor ranges are conducted by considering population diversity
analysis for stagnation and trapping in local optimum situations. The studies
are conducted on 28 benchmark functions provided for the IEEE CEC-2013
competition. Experimental results demonstrate high performance and convergence
speed of the proposed MDEVM algorithm."
"In this paper, we propose a novel neural network structure, namely
\emph{feedforward sequential memory networks (FSMN)}, to model long-term
dependency in time series without using recurrent feedback. The proposed FSMN
is a standard fully-connected feedforward neural network equipped with some
learnable memory blocks in its hidden layers. The memory blocks use a
tapped-delay line structure to encode the long context information into a
fixed-size representation as short-term memory mechanism. We have evaluated the
proposed FSMNs in several standard benchmark tasks, including speech
recognition and language modelling. Experimental results have shown FSMNs
significantly outperform the conventional recurrent neural networks (RNN),
including LSTMs, in modeling sequential signals like speech or language.
Moreover, FSMNs can be learned much more reliably and faster than RNNs or LSTMs
due to the inherent non-recurrent model structure."
"Cortical Learning Algorithms based on the Hierarchical Temporal Memory, HTM
have been developed by Numenta Incorporation from which variations and
modifications are currently being investigated upon. HTM offers better promises
as a future computational model of the neocortex the seat of intelligence in
the brain. Currently, intelligent agents are embedded in almost every modern
day electronic system found in homes, offices and industries worldwide. In this
paper, we present a first step in realising useful HTM like applications
specifically for mining a synthetic and real time dataset based on a novel
intelligent agent framework, and demonstrate how a modified version of this
very important computational technique will lead to improved recognition."
"In case of decision making problems, classification of pattern is a complex
and crucial task. Pattern classification using multilayer perceptron (MLP)
trained with back propagation learning becomes much complex with increase in
number of layers, number of nodes and number of epochs and ultimate increases
computational time [31]. In this paper, an attempt has been made to use fuzzy
MLP and its learning algorithm for pattern classification. The time and space
complexities of the algorithm have been analyzed. A training performance
comparison has been carried out between MLP and the proposed fuzzy-MLP model by
considering six cases. Results are noted against different learning rates
ranging from 0 to 1. A new performance evaluation factor 'convergence gain' has
been introduced. It is observed that the number of epochs drastically reduced
and performance increased compared to MLP. The average and minimum gain has
been found to be 93% and 75% respectively. The best gain is found to be 95% and
is obtained by setting the learning rate to 0.55."
"In recent years the field of neuromorphic low-power systems that consume
orders of magnitude less power gained significant momentum. However, their
wider use is still hindered by the lack of algorithms that can harness the
strengths of such architectures. While neuromorphic adaptations of
representation learning algorithms are now emerging, efficient processing of
temporal sequences or variable length-inputs remain difficult. Recurrent neural
networks (RNN) are widely used in machine learning to solve a variety of
sequence learning tasks. In this work we present a train-and-constrain
methodology that enables the mapping of machine learned (Elman) RNNs on a
substrate of spiking neurons, while being compatible with the capabilities of
current and near-future neuromorphic systems. This ""train-and-constrain"" method
consists of first training RNNs using backpropagation through time, then
discretizing the weights and finally converting them to spiking RNNs by
matching the responses of artificial neurons with those of the spiking neurons.
We demonstrate our approach by mapping a natural language processing task
(question classification), where we demonstrate the entire mapping process of
the recurrent layer of the network on IBM's Neurosynaptic System ""TrueNorth"", a
spike-based digital neuromorphic hardware architecture. TrueNorth imposes
specific constraints on connectivity, neural and synaptic parameters. To
satisfy these constraints, it was necessary to discretize the synaptic weights
and neural activities to 16 levels, and to limit fan-in to 64 inputs. We find
that short synaptic delays are sufficient to implement the dynamical (temporal)
aspect of the RNN in the question classification task. The hardware-constrained
model achieved 74% accuracy in question classification while using less than
0.025% of the cores on one TrueNorth chip, resulting in an estimated power
consumption of ~17 uW."
"We report about probabilistic likelihood estimates that are performed on time
series using an echo state network with orthogonal recurrent connectivity. The
results from tests using synthetic stochastic input time series with temporal
inference indicate that the capability of the network to infer depends on the
balance between input strength and recurrent activity. This balance has an
influence on the network with regard to the quality of inference from the short
term input history versus inference that accounts for influences that date back
a long time. Sensitivity of such networks against noise and the finite accuracy
of network states in the recurrent layer are investigated. In addition, a
measure based on mutual information between the output time series and the
reservoir is introduced. Finally, different types of recurrent connectivity are
evaluated. Orthogonal matrices show the best results of all investigated
connectivity types overall, but also in the way how the network performance
scales with the size of the recurrent layer."
"We present the soft exponential activation function for artificial neural
networks that continuously interpolates between logarithmic, linear, and
exponential functions. This activation function is simple, differentiable, and
parameterized so that it can be trained as the rest of the network is trained.
We hypothesize that soft exponential has the potential to improve neural
network learning, as it can exactly calculate many natural operations that
typical neural networks can only approximate, including addition,
multiplication, inner product, distance, polynomials, and sinusoids."
"We present a spike-based unsupervised regenerative learning scheme to train
Spiking Deep Networks (SpikeCNN) for object recognition problems using
biologically realistic leaky integrate-and-fire neurons. The training
methodology is based on the Auto-Encoder learning model wherein the
hierarchical network is trained layer wise using the encoder-decoder principle.
Regenerative learning uses spike-timing information and inherent latencies to
update the weights and learn representative levels for each convolutional layer
in an unsupervised manner. The features learnt from the final layer in the
hierarchy are then fed to an output layer. The output layer is trained with
supervision by showing a fraction of the labeled training dataset and performs
the overall classification of the input. Our proposed methodology yields
0.92%/29.84% classification error on MNIST/CIFAR10 datasets which is comparable
with state-of-the-art results. The proposed methodology also introduces
sparsity in the hierarchical feature representations on account of event-based
coding resulting in computationally efficient learning."
"While classical neural networks take a position of a leading method in the
machine learning community, spiking neuromorphic systems bring attention and
large projects in neuroscience. Spiking neural networks were shown to be able
to substitute networks of classical neurons in applied tasks. This work
explores recent hardware designs focusing on perspective applications (like
convolutional neural networks) for both neuron types from the energy efficiency
side to analyse whether there is a possibility for spiking neuromorphic
hardware to grow up for a wider use. Our comparison shows that spiking hardware
is at least on the same level of energy efficiency or even higher than
non-spiking on a level of basic operations. However, on a system level, spiking
systems are outmatched and consume much more energy due to inefficient data
representation with a long series of spikes. If spike-driven applications,
minimizing an amount of spikes, are developed, spiking neural systems may reach
the energy efficiency level of classical neural systems. However, in the near
future, both type of neuromorphic systems may benefit from emerging memory
technologies, minimizing the energy consumption of computation and memory for
both neuron types. That would make infrastructure and data transfer energy
dominant on the system level. We expect that spiking neurons have some
benefits, which would allow achieving better energy results. Still the problem
of an amount of spikes will still be the major bottleneck for spiking hardware
systems."
"Training Brain Computer Interface (BCI) systems to understand the intention
of a subject through Electroencephalogram (EEG) data currently requires
multiple training sessions with a subject in order to develop the necessary
expertise to distinguish signals for different tasks. Conventionally the task
of training the subject is done by introducing a training and calibration stage
during which some feedback is presented to the subject. This training session
can take several hours which is not appropriate for on-line EEG-based BCI
systems. An alternative approach is to use previous recording sessions of the
same person or some other subjects that performed the same tasks (subject
transfer) for training the classifiers. The main aim of this study is to
generate a methodology that allows the use of data from other subjects while
reducing the dimensions of the data. The study investigates several
possibilities for reducing the necessary training and calibration period in
subjects and the classifiers and addresses the impact of i) evolutionary
subject transfer and ii) adapting previously trained methods (retraining) using
other subjects data. Our results suggest reduction to 40% of target subject
data is sufficient for training the classifier. Our results also indicate the
superiority of the approaches that incorporated evolutionary subject transfer
and highlights the feasibility of adapting a system trained on other subjects."
"With this paper, we contribute to the growing research area of feature-based
analysis of bio-inspired computing. In this research area, problem instances
are classified according to different features of the underlying problem in
terms of their difficulty of being solved by a particular algorithm. We
investigate the impact of different sets of evolved instances for building
prediction models in the area of algorithm selection. Building on the work of
Poursoltan and Neumann [11,10], we consider how evolved instances can be used
to predict the best performing algorithm for constrained continuous
optimisation from a set of bio-inspired computing methods, namely high
performing variants of differential evolution, particle swarm optimization, and
evolution strategies. Our experimental results show that instances evolved with
a multi-objective approach in combination with random instances of the
underlying problem allow to build a model that accurately predicts the best
performing algorithm for a wide range of problem instances."
"We investigate a new method to augment recurrent neural networks with extra
memory without increasing the number of network parameters. The system has an
associative memory based on complex-valued vectors and is closely related to
Holographic Reduced Representations and Long Short-Term Memory networks.
Holographic Reduced Representations have limited capacity: as they store more
information, each retrieval becomes noisier due to interference. Our system in
contrast creates redundant copies of stored information, which enables
retrieval with reduced noise. Experiments demonstrate faster learning on
multiple memorization tasks."
"The advantage of recurrent neural networks (RNNs) in learning dependencies
between time-series data has distinguished RNNs from other deep learning
models. Recently, many advances are proposed in this emerging field. However,
there is a lack of comprehensive review on memory models in RNNs in the
literature. This paper provides a fundamental review on RNNs and long short
term memory (LSTM) model. Then, provides a surveys of recent advances in
different memory enhancements and learning techniques for capturing long term
dependencies in RNNs."
"Timetabling is a problem faced in all higher education institutions. The
International Timetabling Competition (ITC) has published a dataset that can be
used to test the quality of methods used to solve this problem. A number of
meta-heuristic approaches have obtained good results when tested on the ITC
dataset, however few have used the ant colony optimization technique,
particularly on the ITC 2007 curriculum based university course timetabling
problem. This study describes an ant system that solves the curriculum based
university course timetabling problem and the quality of the algorithm is
tested on the ITC 2007 dataset. The ant system was able to find feasible
solutions in all instances of the dataset and close to optimal solutions in
some instances. The ant system performs better than some published approaches,
however results obtained are not as good as those obtained by the best
published approaches. This study may be used as a benchmark for ant based
algorithms that solve the curriculum based university course timetabling
problem."
"Restricted Boltzmann Machines and Deep Belief Networks have been successfully
used in probabilistic generative model applications such as image occlusion
removal, pattern completion and motion synthesis. Generative inference in such
algorithms can be performed very efficiently on hardware using a Markov Chain
Monte Carlo procedure called Gibbs sampling, where stochastic samples are drawn
from noisy integrate and fire neurons implemented on neuromorphic substrates.
Currently, no satisfactory metrics exist for evaluating the generative
performance of such algorithms implemented on high-dimensional data for
neuromorphic platforms. This paper demonstrates the application of
nonparametric goodness-of-fit testing to both quantify the generative
performance as well as provide decision-directed criteria for choosing the
parameters of the neuromorphic Gibbs sampler and optimizing usage of hardware
resources used during sampling."
"Firefly algorithm is a swarm based metaheuristic algorithm inspired by the
flashing behavior of fireflies. It is an effective and an easy to implement
algorithm. It has been tested on different problems from different disciplines
and found to be effective. Even though the algorithm is proposed for
optimization problems with continuous variables, it has been modified and used
for problems with non-continuous variables, including binary and integer valued
problems. In this paper a detailed review of this modifications of firefly
algorithm for problems with non-continuous variables will be discussed. The
strength and weakness of the modifications along with possible future works
will be presented."
"We introduce an algorithm to do backpropagation on a spiking network. Our
network is ""spiking"" in the sense that our neurons accumulate their activation
into a potential over time, and only send out a signal (a ""spike"") when this
potential crosses a threshold and the neuron is reset. Neurons only update
their states when receiving signals from other neurons. Total computation of
the network thus scales with the number of spikes caused by an input rather
than network size. We show that the spiking Multi-Layer Perceptron behaves
identically, during both prediction and training, to a conventional deep
network of rectified-linear units, in the limiting case where we run the
spiking network for a long time. We apply this architecture to a conventional
classification problem (MNIST) and achieve performance very close to that of a
conventional Multi-Layer Perceptron with the same architecture. Our network is
a natural architecture for learning based on streaming event-based data, and is
a stepping stone towards using spiking neural networks to learn efficiently on
streaming data."
"Multilayered artificial neural networks (ANN) have found widespread utility
in classification and recognition applications. The scale and complexity of
such networks together with the inadequacies of general purpose computing
platforms have led to a significant interest in the development of efficient
hardware implementations. In this work, we focus on designing energy efficient
on-chip storage for the synaptic weights. In order to minimize the power
consumption of typical digital CMOS implementations of such large-scale
networks, the digital neurons could be operated reliably at scaled voltages by
reducing the clock frequency. On the contrary, the on-chip synaptic storage
designed using a conventional 6T SRAM is susceptible to bitcell failures at
reduced voltages. However, the intrinsic error resiliency of NNs to small
synaptic weight perturbations enables us to scale the operating voltage of the
6TSRAM. Our analysis on a widely used digit recognition dataset indicates that
the voltage can be scaled by 200mV from the nominal operating voltage (950mV)
for practically no loss (less than 0.5%) in accuracy (22nm predictive
technology). Scaling beyond that causes substantial performance degradation
owing to increased probability of failures in the MSBs of the synaptic weights.
We, therefore propose a significance driven hybrid 8T-6T SRAM, wherein the
sensitive MSBs are stored in 8T bitcells that are robust at scaled voltages due
to decoupled read and write paths. In an effort to further minimize the area
penalty, we present a synaptic-sensitivity driven hybrid memory architecture
consisting of multiple 8T-6T SRAM banks. Our circuit to system-level simulation
framework shows that the proposed synaptic-sensitivity driven architecture
provides a 30.91% reduction in the memory access power with a 10.41% area
overhead, for less than 1% loss in the classification accuracy."
"Large-scale artificial neural networks have shown significant promise in
addressing a wide range of classification and recognition applications.
However, their large computational requirements stretch the capabilities of
computing platforms. The fundamental components of these neural networks are
the neurons and its synapses. The core of a digital hardware neuron consists of
multiplier, accumulator and activation function. Multipliers consume most of
the processing energy in the digital neurons, and thereby in the hardware
implementations of artificial neural networks. We propose an approximate
multiplier that utilizes the notion of computation sharing and exploits error
resilience of neural network applications to achieve improved energy
consumption. We also propose Multiplier-less Artificial Neuron (MAN) for even
larger improvement in energy consumption and adapt the training process to
ensure minimal degradation in accuracy. We evaluated the proposed design on 5
recognition applications. The results show, 35% and 60% reduction in energy
consumption, for neuron sizes of 8 bits and 12 bits, respectively, with a
maximum of ~2.83% loss in network accuracy, compared to a conventional neuron
implementation. We also achieve 37% and 62% reduction in area for a neuron size
of 8 bits and 12 bits, respectively, under iso-speed conditions."
"Convolutional neural networks (CNNs) are the cutting edge model for
supervised machine learning in computer vision. In recent years CNNs have
outperformed traditional approaches in many computer vision tasks such as
object detection, image classification and face recognition. CNNs are
vulnerable to overfitting, and a lot of research focuses on finding
regularization methods to overcome it. One approach is designing task specific
models based on prior knowledge.
  Several works have shown that properties of natural images can be easily
captured using complex numbers. Motivated by these works, we present a
variation of the CNN model with complex valued input and weights. We construct
the complex model as a generalization of the real model. Lack of order over the
complex field raises several difficulties both in the definition and in the
training of the network. We address these issues and suggest possible
solutions.
  The resulting model is shown to be a restricted form of a real valued CNN
with twice the parameters. It is sensitive to phase structure, and we suggest
it serves as a regularized model for problems where such structure is
important. This suggestion is verified empirically by comparing the performance
of a complex and a real network in the problem of cell detection. The two
networks achieve comparable results, and although the complex model is hard to
train, it is significantly less vulnerable to overfitting. We also demonstrate
that the complex network detects meaningful phase structure in the data."
"Quality assurance in production line demands reliable weld joints. Human made
errors is a major cause of faulty production. Promptly Identifying errors in
the weld while welding is in progress will decrease the post inspection cost
spent on the welding process. Electrical parameters generated during welding,
could able to characterize the process efficiently. Parameter values are
collected using high speed data acquisition system. Time series analysis tasks
such as filtering, pattern recognition etc. are performed over the collected
data. Filtering removes the unwanted noisy signal components and pattern
recognition task segregate error patterns in the time series based upon
similarity, which is performed by Self Organized mapping clustering algorithm.
Welder quality is thus compared by detecting and counting number of error
patterns appeared in his parametric time series. Moreover, Self Organized
mapping algorithm provides the database in which patterns are segregated into
two classes either desirable or undesirable. Database thus generated is used to
train the classification algorithms, and thereby automating the real time error
detection task. Multi Layer Perceptron and Radial basis function are the two
classification algorithms used, and their performance has been compared based
on metrics such as specificity, sensitivity, accuracy and time required in
training."
"We present a digital implementation of the Spike Timing Dependent Plasticity
(STDP) learning rule. The proposed digital implementation consists of an
exponential decay generator array and a STDP adaptor array. On the arrival of a
pre- and post-synaptic spike, the STDP adaptor will send a digital spike to the
decay generator. The decay generator will then generate an exponential decay,
which will be used by the STDP adaptor to perform the weight adaption. The
exponential decay, which is computational expensive, is efficiently implemented
by using a novel stochastic approach, which we analyse and characterise here.
We use a time multiplexing approach to achieve 8192 (8k) virtual STDP adaptors
and decay generators with only one physical implementation of each. We have
validated our stochastic STDP approach with measurement results of a balanced
excitation/inhibition experiment. Our stochastic approach is ideal for
implementing the STDP learning rule in large-scale spiking neural networks
running in real time."
"Online parameter controllers for evolutionary algorithms adjust values of
parameters during the run of an evolutionary algorithm. Recently a new
efficient parameter controller based on reinforcement learning was proposed by
Karafotias et al. In this method ranges of parameters are discretized into
several intervals before the run. However, performing adaptive discretization
during the run may increase efficiency of an evolutionary algorithm. Aleti et
al. proposed another efficient controller with adaptive discretization.
  In the present paper we propose a parameter controller based on reinforcement
learning with adaptive discretization. The proposed controller is compared with
the existing parameter adjusting methods on several test problems using
different configurations of an evolutionary algorithm. For the test problems,
we consider four continuous functions, namely the sphere function, the
Rosenbrock function, the Levi function and the Rastrigin function. Results show
that the new controller outperforms the other controllers on most of the
considered test problems."
"A draft memory model (DM) for neural networks with spike propagation delay
(SNNwD) is described. Novelty in this approach are that the DM learns
immediately, with stimuli presented once, without synaptic weight changes, and
without external learning algorithm. Basal on this model is to trap spikes
within neural loops. In order to construct the DM we developed two functional
blocks, also described herein. The decoder block receives input from a single
spikes source and connect it to one among many outputs. The selector block
operates in the opposite direction, receiving many spikes sources and
connecting one of them to a single output. We realized conceptual proofs by
testing the DM in the prime numbers classifying task. This activation-based
memory can be used as immediate and short-term memory."
"Deep networks are now able to achieve human-level performance on a broad
spectrum of recognition tasks. Independently, neuromorphic computing has now
demonstrated unprecedented energy-efficiency through a new chip architecture
based on spiking neurons, low precision synapses, and a scalable communication
network. Here, we demonstrate that neuromorphic computing, despite its novel
architectural primitives, can implement deep convolution networks that i)
approach state-of-the-art classification accuracy across 8 standard datasets,
encompassing vision and speech, ii) perform inference while preserving the
hardware's underlying energy-efficiency and high throughput, running on the
aforementioned datasets at between 1200 and 2600 frames per second and using
between 25 and 275 mW (effectively > 6000 frames / sec / W) and iii) can be
specified and trained using backpropagation with the same ease-of-use as
contemporary deep learning. For the first time, the algorithmic power of deep
learning can be merged with the efficiency of neuromorphic processors, bringing
the promise of embedded, intelligent, brain-inspired computing one step closer."
"In this paper Hybrid Ant Colony Optimization (HAntCO) approach in solving
Multi--Skill Resource Constrained Project Scheduling Problem (MS--RCPSP) has
been presented. We have proposed hybrid approach that links classical heuristic
priority rules for project scheduling with Ant Colony Optimization (ACO).
Furthermore, a novel approach for updating pheromone value has been proposed,
based on both the best and worst solutions stored by ants. The objective of
this paper is to research the usability and robustness of ACO and its hybrids
with priority rules in solving MS--RCPSP. Experiments have been performed using
artificially created dataset instances, based on real--world ones. We published
those instances that can be used as a benchmark. Presented results show that
ACO--based hybrid method is an efficient approach. More directed search process
by hybrids makes this approach more stable and provides mostly better results
than classical ACO."
"This paper introduces Adaptive Computation Time (ACT), an algorithm that
allows recurrent neural networks to learn how many computational steps to take
between receiving an input and emitting an output. ACT requires minimal changes
to the network architecture, is deterministic and differentiable, and does not
add any noise to the parameter gradients. Experimental results are provided for
four synthetic problems: determining the parity of binary vectors, applying
binary logic operations, adding integers, and sorting real numbers. Overall,
performance is dramatically improved by the use of ACT, which successfully
adapts the number of computational steps to the requirements of the problem. We
also present character-level language modelling results on the Hutter prize
Wikipedia dataset. In this case ACT does not yield large gains in performance;
however it does provide intriguing insight into the structure of the data, with
more computation allocated to harder-to-predict transitions, such as spaces
between words and ends of sentences. This suggests that ACT or other adaptive
computation methods could provide a generic method for inferring segment
boundaries in sequence data."
"Dataflow matrix machines are a powerful generalization of recurrent neural
networks. They work with multiple types of arbitrary linear streams, multiple
types of powerful neurons, and allow to incorporate higher-order constructions.
We expect them to be useful in machine learning and probabilistic programming,
and in the synthesis of dynamic systems and of deterministic and probabilistic
programs."
"In this research, feedforward ANN (Artificial Neural Network) model is
developed and validated for predicting the pH at 10 different locations of the
distribution system of drinking water of Hyderabad city. The developed model is
MLP (Multilayer Perceptron) with back propagation algorithm.The data for the
training and testing of the model are collected through an experimental
analysis on weekly basis in a routine examination for maintaining the quality
of drinking water in the city. 17 parameters are taken into consideration
including pH. These all parameters are taken as input variables for the model
and then pH is predicted for 03 phases;raw water of river Indus,treated water
in the treatment plants and then treated water in the distribution system of
drinking water. The training and testing results of this model reveal that MLP
neural networks are exceedingly extrapolative for predicting the pH of river
water, untreated and treated water at all locations of the distribution system
of drinking water of Hyderabad city. The optimum input and output weights are
generated with minimum MSE (Mean Square Error) < 5%.Experimental, predicted and
tested values of pH are plotted and the effectiveness of the model is
determined by calculating the coefficient of correlation (R2=0.999) of trained
and tested results."
"In most digital communication systems, bandwidth limited channel along with
multipath propagation causes ISI (Inter Symbol Interference) to occur. This
phenomenon causes distortion of the given transmitted symbol due to other
transmitted symbols. With the help of equalization ISI can be reduced. This
paper presents a solution to the ISI problem by performing blind equalization
using ANN (Artificial Neural Networks). The simulated network is a multilayer
feedforward Perceptron ANN, which has been trained by utilizing the error
back-propagation algorithm. The weights of the network are updated in
accordance with training of the network. This paper presents a very effective
method for blind channel equalization, being more efficient than the
pre-existing algorithms. The obtained results show a visible reduction in the
noise content."
"Heuristic algorithms are able to optimize objective functions efficiently
because they use intelligently the information about the objective functions.
Thus, information utilization is critical to the performance of heuristics.
However, the concept of information utilization has remained vague and abstract
because there is no reliable metric to reflect the extent to which the
information about the objective function is utilized by heuristic algorithms.
In this paper, the metric of information utilization ratio (IUR) is defined,
which is the ratio of the utilized information quantity over the acquired
information quantity in the search process. The IUR proves to be well-defined.
Several examples of typical heuristic algorithms are given to demonstrate the
procedure of calculating the IUR. Empirical evidences on the correlation
between the IUR and the performance of a heuristic are also provided. The IUR
can be an index of how finely an algorithm is designed and guide the invention
of new heuristics and the improvement of existing ones."
"We propose a novel activation function that implements piece-wise orthogonal
non-linear mappings based on permutations. It is straightforward to implement,
and very computationally efficient, also it has little memory requirements. We
tested it on two toy problems for feedforward and recurrent networks, it shows
similar performance to tanh and ReLU. OPLU activation function ensures norm
preservance of the backpropagated gradients, therefore it is potentially good
for the training of deep, extra deep, and recurrent neural networks."
"This paper introduces two recurrent neural network structures called Simple
Gated Unit (SGU) and Deep Simple Gated Unit (DSGU), which are general
structures for learning long term dependencies. Compared to traditional Long
Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), both structures
require fewer parameters and less computation time in sequence classification
tasks. Unlike GRU and LSTM, which require more than one gates to control
information flow in the network, SGU and DSGU only use one multiplicative gate
to control the flow of information. We show that this difference can accelerate
the learning speed in tasks that require long dependency information. We also
show that DSGU is more numerically stable than SGU. In addition, we also
propose a standard way of representing inner structure of RNN called RNN
Conventional Graph (RCG), which helps analyzing the relationship between input
units and hidden units of RNN."
"Neural networks and evolutionary computation have a rich intertwined history.
They most commonly appear together when an evolutionary algorithm optimises the
parameters and topology of a neural network for reinforcement learning
problems, or when a neural network is applied as a surrogate fitness function
to aid the evolutionary optimisation of expensive fitness functions. In this
paper we take a different approach, asking the question of whether a neural
network can be used to provide a mutation distribution for an evolutionary
algorithm, and what advantages this approach may offer? Two modern neural
network models are investigated, a Denoising Autoencoder modified to produce
stochastic outputs and the Neural Autoregressive Distribution Estimator.
Results show that the neural network approach to learning genotypes is able to
solve many difficult discrete problems, such as MaxSat and HIFF, and regularly
outperforms other evolutionary techniques."
"Volatility in stock markets has been extensively studied in the applied
finance literature. In this paper, Artificial Neural Network models based on
various back propagation algorithms have been constructed to predict volatility
in the Indian stock market through volatility of NIFTY returns and volatility
of gold returns. This model considers India VIX, CBOE VIX, volatility of crude
oil returns (CRUDESDR), volatility of DJIA returns (DJIASDR), volatility of DAX
returns (DAXSDR), volatility of Hang Seng returns (HANGSDR) and volatility of
Nikkei returns (NIKKEISDR) as predictor variables. Three sets of experiments
have been performed over three time periods to judge the effectiveness of the
approach."
"In this article, a novel neuro-inspired low-resolution online unsupervised
learning rule is proposed to train the reservoir or liquid of Liquid State
Machine. The liquid is a sparsely interconnected huge recurrent network of
spiking neurons. The proposed learning rule is inspired from structural
plasticity and trains the liquid through formation and elimination of synaptic
connections. Hence, the learning involves rewiring of the reservoir connections
similar to structural plasticity observed in biological neural networks. The
network connections can be stored as a connection matrix and updated in memory
by using Address Event Representation (AER) protocols which are generally
employed in neuromorphic systems. On investigating the 'pairwise separation
property' we find that trained liquids provide 1.36 $\pm$ 0.18 times more
inter-class separation while retaining similar intra-class separation as
compared to random liquids. Moreover, analysis of the 'linear separation
property' reveals that trained liquids are 2.05 $\pm$ 0.27 times better than
random liquids. Furthermore, we show that our liquids are able to retain the
'generalization' ability and 'generality' of random liquids. A memory analysis
shows that trained liquids have 83.67 $\pm$ 5.79 ms longer fading memory than
random liquids which have shown 92.8 $\pm$ 5.03 ms fading memory for a
particular type of spike train inputs. We also throw some light on the dynamics
of the evolution of recurrent connections within the liquid. Moreover, compared
to 'Separation Driven Synaptic Modification' - a recently proposed algorithm
for iteratively refining reservoirs, our learning rule provides 9.30%, 15.21%
and 12.52% more liquid separations and 2.8%, 9.1% and 7.9% better
classification accuracies for four, eight and twelve class pattern recognition
tasks respectively."
"Modern films, games and virtual reality applications are dependent on
convincing computer graphics. Highly complex models are a requirement for the
successful delivery of many scenes and environments. While workflows such as
rendering, compositing and animation have been streamlined to accommodate
increasing demands, modelling complex models is still a laborious task. This
paper introduces the computational benefits of an Interactive Genetic Algorithm
(IGA) to computer graphics modelling while compensating the effects of user
fatigue, a common issue with Interactive Evolutionary Computation. An
intelligent agent is used in conjunction with an IGA that offers the potential
to reduce the effects of user fatigue by learning from the choices made by the
human designer and directing the search accordingly. This workflow accelerates
the layout and distribution of basic elements to form complex models. It
captures the designer's intent through interaction, and encourages playful
discovery."
"Evolutionary algorithms have been widely studied from a theoretical
perspective. In particular, the area of runtime analysis has contributed
significantly to a theoretical understanding and provided insights into the
working behaviour of these algorithms. We study how these insights into
evolutionary processes can be used for evolutionary art. We introduce the
notion of evolutionary image transition which transfers a given starting image
into a target image through an evolutionary process. Combining standard
mutation effects known from the optimization of the classical benchmark
function OneMax and different variants of random walks, we present ways of
performing evolutionary image transition with different artistic effects."
"There has been a variety of crossover operators proposed for Real-Coded
Genetic Algorithms (RCGAs), which recombine values from the same location in
pairs of strings. In this article we present a recombination operator for RC-
GAs that selects the locations randomly in both parents, and compare it to
mainstream crossover operators in a set of experiments on a range of standard
multidimensional optimization problems and a clustering problem. We present two
variants of the operator, either selecting both bits uniformly at random in the
strings, or sampling the second bit from a normal distribution centered at the
selected location in the first string. While the operator is biased towards
exploitation of fitness space, the random selection of the second bit for swap-
ping makes it slightly less exploitation-biased. Extensive statistical analysis
using a non-parametric test shows the advantage of the new recombination
operators on a range of test functions."
"Benchmarks and datasets have important role in evaluation of machine learning
algorithms and neural network implementations. Traditional dataset for images
such as MNIST is applied to evaluate efficiency of different training
algorithms in neural networks. This demand is different in Spiking Neural
Networks (SNN) as they require spiking inputs. It is widely believed, in the
biological cortex the timing of spikes is irregular. Poisson distributions
provide adequate descriptions of the irregularity in generating appropriate
spikes. Here, we introduce a spike-based version of MNSIT (handwritten digits
dataset),using Poisson distribution and show the Poissonian property of the
generated streams. We introduce a new version of evt_MNIST which can be used
for neural network evaluation."
"Recurrent neural networks (RNN) are simple dynamical systems whose
computational power has been attributed to their short-term memory. Short-term
memory of RNNs has been previously studied analytically only for the case of
orthogonal networks, and only under annealed approximation, and uncorrelated
input. Here for the first time, we present an exact solution to the memory
capacity and the task-solving performance as a function of the structure of a
given network instance, enabling direct determination of the
function--structure relation in RNNs. We calculate the memory capacity for
arbitrary networks with exponentially correlated input and further related it
to the performance of the system on signal processing tasks in a supervised
learning setup. We compute the expected error and the worst-case error bound as
a function of the spectra of the network and the correlation structure of its
inputs and outputs. Our results give an explanation for learning and
generalization of task solving using short-term memory, which is crucial for
building alternative computer architectures using physical phenomena based on
the short-term memory principle."
"This work presents the application of the artificial neural networks, trained
and structurally optimized by genetic algorithms, for modeling of crude
distillation process at PKN ORLEN S.A. refinery. Models for the main
fractionator distillation column products were developed using historical data.
Quality of the fractions were predicted based on several chosen process
variables. The performance of the model was validated using test data. Neural
networks used in companion with genetic algorithms proved that they can
accurately predict fractions quality shifts, reproducing the results of the
standard laboratory analysis. Simple knowledge extraction method from neural
network model built was also performed. Genetic algorithms can be successfully
utilized in efficient training of large neural networks and finding their
optimal structures."
"This paper concerns applications of genetic algorithms and genetic
programming to tasks for which it is difficult to find a representation that
does not map to a highly complex and discontinuous fitness landscape. In such
cases the standard algorithm is prone to getting trapped in local extremes. The
paper proposes several adaptive mechanisms that are useful in preventing the
search from getting trapped."
"This document details the rationales behind assessing the performance of
numerical black-box optimizers on multi-objective problems within the COCO
platform and in particular on the biobjective test suite bbob-biobj. The
evaluation is based on a hypervolume of all non-dominated solutions in the
archive of candidate solutions and measures the runtime until the hypervolume
value succeeds prescribed target values."
"This research is focused on solving problems in the area of software project
management using metaheuristic search algorithms and as such is research in the
field of search based software engineering. The main aim of this research is to
evaluate the performance of different metaheuristic search techniques in
resource allocation and scheduling problems that would be typical of software
development projects. This paper reports a set of experiments which evaluate
the performance of three algorithms, namely simulated annealing, tabu search
and genetic algorithms. The experimental results indicate that all of the
metaheuristics search techniques can be used to solve problems in resource
allocation and scheduling within a software project. Finally, a comparative
analysis suggests that overall the genetic algorithm had performed better than
simulated annealing and tabu search."
"Previous RNN architectures have largely been superseded by LSTM, or ""Long
Short-Term Memory"". Since its introduction, there have been many variations on
this simple design. However, it is still widely used and we are not aware of a
gated-RNN architecture that outperforms LSTM in a broad sense while still being
as simple and efficient. In this paper we propose a modified LSTM-like
architecture. Our architecture is still simple and achieves better performance
on the tasks that we tested on. We also introduce a new RNN performance
benchmark that uses the handwritten digits and stresses several important
network capabilities."
"We present four training and prediction schedules from the same
character-level recurrent neural network. The efficiency of these schedules is
tested in terms of model effectiveness as a function of training time and
amount of training data seen. We show that the choice of training and
prediction schedule potentially has a considerable impact on the prediction
effectiveness for a given training budget."
"We propose a multi-objective optimization algorithm aimed at achieving good
anytime performance over a wide range of problems. Performance is assessed in
terms of the hypervolume metric. The algorithm called HMO-CMA-ES represents a
hybrid of several old and new variants of CMA-ES, complemented by BOBYQA as a
warm start. We benchmark HMO-CMA-ES on the recently introduced bi-objective
problem suite of the COCO framework (COmparing Continuous Optimizers),
consisting of 55 scalable continuous optimization problems, which is used by
the Black-Box Optimization Benchmarking (BBOB) Workshop 2016."
"We present an any-time performance assessment for benchmarking numerical
optimization algorithms in a black-box scenario, applied within the COCO
benchmarking platform. The performance assessment is based on runtimes measured
in number of objective function evaluations to reach one or several quality
indicator target values. We argue that runtime is the only available measure
with a generic, meaningful, and quantitative interpretation. We discuss the
choice of the target values, runlength-based targets, and the aggregation of
results by using simulated restarts, averages, and empirical distribution
functions."
"This paper is dedicated to the long-term, or multi-step-ahead, time series
prediction problem. We propose a novel method for training feed-forward neural
networks, such as multilayer perceptrons, with tapped delay lines. Special
batch calculation of derivatives called Forecasted Propagation Through Time and
batch modification of the Extended Kalman Filter are introduced. Experiments
were carried out on well-known time series benchmarks, the Mackey-Glass chaotic
process and the Santa Fe Laser Data Series. Recurrent and feed-forward neural
networks were evaluated."
"In the neuroevolution literature, research has primarily focused on evolving
the number of nodes, connections, and weights in artificial neural networks.
Few attempts have been made to evolve activation functions. Research in
evolving activation functions has mainly focused on evolving function
parameters, and developing heterogeneous networks by selecting from a fixed
pool of activation functions. This paper introduces a novel technique for
evolving heterogeneous artificial neural networks through combinatorially
generating piecewise activation functions to enhance expressive power. I
demonstrate this technique on NeuroEvolution of Augmenting Topologies using
ArcTan and Sigmoid, and show that it outperforms the original algorithm on
non-Markovian double pole balancing. This technique expands the landscape of
unconventional activation functions by demonstrating that they are competitive
with canonical choices, and introduces a purview for further exploration of
automatic model selection for artificial neural networks."
"A co-evolutionary algorithm (CA) based chess player is presented.
Implementation details of the algorithms, namely coding, population, variation
operators are described. The alpha-beta or mini-max like behaviour of the
player is achieved through two competitive or cooperative populations. Special
attention is given to the fitness function evaluation (the heart of the
solution). Test results on algorithms vs. algorithms or human player is
provided."
"Most of the problems in genetic algorithms are very complex and demand a
large amount of resources that current technology can not offer. Our purpose
was to develop a Java-JINI distributed library that implements Genetic
Algorithms with sub-populations (coarse grain) and a graphical interface in
order to configure and follow the evolution of the search. The sub-populations
are simulated/evaluated in personal computers connected trough a network,
keeping in mind different models of sub-populations, migration policies and
network topologies. We show that this model delays the convergence of the
population keeping a higher level of genetic diversity and allows a much
greater number of evaluations since they are distributed among several
computers compared with the traditional Genetic Algorithms."
"Deep Neural Networks (DNN) have achieved human level performance in many
image analytics tasks but DNNs are mostly deployed to GPU platforms that
consume a considerable amount of power. Brain-inspired spiking neuromorphic
chips consume low power and can be highly parallelized. However, for deploying
DNNs to energy efficient neuromorphic chips the incompatibility between
continuous neurons and synaptic weights of traditional DNNs, discrete spiking
neurons and synapses of neuromorphic chips has to be overcome. Previous work
has achieved this by training a network to learn continuous probabilities and
deployment to a neuromorphic architecture by random sampling these
probabilities. An ensemble of sampled networks is needed to approximate the
performance of the trained network.
  In the work presented in this paper, we have extended previous research by
directly learning binary synaptic crossbars. Results on MNIST show that better
performance can be achieved with a small network in one time step (92.7%
maximum observed accuracy vs 95.98% accuracy in our work). Top results on a
larger network are similar to previously published results (99.42% maximum
observed accuracy vs 99.45% accuracy in our work). More importantly, in our
work a smaller ensemble is needed to achieve similar or better accuracy than
previous work, which translates into significantly decreased energy consumption
for both networks. Results of our work are stable since they do not require
random sampling."
"The performance of different mutation operators is usually evaluated in
conjunc-tion with specific parameter settings of genetic algorithms and target
problems. Most studies focus on the classical genetic algorithm with different
parameters or on solving unconstrained combinatorial optimization problems such
as the traveling salesman problems. In this paper, a subpopulation-based
genetic al-gorithm that uses only mutation and selection is developed to solve
multi-robot task allocation problems. The target problems are constrained
combinatorial optimization problems, and are more complex if cooperative tasks
are involved as these introduce additional spatial and temporal constraints.
The proposed genetic algorithm can obtain better solutions than classical
genetic algorithms with tournament selection and partially mapped crossover.
The performance of different mutation operators in solving problems
without/with cooperative tasks is evaluated. The results imply that inversion
mutation performs better than others when solving problems without cooperative
tasks, and the swap-inversion combination performs better than others when
solving problems with cooperative tasks."
"Spiking neural networks (SNNs) with adaptive synapses reflect core properties
of biological neural networks. Speech recognition, as an application involving
audio coding and dynamic learning, provides a good test problem to study SNN
functionality. We present a simple, novel, and efficient nonrecurrent SNN that
learns to convert a speech signal into a spike train signature. The signature
is distinguishable from signatures for other speech signals representing
different words, thereby enabling digit recognition and discrimination in
devices that use only spiking neurons. The method uses a small, nonrecurrent
SNN consisting of Izhikevich neurons equipped with spike timing dependent
plasticity (STDP) and biologically realistic synapses. This approach introduces
an efficient and fast network without error-feedback training, although it does
require supervised training. The new simulation results produce discriminative
spike train patterns for spoken digits in which highly correlated spike trains
belong to the same category and low correlated patterns belong to different
categories. The proposed SNN is evaluated using a spoken digit recognition task
where a subset of the Aurora speech dataset is used. The experimental results
show that the network performs well in terms of accuracy rate and complexity."
"It is of some interest to understand how statistically based mechanisms for
signal processing might be integrated with biologically motivated mechanisms
such as neural networks. This paper explores a novel hybrid approach for
classifying segments of sequential data, such as individual spoken works. The
approach combines a hidden Markov model (HMM) with a spiking neural network
(SNN). The HMM, consisting of states and transitions, forms a fixed backbone
with nonadaptive transition probabilities. The SNN, however, implements a
biologically based Bayesian computation that derives from the spike
timing-dependent plasticity (STDP) learning rule. The emission (observation)
probabilities of the HMM are represented in the SNN and trained with the STDP
rule. A separate SNN, each with the same architecture, is associated with each
of the states of the HMM. Because of the STDP training, each SNN implements an
expectation maximization algorithm to learn the emission probabilities for one
HMM state. The model was studied on synthesized spike-train data and also on
spoken word data. Preliminary results suggest its performance compares
favorably with other biologically motivated approaches. Because of the model's
uniqueness and initial promise, it warrants further study. It provides some new
ideas on how the brain might implement the equivalent of an HMM in a neural
circuit."
"The final version of this paper has been published in IEEEXplore available at
http://ieeexplore.ieee.org/document/7727213. Please cite this paper as:
Amirhossein Tavanaei, Timothee Masquelier, and Anthony Maida, Acquisition of
visual features through probabilistic spike-timing-dependent plasticity. IEEE
International Joint Conference on Neural Networks. pp. 307-314, IJCNN 2016.
  This paper explores modifications to a feedforward five-layer spiking
convolutional network (SCN) of the ventral visual stream [Masquelier, T.,
Thorpe, S., Unsupervised learning of visual features through spike timing
dependent plasticity. PLoS Computational Biology, 3(2), 247-257]. The original
model showed that a spike-timing-dependent plasticity (STDP) learning algorithm
embedded in an appropriately selected SCN could perform unsupervised feature
discovery. The discovered features where interpretable and could effectively be
used to perform rapid binary decisions in a classifier. In order to study the
robustness of the previous results, the present research examines the effects
of modifying some of the components of the original model. For improved
biological realism, we replace the original non-leaky integrate-and-fire
neurons with Izhikevich-like neurons. We also replace the original STDP rule
with a novel rule that has a probabilistic interpretation. The probabilistic
STDP slightly but significantly improves the performance for both types of
model neurons. Use of the Izhikevich-like neuron was not found to improve
performance although performance was still comparable to the IF neuron. This
shows that the model is robust enough to handle more biologically realistic
neurons. We also conclude that the underlying reasons for stable performance in
the model are preserved despite the overt changes to the explicit components of
the model."
"The paper investigates a new type of truly critical echo state networks where
individual transfer functions for every neuron can be modified to anticipate
the expected next input. Deviations from expected input are only forgotten
slowly in power law fashion. The paper outlines the theory, numerically
analyzes a one neuron model network and finally discusses technical and also
biological implications of this type of approach."
"The article, after a brief introduction on genetic algorithms and their
functioning, presents a kind of genetic algorithm called Viral Search. We
present the key concepts, we formally derive the algorithm and we perform
numerical tests designed to illustrate the potential and limits."
"The fields of neural computation and artificial neural networks have
developed much in the last decades. Most of the works in these fields focus on
implementing and/or learning discrete functions or behavior. However,
technical, physical, and also cognitive processes evolve continuously in time.
This cannot be described directly with standard architectures of artificial
neural networks such as multi-layer feed-forward perceptrons. Therefore, in
this paper, we will argue that neural networks modeling continuous time are
needed explicitly for this purpose, because with them the synthesis and
analysis of continuous and possibly periodic processes in time are possible
(e.g. for robot behavior) besides computing discrete classification functions
(e.g. for logical reasoning). We will relate possible neural network
architectures with (hybrid) automata models that allow to express continuous
processes."
"Evolutionary algorithms (EAs) have been well acknowledged as a promising
paradigm for solving optimisation problems with multiple conflicting objectives
in the sense that they are able to locate a set of diverse approximations of
Pareto optimal solutions in a single run. EAs drive the search for approximated
solutions through maintaining a diverse population of solutions and by
recombining promising solutions selected from the population. Combining machine
learning techniques has shown great potentials since the intrinsic structure of
the Pareto optimal solutions of an multiobjective optimisation problem can be
learned and used to guide for effective recombination. However, existing
multiobjective EAs (MOEAs) based on structure learning spend too much
computational resources on learning. To address this problem, we propose to use
an online learning scheme. Based on the fact that offsprings along evolution
are streamy, dependent and non-stationary (which implies that the intrinsic
structure, if any, is temporal and scale-variant), an online agglomerative
clustering algorithm is applied to adaptively discover the intrinsic structure
of the Pareto optimal solution set; and to guide effective offspring
recombination. Experimental results have shown significant improvement over
five state-of-the-art MOEAs on a set of well-known benchmark problems with
complicated Pareto sets and complex Pareto fronts."
"The paper is devoted to upper bounds on the expected first hitting times of
the sets of local or global optima for non-elitist genetic algorithms with very
high selection pressure. The results of this paper extend the range of
situations where the upper bounds on the expected runtime are known for genetic
algorithms and apply, in particular, to the Canonical Genetic Algorithm. The
obtained bounds do not require the probability of fitness-decreasing mutation
to be bounded by a constant less than one."
"While gradient descent has proven highly successful in learning connection
weights for neural networks, the actual structure of these networks is usually
determined by hand, or by other optimization algorithms. Here we describe a
simple method to make network structure differentiable, and therefore
accessible to gradient descent. We test this method on recurrent neural
networks applied to simple sequence prediction problems. Starting with initial
networks containing only one node, the method automatically builds networks
that successfully solve the tasks. The number of nodes in the final network
correlates with task difficulty. The method can dynamically increase network
size in response to an abrupt complexification in the task; however, reduction
in network size in response to task simplification is not evident for
reasonable meta-parameters. The method does not penalize network performance
for these test tasks: variable-size networks actually reach better performance
than fixed-size networks of higher, lower or identical size. We conclude by
discussing how this method could be applied to more complex networks, such as
feedforward layered networks, or multiple-area networks of arbitrary shape."
"Over the past 30 years many researchers in the field of evolutionary
computation have put a lot of effort to introduce various approaches for
solving hard problems. Most of these problems have been inspired by major
industries so that solving them, by providing either optimal or near optimal
solution, was of major significance. Indeed, this was a very promising
trajectory as advances in these problem-solving approaches could result in
adding values to major industries. In this paper we revisit this trajectory to
find out whether the attempts that started three decades ago are still aligned
with the same goal, as complexities of real-world problems increased
significantly. We present some examples of modern real-world problems, discuss
why they might be difficult to solve, and whether there is any mismatch between
these examples and the problems that are investigated in the evolutionary
computation area."
"In this paper we present a modified version of the Hyperbolic Tangent
Activation Function as a learning unit generator for neural networks. The
function uses an integer calibration constant as an approximation to the Euler
number, e, based on a quadratic Real Number Formula (RNF) algorithm and an
adaptive normalization constraint on the input activations to avoid the
vanishing gradient. We demonstrate the effectiveness of the proposed
modification using a hypothetical and real world dataset and show that lower
run-times can be achieved by learning algorithms using this function leading to
improved speed-ups and learning accuracies during training."
"Computer simulations have become a very powerful tool for scientific
research. Given the vast complexity that comes with many open scientific
questions, a purely analytical or experimental approach is often not viable.
For example, biological systems (such as the human brain) comprise an extremely
complex organization and heterogeneous interactions across different spatial
and temporal scales. In order to facilitate research on such problems, the
BioDynaMo project (\url{https://biodynamo.web.cern.ch/}) aims at a general
platform for computer simulations for biological research. Since the scientific
investigations require extensive computer resources, this platform should be
executable on hybrid cloud computing systems, allowing for the efficient use of
state-of-the-art computing technology. This paper describes challenges during
the early stages of the software development process. In particular, we
describe issues regarding the implementation and the highly interdisciplinary
as well as international nature of the collaboration. Moreover, we explain the
methodologies, the approach, and the lessons learnt by the team during these
first stages."
"Spike-timing-dependent plasticity (STDP) incurs both causal and acausal
synaptic weight updates, for negative and positive time differences between
pre-synaptic and post-synaptic spike events. For realizing such updates in
neuromorphic hardware, current implementations either require forward and
reverse lookup access to the synaptic connectivity table, or rely on
memory-intensive architectures such as crossbar arrays. We present a novel
method for realizing both causal and acausal weight updates using only forward
lookup access of the synaptic connectivity table, permitting memory-efficient
implementation. A simplified implementation in FPGA, using a single timer
variable for each neuron, closely approximates exact STDP cumulative weight
updates for neuron refractory periods greater than 10 ms, and reduces to exact
STDP for refractory periods greater than the STDP time window. Compared to
conventional crossbar implementation, the forward table-based implementation
leads to substantial memory savings for sparsely connected networks supporting
scalable neuromorphic systems with fully reconfigurable synaptic connectivity
and plasticity."
"We provide a rigorous runtime analysis concerning the update strength, a
vital parameter in probabilistic model-building GAs such as the step size $1/K$
in the compact Genetic Algorithm (cGA) and the evaporation factor $\rho$ in
ACO. While a large update strength is desirable for exploitation, there is a
general trade-off: too strong updates can lead to genetic drift and poor
performance. We demonstrate this trade-off for the cGA and a simple MMAS ACO
algorithm on the OneMax function. More precisely, we obtain lower bounds on the
expected runtime of $\Omega(K\sqrt{n} + n \log n)$ and $\Omega(\sqrt{n}/\rho +
n \log n)$, respectively, showing that the update strength should be limited to
$1/K, \rho = O(1/(\sqrt{n} \log n))$. In fact, choosing $1/K, \rho \sim
1/(\sqrt{n}\log n)$ both algorithms efficiently optimize OneMax in expected
time $O(n \log n)$. Our analyses provide new insights into the stochastic
behavior of probabilistic model-building GAs and propose new guidelines for
setting the update strength in global optimization."
"This paper introduces an Enhanced Boolean version of the Correlation Matrix
Memory (CMM), which is useful to work with binary memories. A novel Boolean
Orthonormalization Process (BOP) is presented to convert a non-orthonormal
Boolean basis, i.e., a set of non-orthonormal binary vectors (in a Boolean
sense) to an orthonormal Boolean basis, i.e., a set of orthonormal binary
vectors (in a Boolean sense). This work shows that it is possible to improve
the performance of Boolean CMM thanks BOP algorithm. Besides, the BOP algorithm
has a lot of additional fields of applications, e.g.: Steganography, Hopfield
Networks, Bi-level image processing, etc. Finally, it is important to mention
that the BOP is an extremely stable and fast algorithm."
"Multi-population evolutionary algorithms are, by nature, highly complex and
difficult to describe. Even two populations working in concert (or opposition)
present a myriad of potential configurations that are often difficult to relate
using text alone. Little effort has been made, however, to depict these kinds
of systems, relying solely on the simple structural connections (related using
ad hoc diagrams) between populations and often leaving out crucial details. In
this paper, we propose a notation and accompanying formalism for consistently
and powerfully depicting these structures and the relationships within them in
an intuitive and consistent way. Using our notation, we examine simple
co-evolutionary systems and discover new configurations by the simple process
of ""drawing on a whiteboard"". Finally, we demonstrate that even complex,
highly-interconnected systems with large numbers of populations can be
understood with ease using the advanced features of our formalism"
"Evolutionary multitasking has recently emerged as a novel paradigm that
enables the similarities and/or latent complementarities (if present) between
distinct optimization tasks to be exploited in an autonomous manner simply by
solving them together with a unified solution representation scheme. An
important matter underpinning future algorithmic advancements is to develop a
better understanding of the driving force behind successful multitask
problem-solving. In this regard, two (seemingly disparate) ideas have been put
forward, namely, (a) implicit genetic transfer as the key ingredient
facilitating the exchange of high-quality genetic material across tasks, and
(b) population diversification resulting in effective global search of the
unified search space encompassing all tasks. In this paper, we present some
empirical results that provide a clearer picture of the relationship between
the two aforementioned propositions. For the numerical experiments we make use
of Sudoku puzzles as case studies, mainly because of their feature that
outwardly unlike puzzle statements can often have nearly identical final
solutions. The experiments reveal that while on many occasions genetic transfer
and population diversity may be viewed as two sides of the same coin, the wider
implication of genetic transfer, as shall be shown herein, captures the true
essence of evolutionary multitasking to the fullest."
"The ability to backpropagate stochastic gradients through continuous latent
distributions has been crucial to the emergence of variational autoencoders and
stochastic gradient variational Bayes. The key ingredient is an unbiased and
low-variance way of estimating gradients with respect to distribution
parameters from gradients evaluated at distribution samples. The
""reparameterization trick"" provides a class of transforms yielding such
estimators for many continuous distributions, including the Gaussian and other
members of the location-scale family. However the trick does not readily extend
to mixture density models, due to the difficulty of reparameterizing the
discrete distribution over mixture weights. This report describes an
alternative transform, applicable to any continuous multivariate distribution
with a differentiable density function from which samples can be drawn, and
uses it to derive an unbiased estimator for mixture density weight derivatives.
Combined with the reparameterization trick applied to the individual mixture
components, this estimator makes it straightforward to train variational
autoencoders with mixture-distributed latent variables, or to perform
stochastic variational inference with a mixture density variational posterior."
"We propose a geometric model-free causality measurebased on multivariate
delay embedding that can efficiently detect linear and nonlinear causal
interactions between time series with no prior information. We then exploit the
proposed causal interaction measure in real MEG data analysis. The results are
used to construct effective connectivity maps of brain activity to decode
different categories of visual stimuli. Moreover, we discovered that the
MEG-based effective connectivity maps as a response to structured images
exhibit more geometric patterns, as disclosed by analyzing the evolution of
toplogical structures of the underlying networks using persistent homology.
Extensive simulation and experimental result have been carried out to
substantiate the capabilities of the proposed approach."
"PSO is a widely recognized optimization algorithm inspired by social swarm.
In this brief we present a heterogeneous strategy particle swarm optimization
(HSPSO), in which a proportion of particles adopt a fully informed strategy to
enhance the converging speed while the rest are singly informed to maintain the
diversity. Our extensive numerical experiments show that HSPSO algorithm is
able to obtain satisfactory solutions, outperforming both PSO and the fully
informed PSO. The evolution process is examined from both structural and
microscopic points of view. We find that the cooperation between two types of
particles can facilitate a good balance between exploration and exploitation,
yielding better performance. We demonstrate the applicability of HSPSO on the
filter design problem."
"Evolutionary algorithms have been used in many ways to generate digital art.
We study how evolutionary processes are used for evolutionary art and present a
new approach to the transition of images. Our main idea is to define
evolutionary processes for digital image transition, combining different
variants of mutation and evolutionary mechanisms. We introduce box and strip
mutation operators which are specifically designed for image transition. Our
experimental results show that the process of an evolutionary algorithm in
combination with these mutation operators can be used as a valuable way to
produce unique generative art."
"This paper is a brief update on developments in the BioDynaMo project, a new
platform for computer simulations for biological research. We will discuss the
new capabilities of the simulator, important new concepts simulation
methodology as well as its numerous applications to the computational biology
and nanoscience communities."
"In this paper, we are concerned with a branch of evolutionary algorithms
termed estimation of distribution (EDA), which has been successfully used to
tackle derivative-free global optimization problems. For existent EDA
algorithms, it is a common practice to use a Gaussian distribution or a mixture
of Gaussian components to represent the statistical property of available
promising solutions found so far. Observing that the Student's t distribution
has heavier and longer tails than the Gaussian, which may be beneficial for
exploring the solution space, we propose a novel EDA algorithm termed ESTDA, in
which the Student's t distribution, rather than Gaussian, is employed. To
address hard multimodal and deceptive problems, we extend ESTDA further by
substituting a single Student's t distribution with a mixture of Student's t
distributions. The resulting algorithm is named as estimation of mixture of
Student's t distribution algorithm (EMSTDA). Both ESTDA and EMSTDA are
evaluated through extensive and in-depth numerical experiments using over a
dozen of benchmark objective functions. Empirical results demonstrate that the
proposed algorithms provide remarkably better performance than their Gaussian
counterparts."
"Existing studies on dynamic multi-objective optimization focus on problems
with time-dependent objective functions, while the ones with a changing number
of objectives have rarely been considered in the literature. Instead of
changing the shape or position of the Pareto-optimal front/set when having
time-dependent objective functions, increasing or decreasing the number of
objectives usually leads to the expansion or contraction of the dimension of
the Pareto-optimal front/set manifold. Unfortunately, most existing dynamic
handling techniques can hardly be adapted to this type of dynamics. In this
paper, we report our attempt toward tackling the dynamic multi-objective
optimization problems with a changing number of objectives. We implement a new
two-archive evolutionary algorithm which maintains two co-evolving populations
simultaneously. In particular, these two populations are complementary to each
other: one concerns more about the convergence while the other concerns more
about the diversity. The compositions of these two populations are adaptively
reconstructed once the environment changes. In addition, these two populations
interact with each other via a mating selection mechanism. Comprehensive
experiments are conducted on various benchmark problems with a time-dependent
number of objectives. Empirical results fully demonstrate the effectiveness of
our proposed algorithm."
"Recurrent Neural Networks (RNNs) produce state-of-art performance on many
machine learning tasks but their demand on resources in terms of memory and
computational power are often high. Therefore, there is a great interest in
optimizing the computations performed with these models especially when
considering development of specialized low-power hardware for deep networks.
One way of reducing the computational needs is to limit the numerical precision
of the network weights and biases. This has led to different proposed rounding
methods which have been applied so far to only Convolutional Neural Networks
and Fully-Connected Networks. This paper addresses the question of how to best
reduce weight precision during training in the case of RNNs. We present results
from the use of different stochastic and deterministic reduced precision
training methods applied to three major RNN types which are then tested on
several datasets. The results show that the weight binarization methods do not
work with the RNNs. However, the stochastic and deterministic ternarization,
and pow2-ternarization methods gave rise to low-precision RNNs that produce
similar and even higher accuracy on certain datasets therefore providing a path
towards training more efficient implementations of RNNs in specialized
hardware."
"Recurrent Bistable Gradient Networks are attractor based neural networks
characterized by bistable dynamics of each single neuron. Coupled together
using linear interaction determined by the interconnection weights, these
networks do not suffer from spurious states or very limited capacity anymore.
Vladimir Chinarov and Michael Menzinger, who invented these networks, trained
them using Hebb's learning rule. We show, that this way of computing the
weights leads to unwanted behaviour and limitations of the networks
capabilities. Furthermore we evince, that using the first order of Hintons
Contrastive Divergence algorithm leads to a quite promising recurrent neural
network. These findings are tested by learning images of the MNIST database for
handwritten numbers."
"The balance between convergence and diversity is a key issue of evolutionary
multi-objective optimization. The recently proposed stable matching-based
selection provides a new perspective to handle this balance under the framework
of decomposition multi-objective optimization. In particular, the stable
matching between subproblems and solutions, which achieves an equilibrium
between their mutual preferences, implicitly strikes a balance between the
convergence and diversity. Nevertheless, the original stable matching model has
a high risk of matching a solution with a unfavorable subproblem which finally
leads to an imbalanced selection result. In this paper, we propose an adaptive
two-level stable matching-based selection for decomposition multi-objective
optimization. Specifically, borrowing the idea of stable matching with
incomplete lists, we match each solution with one of its favorite subproblems
by restricting the length of its preference list during the first-level stable
matching. During the second-level stable matching, the remaining subproblems
are thereafter matched with their favorite solutions according to the classic
stable matching model. In particular, we develop an adaptive mechanism to
automatically set the length of preference list for each solution according to
its local competitiveness. The performance of our proposed method is validated
and compared with several state-of-the-art evolutionary multi-objective
optimization algorithms on 62 benchmark problem instances. Empirical results
fully demonstrate the competitive performance of our proposed method on
problems with complicated Pareto sets and those with more than three
objectives."
"Deep spiking neural networks (SNNs) hold great potential for improving the
latency and energy efficiency of deep neural networks through event-based
computation. However, training such networks is difficult due to the
non-differentiable nature of asynchronous spike events. In this paper, we
introduce a novel technique, which treats the membrane potentials of spiking
neurons as differentiable signals, where discontinuities at spike times are
only considered as noise. This enables an error backpropagation mechanism for
deep SNNs, which works directly on spike signals and membrane potentials. Thus,
compared with previous methods relying on indirect training and conversion, our
technique has the potential to capture the statics of spikes more precisely.
Our novel framework outperforms all previously reported results for SNNs on the
permutation invariant MNIST benchmark, as well as the N-MNIST benchmark
recorded with event-based vision sensors."
"A theoretical framework is developed to describe the transformation that
distributes probability density functions uniformly over space. In one
dimension, the cumulative distribution can be used, but does not generalize to
higher dimensions, or non-separable distributions. A potential function is
shown to link probability density functions to their transformation, and to
generalize the cumulative. A numerical method is developed to compute the
potential, and examples are shown in two dimensions."
"Biological neurons communicate with a sparing exchange of pulses - spikes. It
is an open question how real spiking neurons produce the kind of powerful
neural computation that is possible with deep artificial neural networks, using
only so very few spikes to communicate. Building on recent insights in
neuroscience, we present an Adapting Spiking Neural Network (ASNN) based on
adaptive spiking neurons. These spiking neurons efficiently encode information
in spike-trains using a form of Asynchronous Pulsed Sigma-Delta coding while
homeostatically optimizing their firing rate. In the proposed paradigm of
spiking neuron computation, neural adaptation is tightly coupled to synaptic
plasticity, to ensure that downstream neurons can correctly decode upstream
spiking neurons. We show that this type of network is inherently able to carry
out asynchronous and event-driven neural computation, while performing
identical to corresponding artificial neural networks (ANNs). In particular, we
show that these adaptive spiking neurons can be drop in replacements for ReLU
neurons in standard feedforward ANNs comprised of such units. We demonstrate
that this can also be successfully applied to a ReLU based deep convolutional
neural network for classifying the MNIST dataset. The ASNN thus outperforms
current Spiking Neural Networks (SNNs) implementations, while responding (up
to) an order of magnitude faster and using an order of magnitude fewer spikes.
Additionally, in a streaming setting where frames are continuously classified,
we show that the ASNN requires substantially fewer network updates as compared
to the corresponding ANN."
"Random Neural Networks (RNNs) are a class of Neural Networks (NNs) that can
also be seen as a specific type of queuing network. They have been successfully
used in several domains during the last 25 years, as queuing networks to
analyze the performance of resource sharing in many engineering areas, as
learning tools and in combinatorial optimization, where they are seen as neural
systems, and also as models of neurological aspects of living beings. In this
article we focus on their learning capabilities, and more specifically, we
present a practical guide for using the RNN to solve supervised learning
problems. We give a general description of these models using almost
indistinctly the terminology of Queuing Theory and the neural one. We present
the standard learning procedures used by RNNs, adapted from similar
well-established improvements in the standard NN field. We describe in
particular a set of learning algorithms covering techniques based on the use of
first order and, then, of second order derivatives. We also discuss some issues
related to these objects and present new perspectives about their use in
supervised learning problems. The tutorial describes their most relevant
applications, and also provides a large bibliography."
"Convolutional Neural Networks (CNNs) are one of the most successful deep
machine learning technologies for processing image, voice and video data. CNNs
require large amounts of processing capacity and memory, which can exceed the
resources of low power mobile and embedded systems. Several designs for
hardware accelerators have been proposed for CNNs which typically contain large
numbers of Multiply Accumulate (MAC) units. One approach to reducing data sizes
and memory traffic in CNN accelerators is ""weight sharing"", where the full
range of values in a trained CNN are put in bins and the bin index is stored
instead of the original weight value. In this paper we propose a novel MAC
circuit that exploits binning in weight-sharing CNNs. Rather than computing the
MAC directly we instead count the frequency of each weight and place it in a
bin. We then compute the accumulated value in a subsequent multiply phase. This
allows hardware multipliers in the MAC circuit to be replaced with adders and
selection logic. Experiments show that for the same clock speed our approach
results in fewer gates, smaller logic, and reduced power."
"In evolutionary robotics an encoding of the control software, which maps
sensor data (input) to motor control values (output), is shaped by stochastic
optimization methods to complete a predefined task. This approach is assumed to
be beneficial compared to standard methods of controller design in those cases
where no a-priori model is available that could help to optimize performance.
Also for robots that have to operate in unpredictable environments, an
evolutionary robotics approach is favorable. We demonstrate here that such a
model-free approach is not a free lunch, as already simple tasks can represent
unsolvable barriers for fully open-ended uninformed evolutionary computation
techniques. We propose here the 'Wankelmut' task as an objective for an
evolutionary approach that starts from scratch without pre-shaped controller
software or any other informed approach that would force the behavior to be
evolved in a desired way. Our focal claim is that 'Wankelmut' represents the
simplest set of problems that makes plain-vanilla evolutionary computation
fail. We demonstrate this by a series of simple standard evolutionary
approaches using different fitness functions and standard artificial neural
networks as well as continuous-time recurrent neural networks. All our tested
approaches failed. We claim that any other evolutionary approach will also fail
that does per-se not favor or enforce modularity and does not freeze or protect
already evolved functionalities. Thus we propose a hard-to-pass benchmark and
make a strong statement for self-complexifying and generative approaches in
evolutionary computation. We anticipate that defining such a 'simplest task to
fail' is a valuable benchmark for promoting future development in the field of
artificial intelligence, evolutionary robotics and artificial life."
"Collision avoidance systems can play a vital role in reducing the number of
accidents and saving human lives. In this paper, we introduce and validate a
novel method for vehicles reactive collision avoidance using evolutionary
neural networks (ENN). A single front-facing rangefinder sensor is the only
input required by our method. The training process and the proposed method
analysis and validation are carried out using simulation. Extensive experiments
are conducted to analyse the proposed method and evaluate its performance.
Firstly, we experiment the ability to learn collision avoidance in a static
free track. Secondly, we analyse the effect of the rangefinder sensor
resolution on the learning process. Thirdly, we experiment the ability of a
vehicle to individually and simultaneously learn collision avoidance. Finally,
we test the generality of the proposed method. We used a more realistic and
powerful simulation environment (CarMaker), a camera as an alternative input
sensor, and lane keeping as an extra feature to learn. The results are
encouraging; the proposed method successfully allows vehicles to learn
collision avoidance in different scenarios that are unseen during training. It
also generalizes well if any of the input sensor, the simulator, or the task to
be learned is changed."
"The efficiency of the human brain in performing classification tasks has
attracted considerable research interest in brain-inspired neuromorphic
computing. Hardware implementations of a neuromorphic system aims to mimic the
computations in the brain through interconnection of neurons and synaptic
weights. A leaky-integrate-fire (LIF) spiking model is widely used to emulate
the dynamics of neuronal action potentials. In this work, we propose a spin
based LIF spiking neuron using the magneto-electric (ME) switching of
ferro-magnets. The voltage across the ME oxide exhibits a typical
leaky-integrate behavior, which in turn switches an underlying ferro-magnet.
Due to the effect of thermal noise, the ferro-magnet exhibits probabilistic
switching dynamics, which is reminiscent of the stochasticity exhibited by
biological neurons. The energy-efficiency of the ME switching mechanism coupled
with the intrinsic non-volatility of ferro-magnets result in lower energy
consumption, when compared to a CMOS LIF neuron. A device to system-level
simulation framework has been developed to investigate the feasibility of the
proposed LIF neuron for a hand-written digit recognition problem"
"We present novel techniques to accelerate the convergence of Deep Learning
algorithms by conducting low overhead removal of redundant neurons -- apoptosis
of neurons -- which do not contribute to model learning, during the training
phase itself. We provide in-depth theoretical underpinnings of our heuristics
(bounding accuracy loss and handling apoptosis of several neuron types), and
present the methods to conduct adaptive neuron apoptosis. Specifically, we are
able to improve the training time for several datasets by 2-3x, while reducing
the number of parameters by up to 30x (4-5x on average) on datasets such as
ImageNet classification. For the Higgs Boson dataset, our implementation
improves the accuracy (measured by Area Under Curve (AUC)) for classification
from 0.88/1 to 0.94/1, while reducing the number of parameters by 3x in
comparison to existing literature. The proposed methods achieve a 2.44x speedup
in comparison to the default (no apoptosis) algorithm."
"Layers is an open source neural network toolkit aim at providing an easy way
to implement modern neural networks. The main user target are students and to
this end layers provides an easy scriptting language that can be early adopted.
The user has to focus only on design details as network totpology and parameter
tunning."
"Neural networks are known to be effective function approximators. Recently,
deep neural networks have proven to be very effective in pattern recognition,
classification tasks and human-level control to model highly nonlinear
realworld systems. This paper investigates the effectiveness of deep neural
networks in the modeling of dynamical systems with complex behavior. Three deep
neural network structures are trained on sequential data, and we investigate
the effectiveness of these networks in modeling associated characteristics of
the underlying dynamical systems. We carry out similar evaluations on select
publicly available system identification datasets. We demonstrate that deep
neural networks are effective model estimators from input-output data"
"Evolutionary algorithms have been successfully applied to a variety of
optimisation problems in stationary environments. However, many real world
optimisation problems are set in dynamic environments where the success
criteria shifts regularly. Population diversity affects algorithmic
performance, particularly on multiobjective and dynamic problems. Diversity
mechanisms are methods of altering evolutionary algorithms in a way that
promotes the maintenance of population diversity. This project intends to
measure and compare the performance effect a variety of diversity mechanisms
have on an evolutionary algorithm when facing an assortment of dynamic
problems."
"Various variants of the well known Covariance Matrix Adaptation Evolution
Strategy (CMA-ES) have been proposed recently, which improve the empirical
performance of the original algorithm by structural modifications. However, in
practice it is often unclear which variation is best suited to the specific
optimization problem at hand. As one approach to tackle this issue, algorithmic
mechanisms attached to CMA-ES variants are considered and extracted as
functional \emph{modules}, allowing for combinations of them. This leads to a
configuration space over ES structures, which enables the exploration of
algorithm structures and paves the way toward novel algorithm generation.
Specifically, eleven modules are incorporated in this framework with two or
three alternative configurations for each module, resulting in $4\,608$
algorithms. A self-adaptive Genetic Algorithm (GA) is used to efficiently
evolve effective ES-structures for given classes of optimization problems,
outperforming any classical CMA-ES variants from literature. The proposed
approach is evaluated on noiseless functions from BBOB suite. Furthermore, such
an observation is again confirmed on different function groups and
dimensionality, indicating the feasibility of ES configuration on real-world
problem classes."
"The recently introduced Multi-dimensional Archive of Phenotypic Elites
(MAP-Elites) is an evolutionary algorithm capable of producing a large archive
of diverse, high-performing solutions in a single run. It works by discretizing
a continuous feature space into unique regions according to the desired
discretization per dimension. While simple, this algorithm has a main drawback:
it cannot scale to high-dimensional feature spaces since the number of regions
increase exponentially with the number of dimensions. In this paper, we address
this limitation by introducing a simple extension of MAP-Elites that has a
constant, pre-defined number of regions irrespective of the dimensionality of
the feature space. Our main insight is that methods from computational geometry
could partition a high-dimensional space into well-spread geometric regions. In
particular, our algorithm uses a centroidal Voronoi tessellation (CVT) to
divide the feature space into a desired number of regions; it then places every
generated individual in its closest region, replacing a less fit one if the
region is already occupied. We demonstrate the effectiveness of the new
""CVT-MAP-Elites"" algorithm in high-dimensional feature spaces through
comparisons against MAP-Elites in maze navigation and hexapod locomotion tasks."
"Ensuring sustainability demands more efficient energy management with
minimized energy wastage. Therefore, the power grid of the future should
provide an unprecedented level of flexibility in energy management. To that
end, intelligent decision making requires accurate predictions of future energy
demand/load, both at aggregate and individual site level. Thus, energy load
forecasting have received increased attention in the recent past, however has
proven to be a difficult problem. This paper presents a novel energy load
forecasting methodology based on Deep Neural Networks, specifically Long Short
Term Memory (LSTM) algorithms. The presented work investigates two variants of
the LSTM: 1) standard LSTM and 2) LSTM-based Sequence to Sequence (S2S)
architecture. Both methods were implemented on a benchmark data set of
electricity consumption data from one residential customer. Both architectures
where trained and tested on one hour and one-minute time-step resolution
datasets. Experimental results showed that the standard LSTM failed at
one-minute resolution data while performing well in one-hour resolution data.
It was shown that S2S architecture performed well on both datasets. Further, it
was shown that the presented methods produced comparable results with the other
deep learning methods for energy forecasting in literature."
"In this paper, we propose a novel approach (SAPEO) to support the survival
selection process in multi-objective evolutionary algorithms with surrogate
models - it dynamically chooses individuals to evaluate exactly based on the
model uncertainty and the distinctness of the population. We introduce variants
that differ in terms of the risk they allow when doing survival selection.
Here, the anytime performance of different SAPEO variants is evaluated in
conjunction with an SMS-EMOA using the BBOB bi-objective benchmark. We compare
the obtained results with the performance of the regular SMS-EMOA, as well as
another surrogate-assisted approach. The results open up general questions
about the applicability and required conditions for surrogate-assisted
multi-objective evolutionary algorithms to be tackled in the future."
"The storage and computation requirements of Convolutional Neural Networks
(CNNs) can be prohibitive for exploiting these models over low-power or
embedded devices. This paper reduces the computational complexity of the CNNs
by minimizing an objective function, including the recognition loss that is
augmented with a sparsity-promoting penalty term. The sparsity structure of the
network is identified using the Alternating Direction Method of Multipliers
(ADMM), which is widely used in large optimization problems. This method
alternates between promoting the sparsity of the network and optimizing the
recognition performance, which allows us to exploit the two-part structure of
the corresponding objective functions. In particular, we take advantage of the
separability of the sparsity-inducing penalty functions to decompose the
minimization problem into sub-problems that can be solved sequentially.
Applying our method to a variety of state-of-the-art CNN models, our proposed
method is able to simplify the original model, generating models with less
computation and fewer parameters, while maintaining and often improving
generalization performance. Accomplishments on a variety of models strongly
verify that our proposed ADMM-based method can be a very useful tool for
simplifying and improving deep CNNs."
"Deep neural networks can be obscenely wasteful. When processing video, a
convolutional network expends a fixed amount of computation for each frame with
no regard to the similarity between neighbouring frames. As a result, it ends
up repeatedly doing very similar computations. To put an end to such waste, we
introduce Sigma-Delta networks. With each new input, each layer in this network
sends a discretized form of its change in activation to the next layer. Thus
the amount of computation that the network does scales with the amount of
change in the input and layer activations, rather than the size of the network.
We introduce an optimization method for converting any pre-trained deep network
into an optimally efficient Sigma-Delta network, and show that our algorithm,
if run on the appropriate hardware, could cut at least an order of magnitude
from the computational cost of processing video data."
"Hierarchical feature discovery using non-spiking convolutional neural
networks (CNNs) has attracted much recent interest in machine learning and
computer vision. However, it is still not well understood how to create a
biologically plausible network of brain-like, spiking neurons with multi-layer,
unsupervised learning. This paper explores a novel bio-inspired spiking CNN
that is trained in a greedy, layer-wise fashion. The proposed network consists
of a spiking convolutional-pooling layer followed by a feature discovery layer
extracting independent visual features. Kernels for the convolutional layer are
trained using local learning. The learning is implemented using a sparse,
spiking auto-encoder representing primary visual features. The feature
discovery layer extracts independent features by probabilistic, leaky
integrate-and-fire (LIF) neurons that are sparsely active in response to
stimuli. The layer of the probabilistic, LIF neurons implicitly provides
lateral inhibitions to extract sparse and independent features. Experimental
results show that the convolutional layer is stack-admissible, enabling it to
support a multi-layer learning. The visual features obtained from the proposed
probabilistic LIF neurons in the feature discovery layer are utilized for
training a classifier. Classification results contribute to the independent and
informative visual features extracted in a hierarchy of convolutional and
feature discovery layers. The proposed model is evaluated on the MNIST digit
dataset using clean and noisy images. The recognition performance for clean
images is above 98%. The performance loss for recognizing the noisy images is
in the range 0.1% to 8.5% depending on noise types and densities. This level of
performance loss indicates that the network is robust to additive noise."
"Neurons, modeled as linear threshold unit (LTU), can in theory compute all
thresh- old functions. In practice, however, some of these functions require
synaptic weights of arbitrary large precision. We show here that dendrites can
alleviate this requirement. We introduce here the non-Linear Threshold Unit
(nLTU) that integrates synaptic input sub-linearly within distinct subunits to
take into account local saturation in dendrites. We systematically search
parameter space of the nTLU and TLU to compare them. Firstly, this shows that
the nLTU can compute all threshold functions with smaller precision weights
than the LTU. Secondly, we show that a nLTU can compute significantly more
functions than a LTU when an input can only make a single synapse. This work
paves the way for a new generation of network made of nLTU with binary
synapses."
"Graphs provide a powerful means for representing complex interactions between
entities. Recently, deep learning approaches are emerging for representing and
modeling graph-structured data, although the conventional deep learning methods
(such as convolutional neural networks and recurrent neural networks) have
mainly focused on grid-structured inputs (image and audio). Leveraged by the
capability of representation learning, deep learning based techniques are
reporting promising results for graph applications by detecting structural
characteristics of graphs in an automated fashion. In this paper, we attempt to
advance deep learning for graph-structured data by incorporating another
component, transfer learning. By transferring the intrinsic geometric
information learned in the source domain, our approach can help us to construct
a model for a new but related task in the target domain without collecting new
data and without training a new model from scratch. We thoroughly test our
approach with large-scale real corpora and confirm the effectiveness of the
proposed transfer learning framework for deep learning on graphs. According to
our experiments, transfer learning is most effective when the source and target
domains bear a high level of structural similarity in their graph
representations."
"We introduce the use of high order automatic differentiation, implemented via
the algebra of truncated Taylor polynomials, in genetic programming. Using the
Cartesian Genetic Programming encoding we obtain a high-order Taylor
representation of the program output that is then used to back-propagate errors
during learning. The resulting machine learning framework is called
differentiable Cartesian Genetic Programming (dCGP). In the context of symbolic
regression, dCGP offers a new approach to the long unsolved problem of constant
representation in GP expressions. On several problems of increasing complexity
we find that dCGP is able to find the exact form of the symbolic expression as
well as the constants values. We also demonstrate the use of dCGP to solve a
large class of differential equations and to find prime integrals of dynamical
systems, presenting, in both cases, results that confirm the efficacy of our
approach."
"Recurrent Neural Networks (RNNs) produce state-of-art performance on many
machine learning tasks but their demand on resources in terms of memory and
computational power are often high. Therefore, there is a great interest in
optimizing the computations performed with these models especially when
considering development of specialized low-power hardware for deep networks.
One way of reducing the computational needs is to limit the numerical precision
of the network weights and biases, and this will be addressed for the case of
RNNs. We present results from the use of different stochastic and deterministic
reduced precision training methods applied to two major RNN types, which are
then tested on three datasets. The results show that the stochastic and
deterministic ternarization, pow2- ternarization, and exponential quantization
methods gave rise to low-precision RNNs that produce similar and even higher
accuracy on certain datasets, therefore providing a path towards training more
efficient implementations of RNNs in specialized hardware."
"This paper considers a process for the creation and subsequent firing of
sequences of neuronal patterns, as might be found in the human brain. The scale
is one of larger patterns emerging from an ensemble mass, possibly through some
type of energy equation and a reduction procedure. The links between the
patterns can be formed naturally, as a residual effect of the pattern creation
itself. If the process is valid, then the pattern creation can be relatively
simplistic and automatic, where the neuron does not have to do anything
particularly intelligent. The pattern interfaces become slightly abstract
without firm boundaries and exact structure is determined more by averages or
ratios. This paper follows-on closely from the earlier research, including two
earlier papers in the series and uses the ideas of entropy and cohesion. With a
small addition, it is possible to show how the inter-pattern links can be
determined. A new compact Grid form of an earlier Counting Mechanism is also
demonstrated. Finally, it is possible to explain how a very basic repeating
structure can form the arbitrary patterns and activation sequences between
them. A key question of how nodes synchronise may even be answerable."
"Spiking Neural Networks (SNN) are more closely related to brain-like
computation and inspire hardware implementation. This is enabled by small
networks that give high performance on standard classification problems. In
literature, typical SNNs are deep and complex in terms of network structure,
weight update rules and learning algorithms. This makes it difficult to
translate them into hardware. In this paper, we first develop a simple
2-layered network in software which compares with the state of the art on four
different standard data-sets within SNNs and has improved efficiency. For
example, it uses lower number of neurons (3 x), synapses (3.5 x) and epochs for
training (30 x) for the Fisher Iris classification problem. The efficient
network is based on effective population coding and synapse-neuron co-design.
Second, we develop a computationally efficient (15000 x) and accurate
(correlation of 0.98) method to evaluate the performance of the network without
standard recognition tests. Third, we show that the method produces a
robustness metric that can be used to evaluate noise tolerance."
"The architecture of neural Turing machines is differentiable end to end and
is trainable with gradient descent methods. Due to their large unfolded depth
Neural Turing Machines are hard to train and because of their linear access of
complete memory they do not scale. Other architectures have been studied to
overcome these difficulties. In this report we focus on improving the quality
of prediction of the original linear memory architecture on copy and repeat
copy tasks. Copy task predictions on sequences of length six times larger than
those the neural Turing machine was trained on prove to be highly accurate and
so do predictions of repeat copy tasks for sequences with twice the repetition
number and twice the sequence length neural Turing machine was trained on."
"In this paper, we present a significant improvement of Quick Hypervolume
algorithm, one of the state-of-the-art algorithms for calculating exact
hypervolume of the space dominated by a set of d-dimensional points. This value
is often used as a quality indicator in multiobjective evolutionary algorithms
and other multiobjective metaheuristics and the efficiency of calculating this
indicator is of crucial importance especially in the case of large sets or many
dimensional objective spaces. We use a similar divide and conquer scheme as in
the original Quick Hypervolume algorithm, but in our algorithm we split the
problem into smaller sub-problems in a different way. Through both theoretical
analysis and computational study we show that our approach improves
computational complexity of the algorithm and practical running times."
"The standard LSTM, although it succeeds in the modeling long-range
dependences, suffers from a highly complex structure that can be simplified
through modifications to its gate units. This paper was to perform an empirical
comparison between the standard LSTM and three new simplified variants that
were obtained by eliminating input signal, bias and hidden unit signal from
individual gates, on the tasks of modeling two sequence datasets. The
experiments show that the three variants, with reduced parameters, can achieve
comparable performance with the standard LSTM. Due attention should be paid to
turning the learning rate to achieve high accuracies"
"Deep neural networks are gaining in popularity as they are used to generate
state-of-the-art results for a variety of computer vision and machine learning
applications. At the same time, these networks have grown in depth and
complexity in order to solve harder problems. Given the limitations in power
budgets dedicated to these networks, the importance of low-power, low-memory
solutions has been stressed in recent years. While a large number of dedicated
hardware using different precisions has recently been proposed, there exists no
comprehensive study of different bit precisions and arithmetic in both inputs
and network parameters. In this work, we address this issue and perform a study
of different bit-precisions in neural networks (from floating-point to
fixed-point, powers of two, and binary). In our evaluation, we consider and
analyze the effect of precision scaling on both network accuracy and hardware
metrics including memory footprint, power and energy consumption, and design
area. We also investigate training-time methodologies to compensate for the
reduction in accuracy due to limited bit precision and demonstrate that in most
cases, precision scaling can deliver significant benefits in design metrics at
the cost of very modest decreases in network accuracy. In addition, we propose
that a small portion of the benefits achieved when using lower precisions can
be forfeited to increase the network size and therefore the accuracy. We
evaluate our experiments, using three well-recognized networks and datasets to
show its generality. We investigate the trade-offs and highlight the benefits
of using lower precisions in terms of energy and memory footprint."
"Many neural networks exhibit stability in their activation patterns over time
in response to inputs from sensors operating under real-world conditions. By
capitalizing on this property of natural signals, we propose a Recurrent Neural
Network (RNN) architecture called a delta network in which each neuron
transmits its value only when the change in its activation exceeds a threshold.
The execution of RNNs as delta networks is attractive because their states must
be stored and fetched at every timestep, unlike in convolutional neural
networks (CNNs). We show that a naive run-time delta network implementation
offers modest improvements on the number of memory accesses and computes, but
optimized training techniques confer higher accuracy at higher speedup. With
these optimizations, we demonstrate a 9X reduction in cost with negligible loss
of accuracy for the TIDIGITS audio digit recognition benchmark. Similarly, on
the large Wall Street Journal speech recognition benchmark even existing
networks can be greatly accelerated as delta networks, and a 5.7x improvement
with negligible loss of accuracy can be obtained through training. Finally, on
an end-to-end CNN trained for steering angle prediction in a driving dataset,
the RNN cost can be reduced by a substantial 100X."
"One of the major distinguishing features of the dynamic multiobjective
optimization problems (DMOPs) is the optimization objectives will change over
time, thus tracking the varying Pareto-optimal front becomes a challenge. One
of the promising solutions is reusing the ""experiences"" to construct a
prediction model via statistical machine learning approaches. However most of
the existing methods ignore the non-independent and identically distributed
nature of data used to construct the prediction model. In this paper, we
propose an algorithmic framework, called Tr-DMOEA, which integrates transfer
learning and population-based evolutionary algorithm for solving the DMOPs.
This approach takes the transfer learning method as a tool to help reuse the
past experience for speeding up the evolutionary process, and at the same time,
any population based multiobjective algorithms can benefit from this
integration without any extensive modifications. To verify this, we incorporate
the proposed approach into the development of three well-known algorithms,
nondominated sorting genetic algorithm II (NSGA-II), multiobjective particle
swarm optimization (MOPSO), and the regularity model-based multiobjective
estimation of distribution algorithm (RM-MEDA), and then employ twelve
benchmark functions to test these algorithms as well as compare with some
chosen state-of-the-art designs. The experimental results confirm the
effectiveness of the proposed method through exploiting machine learning
technology."
"Over the last three decades, a large number of evolutionary algorithms have
been developed for solving multiobjective optimization problems. However, there
lacks an up-to-date and comprehensive software platform for researchers to
properly benchmark existing algorithms and for practitioners to apply selected
algorithms to solve their real-world problems. The demand of such a common tool
becomes even more urgent, when the source code of many proposed algorithms has
not been made publicly available. To address these issues, we have developed a
MATLAB platform for evolutionary multi-objective optimization in this paper,
called PlatEMO, which includes more than 50 multi-objective evolutionary
algorithms and more than 100 multi-objective test problems, along with several
widely used performance indicators. With a user-friendly graphical user
interface, PlatEMO enables users to easily compare several evolutionary
algorithms at one time and collect statistical results in Excel or LaTeX files.
More importantly, PlatEMO is completely open source, such that users are able
to develop new algorithms on the basis of it. This paper introduces the main
features of PlatEMO and illustrates how to use it for performing comparative
experiments, embedding new algorithms, creating new test problems, and
developing performance indicators. Source code of PlatEMO is now available at:
http://bimk.ahu.edu.cn/index.php?s=/Index/Software/index.html."
"In distributed evolutionary algorithms, migration interval is used to decide
migration moments. Nevertheless, migration moments predetermined by intervals
cannot match the dynamic situation of evolution. In this paper, a scheme of
setting the success rate of migration based on subpopulation diversity at each
interval is proposed. With the scheme, migration still occurs at intervals, but
the probability of immigrants entering the target subpopulation will be
determined by the diversity of this subpopulation according to a proposed
formula. An analysis shows that the time consumption of our scheme is
acceptable. In our experiments, the basement of parallelism is an evolutionary
algorithm for the traveling salesman problem. Under different value
combinations of parameters for the formula, outcomes for eight benchmark
instances of the distributed evolutionary algorithm with the proposed scheme
are compared with those of a traditional one, respectively. Results show that
the distributed evolutionary algorithm based on our scheme has a significant
advantage on solutions especially for high difficulty instances. Moreover, it
can be seen that the algorithm with the scheme has the most outstanding
performance under three value combinations of above-mentioned parameters for
the formula."
"Several learning rules for synaptic plasticity, that depend on either spike
timing or internal state variables, have been proposed in the past imparting
varying computational capabilities to Spiking Neural Networks. Due to design
complications these learning rules are typically not implemented on
neuromorphic devices leaving the devices to be only capable of inference. In
this work we propose a unidirectional post-synaptic potential dependent
learning rule that is only triggered by pre-synaptic spikes, and easy to
implement on hardware. We demonstrate that such a learning rule is functionally
capable of replicating computational capabilities of pairwise STDP. Further
more, we demonstrate that this learning rule can be used to learn and classify
spatio-temporal spike patterns in an unsupervised manner using individual
neurons. We argue that this learning rule is computationally powerful and also
ideal for hardware implementations due to its unidirectional memory access."
"Brain inspired neuromorphic computing has demonstrated remarkable advantages
over traditional von Neumann architecture for its high energy efficiency and
parallel data processing. However, the limited resolution of synaptic weights
degrades system accuracy and thus impedes the use of neuromorphic systems. In
this work, we propose three orthogonal methods to learn synapses with one-level
precision, namely, distribution-aware quantization, quantization regularization
and bias tuning, to make image classification accuracy comparable to the
state-of-the-art. Experiments on both multi-layer perception and convolutional
neural networks show that the accuracy drop can be well controlled within 0.19%
(5.53%) for MNIST (CIFAR-10) database, compared to an ideal system without
quantization."
"The paper describes an approach to implementing genetic programming, which
uses the LLVM library to just-in-time compile/interpret the evolved abstract
syntax trees. The solution is described in some detail, including a parser
(based on FlexC++ and BisonC++) that can construct the trees from a simple toy
language with C-like syntax. The approach is compared with a previous
implementation (based on direct execution of trees using polymorphic functors)
in terms of execution speed."
"Most existing studies on evolutionary multi-objective optimization focus on
approximating the whole Pareto-optimal front. Nevertheless, rather than the
whole front, which demands for too many points (especially in a
high-dimensional space), the decision maker might only interest in a partial
region, called the region of interest. In this case, solutions outside this
region can be noisy to the decision making procedure. Even worse, there is no
guarantee that we can find the preferred solutions when tackling problems with
complicated properties or a large number of objectives. In this paper, we
develop a systematic way to incorporate the decision maker's preference
information into the decomposition-based evolutionary multi-objective
optimization methods. Generally speaking, our basic idea is a non-uniform
mapping scheme by which the originally uniformly distributed reference points
on a canonical simplex can be mapped to the new positions close to the
aspiration level vector specified by the decision maker. By these means, we are
able to steer the search process towards the region of interest either directly
or in an interactive manner and also handle a large number of objectives. In
the meanwhile, the boundary solutions can be approximated given the decision
maker's requirements. Furthermore, the extent of the region of the interest is
intuitively understandable and controllable in a closed form. Extensive
experiments, both proof-of-principle and on a variety of problems with 3 to 10
objectives, fully demonstrate the effectiveness of our proposed method for
approximating the preferred solutions in the region of interest."
"A particle swarm optimizer (PSO) loosely based on the phenomena of
crystallization and a chaos factor which follows the complimentary error
function is described. The method features three phases: diffusion, directed
motion, and nucleation. During the diffusion phase random walk is the only
contributor to particle motion. As the algorithm progresses the contribution
from chaos decreases and movement toward global best locations is pursued until
convergence has occurred. The algorithm was found to be more robust to local
minima in multimodal test functions than a standard PSO algorithm and is
designed for problems which feature experimental precision."
"We introduce a genetic programming method for solving multiple Boolean
circuit synthesis tasks simultaneously. This allows us to solve a set of
elementary logic functions twice as easily as with a direct, single-task
approach."
"This paper contributes to a development of randomized methods for neural
networks. The proposed learner model is generated incrementally by stochastic
configuration (SC) algorithms, termed as Stochastic Configuration Networks
(SCNs). In contrast to the existing randomised learning algorithms for single
layer feed-forward neural networks (SLFNNs), we randomly assign the input
weights and biases of the hidden nodes in the light of a supervisory mechanism,
and the output weights are analytically evaluated in either constructive or
selective manner. As fundamentals of SCN-based data modelling techniques, we
establish some theoretical results on the universal approximation property.
Three versions of SC algorithms are presented for regression problems
(applicable for classification problems as well) in this work. Simulation
results concerning both function approximation and real world data regression
indicate some remarkable merits of our proposed SCNs in terms of less human
intervention on the network size setting, the scope adaptation of random
parameters, fast learning and sound generalization."
"Increasing nature-inspired metaheuristic algorithms are applied to solving
the real-world optimization problems, as they have some advantages over the
classical methods of numerical optimization. This paper has proposed a new
nature-inspired metaheuristic called Whale Swarm Algorithm for function
optimization, which is inspired by the whales behavior of communicating with
each other via ultrasound for hunting. The proposed Whale Swarm Algorithm has
been compared with several popular metaheuristic algorithms on comprehensive
performance metrics. According to the experimental results, Whale Swarm
Algorithm has a quite competitive performance when compared with other
algorithms."
"Operational maturity of biological control systems have fuelled the
inspiration for a large number of mathematical and logical models for control,
automation and optimisation. The human brain represents the most sophisticated
control architecture known to us and is a central motivation for several
research attempts across various domains. In the present work, we introduce an
algorithm for mathematical optimisation that derives its intuition from the
hierarchical and distributed operations of the human motor system. The system
comprises global leaders, local leaders and an effector population that adapt
dynamically to attain global optimisation via a feedback mechanism coupled with
the structural hierarchy. The hierarchical system operation is distributed into
local control for movement and global controllers that facilitate gross motion
and decision making. We present our algorithm as a variant of the classical
Differential Evolution algorithm, introducing a hierarchical crossover
operation. The discussed approach is tested exhaustively on standard test
functions from the CEC 2017 benchmark. Our algorithm significantly outperforms
various standard algorithms as well as their popular variants as discussed in
the results."
"The abstraction tasks are challenging for multi- modal sequences as they
require a deeper semantic understanding and a novel text generation for the
data. Although the recurrent neural networks (RNN) can be used to model the
context of the time-sequences, in most cases the long-term dependencies of
multi-modal data make the back-propagation through time training of RNN tend to
vanish in the time domain. Recently, inspired from Multiple Time-scale
Recurrent Neural Network (MTRNN), an extension of Gated Recurrent Unit (GRU),
called Multiple Time-scale Gated Recurrent Unit (MTGRU), has been proposed to
learn the long-term dependencies in natural language processing. Particularly
it is also able to accomplish the abstraction task for paragraphs given that
the time constants are well defined. In this paper, we compare the MTRNN and
MTGRU in terms of its learning performances as well as their abstraction
representation on higher level (with a slower neural activation). This was done
by conducting two studies based on a smaller data- set (two-dimension time
sequences from non-linear functions) and a relatively large data-set
(43-dimension time sequences from iCub manipulation tasks with multi-modal
data). We conclude that gated recurrent mechanisms may be necessary for
learning long-term dependencies in large dimension multi-modal data-sets (e.g.
learning of robot manipulation), even when natural language commands was not
involved. But for smaller learning tasks with simple time-sequences, generic
version of recurrent models, such as MTRNN, were sufficient to accomplish the
abstraction task."
"Recurrent neural networks (RNNs) have achieved state-of-the-art performance
on many diverse tasks, from machine translation to surgical activity
recognition, yet training RNNs to capture long-term dependencies remains
difficult. To date, the vast majority of successful RNN architectures alleviate
this problem by facilitating long-term gradient flow using nearly-additive
connections between adjacent states, as originally introduced in long
short-term memory (LSTM). In this paper, we investigate a different approach
for encouraging gradient flow that is based on NARX RNNs, which generalize
typical RNNs by allowing direct connections from the distant past.
Analytically, we 1) generalize previous gradient decompositions for typical
RNNs to general NARX RNNs and 2) formally connect gradient flow to edges along
paths. We then introduce an example architecture that is based on these ideas,
and we demonstrate that this architecture matches or exceeds LSTM performance
on 5 diverse tasks. Finally we describe many avenues for future work, including
the exploration of other NARX RNN architectures, the possible combination of
mechanisms from LSTM and NARX RNNs, and the adoption of recent LSTM-based
advances to NARX RNN architectures."
"Algorithm learning is a core problem in artificial intelligence with
significant implications on automation level that can be achieved by machines.
Recently deep learning methods are emerging for synthesizing an algorithm from
its input-output examples, the most successful being the Neural GPU, capable of
learning multiplication. We present several improvements to the Neural GPU that
substantially reduces training time and improves generalization. We introduce a
technique of general applicability to use hard nonlinearities with saturation
cost. We also introduce a technique of diagonal gates that can be applied to
active-memory models. The proposed architecture is the first capable of
learning decimal multiplication end-to-end."
"Multi-task learning employs shared representation of knowledge for learning
multiple instances from the same or related problems. Time series prediction
consists of several instances that are defined by the way they are broken down
into fixed windows known as embedding dimension. Finding the optimal values for
embedding dimension is a computationally intensive task. Therefore, we
introduce a new category of problem called dynamic time series prediction that
requires a trained model to give prediction when presented with different
values of the embedding dimension. This can be seen a new class of time series
prediction where dynamic prediction is needed. In this paper, we propose a
co-evolutionary multi-task learning method that provides a synergy between
multi-task learning and coevolution. This enables neural networks to retain
modularity during training for building blocks of knowledge for different
instances of the problem. The effectiveness of the proposed method is
demonstrated using one-step-ahead chaotic time series problems. The results
show that the proposed method can effectively be used for different instances
of the related time series problems while providing improved generalisation
performance."
"Emulating spiking neural networks on analog neuromorphic hardware offers
several advantages over simulating them on conventional computers, particularly
in terms of speed and energy consumption. However, this usually comes at the
cost of reduced control over the dynamics of the emulated networks. In this
paper, we demonstrate how iterative training of a hardware-emulated network can
compensate for anomalies induced by the analog substrate. We first convert a
deep neural network trained in software to a spiking network on the BrainScaleS
wafer-scale neuromorphic system, thereby enabling an acceleration factor of 10
000 compared to the biological time domain. This mapping is followed by the
in-the-loop training, where in each training step, the network activity is
first recorded in hardware and then used to compute the parameter updates in
software via backpropagation. An essential finding is that the parameter
updates do not have to be precise, but only need to approximately follow the
correct gradient, which simplifies the computation of updates. Using this
approach, after only several tens of iterations, the spiking network shows an
accuracy close to the ideal software-emulated prototype. The presented
techniques show that deep spiking networks emulated on analog neuromorphic
devices can attain good computational performance despite the inherent
variations of the analog substrate."
"For genetic algorithms using a bit-string representation of length~$n$, the
general recommendation is to take $1/n$ as mutation rate. In this work, we
discuss whether this is really justified for multimodal functions. Taking jump
functions and the $(1+1)$ evolutionary algorithm as the simplest example, we
observe that larger mutation rates give significantly better runtimes. For the
$\jump_{m,n}$ function, any mutation rate between $2/n$ and $m/n$ leads to a
speed-up at least exponential in $m$ compared to the standard choice.
  The asymptotically best runtime, obtained from using the mutation rate $m/n$
and leading to a speed-up super-exponential in $m$, is very sensitive to small
changes of the mutation rate. Any deviation by a small $(1 \pm \eps)$ factor
leads to a slow-down exponential in $m$. Consequently, any fixed mutation rate
gives strongly sub-optimal results for most jump functions.
  Building on this observation, we propose to use a random mutation rate
$\alpha/n$, where $\alpha$ is chosen from a power-law distribution. We prove
that the $(1+1)$ EA with this heavy-tailed mutation rate optimizes any
$\jump_{m,n}$ function in a time that is only a small polynomial (in~$m$)
factor above the one stemming from the optimal rate for this $m$.
  Our heavy-tailed mutation operator yields similar speed-ups (over the best
known performance guarantees) for the vertex cover problem in bipartite graphs
and the matching problem in general graphs.
  Following the example of fast simulated annealing, fast evolution strategies,
and fast evolutionary programming, we propose to call genetic algorithms using
a heavy-tailed mutation operator \emph{fast genetic algorithms}."
"Evolutionary algorithms have recently been used to create a wide range of
artistic work. In this paper, we propose a new approach for the composition of
new images from existing ones, that retain some salient features of the
original images. We introduce evolutionary algorithms that create new images
based on a fitness function that incorporates feature covariance matrices
associated with different parts of the images. This approach is very flexible
in that it can work with a wide range of features and enables targeting
specific regions in the images. For the creation of the new images, we propose
a population-based evolutionary algorithm with mutation and crossover operators
based on random walks. Our experimental results reveal a spectrum of
aesthetically pleasing images that can be obtained with the aid of our
evolutionary process."
"Echo state networks are a recently developed type of recurrent neural network
where the internal layer is fixed with random weights, and only the output
layer is trained on specific data. Echo state networks are increasingly being
used to process spatiotemporal data in real-world settings, including speech
recognition, event detection, and robot control. A strength of echo state
networks is the simple method used to train the output layer - typically a
collection of linear readout weights found using a least squares approach.
Although straightforward to train and having a low computational cost to use,
this method may not yield acceptable accuracy performance on noisy data.
  This study compares the performance of three echo state network output layer
methods to perform classification on noisy data: using trained linear weights,
using sparse trained linear weights, and using trained low-rank approximations
of reservoir states. The methods are investigated experimentally on both
synthetic and natural datasets. The experiments suggest that using regularized
least squares to train linear output weights is superior on data with low
noise, but using the low-rank approximations may significantly improve accuracy
on datasets contaminated with higher noise levels."
"This work presents a new algorithm called evolutionary exploration of
augmenting convolutional topologies (EXACT), which is capable of evolving the
structure of convolutional neural networks (CNNs). EXACT is in part modeled
after the neuroevolution of augmenting topologies (NEAT) algorithm, with
notable exceptions to allow it to scale to large scale distributed computing
environments and evolve networks with convolutional filters. In addition to
multithreaded and MPI versions, EXACT has been implemented as part of a BOINC
volunteer computing project, allowing large scale evolution. During a period of
two months, over 4,500 volunteered computers on the Citizen Science Grid
trained over 120,000 CNNs and evolved networks reaching 98.32% test data
accuracy on the MNIST handwritten digits dataset. These results are even
stronger as the backpropagation strategy used to train the CNNs was fairly
rudimentary (ReLU units, L2 regularization and Nesterov momentum) and these
were initial test runs done without refinement of the backpropagation
hyperparameters. Further, the EXACT evolutionary strategy is independent of the
method used to train the CNNs, so they could be further improved by advanced
techniques like elastic distortions, pretraining and dropout. The evolved
networks are also quite interesting, showing ""organic"" structures and
significant differences from standard human designed architectures."
"A framework for implementing reservoir computing (RC) and extreme learning
machines (ELMs), two types of artificial neural networks, based on 1D
elementary Cellular Automata (CA) is presented, in which two separate CA rules
explicitly implement the minimum computational requirements of the reservoir
layer: hyperdimensional projection and short-term memory. CAs are cell-based
state machines, which evolve in time in accordance with local rules based on a
cells current state and those of its neighbors. Notably, simple single cell
shift rules as the memory rule in a fixed edge CA afforded reasonable success
in conjunction with a variety of projection rules, potentially significantly
reducing the optimal solution search space. Optimal iteration counts for the CA
rule pairs can be estimated for some tasks based upon the category of the
projection rule. Initial results support future hardware realization, where CAs
potentially afford orders of magnitude reduction in size, weight, and power
(SWaP) requirements compared with floating point RC implementations."
"In the evolutionary computation research community, the performance of most
evolutionary algorithms (EAs) depends strongly on their implemented coordinate
system. However, the commonly used coordinate system is fixed and not well
suited for different function landscapes, EAs thus might not search
efficiently. To overcome this shortcoming, in this paper we propose a
framework, named ACoS, to adaptively tune the coordinate systems in EAs. In
ACoS, an Eigen coordinate system is established by making use of the cumulative
population distribution information, which can be obtained based on a
covariance matrix adaptation strategy and an additional archiving mechanism.
Since the population distribution information can reflect the features of the
function landscape to some extent, EAs in the Eigen coordinate system have the
capability to identify the modality of the function landscape. In addition, the
Eigen coordinate system is coupled with the original coordinate system, and
they are selected according to a probability vector. The probability vector
aims to determine the selection ratio of each coordinate system for each
individual, and is adaptively updated based on the collected information from
the offspring. ACoS has been applied to two of the most popular EA paradigms,
i.e., particle swarm optimization (PSO) and differential evolution (DE), for
solving 30 test functions with 30 and 50 dimensions at the 2014 IEEE Congress
on Evolutionary Computation. The experimental studies demonstrate its
effectiveness."
"The Echo State Network (ESN) is a specific recurrent network, which has
gained popularity during the last years. The model has a recurrent network
named reservoir, that is fixed during the learning process. The reservoir is
used for transforming the input space in a larger space. A fundamental property
that provokes an impact on the model accuracy is the Echo State Property (ESP).
There are two main theoretical results related to the ESP. First, a sufficient
condition for the ESP existence that involves the singular values of the
reservoir matrix. Second, a necessary condition for the ESP. The ESP can be
violated according to the spectral radius value of the reservoir matrix. There
is a theoretical gap between these necessary and sufficient conditions. This
article presents an empirical analysis of the accuracy and the projections of
reservoirs that satisfy this theoretical gap. It gives some insights about the
generation of the reservoir matrix. From previous works, it is already known
that the optimal accuracy is obtained near to the border of stability control
of the dynamics. Then, according to our empirical results, we can see that this
border seems to be closer to the sufficient conditions than to the necessary
conditions of the ESP."
"Neuroevolution methods evolve the weights of a neural network, and in some
cases the topology, but little work has been done to analyze the effect of
evolving the activation functions of individual nodes on network size, which is
important when training networks with a small number of samples. In this work
we extend the neuroevolution algorithm NEAT to evolve the activation function
of neurons in addition to the topology and weights of the network. The size and
performance of networks produced using NEAT with uniform activation in all
nodes, or homogenous networks, is compared to networks which contain a mixture
of activation functions, or heterogenous networks. For a number of regression
and classification benchmarks it is shown that, (1) qualitatively different
activation functions lead to different results in homogeneous networks, (2) the
heterogeneous version of NEAT is able to select well performing activation
functions, (3) producing heterogeneous networks that are significantly smaller
than homogeneous networks."
"We evolve binary mux-6 trees for up to 100000 generations evolving some
programs with more than a hundred million nodes. Our unbounded Long-Term
Evolution Experiment LTEE GP appears not to evolve building blocks but does
suggests a limit to bloat. We do see periods of tens even hundreds of
generations where the population is 100 percent functionally converged. The
distribution of tree sizes is not as predicted by theory."
"Grammatical Evolution (GE) is a population-based evolutionary algorithm,
where a formal grammar is used in the genotype to phenotype mapping process.
PonyGE2 is an open source implementation of GE in Python, developed at UCD's
Natural Computing Research and Applications group. It is intended as an
advertisement and a starting-point for those new to GE, a reference for
students and researchers, a rapid-prototyping medium for our own experiments,
and a Python workout. As well as providing the characteristic genotype to
phenotype mapping of GE, a search algorithm engine is also provided. A number
of sample problems and tutorials on how to use and adapt PonyGE2 have been
developed."
"Previous research using evolutionary computation in Multi-Agent Systems
indicates that assigning fitness based on team vs.\ individual behavior has a
strong impact on the ability of evolved teams of artificial agents to exhibit
teamwork in challenging tasks. However, such research only made use of
single-objective evolution. In contrast, when a multiobjective evolutionary
algorithm is used, populations can be subject to individual-level objectives,
team-level objectives, or combinations of the two. This paper explores the
performance of cooperatively coevolved teams of agents controlled by artificial
neural networks subject to these types of objectives. Specifically, predator
agents are evolved to capture scripted prey agents in a torus-shaped grid
world. Because of the tension between individual and team behaviors, multiple
modes of behavior can be useful, and thus the effect of modular neural networks
is also explored. Results demonstrate that fitness rewarding individual
behavior is superior to fitness rewarding team behavior, despite being applied
to a cooperative task. However, the use of networks with multiple modules
allows predators to discover intelligent behavior, regardless of which type of
objectives are used."
"Traffic light timing optimization is still an active line of research despite
the wealth of scientific literature on the topic, and the problem remains
unsolved for any non-toy scenario. One of the key issues with traffic light
optimization is the large scale of the input information that is available for
the controlling agent, namely all the traffic data that is continually sampled
by the traffic detectors that cover the urban network. This issue has in the
past forced researchers to focus on agents that work on localized parts of the
traffic network, typically on individual intersections, and to coordinate every
individual agent in a multi-agent setup. In order to overcome the large scale
of the available state information, we propose to rely on the ability of deep
Learning approaches to handle large input spaces, in the form of Deep
Deterministic Policy Gradient (DDPG) algorithm. We performed several
experiments with a range of models, from the very simple one (one intersection)
to the more complex one (a big city section)."
"In this paper we systematically study the importance, i.e., the influence on
performance, of the main design elements that differentiate scalarizing
functions-based multiobjective evolutionary algorithms (MOEAs). This class of
MOEAs includes Multiobjecitve Genetic Local Search (MOGLS) and Multiobjective
Evolutionary Algorithm Based on Decomposition (MOEA/D) and proved to be very
successful in multiple computational experiments and practical applications.
The two algorithms share the same common structure and differ only in two main
aspects. Using three different multiobjective combinatorial optimization
problems, i.e., the multiobjective symmetric traveling salesperson problem, the
traveling salesperson problem with profits, and the multiobjective set covering
problem, we show that the main differentiating design element is the mechanism
for parent selection, while the selection of weight vectors, either random or
uniformly distributed, is practically negligible if the number of uniform
weight vectors is sufficiently large."
"This paper studies improving solvers based on their past solving experiences,
and focuses on improving solvers by offline training. Specifically, the key
issues of offline training methods are discussed, and research belonging to
this category but from different areas are reviewed in a unified framework.
Existing training methods generally adopt a two-stage strategy in which
selecting the training instances and training instances are treated in two
independent phases. This paper proposes a new training method, dubbed LiangYi,
which addresses these two issues simultaneously. LiangYi includes a training
module for a population-based solver and an instance sampling module for
updating the training instances. The idea behind LiangYi is to promote the
population-based solver by training it (with the training module) to improve
its performance on those instances (discovered by the sampling module) on which
it performs badly, while keeping the good performances obtained by it on
previous instances. An instantiation of LiangYi on the Travelling Salesman
Problem is also proposed. Empirical results on a huge testing set containing
10000 instances showed LiangYi could train solvers that perform significantly
better than the solvers trained by other state-of-the-art training method.
Moreover, empirical investigation of the behaviours of LiangYi confirmed it was
able to continuously improve the solver through training."
"Evolutionary illumination is a recent technique that allows producing many
diverse, optimal solutions in a map of manually defined features. To support
the large amount of objective function evaluations, surrogate model assistance
was recently introduced. Illumination models need to represent many more,
diverse optimal regions than classical surrogate models. In this PhD thesis, we
propose to decompose the sample set, decreasing model complexity, by
hierarchically segmenting the training set according to their coordinates in
feature space. An ensemble of diverse models can then be trained to serve as a
surrogate to illumination."
"A runtime analysis of the Univariate Marginal Distribution Algorithm (UMDA)
is presented on the OneMax function for wide ranges of its parameters $\mu$ and
$\lambda$. If $\mu\ge c\log n$ for some constant $c>0$ and
$\lambda=(1+\Theta(1))\mu$, a general bound $O(\mu n)$ on the expected runtime
is obtained. This bound crucially assumes that all marginal probabilities of
the algorithm are confined to the interval $[1/n,1-1/n]$. If $\mu\ge c'
\sqrt{n}\log n$ for a constant $c'>0$ and $\lambda=(1+\Theta(1))\mu$, the
behavior of the algorithm changes and the bound on the expected runtime becomes
$O(\mu\sqrt{n})$, which typically even holds if the borders on the marginal
probabilities are omitted.
  The results supplement the recently derived lower bound
$\Omega(\mu\sqrt{n}+n\log n)$ by Krejca and Witt (FOGA 2017) and turn out as
tight for the two very different values $\mu=c\log n$ and $\mu=c'\sqrt{n}\log
n$. They also improve the previously best known upper bound $O(n\log n\log\log
n)$ by Dang and Lehre (GECCO 2015)."
"As the title suggests, we will describe (and justify through the presentation
of some of the relevant mathematics) prediction methodologies for sensor
measurements. This exposition will mainly be concerned with the mathematics
related to modeling the sensor measurements."
"The convolutional neural network (CNN), which is one of the deep learning
models, has seen much success in a variety of computer vision tasks. However,
designing CNN architectures still requires expert knowledge and a lot of trial
and error. In this paper, we attempt to automatically construct CNN
architectures for an image classification task based on Cartesian genetic
programming (CGP). In our method, we adopt highly functional modules, such as
convolutional blocks and tensor concatenation, as the node functions in CGP.
The CNN structure and connectivity represented by the CGP encoding method are
optimized to maximize the validation accuracy. To evaluate the proposed method,
we constructed a CNN architecture for the image classification task with the
CIFAR-10 dataset. The experimental result shows that the proposed method can be
used to automatically find the competitive CNN architecture compared with
state-of-the-art models."
"Spiking Neural Network (SNN) naturally inspires hardware implementation as it
is based on biology. For learning, spike time dependent plasticity (STDP) may
be implemented using an energy efficient waveform superposition on memristor
based synapse. However, system level implementation has three challenges.
First, a classic dilemma is that recognition requires current reading for short
voltage$-$spikes which is disturbed by large voltage$-$waveforms that are
simultaneously applied on the same memristor for real$-$time learning i.e. the
simultaneous read$-$write dilemma. Second, the hardware needs to exactly
replicate software implementation for easy adaptation of algorithm to hardware.
Third, the devices used in hardware simulations must be realistic. In this
paper, we present an approach to address the above concerns. First, the
learning and recognition occurs in separate arrays simultaneously in
real$-$time, asynchronously $-$ avoiding non$-$biomimetic clocking based
complex signal management. Second, we show that the hardware emulates software
at every stage by comparison of SPICE (circuit$-$simulator) with MATLAB
(mathematical SNN algorithm implementation in software) implementations. As an
example, the hardware shows 97.5 per cent accuracy in classification which is
equivalent to software for a Fisher$-$Iris dataset. Third, the STDP is
implemented using a model of synaptic device implemented using HfO2 memristor.
We show that an increasingly realistic memristor model slightly reduces the
hardware performance (85 per cent), which highlights the need to engineer RRAM
characteristics specifically for SNN."
"We propose a new way to self-adjust the mutation rate in population-based
evolutionary algorithms in discrete search spaces. Roughly speaking, it
consists of creating half the offspring with a mutation rate that is twice the
current mutation rate and the other half with half the current rate. The
mutation rate is then updated to the rate used in that subpopulation which
contains the best offspring.
  We analyze how the $(1+\lambda)$ evolutionary algorithm with this
self-adjusting mutation rate optimizes the OneMax test function. We prove that
this dynamic version of the $(1+\lambda)$~EA finds the optimum in an expected
optimization time (number of fitness evaluations) of
$O(n\lambda/\!\log\lambda+n\log n)$. This time is asymptotically smaller than
the optimization time of the classic $(1+\lambda)$ EA. Previous work shows that
this performance is best-possible among all $\lambda$-parallel mutation-based
unbiased black-box algorithms.
  This result shows that the new way of adjusting the mutation rate can find
optimal dynamic parameter values on the fly. Since our adjustment mechanism is
simpler than the ones previously used for adjusting the mutation rate and does
not have parameters itself, we are optimistic that it will find other
applications."
"The decomposition-based method has been recognized as a major approach for
multi-objective optimization. It decomposes a multi-objective optimization
problem into several single-objective optimization subproblems, each of which
is usually defined as a scalarizing function using a weight vector. Due to the
characteristics of the contour line of a particular scalarizing function, the
performance of the decomposition-based method strongly depends on the Pareto
front's shape by merely using a single scalarizing function, especially when
facing a large number of objectives. To improve the flexibility of the
decomposition-based method, this paper develops an adversarial decomposition
method that leverages the complementary characteristics of two different
scalarizing functions within a single paradigm. More specifically, we maintain
two co-evolving populations simultaneously by using different scalarizing
functions. In order to avoid allocating redundant computational resources to
the same region of the Pareto front, we stably match these two co-evolving
populations into one-one solution pairs according to their working regions of
the Pareto front. Then, each solution pair can at most contribute one mating
parent during the mating selection process. Comparing with nine
state-of-the-art many-objective optimizers, we have witnessed the competitive
performance of our proposed algorithm on 130 many-objective test instances with
various characteristics and Pareto front's shapes."
"We introduce a method for automatically selecting the path, or syllabus, that
a neural network follows through a curriculum so as to maximise learning
efficiency. A measure of the amount that the network learns from each data
sample is provided as a reward signal to a nonstationary multi-armed bandit
algorithm, which then determines a stochastic syllabus. We consider a range of
signals derived from two distinct indicators of learning progress: rate of
increase in prediction accuracy, and rate of increase in network complexity.
Experimental results for LSTM networks on three curricula demonstrate that our
approach can significantly accelerate learning, in some cases halving the time
required to attain a satisfactory performance level."
"Credit card fraud detection based on machine learning has recently attracted
considerable interest from the research community. One of the most important
tasks in this area is the ability of classifiers to handle the imbalance in
credit card data. In this scenario, classifiers tend to yield poor accuracy on
the fraud class (minority class) despite realizing high overall accuracy. This
is due to the influence of the majority class on traditional training criteria.
In this paper, we aim to apply genetic programming to address this issue by
adapting existing fitness functions. We examine two fitness functions from
previous studies and develop two new fitness functions to evolve GP classifier
with superior accuracy on the minority class and overall. Two UCI credit card
datasets are used to evaluate the effectiveness of the proposed fitness
functions. The results demonstrate that the proposed fitness functions augment
GP classifiers, encouraging fitter solutions on both the minority and the
majority classes."
"Probabilistic generative neural networks are useful for many applications,
such as image classification, speech recognition and occlusion removal.
However, the power budget for hardware implementations of neural networks can
be extremely tight. To address this challenge we describe a design methodology
for using approximate computing methods to implement Approximate Deep Belief
Networks (ApproxDBNs) by systematically exploring the use of (1) limited
precision of variables; (2) criticality analysis to identify the nodes in the
network which can operate with such limited precision while allowing the
network to maintain target accuracy levels; and (3) a greedy search methodology
with incremental retraining to determine the optimal reduction in precision to
enable maximize power savings under user-specified accuracy constraints.
Experimental results show that significant bit-length reduction can be achieved
by our ApproxDBN with constrained accuracy loss."
"The primary aim of automated performance improvement is to reduce the running
time of programs while maintaining (or improving on) functionality. In this
paper, Genetic Programming is used to find performance improvements in regular
expressions for an array of target programs, representing the first application
of automated software improvement for run-time performance in the Regular
Expression language. This particular problem is interesting as there may be
many possible alternative regular expressions which perform the same task while
exhibiting subtle differences in performance. A benchmark suite of candidate
regular expressions is proposed for improvement. We show that the application
of Genetic Programming techniques can result in performance improvements in all
cases.
  As we start evolution from a known good regular expression, diversity is
critical in escaping the local optima of the seed expression. In order to
understand diversity during evolution we compare an initial population
consisting of only seed programs with a population initialised using a
combination of a single seed individual with individuals generated using PI
Grow and Ramped-half-and-half initialisation mechanisms."
"The $(1+(\lambda,\lambda))$ genetic algorithm, first proposed at GECCO 2013,
showed a surprisingly good performance on so me optimization problems. The
theoretical analysis so far was restricted to the OneMax test function, where
this GA profited from the perfect fitness-distance correlation. In this work,
we conduct a rigorous runtime analysis of this GA on random 3-SAT instances in
the planted solution model having at least logarithmic average degree, which
are known to have a weaker fitness distance correlation.
  We prove that this GA with fixed not too large population size again obtains
runtimes better than $\Theta(n \log n)$, which is a lower bound for most
evolutionary algorithms on pseudo-Boolean problems with unique optimum.
However, the self-adjusting version of the GA risks reaching population sizes
at which the intermediate selection of the GA, due to the weaker
fitness-distance correlation, is not able to distinguish a profitable offspring
from others. We show that this problem can be overcome by equipping the
self-adjusting GA with an upper limit for the population size. Apart from
sparse instances, this limit can be chosen in a way that the asymptotic
performance does not worsen compared to the idealistic OneMax case. Overall,
this work shows that the $(1+(\lambda,\lambda))$ GA can provably have a good
performance on combinatorial search and optimization problems also in the
presence of a weaker fitness-distance correlation."
"In this paper, we propose a Hybrid Ant Colony Optimization algorithm (HACO)
for Next Release Problem (NRP). NRP, a NP-hard problem in requirement
engineering, is to balance customer requests, resource constraints, and
requirement dependencies by requirement selection. Inspired by the successes of
Ant Colony Optimization algorithms (ACO) for solving NP-hard problems, we
design our HACO to approximately solve NRP. Similar to traditional ACO
algorithms, multiple artificial ants are employed to construct new solutions.
During the solution construction phase, both pheromone trails and neighborhood
information will be taken to determine the choices of every ant. In addition, a
local search (first found hill climbing) is incorporated into HACO to improve
the solution quality. Extensively wide experiments on typical NRP test
instances show that HACO outperforms the existing algorithms (GRASP and
simulated annealing) in terms of both solution uality and running time."
"We present the optimisation of a neuromorphic adaptation of a spiking neural
network model of the locust Lobula Giant Movement Detector (LGMD), which
detects looming objects and can be used to facilitate obstacle avoidance in
robotic applications. Due to the number of user-defined parameters and the size
of the search space, it is difficult to find values that detect looming events
and ignore translation. Additionally, evaluation of the objective function is
expensive (approximately one minute), the properties of the black box objective
function are unknown, and derivatives are not available. Therefore, we
investigate the use of Differential Evolution and self-adaptive DE (SADE) to
find optimal values. We demonstrate that these optimisation algorithms are
suitable candidates to find suitable parameters for an obstacle avoidance
system on an unmanned aerial vehicle (UAV)."
"A sport tournament problem is considered the Traveling Tournament Problem
(TTP). One interesting type is the mirrored Traveling Tournament Problem
(mTTP). The objective of the problem is to minimize either the total number of
traveling or the total distances of traveling or both. This research aims to
find an optimized solution of the mirrored Traveling Tournament Problem with
minimum total number of traveling. The solutions consisting of traveling and
scheduling tables are solved by using genetic algorithm (GA) with swapping
method. The number of traveling of all teams from obtained solutions are close
to the lower bound theory of number of traveling. Moreover, this algorithm
generates better solutions than known results for most cases."
"Symbolic regression via genetic programming is a flexible approach to machine
learning that does not require up-front specification of model structure.
However, traditional approaches to symbolic regression require the use of
protected operators, which can lead to perverse model characteristics and poor
generalisation. In this paper, we revisit interval arithmetic as one possible
solution to allow genetic programming to perform regression using unprotected
operators. Using standard benchmarks, we show that using interval arithmetic
within model evaluation does not prevent invalid solutions from entering the
population, meaning that search performance remains compromised. We extend the
basic interval arithmetic concept with `safe' search operators that integrate
interval information into their process, thereby greatly reducing the number of
invalid solutions produced during search. The resulting algorithms are able to
more effectively identify good models that generalise well to unseen data. We
conclude with an analysis of the sensitivity of interval arithmetic-based
operators with respect to the accuracy of the supplied input feature intervals."
"We propose a new type of leaf node for use in Symbolic Regression (SR) that
performs linear combinations of feature variables (LCF). These nodes can be
handled in three different modes -- an unsynchronized mode, where all LCFs are
free to change on their own, a synchronized mode, where LCFs are sorted into
groups in which they are forced to be identical throughout the whole
individual, and a globally synchronized mode, which is similar to the previous
mode but the grouping is done across the whole population. We also present two
methods of evolving the weights of the LCFs -- a purely stochastic way via
mutation and a gradient-based way based on the backpropagation algorithm known
from neural networks -- and also a combination of both. We experimentally
evaluate all configurations of LCFs in Multi-Gene Genetic Programming (MGGP),
which was chosen as baseline, on a number of benchmarks. According to the
results, we identified two configurations which increase the performance of the
algorithm."
"Natural evolution has produced a tremendous diversity of functional
organisms. Many believe an essential component of this process was the
evolution of evolvability, whereby evolution speeds up its ability to innovate
by generating a more adaptive pool of offspring. One hypothesized mechanism for
evolvability is developmental canalization, wherein certain dimensions of
variation become more likely to be traversed and others are prevented from
being explored (e.g. offspring tend to have similarly sized legs, and mutations
affect the length of both legs, not each leg individually). While ubiquitous in
nature, canalization almost never evolves in computational simulations of
evolution. Not only does that deprive us of in silico models in which to study
the evolution of evolvability, but it also raises the question of which
conditions give rise to this form of evolvability. Answering this question
would shed light on why such evolvability emerged naturally and could
accelerate engineering efforts to harness evolution to solve important
engineering challenges. In this paper we reveal a unique system in which
canalization did emerge in computational evolution. We document that genomes
entrench certain dimensions of variation that were frequently explored during
their evolutionary history. The genetic representation of these organisms also
evolved to be highly modular and hierarchical, and we show that these
organizational properties correlate with increased fitness. Interestingly, the
type of computational evolutionary experiment that produced this evolvability
was very different from traditional digital evolution in that there was no
objective, suggesting that open-ended, divergent evolutionary processes may be
necessary for the evolution of evolvability."
"Optimization techniques play an important role in several scientific and
real-world applications, thus becoming of great interest for the community. As
a consequence, a number of open-source libraries are available in the
literature, which ends up fostering the research and development of new
techniques and applications. In this work, we present a new library for the
implementation and fast prototyping of nature-inspired techniques called
LibOPT. Currently, the library implements 15 techniques and 112 benchmarking
functions, as well as it also supports 11 hypercomplex-based optimization
approaches, which makes it one of the first of its kind. We showed how one can
easily use and also implement new techniques in LibOPT under the C paradigm.
Examples are provided with samples of source-code using benchmarking functions."
"Behavior domination is proposed as a tool for understanding and harnessing
the power of evolutionary systems to discover and exploit useful stepping
stones. Novelty search has shown promise in overcoming deception by collecting
diverse stepping stones, and several algorithms have been proposed that combine
novelty with a more traditional fitness measure to refocus search and help
novelty search scale to more complex domains. However, combinations of novelty
and fitness do not necessarily preserve the stepping stone discovery that
novelty search affords. In several existing methods, competition between
solutions can lead to an unintended loss of diversity. Behavior domination
defines a class of algorithms that avoid this problem, while inheriting
theoretical guarantees from multiobjective optimization. Several existing
algorithms are shown to be in this class, and a new algorithm is introduced
based on fast non-dominated sorting. Experimental results show that this
algorithm outperforms existing approaches in domains that contain useful
stepping stones, and its advantage is sustained with scale. The conclusion is
that behavior domination can help illuminate the complex dynamics of
behavior-driven search, and can thus lead to the design of more scalable and
robust algorithms."
"Genetic Algorithms are widely used in many different optimization problems
including layout design. The layout of the shelves play an important role in
the total sales metrics for superstores since this affects the customers'
shopping behaviour. This paper employed a genetic algorithm based approach to
design shelf layout of superstores. The layout design problem was tackled by
using a novel chromosome representation which takes many different parameters
to prevent dead-ends and improve shelf visibility into consideration. Results
show that the approach can produce reasonably good layout designs in very short
amounts of time."
"We propose sensorimotor tappings, a new graphical technique that explicitly
represents relations between the time steps of an agent's sensorimotor loop and
a single training step of an adaptive model that the agent is using internally.
In the simplest case this is a relation linking two time steps. In realistic
cases these relations can extend over several time steps and over different
sensory channels. The aim is to capture the footprint of information intake
relative to the agent's current time step. We argue that this view allows us to
make prior considerations explicit and then use them in implementations without
modification once they are established. In the paper we introduce the problem
domain, explain the basic idea, provide example tappings for standard
configurations used in developmental models, and show how tappings can be
applied to problems in related fields."
"The evolutionary edit distance between two individuals in a population, i.e.,
the amount of applications of any genetic operator it would take the
evolutionary process to generate one individual starting from the other, seems
like a promising estimate for the diversity between said individuals. We
introduce genealogical diversity, i.e., estimating two individuals' degree of
relatedness by analyzing large, unused parts of their genome, as a
computationally efficient method to approximate that measure for diversity."
"Rapid development of evolutionary algorithms in handling many-objective
optimization problems requires viable methods of visualizing a high-dimensional
solution set. Parallel coordinates which scale well to high-dimensional data
are such a method, and have been frequently used in evolutionary many-objective
optimization. However, the parallel coordinates plot is not as straightforward
as the classic scatter plot to present the information contained in a solution
set. In this paper, we make some observations of the parallel coordinates plot,
in terms of comparing the quality of solution sets, understanding the shape and
distribution of a solution set, and reflecting the relation between objectives.
We hope that these observations could provide some guidelines as to the proper
use of parallel coordinates in evolutionary many-objective optimization."
"We consider approximations of 1D Lipschitz functions by deep ReLU networks of
a fixed width. We prove that without the assumption of continuous weight
selection the uniform approximation error is lower than with this assumption at
least by a factor logarithmic in the size of the network."
"The dynamic problem of enclosing an expanding fire can be modelled by a
discrete variant in a grid graph. While the fire expands to all neighbouring
cells in any time step, the fire fighter is allowed to block $c$ cells in the
average outside the fire in the same time interval. It was shown that the
success of the fire fighter is guaranteed for $c>1.5$ but no strategy can
enclose the fire for $c\leq 1.5$. For achieving such a critical threshold the
correctness (sometimes even optimality) of strategies and lower bounds have
been shown by integer programming or by direct but often very sophisticated
arguments. We investigate the problem whether it is possible to find or to
approach such a threshold and/or optimal strategies by means of evolutionary
algorithms, i.e., we just try to learn successful strategies for different
constants $c$ and have a look at the outcome. The main general idea is that
this approach might give some insight in the power of evolutionary strategies
for similar geometrically motivated threshold questions. We investigate the
variant of protecting a highway with still unknown threshold and found
interesting strategic paradigms.
  Keywords: Dynamic environments, fire fighting, evolutionary strategies,
threshold approximation"
"In this work, we have proposed a revolutionary neuromorphic computing
methodology to implement All-Skyrmion Spiking Neural Network (AS-SNN). Such
proposed methodology is based on our finding that skyrmion is a topological
stable spin texture and its spatiotemporal motion along the magnetic nano-track
intuitively interprets the pulse signal transmission between two interconnected
neurons. In such design, spike train in SNN could be encoded as particle-like
skyrmion train and further processed by the proposed skyrmion-synapse and
skyrmion-neuron within the same magnetic nano-track to generate output skyrmion
as post-spike. Then, both pre-neuron spikes and post-neuron spikes are encoded
as particle-like skyrmions without conversion between charge and spin signals,
which fundamentally differentiates our proposed design from other hybrid
Spin-CMOS designs. The system level simulation shows 87.1% inference accuracy
for handwritten digit recognition task, while the energy dissipation is ~1
fJ/per spike which is 3 orders smaller in comparison with CMOS based IBM
TrueNorth system."
"To predict the final result of an athlete in a marathon run thoroughly is the
eternal desire of each trainer. Usually, the achieved result is weaker than the
predicted one due to the objective (e.g., environmental conditions) as well as
subjective factors (e.g., athlete's malaise). Therefore, making up for the
deficit between predicted and achieved results is the main ingredient of the
analysis performed by trainers after the competition. In the analysis, they
search for parts of a marathon course where the athlete lost time. This paper
proposes an automatic making up for the deficit by using a Differential
Evolution algorithm. In this case study, the results that were obtained by a
wearable sports-watch by an athlete in a real marathon are analyzed. The first
experiments with Differential Evolution show the possibility of using this
method in the future."
"While Deep Neural Networks (DNNs) push the state-of-the-art in many machine
learning applications, they often require millions of expensive floating-point
operations for each input classification. This computation overhead limits the
applicability of DNNs to low-power, embedded platforms and incurs high cost in
data centers. This motivates recent interests in designing low-power,
low-latency DNNs based on fixed-point, ternary, or even binary data precision.
While recent works in this area offer promising results, they often lead to
large accuracy drops when compared to the floating-point networks. We propose a
novel approach to map floating-point based DNNs to 8-bit dynamic fixed-point
networks with integer power-of-two weights with no change in network
architecture. Our dynamic fixed-point DNNs allow different radix points between
layers. During inference, power-of-two weights allow multiplications to be
replaced with arithmetic shifts, while the 8-bit fixed-point representation
simplifies both the buffer and adder design. In addition, we propose a hardware
accelerator design to achieve low-power, low-latency inference with
insignificant degradation in accuracy. Using our custom accelerator design with
the CIFAR-10 and ImageNet datasets, we show that our method achieves
significant power and energy savings while increasing the classification
accuracy."
"The key component in forecasting demand and consumption of resources in a
supply network is an accurate prediction of real-valued time series. Indeed,
both service interruptions and resource waste can be reduced with the
implementation of an effective forecasting system. Significant research has
thus been devoted to the design and development of methodologies for short term
load forecasting over the past decades. A class of mathematical models, called
Recurrent Neural Networks, are nowadays gaining renewed interest among
researchers and they are replacing many practical implementation of the
forecasting systems, previously based on static methods. Despite the undeniable
expressive power of these architectures, their recurrent nature complicates
their understanding and poses challenges in the training procedures. Recently,
new important families of recurrent architectures have emerged and their
applicability in the context of load forecasting has not been investigated
completely yet. In this paper we perform a comparative study on the problem of
Short-Term Load Forecast, by using different classes of state-of-the-art
Recurrent Neural Networks. We test the reviewed models first on controlled
synthetic tasks and then on different real datasets, covering important
practical cases of study. We provide a general overview of the most important
architectures and we define guidelines for configuring the recurrent networks
to predict real-valued time series."
"The Building Block Hypothesis (BBH) states that adaptive systems combine good
partial solutions (so-called building blocks) to find increasingly better
solutions. It is thought that Genetic Algorithms (GAs) implement the BBH.
However, for GAs building blocks are semi-theoretical objects in that they are
thought only to be implicitly exploited via the selection and crossover
operations of a GA. In the current work, we discover a mathematical method to
identify the complete set of schemata present in a given population of a GA; as
such a natural way to study schema processing (and thus the BBH) is revealed.
We demonstrate how this approach can be used both theoretically and
experimentally. Theoretically, we show that the search space for good schemata
is a complete lattice and that each generation samples a complete sub-lattice
of this search space. In addition, we show that combining schemata can only
explore a subset of the search space. Experimentally, we compare how well
different crossover methods combine building blocks. We find that for most
crossover methods approximately 25-35% of building blocks in a generation
result from the combination of the previous generation's building blocks. We
also find that an increase in the combination of building blocks does not lead
to an increase in the efficiency of a GA. To complement this article, we
introduce an open source Python package called schematax, which allows one to
calculate the schemata present in a population using the methods described in
this article."
"Machine learning algorithms are inherently multiobjective in nature, where
approximation error minimization and model's complexity simplification are two
conflicting objectives. We proposed a multiobjective genetic programming (MOGP)
for creating a heterogeneous flexible neural tree (HFNT), tree-like flexible
feedforward neural network model. The functional heterogeneity in neural tree
nodes was introduced to capture a better insight of data during learning
because each input in a dataset possess different features. MOGP guided an
initial HFNT population towards Pareto-optimal solutions, where the final
population was used for making an ensemble system. A diversity index measure
along with approximation error and complexity was introduced to maintain
diversity among the candidates in the population. Hence, the ensemble was
created by using accurate, structurally simple, and diverse candidates from
MOGP final population. Differential evolution algorithm was applied to
fine-tune the underlying parameters of the selected candidates. A comprehensive
test over classification, regression, and time-series datasets proved the
efficiency of the proposed algorithm over other available prediction methods.
Moreover, the heterogeneous creation of HFNT proved to be efficient in making
ensemble system from the final population."
"Neuromorphic computing has come to refer to a variety of brain-inspired
computers, devices, and models that contrast the pervasive von Neumann computer
architecture. This biologically inspired approach has created highly connected
synthetic neurons and synapses that can be used to model neuroscience theories
as well as solve challenging machine learning problems. The promise of the
technology is to create a brain-like ability to learn and adapt, but the
technical challenges are significant, starting with an accurate neuroscience
model of how the brain works, to finding materials and engineering
breakthroughs to build devices to support these models, to creating a
programming framework so the systems can learn, to creating applications with
brain-like capabilities. In this work, we provide a comprehensive survey of the
research and motivations for neuromorphic computing over its history. We begin
with a 35-year review of the motivations and drivers of neuromorphic computing,
then look at the major research areas of the field, which we define as
neuro-inspired models, algorithms and learning approaches, hardware and
devices, supporting systems, and finally applications. We conclude with a broad
discussion on the major research topics that need to be addressed in the coming
years to see the promise of neuromorphic computing fulfilled. The goals of this
work are to provide an exhaustive review of the research conducted in
neuromorphic computing since the inception of the term, and to motivate further
work by illuminating gaps in the field where new research is needed."
"Generality is one of the main advantages of heuristic algorithms, as such,
multiple parameters are exposed to the user with the objective of allowing them
to shape the algorithms to their specific needs. Parameter selection,
therefore, becomes an intrinsic problem of every heuristic algorithm. Selecting
good parameter values relies not only on knowledge related to the problem at
hand, but to the algorithms themselves. This research explores the usage of
self-organized criticality to reduce user interaction in the process of
selecting suitable parameters for particle swarm optimization (PSO) heuristics.
A particle swarm variant (named Adaptive PSO) with self-organized criticality
is developed and benchmarked against the standard PSO. Criticality is observed
in the dynamic behaviour of this swarm and excellent results are observed in
the long run. In contrast with the standard PSO, the Adaptive PSO does not
stagnate at any point in time, balancing the concepts of exploration and
exploitation better. A software platform for experimenting with particle
swarms, called PSO Laboratory, is also developed. This software is used to test
the standard PSO as well as all other PSO variants developed in the process of
creating the Adaptive PSO. As the software is intended to be of aid to future
and related research, special attention has been put in the development of a
friendly graphical user interface. Particle swarms are executed in real time,
allowing users to experiment by changing parameters on-the-fly."
"A long-term goal of AI is to produce agents that can learn a diversity of
skills throughout their lifetimes and continuously improve those skills via
experience. A longstanding obstacle towards that goal is catastrophic
forgetting, which is when learning new information erases previously learned
information. Catastrophic forgetting occurs in artificial neural networks
(ANNs), which have fueled most recent advances in AI. A recent paper proposed
that catastrophic forgetting in ANNs can be reduced by promoting modularity,
which can limit forgetting by isolating task information to specific clusters
of nodes and connections (functional modules). While the prior work did show
that modular ANNs suffered less from catastrophic forgetting, it was not able
to produce ANNs that possessed task-specific functional modules, thereby
leaving the main theory regarding modularity and forgetting untested. We
introduce diffusion-based neuromodulation, which simulates the release of
diffusing, neuromodulatory chemicals within an ANN that can modulate (i.e. up
or down regulate) learning in a spatial region. On the simple diagnostic
problem from the prior work, diffusion-based neuromodulation 1) induces
task-specific learning in groups of nodes and connections (task-specific
localized learning), which 2) produces functional modules for each subtask, and
3) yields higher performance by eliminating catastrophic forgetting. Overall,
our results suggest that diffusion-based neuromodulation promotes task-specific
localized learning and functional modularity, which can help solve the
challenging, but important problem of catastrophic forgetting."
"Three approaches to implement genetic programming on GPU hardware are
compilation, interpretation and direct generation of machine code. The compiled
approach is known to have a prohibitive overhead compared to other two. This
paper investigates methods to accelerate compilation of individuals for genetic
programming on GPU hardware. We apply in-process compilation to minimize the
compilation overhead at each generation; and we investigate ways to parallelize
in-process compilation. In-process compilation doesn't lend itself to trivial
parallelization with threads; we propose a multiprocess parallelization using
memory sharing and operating systems interprocess communication primitives.
With parallelized compilation we achieve further reductions on compilation
overhead. Another contribution of this work is the code framework we built in
C# for the experiments. The framework makes it possible to build arbitrary
grammatical genetic programming experiments that run on GPU with minimal extra
coding effort, and is available as open source."
"Symbolic regression that aims to detect underlying data-driven model has
become increasingly important for industrial data analysis. For most of
existing algorithms, such as genetic programming (GP), the convergence speed
might be too slow for large scale problems with a large number of variables.
This situation may become even worse with increasing problem size. The
aforementioned difficulty makes symbolic regression limited in practical
applications. Fortunately, in many engineering problems, the independent
variables in target models are separable or partially separable. This feature
inspires us to develop a new approach, block building programming (BBP), in
this paper. BBP divides the original target function into several blocks, and
further into factors. The factors are then optimized by an optimization engine
(e.g., GP). Under such circumstance, BBP can make large reductions to the
search space. The partition of separability is based on a special method, block
and factor detection. Two different optimization engines are applied to test
the performance of BBP on a set of symbolic regression problems. Numerical
results show that BBP has a good capability of `structure and coefficient
optimization' with high computational efficiency."
"Symbolic regression aims to find a function that best explains the
relationship between independent variables and the objective value based on a
given set of sample data. Genetic programming (GP) is usually considered as an
appropriate method for the problem since it can optimize functional structure
and coefficients simultaneously. However, the convergence speed of GP might be
too slow for large scale problems that involve a large number of variables.
Fortunately, in many applications, the target function is separable or
partially separable. This feature motivated us to develop a new method, divide
and conquer (D&C), for symbolic regression, in which the target function is
divided into a number of sub-functions and the sub-functions are then
determined by any of a GP algorithm. The separability is probed by a new
proposed technique, Bi-Correlation test (BiCT). D&C powered GP has been tested
on some real-world applications, and the study shows that D&C can help GP to
get the target function much more rapidly."
"Processing sequential data of variable length is a major challenge in a wide
range of applications, such as speech recognition, language modeling,
generative image modeling and machine translation. Here, we address this
challenge by proposing a novel recurrent neural network (RNN) architecture, the
Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both
multiscale RNNs and deep transition RNNs as it processes sequential data on
different timescales and learns complex transition functions from one time step
to the next. We evaluate the FS-RNN on two character level language modeling
data sets, Penn Treebank and Hutter Prize Wikipedia, where we improve state of
the art results to $1.19$ and $1.25$ bits-per-character (BPC), respectively. In
addition, an ensemble of two FS-RNNs achieves $1.20$ BPC on Hutter Prize
Wikipedia outperforming the best known compression algorithm with respect to
the BPC measure. We also present an empirical investigation of the learning and
network dynamics of the FS-RNN, which explains the improved performance
compared to other RNN architectures. Our approach is general as any kind of RNN
cell is a possible building block for the FS-RNN architecture, and thus can be
flexibly applied to different tasks."
"We present a neural network technique for the analysis and extrapolation of
time-series data called Neural Decomposition (ND). Units with a sinusoidal
activation function are used to perform a Fourier-like decomposition of
training samples into a sum of sinusoids, augmented by units with nonperiodic
activation functions to capture linear trends and other nonperiodic components.
We show how careful weight initialization can be combined with regularization
to form a simple model that generalizes well. Our method generalizes
effectively on the Mackey-Glass series, a dataset of unemployment rates as
reported by the U.S. Department of Labor Statistics, a time-series of monthly
international airline passengers, the monthly ozone concentration in downtown
Los Angeles, and an unevenly sampled time-series of oxygen isotope measurements
from a cave in north India. We find that ND outperforms popular time-series
forecasting techniques including LSTM, echo state networks, ARIMA, SARIMA, SVR
with a radial basis function, and Gashler and Ashmore's model."
"We propose an integer approximation of Echo State Networks (ESN) based on the
mathematics of hyperdimensional computing. The reservoir of the proposed
Integer Echo State Network (intESN) contains only n-bits integers and replaces
the recurrent matrix multiply with an efficient cyclic shift operation. Such an
architecture results in dramatic improvements in memory footprint and
computational efficiency, with minimal performance loss. Our architecture
naturally supports the usage of the trained reservoir in symbolic processing
tasks of analogy making and logical inference."
"Echo state networks represent a special type of recurrent neural networks.
Recent papers stated that the echo state networks maximize their computational
performance on the transition between order and chaos, the so-called edge of
chaos. This work confirms this statement in a comprehensive set of experiments.
Furthermore, the echo state networks are compared to networks evolved via
neuroevolution. The evolved networks outperform the echo state networks,
however, the evolution consumes significant computational resources. It is
demonstrated that echo state networks with local connections combine the best
of both worlds, the simplicity of random echo state networks and the
performance of evolved networks. Finally, it is shown that evolution tends to
stay close to the ordered side of the edge of chaos."
"Inspired by the notion of surprise for unconventional discovery we introduce
a general search algorithm we name surprise search as a new method of
evolutionary divergent search. Surprise search is grounded in the divergent
search paradigm and is fabricated within the principles of evolutionary search.
The algorithm mimics the self-surprise cognitive process and equips
evolutionary search with the ability to seek for solutions that deviate from
the algorithm's expected behaviour. The predictive model of expected solutions
is based on historical trails of where the search has been and local
information about the search space. Surprise search is tested extensively in a
robot maze navigation task: experiments are held in four authored deceptive
mazes and in 60 generated mazes and compared against objective-based
evolutionary search and novelty search. The key findings of this study reveal
that surprise search is advantageous compared to the other two search
processes. In particular, it outperforms objective search and it is as
efficient as novelty search in all tasks examined. Most importantly, surprise
search is faster, on average, and more robust in solving the navigation problem
compared to any other algorithm examined. Finally, our analysis reveals that
surprise search explores the behavioural space more extensively and yields
higher population diversity compared to novelty search. What distinguishes
surprise search from other forms of divergent search, such as the search for
novelty, is its ability to diverge not from earlier and seen solutions but
rather from predicted and unseen points in the domain considered."
"In this report, we suggest nine test problems for multi-task multi-objective
optimization (MTMOO), each of which consists of two multiobjective optimization
tasks that need to be solved simultaneously. The relationship between tasks
varies between different test problems, which would be helpful to have a
comprehensive evaluation of the MO-MFO algorithms. It is expected that the
proposed test problems will germinate progress the field of the MTMOO research."
"We establish global convergence of the (1+1)-ES algorithm, i.e., convergence
to a critical point independent of the initial state.
  The analysis is based on two ingredients. We establish a sufficient decrease
condition for elitist, rank-based evolutionary algorithms, formulated for an
essentially monotonically transformed variant of the objective function. This
tool is of general value, and it is therefore formulated for general search
spaces. To make it applicable to the (1+1)-ES, we show that the algorithm state
is found infinitely often in a regime where step size and success rate are
simultaneously bounded away from zero, with full probability.
  The main result is proven by combining both statements. Under minimal
technical preconditions, the theorem ensures that the sequence of iterates has
a limit point that cannot be improved in the limit of vanishing step size, a
generalization of the notion of critical points of smooth functions.
Importantly, our analysis reflects the actual dynamics of the algorithm and
hence supports our understanding of its mechanisms, in particular success-based
step size control.
  We apply the theorem to the analysis of the optimization behavior of the
(1+1)-ES on various problems ranging from the smooth (non-convex) cases over
different types of saddle points and ridge functions to discontinuous and
extremely rugged problems."
"In this report, we suggest nine test problems for multi-task single-objective
optimization (MTSOO), each of which consists of two single-objective
optimization tasks that need to be solved simultaneously. The relationship
between tasks varies between different test problems, which would be helpful to
have a comprehensive evaluation of the MFO algorithms. It is expected that the
proposed test problems will germinate progress the field of the MTSOO research."
"In this paper, we propose an improved gravitational search algorithm named
GSABC. The algorithm improves gravitational search algorithm (GSA) results
improved by using artificial bee colony algorithm (ABC) to solve constrained
numerical optimization problems. In GSA, solutions are attracted towards each
other by applying gravitational forces, which depending on the masses assigned
to the solutions, to each other. The heaviest mass will move slower than other
masses and gravitate others. Due to nature of gravitation, GSA may pass global
minimum if some solutions stuck to local minimum. ABC updates the positions of
the best solutions that has obtained from GSA, preventing the GSA from sticking
to the local minimum by its strong searching ability. The proposed algorithm
improves the performance of GSA. The proposed method tested on 23 well-known
unimodal, multimodal and fixed-point multimodal benchmark test functions.
Experimental results show that GSABC outperforms or performs similarly to five
state-of-the-art optimization approaches."
"We extended the work of proposed activation function, Noisy Softplus, to fit
into training of layered up spiking neural networks (SNNs). Thus, any ANN
employing Noisy Softplus neurons, even of deep architecture, can be trained
simply by the traditional algorithm, for example Back Propagation (BP), and the
trained weights can be directly used in the spiking version of the same network
without any conversion. Furthermore, the training method can be generalised to
other activation units, for instance Rectified Linear Units (ReLU), to train
deep SNNs off-line. This research is crucial to provide an effective approach
for SNN training, and to increase the classification accuracy of SNNs with
biological characteristics and to close the gap between the performance of SNNs
and ANNs."
"The practice of evolutionary algorithms involves a mundane yet inescapable
phase, namely, finding parameters that work well. How big should the population
be? How many generations should the algorithm run? What is the (tournament
selection) tournament size? What probabilities should one assign to crossover
and mutation? All these nagging questions need good answers if one is to
embrace success. Through an extensive series of experiments over multiple
evolutionary algorithm implementations and problems we show that parameter
space tends to be rife with viable parameters. We aver that this renders the
life of the practitioner that much easier, and cap off our study with an
advisory digest for the weary."
"The motor control problem involves determining the time-varying muscle
activation trajectories required to accomplish a given movement. Muscle
redundancy makes motor control a challenging task: there are many possible
activation trajectories that accomplish the same movement. Despite this
redundancy, most movements are accomplished in highly stereotypical ways. For
example, point-to-point reaching movements are almost universally performed
with very similar smooth trajectories. Optimization methods are commonly used
to predict muscle forces for measured movements. However, these approaches
require computationally expensive simulations and are sensitive to the chosen
optimality criteria and regularization. In this work, we investigate deep
autoencoders for the prediction of muscle activation trajectories for
point-to-point reaching movements. We evaluate our DNN predictions with
simulated reaches and two methods to generate the muscle activations: inverse
dynamics (ID) and optimal control (OC) criteria. We also investigate optimal
network parameters and training criteria to improve the accuracy of the
predictions."
"The vast majority of natural sensory data is temporally redundant. Video
frames or audio samples which are sampled at nearby points in time tend to have
similar values. Typically, deep learning algorithms take no advantage of this
redundancy to reduce computation. This can be an obscene waste of energy. We
present a variant on backpropagation for neural networks in which computation
scales with the rate of change of the data - not the rate at which we process
the data. We do this by having neurons communicate a combination of their
state, and their temporal change in state. Intriguingly, this simple
communication rule give rise to units that resemble biologically-inspired leaky
integrate-and-fire neurons, and to a weight-update rule that is equivalent to a
form of Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule
observed in the brain. We demonstrate that on MNIST and a temporal variant of
MNIST, our algorithm performs about as well as a Multilayer Perceptron trained
with backpropagation, despite only communicating discrete values between
layers."
"- The primary author has withdrawn this paper due to conflict of interest -
We present MATIC (Memory-Adaptive Training and In-situ Canaries), a voltage
scaling methodology that addresses the SRAM efficiency bottleneck in DNN
accelerators. To overscale "
"A neural network-based chart pattern represents adaptive parametric features,
including non-linear transformations, and a template that can be applied in the
feature space. The search of neural network-based chart patterns has been
unexplored despite its potential expressiveness. In this paper, we formulate a
general chart pattern search problem to enable cross-representational
quantitative comparison of various search schemes. We suggest a HyperNEAT
framework applying state-of-the-art deep neural network techniques to find
attractive neural network-based chart patterns; These techniques enable a fast
evaluation and search of robust patterns, as well as bringing a performance
gain. The proposed framework successfully found attractive patterns on the
Korean stock market. We compared newly found patterns with those found by
different search schemes, showing the proposed approach has potential."
"We propose a new genetic algorithm with optimal recombination for the
asymmetric instances of travelling salesman problem. The algorithm incorporates
several new features that contribute to its effectiveness: (i) Optimal
recombination problem is solved within crossover operator. (ii) A new mutation
operator performs a random jump within 3-opt or 4-opt neighborhood. (iii)
Greedy constructive heuristic of W.Zhang and 3-opt local search heuristic are
used to generate the initial population. A computational experiment on TSPLIB
instances shows that the proposed algorithm yields competitive results to other
well-known memetic algorithms for asymmetric travelling salesman problem."
"We consider the problem of power demand forecasting in residential
micro-grids. Several approaches using ARMA models, support vector machines, and
recurrent neural networks that perform one-step ahead predictions have been
proposed in the literature. Here, we extend them to perform multi-step ahead
forecasting and we compare their performance. Toward this end, we implement a
parallel and efficient training framework, using power demand traces from real
deployments to gauge the accuracy of the considered techniques. Our results
indicate that machine learning schemes achieve smaller prediction errors in the
mean and the variance with respect to ARMA, but there is no clear algorithm of
choice among them. Pros and cons of these approaches are discussed and the
solution of choice is found to depend on the specific use case requirements. A
hybrid approach, that is driven by the prediction interval, the target error,
and its uncertainty, is then recommended."
"Genetic algorithms (GAs) are an optimization technique that has been
successfully used on many real-world problems. There exist different approaches
to their theoretical study. In this paper we complete a recently presented
approach to model one-point crossover using pretopologies (or Cech topologies)
in two ways. First, we extend it to the case of n-points crossover. Then, we
experimentally study how the distance distribution changes when the number of
crossover points increases."
"In this paper, we use recurrent autoencoder model to predict the time series
in single and multiple steps ahead. Previous prediction methods, such as
recurrent neural network (RNN) and deep belief network (DBN) models, cannot
learn long term dependencies. And conventional long short-term memory (LSTM)
model doesn't remember recent inputs. Combining LSTM and autoencoder (AE), the
proposed model can capture long-term dependencies across data points and uses
features extracted from recent observations for augmenting LSTM at the same
time. Based on comprehensive experiments, we show that the proposed methods
significantly improves the state-of-art performance on chaotic time series
benchmark and also has better performance on real-world data. Both
single-output and multiple-output predictions are investigated."
"Modeling preference time in triathlons means predicting the intermediate
times of particular sports disciplines by a given overall finish time in a
specific triathlon course for the athlete with the known personal best result.
This is a hard task for athletes and sport trainers due to a lot of different
factors that need to be taken into account, e.g., athlete's abilities, health,
mental preparations and even their current sports form. So far, this process
was calculated manually without any specific software tools or using the
artificial intelligence. This paper presents the new solution for modeling
preference time in middle distance triathlons based on particle swarm
optimization algorithm and archive of existing sports results. Initial results
are presented, which suggest the usefulness of proposed approach, while remarks
for future improvements and use are also emphasized."
"A popular testbed for deep learning has been multimodal recognition of human
activity or gesture involving diverse inputs such as video, audio, skeletal
pose and depth images. Deep learning architectures have excelled on such
problems due to their ability to combine modality representations at different
levels of nonlinear feature extraction. However, designing an optimal
architecture in which to fuse such learned representations has largely been a
non-trivial human engineering effort. We treat fusion structure optimization as
a hyper-parameter search and cast it as a discrete optimization problem under
the Bayesian optimization framework. We propose a novel graph-induced kernel to
compute structural similarities in the search space of tree-structured
multimodal architectures and demonstrate its effectiveness using two
challenging multimodal human activity recognition datasets."
"This study presents a new strategy for the identification of material
parameters in the case of restricted or redundant data, based on a hybrid
approach combining a genetic algorithm and the Levenberg-Marquardt method. The
proposed methodology consists essentially in a statistically based topological
analysis of the search domain, after this one has been reduced by the analysis
of the parameters ranges. This is used to identify the parameters of a model
representing the behavior of damaged elastic, visco-elastic, plastic and
visco-plastic composite laminates. Optimization of the experimental tests on
tubular samples leads to the selective identification of these parameters."
"Noise is a consequence of acquiring and pre-processing data from the
environment, and shows fluctuations from different sources---e.g., from
sensors, signal processing technology or even human error. As a machine
learning technique, Genetic Programming (GP) is not immune to this problem,
which the field has frequently addressed. Recently, Geometric Semantic Genetic
Programming (GSGP), a semantic-aware branch of GP, has shown robustness and
high generalization capability. Researchers believe these characteristics may
be associated with a lower sensibility to noisy data. However, there is no
systematic study on this matter. This paper performs a deep analysis of the
GSGP performance over the presence of noise. Using 15 synthetic datasets where
noise can be controlled, we added different ratios of noise to the data and
compared the results obtained with those of a canonical GP. The results show
that, as we increase the percentage of noisy instances, the generalization
performance degradation is more pronounced in GSGP than GP. However, in
general, GSGP is more robust to noise than GP in the presence of up to 10% of
noise, and presents no statistical difference for values higher than that in
the test bed."
"To understand cognitive reasoning in the brain, it has been proposed that
symbols and compositions of symbols are represented by activity patterns
(vectors) in a large population of neurons. Formal models implementing this
idea [Plate 2003], [Kanerva 2009], [Gayler 2003], [Eliasmith 2012] include a
reversible superposition operation for representing with a single vector an
entire set of symbols or an ordered sequence of symbols. If the representation
space is high-dimensional, large sets of symbols can be superposed and
individually retrieved. However, crosstalk noise limits the accuracy of
retrieval and information capacity. To understand information processing in the
brain and to design artificial neural systems for cognitive reasoning, a theory
of this superposition operation is essential. Here, such a theory is presented.
The superposition operations in different existing models are mapped to linear
neural networks with unitary recurrent matrices, in which retrieval accuracy
can be analyzed by a single equation. We show that networks representing
information in superposition can achieve a channel capacity of about half a bit
per neuron, a significant fraction of the total available entropy. Going beyond
existing models, superposition operations with recency effects are proposed
that avoid catastrophic forgetting when representing the history of infinite
data streams. These novel models correspond to recurrent networks with
non-unitary matrices or with nonlinear neurons, and can be analyzed and
optimized with an extension of our theory."
"Recognizing seismic waves immediately is very important for the realization
of efficient disaster prevention. Generally these systems consist of a network
of seismic detectors that send real time data to a central server. The server
elaborates the data and attempts to recognize the first signs of an earthquake.
The current problem with this approach is that it is subject to false alarms. A
critical trade-off exists between sensitivity of the system and error rate. To
overcame this problems, an artificial neural network based intelligent learning
systems can be used. However, conventional supervised ANN systems are difficult
to train, CPU intensive and prone to false alarms. To surpass these problems,
here we attempt to use a next-generation unsupervised cortical algorithm HTM.
This novel approach does not learn particular waveforms, but adapts to
continuously fed data reaching the ability to discriminate between normality
(seismic sensor background noise in no-earthquake conditions) and anomaly
(sensor response to a jitter or an earthquake). Main goal of this study is test
the ability of the HTM algorithm to be used to signal earthquakes automatically
in a feasible disaster prevention system. We describe the methodology used and
give the first qualitative assessments of the recognition ability of the
system. Our preliminary results show that the cortical algorithm used is very
robust to noise and that can successfully recognize synthetic earthquake-like
signals efficiently and reliably."
"Optimization of neural network (NN) significantly influenced by the transfer
function used in its active nodes. It has been observed that the homogeneity in
the activation nodes does not provide the best solution. Therefore, the
customizable transfer functions whose underlying parameters are subjected to
optimization were used to provide heterogeneity to NN. For the experimental
purpose, a meta-heuristic framework using a combined genotype representation of
connection weights and transfer function parameter was used. The performance of
adaptive Logistic, Tangent-hyperbolic, Gaussian and Beta functions were
analyzed. In present research work, concise comparisons between different
transfer function and between the NN optimization algorithms are presented. The
comprehensive analysis of the results obtained over the benchmark dataset
suggests that the Artificial Bee Colony with adaptive transfer function
provides the best results in terms of classification accuracy over the particle
swarm optimization and differential evolution."
"The performance of the meta-heuristic algorithms often depends on their
parameter settings. Appropriate tuning of the underlying parameters can
drastically improve the performance of a meta-heuristic. The Ant Colony
Optimization (ACO), a population based meta-heuristic algorithm inspired by the
foraging behavior of the ants, is no different. Fundamentally, the ACO depends
on the construction of new solutions, variable by variable basis using Gaussian
sampling of the selected variables from an archive of solutions. A
comprehensive performance analysis of the underlying parameters such as:
selection strategy, distance measure metric and pheromone evaporation rate of
the ACO suggests that the Roulette Wheel Selection strategy enhances the
performance of the ACO due to its ability to provide non-uniformity and
adequate diversity in the selection of a solution. On the other hand, the
Squared Euclidean distance-measure metric offers better performance than other
distance-measure metrics. It is observed from the analysis that the ACO is
sensitive towards the evaporation rate. Experimental analysis between classical
ACO and other meta-heuristic suggested that the performance of the well-tuned
ACO surpasses its counterparts."
"Human fatalities are reported due to the excessive proportional presence of
hazardous gas components in the manhole, such as Hydrogen Sulfide, Ammonia,
Methane, Carbon Dioxide, Nitrogen Oxide, Carbon Monoxide, etc. Hence,
predetermination of these gases is imperative. A neural network (NN) based
intelligent sensory system is proposed for the avoidance of such fatalities.
Backpropagation (BP) was applied for the supervised training of the neural
network. A Gas sensor array consists of many sensor elements was employed for
the sensing manhole gases. Sensors in the sensor array are responsible for
sensing their target gas components only. Therefore, the presence of multiple
gases results in cross sensitivity. The cross sensitivity is a crucial issue to
this problem and it is viewed as pattern recognition and noise reduction
problem. Various performance parameters and complexity of the problem
influences NN training. In present chapter the performance of BP algorithm on
such a real life application problem was comprehensively studied, compared and
contrasted with the several other hybrid intelligent approaches both, in
theoretical and in the statistical sense."
"When applying optimization method to a real-world problem, the possession of
prior knowledge and preliminary analysis on the landscape of a global
optimization problem can give us an insight into the complexity of the problem.
This knowledge can better inform us in deciding what optimization method should
be used to tackle the problem. However, this analysis becomes problematic when
the dimensionality of the problem is high. This paper presents a framework to
take a deeper look at the global optimization problem to be tackled: by
analyzing the low-dimensional representation of the problem through discovering
the active subspaces of the given problem. The virtue of this is that the
problem's complexity can be visualized in a one or two-dimensional plot, thus
allow one to get a better grip about the problem's difficulty. One could then
have a better idea regarding the complexity of their problem to determine the
choice of global optimizer or what surrogate-model type to be used.
Furthermore, we also demonstrate how the active subspaces can be used to
perform design exploration and analysis."
"In this note we calculate the gradient of the network function in matrix
notation."
"Artificial neural networks (ANNs) trained using backpropagation are powerful
learning architectures that have achieved state-of-the-art performance in
various benchmarks. Significant effort has been devoted to developing custom
silicon devices to accelerate inference in ANNs. Accelerating the training
phase, however, has attracted relatively little attention. In this paper, we
describe a hardware-efficient on-line learning technique for feedforward
multi-layer ANNs that is based on pipelined backpropagation. Learning is
performed in parallel with inference in the forward pass, removing the need for
an explicit backward pass and requiring no extra weight lookup. This saves 50\%
of the weight lookups needed by a standard online implementation of
backpropagation. By using binary state variables in the feedforward network and
ternary errors in truncated-error backpropagation, the need for any
multiplications in the forward and backward passes is removed, and memory
requirements for the pipelining are drastically reduced. For proof-of-concept
validation, we demonstrate on-line learning of MNIST handwritten digit
classification on a Spartan 6 FPGA interfacing with an external 1Gb DDR2 DRAM,
that shows small degradation in test error performance compared to an
equivalently sized ANN trained off-line using standard back-propagation and
exact errors. Our results highlight an attractive synergy between pipelined
backpropagation and binary-state networks in substantially reducing computation
and memory requirements, making pipelined on-line learning practical in deep
networks."
"The concept of gray-box optimization, in juxtaposition to black-box
optimization, revolves about the idea of exploiting the problem structure to
implement more efficient evolutionary algorithms (EAs). Work on factorized
distribution algorithms (FDAs), whose factorizations are directly derived from
the problem structure, has also contributed to show how exploiting the problem
structure produces important gains in the efficiency of EAs. In this paper we
analyze the general question of using problem structure in EAs focusing on
confronting work done in gray-box optimization with related research
accomplished in FDAs. This contrasted analysis helps us to identify, in current
studies on the use problem structure in EAs, two distinct analytical
characterizations of how these algorithms work. Moreover, we claim that these
two characterizations collide and compete at the time of providing a coherent
framework to investigate this type of algorithms. To illustrate this claim, we
present a contrasted analysis of formalisms, questions, and results produced in
FDAs and gray-box optimization. Common underlying principles in the two
approaches, which are usually overlooked, are identified and discussed.
Besides, an extensive review of previous research related to different uses of
the problem structure in EAs is presented. The paper also elaborates on some of
the questions that arise when extending the use of problem structure in EAs,
such as the question of evolvability, high cardinality of the variables and
large definition sets, constrained and multi-objective problems, etc. Finally,
emergent approaches that exploit neural models to capture the problem structure
are covered."
"Artificial neural networks (ANN) are inadequate to biological neural
networks. This inadequacy is manifested in the use of the obsolete model of the
neuron and the connectionist paradigm of constructing ANN. The result of this
inadequacy is the existence of many shortcomings of the ANN and the problems of
their practical implementation. The alternative principle of ANN construction
is proposed in the article. This principle was called the detector principle.
The basis of the detector principle is the consideration of the binding
property of the input signals of a neuron. A new model of the neuron-detector,
a new approach to teaching ANN - counter training and a new approach to the
formation of the ANN architecture are used in this principle."
"We have calculated the key characteristics of associative
(content-addressable) spatial-temporal memories based on neuromorphic networks
with restricted connectivity - ""CrossNets"". Such networks may be naturally
implemented in nanoelectronic hardware using hybrid CMOS/memristor circuits,
which may feature extremely high energy efficiency, approaching that of
biological cortical circuits, at much higher operation speed. Our numerical
simulations, in some cases confirmed by analytical calculations, have shown
that the characteristics depend substantially on the method of information
recording into the memory. Of the four methods we have explored, two look
especially promising - one based on the quadratic programming, and the other
one being a specific discrete version of the gradient descent. The latter
method provides a slightly lower memory capacity (at the same fidelity) then
the former one, but it allows local recording, which may be more readily
implemented in nanoelectronic hardware. Most importantly, at the synchronous
retrieval, both methods provide a capacity higher than that of the well-known
Ternary Content-Addressable Memories with the same number of nonvolatile memory
cells (e.g., memristors), though the input noise immunity of the CrossNet
memories is somewhat lower."
"Quantum superposition says that any physical system simultaneously exists in
all of its possible states, the number of which is exponential in the number of
entities composing the system. The strength of presence of each possible state
in the superposition, i.e., its probability of being observed, is represented
by its probability amplitude coefficient. The assumption that these
coefficients must be represented physically disjointly from each other, i.e.,
localistically, is nearly universal in the quantum theory/computing literature.
Alternatively, these coefficients can be represented using sparse distributed
representations (SDR), wherein each coefficient is represented by small subset
of an overall population of units, and the subsets can overlap. Specifically, I
consider an SDR model in which the overall population consists of Q WTA
clusters, each with K binary units. Each coefficient is represented by a set of
Q units, one per cluster. Thus, K^Q coefficients can be represented with KQ
units. Thus, the particular world state, X, whose coefficient's representation,
R(X), is the set of Q units active at time t has the max probability and the
probability of every other state, Y_i, at time t, is measured by R(Y_i)'s
intersection with R(X). Thus, R(X) simultaneously represents both the
particular state, X, and the probability distribution over all states. Thus,
set intersection may be used to classically implement quantum superposition. If
algorithms exist for which the time it takes to store (learn) new
representations and to find the closest-matching stored representation
(probabilistic inference) remains constant as additional representations are
stored, this meets the criterion of quantum computing. Such an algorithm has
already been described: it achieves this ""quantum speed-up"" without esoteric
hardware, and in fact, on a single-processor, classical (Von Neumann) computer."
"Balancing assembly lines, a family of optimization problems commonly known as
Assembly Line Balancing Problem, is notoriously NP-Hard. They comprise a set of
problems of enormous practical interest to manufacturing industry due to the
relevant frequency of this type of production paradigm. For this reason, many
researchers on Computational Intelligence and Industrial Engineering have been
conceiving algorithms for tackling different versions of assembly line
balancing problems utilizing different methodologies. In this article, it was
proposed a problem version referred as Mixed Model Workplace Time-dependent
Assembly Line Balancing Problem with the intention of including pressing issues
of real assembly lines in the optimization problem, to which four versions were
conceived. Heuristic search procedures were used, namely two Swarm Intelligence
algorithms from the Fish School Search family: the original version, named
""vanilla"", and a special variation including a stagnation avoidance routine.
Either approaches solved the newly posed problem achieving good results when
compared to Particle Swarm Optimization algorithm."
"In this work we investigate the effectiveness of the application of niching
able swarm metaheuristic approaches in order to solve constrained optimization
problems. Sub-swarms are used in order to allow the achievement of many
feasible regions to be exploited in terms of fitness function. The niching
approach employed was wFSS, a version of the Fish School Search algorithm
devised specifically to deal with multi-modal search spaces. A base technique
referred as wrFSS was conceived and three variations applying different
constraint handling procedures were also proposed. Tests were performed in
seven problems from CEC 2010 and a comparison with other approaches was carried
out. Results show that the search strategy proposed is able to handle some
heavily constrained problems and achieve results comparable to the
state-of-the-art algorithms. However, we also observed that the local search
operator present in wFSS and inherited by wrFSS makes the fitness convergence
difficult when the feasible region presents some specific geometrical features."
"Many assembly lines related optimization problems have been tackled by
researchers in the last decades due to its relevance for the decision makers
within manufacturing industry. Many of theses problems, more specifically
Assembly Lines Balancing and Sequencing problems, are known to be NP-Hard.
Therefore, Computational Intelligence solution approaches have been conceived
in order to provide practical use decision making tools. In this work, we
proposed a simultaneous solution approach in order to tackle both Balancing and
Sequencing problems utilizing an effective meta-heuristic algorithm referred as
Fish School Search. Three different test instances were solved with the
original and two modified versions of this algorithm and the results were
compared with Particle Swarm Optimization Algorithm."
"Patterns stored within pre-trained deep neural networks compose large and
powerful descriptive languages that can be used for many different purposes.
Typically, deep network representations are implemented within vector embedding
spaces, which enables the use of traditional machine learning algorithms on top
of them. In this short paper we propose the construction of a graph embedding
space instead, introducing a methodology to transform the knowledge coded
within a deep convolutional network into a topological space (i.e. a network).
We outline how such graph can hold data instances, data features, relations
between instances and features, and relations among features. Finally, we
introduce some preliminary experiments to illustrate how the resultant graph
embedding space can be exploited through graph analytics algorithms."
"Time series account for a large proportion of the data stored in financial,
medical and scientific databases. The efficient storage of time series is
important in practical applications. In this paper, we propose a novel
\emph{lossy} compression scheme for time series. The encoder and decoder are
both composed by recurrent neural networks (RNN) such as long short-term memory
(LSTM). There is an autoencoder between encoder and decoder, which encodes the
hidden state and input together and decodes them at the decoder side. The input
window size is adaptively changing based on the local statistics of time
series. The experimental study shows that the proposed algorithm can achieve
competitive compression ratio on real-world time series."
"This paper proposes an improved epsilon constraint-handling mechanism, and
combines it with a decomposition-based multi-objective evolutionary algorithm
(MOEA/D) to solve constrained multi-objective optimization problems (CMOPs).
The proposed constrained multi-objective evolutionary algorithm (CMOEA) is
named MOEA/D-IEpsilon. It adjusts the epsilon level dynamically according to
the ratio of feasible to total solutions (RFS) in the current population. In
order to evaluate the performance of MOEA/D-IEpsilon, a new set of CMOPs with
two and three objectives is designed, having large infeasible regions (relative
to the feasible regions), and they are called LIR-CMOPs. Then the fourteen
benchmarks, including LIR-CMOP1-14, are used to test MOEA/D-IEpsilon and four
other decomposition-based CMOEAs, including MOEA/D-Epsilon, MOEA/D-SR,
MOEA/D-CDP and C-MOEA/D. The experimental results indicate that MOEA/D-IEpsilon
is significantly better than the other four CMOEAs on all of the test
instances, which shows that MOEA/D-IEpsilon is more suitable for solving CMOPs
with large infeasible regions. Furthermore, a real-world problem, namely the
robot gripper optimization problem, is used to test the five CMOEAs. The
experimental results demonstrate that MOEA/D-IEpsilon also outperforms the
other four CMOEAs on this problem."
"We introduce an evolutionary stochastic-local-search (SLS) algorithm for
addressing a generalized version of the so-called 1/V/D/R cutting-stock
problem. Cutting-stock problems are encountered often in industrial
environments and the ability to address them efficiently usually results in
large economic benefits. Traditionally linear-programming-based techniques have
been utilized to address such problems, however their flexibility might be
limited when nonlinear constraints and objective functions are introduced. To
this end, this paper proposes an evolutionary SLS algorithm for addressing
one-dimensional cutting-stock problems. The contribution lies in the
introduction of a flexible structural framework of the optimization that may
accommodate a large family of diversification strategies including a novel
parallel pattern appropriate for SLS algorithms (not necessarily restricted to
cutting-stock problems). We finally demonstrate through experiments in a
real-world manufacturing problem the benefit in cost reduction of the
considered diversification strategies."
"Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks
(DNNs), is presented and evaluated on Convolutional Neural Networks. TRT
exploits the variable per layer precision requirements of DNNs to deliver
execution time that is proportional to the precision p in bits used per layer
for convolutional and fully-connected layers. Prior art has demonstrated an
accelerator with the same execution performance only for convolutional layers.
Experiments on image classification CNNs show that on average across all
networks studied, TRT outperforms a state-of-the-art bit-parallel accelerator
by 1:90x without any loss in accuracy while it is 1:17x more energy efficient.
TRT requires no network retraining while it enables trading off accuracy for
additional improvements in execution performance and energy efficiency. For
example, if a 1% relative loss in accuracy is acceptable, TRT is on average
2:04x faster and 1:25x more energy efficient than a conventional bit-parallel
accelerator. A Tartan configuration that processes 2-bits at time, requires
less area than the 1-bit configuration, improves efficiency to 1:24x over the
bit-parallel baseline while being 73% faster for convolutional layers and 60%
faster for fully-connected layers is also presented."
"Dots-and-Boxes is a child's game which remains analytically unsolved. We
implement and evolve artificial neural networks to play this game, evaluating
them against simple heuristic players. Our networks do not evaluate or predict
the final outcome of the game, but rather recommend moves at each stage.
Superior generalisation of play by co-evolved populations is found, and a
comparison made with networks trained by back-propagation using simple
heuristics as an oracle."
"In this paper we introduce a neural network model of self-organization. This
model uses a variation of Hebb rule for updating its synaptic weights, and
surely converges to the equilibrium status. The key point of the convergence is
the update rule that constrains the total synaptic weight and this seems to
make the model stable. We investigate the role of the constraint and show that
it is the constraint that makes the model stable. For analyzing this setting,
we propose a simple probabilistic game that models the neural network and the
self-organization process. Then, we investigate the characteristics of this
game, namely, the probability that the game becomes stable and the number of
the steps it takes."
"A combination of a neural network with rule firing information from a
rule-based system is used to generate segment durations for a text-to-speech
system. The system shows a slight improvement in performance over a neural
network system without the rule firing information. Synthesized speech using
segment durations was accepted by listeners as having about the same quality as
speech generated using segment durations extracted from natural speech."
"Text-to-speech conversion has traditionally been performed either by
concatenating short samples of speech or by using rule-based systems to convert
a phonetic representation of speech into an acoustic representation, which is
then converted into speech. This paper describes a system that uses a
time-delay neural network (TDNN) to perform this phonetic-to-acoustic mapping,
with another neural network to control the timing of the generated speech. The
neural network system requires less memory than a concatenation system, and
performed well in tests comparing it to commercial systems using other
technologies."
"This paper describes the design of a neural network that performs the
phonetic-to-acoustic mapping in a speech synthesis system. The use of a
time-domain neural network architecture limits discontinuities that occur at
phone boundaries. Recurrent data input also helps smooth the output parameter
tracks. Independent testing has demonstrated that the voice quality produced by
this system compares favorably with speech from existing commercial
text-to-speech systems."
"While neural networks have been employed to handle several different
text-to-speech tasks, ours is the first system to use neural networks
throughout, for both linguistic and acoustic processing. We divide the
text-to-speech task into three subtasks, a linguistic module mapping from text
to a linguistic representation, an acoustic module mapping from the linguistic
representation to speech, and a video module mapping from the linguistic
representation to animated images. The linguistic module employs a
letter-to-sound neural network and a postlexical neural network. The acoustic
module employs a duration neural network and a phonetic neural network. The
visual neural network is employed in parallel to the acoustic module to drive a
talking head. The use of neural networks that can be retrained on the
characteristics of different voices and languages affords our system a degree
of adaptability and naturalness heretofore unavailable."
"In [N. A. Baas, Emergence, Hierarchies, and Hyper-structures, in C.G. Langton
ed., Artificial Life III, Addison Wesley, 1994.] a general framework for the
study of Emergence and hyper-structure was presented. This approach is mostly
concerned with the description of such systems. In this paper we will try to
bring forth a different aspect of this model we feel will be useful in the
engineering of agent based solutions, namely the symbiotic approach. In this
approach a self-organizing method of dividing the more complex ""main-problem""
to a hyper-structure of ""sub-problems"" with the aim of reducing complexity is
desired. A description of the general problem will be given along with some
instances of related work. This paper is intended to serve as an introductory
challenge for general solutions to the described problem."
"Several researchers have experimentally shown that substantial improvements
can be obtained in difficult pattern recognition problems by combining or
integrating the outputs of multiple classifiers. This chapter provides an
analytical framework to quantify the improvements in classification results due
to combining. The results apply to both linear combiners and order statistics
combiners. We first show that to a first order approximation, the error rate
obtained over and above the Bayes error rate, is directly proportional to the
variance of the actual decision boundaries around the Bayes optimum boundary.
Combining classifiers in output space reduces this variance, and hence reduces
the ""added"" error. If N unbiased classifiers are combined by simple averaging,
the added error rate can be reduced by a factor of N if the individual errors
in approximating the decision boundaries are uncorrelated. Expressions are then
derived for linear combiners which are biased or correlated, and the effect of
output correlations on ensemble performance is quantified. For order statistics
based non-linear combiners, we derive expressions that indicate how much the
median, the maximum and in general the ith order statistic can improve
classifier performance. The analysis presented here facilitates the
understanding of the relationships among error rates, classifier boundary
distributions, and combining in output space. Experimental results on several
public domain data sets are provided to illustrate the benefits of combining
and to support the analytical results."
"Radial Basis Function Networks (RBFNs) are used primarily to solve
curve-fitting problems and for non-linear system modeling. Several algorithms
are known for the approximation of a non-linear curve from a sparse data set by
means of RBFNs. However, there are no procedures that permit to define
constrains on the derivatives of the curve. In this paper, the Orthogonal Least
Squares algorithm for the identification of RBFNs is modified to provide the
approximation of a non-linear 1-in 1-out map along with its derivatives, given
a set of training data. The interest on the derivatives of non-linear functions
concerns many identification and control tasks where the study of system
stability and robustness is addressed. The effectiveness of the proposed
algorithm is demonstrated by a study on the stability of a single loop feedback
system."
"Noise is source of ambiguity for fuzzy systems. Although being an important
aspect, the effects of noise in fuzzy modeling have been little investigated.
This paper presents a set of tests using three well-known fuzzy modeling
algorithms. These evaluate perturbations in the extracted rule-bases caused by
noise polluting the learning data, and the corresponding deformations in each
learned functional relation. We present results to show: 1) how these fuzzy
modeling systems deal with noise; 2) how the established fuzzy model structure
influences noise sensitivity of each algorithm; and 3) whose characteristics of
the learning algorithms are relevant to noise attenuation."
"Thinking is one of the most interesting mental processes. Its complexity is
sometimes simplified and its different manifestations are classified into
normal and abnormal, like the delusional and disorganized thought or the
creative one. The boundaries between these facets of thinking are fuzzy causing
difficulties in medical, academic, and philosophical discussions. Considering
the dopaminergic signal-to-noise neuronal modulation in the central nervous
system, and the existence of semantic maps in human brain, a self-organizing
neural network model was developed to unify the different thought processes
into a single neurocomputational substrate. Simulations were performed varying
the dopaminergic modulation and observing the different patterns that emerged
at the semantic map. Assuming that the thought process is the total pattern
elicited at the output layer of the neural network, the model shows how the
normal and abnormal thinking are generated and that there are no borders
between their different manifestations. Actually, a continuum of different
qualitative reasoning, ranging from delusion to disorganization of thought, and
passing through the normal and the creative thinking, seems to be more
plausible. The model is far from explaining the complexities of human thinking
but, at least, it seems to be a good metaphorical and unifying view of the many
facets of this phenomenon usually studied in separated settings."
"Rainfall in Kerala State, the southern part of Indian Peninsula in particular
is caused by the two monsoons and the two cyclones every year. In general,
climate and rainfall are highly nonlinear phenomena in nature giving rise to
what is known as the `butterfly effect'. We however attempt to train an ABF
neural network on the time series rainfall data and show for the first time
that in spite of the fluctuations resulting from the nonlinearity in the
system, the trends in the rainfall pattern in this corner of the globe have
remained unaffected over the past 87 years from 1893 to 1980. We also
successfully filter out the chaotic part of the system and illustrate that its
effects are marginal over long term predictions."
"It is still unclear how an evolutionary algorithm (EA) searches a fitness
landscape, and on what fitness landscapes a particular EA will do well. The
validity of the building-block hypothesis, a major tenet of traditional genetic
algorithm theory, remains controversial despite its continued use to justify
claims about EAs. This paper outlines a research program to begin to answer
some of these open questions, by extending the work done in the royal road
project. The short-term goal is to find a simple class of functions which the
simple genetic algorithm optimizes better than other optimization methods, such
as hillclimbers. A dialectical heuristic for searching for such a class is
introduced. As an example of using the heuristic, the simple genetic algorithm
is compared with a set of hillclimbers on a simple subset of the
hyperplane-defined functions, the pothole functions."
"Artificial neurons with arbitrarily complex internal structure are
introduced. The neurons can be described in terms of a set of internal
variables, a set activation functions which describe the time evolution of
these variables and a set of characteristic functions which control how the
neurons interact with one another. The information capacity of attractor
networks composed of these generalized neurons is shown to reach the maximum
allowed bound. A simple example taken from the domain of pattern recognition
demonstrates the increased computational power of these neurons. Furthermore, a
specific class of generalized neurons gives rise to a simple transformation
relating attractor networks of generalized neurons to standard three layer
feed-forward networks. Given this correspondence, we conjecture that the
maximum information capacity of a three layer feed-forward network is 2 bits
per weight."
"Non-negative sparse coding is a method for decomposing multivariate data into
non-negative sparse components. In this paper we briefly describe the
motivation behind this type of data representation and its relation to standard
sparse coding and non-negative matrix factorization. We then give a simple yet
efficient multiplicative algorithm for finding the optimal values of the hidden
components. In addition, we show how the basis vectors can be learned from the
observed data. Simulations demonstrate the effectiveness of the proposed
method."
"Search engines perform the task of retrieving information related to the
user-supplied query words. This task has two parts; one is finding ""featured
words"" which describe an article best and the other is finding a match among
these words to user-defined search terms. There are two main independent
approaches to achieve this task. The first one, using the concepts of
semantics, has been implemented partially. For more details see another paper
of Marko et al., 2002. The second approach is reported in this paper. It is a
theoretical model based on using Neural Network (NN). Instead of using keywords
or reading from the first few lines from papers/articles, the present model
gives emphasis on extracting ""featured words"" from an article. Obviously we
propose to exclude prepositions, articles and so on, that is, English words
like ""of, the, are, so, therefore, "" etc. from such a list. A neural model is
taken with its nodes pre-assigned energies. Whenever a match is found with
featured words and userdefined search words, the node is fired and jumps to a
higher energy. This firing continues until the model attains a steady energy
level and total energy is now calculated. Clearly, higher match will generate
higher energy; so on the basis of total energy, a ranking is done to the
article indicating degree of relevance to the user's interest. Another
important feature of the proposed model is incorporating a semantic module to
refine the search words; like finding association among search words, etc. In
this manner, information retrieval can be improved markedly."
"A recently introduced general-purpose heuristic for finding high-quality
solutions for many hard optimization problems is reviewed. The method is
inspired by recent progress in understanding far-from-equilibrium phenomena in
terms of {\em self-organized criticality,} a concept introduced to describe
emergent complexity in physical systems. This method, called {\em extremal
optimization,} successively replaces the value of extremely undesirable
variables in a sub-optimal solution with new, random ones. Large,
avalanche-like fluctuations in the cost function self-organize from this
dynamics, effectively scaling barriers to explore local optima in distant
neighborhoods of the configuration space while eliminating the need to tune
parameters. Drawing upon models used to simulate the dynamics of granular
media, evolution, or geology, extremal optimization complements approximation
methods inspired by equilibrium statistical physics, such as {\em simulated
annealing}. It may be but one example of applying new insights into {\em
non-equilibrium phenomena} systematically to hard optimization problems. This
method is widely applicable and so far has proved competitive with -- and even
superior to -- more elaborate general-purpose heuristics on testbeds of
constrained optimization problems with up to $10^5$ variables, such as
bipartitioning, coloring, and satisfiability. Analysis of a suitable model
predicts the only free parameter of the method in accordance with all
experimental results."
"JohnnyVon is an implementation of self-replicating automata in continuous
two-dimensional space. Two types of particles drift about in a virtual liquid.
The particles are automata with discrete internal states but continuous
external relationships. Their internal states are governed by finite state
machines but their external relationships are governed by a simulated physics
that includes brownian motion, viscosity, and spring-like attractive and
repulsive forces. The particles can be assembled into patterns that can encode
arbitrary strings of bits. We demonstrate that, if an arbitrary ""seed"" pattern
is put in a ""soup"" of separate individual particles, the pattern will replicate
by assembling the individual particles into copies of itself. We also show
that, given sufficient time, a soup of separate individual particles will
eventually spontaneously form self-replicating patterns. We discuss the
implications of JohnnyVon for research in nanotechnology, theoretical biology,
and artificial life."
"The scope of this teaching package is to make a brief induction to Artificial
Neural Networks (ANNs) for people who have no previous knowledge of them. We
first make a brief introduction to models of networks, for then describing in
general terms ANNs. As an application, we explain the backpropagation
algorithm, since it is widely used and many other algorithms are derived from
it. The user should know algebra and the handling of functions and vectors.
Differential calculus is recommendable, but not necessary. The contents of this
package should be understood by people with high school education. It would be
useful for people who are just curious about what are ANNs, or for people who
want to become familiar with them, so when they study them more fully, they
will already have clear notions of ANNs. Also, people who only want to apply
the backpropagation algorithm without a detailed and formal explanation of it
will find this material useful. This work should not be seen as ""Nets for
dummies"", but of course it is not a treatise. Much of the formality is skipped
for the sake of simplicity. Detailed explanations and demonstrations can be
found in the referred readings. The included exercises complement the
understanding of the theory. The on-line resources are highly recommended for
extending this brief induction."
"The paper presents a hybrid system controller, incorporating a neural and an
LQG controller. The neural controller has been optimized by genetic algorithms
directly on the inverted pendulum system. The failure free optimization process
stipulated a relatively small region of the asymptotic stability of the neural
controller, which is concentrated around the regulation point. The presented
hybrid controller combines benefits of a genetically optimized neural
controller and an LQG controller in a single system controller. High quality of
the regulation process is achieved through utilization of the neural
controller, while stability of the system during transient processes and a wide
range of operation are assured through application of the LQG controller. The
hybrid controller has been validated by applying it to a simulation model of an
inherently unstable system of inverted pendulum."
"The paper presents a method for failure free genetic algorithm optimization
of a system controller. Genetic algorithms present a powerful tool that
facilitates producing near-optimal system controllers. Applied to such methods
of computational intelligence as neural networks or fuzzy logic, these methods
are capable of combining the non-linear mapping capabilities of the latter with
learning the system behavior directly, that is, without a prior model. At the
same time, genetic algorithms routinely produce solutions that lead to the
failure of the controlled system. Such solutions are generally unacceptable for
applications where safe operation must be guaranteed. We present here a method
of design, which allows failure-free application of genetic algorithms through
utilization of SAFE and LEARNING controllers in tandem, where the SAFE
controller recovers the system from dangerous states while the LEARNING
controller learns its behavior. The method has been validated by applying it to
an inherently unstable system of inverted pendulum."
"Neuromodulatory receptors in presynaptic position have the ability to
suppress synaptic transmission for seconds to minutes when fully engaged. This
effectively alters the synaptic strength of a connection. Much work on
neuromodulation has rested on the assumption that these effects are uniform at
every neuron. However, there is considerable evidence to suggest that
presynaptic regulation may be in effect synapse-specific. This would define a
second ""weight modulation"" matrix, which reflects presynaptic receptor efficacy
at a given site. Here we explore functional consequences of this hypothesis. By
analyzing and comparing the weight matrices of networks trained on different
aspects of a task, we identify the potential for a low complexity ""modulation
matrix"", which allows to switch between differently trained subtasks while
retaining general performance characteristics for the task. This means that a
given network can adapt itself to different task demands by regulating its
release of neuromodulators. Specifically, we suggest that (a) a network can
provide optimized responses for related classification tasks without the need
to train entirely separate networks and (b) a network can blend a ""memory mode""
which aims at reproducing memorized patterns and a ""novelty mode"" which aims to
facilitate classification of new patterns. We relate this work to the known
effects of neuromodulators on brain-state dependent processing."
"We discuss the computational complexity of random 2D Ising spin glasses,
which represent an interesting class of constraint satisfaction problems for
black box optimization. Two extremal cases are considered: (1) the +/- J spin
glass, and (2) the Gaussian spin glass. We also study a smooth transition
between these two extremal cases. The computational complexity of all studied
spin glass systems is found to be dominated by rare events of extremely hard
spin glass samples. We show that complexity of all studied spin glass systems
is closely related to Frechet extremal value distribution. In a hybrid
algorithm that combines the hierarchical Bayesian optimization algorithm (hBOA)
with a deterministic bit-flip hill climber, the number of steps performed by
both the global searcher (hBOA) and the local searcher follow Frechet
distributions. Nonetheless, unlike in methods based purely on local search, the
parameters of these distributions confirm good scalability of hBOA with local
search. We further argue that standard performance measures for optimization
algorithms--such as the average number of evaluations until convergence--can be
misleading. Finally, our results indicate that for highly multimodal constraint
satisfaction problems, such as Ising spin glasses, recombination-based search
can provide qualitatively better results than mutation-based search."
"The parameter-less hierarchical Bayesian optimization algorithm (hBOA)
enables the use of hBOA without the need for tuning parameters for solving each
problem instance. There are three crucial parameters in hBOA: (1) the selection
pressure, (2) the window size for restricted tournaments, and (3) the
population size. Although both the selection pressure and the window size
influence hBOA performance, performance should remain low-order polynomial with
standard choices of these two parameters. However, there is no standard
population size that would work for all problems of interest and the population
size must thus be eliminated in a different way. To eliminate the population
size, the parameter-less hBOA adopts the population-sizing technique of the
parameter-less genetic algorithm. Based on the existing theory, the
parameter-less hBOA should be able to solve nearly decomposable and
hierarchical problems in quadratic or subquadratic number of function
evaluations without the need for setting any parameters whatsoever. A number of
experiments are presented to verify scalability of the parameter-less hBOA."
"Evolutionary computation algorithms are increasingly being used to solve
optimization problems as they have many advantages over traditional
optimization algorithms. In this paper we use evolutionary computation to study
the trade-off between pleiotropy and redundancy in a client-server based
network. Pleiotropy is a term used to describe components that perform multiple
tasks, while redundancy refers to multiple components performing one same task.
Pleiotropy reduces cost but lacks robustness, while redundancy increases
network reliability but is more costly, as together, pleiotropy and redundancy
build flexibility and robustness into systems. Therefore it is desirable to
have a network that contains a balance between pleiotropy and redundancy. We
explore how factors such as link failure probability, repair rates, and the
size of the network influence the design choices that we explore using genetic
algorithms."
"This paper explores the use of genetic algorithms for the design of networks,
where the demands on the network fluctuate in time. For varying network
constraints, we find the best network using the standard genetic algorithm
operators such as inversion, mutation and crossover. We also examine how the
choice of genetic algorithm operators affects the quality of the best network
found. Such networks typically contain redundancy in servers, where several
servers perform the same task and pleiotropy, where servers perform multiple
tasks. We explore this trade-off between pleiotropy versus redundancy on the
cost versus reliability as a measure of the quality of the network."
"Different classes of communication network topologies and their
representation in the form of adjacency matrix and its eigenvalues are
presented. A self-organizing feature map neural network is used to map
different classes of communication network topological patterns. The neural
network simulation results are reported."
"A speculative overview of a future topic of research. The paper is a
collection of ideas concerning two related areas:
  1) Graph computation machines (""computing with graphs""). This is the class of
models of computation in which the state of the computation is represented as a
graph or network.
  2) Arc-based neural networks, which store information not as activation in
the nodes, but rather by adding and deleting arcs. Sometimes the arcs may be
interpreted as synchronization.
  Warnings to readers: this is not the sort of thing that one might submit to a
journal or conference. No proofs are presented. The presentation is informal,
and written at an introductory level. You'll probably want to wait for a more
concise presentation."
"Estimation of Distribution Algorithms have been proposed as a new paradigm
for evolutionary optimization. This paper focuses on the parallelization of
Estimation of Distribution Algorithms. More specifically, the paper discusses
how to predict performance of parallel Mixed Bayesian Optimization Algorithm
(MBOA) that is based on parallel construction of Bayesian networks with
decision trees. We determine the time complexity of parallel Mixed Bayesian
Optimization Algorithm and compare this complexity with experimental results
obtained by solving the spin glass optimization problem. The empirical results
fit well the theoretical time complexity, so the scalability and efficiency of
parallel Mixed Bayesian Optimization Algorithm for unknown instances of spin
glass benchmarks can be predicted. Furthermore, we derive the guidelines that
can be used to design effective parallel Estimation of Distribution Algorithms
with the speedup proportional to the number of variables in the problem."
"In this paper it is shown how to map a data manifold into a simpler form by
progressively discarding small degrees of freedom. This is the key to
self-organising data fusion, where the raw data is embedded in a very
high-dimensional space (e.g. the pixel values of one or more images), and the
requirement is to isolate the important degrees of freedom which lie on a
low-dimensional manifold. A useful advantage of the approach used in this paper
is that the computations are arranged as a feed-forward processing chain, where
all the details of the processing in each stage of the chain are learnt by
self-organisation. This approach is demonstrated using hierarchically
correlated data, which causes the processing chain to split the data into
separate processing channels, and then to progressively merge these channels
wherever they are correlated with each other. This is the key to
self-organising data fusion."
"In this paper a stochastic generalisation of the standard Linde-Buzo-Gray
(LBG) approach to vector quantiser (VQ) design is presented, in which the
encoder is implemented as the sampling of a vector of code indices from a
probability distribution derived from the input vector, and the decoder is
implemented as a superposition of reconstruction vectors. This stochastic VQ
(SVQ) is optimised using a minimum mean Euclidean reconstruction distortion
criterion, as in the LBG case. Numerical simulations are used to demonstrate
how this leads to self-organisation of the SVQ, where different stochastically
sampled code indices become associated with different input subspaces."
"The theory of stochastic vector quantisers (SVQ) has been extended to allow
the quantiser to develop invariances, so that only ""large"" degrees of freedom
in the input vector are represented in the code. This has been applied to the
problem of encoding data vectors which are a superposition of a ""large"" jammer
and a ""small"" signal, so that only the jammer is represented in the code. This
allows the jammer to be subtracted from the total input vector (i.e. the jammer
is nulled), leaving a residual that contains only the underlying signal. The
main advantage of this approach to jammer nulling is that little prior
knowledge of the jammer is assumed, because these properties are automatically
discovered by the SVQ as it is trained on examples of input vectors."
"Using the maximum entropy method, we derive the ""adaptive cluster expansion""
(ACE), which can be trained to estimate probability density functions in high
dimensional spaces. The main advantage of ACE over other Bayesian networks is
its ability to capture high order statistics after short training times, which
it achieves by making use of a hierarchical vector quantisation of the input
data. We derive a scheme for representing the state of an ACE network as a
""probability image"", which allows us to identify statistically anomalous
regions in an otherwise statistically homogeneous image, for instance. Finally,
we present some probability images that we obtained after training ACE on some
Brodatz texture images - these demonstrate the ability of ACE to detect subtle
textural anomalies."
"We study in this paper the effect of an unique initial stimulation on random
recurrent networks of leaky integrate and fire neurons. Indeed given a
stochastic connectivity this so-called spontaneous mode exhibits various non
trivial dynamics. This study brings forward a mathematical formalism that
allows us to examine the variability of the afterward dynamics according to the
parameters of the weight distribution. Provided independence hypothesis (e.g.
in the case of very large networks) we are able to compute the average number
of neurons that fire at a given time -- the spiking activity. In accordance
with numerical simulations, we prove that this spiking activity reaches a
steady-state, we characterize this steady-state and explore the transients."
"Multi-dimensional data classification is an important and challenging problem
in many astro-particle experiments. Neural networks have proved to be versatile
and robust in multi-dimensional data classification. In this article we shall
study the classification of gamma from the hadrons for the MAGIC Experiment.
Two neural networks have been used for the classification task. One is
Multi-Layer Perceptron based on supervised learning and other is
Self-Organising Map (SOM), which is based on unsupervised learning technique.
The results have been shown and the possible ways of combining these networks
have been proposed to yield better and faster classification results."
"Jackendoff (2002) posed four challenges that linguistic combinatoriality and
rules of language present to theories of brain function. The essence of these
problems is the question of how to neurally instantiate the rapid construction
and transformation of the compositional structures that are typically taken to
be the domain of symbolic processing. He contended that typical connectionist
approaches fail to meet these challenges and that the dialogue between
linguistic theory and cognitive neuroscience will be relatively unproductive
until the importance of these problems is widely recognised and the challenges
answered by some technical innovation in connectionist modelling. This paper
claims that a little-known family of connectionist models (Vector Symbolic
Architectures) are able to meet Jackendoff's challenges."
"The problem of finding out the global minimum of a multiextremal functional
is discussed. One frequently faces with such a functional in various
applications. We propose a procedure, which depends on the dimensionality of
the problem polynomially. In our approach we use the eigenvalues and
eigenvectors of the connection matrix."
"An effective neural network algorithm of the perceptron type is proposed. The
algorithm allows us to identify strongly distorted input vector reliably. It is
shown that its reliability and processing speed are orders of magnitude higher
than that of full connected neural networks. The processing speed of our
algorithm exceeds the one of the stack fast-access retrieval algorithm that is
modified for working when there are noises in the input channel."
"Genetic algorithms (GAs) that solve hard problems quickly, reliably and
accurately are called competent GAs. When the fitness landscape of a problem
changes overtime, the problem is called non--stationary, dynamic or
time--variant problem. This paper investigates the use of competent GAs for
optimizing non--stationary optimization problems. More specifically, we use an
information theoretic approach based on the minimum description length
principle to adaptively identify regularities and substructures that can be
exploited to respond quickly to changes in the environment. We also develop a
special type of problems with bounded difficulties to test non--stationary
optimization problems. The results provide new insights into non-stationary
optimization problems and show that a search algorithm which automatically
identifies and exploits possible decompositions is more robust and responds
quickly to changes than a simple genetic algorithm."
"Niching enables a genetic algorithm (GA) to maintain diversity in a
population. It is particularly useful when the problem has multiple optima
where the aim is to find all or as many as possible of these optima. When the
fitness landscape of a problem changes overtime, the problem is called
non--stationary, dynamic or time--variant problem. In these problems, niching
can maintain useful solutions to respond quickly, reliably and accurately to a
change in the environment. In this paper, we present a niching method that
works on the problem substructures rather than the whole solution, therefore it
has less space complexity than previously known niching mechanisms. We show
that the method is responding accurately when environmental changes occur."
"We propose a sub-structural niching method that fully exploits the problem
decomposition capability of linkage-learning methods such as the estimation of
distribution algorithms and concentrate on maintaining diversity at the
sub-structural level. The proposed method consists of three key components: (1)
Problem decomposition and sub-structure identification, (2) sub-structure
fitness estimation, and (3) sub-structural niche preservation. The
sub-structural niching method is compared to restricted tournament selection
(RTS)--a niching method used in hierarchical Bayesian optimization
algorithm--with special emphasis on sustained preservation of multiple global
solutions of a class of boundedly-difficult, additively-separable multimodal
problems. The results show that sub-structural niching successfully maintains
multiple global optima over large number of generations and does so with
significantly less population than RTS. Additionally, the market share of each
of the niche is much closer to the expected level in sub-structural niching
when compared to RTS."
"This paper discusses scalability of standard genetic programming (GP) and the
probabilistic incremental program evolution (PIPE). To investigate the need for
both effective mixing and linkage learning, two test problems are considered:
ORDER problem, which is rather easy for any recombination-based GP, and TRAP or
the deceptive trap problem, which requires the algorithm to learn interactions
among subsets of terminals. The scalability results show that both GP and PIPE
scale up polynomially with problem size on the simple ORDER problem, but they
both scale up exponentially on the deceptive problem. This indicates that while
standard recombination is sufficient when no interactions need to be
considered, for some problems linkage learning is necessary. These results are
in agreement with the lessons learned in the domain of binary-string genetic
algorithms (GAs). Furthermore, the paper investigates the effects of
introducing utnnecessary and irrelevant primitives on the performance of GP and
PIPE."
"This paper describes a scalable algorithm for solving multiobjective
decomposable problems by combining the hierarchical Bayesian optimization
algorithm (hBOA) with the nondominated sorting genetic algorithm (NSGA-II) and
clustering in the objective space. It is first argued that for good
scalability, clustering or some other form of niching in the objective space is
necessary and the size of each niche should be approximately equal.
Multiobjective hBOA (mohBOA) is then described that combines hBOA, NSGA-II and
clustering in the objective space. The algorithm mohBOA differs from the
multiobjective variants of BOA and hBOA proposed in the past by including
clustering in the objective space and allocating an approximately equally sized
portion of the population to each cluster. The algorithm mohBOA is shown to
scale up well on a number of problems on which standard multiobjective
evolutionary algorithms perform poorly."
"The paper analyzes the scalability of multiobjective estimation of
distribution algorithms (MOEDAs) on a class of boundedly-difficult
additively-separable multiobjective optimization problems. The paper
illustrates that even if the linkage is correctly identified, massive
multimodality of the search problems can easily overwhelm the nicher and lead
to exponential scale-up. Facetwise models are subsequently used to propose a
growth rate of the number of differing substructures between the two objectives
to avoid the niching method from being overwhelmed and lead to polynomial
scalability of MOEDAs."
"We show how an evolutionary algorithm can successfully be used to evolve a
set of difficult to solve symmetric travelling salesman problem instances for
two variants of the Lin-Kernighan algorithm. Then we analyse the instances in
those sets to guide us towards deferring general knowledge about the efficiency
of the two variants in relation to structural properties of the symmetric
travelling sale sman problem."
"A commonly experienced problem with population based optimisation methods is
the gradual decline in population diversity that tends to occur over time. This
can slow a system's progress or even halt it completely if the population
converges on a local optimum from which it cannot escape. In this paper we
present the Fitness Uniform Deletion Scheme (FUDS), a simple but somewhat
unconventional approach to this problem. Under FUDS the deletion operation is
modified to only delete those individuals which are ""common"" in the sense that
there exist many other individuals of similar fitness in the population. This
makes it impossible for the population to collapse to a collection of highly
related individuals with similar fitness. Our experimental results on a range
of optimisation problems confirm this, in particular for deceptive optimisation
problems the performance is significantly more robust to variation in the
selection intensity."
"We describe a new algorithm for learning multi-class neural-network models
from large-scale clinical electroencephalograms (EEGs). This algorithm trains
hidden neurons separately to classify all the pairs of classes. To find best
pairwise classifiers, our algorithm searches for input variables which are
relevant to the classification problem. Despite patient variability and heavily
overlapping classes, a 16-class model learnt from EEGs of 65 sleeping newborns
correctly classified 80.8% of the training and 80.1% of the testing examples.
Additionally, the neural-network model provides a probabilistic interpretation
of decisions."
"Knowledge discovery is defined as non-trivial extraction of implicit,
previously unknown and potentially useful information from given data.
Knowledge extraction from web documents deals with unstructured, free-format
documents whose number is enormous and rapidly growing. The artificial neural
networks are well suitable to solve a problem of knowledge discovery from web
documents because trained networks are able more accurately and easily to
classify the learning and testing examples those represent the text mining
domain. However, the neural networks that consist of large number of weighted
connections and activation units often generate the incomprehensible and
hard-to-understand models of text classification. This problem may be also
addressed to most powerful recurrent neural networks that employ the feedback
links from hidden or output units to their input units. Due to feedback links,
recurrent neural networks are able take into account of a context in document.
To be useful for data mining, self-organizing neural network techniques of
knowledge extraction have been explored and developed. Self-organization
principles were used to create an adequate neural-network structure and reduce
a dimensionality of features used to describe text documents. The use of these
principles seems interesting because ones are able to reduce a neural-network
redundancy and considerably facilitate the knowledge representation."
"A new learning algorithm for Evolving Cascade Neural Networks (ECNNs) is
described. An ECNN starts to learn with one input node and then adding new
inputs as well as new hidden neurons evolves it. The trained ECNN has a nearly
minimal number of input and hidden neurons as well as connections. The
algorithm was successfully applied to classify artifacts and normal segments in
clinical electroencephalograms (EEGs). The EEG segments were visually labeled
by EEG-viewer. The trained ECNN has correctly classified 96.69% of the testing
segments. It is slightly better than a standard fully connected neural network."
"The principles of self-organizing the neural networks of optimal complexity
is considered under the unrepresentative learning set. The method of
self-organizing the multi-layered neural networks is offered and used to train
the logical neural networks which were applied to the medical diagnostics."
"The neural networks have trained on incomplete sets that a doctor could
collect. Trained neural networks have correctly classified all the presented
instances. The number of intervals entered for encoding the quantitative
variables is equal two. The number of features as well as the number of neurons
and layers in trained neural networks was minimal. Trained neural networks are
adequately represented as a set of logical formulas that more comprehensible
and easy-to-understand. These formulas are as the syndrome-complexes, which may
be easily tabulated and represented as a diagnostic table that the doctors
usually use. Decision rules provide the evaluations of their confidence in
which interested a doctor. Conducted clinical researches have shown that
iagnostic decisions produced by symbolic rules have coincided with the doctor's
conclusions."
"A neural network based technique is presented, which is able to successfully
extract polynomial classification rules from labeled electroencephalogram (EEG)
signals. To represent the classification rules in an analytical form, we use
the polynomial neural networks trained by a modified Group Method of Data
Handling (GMDH). The classification rules were extracted from clinical EEG data
that were recorded from an Alzheimer patient and the sudden death risk
patients. The third data is EEG recordings that include the normal and artifact
segments. These EEG data were visually identified by medical experts. The
extracted polynomial rules verified on the testing EEG data allow to correctly
classify 72% of the risk group patients and 96.5% of the segments. These rules
performs slightly better than standard feedforward neural networks."
"To learn the multi-class conceptions from the electroencephalogram (EEG) data
we developed a neural network decision tree (DT), that performs the linear
tests, and a new training algorithm. We found that the known methods fail
inducting the classification models when the data are presented by the features
some of them are irrelevant, and the classes are heavily overlapped. To train
the DT, our algorithm exploits a bottom up search of the features that provide
the best classification accuracy of the linear tests. We applied the developed
algorithm to induce the DT from the large EEG dataset consisted of 65 patients
belonging to 16 age groups. In these recordings each EEG segment was
represented by 72 calculated features. The DT correctly classified 80.8% of the
training and 80.1% of the testing examples. Correspondingly it correctly
classified 89.2% and 87.7% of the EEG recordings."
"Evolving Cascade Neural Networks (ECNNs) and a new training algorithm capable
of selecting informative features are described. The ECNN initially learns with
one input node and then evolves by adding new inputs as well as new hidden
neurons. The resultant ECNN has a near minimal number of hidden neurons and
inputs. The algorithm is successfully used for training ECNN to recognise
artefacts in sleep electroencephalograms (EEGs) which were visually labelled by
EEG-viewers. In our experiments, the ECNN outperforms the standard
neural-network as well as evolutionary techniques."
"The optimal complexity of neural networks is achieved when the
self-organization principles is used to eliminate the contradictions existing
in accordance with the K. Godel theorem about incompleteness of the systems
based on axiomatics. The principle of S. Beer exterior addition the Heuristic
Group Method of Data Handling by A. Ivakhnenko realized is used."
"We report the first results of simulating the coupling of neuronal,
astrocyte, and cerebrovascular activity. It is suggested that the dynamics of
the system is different from systems that only include neurons. In the
neuron-vascular coupling, distribution of synapse strengths affects neuronal
behavior and thus balance of the blood flow; oscillations are induced in the
neuron-to-astrocyte coupling."
"We propose a Self-Regulated Swarm (SRS) algorithm which hybridizes the
advantageous characteristics of Swarm Intelligence as the emergence of a
societal environmental memory or cognitive map via collective pheromone laying
in the landscape (properly balancing the exploration/exploitation nature of our
dynamic search strategy), with a simple Evolutionary mechanism that trough a
direct reproduction procedure linked to local environmental features is able to
self-regulate the above exploratory swarm population, speeding it up globally.
In order to test his adaptive response and robustness, we have recurred to
different dynamic multimodal complex functions as well as to Dynamic
Optimization Control problems, measuring reaction speeds and performance. Final
comparisons were made with standard Genetic Algorithms (GAs), Bacterial
Foraging strategies (BFOA), as well as with recent Co-Evolutionary approaches.
SRS's were able to demonstrate quick adaptive responses, while outperforming
the results obtained by the other approaches. Additionally, some successful
behaviors were found. One of the most interesting illustrate that the present
SRS collective swarm of bio-inspired ant-like agents is able to track about 65%
of moving peaks traveling up to ten times faster than the velocity of a single
individual composing that precise swarm tracking system."
"In a Spiking Neural Networks (SNN), spike emissions are sparsely and
irregularly distributed both in time and in the network architecture. Since a
current feature of SNNs is a low average activity, efficient implementations of
SNNs are usually based on an Event-Driven Simulation (EDS). On the other hand,
simulations of large scale neural networks can take advantage of distributing
the neurons on a set of processors (either workstation cluster or parallel
computer). This article presents DAMNED, a large scale SNN simulation framework
able to gather the benefits of EDS and parallel computing. Two levels of
parallelism are combined: Distributed mapping of the neural topology, at the
network level, and local multithreaded allocation of resources for simultaneous
processing of events, at the neuron level. Based on the causality of events, a
distributed solution is proposed for solving the complex problem of scheduling
without synchronization barrier."
"In this paper, inspired from our previous algorithm, which was based on the
theory of Tsallis statistical mechanics, we develop a new evolving stochastic
learning algorithm for neural networks. The new algorithm combines
deterministic and stochastic search steps by employing a different adaptive
stepsize for each network weight, and applies a form of noise that is
characterized by the nonextensive entropic index q, regulated by a weight decay
term. The behavior of the learning algorithm can be made more stochastic or
deterministic depending on the trade off between the temperature T and the q
values. This is achieved by introducing a formula that defines a
time--dependent relationship between these two important learning parameters.
Our experimental study verifies that there are indeed improvements in the
convergence speed of this new evolving stochastic learning algorithm, which
makes learning faster than using the original Hybrid Learning Scheme (HLS). In
addition, experiments are conducted to explore the influence of the entropic
index q and temperature T on the convergence speed and stability of the
proposed method."
"This paper presents a review of instantaneously trained neural networks
(ITNNs). These networks trade learning time for size and, in the basic model, a
new hidden node is created for each training sample. Various versions of the
corner-classification family of ITNNs, which have found applications in
artificial intelligence (AI), are described. Implementation issues are also
considered."
"A method of {\it topological grammars} is proposed for multidimensional data
approximation. For data with complex topology we define a {\it principal cubic
complex} of low dimension and given complexity that gives the best
approximation for the dataset. This complex is a generalization of linear and
non-linear principal manifolds and includes them as particular cases. The
problem of optimal principal complex construction is transformed into a series
of minimization problems for quadratic functionals. These quadratic functionals
have a physically transparent interpretation in terms of elastic energy. For
the energy computation, the whole complex is represented as a system of nodes
and springs. Topologically, the principal complex is a product of
one-dimensional continuums (represented by graphs), and the grammars describe
how these continuums transform during the process of optimal complex
construction. This factorization of the whole process onto one-dimensional
transformations using minimization of quadratic energy functionals allow us to
construct efficient algorithms."
"In this paper we present a deeper analysis than has previously been carried
out of a selective attention problem, and the evolution of continuous-time
recurrent neural networks to solve it. We show that the task has a rich
structure, and agents must solve a variety of subproblems to perform well. We
consider the relationship between the complexity of an agent and the ease with
which it can evolve behavior that generalizes well across subproblems, and
demonstrate a shaping protocol that improves generalization."
"In 1960s V.Geodakian proposed a theory that explains sexes as a mechanism for
evolutionary adaptation of the species to changing environmental conditions. In
2001 V.Iskrin refined and augmented the concepts of Geodakian and gave a new
and interesting explanation to several phenomena which involve sex, and sex
ratio, including the war-years phenomena. He also introduced a new concept of
the ""catastrophic sex ratio."" This note is an attempt to digest technical
aspects of the new ideas by Iskrin."
"We develop a Genetic Programming-based methodology that enables discovery of
novel functional forms for classical inter-atomic force-fields, used in
molecular dynamics simulations. Unlike previous efforts in the field, that fit
only the parameters to the fixed functional forms, we instead use a novel
algorithm to search the space of many possible functional forms. While a
follow-on practical procedure will use experimental and {\it ab inito} data to
find an optimal functional form for a forcefield, we first validate the
approach using a manufactured solution. This validation has the advantage of a
well-defined metric of success. We manufactured a training set of atomic
coordinate data with an associated set of global energies using the well-known
Lennard-Jones inter-atomic potential. We performed an automatic functional form
fitting procedure starting with a population of random functions, using a
genetic programming functional formulation, and a parallel tempering
Metropolis-based optimization algorithm. Our massively-parallel method
independently discovered the Lennard-Jones function after searching for several
hours on 100 processors and covering a miniscule portion of the configuration
space. We find that the method is suitable for unsupervised discovery of
functional forms for inter-atomic potentials/force-fields. We also find that
our parallel tempering Metropolis-based approach significantly improves the
optimization convergence time, and takes good advantage of the parallel cluster
architecture."
"In evolutionary algorithms, the fitness of a population increases with time
by mutating and recombining individuals and by a biased selection of more fit
individuals. The right selection pressure is critical in ensuring sufficient
optimization progress on the one hand and in preserving genetic diversity to be
able to escape from local optima on the other hand. Motivated by a universal
similarity relation on the individuals, we propose a new selection scheme,
which is uniform in the fitness values. It generates selection pressure toward
sparsely populated fitness regions, not necessarily toward higher fitness, as
is the case for all other selection schemes. We show analytically on a simple
example that the new selection scheme can be much more effective than standard
selection schemes. We also propose a new deletion scheme which achieves a
similar result via deletion and show how such a scheme preserves genetic
diversity more effectively than standard approaches. We compare the performance
of the new schemes to tournament selection and random deletion on an artificial
deceptive problem and a range of NP-hard problems: traveling salesman, set
covering and satisfiability."
"This paper presents the design of an associative memory with feedback that is
capable of on-line temporal sequence learning. A framework for on-line sequence
learning has been proposed, and different sequence learning models have been
analysed according to this framework. The network model is an associative
memory with a separate store for the sequence context of a symbol. A sparse
distributed memory is used to gain scalability. The context store combines the
functionality of a neural layer with a shift register. The sensitivity of the
machine to the sequence context is controllable, resulting in different
characteristic behaviours. The model can store and predict on-line sequences of
various types and length. Numerical simulations on the model have been carried
out to determine its properties."
"Evolutionary processes proved very useful for solving optimization problems.
In this work, we build a formalization of the notion of cooperation and
competition of multiple systems working toward a common optimization goal of
the population using evolutionary computation techniques. It is justified that
evolutionary algorithms are more expressive than conventional recursive
algorithms. Three subclasses of evolutionary algorithms are proposed here:
bounded finite, unbounded finite and infinite types. Some results on
completeness, optimality and search decidability for the above classes are
presented. A natural extension of Evolutionary Turing Machine model developed
in this paper allows one to mathematically represent and study properties of
cooperation and competition in a population of optimized species."
"This article underlines the learning and discrimination capabilities of a
model of associative memory based on artificial networks of spiking neurons.
Inspired from neuropsychology and neurobiology, the model implements top-down
modulations, as in neocortical layer V pyramidal neurons, with a learning rule
based on synaptic plasticity (STDP), for performing a multimodal association
learning task. A temporal correlation method of analysis proves the ability of
the model to associate specific activity patterns to different samples of
stimulation. Even in the absence of initial learning and with continuously
varying weights, the activity patterns become stable enough for discrimination."
"It is commonly assumed that the ability to track the frequencies of a set of
schemata in the evolving population of an infinite population genetic algorithm
(IPGA) under different fitness functions will advance efforts to obtain a
theory of adaptation for the simple GA. Unfortunately, for IPGAs with long
genomes and non-trivial fitness functions there do not currently exist
theoretical results that allow such a study. We develop a simple framework for
analyzing the dynamics of an infinite population evolutionary algorithm (IPEA).
This framework derives its simplicity from its abstract nature. In particular
we make no commitment to the data-structure of the genomes, the kind of
variation performed, or the number of parents involved in a variation
operation. We use this framework to derive abstract conditions under which the
dynamics of an IPEA can be coarse-grained. We then use this result to derive
concrete conditions under which it becomes computationally feasible to closely
approximate the frequencies of a family of schemata of relatively low order
over multiple generations, even when the bitstsrings in the evolving population
of the IPGA are long."
"We demonstrate how a genetic algorithm solves the problem of minimizing the
resources used for network coding, subject to a throughput constraint, in a
multicast scenario. A genetic algorithm avoids the computational complexity
that makes the problem NP-hard and, for our experiments, greatly improves on
sub-optimal solutions of established methods. We compare two different genotype
encodings, which tradeoff search space size with fitness landscape, as well as
the associated genetic operators. Our finding favors a smaller encoding despite
its fewer intermediate solutions and demonstrates the impact of the modularity
enforced by genetic operators on the performance of the algorithm."
"The Building Block Hypothesis suggests that Genetic Algorithms (GAs) are
well-suited for hierarchical problems, where efficient solving requires proper
problem decomposition and assembly of solution from sub-solution with strong
non-linear interdependencies. The paper proposes a hill-climber operating over
the building block (BB) space that can efficiently address hierarchical
problems. The new Building Block Hill-Climber (BBHC) uses past hill-climb
experience to extract BB information and adapts its neighborhood structure
accordingly. The perpetual adaptation of the neighborhood structure allows the
method to climb the hierarchical structure solving successively the
hierarchical levels. It is expected that for fully non deceptive hierarchical
BB structures the BBHC can solve hierarchical problems in linearithmic time.
Empirical results confirm that the proposed method scales almost linearly with
the problem size thus clearly outperforms population based recombinative
methods."
"This work is an attempt for a state-of-the-art survey of natural and life
sciences with the goal to define the scope and address the central questions of
an original research program. It is focused on the phenomena of emergence,
adaptive dynamics and evolution of self-assembling, self-organizing,
self-maintaining and self-replicating biosynthetic systems viewed from a
newly-arranged perspective and understanding of computation and communication
in the living nature."
"Robustness to a wide variety of negative factors and the ability to
self-repair is an inherent and natural characteristic of all life forms on
earth. As opposed to nature, man-made systems are in most cases not inherently
robust and a significant effort has to be made in order to make them resistant
against failures. This can be done in a wide variety of ways and on various
system levels. In the field of digital systems, for example, techniques such as
triple modular redundancy (TMR) are frequently used, which results in a
considerable hardware overhead. Biologically-inspired computing by means of
bio-chemical metaphors offers alternative paradigms, which need to be explored
and evaluated.
  Here, we are interested to evaluate the potential of nature-inspired
artificial chemistries and membrane systems as an alternative information
representing and processing paradigm in order to obtain robust and spatially
extended Boolean computing systems in a distributed environment. We investigate
conceptual approaches inspired by artificial chemistries and membrane systems
and compare proof-of-concepts. First, we show, that elementary logical
functions can be implemented. Second, we illustrate how they can be made more
robust and how they can be assembled to larger-scale systems. Finally, we
discuss the implications for and paths to possible genuine implementations.
Compared to the main body of work in artificial chemistries, we take a very
pragmatic and implementation-oriented approach and are interested in realizing
Boolean computations only. The results emphasize that artificial chemistries
can be used to implement Boolean logic in a spatially extended and distributed
environment and can also be made highly robust, but at a significant price."
"The intelligent acoustic emission locator is described in Part I, while Part
II discusses blind source separation, time delay estimation and location of two
simultaneously active continuous acoustic emission sources.
  The location of acoustic emission on complicated aircraft frame structures is
a difficult problem of non-destructive testing. This article describes an
intelligent acoustic emission source locator. The intelligent locator comprises
a sensor antenna and a general regression neural network, which solves the
location problem based on learning from examples. Locator performance was
tested on different test specimens. Tests have shown that the accuracy of
location depends on sound velocity and attenuation in the specimen, the
dimensions of the tested area, and the properties of stored data. The location
accuracy achieved by the intelligent locator is comparable to that obtained by
the conventional triangulation method, while the applicability of the
intelligent locator is more general since analysis of sonic ray paths is
avoided. This is a promising method for non-destructive testing of aircraft
frame structures by the acoustic emission method."
"Part I describes an intelligent acoustic emission locator, while Part II
discusses blind source separation, time delay estimation and location of two
continuous acoustic emission sources.
  Acoustic emission (AE) analysis is used for characterization and location of
developing defects in materials. AE sources often generate a mixture of various
statistically independent signals. A difficult problem of AE analysis is
separation and characterization of signal components when the signals from
various sources and the mode of mixing are unknown. Recently, blind source
separation (BSS) by independent component analysis (ICA) has been used to solve
these problems. The purpose of this paper is to demonstrate the applicability
of ICA to locate two independent simultaneously active acoustic emission
sources on an aluminum band specimen. The method is promising for
non-destructive testing of aircraft frame structures by acoustic emission
analysis."
"Advances in semiconductor technology are contributing to the increasing
complexity in the design of embedded systems. Architectures with novel
techniques such as evolvable nature and autonomous behavior have engrossed lot
of attention. This paper demonstrates conceptually evolvable embedded systems
can be characterized basing on acausal nature. It is noted that in acausal
systems, future input needs to be known, here we make a mechanism such that the
system predicts the future inputs and exhibits pseudo acausal nature. An
embedded system that uses theoretical framework of acausality is proposed. Our
method aims at a novel architecture that features the hardware evolability and
autonomous behavior alongside pseudo acausality. Various aspects of this
architecture are discussed in detail along with the limitations."
"We present a genetic algorithm which is distributed in two novel ways: along
genotype and temporal axes. Our algorithm first distributes, for every member
of the population, a subset of the genotype to each network node, rather than a
subset of the population to each. This genotype distribution is shown to offer
a significant gain in running time. Then, for efficient use of the
computational resources in the network, our algorithm divides the candidate
solutions into pipelined sets and thus the distribution is in the temporal
domain, rather that in the spatial domain. This temporal distribution may lead
to temporal inconsistency in selection and replacement, however our experiments
yield better efficiency in terms of the time to convergence without incurring
significant penalties."
"This paper describes a systems architecture for a hybrid Centralised/Swarm
based multi-agent system. The issue of local goal assignment for agents is
investigated through the use of a global agent which teaches the agents
responses to given situations. We implement a test problem in the form of a
Pursuit game, where the Multi-Agent system is a set of captor agents. The
agents learn solutions to certain board positions from the global agent if they
are unable to find a solution. The captor agents learn through the use of
multi-layer perceptron neural networks. The global agent is able to solve board
positions through the use of a Genetic Algorithm. The cooperation between
agents and the results of the simulation are discussed here. ."
"The work proposes the application of neural networks with particle swarm
optimisation (PSO) and genetic algorithms (GA) to compensate for missing data
in classifying high voltage bushings. The classification is done using DGA data
from 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates
methods for oil impregnated paper (OIP) bushings. PSO and GA were compared in
terms of accuracy and computational efficiency. Both GA and PSO simulations
were able to estimate missing data values to an average 95% accuracy when only
one variable was missing. However PSO rapidly deteriorated to 66% accuracy with
two variables missing simultaneously, compared to 84% for GA. The data
estimated using GA was found to classify the conditions of bushings than the
PSO."
"We consider the computational complexity of producing the best possible
offspring in a crossover, given two solutions of the parents. The crossover
operators are studied on the class of Boolean linear programming problems,
where the Boolean vector of variables is used as the solution representation.
By means of efficient reductions of the optimized gene transmitting crossover
problems (OGTC) we show the polynomial solvability of the OGTC for the maximum
weight set packing problem, the minimum weight set partition problem and for
one of the versions of the simple plant location problem. We study a connection
between the OGTC for linear Boolean programming problem and the maximum weight
independent set problem on 2-colorable hypergraph and prove the NP-hardness of
several special cases of the OGTC problem in Boolean linear programming."
"Robotic hardware designs are becoming more complex as the variety and number
of on-board sensors increase and as greater computational power is provided in
ever-smaller packages on-board robots. These advances in hardware, however, do
not automatically translate into better software for controlling complex
robots. Evolutionary techniques hold the potential to solve many difficult
problems in robotics which defy simple conventional approaches, but present
many challenges as well. Numerous disciplines including artificial life,
cognitive science and neural networks, rule-based systems, behavior-based
control, genetic algorithms and other forms of evolutionary computation have
contributed to shaping the current state of evolutionary robotics. This paper
provides an overview of developments in the emerging field of evolutionary
robotics, and discusses some of the opportunities and challenges which
currently face practitioners in the field."
"Design, implementation, and machine learning issues associated with
developing a control system for a serpentine robotic manipulator are explored.
The controller developed provides autonomous control of the serpentine robotic
manipulatorduring operation of the manipulator within an enclosed environment
such as an underground storage tank. The controller algorithms make use of both
low-level joint angle control employing force/position feedback constraints,
and high-level coordinated control of end-effector positioning. This approach
has resulted in both high-level full robotic control and low-level telerobotic
control modes, and provides a high level of dexterity for the operator."
"In this paper we embed $m$-dimensional Euclidean space in the geometric
algebra $Cl_m $ to extend the operators of incidence in ${R^m}$ to operators of
incidence in the geometric algebra to generalize the notion of separator to a
decision boundary hyperconic in the Clifford algebra of hyperconic sections
denoted as ${Cl}({Co}_{2})$. This allows us to extend the concept of a linear
perceptron or the spherical perceptron in conformal geometry and introduce the
more general conic perceptron, namely the {elliptical perceptron}. Using
Clifford duality a vector orthogonal to the decision boundary hyperplane is
determined. Experimental results are shown in 2-dimensional Euclidean space
where we separate data that are naturally separated by some typical plane conic
separators by this procedure. This procedure is more general in the sense that
it is independent of the dimension of the input data and hence we can speak of
the hyperconic elliptic perceptron."
"In this paper, we study instances of complex neural networks, i.e. neural
netwo rks with complex topologies. We use Self-Organizing Map neural networks
whose n eighbourhood relationships are defined by a complex network, to
classify handwr itten digits. We show that topology has a small impact on
performance and robus tness to neuron failures, at least at long learning
times. Performance may howe ver be increased (by almost 10%) by artificial
evolution of the network topo logy. In our experimental conditions, the evolved
networks are more random than their parents, but display a more heterogeneous
degree distribution."
"The subcellular location of a protein can provide valuable information about
its function. With the rapid increase of sequenced genomic data, the need for
an automated and accurate tool to predict subcellular localization becomes
increasingly important. Many efforts have been made to predict protein
subcellular localization. This paper aims to merge the artificial neural
networks and bioinformatics to predict the location of protein in yeast genome.
We introduce a new subcellular prediction method based on a backpropagation
neural network. The results show that the prediction within an error limit of 5
to 10 percentage can be achieved with the system."
"The adoption of probabilistic models for the best individuals found so far is
a powerful approach for evolutionary computation. Increasingly more complex
models have been used by estimation of distribution algorithms (EDAs), which
often result better effectiveness on finding the global optima for hard
optimization problems. Supervised and unsupervised learning of Bayesian
networks are very effective options, since those models are able to capture
interactions of high order among the variables of a problem. Diversity
preservation, through niching techniques, has also shown to be very important
to allow the identification of the problem structure as much as for keeping
several global optima. Recently, clustering was evaluated as an effective
niching technique for EDAs, but the performance of simpler low-order EDAs was
not shown to be much improved by clustering, except for some simple multimodal
problems. This work proposes and evaluates a combination operator guided by a
measure from information theory which allows a clustered low-order EDA to
effectively solve a comprehensive range of benchmark optimization problems."
"The pace of progress in the fields of Evolutionary Computation and Machine
Learning is currently limited -- in the former field, by the improbability of
making advantageous extensions to evolutionary algorithms when their capacity
for adaptation is poorly understood, and in the latter by the difficulty of
finding effective semi-principled reductions of hard real-world problems to
relatively simple optimization problems. In this paper we explain why a theory
which can accurately explain the simple genetic algorithm's remarkable capacity
for adaptation has the potential to address both these limitations. We describe
what we believe to be the impediments -- historic and analytic -- to the
discovery of such a theory and highlight the negative role that the building
block hypothesis (BBH) has played. We argue based on experimental results that
a fundamental limitation which is widely believed to constrain the SGA's
adaptive ability (and is strongly implied by the BBH) is in fact illusionary
and does not exist. The SGA therefore turns out to be more powerful than it is
currently thought to be. We give conditions under which it becomes feasible to
numerically approximate and study the multivariate marginals of the search
distribution of an infinite population SGA over multiple generations even when
its genomes are long, and explain why this analysis is relevant to the riddle
of the SGA's remarkable adaptive abilities."