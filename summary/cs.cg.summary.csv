summary
"Problems presented at the open-problem session of the 14th Annual ACM
Symposium on Computational Geometry are listed."
"We present an algorithm for maintaining the width of a planar point set
dynamically, as points are inserted or deleted. Our algorithm takes time
O(kn^epsilon) per update, where k is the amount of change the update causes in
the convex hull, n is the number of points in the set, and epsilon is any
arbitrarily small constant. For incremental or decremental update sequences,
the amortized time per update is O(n^epsilon)."
"We study the problem of moving a vertex in an unstructured mesh of
triangular, quadrilateral, or tetrahedral elements to optimize the shapes of
adjacent elements. We show that many such problems can be solved in linear time
using generalized linear programming. We also give efficient algorithms for
some mesh smoothing problems that do not fit into the generalized linear
programming paradigm."
"We show that any polyhedron forming a topological ball with an even number of
quadrilateral sides can be partitioned into O(n) topological cubes, meeting
face to face. The result generalizes to non-simply-connected polyhedra
satisfying an additional bipartiteness condition. The same techniques can also
be used to reduce the geometric version of the hexahedral mesh generation
problem to a finite case analysis amenable to machine solution."
"We use here the results on the influence graph by Boissonnat et al. to adapt
them for particular cases where additional information is available. In some
cases, it is possible to improve the expected randomized complexity of
algorithms from O(n log n) to O(n log star n).
  This technique applies in the following applications: triangulation of a
simple polygon, skeleton of a simple polygon, Delaunay triangulation of points
knowing the EMST (euclidean minimum spanning tree)."
"We present an empirical analysis of data structures for approximate nearest
neighbor searching. We compare the well-known optimized kd-tree splitting
method against two alternative splitting methods. The first, called the
sliding-midpoint method, which attempts to balance the goals of producing
subdivision cells of bounded aspect ratio, while not producing any empty cells.
The second, called the minimum-ambiguity method is a query-based approach. In
addition to the data points, it is also given a training set of query points
for preprocessing. It employs a simple greedy algorithm to select the splitting
plane that minimizes the average amount of ambiguity in the choice of the
nearest neighbor for the training points. We provide an empirical analysis
comparing these two methods against the optimized kd-tree construction for a
number of synthetically generated data and query sets. We demonstrate that for
clustered data and query sets, these algorithms can provide significant
improvements over the standard kd-tree construction for approximate nearest
neighbor searching."
"The subquadratic algorithm of Kapoor for finding shortest paths on a
polyhedron is described."
"This paper presents how the space of spheres and shelling may be used to
delete a point from a $d$-dimensional triangulation efficiently. In dimension
two, if k is the degree of the deleted vertex, the complexity is O(k log k),
but we notice that this number only applies to low cost operations, while time
consuming computations are only done a linear number of times.
  This algorithm may be viewed as a variation of Heller's algorithm, which is
popular in the geographic information system community. Unfortunately, Heller
algorithm is false, as explained in this paper."
"We propose a new data structure to compute the Delaunay triangulation of a
set of points in the plane. It combines good worst case complexity, fast
behavior on real data, and small memory occupation.
  The location structure is organized into several levels. The lowest level
just consists of the triangulation, then each level contains the triangulation
of a small sample of the levels below. Point location is done by marching in a
triangulation to determine the nearest neighbor of the query at that level,
then the march restarts from that neighbor at the level below. Using a small
sample (3%) allows a small memory occupation; the march and the use of the
nearest neighbor to change levels quickly locate the query."
"We provide a lower bound construction showing that the union of unit balls in
three-dimensional space has quadratic complexity, even if they all contain the
origin. This settles a conjecture of Sharir."
"An efficient technique to solve precision problems consists in using exact
computations. For geometric predicates, using systematically expensive exact
computations can be avoided by the use of filters. The predicate is first
evaluated using rounding computations, and an error estimation gives a
certificate of the validity of the result. In this note, we studies the
statistical efficiency of filters for cosphericity predicate with an assumption
of regular distribution of the points. We prove that the expected value of the
polynomial corresponding to the in sphere test is greater than epsilon with
probability O(epsilon log 1/epsilon) improving the results of a previous paper
by the same authors."
"The assumption of real-number arithmetic, which is at the basis of
conventional geometric algorithms, has been seriously challenged in recent
years, since digital computers do not exhibit such capability.
  A geometric predicate usually consists of evaluating the sign of some
algebraic expression. In most cases, rounded computations yield a reliable
result, but sometimes rounded arithmetic introduces errors which may invalidate
the algorithms. The rounded arithmetic may produce an incorrect result only if
the exact absolute value of the algebraic expression is smaller than some
(small) varepsilon, which represents the largest error that may arise in the
evaluation of the expression. The threshold varepsilon depends on the structure
of the expression and on the adopted computer arithmetic, assuming that the
input operands are error-free.
  A pair (arithmetic engine,threshold) is an ""arithmetic filter"". In this paper
we develop a general technique for assessing the efficacy of an arithmetic
filter. The analysis consists of evaluating both the threshold and the
probability of failure of the filter.
  To exemplify the approach, under the assumption that the input points be
chosen randomly in a unit ball or unit cube with uniform density, we analyze
the two important predicates ""which-side"" and ""insphere"". We show that the
probability that the absolute values of the corresponding determinants be no
larger than some positive value V, with emphasis on small V, is Theta(V) for
the which-side predicate, while for the insphere predicate it is Theta(V^(2/3))
in dimension 1, O(sqrt(V)) in dimension 2, and O(sqrt(V) ln(1/V)) in higher
dimensions. Constants are small, and are given in the paper."
"We describe simple linear time algorithms for coloring the squares of
balanced and unbalanced quadtrees so that no two adjacent squares are given the
same color. If squares sharing sides are defined as adjacent, we color balanced
quadtrees with three colors, and unbalanced quadtrees with four colors; these
results are both tight, as some quadtrees require this many colors. If squares
sharing corners are defined as adjacent, we color balanced or unbalanced
quadtrees with six colors; for some quadtrees, at least five colors are
required."
"Two results in ""computational origami"" are illustrated."
Open problems from the 15th Annual ACM Symposium on Computational Geometry.
"We use circle-packing methods to generate quadrilateral meshes for polygonal
domains, with guaranteed bounds both on the quality and the number of elements.
We show that these methods can generate meshes of several types: (1) the
elements form the cells of a Voronoi diagram, (2) all elements have two
opposite right angles, (3) all elements are kites, or (4) all angles are at
most 120 degrees. In each case the total number of elements is O(n), where n is
the number of input vertices."
"We consider the motion planning problem for a point constrained to move along
a smooth closed convex path of bounded curvature. The workspace of the moving
point is bounded by a convex polygon with m vertices, containing an obstacle in
a form of a simple polygon with $n$ vertices. We present an O(m+n) time
algorithm finding the path, going around the obstacle, whose curvature is the
smallest possible."
"A circle $C$ separates two planar sets if it encloses one of the sets and its
open interior disk does not meet the other set. A separating circle is a
largest one if it cannot be locally increased while still separating the two
given sets. An Theta(n log n) optimal algorithm is proposed to find all largest
circles separating two given sets of line segments when line segments are
allowed to meet only at their endpoints. In the general case, when line
segments may intersect $\Omega(n^2)$ times, our algorithm can be adapted to
work in O(n alpha(n) log n) time and O(n \alpha(n)) space, where alpha(n)
represents the extremely slowly growing inverse of the Ackermann function."
"We study the problem of computing the free space F of a simple legged robot
called the spider robot. The body of this robot is a single point and the legs
are attached to the body. The robot is subject to two constraints: each leg has
a maximal extension R (accessibility constraint) and the body of the robot must
lie above the convex hull of its feet (stability constraint). Moreover, the
robot can only put its feet on some regions, called the foothold regions. The
free space F is the set of positions of the body of the robot such that there
exists a set of accessible footholds for which the robot is stable. We present
an efficient algorithm that computes F in O(n2 log n) time using O(n2 alpha(n))
space for n discrete point footholds where alpha(n) is an extremely slowly
growing function (alpha(n) <= 3 for any practical value of n). We also present
an algorithm for computing F when the foothold regions are pairwise disjoint
polygons with n edges in total. This algorithm computes F in O(n2 alpha8(n) log
n) time using O(n2 alpha8(n)) space (alpha8(n) is also an extremely slowly
growing function). These results are close to optimal since Omega(n2) is a
lower bound for the size of F."
"Two planar sets are circularly separable if there exists a circle enclosing
one of the sets and whose open interior disk does not intersect the other set.
  This paper studies two problems related to circular separability. A
linear-time algorithm is proposed to decide if two polygons are circularly
separable. The algorithm outputs the smallest separating circle. The second
problem asks for the largest circle included in a preprocessed, convex polygon,
under some point and/or line constraints. The resulting circle must contain the
query points and it must lie in the halfplanes delimited by the query lines."
"Given a finite set of non-collinear points in the plane, there exists a line
that passes through exactly two points. Such a line is called an ordinary line.
An efficient algorithm for computing such a line was proposed by Mukhopadhyay
et al. In this note we extend this result in two directions. We first show how
to use this algorithm to compute an ordinary conic, that is, a conic passing
through exactly five points, assuming that all the points do not lie on the
same conic. Both our proofs of existence and the consequent algorithms are
simpler than previous ones. We next show how to compute an ordinary hyperplane
in three and higher dimensions."
"In this paper we present algorithms for a number of problems in geometric
pattern matching where the input consist of a collections of segments in the
plane. Our work consists of two main parts. In the first, we address problems
and measures that relate to collections of orthogonal line segments in the
plane. Such collections arise naturally from problems in mapping buildings and
robot exploration.
  We propose a new measure of segment similarity called a \emph{coverage
measure}, and present efficient algorithms for maximising this measure between
sets of axis-parallel segments under translations. Our algorithms run in time
$O(n^3\polylog n)$ in the general case, and run in time $O(n^2\polylog n)$ for
the case when all segments are horizontal. In addition, we show that when
restricted to translations that are only vertical, the Hausdorff distance
between two sets of horizontal segments can be computed in time roughly
$O(n^{3/2}{\sl polylog}n)$. These algorithms form significant improvements over
the general algorithm of Chew et al. that takes time $O(n^4 \log^2 n)$. In the
second part of this paper we address the problem of matching polygonal chains.
We study the well known \Frd, and present the first algorithm for computing the
\Frd under general translations. Our methods also yield algorithms for
computing a generalization of the \Fr distance, and we also present a simple
approximation algorithm for the \Frd that runs in time $O(n^2\polylog n)$."
"We give algorithms for computing the regression depth of a k-flat for a set
of n points in R^d. The running time is O(n^(d-2) + n log n) when 0 < k < d-1,
faster than the best time bound for hyperplane regression or for data depth."
"It has recently been established by Below, De Loera, and Richter-Gebert that
finding a minimum size (or even just a small) triangulation of a convex
polyhedron is NP-complete. Their 3SAT-reduction proof is discussed."
"We give linear-time quasiconvex programming algorithms for finding a Moebius
transformation of a set of spheres in a unit ball or on the surface of a unit
sphere that maximizes the minimum size of a transformed sphere. We can also use
similar methods to maximize the minimum distance among a set of pairs of input
points. We apply these results to vertex separation and symmetry display in
spherical graph drawing, viewpoint selection in hyperbolic browsing, element
size control in conformal structured mesh generation, and brain flat mapping."
"We consider the complexity of Delaunay triangulations of sets of points in
R^3 under certain practical geometric constraints. The spread of a set of
points is the ratio between the longest and shortest pairwise distances. We
show that in the worst case, the Delaunay triangulation of n points in R^3 with
spread D has complexity Omega(min{D^3, nD, n^2}) and O(min{D^4, n^2}). For the
case D = Theta(sqrt{n}), our lower bound construction consists of a uniform
sample of a smooth convex surface with bounded curvature. We also construct a
family of smooth connected surfaces such that the Delaunay triangulation of any
good point sample has near-quadratic complexity."
"We define and examine flip operations for quadrilateral and hexahedral
meshes, similar to the flipping transformations previously used in triangular
and tetrahedral mesh generation."
"In this paper, we give upper and lower bounds on the number of Steiner points
required to construct a strictly convex quadrilateral mesh for a planar point
set. In particular, we show that $3{\lfloor\frac{n}{2}\rfloor}$ internal
Steiner points are always sufficient for a convex quadrilateral mesh of $n$
points in the plane. Furthermore, for any given $n\geq 4$, there are point sets
for which $\lceil\frac{n-3}{2}\rceil-1$ Steiner points are necessary for a
convex quadrilateral mesh."
"In this note we consider the problem of manufacturing a convex polyhedral
object via casting. We consider a generalization of the sand casting process
where the object is manufactured by gluing together two identical faces of
parts cast with a single piece mold. In this model we show that the class of
convex polyhedra which can be enclosed between two concentric spheres of the
ratio of their radii less than 1.07 cannot be manufactured using only two cast
parts."
"We examine a computational geometric problem concerning the structure of
polymers. We model a polymer as a polygonal chain in three dimensions. Each
edge splits the polymer into two subchains, and a dihedral rotation rotates one
of these chains rigidly about this edge. The problem is to determine, given a
chain, an edge, and an angle of rotation, if the motion can be performed
without causing the chain to self-intersect. An Omega(n log n) lower bound on
the time complexity of this problem is known.
  We prove that preprocessing a chain of n edges and answering n dihedral
rotation queries is 3SUM-hard, giving strong evidence that solving n queries
requires Omega(n^2) time in the worst case. For dynamic queries, which also
modify the chain if the requested dihedral rotation is feasible, we show that
answering n queries is by itself 3SUM-hard, suggesting that sublinear query
time is impossible after any amount of preprocessing."
"This paper addresses the problem of finding shortest paths homotopic to a
given disjoint set of paths that wind amongst point obstacles in the plane. We
present a faster algorithm than previously known."
"We present an algorithm to construct meshes suitable for space-time
discontinuous Galerkin finite-element methods. Our method generalizes and
improves the `Tent Pitcher' algorithm of \""Ung\""or and Sheffer. Given an
arbitrary simplicially meshed domain M of any dimension and a time interval
[0,T], our algorithm builds a simplicial mesh of the space-time domain Mx[0,T],
in constant time per element. Our algorithm avoids the limitations of previous
methods by carefully adapting the durations of space-time elements to the local
quality and feature size of the underlying space mesh."
"We consider the problem of simultaneous embedding of planar graphs. There are
two variants of this problem, one in which the mapping between the vertices of
the two graphs is given and another where the mapping is not given. In
particular, we show that without mapping, any number of outerplanar graphs can
be embedded simultaneously on an $O(n)\times O(n)$ grid, and an outerplanar and
general planar graph can be embedded simultaneously on an $O(n^2)\times O(n^3)$
grid. If the mapping is given, we show how to embed two paths on an $n \times
n$ grid, a caterpillar and a path on an $n \times 2n$ grid, or two caterpillar
graphs on an $O(n^2)\times O(n^3)$ grid. We also show that 5 paths, or 3
caterpillars, or two general planar graphs cannot be simultaneously embedded
given the mapping."
"Traditional representations of graphs and their duals suggest the requirement
that the dual vertices be placed inside their corresponding primal faces, and
the edges of the dual graph cross only their corresponding primal edges. We
consider the problem of simultaneously embedding a planar graph and its dual
into a small integer grid such that the edges are drawn as straight-line
segments and the only crossings are between primal-dual pairs of edges. We
provide a linear-time algorithm that simultaneously embeds a 3-connected planar
graph and its dual on a (2n-2) by (2n-2) integer grid, where n is the total
number of vertices in the graph and its dual. Furthermore our embedding
algorithm satisfies the two natural requirements mentioned above."
"In this paper, we analyze the complexity of natural parallelizations of
Delaunay refinement methods for mesh generation. The parallelizations employ a
simple strategy: at each iteration, they choose a set of ``independent'' points
to insert into the domain, and then update the Delaunay triangulation. We show
that such a set of independent points can be constructed efficiently in
parallel and that the number of iterations needed is $O(\log^2(L/s))$, where
$L$ is the diameter of the domain, and $s$ is the smallest edge in the output
mesh. In addition, we show that the insertion of each independent set of points
can be realized sequentially by Ruppert's method in two dimensions and
Shewchuk's in three dimensions. Therefore, our parallel Delaunay refinement
methods provide the same element quality and mesh size guarantees as the
sequential algorithms in both two and three dimensions. For quasi-uniform
meshes, such as those produced by Chew's method, we show that the number of
iterations can be reduced to $O(\log(L/s))$. To the best of our knowledge,
these are the first provably polylog$(L/s)$ parallel time Delaunay meshing
algorithms that generate well-shaped meshes of size optimal to within a
constant."
"We propose an interpolation method that is invariant under Moebius
transformations; that is, interpolation followed by transformation gives the
same result as transformation followed by interpolation. The method uses
natural (Delaunay) neighbors, but weights neighbors according to angles formed
by Delaunay circles."
"We survey most of the different types of approximation algorithms which
minimize the number of output vertices. We present their main qualities and
their inherent drawbacks."
"The well-known $O(n^{1-1/d})$ behaviour of the optimal tour length for TSP in
d-dimensional Cartesian space causes breaches of the triangle inequality. Other
practical inadequacies of this model are discussed, including its use as basis
for approximation of the TSP optimal tour length or bounds derivations, which I
attempt to remedy."
"We study algorithmic aspects of bending wires and sheet metal into a
specified structure. Problems of this type are closely related to the question
of deciding whether a simple non-self-intersecting wire structure (a
carpenter's ruler) can be straightened, a problem that was open for several
years and has only recently been solved in the affirmative.
  If we impose some of the constraints that are imposed by the manufacturing
process, we obtain quite different results. In particular, we study the variant
of the carpenter's ruler problem in which there is a restriction that only one
joint can be modified at a time. For a linkage that does not self-intersect or
self-touch, the recent results of Connelly et al. and Streinu imply that it can
always be straightened, modifying one joint at a time. However, we show that
for a linkage with even a single vertex degeneracy, it becomes NP-hard to
decide if it can be straightened while altering only one joint at a time. If we
add the restriction that each joint can be altered at most once, we show that
the problem is NP-complete even without vertex degeneracies.
  In the special case, arising in wire forming manufacturing, that each joint
can be altered at most once, and must be done sequentially from one or both
ends of the linkage, we give an efficient algorithm to determine if a linkage
can be straightened."
"We introduce relaxed scheduling as a paradigm for mesh maintenance and
demonstrate its applicability to triangulating a skin surface in $\Rspace^3$."
"In the first part of the paper, we present an (1+\mu)-approximation algorithm
to the minimum-spanning tree of points in a planar arrangement of lines, where
the metric is the number of crossings between the spanning tree and the lines.
The expected running time is O((n/\mu^5) alpha^3(n) log^5 n), where \mu > 0 is
a prescribed constant.
  In the second part of our paper, we show how to embed such a crossing metric,
into high-dimensions, so that the distances are preserved. As a result, we can
deploy a large collection of subquadratic approximations algorithms \cite
im-anntr-98,giv-rahdg-99 for problems involving points with the crossing metric
as a distance function. Applications include matching, clustering,
nearest-neighbor, and furthest-neighbor."
"We explore an instance of the question of partitioning a polygon into pieces,
each of which is as ``circular'' as possible, in the sense of having an aspect
ratio close to 1. The aspect ratio of a polygon is the ratio of the diameters
of the smallest circumscribing circle to the largest inscribed disk. The
problem is rich even for partitioning regular polygons into convex pieces, the
focus of this paper. We show that the optimal (most circular) partition for an
equilateral triangle has an infinite number of pieces, with the lower bound
approachable to any accuracy desired by a particular finite partition. For
pentagons and all regular k-gons, k > 5, the unpartitioned polygon is already
optimal. The square presents an interesting intermediate case. Here the
one-piece partition is not optimal, but nor is the trivial lower bound
approachable. We narrow the optimal ratio to an aspect-ratio gap of 0.01082
with several somewhat intricate partitions."
"The open problem of whether or not every pair of equal-area polygons has a
hinged dissection is discussed."
"We show how to test the bipartiteness of an intersection graph of n line
segments or simple polygons in the plane, or of balls in R^d, in time O(n log
n). More generally we find subquadratic algorithms for connectivity and
bipartiteness testing of intersection graphs of a broad class of geometric
objects. For unit balls in R^d, connectivity testing has equivalent randomized
complexity to construction of Euclidean minimum spanning trees, and hence is
unlikely to be solved as efficiently as bipartiteness testing. For line
segments or planar disks, testing k-colorability of intersection graphs for k>2
is NP-complete."
"We present memory-efficient deterministic algorithms for constructing
epsilon-nets and epsilon-approximations of streams of geometric data. Unlike
probabilistic approaches, these deterministic samples provide guaranteed bounds
on their approximation factors. We show how our deterministic samples can be
used to answer approximate online iceberg geometric queries on data streams. We
use these techniques to approximate several robust statistics of geometric data
streams, including Tukey depth, simplicial depth, regression depth, the
Thiel-Sen estimator, and the least median of squares. Our algorithms use only a
polylogarithmic amount of memory, provided the desired approximation factors
are inverse-polylogarithmic. We also include a lower bound for non-iceberg
geometric queries."
"We show that the traditional criterion for a simplex to belong to the
Delaunay triangulation of a point set is equivalent to a criterion which is a
priori weaker. The argument is quite general; as well as the classical
Euclidean case, it applies to hyperbolic and hemispherical geometries and to
Edelsbrunner's weighted Delaunay triangulation. In spherical geometry, we
establish a similar theorem under a genericity condition. The weak definition
finds natural application in the problem of approximating a point-cloud data
set with a simplical complex."
"Suppose $<A_i, \vec{c}_i>$ are planar (convex) H-polyhedra, that is, $A_i \in
\mathbb{R}^{n_i \times 2}$ and $\vec{c}_i \in \mathbb{R}^{n_i}$. Let $P_i =
\{\vec{x} \in \mathbb{R}^2 \mid A_i\vec{x} \leq \vec{c}_i \}$ and $n = n_1 +
n_2$. We present an $O(n \log n)$ algorithm for calculating an H-polyhedron
$<A, \vec{c}>$ with the smallest $P = \{\vec{x} \in \mathbb{R}^2 \mid A\vec{x}
\leq \vec{c} \}$ such that $P_1 \cup P_2 \subseteq P$."
"We study the problem of covering a given set of $n$ points in a high,
$d$-dimensional space by the minimum enclosing polytope of a given arbitrary
shape. We present algorithms that work for a large family of shapes, provided
either only translations and no rotations are allowed, or only rotation about a
fixed point is allowed; that is, one is allowed to only scale and translate a
given shape, or scale and rotate the shape around a fixed point. Our algorithms
start with a polytope guessed to be of optimal size and iteratively moves it
based on a greedy principle: simply move the current polytope directly towards
any outside point till it touches the surface. For computing the minimum
enclosing ball, this gives a simple greedy algorithm with running time
$O(nd/\eps)$ producing a ball of radius $1+\eps$ times the optimal. This simple
principle generalizes to arbitrary convex shape when only translations are
allowed, requiring at most $O(1/\eps^2)$ iterations. Our algorithm implies that
{\em core-sets} of size $O(1/\eps^2)$ exist not only for minimum enclosing ball
but also for any convex shape with a fixed orientation. A {\em Core-Set} is a
small subset of $poly(1/\eps)$ points whose minimum enclosing polytope is
almost as large as that of the original points. Although we are unable to
combine our techniques for translations and rotations for general shapes, for
the min-cylinder problem, we give an algorithm similar to the one in
\cite{HV03}, but with an improved running time of $2^{O(\frac{1}{\eps^2}\log
\frac{1}{\eps})} nd$."
"The dilation of a Euclidean graph is defined as the ratio of distance in the
graph divided by distance in R^d. In this paper we consider the problem of
positioning the root of a star such that the dilation of the resulting star is
minimal. We present a deterministic O(n log n)-time algorithm for evaluating
the dilation of a given star; a randomized O(n log n) expected-time algorithm
for finding an optimal center in R^d; and for the case d=2, a randomized O(n
2^(alpha(n)) log^2 n) expected-time algorithm for finding an optimal center
among the input points."
"We define quasiconvex programming, a form of generalized linear programming
in which one seeks the point minimizing the pointwise maximum of a collection
of quasiconvex functions. We survey algorithms for solving quasiconvex programs
either numerically or via generalizations of the dual simplex method from
linear programming, and describe varied applications of this geometric
optimization technique in meshing, scientific computation, information
visualization, automated algorithm analysis, and robust statistics."
"We propose a new refinement algorithm to generate size-optimal
quality-guaranteed Delaunay triangulations in the plane. The algorithm takes
$O(n \log n + m)$ time, where $n$ is the input size and $m$ is the output size.
This is the first time-optimal Delaunay refinement algorithm."
"In the study of depth functions it is important to decide whether we want
such a function to be sensitive to multimodality or not. In this paper we
analyze the Delaunay depth function, which is sensitive to multimodality and
compare this depth with others, as convex depth and location depth. We study
the stratification that Delaunay depth induces in the point set (layers) and in
the whole plane (levels), and we develop an algorithm for computing the
Delaunay depth contours, associated to a point set in the plane, with running
time O(n log^2 n). The depth of a query point p with respect to a data set S in
the plane is the depth of p in the union of S and p. When S and p are given in
the input the Delaunay depth can be computed in O(n log n), and we prove that
this value is optimal."
"We give a simple proof of the following fundamental result independently due
to Fary (1948) and Wagner (1936): Every plane graph has a drawing in which
every edge is straight."
"In this note we show that the maximum number of vertices in any polyhedron
$P=\{x\in \mathbb{R}^d : Ax\leq b\}$ with $0,1$-constraint matrix $A$ and a
real vector $b$ is at most $d!$."
"We present a new multi-dimensional data structure, which we call the skip
quadtree (for point data in R^2) or the skip octree (for point data in R^d,
with constant d>2). Our data structure combines the best features of two
well-known data structures, in that it has the well-defined ""box""-shaped
regions of region quadtrees and the logarithmic-height search and update
hierarchical structure of skip lists. Indeed, the bottom level of our structure
is exactly a region quadtree (or octree for higher dimensional data). We
describe efficient algorithms for inserting and deleting points in a skip
quadtree, as well as fast methods for performing point location and approximate
range queries."
"Anomaly detection has important applications in biosurveilance and
environmental monitoring. When comparing measured data to data drawn from a
baseline distribution, merely, finding clusters in the measured data may not
actually represent true anomalies. These clusters may likely be the clusters of
the baseline distribution. Hence, a discrepancy function is often used to
examine how different measured data is to baseline data within a region. An
anomalous region is thus defined to be one with high discrepancy.
  In this paper, we present algorithms for maximizing statistical discrepancy
functions over the space of axis-parallel rectangles. We give provable
approximation guarantees, both additive and relative, and our methods apply to
any convex discrepancy function. Our algorithms work by connecting statistical
discrepancy to combinatorial discrepancy; roughly speaking, we show that in
order to maximize a convex discrepancy function over a class of shapes, one
needs only maximize a linear discrepancy function over the same set of shapes.
  We derive general discrepancy functions for data generated from a one-
parameter exponential family. This generalizes the widely-used Kulldorff scan
statistic for data from a Poisson distribution. We present an algorithm running
in $O(\smash[tb]{\frac{1}{\epsilon} n^2 \log^2 n})$ that computes the maximum
discrepancy rectangle to within additive error $\epsilon$, for the Kulldorff
scan statistic. Similar results hold for relative error and for discrepancy
functions for data coming from Gaussian, Bernoulli, and gamma distributions.
Prior to our work, the best known algorithms were exact and ran in time
$\smash[t]{O(n^4)}$."
"We generalize the tree-confluent graphs to a broader class of graphs called
Delta-confluent graphs. This class of graphs and distance-hereditary graphs, a
well-known class of graphs, coincide. Some results about the visualization of
Delta-confluent graphs are also given."
"Any planar graph has a crossing-free straight-line drawing in the plane. A
simultaneous geometric embedding of two n-vertex graphs is a straight-line
drawing of both graphs on a common set of n points, such that the edges withing
each individual graph do not cross. We consider simultaneous embeddings of two
labeled trees, with predescribed vertex correspondences, and present an
instance of such a pair that cannot be embedded. Further we provide an example
of a planar graph that cannot be embedded together with a path when vertex
correspondences are given."
"Given a metric space $(X,d_X)$, $c\ge 1$, $r>0$, and $p,q\in [0,1]$, a
distribution over mappings $\h:X\to \mathbb N$ is called a
$(r,cr,p,q)$-sensitive hash family if any two points in $X$ at distance at most
$r$ are mapped by $\h$ to the same value with probability at least $p$, and any
two points at distance greater than $cr$ are mapped by $\h$ to the same value
with probability at most $q$. This notion was introduced by Indyk and Motwani
in 1998 as the basis for an efficient approximate nearest neighbor search
algorithm, and has since been used extensively for this purpose. The
performance of these algorithms is governed by the parameter
$\rho=\frac{\log(1/p)}{\log(1/q)}$, and constructing hash families with small
$\rho$ automatically yields improved nearest neighbor algorithms. Here we show
that for $X=\ell_1$ it is impossible to achieve $\rho\le \frac{1}{2c}$. This
almost matches the construction of Indyk and Motwani which achieves $\rho\le
\frac{1}{c}$."
"To definite and compute differential invariants, like curvatures, for
triangular meshes (or polyhedral surfaces) is a key problem in CAGD and the
computer vision. The Gaussian curvature and the mean curvature are determined
by the differential of the Gauss map of the underlying surface. The Gauss map
assigns to each point in the surface the unit normal vector of the tangent
plane to the surface at this point. We follow the ideas developed in Chen and
Wu \cite{Chen2}(2004) and Wu, Chen and Chi\cite{Wu}(2005) to describe a new and
simple approach to estimate the differential of the Gauss map and curvatures
from the viewpoint of the gradient and the centroid weights. This will give us
a much better estimation of curvatures than Taubin's algorithm \cite{Taubin}
(1995)."
"We establish a new lower bound for the number of sides required for the
component curves of simple Venn diagrams made from polygons. Specifically, for
any n-Venn diagram of convex k-gons, we prove that k >= (2^n - 2 - n) / (n
(n-2)). In the process we prove that Venn diagrams of seven curves, simple or
not, cannot be formed from triangles. We then give an example achieving the new
lower bound of a (simple, symmetric) Venn diagram of seven quadrilaterals.
Previously Grunbaum had constructed a 7-Venn diagram of non-convex 5-gons
[``Venn Diagrams II'', Geombinatorics 2:25-31, 1992]."
"We present an algorithm that computes a shortest non-contractible and a
shortest non-separating cycle on an orientable combinatorial surface of bounded
genus in O(n \log n) time, where n denotes the complexity of the surface. This
solves a central open problem in computational topology, improving upon the
current-best O(n^{3/2})-time algorithm by Cabello and Mohar (ESA 2005). Our
algorithm uses universal-cover constructions to find short cycles and makes
extensive use of existing tools from the field."
"Consider a plane graph G, drawn with straight lines. For every pair a,b of
vertices of G, we compare the shortest-path distance between a and b in G (with
Euclidean edge lengths) to their actual distance in the plane. The worst-case
ratio of these two values, for all pairs of points, is called the dilation of
G. All finite plane graphs of dilation 1 have been classified. They are closely
related to the following iterative procedure. For a given point set P in R^2,
we connect every pair of points in P by a line segment and then add to P all
those points where two such line segments cross. Repeating this process
infinitely often, yields a limit point set P*.
  The main result of this paper is the following gap theorem: For any finite
point set P in the plane for which the above P* is infinite, there exists a
threshold t > 1 such that P is not contained in the vertex set of any finite
plane graph of dilation at most t. We construct a concrete point set Q such
that any planar graph that contains this set amongst its vertices must have a
dilation larger than 1.0000047."
"We prove that, when a path of length n is embedded in R^2, the 3-distortion
is an Omega(n^{1/2}), and that, when embedded in R^d, the 3-distortion is an
O(n^{1/d-1})."
"A pants decomposition of an orientable surface S is a collection of simple
cycles that partition S into pants, i.e., surfaces of genus zero with three
boundary cycles. Given a set P of n points in the plane, we consider the
problem of computing a pants decomposition of the surface S which is the plane
minus P, of minimum total length. We give a polynomial-time approximation
scheme using Mitchell's guillotine rectilinear subdivisions. We give a
quartic-time algorithm to compute the shortest pants decomposition of S when
the cycles are restricted to be axis-aligned boxes, and a quadratic-time
algorithm when all the points lie on a line; both exact algorithms use dynamic
programming with Yao's speedup."
"An unfolding of a polyhedron is produced by cutting the surface and
flattening to a single, connected, planar piece without overlap (except
possibly at boundary points). It is a long unsolved problem to determine
whether every polyhedron may be unfolded. Here we prove, via an algorithm, that
every orthogonal polyhedron (one whose faces meet at right angles) of genus
zero may be unfolded. Our cuts are not necessarily along edges of the
polyhedron, but they are always parallel to polyhedron edges. For a polyhedron
of n vertices, portions of the unfolding will be rectangular strips which, in
the worst case, may need to be as thin as epsilon = 1/2^{Omega(n)}."
"Let $k$ and $n$ be positive integers, $n>k$. Define $r(n,k)$ to be the
minimum positive value of $$ |\sqrt{a_1} + ... + \sqrt{a_k} - \sqrt{b_1} - >...
-\sqrt{b_k} | $$ where $ a_1, a_2, ..., a_k, b_1, b_2, ..., b_k $ are positive
integers no larger than $n$. It is an important problem in computational
geometry to determine a good upper bound of $-\log r(n,k)$. In this paper we
prove an upper bound of $ 2^{O(n/\log n)} \log n$, which is better than the
best known result $O(2^{2k} \log n)$ whenever $ n \leq ck\log k$ for some
constant $c$. In particular, our result implies a {\em subexponential}
algorithm to compare two sums of square roots of integers of size $o(k\log k)$."
"Motivated by secure wireless networking, we consider the problem of placing
fixed localizers that enable mobile communication devices to prove they belong
to a secure region that is defined by the interior of a polygon. Each localizer
views an infinite wedge of the plane, and a device can prove membership in the
secure region if it is inside the wedges for a set of localizers whose common
intersection contains no points outside the polygon. This model leads to a
broad class of new art gallery type problems, for which we provide upper and
lower bounds."
"We extend linkage unfolding results from the well-studied case of polygonal
linkages to the more general case of linkages of polygons. More precisely, we
consider chains of nonoverlapping rigid planar shapes (Jordan regions) that are
hinged together sequentially at rotatable joints. Our goal is to characterize
the families of planar shapes that admit locked chains, where some
configurations cannot be reached by continuous reconfiguration without
self-intersection, and which families of planar shapes guarantee universal
foldability, where every chain is guaranteed to have a connected configuration
space. Previously, only obtuse triangles were known to admit locked shapes, and
only line segments were known to guarantee universal foldability. We show that
a surprisingly general family of planar shapes, called slender adornments,
guarantees universal foldability: roughly, the distance from each edge along
the path along the boundary of the slender adornment to each hinge should be
monotone. In contrast, we show that isosceles triangles with any desired apex
angle less than 90 degrees admit locked chains, which is precisely the
threshold beyond which the inward-normal property no longer holds."
"We provide efficient constant factor approximation algorithms for the
problems of finding a hierarchical clustering of a point set in any metric
space, minimizing the sum of minimimum spanning tree lengths within each
cluster, and in the hyperbolic or Euclidean planes, minimizing the sum of
cluster perimeters. Our algorithms for the hyperbolic and Euclidean planes can
also be used to provide a pants decomposition, that is, a set of disjoint
simple closed curves partitioning the plane minus the input points into subsets
with exactly three boundary components, with approximately minimum total
length. In the Euclidean case, these curves are squares; in the hyperbolic
case, they combine our Euclidean square pants decomposition with our tree
clustering method for general metric spaces."
"Robustness problems due to the substitution of the exact computation on real
numbers by the rounded floating point arithmetic are often an obstacle to
obtain practical implementation of geometric algorithms. If the adoption of the
--exact computation paradigm--[Yap et Dube] gives a satisfactory solution to
this kind of problems for purely combinatorial algorithms, this solution does
not allow to solve in practice the case of algorithms that cascade the
construction of new geometric objects. In this report, we consider the problem
of rounding the intersection of two polygonal regions onto the integer lattice
with inclusion properties. Namely, given two polygonal regions A and B having
their vertices on the integer lattice, the inner and outer rounding modes
construct two polygonal regions with integer vertices which respectively is
included and contains the true intersection. We also prove interesting results
on the Hausdorff distance, the size and the convexity of these polygonal
regions."
"Efficient algorithms are presented for constructing spanners in geometric
intersection graphs. For a unit ball graph in R^k, a (1+\epsilon)-spanner is
obtained using efficient partitioning of the space into hypercubes and solving
bichromatic closest pair problems. The spanner construction has almost
equivalent complexity to the construction of Euclidean minimum spanning trees.
The results are extended to arbitrary ball graphs with a sub-quadratic running
time.
  For unit ball graphs, the spanners have a small separator decomposition which
can be used to obtain efficient algorithms for approximating proximity problems
like diameter and distance queries. The results on compressed quadtrees,
geometric graph separators, and diameter approximation might be of independent
interest."
"A point p is 1-well illuminated by a set F of n point lights if p lies in the
interior of the convex hull of F. This concept corresponds to triangle-guarding
or well-covering. In this paper we consider the illumination range of the light
sources as a parameter to be optimized. First, we solve the problem of
minimizing the light sources' illumination range to 1-well illuminate a given
point p. We also compute a minimal set of light sources that 1-well illuminates
p with minimum illumination range. Second, we solve the problem of minimizing
the light sources' illumination range to 1-well illuminate all the points of a
line segment with an O(n^2) algorithm. Finally, we give an O(n^2 log n)
algorithm for preprocessing the data so that one can obtain the illumination
range needed to 1-well illuminate a point of a line segment in O(log n) time.
These results can be applied to solve problems of 1-well illuminating a
trajectory by approaching it to a polygonal path."
"We analyse the axioms of Euclidean geometry according to standard
object-oriented software development methodology. We find a perfect match: the
main undefined concepts of the axioms translate to object classes. The result
is a suite of C++ classes that efficiently supports the construction of complex
geometric configurations. Although all computations are performed in
floating-point arithmetic, they correctly implement as semi-decision algorithms
the tests for equality of points, a point being on a line or in a plane, a line
being in a plane, parallelness of lines, of a line and a plane, and of planes.
That is, in accordance to the fundamental limitations to computability
requiring that only negative outcomes are given with certainty, while positive
outcomes only imply possibility of these conditions being true."
"This paper contains some results of pentagons which can be determined by two
X-rays. The results reveal this problem is more complicated."
"We consider graph drawing algorithms for learning spaces, a type of
st-oriented partial cube derived from antimatroids and used to model states of
knowledge of students. We show how to draw any st-planar learning space so all
internal faces are convex quadrilaterals with the bottom side horizontal and
the left side vertical, with one minimal and one maximal vertex. Conversely,
every such drawing represents an st-planar learning space. We also describe
connections between these graphs and arrangements of translates of a quadrant."
"We consider drawings of trees in which all edges incident to leaves can be
extended to infinite rays without crossing, partitioning the plane into
infinite convex polygons. Among all such drawings we seek the one maximizing
the angular resolution of the drawing. We find linear time algorithms for
solving this problem, both for plane trees and for trees without a fixed
embedding. In any such drawing, the edge lengths may be set independently of
the angles, without crossing; we describe multiple strategies for setting these
lengths."
"The Divider set, as an innovative alternative concept to maximal disks,
Voronoi sets and cut loci, is presented with a formal definition based on
topology and differential geometry. The relevant mathematical theory by
previous authors and a comparison with other medial axis definitions is
presented. Appropriate applications are proposed and examined."
"Graph drawing research traditionally focuses on producing geometric
embeddings of graphs satisfying various aesthetic constraints. After the
geometric embedding is specified, there is an additional step that is often
overlooked or ignored: assigning display colors to the graph's vertices. We
study the additional aesthetic criterion of assigning distinct colors to
vertices of a geometric graph so that the colors assigned to adjacent vertices
are as different from one another as possible. We formulate this as a problem
involving perceptual metrics in color space and we develop algorithms for
solving this problem by embedding the graph in color space. We also present an
application of this work to a distributed load-balancing visualization problem."
"A continuum description of unstructured meshes in two dimensions, both for
planar and curved surface domains, is proposed. The meshes described are those
which, in the limit of an increasingly finer mesh (smaller cells), and away
from irregular vertices, have ideally-shaped cells (squares or equilateral
triangles), and can therefore be completely described by two local properties:
local cell size and local edge directions. The connection between the two
properties is derived by defining a Riemannian manifold whose geodesics trace
the edges of the mesh. A function $\phi$, proportional to the logarithm of the
cell size, is shown to obey the Poisson equation, with localized charges
corresponding to irregular vertices. The problem of finding a suitable manifold
for a given domain is thus shown to exactly reduce to an Inverse Poisson
problem on $\phi$, of finding a distribution of localized charges adhering to
the conditions derived for boundary alignment. Possible applications to mesh
generation are discussed."
"Given a disk O in the plane called the objective, we want to find n small
disks P_1,...,P_n called the pupils such that $\bigcup_{i,j=1}^n P_i \ominus
P_j \supseteq O$, where $\ominus$ denotes the Minkowski difference operator,
while minimizing the number of pupils, the sum of the radii or the total area
of the pupils. This problem is motivated by the construction of very large
telescopes from several smaller ones by so-called Optical Aperture Synthesis.
In this paper, we provide exact, approximate and heuristic solutions to several
variations of the problem."
"Circular chromatic number, $\chi_c$ is a natural generalization of chromatic
number. It is known that it is \NP-hard to determine whether or not an
arbitrary graph $G$ satisfies $\chi(G) = \chi_c(G)$. In this paper we prove
that this problem is \NP-hard even if the chromatic number of the graph is
known. This answers a question of Xuding Zhu. Also we prove that for all
positive integers $k \ge 2$ and $n \ge 3$, for a given graph $G$ with
$\chi(G)=n$, it is \NP-complete to verify if $\chi_c(G) \le n- \frac{1}{k}$."
"In this paper we improve the approach of a previous paper about the domino
problem in the hyperbolic plane, see arXiv.cs.CG/0603093. This time, we prove
that the general problem of the hyperbolic plane with \`a la Wang tiles is
undecidable."
"We prove Helly-type theorems for line transversals to disjoint unit balls in
$\R^{d}$. In particular, we show that a family of $n \geq 2d$ disjoint unit
balls in $\R^d$ has a line transversal if, for some ordering $\prec$ of the
balls, any subfamily of 2d balls admits a line transversal consistent with
$\prec$. We also prove that a family of $n \geq 4d-1$ disjoint unit balls in
$\R^d$ admits a line transversal if any subfamily of size $4d-1$ admits a
transversal."
"Given a set S of n points in R^D, and an integer k such that 0 <= k < n, we
show that a geometric graph with vertex set S, at most n - 1 + k edges, maximum
degree five, and dilation O(n / (k+1)) can be computed in time O(n log n). For
any k, we also construct planar n-point sets for which any geometric graph with
n-1+k edges has dilation Omega(n/(k+1)); a slightly weaker statement holds if
the points of S are required to be in convex position."
"It is a widely observed phenomenon in computer graphics that the size of the
silhouette of a polyhedron is much smaller than the size of the whole
polyhedron. This paper provides, for the first time, theoretical evidence
supporting this for a large class of objects, namely for polyhedra that
approximate surfaces in some reasonable way; the surfaces may be non-convex and
non-differentiable and they may have boundaries. We prove that such polyhedra
have silhouettes of expected size $O(\sqrt{n})$ where the average is taken over
all points of view and n is the complexity of the polyhedron."
"We introduce a family of directed geometric graphs, denoted $\paz$, that
depend on two parameters $\lambda$ and $\theta$. For $0\leq
\theta<\frac{\pi}{2}$ and ${1/2} < \lambda < 1$, the $\paz$ graph is a strong
$t$-spanner, with $t=\frac{1}{(1-\lambda)\cos\theta}$. The out-degree of a node
in the $\paz$ graph is at most $\lfloor2\pi/\min(\theta,
\arccos\frac{1}{2\lambda})\rfloor$. Moreover, we show that routing can be
achieved locally on $\paz$. Next, we show that all strong $t$-spanners are also
$t$-spanners of the unit disk graph. Simulations for various values of the
parameters $\lambda$ and $\theta$ indicate that for random point sets, the
spanning ratio of $\paz$ is better than the proven theoretical bounds."
"In a geometric network G = (S, E), the graph distance between two vertices u,
v in S is the length of the shortest path in G connecting u to v. The dilation
of G is the maximum factor by which the graph distance of a pair of vertices
differs from their Euclidean distance. We show that given a set S of n points
with integer coordinates in the plane and a rational dilation delta > 1, it is
NP-hard to determine whether a spanning tree of S with dilation at most delta
exists."
"We show that a closed piecewise-linear hypersurface immersed in $R^n$ ($n\ge
3$) is the boundary of a convex body if and only if every point in the interior
of each $(n-3)$-face has a neighborhood that lies on the boundary of some
convex body; no assumptions about the hypersurface's topology are needed. We
derive this criterion from our generalization of Van Heijenoort's (1952)
theorem on locally convex hypersurfaces in $R^n$ to spherical spaces. We also
give an easy-to-implement convexity testing algorithm, which is based on our
criterion. For $R^3$ the number of arithmetic operations used by the algorithm
is at most linear in the number of vertices, while in general it is at most
linear in the number of incidences between the $(n-2)$-faces and $(n-3)$-faces.
When the dimension $n$ is not fixed and only ring arithmetic is allowed, the
algorithm still remains polynomial. Our method works in more general situations
than the convexity verification algorithms developed by Mehlhorn et al. (1996)
and Devillers et al. (1998) -- for example, our method does not require the
input surface to be orientable, nor it requires the input data to include
normal vectors to the facets that are oriented ""in a coherent way"". For $R^3$
the complexity of our algorithm is the same as that of previous algorithms; for
higher dimensions there seems to be no clear winner, but our approach is the
only one that easily handles inputs in which the facet normals are not known to
be coherently oriented or are not given at all. Furthermore, our method can be
extended to piecewise-polynomial surfaces of small degree."
"For two points $p$ and $q$ in the plane, a straight line $h$, called a
highway, and a real $v>1$, we define the \emph{travel time} (also known as the
\emph{city distance}) from $p$ and $q$ to be the time needed to traverse a
quickest path from $p$ to $q$, where the distance is measured with speed $v$ on
$h$ and with speed 1 in the underlying metric elsewhere.
  Given a set $S$ of $n$ points in the plane and a highway speed $v$, we
consider the problem of finding a \emph{highway} that minimizes the maximum
travel time over all pairs of points in $S$. If the orientation of the highway
is fixed, the optimal highway can be computed in linear time, both for the
$L_1$- and the Euclidean metric as the underlying metric. If arbitrary
orientations are allowed, then the optimal highway can be computed in $O(n^{2}
\log n)$ time. We also consider the problem of computing an optimal pair of
highways, one being horizontal, one vertical."
"We study a simple geometric model of transportation facility that consists of
two points between which the travel speed is high. This elementary definition
can model shuttle services, tunnels, bridges, teleportation devices, escalators
or moving walkways. The travel time between a pair of points is defined as a
time distance, in such a way that a customer uses the transportation facility
only if it is helpful.
  We give algorithms for finding the optimal location of such a transportation
facility, where optimality is defined with respect to the maximum travel time
between two points in a given set."
"The concept of data depth in non-parametric multivariate descriptive
statistics is the generalization of the univariate rank method to multivariate
data. Halfspace depth is a measure of data depth. Given a set S of points and a
point p, the halfspace depth (or rank) k of p is defined as the minimum number
of points of S contained in any closed halfspace with p on its boundary.
Computing halfspace depth is NP-hard, and it is equivalent to the Maximum
Feasible Subsystem problem. In this thesis a mixed integer program is
formulated with the big-M method for the halfspace depth problem. We suggest a
branch and cut algorithm. In this algorithm, Chinneck's heuristic algorithm is
used to find an upper bound and a related technique based on sensitivity
analysis is used for branching. Irreducible Infeasible Subsystem (IIS) hitting
set cuts are applied. We also suggest a binary search algorithm which may be
more stable numerically. The algorithms are implemented with the BCP framework
from the COIN-OR project."
"In binary images, the distance transformation (DT) and the geometrical
skeleton extraction are classic tools for shape analysis. In this paper, we
present time optimal algorithms to solve the reverse Euclidean distance
transformation and the reversible medial axis extraction problems for
$d$-dimensional images. We also present a $d$-dimensional medial axis filtering
process that allows us to control the quality of the reconstructed shape."
"We demonstrate relationships between the classic Euclidean algorithm and many
other fields of study, particularly in the context of music and distance
geometry. Specifically, we show how the structure of the Euclidean algorithm
defines a family of rhythms which encompass over forty timelines
(\emph{ostinatos}) from traditional world music. We prove that these
\emph{Euclidean rhythms} have the mathematical property that their onset
patterns are distributed as evenly as possible: they maximize the sum of the
Euclidean distances between all pairs of onsets, viewing onsets as points on a
circle. Indeed, Euclidean rhythms are the unique rhythms that maximize this
notion of \emph{evenness}. We also show that essentially all Euclidean rhythms
are \emph{deep}: each distinct distance between onsets occurs with a unique
multiplicity, and these multiplicies form an interval $1,2,...,k-1$. Finally,
we characterize all deep rhythms, showing that they form a subclass of
generated rhythms, which in turn proves a useful property called shelling. All
of our results for musical rhythms apply equally well to musical scales. In
addition, many of the problems we explore are interesting in their own right as
distance geometry problems on the circle; some of the same problems were
explored by Erd\H{o}s in the plane."
"It is shown that every orthogonal terrain, i.e., an orthogonal (right-angled)
polyhedron based on a rectangle that meets every vertical line in a segment,
has a grid unfolding: its surface may be unfolded to a single non-overlapping
piece by cutting along grid edges defined by coordinate planes through every
vertex."
"A convex combination mapping of a planar graph is a plane mapping in which
the external vertices are mapped to the corners of a convex polygon and every
internal vertex is a proper weighted average of its neighbours. If a planar
graph is nodally 3-connected or triangulated then every such mapping is an
embedding (Tutte, Floater).
  We give a simple characterisation of nodally 3-connected planar graphs, and
generalise the above result to any planar graph which admits any convex
embedding."
"This note shows that the hope expressed in [ADL+07]--that the new algorithm
for edge-unfolding any polyhedral band without overlap might lead to an
algorithm for unfolding any prismatoid without overlap--cannot be realized. A
prismatoid is constructed whose sides constitute a nested polyhedral band, with
the property that every placement of the prismatoid top face overlaps with the
band unfolding."
"Angular Voronoi diagram was introduced by Asano et al. as fundamental
research for a mesh generation. In an angular Voronoi diagram, the edges are
curves of degree three. From view of computational robustness we need to treat
the curves carefully, because they might have a singularity.
  We enumerate all the possible types of curves that appear as an edge of an
angular Voronoi diagram, which tells us what kind of degeneracy is possible and
tells us necessity of considering a singularity for computational robustness."
"Given an integer $k \geq 2$, we consider the problem of computing the
smallest real number $t(k)$ such that for each set $P$ of points in the plane,
there exists a $t(k)$-spanner for $P$ that has chromatic number at most $k$. We
prove that $t(2) = 3$, $t(3) = 2$, $t(4) = \sqrt{2}$, and give upper and lower
bounds on $t(k)$ for $k>4$. We also show that for any $\epsilon >0$, there
exists a $(1+\epsilon)t(k)$-spanner for $P$ that has $O(|P|)$ edges and
chromatic number at most $k$. Finally, we consider an on-line variant of the
problem where the points of $P$ are given one after another, and the color of a
point must be assigned at the moment the point is given. In this setting, we
prove that $t(2) = 3$, $t(3) = 1+ \sqrt{3}$, $t(4) = 1+ \sqrt{2}$, and give
upper and lower bounds on $t(k)$ for $k>4$."
"A convex surface that is flat everywhere but on finitely many smooth curves
(or ""seams"") and points is a seam form. We show that the only creases through
the flat components of a seam form are either between vertices or tangent to
the seams. As corollaries we resolve open problems about certain special seam
forms: the flat components of a D-form have no creases at all, and the flat
component of a pita-form has at most one crease, between the seam's endpoints."
"In [1], a new construction called red-black hierarchy characterizing Laman
graphs and an algorithm for computing it were presented. For a Laman graph
G=(V,E) with n vertices it runs in O(n^2) time assuming that a partition of
(V,E+e) into two spanning trees is given. We show that a simple modification
reduces the running time to O(n\log n). The total running time can be reduced
O(n\sqrt{n\log n}) using the algorithm by Gabow and Westermann [2] for
partitioning a graph into two forests. The existence of a red-black hierarchy
is a necessary and sufficient condition for a graph to be a Laman graph. The
algorithm for constructing a red-black hierarchy can be then modified to
recognize Laman graphs in the same time."
"In this paper, we generalize the notions of centroids and barycenters to the
broad class of information-theoretic distortion measures called Bregman
divergences. Bregman divergences are versatile, and unify quadratic geometric
distances with various statistical entropic measures. Because Bregman
divergences are typically asymmetric, we consider both the left-sided and
right-sided centroids and the symmetrized centroids, and prove that all three
are unique. We give closed-form solutions for the sided centroids that are
generalized means, and design a provably fast and efficient approximation
algorithm for the symmetrized centroid based on its exact geometric
characterization that requires solely to walk on the geodesic linking the two
sided centroids. We report on our generic implementation for computing entropic
centers of image clusters and entropic centers of multivariate normals, and
compare our results with former ad-hoc methods."
"Soss proved that it is NP-hard to find the maximum 2D span of a fixed-angle
polygonal chain: the largest distance achievable between the endpoints in a
planar embedding. These fixed-angle chains can serve as models of protein
backbones. The corresponding problem in 3D is open. We show that three special
cases of particular relevance to the protein model are solvable in polynomial
time. When all link lengths and all angles are equal, the maximum 3D span is
achieved in a flat configuration and can be computed in constant time. When all
angles are equal and the chain is simple (non-self-crossing), the maximum flat
span can be found in linear time. In 3D, when all angles are equal to 90 deg
(but the link lengths arbitrary), the maximum 3D span is in general nonplanar
but can be found in quadratic time."
"Consider a point set D with a measure function w : D -> R. Let A be the set
of subsets of D induced by containment in a shape from some geometric family
(e.g. axis-aligned rectangles, half planes, balls, k-oriented polygons). We say
a range space (D, A) has an eps-approximation P if max {R \in A} | w(R \cap
P)/w(P) - w(R \cap D)/w(D) | <= eps. We describe algorithms for
deterministically constructing discrete eps-approximations for continuous point
sets such as distributions or terrains. Furthermore, for certain families of
subsets A, such as those described by axis-aligned rectangles, we reduce the
size of the eps-approximations by almost a square root from O(1/eps^2 log
1/eps) to O(1/eps polylog 1/eps). This is often the first step in transforming
a continuous problem into a discrete one for which combinatorial techniques can
be applied. We describe applications of this result in geo-spatial analysis,
biosurveillance, and sensor networks."
"We study the problem of computing geometric spanners for (additively)
weighted point sets. A weighted point set is a set of pairs $(p,r)$ where $p$
is a point in the plane and $r$ is a real number. The distance between two
points $(p_i,r_i)$ and $(p_j,r_j)$ is defined as $|p_ip_j|-r_i-r_j$. We show
that in the case where all $r_i$ are positive numbers and $|p_ip_j|\geq
r_i+r_j$ for all $i,j$ (in which case the points can be seen as
non-intersecting disks in the plane), a variant of the Yao graph is a
$(1+\epsilon)$-spanner that has a linear number of edges. We also show that the
Additively Weighted Delaunay graph (the face-dual of the Additively Weighted
Voronoi diagram) has constant spanning ratio. The straight line embedding of
the Additively Weighted Delaunay graph may not be a plane graph. We show how to
compute a plane embedding that also has a constant spanning ratio."
"We construct a sequence of convex polyhedra on n vertices with the property
that, as n -> infinity, the fraction of its edge unfoldings that avoid overlap
approaches 0, and so the fraction that overlap approaches 1. Nevertheless, each
does have (several) nonoverlapping edge unfoldings."
"We give a counterexample to a conjecture of Poon [Poo06] that any orthogonal
tree in two dimensions can always be flattened by a continuous motion that
preserves edge lengths and avoids self-intersection. We show our example is
locked by extending results on strongly locked self-touching linkages due to
Connelly, Demaine and Rote [CDR02] to allow zero-length edges as defined in
[ADG07], which may be of independent interest. Our results also yield a locked
tree with only eleven edges, which is the smallest known example of a locked
tree."
"We consider the problem of monitoring an art gallery modeled as a polygon,
the edges of which are arcs of curves, with edge or mobile guards. Our focus is
on piecewise-convex polygons, i.e., polygons that are locally convex, except
possibly at the vertices, and their edges are convex arcs. We transform the
problem of monitoring a piecewise-convex polygon to the problem of 2-dominating
a properly defined triangulation graph with edges or diagonals, where
2-dominance requires that every triangle in the triangulation graph has at
least two of its vertices in its 2-dominating set. We show that
$\lfloor\frac{n+1}{3}\rfloor$ diagonal guards or $\lfloor\frac{2n+1}{5}\rfloor$
edge guards are always sufficient and sometimes necessary, in order to
2-dominate a triangulation graph. Furthermore, we show how to compute: a
diagonal 2-dominating set of size $\lfloor\frac{n+1}{3}\rfloor$ in linear time,
an edge 2-dominating set of size $\lfloor\frac{2n+1}{5}\rfloor$ in $O(n^2)$
time, and an edge 2-dominating set of size $\lfloor\frac{3n}{7}\rfloor$ in O(n)
time. Based on the above-mentioned results, we prove that, for piecewise-convex
polygons, we can compute: a mobile guard set of size
$\lfloor\frac{n+1}{3}\rfloor$ in $O(n\log{}n)$ time, an edge guard set of size
$\lfloor\frac{2n+1}{5}\rfloor$ in $O(n^2)$ time, and an edge guard set of size
$\lfloor\frac{3n}{7}\rfloor$ in $O(n\log{}n)$ time. Finally, we show that
$\lfloor\frac{n}{3}\rfloor$ mobile or $\lceil\frac{n}{3}\rceil$ edge guards are
sometimes necessary. When restricting our attention to monotone
piecewise-convex polygons, the bounds mentioned above drop:
$\lceil\frac{n+1}{4}\rceil$ edge or mobile guards are always sufficient and
sometimes necessary; such an edge or mobile guard set, of size at most
$\lceil\frac{n+1}{4}\rceil$, can be computed in O(n) time."
"One of the earliest and most well known problems in computational geometry is
the so-called art gallery problem. The goal is to compute the minimum possible
number guards placed on the vertices of a simple polygon in such a way that
they cover the interior of the polygon.
  In this paper we consider the problem of guarding an art gallery which is
modeled as a polygon with curvilinear walls. Our main focus is on polygons the
edges of which are convex arcs pointing towards the exterior or interior of the
polygon (but not both), named piecewise-convex and piecewise-concave polygons.
We prove that, in the case of piecewise-convex polygons, if we only allow
vertex guards, $\lfloor\frac{4n}{7}\rfloor-1$ guards are sometimes necessary,
and $\lfloor\frac{2n}{3}\rfloor$ guards are always sufficient. Moreover, an
$O(n\log{}n)$ time and O(n) space algorithm is described that produces a vertex
guarding set of size at most $\lfloor\frac{2n}{3}\rfloor$. When we allow point
guards the afore-mentioned lower bound drops down to
$\lfloor\frac{n}{2}\rfloor$. In the special case of monotone piecewise-convex
polygons we can show that $\lfloor\frac{n}{2}\rfloor$ vertex guards are always
sufficient and sometimes necessary; these bounds remain valid even if we allow
point guards.
  In the case of piecewise-concave polygons, we show that $2n-4$ point guards
are always sufficient and sometimes necessary, whereas it might not be possible
to guard such polygons by vertex guards. We conclude with bounds for other
types of curvilinear polygons and future work."
"Suppose we are given a finite set of points $P$ in $\R^3$ and a collection of
polytopes $\mathcal{T}$ that are all translates of the same polytope $T$. We
consider two problems in this paper. The first is the set cover problem where
we want to select a minimal number of polytopes from the collection
$\mathcal{T}$ such that their union covers all input points $P$. The second
problem that we consider is finding a hitting set for the set of polytopes
$\mathcal{T}$, that is, we want to select a minimal number of points from the
input points $P$ such that every given polytope is hit by at least one point.
We give the first constant-factor approximation algorithms for both problems.
We achieve this by providing an epsilon-net for translates of a polytope in
$R^3$ of size $\bigO(\frac{1{\epsilon)$."
"We introduce staged self-assembly of Wang tiles, where tiles can be added
dynamically in sequence and where intermediate constructions can be stored for
later mixing. This model and its various constraints and performance measures
are motivated by a practical nanofabrication scenario through protein-based
bioengineering. Staging allows us to break through the traditional lower bounds
in tile self-assembly by encoding the shape in the staging algorithm instead of
the tiles. All of our results are based on the practical assumption that only a
constant number of glues, and thus only a constant number of tiles, can be
engineered, as each new glue type requires significant biochemical research and
experiments. Under this assumption, traditional tile self-assembly cannot even
manufacture an n*n square; in contrast, we show how staged assembly enables
manufacture of arbitrary orthogonal shapes in a variety of precise formulations
of the model."
"It is a well-known fact that, under mild sampling conditions, the restricted
Delaunay triangulation provides good topological approximations of 1- and
2-manifolds. We show that this is not the case for higher-dimensional
manifolds, even under stronger sampling conditions. Specifically, it is not
true that, for any compact closed submanifold M of R^n, and any sufficiently
dense uniform sampling L of M, the Delaunay triangulation of L restricted to M
is homeomorphic to M, or even homotopy equivalent to it. Besides, it is not
true either that, for any sufficiently dense set W of witnesses, the witness
complex of L relative to M contains or is contained in the restricted Delaunay
triangulation of L."
"We prove that for every centrally symmetric convex polygon Q, there exists a
constant alpha such that any alpha*k-fold covering of the plane by translates
of Q can be decomposed into k coverings. This improves on a quadratic upper
bound proved by Pach and Toth (SoCG'07). The question is motivated by a sensor
network problem, in which a region has to be monitored by sensors with limited
battery lifetime."
"We establish a bound of $O(n^2k^{1+\eps})$, for any $\eps>0$, on the
combinatorial complexity of the set $\T$ of line transversals of a collection
$\P$ of $k$ convex polyhedra in $\reals^3$ with a total of $n$ facets, and
present a randomized algorithm which computes the boundary of $\T$ in
comparable expected time. Thus, when $k\ll n$, the new bounds on the complexity
(and construction cost) of $\T$ improve upon the previously best known bounds,
which are nearly cubic in $n$.
  To obtain the above result, we study the set $\TL$ of line transversals which
emanate from a fixed line $\ell_0$, establish an almost tight bound of
$O(nk^{1+\eps})$ on the complexity of $\TL$, and provide a randomized algorithm
which computes $\TL$ in comparable expected time. Slightly improved
combinatorial bounds for the complexity of $\TL$, and comparable improvements
in the cost of constructing this set, are established for two special cases,
both assuming that the polyhedra of $\P$ are pairwise disjoint: the case where
$\ell_0$ is disjoint from the polyhedra of $\P$, and the case where the
polyhedra of $\P$ are unbounded in a direction parallel to $\ell_0$."
"We describe polynomial time algorithms for determining whether an undirected
graph may be embedded in a distance-preserving way into the hexagonal tiling of
the plane, the diamond structure in three dimensions, or analogous structures
in higher dimensions. The graphs that may be embedded in this way form an
interesting subclass of the partial cubes."
"We present a 4-approximation algorithm for the problem of placing a fewest
guards on a 1.5D terrain so that every point of the terrain is seen by at least
one guard. This improves on the currently best approximation factor of 5. Our
method is based on rounding the linear programming relaxation of the
corresponding covering problem. Besides the simplicity of the analysis, which
mainly relies on decomposing the constraint matrix of the LP into totally
balanced matrices, our algorithm, unlike previous work, generalizes to the
weighted and partial versions of the basic problem."
"The Largest Empty Circle problem seeks the largest circle centered within the
convex hull of a set $P$ of $n$ points in $\mathbb{R}^2$ and devoid of points
from $P$. In this paper, we introduce a query version of this well-studied
problem. In our query version, we are required to preprocess $P$ so that when
given a query line $Q$, we can quickly compute the largest empty circle
centered at some point on $Q$ and within the convex hull of $P$.
  We present solutions for two special cases and the general case; all our
queries run in $O(\log n)$ time. We restrict the query line to be horizontal in
the first special case, which we preprocess in $O(n \alpha(n) \log n)$ time and
space, where $\alpha(n)$ is the slow growing inverse of the Ackermann's
function. When the query line is restricted to pass through a fixed point, the
second special case, our preprocessing takes $O(n \alpha(n)^{O(\alpha(n))} \log
n)$ time and space. We use insights from the two special cases to solve the
general version of the problem with preprocessing time and space in $O(n^3 \log
n)$ and $O(n^3)$ respectively."
"Let $V$ be a finite set of points in the plane. We present a 2-local
algorithm that constructs a plane $\frac{4 \pi \sqrt{3}}{9}$-spanner of the
unit-disk graph $\UDG(V)$. This algorithm makes only one round of communication
and each point of $V$ broadcasts at most 5 messages. This improves the
previously best message-bound of 11 by Ara\'{u}jo and Rodrigues (Fast localized
Delaunay triangulation, Lecture Notes in Computer Science, volume 3544, 2004)."
"We define and study exact, efficient representations of realization spaces
Euclidean Distance Constraint Systems (EDCS), which includes Linkages and
Frameworks. Each representation corresponds to a choice of Cayley parameters
and yields a different parametrized configuration space. Significantly, we give
purely graph-theoretic, forbidden minor characterizations that capture (i) the
class of graphs that always admit efficient configuration spaces and (ii) the
possible choices of representation parameters that yield efficient
configuration spaces for a given graph. In addition, our results are tight: we
show counterexamples to obvious extensions. This is the first step in a
systematic and graded program of combinatorial characterizations of efficient
configuration spaces. We discuss several future theoretical and applied
research directions. Some of our proofs employ an unusual interplay of (a)
classical analytic results related to positive semi-definiteness of Euclidean
distance matrices, with (b) recent forbidden minor characterizations and
algorithms related to the notion of d-realizability of EDCS. We further
introduce a novel type of restricted edge contraction or reduction to a graph
minor, a ""trick"" that we anticipate will be useful in other situations."
"This paper studies the configuration spaces of linkages whose underlying
graph is a single cycle. Assume that the edge lengths are such that there are
no configurations in which all the edges lie along a line. The main results are
that, modulo translations and rotations, each component of the space of convex
configurations is homeomorphic to a closed Euclidean ball and each component of
the space of embedded configurations is homeomorphic to a Euclidean space. This
represents an elaboration on the topological information that follows from the
convexification theorem of Connelly, Demaine, and Rote."
"A rectangular layout is a partition of a rectangle into a finite set of
interior-disjoint rectangles. Rectangular layouts appear in various
applications: as rectangular cartograms in cartography, as floorplans in
building architecture and VLSI design, and as graph drawings. Often areas are
associated with the rectangles of a rectangular layout and it might hence be
desirable if one rectangular layout can represent several area assignments. A
layout is area-universal if any assignment of areas to rectangles can be
realized by a combinatorially equivalent rectangular layout. We identify a
simple necessary and sufficient condition for a rectangular layout to be
area-universal: a rectangular layout is area-universal if and only if it is
one-sided. More generally, given any rectangular layout L and any assignment of
areas to its regions, we show that there can be at most one layout (up to
horizontal and vertical scaling) which is combinatorially equivalent to L and
achieves a given area assignment. We also investigate similar questions for
perimeter assignments. The adjacency requirements for the rectangles of a
rectangular layout can be specified in various ways, most commonly via the dual
graph of the layout. We show how to find an area-universal layout for a given
set of adjacency requirements whenever such a layout exists."
"In this paper we consider several instances of the k-center on a line problem
where the goal is, given a set of points S in the plane and a parameter k >= 1,
to find k disks with centers on a line l such that their union covers S and the
maximum radius of the disks is minimized. This problem is a constraint version
of the well-known k-center problem in which the centers are constrained to lie
in a particular region such as a segment, a line, and a polygon. We first
consider the simplest version of the problem where the line l is given in
advance; we can solve this problem in O(n log^2 n) time. We then investigate
the cases where only the orientation of the line l is fixed and where the line
l can be arbitrary. We can solve these problems in O(n^2 log^2 n) time and in
O(n^4 log^2 n) expected time, respectively. For the last two problems, we
present (1 + e)-approximation algorithms, which run in O((1/e) n log^2 n) time
and O((1/e^2) n log^2 n) time, respectively."
"We analyze a probabilistic algorithm for matching shapes modeled by planar
regions under translations and rigid motions (rotation and translation). Given
shapes $A$ and $B$, the algorithm computes a transformation $t$ such that with
high probability the area of overlap of $t(A)$ and $B$ is close to maximal. In
the case of polygons, we give a time bound that does not depend significantly
on the number of vertices."
"We consider the set multi-cover problem in geometric settings. Given a set of
points P and a collection of geometric shapes (or sets) F, we wish to find a
minimum cardinality subset of F such that each point p in P is covered by
(contained in) at least d(p) sets. Here d(p) is an integer demand (requirement)
for p. When the demands d(p)=1 for all p, this is the standard set cover
problem. The set cover problem in geometric settings admits an approximation
ratio that is better than that for the general version. In this paper, we show
that similar improvements can be obtained for the multi-cover problem as well.
In particular, we obtain an O(log Opt) approximation for set systems of bounded
VC-dimension, where Opt is the cardinality of an optimal solution, and an O(1)
approximation for covering points by half-spaces in three dimensions and for
some other classes of shapes."
"We re-examine the notion of relative $(p,\eps)$-approximations, recently
introduced in [CKMS06], and establish upper bounds on their size, in general
range spaces of finite VC-dimension, using the sampling theory developed in
[LLS01] and in several earlier studies [Pol86, Hau92, Tal94]. We also survey
the different notions of sampling, used in computational geometry, learning,
and other areas, and show how they relate to each other. We then give
constructions of smaller-size relative $(p,\eps)$-approximations for range
spaces that involve points and halfspaces in two and higher dimensions. The
planar construction is based on a new structure--spanning trees with small
relative crossing number, which we believe to be of independent interest.
Relative $(p,\eps)$-approximations arise in several geometric problems, such as
approximate range counting, and we apply our new structures to obtain efficient
solutions for approximate range counting in three dimensions. We also present a
simple solution for the planar case."
"This paper introduces a method of navigation in a large family of tilings of
the hyperbolic plane and looks at the question of possible applications in the
light of the few ones which were already obtained."
"A geometric graph is a graph embedded in the plane with vertices at points
and edges drawn as curves (which are usually straight line segments) between
those points. The average transversal complexity of a geometric graph is the
number of edges of that graph that are crossed by random line or line segment.
In this paper, we study the average transversal complexity of road networks. By
viewing road networks as multiscale-dispersed graphs, we show that a random
line will cross the edges of such a graph O(sqrt(n)) times on average. In
addition, we provide by empirical evidence from experiments on the road
networks of the fifty states of United States and the District of Columbia that
this bound holds in practice and has a small constant factor. Combining this
result with data structuring techniques from computational geometry, allows us
to show that we can then do point location and ray-shooting navigational
queries with respect to road networks in O(sqrt(n) log n) expected time.
Finally, we provide empirical justification for this claim as well."
"We give the first nontrivial upper and lower bounds on the maximum volume of
an empty axis-parallel box inside an axis-parallel unit hypercube in $\RR^d$
containing $n$ points. For a fixed $d$, we show that the maximum volume is of
the order $\Theta(\frac{1}{n})$. We then use the fact that the maximum volume
is $\Omega(\frac{1}{n})$ in our design of the first efficient
$(1-\eps)$-approximation algorithm for the following problem: Given an
axis-parallel $d$-dimensional box $R$ in $\RR^d$ containing $n$ points, compute
a maximum-volume empty axis-parallel $d$-dimensional box contained in $R$. The
running time of our algorithm is nearly linear in $n$, for small $d$, and
increases only by an $O(\log{n})$ factor when one goes up one dimension. No
previous efficient exact or approximation algorithms were known for this
problem for $d \geq 4$. As the problem has been recently shown to be NP-hard in
arbitrary high dimensions (i.e., when $d$ is part of the input), the existence
of efficient exact algorithms is unlikely.
  We also obtain tight estimates on the maximum volume of an empty
axis-parallel hypercube inside an axis-parallel unit hypercube in $\RR^d$
containing $n$ points. For a fixed $d$, this maximum volume is of the same
order order $\Theta(\frac{1}{n})$. A faster $(1-\eps)$-approximation algorithm,
with a milder dependence on $d$ in the running time, is obtained in this case."
"We revisit several maximization problems for geometric networks design under
the non-crossing constraint, first studied by Alon, Rajagopalan and Suri (ACM
Symposium on Computational Geometry, 1993). Given a set of $n$ points in the
plane in general position (no three points collinear), compute a longest
non-crossing configuration composed of straight line segments that is: (a) a
matching (b) a Hamiltonian path (c) a spanning tree. Here we obtain new results
for (b) and (c), as well as for the Hamiltonian cycle problem:
  (i) For the longest non-crossing Hamiltonian path problem, we give an
approximation algorithm with ratio $\frac{2}{\pi+1} \approx 0.4829$. The
previous best ratio, due to Alon et al., was $1/\pi \approx 0.3183$. Moreover,
the ratio of our algorithm is close to $2/\pi$ on a relatively broad class of
instances: for point sets whose perimeter (or diameter) is much shorter than
the maximum length matching. The algorithm runs in $O(n^{7/3}\log{n})$ time.
  (ii) For the longest non-crossing spanning tree problem, we give an
approximation algorithm with ratio 0.502 which runs in $O(n \log{n})$ time. The
previous ratio, 1/2, due to Alon et al., was achieved by a quadratic time
algorithm. Along the way, we first re-derive the result of Alon et al. with a
faster $O(n \log{n})$-time algorithm and a very simple analysis.
  (iii) For the longest non-crossing Hamiltonian cycle problem, we give an
approximation algorithm whose ratio is close to $2/\pi$ on a relatively broad
class of instances: for point sets with the product $\bf{<}$diameter$\times$
convex hull size $\bf{>}$ much smaller than the maximum length matching. The
algorithm runs in $O(n^{7/3}\log{n})$ time. No previous approximation results
were known for this problem."
"We consider the directed Hausdorff distance between point sets in the plane,
where one or both point sets consist of imprecise points. An imprecise point is
modelled by a disc given by its centre and a radius. The actual position of an
imprecise point may be anywhere within its disc. Due to the direction of the
Hausdorff Distance and whether its tight upper or lower bound is computed there
are several cases to consider. For every case we either show that the
computation is NP-hard or we present an algorithm with a polynomial running
time. Further we give several approximation algorithms for the hard cases and
show that one of them cannot be approximated better than with factor 3, unless
P=NP."
"We present a universal crease pattern--known in geometry as the tetrakis
tiling and in origami as box pleating--that can fold into any object made up of
unit cubes joined face-to-face (polycubes). More precisely, there is one
universal finite crease pattern for each number n of unit cubes that need to be
folded. This result contrasts previous universality results for origami, which
require a different crease pattern for each target object, and confirms
intuition in the origami community that box pleating is a powerful design
technique."
"A memoryless routing algorithm is one in which the decision about the next
edge on the route to a vertex t for a packet currently located at vertex v is
made based only on the coordinates of v, t, and the neighbourhood, N(v), of v.
The current paper explores the limitations of such algorithms by showing that,
for any (randomized) memoryless routing algorithm A, there exists a convex
subdivision on which A takes Omega(n^2) expected time to route a message
between some pair of vertices. Since this lower bound is matched by a random
walk, this result implies that the geometric information available in convex
subdivisions is not helpful for this class of routing algorithms. The current
paper also shows the existence of triangulations for which the Random-Compass
algorithm proposed by Bose etal (2002,2004) requires 2^{\Omega(n)} time to
route between some pair of vertices."
"In this paper, we remind previous results about the tilings $\{p,q\}$ of the
hyperbolic plane. We introduce two new ways to split the hyperbolic plane in
order to algorithmically construct the tilings $\{p,q\}$ when $q$ is odd."
"Given a polygon $P$ in the plane, a {\em pop} operation is the reflection of
a vertex with respect to the line through its adjacent vertices. We define a
family of alternating polygons, and show that any polygon from this family
cannot be convexified by pop operations. This family contains simple, as well
as non-simple (i.e., self-intersecting) polygons, as desired. We thereby answer
in the negative an open problem posed by Demaine and O'Rourke \cite[Open
Problem 5.3]{DO07}."
"We consider a variant of two-point Euclidean shortest path query problem:
given a polygonal domain, build a data structure for two-point shortest path
query, provided that query points always lie on the boundary of the domain. As
a main result, we show that a logarithmic-time query for shortest paths between
boundary points can be performed using O~ (n^5) preprocessing time and O(n^5)
space where n is the number of corners of the polygonal domain and the O~
notation suppresses the polylogarithmic factor. This is realized by observing a
connection between Davenport-Schinzel sequences and our problem in the
parameterized space. We also provide a tradeoff between space and query time; a
sublinear time query is possible using O(n^{3+epsilon}) space. Our approach
also extends to the case where query points should lie on a given set of line
segments."
"The 3D tree visualization faces multiple challenges: the election of an
appropriate layout, the use of the interactions that make the data exploration
easier and a metaphor that helps in the process of information understanding. A
good combination of these elements will result in a visualization that
effectively conveys the key features of a complex structure or system to a wide
range of users and permits the analytical reasoning process. In previous works
we presented the Spherical Layout, a technique for 3D tree visualization that
provides an excellent base to achieve those key features. The layout was
implemented using the TriSphere algorithm, a method that discretized the
spheres's surfaces with triangles to achieve a uniform distribution of the
nodes. The goal of this work was centered in a new algorithm for the
implementation of the Spherical layout; we called it the Weighted Spherical
Centroidal Voronoi Tessellations (WSCVT). In this paper we present a detailed
description of this new implementation and a comparison with the TriSphere
algorithm."
"It is known that in the Minkowski sum of $r$ polytopes in dimension $d$, with
$r<d$, the number of vertices of the sum can potentially be as high as the
product of the number of vertices in each summand. However, the number of
vertices for sums of more polytopes was unknown so far. In this paper, we study
sums of polytopes in general orientations, and show a linear relation between
the number of faces of a sum of $r$ polytopes in dimension $d$, with $r\geq d$,
and the number of faces in the sums of less than $d$ of the summand polytopes.
We deduce from this exact formula a tight bound on the maximum possible number
of vertices of the Minkowski sum of any number of polytopes in any dimension.
In particular, the linear relation implies that a sum of $r$ polytopes in
dimension $d$ has a number of vertices in $O(n^{d-1})$ of the total number of
vertices in the summands, even when $r\geq d$. This bound is tight, in the
sense that some sums do have that many vertices."
"This paper defines the Arrwwid number of a recursive tiling (or space-filling
curve) as the smallest number w such that any ball Q can be covered by w tiles
(or curve sections) with total volume O(vol(Q)). Recursive tilings and
space-filling curves with low Arrwwid numbers can be applied to optimise disk,
memory or server access patterns when processing sets of points in
d-dimensional space. This paper presents recursive tilings and space-filling
curves with optimal Arrwwid numbers. For d >= 3, we see that regular cube
tilings and space-filling curves cannot have optimal Arrwwid number, and we see
how to construct alternatives with better Arrwwid numbers."
"Persistence homology is a tool used to measure topological features that are
present in data sets and functions. Persistence pairs births and deaths of
these features as we iterate through the sublevel sets of the data or function
of interest. I am concerned with using persistence to characterize the
difference between two functions f, g : M -> R, where M is a topological space.
Furthermore, I formulate a homotopy from g to f by applying the heat equation
to the difference function g-f. By stacking the persistence diagrams associated
with this homotopy, we create a vineyard of curves that connect the points in
the diagram for f with the points in the diagram for g. I look at the diagrams
where M is a square, a sphere, a torus, and a Klein bottle. Looking at these
four topologies, we notice trends (and differences) as the persistence diagrams
change with respect to time."
"Higher order Delaunay triangulations are a generalization of the Delaunay
triangulation which provides a class of well-shaped triangulations, over which
extra criteria can be optimized. A triangulation is order-$k$ Delaunay if the
circumcircle of each triangle of the triangulation contains at most $k$ points.
In this paper we study lower and upper bounds on the number of higher order
Delaunay triangulations, as well as their expected number for randomly
distributed points. We show that arbitrarily large point sets can have a single
higher order Delaunay triangulation, even for large orders, whereas for first
order Delaunay triangulations, the maximum number is $2^{n-3}$. Next we show
that uniformly distributed points have an expected number of at least
$2^{\rho_1 n(1+o(1))}$ first order Delaunay triangulations, where $\rho_1$ is
an analytically defined constant ($\rho_1 \approx 0.525785$), and for $k > 1$,
the expected number of order-$k$ Delaunay triangulations (which are not
order-$i$ for any $i < k$) is at least $2^{\rho_k n(1+o(1))}$, where $\rho_k$
can be calculated numerically."
"We present simple and practical $(1+\eps)$-approximation algorithm for the
Frechet distance between curves. To analyze this algorithm we introduce a new
realistic family of curves, $c$-packed curves, that is closed under
simplification. We believe the notion of $c$-packed curves to be of independent
interest. We show that our algorithm has near linear running time for
$c$-packed curves, and show similar results for other input models."
"Indyk and Sidiropoulos (2007) proved that any orientable graph of genus $g$
can be probabilistically embedded into a graph of genus $g-1$ with constant
distortion. Viewing a graph of genus $g$ as embedded on the surface of a sphere
with $g$ handles attached, Indyk and Sidiropoulos' method gives an embedding
into a distribution over planar graphs with distortion $2^{O(g)}$, by
iteratively removing the handles. By removing all $g$ handles at once, we
present a probabilistic embedding with distortion $O(g^2)$ for both orientable
and non-orientable graphs. Our result is obtained by showing that the nimum-cut
graph of Erickson and Har Peled (2004) has low dilation, and then randomly
cutting this graph out of the surface using the Peeling Lemma of Lee and
Sidiropoulos (2009)."
"Creating virtual models of real spaces and objects is cumbersome and time
consuming. This paper focuses on the problem of geometric reconstruction from
sparse data obtained from certain image-based modeling approaches. A number of
elegant and simple-to-state problems arise concerning when the geometry can be
reconstructed. We describe results and counterexamples, and list open problems."
"We prove that Y_6 is a spanner. Y_6 is the Yao graph on a set of planar
points, which has an edge from each point x to a closest point y within each of
the six angular cones of 60 deg surrounding x."
"Reeb graphs provide a method for studying the shape of a manifold by encoding
the evolution and arrangement of level sets of a simple Morse function defined
on the manifold. Since their introduction in computer graphics they have been
gaining popularity as an effective tool for shape analysis and matching. In
this context one question deserving attention is whether Reeb graphs are robust
against function perturbations. Focusing on 1-dimensional manifolds, we define
an editing distance between Reeb graphs of curves, in terms of the cost
necessary to transform one graph into another. Our main result is that changes
in Morse functions induce smaller changes in the editing distance between Reeb
graphs of curves, implying stability of Reeb graphs under function
perturbations."
"Given a set $P$ of $n$ points in the plane, we show how to compute in $O(n
\log n)$ time a subgraph of their Delaunay triangulation that has maximum
degree 7 and is a strong planar $t$-spanner of $P$ with $t =(1+ \sqrt{2})^2
*\delta$, where $\delta$ is the spanning ratio of the Delaunay triangulation.
Furthermore, given a Delaunay triangulation, we show a distributed algorithm
that computes the same bounded degree planar spanner in O(n) time."
"Given a set P of n points in |R^d, an eps-kernel K subset P approximates the
directional width of P in every direction within a relative (1-eps) factor. In
this paper we study the stability of eps-kernels under dynamic insertion and
deletion of points to P and by changing the approximation factor eps. In the
first case, we say an algorithm for dynamically maintaining a eps-kernel is
stable if at most O(1) points change in K as one point is inserted or deleted
from P. We describe an algorithm to maintain an eps-kernel of size
O(1/eps^{(d-1)/2}) in O(1/eps^{(d-1)/2} + log n) time per update. Not only does
our algorithm maintain a stable eps-kernel, its update time is faster than any
known algorithm that maintains an eps-kernel of size O(1/eps^{(d-1)/2}). Next,
we show that if there is an eps-kernel of P of size k, which may be
dramatically less than O(1/eps^{(d-1)/2}), then there is an (eps/2)-kernel of P
of size O(min {1/eps^{(d-1)/2}, k^{floor(d/2)} log^{d-2} (1/eps)}). Moreover,
there exists a point set P in |R^d and a parameter eps > 0 such that if every
eps-kernel of P has size at least k, then any (eps/2)-kernel of P has size
Omega(k^{floor(d/2)})."
"We prove the conjecture of Chen, Wang and Liu in [8] concerning how to
calculate the parameter values corresponding to all the singu- larities,
including the infinitely near singularities, of rational planar curves from the
Smith normal forms of certain Bezout resultant ma- trices derived from
mu-bases."
"We first describe a reduction from the problem of lower-bounding the number
of distinct distances determined by a set $S$ of $s$ points in the plane to an
incidence problem between points and a certain class of helices (or parabolas)
in three dimensions. We offer conjectures involving the new setup, but are
still unable to fully resolve them.
  Instead, we adapt the recent new algebraic analysis technique of Guth and
Katz \cite{GK}, as further developed by Elekes et al. \cite{EKS}, to obtain
sharp bounds on the number of incidences between these helices or parabolas and
points in $\reals^3$. Applying these bounds, we obtain, among several other
results, the upper bound $O(s^3)$ on the number of rotations (rigid motions)
which map (at least) three points of $S$ to three other points of $S$. In fact,
we show that the number of such rotations which map at least $k\ge 3$ points of
$S$ to $k$ other points of $S$ is close to $O(s^3/k^{12/7})$.
  One of our unresolved conjectures is that this number is $O(s^3/k^2)$, for
$k\ge 2$. If true, it would imply the lower bound $\Omega(s/\log s)$ on the
number of distinct distances in the plane."
"A technique is described for constructing three-dimensional vector graphics
representations of planar regions bounded by cubic B\'ezier curves, such as
smooth glyphs. It relies on a novel algorithm for compactly partitioning planar
B\'ezier regions into nondegenerate Coons patches. New optimizations are also
described for B\'ezier inside-outside tests and the computation of global
bounds of directionally monotonic functions over a B\'ezier surface (such as
its bounding box or optimal field-of-view angle). These algorithms underlie the
three-dimensional illustration and typography features of the TeX-aware vector
graphics language Asymptote."
"We prove that it is NP-hard to decide whether two points in a polygonal
domain with holes can be connected by a wire. This implies that finding any
approximation to the shortest path for a long snake amidst polygonal obstacles
is NP-hard. On the positive side, we show that snake's problem is
""length-tractable"": if the snake is ""fat"", i.e., its length/width ratio is
small, the shortest path can be computed in polynomial time."
"Consider the Delaunay triangulation T of a set P of points in the plane as a
Euclidean graph, in which the weight of every edge is its length. It has long
been conjectured that the dilation in T of any pair p, p \in P, which is the
ratio of the length of the shortest path from p to p' in T over the Euclidean
distance ||pp'||, can be at most {\pi}/2 \approx 1.5708. In this paper, we show
how to construct point sets in convex position with dilation > 1.5810 and in
general position with dilation > 1.5846. Furthermore, we show that a
sufficiently large set of points drawn independently from any distribution will
in the limit approach the worst-case dilation for that distribution."
"Motivated by constraint-based CAD software, we develop the foundation for the
rigidity theory of a very general model: the body-and-cad structure, composed
of rigid bodies in 3D constrained by pairwise coincidence, angular and distance
constraints. We identify 21 relevant geometric constraints and develop the
corresponding infinitesimal rigidity theory for these structures. The classical
body-and-bar rigidity model can be viewed as a body-and-cad structure that uses
only one constraint from this new class. As a consequence, we identify a new,
necessary, but not sufficient, counting condition for minimal rigidity of
body-and-cad structures: nested sparsity. This is a slight generalization of
the well-known sparsity condition of Maxwell."
"We show that every graph of maximum degree three can be drawn in three
dimensions with at most two bends per edge, and with 120-degree angles between
any two edge segments meeting at a vertex or a bend. We show that every graph
of maximum degree four can be drawn in three dimensions with at most three
bends per edge, and with 109.5-degree angles, i.e., the angular resolution of
the diamond lattice, between any two edge segments meeting at a vertex or bend."
"We study the classic graph drawing problem of drawing a planar graph using
straight-line edges with a prescribed convex polygon as the outer face. Unlike
previous algorithms for this problem, which may produce drawings with
exponential area, our method produces drawings with polynomial area. In
addition, we allow for collinear points on the boundary, provided such vertices
do not create overlapping edges. Thus, we solve an open problem of Duncan et
al., which, when combined with their work, implies that we can produce a planar
straight-line drawing of a combinatorially-embedded genus-g graph with the
graph's canonical polygonal schema drawn as a convex polygonal external face."
"We introduce the notion of Lombardi graph drawings, named after the American
abstract artist Mark Lombardi. In these drawings, edges are represented as
circular arcs rather than as line segments or polylines, and the vertices have
perfect angular resolution: the edges are equally spaced around each vertex. We
describe algorithms for finding Lombardi drawings of regular graphs, graphs of
bounded degeneracy, and certain families of planar graphs."
"We study methods for drawing trees with perfect angular resolution, i.e.,
with angles at each node v equal to 2{\pi}/d(v). We show:
  1. Any unordered tree has a crossing-free straight-line drawing with perfect
angular resolution and polynomial area.
  2. There are ordered trees that require exponential area for any
crossing-free straight-line drawing having perfect angular resolution.
  3. Any ordered tree has a crossing-free Lombardi-style drawing (where each
edge is represented by a circular arc) with perfect angular resolution and
polynomial area. Thus, our results explore what is achievable with
straight-line drawings and what more is achievable with Lombardi-style
drawings, with respect to drawings of trees with perfect angular resolution."
"This paper presents a novel implicit representation of solid models. With
this representation, every solid model can be effectively presented by three
layered depth-normal images (LDNIs) that are perpendicular to three orthogonal
axes respectively. The layered depth-normal images for a solid model, whose
boundary is presented by a polygonal mesh, can be generated efficiently with
help of the graphics hardware accelerated sampling. Based on this implicit
representation - LDNIs, solid modeling operations including the Boolean
operations and the offsetting operation have been developed. A contouring
algorithm is also introduced in this paper to generate thin structure and sharp
feature preserved mesh surfaces from the layered depth-normal images.
Comparisons between LDNIs and other implicit representation of solid models are
given at the end of the paper to demonstrate the advantages of LDNIs."
"Let $\mathcal{T}$ be a rooted and weighted tree, where the weight of any node
is equal to the sum of the weights of its children. The popular Treemap
algorithm visualizes such a tree as a hierarchical partition of a square into
rectangles, where the area of the rectangle corresponding to any node in
$\mathcal{T}$ is equal to the weight of that node. The aspect ratio of the
rectangles in such a rectangular partition necessarily depends on the weights
and can become arbitrarily high.
  We introduce a new hierarchical partition scheme, called a polygonal
partition, which uses convex polygons rather than just rectangles. We present
two methods for constructing polygonal partitions, both having guarantees on
the worst-case aspect ratio of the constructed polygons; in particular, both
methods guarantee a bound on the aspect ratio that is independent of the
weights of the nodes.
  We also consider rectangular partitions with slack, where the areas of the
rectangles may differ slightly from the weights of the corresponding nodes. We
show that this makes it possible to obtain partitions with constant aspect
ratio. This result generalizes to hyper-rectangular partitions in
$\mathbb{R}^d$. We use these partitions with slack for embedding ultrametrics
into $d$-dimensional Euclidean space: we give a $\mathop{\rm
polylog}(\Delta)$-approximation algorithm for embedding $n$-point ultrametrics
into $\mathbb{R}^d$ with minimum distortion, where $\Delta$ denotes the spread
of the metric, i.e., the ratio between the largest and the smallest distance
between two points. The previously best-known approximation ratio for this
problem was polynomial in $n$. This is the first algorithm for embedding a
non-trivial family of weighted-graph metrics into a space of constant dimension
that achieves polylogarithmic approximation ratio."
"In 1959, Erd\H{o}s and Moser asked for the maximum number of unit distances
that may be formed among the vertices of a convex $n$-gon; until now, the best
known upper bound has been $2\pi n \log_2 n + O(n)$, achieved by F\""uredi in
1990. In this paper, we examine two properties that any convex polygon must
satisfy and use them to prove several new facts related to the question posed
by Erd\H{o}s and Moser. In particular, we improve on F\""uredi's result, and
instead obtain a bound of $n \log_2 n + O(n)$; we exhibit a class of `cycles'
formed by unit distances that are forbidden in convex polygons; and we provide
a lower bound that shows the limitations of our methods. The second result
addresses a question asked by Fishburn and Reeds regarding the possible
configurations of vertices that form a convex polygon."
"Given a set S of n points in the plane and a fixed angle 0 < omega < pi, we
show how to find in O(n log n) time all triangles of minimum area with one
angle omega that enclose S. We prove that in general, the solution cannot be
written without cubic roots. We also prove an Omega(n log n) lower bound for
this problem in the algebraic computation tree model. If the input is a convex
n-gon, our algorithm takes Theta(n) time."
"We study connectivity relations among points, where the precise location of
each input point lies in a region of uncertainty. We distinguish two
fundamental scenarios under which uncertainty arises. In the favorable
Best-Case Uncertainty (BU), each input point can be chosen from a given set to
yield the best possible objective value. In the unfavorable Worst-Case
Uncertainty (WU), the input set has worst possible objective value among all
possible point locations, which are uncertain due, for example, to imprecise
data. We consider these notions of uncertainty for the bottleneck spanning tree
problem, giving rise to the following Best-Case Connectivity with Uncertainty
(BCU) problem: Given a family of geometric regions, choose one point per
region, such that the longest edge length of an associated geometric spanning
tree is minimized. We show that this problem is NP-hard even for very simple
scenarios in which the regions are line segments or squares. On the other hand,
we give an exact solution for the case in which there are n+k regions, where k
of the regions are line segments and n of the regions are fixed points. We then
give approximation algorithms for cases where the regions are either all line
segments or all unit discs. We also provide approximation methods for the
corresponding Worst-Case Connectivity with Uncertainty (WCU) problem: Given a
set of uncertainty regions, find the minimal distance r such that for any
choice of points, one per region, there is a spanning tree among the points
with edge length at most r."
"We describe a data structure, called a priority range tree, which
accommodates fast orthogonal range reporting queries on prioritized points. Let
$S$ be a set of $n$ points in the plane, where each point $p$ in $S$ is
assigned a weight $w(p)$ that is polynomial in $n$, and define the rank of $p$
to be $r(p)=\lfloor \log w(p) \rfloor$. Then the priority range tree can be
used to report all points in a three- or four-sided query range $R$ with rank
at least $\lfloor \log w \rfloor$ in time $O(\log W/w + k)$, and report $k$
highest-rank points in $R$ in time $O(\log\log n + \log W/w' + k)$, where
$W=\sum_{p\in S}{w(p)}$, $w'$ is the smallest weight of any point reported, and
$k$ is the output size. All times assume the standard RAM model of computation.
If the query range of interest is three sided, then the priority range tree
occupies $O(n)$ space, otherwise $O(n\log n)$ space is used to answer
four-sided queries. These queries are motivated by the Weber--Fechner Law,
which states that humans perceive and interpret data on a logarithmic scale."
"CAT(0) metric spaces constitute a far-reaching common generalization of
Euclidean and hyperbolic spaces and simple polygons: any two points x and y of
a CAT(0) metric space are connected by a unique shortest path {\gamma}(x,y). In
this paper, we present an efficient algorithm for answering two-point distance
queries in CAT(0) rectangular complexes and two of theirs subclasses, ramified
rectilinear polygons (CAT(0) rectangular complexes in which the links of all
vertices are bipartite graphs) and squaregraphs (CAT(0) rectangular complexes
arising from plane quadrangulations in which all inner vertices have degrees
\geq4). Namely, we show that for a CAT(0) rectangular complex K with n
vertices, one can construct a data structure D of size $O(n^2)$ so that, given
any two points x,y in K, the shortest path {\gamma}(x,y) between x and y can be
computed in O(d(p,q)) time, where p and q are vertices of two faces of K
containing the points x and y, respectively, such that {\gamma}(x,y) is
contained in K(I(p,q)) and d(p,q) is the distance between p and q in the
underlying graph of K. If K is a ramified rectilinear polygon, then one can
construct a data structure D of optimal size O(n) and answer two-point shortest
path queries in O(d(p,q)log{\Delta}) time, where {\Delta} is the maximal degree
of a vertex of G(K). Finally, if K is a squaregraph, then one can construct a
data structure D of size O(nlogn) and answer two-point shortest path queries in
O(d(p,q)) time."
"We present an innovative algorithm that simplifies the topology of a
cross-field. Our algorithm works through macro-operations that allow us editing
the graph of separatrices, which is extracted from a cross-field, while
maintaining it topologically consistent. We present preliminary results of our
implementation."
"The problem of combinatorially determining the rank of the 3-dimensional
bar-joint {\em rigidity matroid} of a graph is an important open problem in
combinatorial rigidity theory. Maxwell's condition states that the edges of a
graph $G=(V, E)$ are {\em independent} in its $d$-dimensional generic rigidity
matroid only if $(a)$ the number of edges $|E|$ $\le$ $d|V| - {d+1\choose 2}$,
and $(b)$ this holds for every induced subgraph with at least $d$ vertices. We
call such graphs {\em Maxwell-independent} in $d$ dimensions. Laman's theorem
shows that the converse holds for $d=2$ and thus every maximal
Maxwell-independent set of $G$ has size equal to the rank of the 2-dimensional
generic rigidity matroid. While this is false for $d=3$, we show that every
maximal, Maxwell-independent set of a graph $G$ has size at least the rank of
the 3-dimensional generic rigidity matroid of $G$. This answers a question
posed by Tib\'or Jord\'an at the 2008 rigidity workshop at BIRS
\cite{bib:birs}.
  Along the way, we construct subgraphs (1) that yield alternative formulae for
a rank upper bound for Maxwell-independent graphs and (2) that contain a
maximal (true) independent set. We extend this bound to special classes of
non-Maxwell-independent graphs. One further consequence is a simpler proof of
correctness for existing algorithms that give rank bounds."
"This paper deals with the containment problem under homothetics which has the
minimal enclosing ball (MEB) problem as a prominent representative. We connect
the problem to results in classic convex geometry and introduce a new series of
radii, which we call core-radii. For the MEB problem, these radii have already
been considered from a different point of view and sharp inequalities between
them are known. In this paper sharp inequalities between core-radii for general
containment under homothetics are obtained. Moreover, the presented
inequalities are used to derive sharp upper bounds on the size of core-sets for
containment under homothetics. In the MEB case, this yields a tight (dimension
independent) bound for the size of such core-sets. In the general case, we show
that there are core-sets of size linear in the dimension and that this bound
stays sharp even if the container is required to be symmetric."
"We present an engineered version of the divide-and-conquer algorithm for
finding the closest pair of points, within a given set of points in the
XY-plane. For this version of the algorithm we show that only two pairwise
comparisons are required in the combine step, for each point that lies in the 2
delta-wide vertical slab. The correctness of the algorithm is shown for all
Minkowski distances with p>=1. We also show empirically that, although the time
complexity of the algorithm is still O(n lg n), the reduction in the total
number of comparisons leads to a significant reduction in the total execution
time, for inputs with size sufficiently large."
"The inclusion relation between simple objects in the plane may be used to
define geometric set systems, or hypergraphs. Properties of various types of
colorings of these hypergraphs have been the subject of recent investigations,
with applications to wireless networking.
  We first prove that every set of homothetic copies of a given convex body in
the plane can be colored with four colors so that any point covered by at least
two copies is covered by two copies with distinct colors. This generalizes a
previous result from Smorodinsky [18]. As a corollary, we find improvements to
well studied variations of the coloring problem such as conflict-free
colorings, k-strong (conflict-free) colorings and choosability. We also show a
relation between our proof and Schnyder's characterization of planar graphs.
Then we show that for any k >1, every three-dimensional hypergraph can be
colored with 6(k - 1) colors so that every hyperedge e contains min{|e|, k}
vertices with mutually distinct colors. Furthermore, we also show that at least
2k colors might be necessary. This refines a previous result from Aloupis et
al. [2]."
"A planar straight-line graph which causes the non-termination Ruppert's
algorithm for a minimum angle threshold larger than about 29.5 degrees is
given. The minimum input angle of this example is about 74.5 degrees meaning
that failure is not due to small input angles. Additionally, a similar
non-acute input is given for which Chew's second algorithm does not terminate
for a minimum angle threshold larger than about 30.7 degrees."
"We present approximation algorithms with O(n^3) processing time for the
minimum vertex and edge guard problems in simple polygons. It is improved from
previous O(n^4) time algorithms of Ghosh. For simple polygon, there are O(n^3)
visibility regions, thus any approximation algorithm for the set covering
problem with approximation ratio of log(n) can be used for the approximation of
n vertex and edge guard problems with O(n^3) visibility sequence. We prove that
the visibility of all points in simple polygons is guaranteed by covering
O(n^2) sinks from vertices and edges : It comes to O(n^3) time bound."
"For a polygon P with n vertices, the vertex guarding problem asks for the
minimum subset G of P's vertices such that every point in P is seen by at least
one point in G. This problem is NP-complete and APX-hard. The first
approximation algorithm (Ghosh, 1987) involves decomposing P into O(n^4) cells
that are equivalence classes for visibility from the vertices of P. This
discretized problem can then be treated as an instance of set cover and solved
in O(n^5) time with a greedy O(log n)-approximation algorithm. Ghosh (2010)
recently revisited the algorithm, noting that minimum visibility decompositions
for simple polygons (Bose et al., 2000) have only O(n^3) cells, improving the
running time of the algorithm to O(n^4) for simple polygons.
  In this paper we show that, since minimum visibility decompositions for
simple polygons have only O(n^2) cells of minimal visibility (Bose et al.,
2000), the running time of the algorithm can be further improved to O(n^3).
This result was obtained independently by Jang and Kwon (2011). We extend the
result of Bose et al. to polygons with holes, showing that a minimum visibility
decomposition of a polygon with h holes has only O((h+1)n^3) cells and only
O((h+1)^2 n^2) cells of minimal visibility. We exploit this result to obtain a
faster algorithm for vertex guarding polygons with holes. We then show that, in
the same time complexity, we can attain approximation factors of O(log
log(opt)) for simple polygons and O((1+\log((h+1))) log(opt)) for polygons with
holes."
"Given a convex region in the plane, and a sweep-line as a tool, what is best
way to reduce the region to a single point by a sequence of sweeps? The problem
of sweeping points by orthogonal sweeps was first studied in [2]. Here we
consider the following \emph{slanted} variant of sweeping recently introduced
in [1]: In a single sweep, the sweep-line is placed at a start position
somewhere in the plane, then moved continuously according to a sweep vector
$\vec v$ (not necessarily orthogonal to the sweep-line) to another parallel end
position, and then lifted from the plane. The cost of a sequence of sweeps is
the sum of the lengths of the sweep vectors. The (optimal) sweeping cost of a
region is the infimum of the costs over all finite sweeping sequences for that
region. An optimal sweeping sequence for a region is one with a minimum total
cost, if it exists. Another parameter of interest is the number of sweeps.
  We show that there exist convex regions for which the optimal sweeping cost
cannot be attained by two sweeps. This disproves a conjecture of Bousany,
Karker, O'Rourke, and Sparaco stating that two sweeps (with vectors along the
two adjacent sides of a minimum-perimeter enclosing parallelogram) always
suffice [1]. Moreover, we conjecture that for some convex regions, no finite
sweeping sequence is optimal. On the other hand, we show that both the 2-sweep
algorithm based on minimum-perimeter enclosing rectangle and the 2-sweep
algorithm based on minimum-perimeter enclosing parallelogram achieve a $4/\pi
\approx 1.27$ approximation in this sweeping model."
"In this paper, we generalize the simple Euclidean 1-center approximation
algorithm of Badoiu and Clarkson (2003) to Riemannian geometries and study
accordingly the convergence rate. We then show how to instantiate this generic
algorithm to two particular cases: (1) hyperbolic geometry, and (2) Riemannian
manifold of symmetric positive definite matrices."
"We consider the following cake cutting game: Alice chooses a set P of n
points in the square (cake) [0,1]^2, where (0,0) is in P; Bob cuts out n
axis-parallel rectangles with disjoint interiors, each of them having a point
of P as the lower left corner; Alice keeps the rest. It has been conjectured
that Bob can always secure at least half of the cake. This remains unsettled,
and it is not even known whether Bob can get any positive fraction independent
of n. We prove that if Alice can force Bob's share to tend to zero, then she
must use very many points; namely, to prevent Bob from gaining more than 1/r of
the cake, she needs at least 2^{2^{\Omega(r)}} points."
"We consider the problem of maintaining the Euclidean Delaunay triangulation
$\DT$ of a set $P$ of $n$ moving points in the plane, along algebraic
trajectories of constant description complexity. Since the best known upper
bound on the number of topological changes in the full $\DT$ is nearly cubic,
we seek to maintain a suitable portion of it that is less volatile yet retains
many useful properties. We introduce the notion of a stable Delaunay graph,
which is a dynamic subgraph of the Delaunay triangulation. The stable Delaunay
graph (a) is easy to define, (b) experiences only a nearly quadratic number of
discrete changes, (c) is robust under small changes of the norm, and (d)
possesses certain useful properties.
  The stable Delaunay graph ($\SDG$ in short) is defined in terms of a
parameter $\alpha>0$, and consists of Delaunay edges $pq$ for which the angles
at which $p$ and $q$ see their Voronoi edge $e_{pq}$ are at least $\alpha$. We
show that (i) $\SDG$ always contains at least roughly one third of the Delaunay
edges; (ii) it contains the $\beta$-skeleton of $P$, for
$\beta=1+\Omega(\alpha^2)$; (iii) it is stable, in the sense that its edges
survive for long periods of time, as long as the orientations of the segments
connecting (nearby) points of $P$ do not change by much; and (iv) stable
Delaunay edges remain stable (with an appropriate redefinition of stability) if
we replace the Euclidean norm by any sufficiently close norm.
  In particular, we can approximate the Euclidean norm by a polygonal norm
(namely, a regular $k$-gon, with $k=\Theta(1/\alpha)$), and keep track of a
Euclidean $\SDG$ by maintaining the full Delaunay triangulation of $P$ under
the polygonal norm.
  We describe two kinetic data structures for maintaining $\SDG$. Both
structures use $O^*(n)$ storage and process $O^*(n^2)$ events during the
motion, each in $O^*(1)$ time."
"In this paper we study a facility location problem in the plane in which a
single point (facility) and a rapid transit line (highway) are simultaneously
located in order to minimize the total travel time of the clients to the
facility, using the $L_1$ or Manhattan metric. The rapid transit line is
represented by a line segment with fixed length and arbitrary orientation. The
highway is an alternative transportation system that can be used by the clients
to reduce their travel time to the facility. This problem was introduced by
Espejo and Ch\'ia in [7]. They gave both a characterization of the optimal
solutions and an algorithm running in $O(n^3\log n)$ time, where $n$ represents
the number of clients. In this paper we show that the Espejo and Ch\'ia's
algorithm does not always work correctly. At the same time, we provide a proper
characterization of the solutions with a simpler proof and give an algorithm
solving the problem in $O(n^3)$ time."
"There is a high demand of space-efficient algorithms in built-in or embedded
softwares. In this paper, we consider the problem of designing space-efficient
algorithms for computing the maximum area empty rectangle (MER) among a set of
points inside a rectangular region $\cal R$ in 2D. We first propose an inplace
algorithm for computing the priority search tree with a set of $n$ points in
$\cal R$ using $O(\log n)$ extra bit space in $O(n\log n)$ time. It supports
all the standard queries on priority search tree in $O(\log^2n)$ time. We also
show an application of this algorithm in computing the largest empty
axis-parallel rectangle. Our proposed algorithm needs $O(n\log^2n +m)$ time and
$O(\log n)$ work-space apart from the array used for storing $n$ input points.
Here $m$ is the number of maximal empty rectangles present in $\cal R$.
Finally, we consider the problem of locating the maximum area empty rectangle
of arbitrary orientation among a set of $n$ points, and propose an $O(n^3\log
n)$ time in-place algorithm for that problem."
"The Searchlight Scheduling Problem was first studied in 2D polygons, where
the goal is for point guards in fixed positions to rotate searchlights to catch
an evasive intruder. Here the problem is extended to 3D polyhedra, with the
guards now boundary segments who rotate half-planes of illumination. After
carefully detailing the 3D model, several results are established. The first is
a nearly direct extension of the planar one-way sweep strategy using what we
call exhaustive guards, a generalization that succeeds despite there being no
well-defined notion in 3D of planar ""clockwise rotation"". Next follow two
results: every polyhedron with r>0 reflex edges can be searched by at most r^2
suitably placed guards, whereas just r guards suffice if the polyhedron is
orthogonal. (Minimizing the number of guards to search a given polyhedron is
easily seen to be NP-hard.) Finally we show that deciding whether a given set
of guards has a successful search schedule is strongly NP-hard, and that
deciding if a given target area is searchable at all is strongly PSPACE-hard,
even for orthogonal polyhedra. A number of peripheral results are proved en
route to these central theorems, and several open problems remain for future
work."
"In this paper, we consider the problem of choosing disks (that we can think
of as corresponding to wireless sensors) so that given a set of input points in
the plane, there exists no path between any pair of these points that is not
intercepted by some disk. We try to achieve this separation using a minimum
number of a given set of unit disks. We show that a constant factor
approximation to this problem can be found in polynomial time using a greedy
algorithm. To the best of our knowledge we are the first to study this
optimization problem."
"Dynamic maps that allow continuous map rotations, e.g., on mobile devices,
encounter new issues unseen in static map labeling before. We study the
following dynamic map labeling problem: The input is a static, labeled map,
i.e., a set P of points in the plane with attached non-overlapping horizontal
rectangular labels. The goal is to find a consistent labeling of P under
rotation that maximizes the number of visible labels for all rotation angles
such that the labels remain horizontal while the map is rotated. A labeling is
consistent if a single active interval of angles is selected for each label
such that labels neither intersect each other nor occlude points in P at any
rotation angle. We first introduce a general model for labeling rotating maps
and derive basic geometric properties of consistent solutions. We show
NP-completeness of the active interval maximization problem even for
unit-square labels. We then present a constant-factor approximation for this
problem based on line stabbing, and refine it further into an efficient
polynomial-time approximation scheme (EPTAS). Finally, we extend the EPTAS to
the more general setting of rectangular labels of bounded size and aspect
ratio."
"In this article, we re-introduce the so called ""Arkaden-Faden-Lage"" (AFL for
short) representation of knots in 3 dimensional space introduced by Kurt
Reidemeister and show how it can be used to develop efficient algorithms to
compute some important topological knot structures. In particular, we introduce
an efficient algorithm to calculate holonomic representation of knots
introduced by V. Vassiliev and give the main ideas how to use the AFL
representations of knots to compute the Kontsevich Integral.
  The methods introduced here are to our knowledge novel and can open new
perspectives in the development of fast algorithms in low dimensional topology."
"What is the minimum angle $\alpha >0$ such that given any set of
$\alpha$-directional antennas (that is, antennas each of which can communicate
along a wedge of angle $\alpha$), one can always assign a direction to each
antenna such that the resulting communication graph is connected? Here two
antennas are connected by an edge if and only if each lies in the wedge
assigned to the other. This problem was recently presented by Carmi, Katz,
Lotker, and Ros\'en \cite{CKLR10} who also found the minimum such $\alpha$
namely $\alpha=\frac{\pi}{3}$. In this paper we give a simple proof of this
result. Moreover, we obtain a much stronger and optimal result (see Theorem
\ref{theorem:main}) saying in particular that one can chose the directions of
the antennas so that the communication graph has diameter $\le 4$.
  Our main tool is a surprisingly basic geometric lemma that is of independent
interest. We show that for every compact convex set $S$ in the plane and every
$0 < \alpha < \pi$, there exist a point $O$ and two supporting lines to $S$
passing through $O$ and touching $S$ at two \emph{single points} $X$ and $Y$,
respectively, such that $|OX|=|OY|$ and the angle between the two lines is
$\alpha$."
"In his article ""Powerlist: A Structure for Parallel Recursion"" Jayadev Misra
wrote:
  ""Many data parallel algorithms - Fast Fourier Transform, Batcher's sorting
schemes and prefix sum - exhibit recursive structure. We propose a data
structure, powerlist, that permits succinct descriptions of such algorithms,
highlighting the roles of both parallelism and recursion. Simple algebraic
properties of this data structure can be exploited to derive properties of
these algorithms and establish equivalence of different algorithms that solve
the same problem.""
  The quote above illustrates a widely shared assumption about recursion
implementations: either they are done in purely structural terms or they cannot
be done at all.
  Multi-dimensional interpolation on a grid is one of hosts of semi-recursive
schemes that, while often referred to as recursive and routinely described in
vaguely recursive terms, cannot be implemented as a recursion in their
structural entirety.
  This article describes a computer-implemented scheme for isolating the
recursive core of interpolation on a multi-grid, an arrangement that both stems
from and provides a structural framework to a number of multi-dimensional
interpolation optimization techniques that, once implemented, provide gains in
multi-dimensional interpolation speed that, compared to some known benchmarks,
measure in multiple orders of magnitude.
  Categories and Subject Descriptors: Multi-dimensional Programming; Concurrent
Programming; Recursion
  General terms: Parallel Processing, Prioritized Processing, Interpolation,
Recursion, Multi-Cube"
"We study the motion-planning problem for a car-like robot whose turning
radius is bounded from below by one and which is allowed to move in the forward
direction only (Dubins car). For two robot configurations $\sigma, \sigma'$,
let $\ell(\sigma, \sigma')$ be the shortest bounded-curvature path from
$\sigma$ to $\sigma'$. For $d \geq 0$, let $\ell(d)$ be the supremum of
$\ell(\sigma, \sigma')$, over all pairs $(\sigma, \sigma')$ that are at
Euclidean distance $d$. We study the function $\dub(d) = \ell(d) - d$, which
expresses the difference between the bounded-curvature path length and the
Euclidean distance of its endpoints. We show that $\dub(d)$ decreases
monotonically from $\dub(0) = 7\pi/3$ to $\dub(\ds) = 2\pi$, and is constant
for $d \geq \ds$. Here $\ds \approx 1.5874$. We describe pairs of
configurations that exhibit the worst-case of $\dub(d)$ for every distance $d$."
"Let P be a set of points in the plane, representing directional antennas of
angle a and range r. The coverage area of the antenna at point p is a circular
sector of angle a and radius r, whose orientation is adjustable. For a given
orientation assignment, the induced symmetric communication graph (SCG) of P is
the undirected graph that contains the edge (u,v) iff v lies in u's sector and
vice versa. In this paper we ask what is the smallest angle a for which there
exists an integer n=n(a), such that for any set P of n antennas of angle a and
unbounded range, one can orient the antennas so that the induced SCG is
connected, and the union of the corresponding wedges is the entire plane. We
show that the answer to this problem is a=\pi/2, for which n=4. Moreover, we
prove that if Q_1 and Q_2 are quadruplets of antennas of angle \pi/2 and
unbounded range, separated by a line, to which one applies the above
construction, independently, then the induced SCG of Q_1 \cup Q_2 is connected.
This latter result enables us to apply the construction locally, and to solve
the following two further problems.
  In the first problem, we are given a connected unit disk graph (UDG),
corresponding to a set P of omni-directional antennas of range 1, and the goal
is to replace these antennas by directional antennas of angle \pi/2 and range
r=O(1) and to orient them, such that the induced SCG is connected, and,
moreover, is an O(1)-spanner of the UDG, w.r.t. hop distance. In our solution r
= 14\sqrt{2} and the spanning ratio is 8. In the second problem, we are given a
set P of directional antennas of angle \pi/2 and adjustable range. The goal is
to assign to each antenna p, an orientation and a range r_p, such that the
resulting SCG is connected, and \sum_{p \in P} r_p^\beta is minimized, where
\beta \ge 1 is a constant. We present an O(1)-approximation algorithm."
"In this paper we present the first provable approximate nearest-neighbor
(ANN) algorithms for Bregman divergences. Our first algorithm processes queries
in O(log^d n) time using O(n log^d n) space and only uses general properties of
the underlying distance function (which includes Bregman divergences as a
special case). The second algorithm processes queries in O(log n) time using
O(n) space and exploits structural constants associated specifically with
Bregman divergences. An interesting feature of our algorithms is that they
extend the ring-tree + quad-tree paradigm for ANN searching beyond Euclidean
distances and metrics of bounded doubling dimension to distances that might not
even be symmetric or satisfy a triangle inequality."
"In a witness rectangle graph (WRG) on vertex point set P with respect to
witness points set W in the plane, two points x, y in P are adjacent whenever
the open isothetic rectangle with x and y as opposite corners contains at least
one point in W. WRGs are representative of a a larger family of witness
proximity graphs introduced in two previous papers. We study graph-theoretic
properties of WRGs. We prove that any WRG has at most two non-trivial connected
components. We bound the diameter of the non-trivial connected components of a
WRG in both the one-component and two-component cases. In the latter case, we
prove that a graph is representable as a WRG if and only if each component is a
connected co-interval graph, thereby providing a complete characterization of
WRGs of this type. We also completely characterize trees drawable as WRGs. In
addition, we prove that a WRG with no isolated vertices has domination number
at most four. Moreover, we show that any combinatorial graph can be drawn as a
WRG using a combination of positive and negative witnesses. Finally we conclude
with some related results on the number of points required to stab all the
rectangles defined by a set of n point."
"The theory of zigzag persistence is a substantial extension of persistent
homology, and its development has enabled the investigation of several
unexplored avenues in the area of topological data analysis. In this paper, we
discuss three applications of zigzag persistence: topological bootstrapping,
parameter thresholding, and the comparison of witness complexes."
"When designing a product that needs to fit the human shape, designers often
use a small set of 3D models, called design models, either in physical or
digital form, as representative shapes to cover the shape variabilities of the
population for which the products are designed. Until recently, the process of
creating these models has been an art involving manual interaction and
empirical guesswork. The availability of the 3D anthropometric databases
provides an opportunity to create design models optimally. In this paper, we
propose a novel way to use 3D anthropometric databases to generate design
models that represent a given population for design applications such as the
sizing of garments and gear. We generate the representative shapes by solving a
covering problem in a parameter space. Well-known techniques in computational
geometry are used to solve this problem. We demonstrate the method using
examples in designing glasses and helmets."
"We show that several problems of compacting orthogonal graph drawings to use
the minimum number of rows, area, length of longest edge or total edge length
cannot be approximated better than within a polynomial factor of optimal in
polynomial time unless P = NP. We also provide a fixed-parameter-tractable
algorithm for testing whether a drawing can be compacted to a small number of
rows."
"We present algorithms for length-constrained maximum sum segment and maximum
density segment problems, in particular, and the problem of finding
length-constrained heaviest segments, in general, for a sequence of real
numbers. Given a sequence of n real numbers and two real parameters L and U (L
<= U), the maximum sum segment problem is to find a consecutive subsequence,
called a segment, of length at least L and at most U such that the sum of the
numbers in the subsequence is maximum. The maximum density segment problem is
to find a segment of length at least L and at most U such that the density of
the numbers in the subsequence is the maximum. For the first problem with
non-uniform width there is an algorithm with time and space complexities in
O(n). We present an algorithm with time complexity in O(n) and space complexity
in O(U). For the second problem with non-uniform width there is a combinatorial
solution with time complexity in O(n) and space complexity in O(U). We present
a simple geometric algorithm with the same time and space complexities.
  We extend our algorithms to respectively solve the length-constrained k
maximum sum segments problem in O(n+k) time and O(max{U, k}) space, and the
length-constrained $k$ maximum density segments problem in O(n min{k, U-L})
time and O(U+k) space. We present extensions of our algorithms to find all the
length-constrained segments having user specified sum and density in O(n+m) and
O(nlog (U-L)+m) times respectively, where m is the number of output.
Previously, there was no known algorithm with non-trivial result for these
problems. We indicate the extensions of our algorithms to higher dimensions.
All the algorithms can be extended in a straight forward way to solve the
problems with non-uniform width and non-uniform weight."
"Separable nonlinear least squares (SNLS)problem is a special class of
nonlinear least squares (NLS)problems, whose objective function is a mixture of
linear and nonlinear functions. It has many applications in many different
areas, especially in Operations Research and Computer Sciences. They are
difficult to solve with the infinite-norm metric. In this paper, we give a
short note on the separable nonlinear least squares problem, unseparated scheme
for NLS, and propose an algorithm for solving mixed linear-nonlinear
minimization problem, method of which results in solving a series of least
squares separable problems."
"We investigate the problem of finding the visible pieces of a scene of
objects from a specified viewpoint. In particular, we are interested in the
design of an efficient hidden surface removal algorithm for a scene comprised
of iso-oriented rectangles. We propose an algorithm where given a set of $n$
iso-oriented rectangles we report all visible surfaces in $O((n+k)\log n)$ time
and linear space, where $k$ is the number of surfaces reported. The previous
best result by Bern, has the same time complexity but uses $O(n\log n)$ space."
"We study the problem of arranging a set of $n$ disks with prescribed radii on
$n$ rays emanating from the origin such that two neighboring rays are separated
by an angle of $2\pi/n$. The center of the disks have to lie on the rays, and
no two disk centers are allowed to lie on the same ray. We require that the
disks have disjoint interiors, and that for every ray the segment between the
origin and the boundary of its associated disk avoids the interior of the
disks. Let $\r$ be the sum of the disk radii. We introduce a greedy strategy
that constructs such a disk arrangement that can be covered with a disk
centered at the origin whose radius is at most $2\r$, which is best possible.
The greedy strategy needs O(n) arithmetic operations.
  As an application of our result we present an algorithm for embedding
unordered trees with straight lines and perfect angular resolution such that it
can be covered with a disk of radius $n^{3.0367}$, while having no edge of
length smaller than 1. The tree drawing algorithm is an enhancement of a recent
result by Duncan et al. [Symp. of Graph Drawing, 2010] that exploits the
heavy-edge tree decomposition technique to construct a drawing of the tree that
can be covered with a disk of radius $2 n^4$."
"Hilbert's two-dimensional space-filling curve is appreciated for its good
locality properties for many applications. However, it is not clear what is the
best way to generalize this curve to filling higher-dimensional spaces. We
argue that the properties that make Hilbert's curve unique in two dimensions,
are shared by 10694807 structurally different space-filling curves in three
dimensions. These include several curves that have, in some sense, better
locality properties than any generalized Hilbert curve that has been considered
in the literature before."
"In this paper we consider the question of sensor network coverage for a
2-dimensional domain. We seek to compute the probability that a set of sensors
fails to cover given only non-metric, local (who is talking to whom)
information and a probability distribution of failure of each node. This builds
on the work of de Silva and Ghrist who analyzed this problem in the
deterministic situation. We first show that a it is part of a slightly larger
class of problems which is #P-complete, and thus fast algorithms likely do not
exist unless P$=$NP. We then give a deterministic algorithm which is feasible
in the case of a small set of sensors, and give a dynamic algorithm for an
arbitrary set of sensors failing over time which utilizes a new criterion for
coverage based on the one proposed by de Silva and Ghrist. These algorithms
build on the theory of topological persistence."
"We show that any combinatorial triangulation on n vertices can be transformed
into a 4-connected one using at most floor((3n - 9)/5) edge flips. We also give
an example of an infinite family of triangulations that requires this many
flips to be made 4-connected, showing that our bound is tight. In addition, for
n >= 19, we improve the upper bound on the number of flips required to
transform any 4-connected triangulation into the canonical triangulation (the
triangulation with two dominant vertices), matching the known lower bound of 2n
- 15. Our results imply a new upper bound on the diameter of the flip graph of
5.2n - 33.6, improving on the previous best known bound of 6n - 30."
"We investigate the combinatorial complexity of geodesic Voronoi diagrams on
polyhedral terrains using a probabilistic analysis. Aronov etal [ABT08] prove
that, if one makes certain realistic input assumptions on the terrain, this
complexity is \Theta(n + m \sqrt n) in the worst case, where n denotes the
number of triangles that define the terrain and m denotes the number of Voronoi
sites. We prove that under a relaxed set of assumptions the Voronoi diagram has
expected complexity O(n+m), given that the sites have a uniform distribution on
the domain of the terrain(or the surface of the terrain). Furthermore, we
present a worst-case construction of a terrain which implies a lower bound of
Vmega(n m2/3) on the expected worst-case complexity if these assumptions on the
terrain are dropped. As an additional result, we can show that the expected
fatness of a cell in a random planar Voronoi diagram is bounded by a constant."
"Let $S$ be a set of $n$ points in $\mathbb{R}^d$. A Steiner convex partition
is a tiling of ${\rm conv}(S)$ with empty convex bodies. For every integer $d$,
we show that $S$ admits a Steiner convex partition with at most $\lceil
(n-1)/d\rceil$ tiles. This bound is the best possible for points in general
position in the plane, and it is best possible apart from constant factors in
every fixed dimension $d\geq 3$. We also give the first constant-factor
approximation algorithm for computing a minimum Steiner convex partition of a
planar point set in general position. Establishing a tight lower bound for the
maximum volume of a tile in a Steiner convex partition of any $n$ points in the
unit cube is equivalent to a famous problem of Danzer and Rogers. It is
conjectured that the volume of the largest tile is $\omega(1/n)$.
  Here we give a $(1-\varepsilon)$-approximation algorithm for computing the
maximum volume of an empty convex body amidst $n$ given points in the
$d$-dimensional unit box $[0,1]^d$."
"This paper discusses the dimension of spline spaces with highest order
smoothness over hierarchical T-meshes over certain type of hierarchical
T-meshes. The major step is to set up a bijection between the spline space with
highest order smoothness over a hierarchical T-mesh and a univariate spline
space whose definition depends on the l-edges of the extended T-mesh. We
decompose the univariate spline space into direct sums in the sense of
isomorphism using the theory of the short exact sequence in homological
algebra. According to the decomposition of the univariate spline space, the
dimension formula of the spline space with highest order smoothness over
certain type of hierarchical T-mesh is presented. A set of basis functions of
the spline space is also constructed."
"We present a new circular-arc cartogram model in which countries are drawn as
polygons with circular arcs instead of straight-line segments. Given a
political map and values associated with each country in the map, a cartogram
is a distorted map in which the areas of the countries are proportional to the
corresponding values. In the circular-arc cartogram model straight-line
segments can be replaced by circular arcs in order to modify the areas of the
polygons, while the corners of the polygons remain fixed. The countries in
circular-arc cartograms have the aesthetically pleasing appearance of clouds or
snowflakes, depending on whether their edges are bent outwards or inwards. This
makes it easy to determine whether a country has grown or shrunk, just by its
overall shape. We show that determining whether a given map and given
area-values can be realized as a circular-arc cartogram is an NP-hard problem.
Next we describe a heuristic method for constructing circular-arc cartograms,
which uses a max-flow computation on the dual graph of the map, along with a
computation of the straight skeleton of the underlying polygonal decomposition.
Our method is implemented and produces cartograms that, while not yet perfectly
accurate, achieve many of the desired areas in our real-world examples."
"We analyze the setting of minimum-cost perfect matchings with selfish
vertices through the price of anarchy (PoA) and price of stability (PoS) lens.
The underlying solution concept used for this analysis is the Gale-Shapley
stable matching notion, where the preferences are determined so that each
player (vertex) wishes to minimize the cost of her own matching edge."
"A constant-workspace algorithm has read-only access to an input array and may
use only O(1) additional words of $O(\log n)$ bits, where $n$ is the size of
the input. We assume that a simple $n$-gon is given by the ordered sequence of
its vertices. We show that we can find a triangulation of a plane straight-line
graph in $O(n^2)$ time. We also consider preprocessing a simple polygon for
shortest path queries when the space constraint is relaxed to allow $s$ words
of working space. After a preprocessing of $O(n^2)$ time, we are able to solve
shortest path queries between any two points inside the polygon in $O(n^2/s)$
time."
"We consider longstanding questions concerning configuration spaces of 1-dof
tree-decomposable linkages in 2D. By employing the notion Cayley configuration
space, i.e., a set of intervals of realizable distance-values for an
independent non-edge, we answer the following. (1) How to measure the
complexity of the configuration space and efficiently compute that of low
algebraic complexity? (2) How to restrict the Cayley configuration space to be
a single interval? (3) How to efficiently obtain continuous motion paths
between realizations? (4) How to bijectively represent of the Cartesian
realization space as a curve in an ambient space of minimum dimension? (5) How
robust is the complexity measure (1) and how to efficiently classify linkages
according to it?
  In Part I of this paper, we deal with problems (1)-(4) by introducing the
notions of (a) Cayley size, the number of intervals in the Cayley configuration
space, (b) Cayley computational complexity of computing the interval endpoints,
and (c) Cayley (algebraic) complexity of describing the interval endpoints.
Specifically (i) We give an algorithm to find the interval endpoints of a
Cayley configuration spac. For graphs with low Cayley complexity, we give the
following. (ii) A natural, minimal set of local orientations, whose
specification guarantees Cayley size of 1 and $O(|V|^2)$ Cayley computational
complexity. Specifying fewer local orientations results in a superpolynomial
blow-up of both Cayley size and computational complexity, provided P is
different from NP. (iii) An algorithm--for generic linkages--to find a path of
continuous motion (provided exists) between two given realizations, in time
linear in a natural measure of path length. (iv) A canonical bijective
representation of the Cartesian realization space in minimal ambient dimension,
also for generic linkages."
"We continue to study Cayley configuration spaces of 1-dof linkages in 2D
begun in Part I of this paper, i.e. the set of attainable lengths for a
non-edge. In Part II, we focus on the algebraic complexity of describing
endpoints of the intervals in the set, i.e., the Cayley complexity.
  Specifically, We focus on Cayley configuration spaces of a natural class of
1-dof linkages, called 1-dof tree-decomposable linkages. The underlying graphs
G satisfy the following: for some base non-edge f, G \cup f is
quadratic-radically solvable (QRS), meaning that G \cup f is minimally rigid,
and given lengths \bar{l} of all edges, the corresponding linkage (G \cup f,
\bar{l}) can be simply realized by ruler and compass starting from f. It is
clear that the Cayley complexity only depends on the graph G and possibly the
non-edge f. Here we ask whether the Cayley complexity depends on the choice of
a base non-edge f. We answer this question in the negative, thereby showing
that low Cayley complexity is a property of the graph G (independent of the
non-edge f).
  Then, we give a simple characterization of graphs with low Cayley complexity,
leading to an efficient algorithmic characterization, i.e. an efficient
algorithm for recognizing such graphs.
  Next, we show a surprising result that (graph) planarity is equivalent to low
Cayley complexity for a natural subclass of 1-dof triangle-decomposable graphs.
While this is a finite forbidden minor graph characterization of low Cayley
complexity, we provide counterexamples showing impossibility of such finite
forbidden minor characterizations when the above subclass is enlarged."
"We give exact and approximation algorithms for two-center problems when the
input is a set $\mathcal{D}$ of disks in the plane. We first study the problem
of finding two smallest congruent disks such that each disk in $\mathcal{D}$
intersects one of these two disks. Then we study the problem of covering the
set $\mathcal{D}$ by two smallest congruent disks."
"The problem of searching a polygonal region for an unpredictably moving
intruder by a set of stationary guards, each carrying an orientable laser, is
known as the Searchlight Scheduling Problem. Determining the computational
complexity of deciding if the polygon can be searched by a given set of guards
is a long-standing open problem.
  Here we propose a generalization called the Partial Searchlight Scheduling
Problem, in which only a given subregion of the environment has to be searched,
as opposed to the entire area. We prove that the corresponding decision problem
is strongly PSPACE-complete, both in general and restricted to orthogonal
polygons where the region to be searched is a rectangle.
  Our technique is to reduce from the ""edge-to-edge"" problem for
nondeterministic constraint logic machines, after showing that the
computational power of such machines does not change if we allow ""asynchronous""
edge reversals (as opposed to ""sequential"")."
"The theory of multidimensional persistent homology was initially developed in
the discrete setting, and involved the study of simplicial complexes filtered
through an ordering of the simplices. Later, stability properties of
multidimensional persistence have been proved to hold when topological spaces
are filtered by continuous functions, i.e. for continuous data. This paper aims
to provide a bridge between the continuous setting, where stability properties
hold, and the discrete setting, where actual computations are carried out. More
precisely, a stability preserving method is developed to compare rank
invariants of vector functions obtained from discrete data. These advances
confirm that multidimensional persistent homology is an appropriate tool for
shape comparison in computer vision and computer graphics applications. The
results are supported by numerical tests."
"The problem of mathematical modeling in geography is one of the most
important strategies in order to establish the evolution and the prevision of
geographical phenomena. Models must have a simplified structure, to reflect
essential components and must be selective, structured, and suggestive and
approximate the reality. Models could be static or dynamic, developed in a
theoretical, symbolic, conceptual or mental way, mathematically modeled. The
present paper is focused on the virtual model which uses GeoGebra software,
free and available at www.geogebra.org, in order to establish new methods of
geographical analysis in a dynamic, didactic way."
"It is known that every multigraph with an even number of edges has an even
orientation (i.e., all indegrees are even). We study parity constrained graph
orientations under additional constraints. We consider two types of constraints
for a multigraph G=(V,E): (1) an exact conflict constraint is an edge set C in
E and a vertex v in V such that C should not equal the set of incoming edges at
v; (2) a subset conflict constraint is an edge set C in E and a vertex v in V
such that C should not be a subset of incoming edges at v. We show that it is
NP-complete to decide whether G has an even orientation with exact or subset
conflicts, for all conflict sets of size two or higher. We present efficient
algorithms for computing parity constrained orientations with disjoint exact or
subset conflict pairs."
"In this paper, a robust distributed malicious node detection and precise
localization and tracking method is proposed for Cluster based Mobile Ad hoc
Network (MANET). Certificate Authority (CA) node is selected as the most stable
node among trusted nodes, surrounded by Registration Authority nodes (RAs) in
each cluster to generate the Dynamic Demilitarized Zone (DDMZ) to defend CA
from probable attackers and mitigate the authentication overhead. The RAs also
co-operate with member nodes to detect a target node and determine whether it
is malicious or not, by providing the public key certificate and trust value.
In addition, Internet Protocol (IP) based Triangulation and multi-lateration
method are deployed based on using the average time difference of Time of
Arrival (ToA) and Time of Departure (ToD) of the management packets.
Triangulation uses three reference nodes which are elected within each cluster
based on Best Criterion Function (BCF) to localize each member node inside the
cluster in 2D. Multi-lateration is employed to localize the malicious target
node in 2D using four neighbor nodes. After localization of two consecutive
positions, the target node is continuously localized and tracked by a
particular node using the modified real time Position Localization and Tracking
(PL&T) algorithm by adaptive beam forming and mapping the energy contours of
tracking zone into coverage radii distance. The performance of the proposed
scheme demonstrates the significant accuracy in the detection of malicious
nodes within each cluster."
"To elucidate the structure of assembly configuration spaces, the EASAL
software combines classical concepts, such as stratifications of semialgebraic
sets, with recent algorithms for efficiently realizing geometric constraint
systems, and theoretical advances concerning convex parametrization: in
contrast to folding configuration spaces, most regions of assembly and packing
configurations admit a convex parametrization. This allows for a novel,
efficient and intuitive representation of configuration spaces so that the
corresponding atlas can be efficiently generated and sampled. This paper
describes the approach, theory, structure and algorithms underlying EASAL and
outlines its use for generating atlases of dimeric assemblies of the AAV2 coat
protein, and alpha helix packing in transmembrane proteins."
"Let $P$ be a set of $n$ points in the plane. In this paper we study a new
variant of the circular separability problem in which a point set $P$ is
preprocessed so that one can quickly answer queries of the following form:
Given a geometric object $Q$, report the minimum circle containing $P$ and
exluding $Q$. Our data structure can be constructed in $O(n\log n)$ time using
O(n) space, and can be used to answer the query when $Q$ is either a circle or
a convex $m$-gon in $O(\log n)$ or $O(\log n + \log m)$ time, respectively."
"The implementation of reliable and efficient geometric algorithms is a
challenging task. The reason is the following conflict: On the one hand,
computing with rounded arithmetic may question the reliability of programs
while, on the other hand, computing with exact arithmetic may be too expensive
and hence inefficient. One solution is the implementation of controlled
perturbation algorithms which combine the speed of floating-point arithmetic
with a protection mechanism that guarantees reliability, nonetheless.
  This paper is concerned with the performance analysis of controlled
perturbation algorithms in theory. We answer this question with the
presentation of a general analysis tool box. This tool box is separated into
independent components which are presented individually with their interfaces.
This way, the tool box supports alternative approaches for the derivation of
the most crucial bounds. We present three approaches for this task.
Furthermore, we have thoroughly reworked the concept of controlled perturbation
in order to include rational function based predicates into the theory;
polynomial based predicates are included anyway. Even more we introduce
object-preserving perturbations. Moreover, the tool box is designed such that
it reflects the actual behavior of the controlled perturbation algorithm at
hand without any simplifying assumptions."
"In this paper, an algorithm to compute a certified $G^1$ rational parametric
approximation for algebraic space curves is given by extending the local
generic position method for solving zero dimensional polynomial equation
systems to the case of dimension one. By certified, we mean the approximation
curve and the original curve have the same topology and their Hausdauff
distance is smaller than a given precision. Thus, the method also gives a new
algorithm to compute the topology for space algebraic curves. The main
advantage of the algorithm, inhering from the local generic method, is that
topology computation and approximation for a space curve is directly reduced to
the same tasks for two plane curves. In particular, the error bound of the
approximation space curve is obtained from the error bounds of the
approximation plane curves explicitly. Nontrivial examples are used to show the
effectivity of the method."
"Given $S= \{v_1, \dots, v_n\} \subset \mathbb{R} ^m$ and $p \in \mathbb{R}
^m$, testing if $p \in conv(S)$, the convex hull of $S$, is a fundamental
problem in computational geometry and linear programming. First, we prove a
Euclidean {\it distance duality}, distinct from classical separation theorems
such as Farkas Lemma: $p$ lies in $conv(S)$ if and only if for each $p' \in
conv(S)$ there exists a {\it pivot}, $v_j \in S$ satisfying $d(p',v_j) \geq
d(p,v_j)$. Equivalently, $p \not \in conv(S)$ if and only if there exists a
{\it witness}, $p' \in conv(S)$ whose Voronoi cell relative to $p$ contains
$S$. A witness separates $p$ from $conv(S)$ and approximate $d(p, conv(S))$ to
within a factor of two. Next, we describe the {\it Triangle Algorithm}: given
$\epsilon \in (0,1)$, an {\it iterate}, $p' \in conv(S)$, and $v \in S$, if
$d(p, p') < \epsilon d(p,v)$, it stops. Otherwise, if there exists a pivot
$v_j$, it replace $v$ with $v_j$ and $p'$ with the projection of $p$ onto the
line $p'v_j$. Repeating this process, the algorithm terminates in $O(mn \min
\{\epsilon^{-2}, c^{-1}\ln \epsilon^{-1} \})$ arithmetic operations, where $c$
is the {\it visibility factor}, a constant satisfying $c \geq \epsilon^2$ and
$\sin (\angle pp'v_j) \leq 1/\sqrt{1+c}$, over all iterates $p'$. Additionally,
(i) we prove a {\it strict distance duality} and a related minimax theorem,
resulting in more effective pivots; (ii) describe $O(mn \ln
\epsilon^{-1})$-time algorithms that may compute a witness or a good
approximate solution; (iii) prove {\it generalized distance duality} and
describe a corresponding generalized Triangle Algorithm; (iv) prove a {\it
sensitivity theorem} to analyze the complexity of solving LP feasibility via
the Triangle Algorithm. The Triangle Algorithm is practical and competitive
with the simplex method, sparse greedy approximation and first-order methods."
"Given a simple polygon $P$ consisting of $n$ vertices, we study the problem
of designing space-efficient algorithms for computing (i) the visibility
polygon of a point inside $P$, (ii) the weak visibility polygon of a line
segment inside $P$ and (iii) the minimum link path between a pair of points
inside $P$. For problem (i) two algorithms are proposed. The first one is an
in-place algorithm where the input array may be lost. It uses only O(1) extra
space apart from the input array. The second one assumes that the input is
given in a read-only array, and it needs $O(\sqrt{n})$ extra space. The time
complexity of both the algorithms are O(n). For problem (ii), we have assumed
that the input polygon is given in a read-only array. Our proposed algorithm
runs in $O(n^2)$ time using O(1) extra space. For problem (iii) the time and
space complexities of our proposed algorithm are $O(kn)$ and O(1) respectively;
$k$ is the length (number of links) in a minimum link path between the given
pair of points."
"This paper gives an introduction to the problem of mapping simple polygons
with autonomous agents. We focus on minimalistic agents that move from vertex
to vertex along straight lines inside a polygon, using their sensors to gather
local observations at each vertex. Our attention revolves around the question
whether a given configuration of sensors and movement capabilities of the
agents allows them to capture enough data in order to draw conclusions
regarding the global layout of the polygon. In particular, we study the problem
of reconstructing the visibility graph of a simple polygon by an agent moving
either inside or on the boundary of the polygon. Our aim is to provide insight
about the algorithmic challenges faced by an agent trying to map a polygon. We
present an overview of techniques for solving this problem with agents that are
equipped with simple sensorial capabilities. We illustrate these techniques on
examples with sensors that mea- sure angles between lines of sight or identify
the previous location. We give an overview over related problems in
combinatorial geometry as well as graph exploration."
"We investigate higher-order Voronoi diagrams in the city metric. This metric
is induced by quickest paths in the L1 metric in the presence of an
accelerating transportation network of axis-parallel line segments. For the
structural complexity of kth-order city Voronoi diagrams of n point sites, we
show an upper bound of O(k(n - k) + kc) and a lower bound of {\Omega}(n + kc),
where c is the complexity of the transportation network. This is quite
different from the bound O(k(n - k)) in the Euclidean metric. For the special
case where k = n - 1 the complexity in the Euclidean metric is O(n), while that
in the city metric is {\Theta}(nc).
  Furthermore, we develop an O(k^2(n + c) log n)-time iterative algorithm to
compute the kth-order city Voronoi diagram and an O(nc log^2(n + c) log n)-time
divide-and-conquer algorithm to compute the farthest-site city Voronoi diagram."
"The Fr\'echet distance is a similarity measure between two curves $A$ and
$B$: Informally, it is the minimum length of a leash required to connect a dog,
constrained to be on $A$, and its owner, constrained to be on $B$, as they walk
without backtracking along their respective curves from one endpoint to the
other. The advantage of this measure on other measures such as the Hausdorff
distance is that it takes into account the ordering of the points along the
curves.
  The discrete Fr\'echet distance replaces the dog and its owner by a pair of
frogs that can only reside on $n$ and $m$ specific pebbles on the curves $A$
and $B$, respectively. These frogs hop from a pebble to the next without
backtracking. The discrete Fr\'echet distance can be computed by a rather
straightforward quadratic dynamic programming algorithm. However, despite a
considerable amount of work on this problem and its variations, there is no
subquadratic algorithm known, even for approximation versions of the problem.
  In this paper we present a subquadratic algorithm for computing the discrete
Fr\'echet distance between two sequences of points in the plane, of respective
lengths $m\le n$. The algorithm runs in $O(\dfrac{mn\log\log n}{\log n})$ time
and uses $O(n+m)$ storage. Our approach uses the geometry of the problem in a
subtle way to encode legal positions of the frogs as states of a finite
automata."
"A graph is called (generically) rigid in R^d if, for any choice of
sufficiently generic edge lengths, it can be embedded in R^d in a finite number
of distinct ways, modulo rigid transformations. Here, we deal with the problem
of determining the maximum number of planar Euclidean embeddings of minimally
rigid graphs with 8 vertices, because this is the smallest unknown case in the
plane."
"In this paper, we study a new type of clustering problem, called {\em
Chromatic Clustering}, in high dimensional space. Chromatic clustering seeks to
partition a set of colored points into groups (or clusters) so that no group
contains points with the same color and a certain objective function is
optimized. In this paper, we consider two variants of the problem, chromatic
$k$-means clustering (denoted as $k$-CMeans) and chromatic $k$-medians
clustering (denoted as $k$-CMedians), and investigate their hardness and
approximation solutions. For $k$-CMeans, we show that the additional coloring
constraint destroys several key properties (such as the locality property) used
in existing $k$-means techniques (for ordinary points), and significantly
complicates the problem. There is no FPTAS for the chromatic clustering
problem, even if $k=2$. To overcome the additional difficulty, we develop a
standalone result, called {\em Simplex Lemma}, which enables us to efficiently
approximate the mean point of an unknown point set through a fixed dimensional
simplex. A nice feature of the simplex is its independence with the
dimensionality of the original space, and thus can be used for problems in very
high dimensional space. With the simplex lemma, together with several random
sampling techniques, we show that a $(1+\epsilon)$-approximation of $k$-CMeans
can be achieved in near linear time through a sphere peeling algorithm. For
$k$-CMedians, we show that a similar sphere peeling algorithm exists for
achieving constant approximation solutions."
"Projective clustering is a problem with both theoretical and practical
importance and has received a great deal of attentions in recent years. Given a
set of points $P$ in $\mathbb{R}^{d}$ space, projective clustering is to find a
set $\mathbb{F}$ of $k$ lower dimensional $j$-flats so that the average
distance (or squared distance) from points in $P$ to their closest flats is
minimized. Existing approaches for this problem are mainly based on
adaptive/volume sampling or core-sets techniques which suffer from several
limitations. In this paper, we present the first uniform random sampling based
approach for this challenging problem and achieve linear time solutions for
three cases, general projective clustering, regular projective clustering, and
$L_{\tau}$ sense projective clustering. For the general projective clustering
problem, we show that for any given small numbers $0<\gamma, \epsilon <1$, our
approach first removes $\gamma|P|$ points as outliers and then determines $k$
$j$-flats to cluster the remaining points into $k$ clusters with an objective
value no more than $(1+\epsilon)$ times of the optimal for all points. For
regular projective clustering, we demonstrate that when the input points
satisfy some reasonable assumption on its input, our approach for the general
case can be extended to yield a PTAS for all points. For $L_{\tau}$ sense
projective clustering, we show that our techniques for both the general and
regular cases can be naturally extended to the $L_{\tau}$ sense projective
clustering problem for any $1 \le \tau < \infty$. Our results are based on
several novel techniques, such as slab partition, $\Delta$-rotation, symmetric
sampling, and recursive projection, and can be easily implemented for
applications."
"Given two combinatorial triangulations, how many edge flips are necessary and
sufficient to convert one into the other? This question has occupied
researchers for over 75 years. We provide a comprehensive survey, including
full proofs, of the various attempts to answer it."
"As a pedagogical exercise, we derive the shape of a particularly elegant
pop-up card design, and show that it connects to a classically studied plane
curve that is (among other interpretations) a caustic of a circle."
"We consider guarding classes of simple polygons using mobile guards (polygon
edges and diagonals) under the constraint that no two guards may see each
other. In contrast to most other art gallery problems, existence is the primary
question: does a specific type of polygon admit some guard set? Types include
simple polygons and the subclasses of orthogonal, monotone, and starshaped
polygons. Additionally, guards may either exclude or include the endpoints
(so-called open and closed guards). We provide a nearly complete set of answers
to existence questions of open and closed edge, diagonal, and mobile guards in
simple, orthogonal, monotone, and starshaped polygons, with some surprising
results. For instance, every monotone or starshaped polygon can be guarded
using hidden open mobile (edge or diagonal) guards, but not necessarily with
hidden open edge or hidden open diagonal guards."
"A deflated polygon is a polygon with no visibility crossings. We answer a
question posed by Devadoss et al. (2012) by presenting a polygon that cannot be
deformed via continuous visibility-decreasing motion into a deflated polygon.
We show that the least n for which there exists such an n-gon is seven. In
order to demonstrate non-deflatability, we use a new combinatorial structure
for polygons, the directed dual, which encodes the visibility properties of
deflated polygons. We also show that any two deflated polygons with the same
directed dual can be deformed, one into the other, through a
visibility-preserving deformation."
"We describe an algorithm that takes as input n points in the plane and a
parameter {\epsilon}, and produces as output an embedded planar graph having
the given points as a subset of its vertices in which the graph distances are a
(1 + {\epsilon})-approximation to the geometric distances between the given
points. For point sets in which the Delaunay triangulation has bounded sharpest
angle, our algorithm's output has O(n) vertices, its weight is O(1) times the
minimum spanning tree weight, and the algorithm's running time is bounded by
O(n \sqrt{log log n}). We use this result in a similarly fast deterministic
approximation scheme for the traveling salesperson problem."
"In this work we consider triangulations of point sets in the Euclidean plane,
i.e., maximal straight-line crossing-free graphs on a finite set of points.
Given a triangulation of a point set, an edge flip is the operation of removing
one edge and adding another one, such that the resulting graph is again a
triangulation. Flips are a major way of locally transforming triangular meshes.
We show that, given a point set $S$ in the Euclidean plane and two
triangulations $T_1$ and $T_2$ of $S$, it is an APX-hard problem to minimize
the number of edge flips to transform $T_1$ to $T_2$."
"We study distributions of persistent homology barcodes associated to taking
subsamples of a fixed size from metric measure spaces. We show that such
distributions provide robust invariants of metric measure spaces, and
illustrate their use in hypothesis testing and providing confidence intervals
for topological data analysis."
"In this paper we are proving the following fact. Let P be an arbitrary simple
polygon, and let S be an arbitrary set of 15 points inside P. Then there exists
a subset T of S that is not ""visually discernible"", that is, T is not equal to
the intersection of S with the visibility region vis(v) of any point v in P. In
other words, the VC-dimension d of visibility regions in a simple polygon
cannot exceed 14. Since Valtr proved in 1998 that d \in [6,23] holds, no
progress has been made on this bound. By epsilon-net theorems our reduction
immediately implies a smaller upper bound to the number of guards needed to
cover P."
"We prove that every planar graph with maximum degree three has a planar
drawing in which the edges are drawn as circular arcs that meet at equal angles
around every vertex. Our construction is based on the Koebe-Thurston-Andreev
circle packing theorem, and uses a novel type of Voronoi diagram for circle
packings that is invariant under Moebius transformations, defined using
three-dimensional hyperbolic geometry. We also use circle packing to construct
planar Lombardi drawings of a special class of 4-regular planar graphs, the
medial graphs of polyhedral graphs, and we show that not every 4-regular planar
graph has a Lombardi drawing. We have implemented our algorithm for 3-connected
planar cubic graphs."
"The Frechet distance is a metric to compare two curves, which is based on
monotonous matchings between these curves. We call a matching that results in
the Frechet distance a Frechet matching. There are often many different Frechet
matchings and not all of these capture the similarity between the curves well.
We propose to restrict the set of Frechet matchings to ""natural"" matchings and
to this end introduce locally correct Frechet matchings. We prove that at least
one such matching exists for two polygonal curves and give an O(N^3 log N)
algorithm to compute it, where N is the total number of edges in both curves.
We also present an O(N^2) algorithm to compute a locally correct discrete
Frechet matching."
"Given a set $S$ of points in the plane, a geometric network for $S$ is a
graph $G$ with vertex set $S$ and straight edges. We consider a broadcasting
situation, where one point $r \in S$ is a designated source. Given a dilation
factor $\delta$, we ask for a geometric network $G$ such that for every point
$v \in S$ there is a path from $r$ to $v$ in $G$ of length at most
$\delta|rv|$, and such that the total edge length is minimized. We show that
finding such a network of minimum total edge length is NP-hard, and give an
approximation algorithm."
"The computation of determinants or their signs is the core procedure in many
important geometric algorithms, such as convex hull, volume and point location.
As the dimension of the computation space grows, a higher percentage of the
total computation time is consumed by these computations. In this paper we
study the sequences of determinants that appear in geometric algorithms. The
computation of a single determinant is accelerated by using the information
from the previous computations in that sequence.
  We propose two dynamic determinant algorithms with quadratic arithmetic
complexity when employed in convex hull and volume computations, and with
linear arithmetic complexity when used in point location problems. We implement
the proposed algorithms and perform an extensive experimental analysis. On one
hand, our analysis serves as a performance study of state-of-the-art
determinant algorithms and implementations. On the other hand, we demonstrate
the supremacy of our methods over state-of-the-art implementations of
determinant and geometric algorithms. Our experimental results include a 20 and
78 times speed-up in volume and point location computations in dimension 6 and
11 respectively."
"Given a polygon representing a transportation network together with a point p
in its interior, we aim to extend the network by inserting a line segment,
called a feed-link, which connects p to the boundary of the polygon. Once a
feed link is fixed, the geometric dilation of some point q on the boundary is
the ratio between the length of the shortest path from p to q through the
extended network, and their Euclidean distance. The utility of a feed-link is
inversely proportional to the maximal dilation over all boundary points.
  We give a linear time algorithm for computing the feed-link with the minimum
overall dilation, thus improving upon the previously known algorithm of
complexity that is roughly O(n log n)."
"In mathematics curves are typically defined as the images of continuous real
functions (parametrizations) defined on a closed interval. They can also be
defined as connected one-dimensional compact subsets of points. For simple
curves of finite lengths, parametrizations can be further required to be
injective or even length-normalized. All of these four approaches to curves are
classically equivalent. In this paper we investigate four different versions of
computable curves based on these four approaches. It turns out that they are
all different, and hence, we get four different classes of computable curves.
More interestingly, these four classes are even point-separable in the sense
that the sets of points covered by computable curves of different versions are
also different. However, if we consider only computable curves of computable
lengths, then all four versions of computable curves become equivalent. This
shows that the definition of computable curves is robust, at least for those of
computable lengths. In addition, we show that the class of computable curves of
computable lengths is point-separable from the other four classes of computable
curves."
"We present explicit analytical solution for the problem of minimization of
the function $ F(x,y)= \sum_{j=1}^3 m_j \sqrt{(x-x_j)^2+(y-y_j)^2} $, i.e. we
find the coordinates of stationary point and the corresponding critical value
of $ F(x,y) $ as functions of $ {m_j,x_j,y_j}_{j=1}^3 $. In addition, we also
discuss inverse problem of finding such values of $ m_1,m_2,m_3 $ with the aim
for the corresponding function $ F $ to posses a prescribed position of
stationary point."
"An ensemble of symbolic, numeric and graphic computations developed to
construct the Octonionic and compact G2 structures in Mathematica 8.0.
Cayley-Dickenson Construction symbolically applied from Reals to Octonions.
Baker- Campbell-Hausdorff formula (BCH) in bracket form verified for Octonions.
Algorithms for both exponentiation and logarithm of Octonions developed.
Exclusive validity of vector Product verified for 0, 1, 3 and 7 dimensions.
Symbolic exponential computations carried out for two distinct g2 basis(s) and
arbitrary precision BCH for G2 was coded. Example and counter-example Maximal
Torus for G2 was uncovered. Densely coiled shapes of actions of G2 rendered.
Kolmogorov Complexity for BCH investigated and upper bounds computed:
Complexity of non-commutative non- associative algebraic expression is at most
the Complexity of corresponding commutative associative algebra plus K(BCH)."
"This paper discusses improvements to the numerical robustness of the
algorithm described in Kasper Fauerby's ""Improved Collision Detection and
Response."" The algorithm addresses a common collision detection query: a moving
sphere or ellipsoid vs. a set of motionless triangles. In its current form, the
algorithm allows the sphere to penetrate the triangles. The sphere also
displays ""jittering"" behavior when colliding with certain geometry. Most of
these problems are the product of insufficient attention to numerical
robustness, the focus of this paper. Motivated by the importance of numerical
robustness in collision detection code, this paper addresses these problems in
detail and proposes efficient solutions to them."
"This paper introduces a new way of generalizing Hilbert's two-dimensional
space-filling curve to arbitrary dimensions. The new curves, called harmonious
Hilbert curves, have the unique property that for any d' < d, the d-dimensional
curve is compatible with the d'-dimensional curve with respect to the order in
which the curves visit the points of any d'-dimensional axis-parallel space
that contains the origin. Similar generalizations to arbitrary dimensions are
described for several variants of Peano's curve (the original Peano curve, the
coil curve, the half-coil curve, and the Meurthe curve). The d-dimensional
harmonious Hilbert curves and the Meurthe curves have neutral orientation: as
compared to the curve as a whole, arbitrary pieces of the curve have each of d!
possible rotations with equal probability. Thus one could say these curves are
`statistically invariant' under rotation---unlike the Peano curves, the coil
curves, the half-coil curves, and the familiar generalization of Hilbert curves
by Butz and Moore.
  In addition, prompted by an application in the construction of R-trees, this
paper shows how to construct a 2d-dimensional generalized Hilbert or Peano
curve that traverses the points of a certain d-dimensional diagonally placed
subspace in the order of a given d-dimensional generalized Hilbert or Peano
curve.
  Pseudocode is provided for comparison operators based on the curves presented
in this paper."
"Let P be a polygonal curve in R^d of length n, and S be a point-set of size
k. We consider the problem of finding a polygonal curve Q on S such that all
points in S are visited and the Fr\'echet distance from $P$ is less than a
given epsilon. We show that this problem is NP-complete, regardless of whether
or not points from S are allowed be visited more than once. However, we also
show that if the problem instance satisfies certain restrictions, the problem
is polynomial-time solvable, and we briefly outline an algorithm that computes
Q."
"We prove that every simple polygon contains a degree 3 tree encompassing a
prescribed set of vertices. We give tight bounds on the minimal number of
degree 3 vertices. We apply this result to reprove a result from Bose et al.
that every set of disjoint line segments in the plane admits a binary tree."
"Polygon clipping is a frequent operation in Arbitrary Lagrangian-Eulerian
methods, Computer Graphics, GIS, and CAD. In fact, clipping algorithms are said
to be one of the most important operations in computer graphics. Thus,
efficient and general polygon clipping algorithms are of great importance.
Greiner et al. developed a time efficient algorithm which could clip arbitrary
polygons, including concave and self intersecting polygons. However, the
Greiner-Hormann algorithm does not properly handle degenerate cases, without
the undesirable need for perturbing vertices. We present an extension to the
Greiner-Hormann polygon clipping algorithm which properly deals with degenerate
cases. We combine the method proposed by Kim et al. and the method mentioned by
Liu et al. to remove or properly label degenerate cases. Additionally, the
algorithm presented avoids the need for calculating midpoints, doesn't require
additional entry/exit flags, and avoids changing the vertex data structure used
in the original Greiner-Hormann algorithm, which was required by the extension
presented by Kim et al."
"Given a polygonal curve P, a pointset S, and an \epsilon > 0, we study the
problem of finding a polygonal curve Q whose vertices are from S and has a
Frechet distance less or equal to \epsilon to curve P. In this problem, Q must
visit every point in S and we are allowed to reuse points of pointset in
building Q. First, we show that this problem in NP-Complete. Then, we present a
polynomial time algorithm for a special cases of this problem, when P is a
convex polygon."
"In this paper we study problems of drawing graphs in the plane using edge
length constraints and angle optimization. Specifically we consider the problem
of maximizing the minimum angle, the MMA problem. We solve the MMA problem
using a spring-embedding approach where two forces are applied to the vertices
of the graph: a force optimizing edge lengths and a force optimizing angles. We
solve analytically the problem of computing an optimal displacement of a graph
vertex optimizing the angles between edges incident to it if the degree of the
vertex is at most three. We also apply a numerical approach for computing the
forces applied to vertices of higher degree. We implemented our algorithm in
Java and present drawings of some graphs."
"In this paper, we prove the problem of stabbing a set of disjoint bends by a
convex stabber to be NP-hard. We also consider the optimization version of the
convex stabber problem and prove this problem to be APX-hard for sets of line
segments."
"The rapid development in information technology has immensely contributed to
the use of modern approaches for visualizing volumetric data. Consequently,
medical volume visualization is increasingly attracting attention towards
achieving an effective visualization algorithm for medical diagnosis and
pre-treatment planning. Previously, research has been addressing implementation
of algorithm that can visualize 2-D images into 3-D. Meanwhile, achieving such
a rendering algorithm at an interactive speed and of good robustness to handle
mass data still remains a challenging issue. However, in medical diagnosis,
finding the exact location of brain tumor or diseases is an important step of
surgery / disease management. This paper proposes a GPU-based raycasting
algorithm for accurate allocation and localization of human brain abnormalities
using magnetic resonance (MRI) images of normal and abnormal patients.
  Keywords: Brain tumor, graphic processing units, magnetic resonance imaging,
volume visualization"
"We prove that it is NP-complete to decide whether a given (3-dimensional)
simplicial complex is collapsible. This work extends a result of Malgouyres and
Franc\'{e}s showing that it is NP-complete to decide whether a given simplicial
complex collapses to a 1-complex."
"This paper discusses a distance guarding concept on triangulation graphs,
which can be associated with distance domination and distance vertex cover. We
show how these subjects are interconnected and provide tight bounds for any
n-vertex maximal outerplanar graph: the 2d-guarding number, g_{2d}(n) = n/5;
the 2d-distance domination number, gamma_{2d}(n) = n/5; and the 2d-distance
vertex cover number, beta_{2d}(n) = n/4."
"We study the shortcut Fr\'{e}chet distance, a natural variant of the
Fr\'{e}chet distance, that allows us to take shortcuts from and to any point
along one of the curves. The classic Fr\'echet distance is a bottle-neck
distance measure and hence quite sensitive to outliers. The shortcut
Fr\'{e}chet distance allows us to cut across outliers and hence produces
significantly more meaningful results when dealing with real world data.
Driemel and Har-Peled recently described approximation algorithms for the
restricted case where shortcuts have to start and end at input vertices. We
show that, in the general case, the problem of computing the shortcut
Fr\'{e}chet distance is NP-hard. This is the first hardness result for a
variant of the Fr\'{e}chet distance between two polygonal curves in the plane.
We also present two algorithms for the decision problem: a 3-approximation
algorithm for the general case and an exact algorithm for the vertex-restricted
case. Both algorithms run in O(n^3 log n) time."
"This paper presents a simple kinetic data structure for maintaining all the
nearest neighbors of a set of $n$ moving points in $\mathbb{R}^d$, where the
trajectory of each point is an algebraic function of at most constant degree
$s$. The approach is based on maintaining the edges of the Semi-Yao graph, a
sparse graph whose edge set includes the pairs of nearest neighbors as a
subset.
  Our kinetic data structure (KDS) for maintaining all the nearest neighbors is
deterministic. It processes $O(n^2\beta_{2s+2}^2(n)\log n)$ events with a total
cost of $O(n^2\beta_{2s+2}(n)\log^{d+1} n)$. Here, $\beta_s(n)$ is an extremely
slow-growing function. The best previous KDS for all the nearest neighbors in $
\mathbb{R}^d$ is by Agarwal, Kaplan, and Sharir (TALG 2008). It is a randomized
result. Our structure and analysis are simpler than theirs. Also, we improve
their result by a factor of $\log^d n$ in the number of events and by a $\log
n$ factor in the total cost.
  This paper generalizes and improves the 2013 work of Rahmati, King and
Whitesides (SoCG 2013) on maintaining the Semi-Yao graph in $\mathbb{R}^2$; its
new technique provides the first KDS for the Semi-Yao graph in $\mathbb{R}^d$.
Our KDS is local in the worst case, meaning that only a constant number of
events is associated with any one point at any time.
  For maintaining all the nearest neighbors, neither our KDS nor the KDS by
Agarwal~\etal~is local, and furthermore, each event in our KDS and in their KDS
is handled in polylogarithmic time in an amortized sense.
  Finally, in this paper, we also give a KDS for maintenance of all the
$(1+\epsilon)$-nearest neighbors which is local and each event can be handled
in a polylogarithmic worst-case time."
"In this paper we prove that $Y_5$, the Yao graph with five cones, is a
spanner with stretch factor $\rho = 2+\sqrt{3} \approx 3.74$. Since $Y_5$ is
the only Yao graph whose status of being a spanner or not was open, this
completes the picture of the Yao graphs that are spanners: a Yao graph $Y_k$ is
a spanner if and only if $k \geq 4$.
  We complement the above result with a lower bound of 2.87 on the stretch
factor of $Y_5$. We also show that $YY_5$, the Yao-Yao graph with five cones,
is not a spanner."
"We consider the problem of reporting convex hull points in an orthogonal
range query in two dimensions. Formally, let $P$ be a set of $n$ points in
$\mathbb{R}^{2}$. A point lies on the convex hull of a point set $S$ if it lies
on the boundary of the minimum convex polygon formed by $S$. In this paper, we
are interested in finding the points that lie on the boundary of the convex
hull of the points in $P$ that also fall with in an orthogonal
range$[x_{lt},x_{rt}]\times{}[y_b, y_t]$. We propose a $O(n \log^{2} n) $ space
data structure that can support reporting points on a convex hull inside an
orthogonal range query, in time $O(\log^{3} n + h)$. Here $h$ is the size of
the output. This work improves the result of (Brass et al. 2013) \cite{brass}
that builds a data structure that uses $O(n \log^{2} n)$ space and has a
$O(\log^{5} n + h)$ query time. Additionally, we show that our data structure
can be modified slightly to solve other related problems. For instance, for
counting the number of points on the convex hull in an orthogonal query
rectangle, we propose an $O(n \log^{2}n)$ space data structure that can be
queried upon in $O(\log^{3} n)$ time. We also propose a $O(n \log^{2} n) $
space data structure that can compute the $area$ and $perimeter$ of the convex
hull inside an orthogonal range query in $O(\log^{3} n$) time."
"For a set of points in the plane and a fixed integer $k > 0$, the Yao graph
$Y_k$ partitions the space around each point into $k$ equiangular cones of
angle $\theta=2\pi/k$, and connects each point to a nearest neighbor in each
cone. It is known for all Yao graphs, with the sole exception of $Y_5$, whether
or not they are geometric spanners. In this paper we close this gap by showing
that for odd $k \geq 5$, the spanning ratio of $Y_k$ is at most
$1/(1-2\sin(3\theta/8))$, which gives the first constant upper bound for $Y_5$,
and is an improvement over the previous bound of $1/(1-2\sin(\theta/2))$ for
odd $k \geq 7$. We further reduce the upper bound on the spanning ratio for
$Y_5$ from $10.9$ to $2+\sqrt{3} \approx 3.74$, which falls slightly below the
lower bound of $3.79$ established for the spanning ratio of $\Theta_5$
($\Theta$-graphs differ from Yao graphs only in the way they select the closest
neighbor in each cone). This is the first such separation between a Yao and
$\Theta$-graph with the same number of cones. We also give a lower bound of
$2.87$ on the spanning ratio of $Y_5$. Finally, we revisit the $Y_6$ graph,
which plays a particularly important role as the transition between the graphs
($k > 6$) for which simple inductive proofs are known, and the graphs ($k \le
6$) whose best spanning ratios have been established by complex arguments. Here
we reduce the known spanning ratio of $Y_6$ from $17.6$ to $5.8$, getting
closer to the spanning ratio of 2 established for $\Theta_6$."
"We show that geometric inference of a point cloud can be calculated by
examining its kernel density estimate with a Gaussian kernel. This allows one
to consider kernel density estimates, which are robust to spatial noise,
subsampling, and approximate computation in comparison to raw point sets. This
is achieved by examining the sublevel sets of the kernel distance, which
isomorphically map to superlevel sets of the kernel density estimate. We prove
new properties about the kernel distance, demonstrating stability results and
allowing it to inherit reconstruction results from recent advances in
distance-based topological reconstruction. Moreover, we provide an algorithm to
estimate its topology using weighted Vietoris-Rips complexes."
"Helly's theorem is a fundamental result in discrete geometry, describing the
ways in which convex sets intersect with each other. If $S$ is a set of $n$
points in $R^d$, we say that $S$ is $(k,G)$-clusterable if it can be
partitioned into $k$ clusters (subsets) such that each cluster can be contained
in a translated copy of a geometric object $G$. In this paper, as an
application of Helly's theorem, by taking a constant size sample from $S$, we
present a testing algorithm for $(k,G)$-clustering, i.e., to distinguish
between two cases: when $S$ is $(k,G)$-clusterable, and when it is
$\epsilon$-far from being $(k,G)$-clusterable. A set $S$ is $\epsilon$-far
$(0<\epsilon\leq1)$ from being $(k,G)$-clusterable if at least $\epsilon n$
points need to be removed from $S$ to make it $(k,G)$-clusterable. We solve
this problem for $k=1$ and when $G$ is a symmetric convex object. For $k>1$, we
solve a weaker version of this problem. Finally, as an application of our
testing result, in clustering with outliers, we show that one can find the
approximate clusters by querying a constant size sample, with high probability."
"Let $S$ be a point set in the plane such that each of its elements is colored
either red or blue. A matching of $S$ with rectangles is any set of
pairwise-disjoint axis-aligned rectangles such that each rectangle contains
exactly two points of $S$. Such a matching is monochromatic if every rectangle
contains points of the same color, and is bichromatic if every rectangle
contains points of different colors. In this paper we study the following two
problems:
  1. Find a maximum monochromatic matching of $S$ with rectangles.
  2. Find a maximum bichromatic matching of $S$ with rectangles.
  For each problem we provide a polynomial-time approximation algorithm that
constructs a matching with at least $1/4$ of the number of rectangles of an
optimal matching. We show that the first problem is $\mathsf{NP}$-hard even if
either the matching rectangles are restricted to axis-aligned segments or $S$
is in general position, that is, no two points of $S$ share the same $x$ or $y$
coordinate. We further show that the second problem is also $\mathsf{NP}$-hard,
even if $S$ is in general position. These $\mathsf{NP}$-hardness results follow
by showing that deciding the existence of a perfect matching is
$\mathsf{NP}$-complete in each case. The approximation results are based on a
relation of our problem with the problem of finding a maximum independent set
in a family of axis-aligned rectangles. With this paper we extend previous ones
on matching one-colored points with rectangles and squares, and matching
two-colored points with segments. Furthermore, using our techniques, we prove
that it is $\mathsf{NP}$-complete to decide a perfect matching with rectangles
in the case where all points have the same color, solving an open problem of
Bereg, Mutsanas, and Wolff [CGTA (2009)]."
"Polygons are a paramount data structure in computational geometry. While the
complexity of many algorithms on simple polygons or polygons with holes depends
on the size of the input polygon, the intrinsic complexity of the problems
these algorithms solve is often related to the reflex vertices of the polygon.
In this paper, we give an easy-to-describe linear-time method to replace an
input polygon $\mathcal{P}$ by a polygon $\mathcal{P}'$ such that (1)
$\mathcal{P}'$ contains $\mathcal{P}$, (2) $\mathcal{P}'$ has its reflex
vertices at the same positions as $\mathcal{P}$, and (3) the number of vertices
of $\mathcal{P}'$ is linear in the number of reflex vertices. Since the
solutions of numerous problems on polygons (including shortest paths, geodesic
hulls, separating point sets, and Voronoi diagrams) are equivalent for both
$\mathcal{P}$ and $\mathcal{P}'$, our algorithm can be used as a preprocessing
step for several algorithms and makes their running time dependent on the
number of reflex vertices rather than on the size of $\mathcal{P}$."
"In this paper we introduce trajectory-based labeling, a new variant of
dynamic map labeling, where a movement trajectory for the map viewport is
given. We define a general labeling model and study the active range
maximization problem in this model. The problem is NP-complete and W[1]-hard.
In the restricted, yet practically relevant case that no more than k labels can
be active at any time, we give polynomial-time algorithms. For the general case
we present a practical ILP formulation with an experimental evaluation as well
as approximation algorithms."
"We study the problem of visibility in polyhedral terrains in the presence of
multiple viewpoints. We consider a triangulated terrain with $m>1$ viewpoints
(or guards) located on the terrain surface. A point on the terrain is
considered \emph{visible} if it has an unobstructed line of sight to at least
one viewpoint. We study several natural and fundamental visibility structures:
(1) the visibility map, which is a partition of the terrain into visible and
invisible regions; (2) the \emph{colored} visibility map, which is a partition
of the terrain into regions whose points have exactly the same visible
viewpoints; and (3) the Voronoi visibility map, which is a partition of the
terrain into regions whose points have the same closest visible viewpoint. We
study the complexity of each structure for both 1.5D and 2.5D terrains, and
provide efficient algorithms to construct them. Our algorithm for the
visibility map in 2.5D terrains improves on the only existing algorithm in this
setting. To the best of our knowledge, the other structures have not been
studied before."
"Comparing two geometric graphs embedded in space is important in the field of
transportation network analysis. Given street maps of the same city collected
from different sources, researchers often need to know how and where they
differ. However, the majority of current graph comparison algorithms are based
on structural properties of graphs, such as their degree distribution or their
local connectivity properties, and do not consider their spatial embedding.
This ignores a key property of road networks since similarity of travel over
two road networks is intimately tied to the specific spatial embedding.
Likewise, many current street map comparison algorithms focus on the spatial
embeddings only and do not take structural properties into account, which makes
these algorithms insensitive to local connectivity properties and shortest path
similarities. We propose a new path-based distance measure to compare two
planar geometric graphs embedded in the plane. Our distance measure takes
structural as well as spatial properties into account by imposing a distance
measure between two road networks based on the Hausdorff distance between the
two sets of travel paths they represent. We show that this distance can be
approximated in polynomial time and that it preserves structural and spatial
properties of the graphs."
"This paper lays the foundations of an approach to applying Gromov's ideas on
quantitative topology to topological data analysis. We introduce the
""contiguity complex"", a simplicial complex of maps between simplicial complexes
defined in terms of the combinatorial notion of contiguity. We generalize the
Simplicial Approximation Theorem to show that the contiguity complex
approximates the homotopy type of the mapping space as we subdivide the domain.
We describe algorithms for approximating the rate of growth of the components
of the contiguity complex under subdivision of the domain; this procedure
allows us to computationally distinguish spaces with isomorphic homology but
different homotopy types."
"Finding a maximum independent set (MIS) of a given fam- ily of axis-parallel
rectangles is a basic problem in computational geom- etry and combinatorics.
This problem has attracted significant atten- tion since the sixties, when
Wegner conjectured that the corresponding duality gap, i.e., the maximum
possible ratio between the maximum independent set and the minimum hitting set
(MHS), is bounded by a universal constant. An interesting special case, that
may prove use- ful to tackling the general problem, is the
diagonal-intersecting case, in which the given family of rectangles is
intersected by a diagonal. Indeed, Chepoi and Felsner recently gave a factor 6
approximation algorithm for MHS in this setting, and showed that the duality
gap is between 3/2 and 6. In this paper we improve upon these results. First we
show that MIS in diagonal-intersecting families is NP-complete, providing one
smallest subclass for which MIS is provably hard. Then, we derive an
$O(n^2)$-time algorithm for the maximum weight independent set when, in
addition the rectangles intersect below the diagonal. This improves and extends
a classic result of Lubiw, and amounts to obtain a 2-approximation algo- rithm
for the maximum weight independent set of rectangles intersecting a diagonal.
Finally, we prove that for diagonal-intersecting families the duality gap is
between 2 and 4. The upper bound, which implies an approximation algorithm of
the same factor, follows from a simple com- binatorial argument, while the
lower bound represents the best known lower bound on the duality gap, even in
the general case."
"This paper considers the problem of computing the weak visibility polygon
(WVP) of any query line segment pq (or WVP(pq)) inside a given simple polygon
P. We present an algorithm that preprocesses P and creates a data structure
from which WVP(pq) is efficiently reported in an output sensitive manner.
  Our algorithm needs O(n^2 log n) time and O(n^2) space in the preprocessing
phase to report WVP(pq) of any query line segment pq in time O(log^2 n +
|WVP(pq)|). We improve the preprocessing time and space of current results for
this problem at the expense of more query time."
"Bounding hull, such as convex hull, concave hull, alpha shapes etc. has vast
applications in different areas especially in computational geometry. Alpha
shape and concave hull are generalizations of convex hull. Unlike the convex
hull, they construct non-convex enclosure on a set of points. In this paper, we
introduce another generalization of convex hull, named alpha-concave hull, and
compare this concept with convex hull and alpha shape. We show that the
alpha-concave hull is also a generalization of an NP-complete problem named
min-area TSP. We prove that computing the alpha-concave hull is NP-hard on a
set of points."
"In this paper, we consider the following geometric puzzle whose origin was
traced to Allan Freedman \cite{croft91,tutte69} in the 1960s by Dumitrescu and
T{\'o}th \cite{adriancasaba2011}. The puzzle has been popularized of late by
Peter Winkler \cite{Winkler2007}. Let $P_{n}$ be a set of $n$ points, including
the origin, in the unit square $U = [0,1]^2$. The problem is to construct $n$
axis-parallel and mutually disjoint rectangles inside $U$ such that the
bottom-left corner of each rectangle coincides with a point in $P_{n}$ and the
total area covered by the rectangles is maximized. We would term the above
rectangles as \emph{anchored rectangles}. The longstanding conjecture has been
that at least half of $U$ can be covered when anchored rectangles are properly
placed. Dumitrescu and T{\'o}th \cite{Dumitrescu2012} have shown a construction
method that can cover at least $0.09121$, i.e., roughly $9\%$ of the area."
"The volume is an important attribute of a convex body. In general, it is
quite difficult to calculate the exact volume. But in many cases, it suffices
to have an approximate value. Volume estimation methods for convex bodies have
been extensively studied in theory, however, there is still a lack of practical
implementations of such methods. In this paper, we present an efficient method
which is based on the Multiphase Monte-Carlo algorithm to estimate volumes of
convex polytopes. It uses the coordinate directions hit-and-run method, and
employs a technique of reutilizing sample points. The experiments show that our
method can efficiently handle instances with dozens of dimensions with high
accuracy."
"Given a set $\mathsf{P}$ of $n$ points in $\mathbb{R}^d$, we show how to
insert a set $\mathsf{X}$ of $O( n^{1-1/d} )$ additional points, such that
$\mathsf{P}$ can be broken into two sets $\mathsf{P}_1$ and $\mathsf{P}_2$, of
roughly equal size, such that in the Voronoi diagram $\mathcal{V}( \mathsf{P}
\cup \mathsf{X} )$, the cells of $\mathsf{P}_1$ do not touch the cells of
$\mathsf{P}_2$; that is, $\mathsf{X}$ separates $\mathsf{P}_1$ from
$\mathsf{P}_2$ in the Voronoi diagram.
  Given such a partition $(\mathsf{P}_1,\mathsf{P}_2)$ of $\mathsf{P}$, we
present approximation algorithms to compute the minimum size separator
realizing this partition.
  Finally, we present a simple local search algorithm that is a PTAS for
geometric hitting set of fat objects (which can also be used to approximate the
optimal Voronoi partition)."
"Selection lemmas are classical results in discrete geometry that have been
well studied and have applications in many geometric problems like weak epsilon
nets and slimming Delaunay triangulations. Selection lemma type results
typically show that there exists a point that is contained in many objects that
are induced (spanned) by an underlying point set.
  In the first selection lemma, we consider the set of all the objects induced
(spanned) by a point set $P$. This question has been widely explored for
simplices in $\mathbb{R}^d$, with tight bounds in $\mathbb{R}^2$. In our paper,
we prove first selection lemma for other classes of geometric objects. We also
consider the strong variant of this problem where we add the constraint that
the piercing point comes from $P$. We prove an exact result on the strong and
the weak variant of the first selection lemma for axis-parallel rectangles,
special subclasses of axis-parallel rectangles like quadrants and slabs, disks
(for centrally symmetric point sets). We also show non-trivial bounds on the
first selection lemma for axis-parallel boxes and hyperspheres in
$\mathbb{R}^d$.
  In the second selection lemma, we consider an arbitrary $m$ sized subset of
the set of all objects induced by $P$. We study this problem for axis-parallel
rectangles and show that there exists an point in the plane that is contained
in $\frac{m^3}{24n^4}$ rectangles. This is an improvement over the previous
bound by Smorodinsky and Sharir when $m$ is almost quadratic."
"The classical sphere packing problem asks for the best (infinite) arrangement
of non-overlapping unit balls which cover as much space as possible. We define
a generalized version of the problem, where we allow each ball a limited amount
of overlap with other balls. We study two natural choices of overlap measures
and obtain the optimal lattice packings in a parameterized family of lattices
which contains the FCC, BCC, and integer lattice."
"Given a set of n disjoint balls b1, . . ., bn in IRd, we provide a data
structure, of near linear size, that can answer (1 \pm \epsilon)-approximate
kth-nearest neighbor queries in O(log n + 1/\epsilon^d) time, where k and
\epsilon are provided at query time. If k and \epsilon are provided in advance,
we provide a data structure to answer such queries, that requires (roughly)
O(n/k) space; that is, the data structure has sublinear space requirement if k
is sufficiently large."
"In this paper, we provide an $O(n \mathrm{polylog} n)$ bound on the expected
complexity of the randomly weighted Voronoi diagram of a set of $n$ sites in
the plane, where the sites can be either points, interior-disjoint convex sets,
or other more general objects. Here the randomness is on the weight of the
sites, not their location. This compares favorably with the worst case
complexity of these diagrams, which is quadratic. As a consequence we get an
alternative proof to that of Agarwal etal [AHKS13] of the near linear
complexity of the union of randomly expanded disjoint segments or convex sets
(with an improved bound on the latter). The technique we develop is elegant and
should be applicable to other problems."
"We present tight upper and lower bounds on the spanning ratio of a large
family of constrained $\theta$-graphs. We show that constrained $\theta$-graphs
with $4k + 2$ ($k \geq 1$ and integer) cones have a tight spanning ratio of $1
+ 2 \sin(\theta/2)$, where $\theta$ is $2 \pi / (4k + 2)$. We also present
improved upper bounds on the spanning ratio of the other families of
constrained $\theta$-graphs."
"We study pursuit-evasion in a polygonal environment with polygonal obstacles.
In this turn based game, an evader $e$ is chased by pursuers $p_1, p_2, ...,
p_{\ell}$. The players have full information about the environment and the
location of the other players. The pursuers are allowed to coordinate their
actions. On the pursuer turn, each $p_i$ can move to any point at distance at
most 1 from his current location. On the evader turn, he moves similarly. The
pursuers win if some pursuer becomes co-located with the evader in finite time.
The evader wins if he can evade capture forever.
  It is known that one pursuer can capture the evader in any simply-connected
polygonal environment, and that three pursuers are always sufficient in any
polygonal environment (possibly with polygonal obstacles). We contribute two
new results to this field. First, we fully characterize when an environment
with a single obstacles is one-pursuer-win or two-pursuer-win. Second, we give
sufficient (but not necessary) conditions for an environment to have a winning
strategy for two pursuers. Such environments can be swept by a \emph{leapfrog
strategy} in which the two cops alternately guard/increase the currently
controlled area. The running time of this algorithm is $O(n \cdot h \cdot
{diam}(P))$ where $n$ is the number of vertices, $h$ is the number of obstacles
and ${diam}(P)$ is the diameter of $P$.
  More concretely, for an environment with $n$ vertices, we describe an
$O(n^2)$ algorithm that (1) determines whether the obstacles are
well-separated, and if so, (2) constructs the required partition for a leapfrog
strategy."
"In this paper, we show an improved bound and new algorithm for the online
square-into-square packing problem. This two-dimensional packing problem
involves packing an online sequence of squares into a unit square container
without any two squares overlapping. The goal is to find the largest area
$\alpha$ such that any set of squares with total area $\alpha$ can be packed.
We show an algorithm that can pack any set of squares with total area $\alpha
\leq 3/8$ into a unit square in an online setting, improving the previous bound
of $11/32$."
"We describe a $O(\log n )$-approximation algorithm for computing the
homotopic \Frechet distance between two polygonal curves that lie on the
boundary of a triangulated topological disk. Prior to this work, algorithms
were known only for curves on the Euclidean plane with polygonal obstacles.
  A key technical ingredient in our analysis is a $O(\log n)$-approximation
algorithm for computing the minimum height of a homotopy between two curves. No
algorithms were previously known for approximating this parameter.
Surprisingly, it is not even known if computing either the homotopic \Frechet
distance, or the minimum height of a homotopy, is in NP."
"This paper describes how Large Deformation Diffeomorphic Metric Mapping
(LDDMM) can be coupled with a Fast Multipole (FM) Boundary Element Method (BEM)
to investigate the relationship between morphological changes in the head,
torso, and outer ears and their acoustic filtering (described by Head Related
Transfer Functions, HRTFs). The LDDMM technique provides the ability to study
and implement morphological changes in ear, head and torso shapes. The FM-BEM
technique provides numerical simulations of the acoustic properties of an
individual's head, torso, and outer ears. This paper describes the first
application of LDDMM to the study of the relationship between a listener's
morphology and a listener's HRTFs. To demonstrate some of the new capabilities
provided by the coupling of these powerful tools, we examine the classical
question of what it means to ``listen through another individual's outer
ears.'' This work utilizes the data provided by the Sydney York Morphological
and Acoustic Recordings of Ears (SYMARE) database."
"This paper details an algorithm for unfolding a class of convex polyhedra,
where each polyhedron in the class consists of a convex cap over a rectangular
base, with several restrictions: the cap's faces are quadrilaterals, with
vertices over an underlying integer lattice, and such that the cap convexity is
``radially monotone,'' a type of smoothness constraint. Extensions of Cauchy's
arm lemma are used in the proof of non-overlap."
"We develop a multiresolution approach to the problem of polygonal curve
approximation. We show theoretically and experimentally that, if the
simplification algorithm A used between any two successive levels of resolution
satisfies some conditions, the multiresolution algorithm MR will have a
complexity lower than the complexity of A. In particular, we show that if A has
a O(N2/K) complexity (the complexity of a reduced search dynamic solution
approach), where N and K are respectively the initial and the final number of
segments, the complexity of MR is in O(N).We experimentally compare the
outcomes of MR with those of the optimal ""full search"" dynamic programming
solution and of classical merge and split approaches. The experimental
evaluations confirm the theoretical derivations and show that the proposed
approach evaluated on 2D coastal maps either shows a lower complexity or
provides polygonal approximations closer to the initial curves."
"The Voronoi diagram of a finite set of objects is a fundamental geometric
structure that subdivides the embedding space into regions, each region
consisting of the points that are closer to a given object than to the others.
We may define many variants of Voronoi diagrams depending on the class of
objects, the distance functions and the embedding space. In this paper, we
investigate a framework for defining and building Voronoi diagrams for a broad
class of distance functions called Bregman divergences. Bregman divergences
include not only the traditional (squared) Euclidean distance but also various
divergence measures based on entropic functions. Accordingly, Bregman Voronoi
diagrams allow to define information-theoretic Voronoi diagrams in statistical
parametric spaces based on the relative entropy of distributions. We define
several types of Bregman diagrams, establish correspondences between those
diagrams (using the Legendre transformation), and show how to compute them
efficiently. We also introduce extensions of these diagrams, e.g. k-order and
k-bag Bregman Voronoi diagrams, and introduce Bregman triangulations of a set
of points and their connexion with Bregman Voronoi diagrams. We show that these
triangulations capture many of the properties of the celebrated Delaunay
triangulation. Finally, we give some applications of Bregman Voronoi diagrams
which are of interest in the context of computational geometry and machine
learning."
"We consider the problem of computing a triangulation of the real projective
plane P2, given a finite point set S={p1, p2,..., pn} as input. We prove that a
triangulation of P2 always exists if at least six points in S are in general
position, i.e., no three of them are collinear. We also design an algorithm for
triangulating P2 if this necessary condition holds. As far as we know, this is
the first computational result on the real projective plane."
"Spacetime discontinuous Galerkin (SDG) finite element methods are used to
solve such PDEs involving space and time variables arising from wave
propagation phenomena in important applications in science and engineering.
  To support an accurate and efficient solution procedure using SDG methods and
to exploit the flexibility of these methods, we give a meshing algorithm to
construct an unstructured simplicial spacetime mesh over an arbitrary
simplicial space domain. Our algorithm is the first spacetime meshing algorithm
suitable for efficient solution of nonlinear phenomena in anisotropic media
using novel discontinuous Galerkin finite element methods for implicit
solutions directly in spacetime. Given a triangulated d-dimensional Euclidean
space domain M (a simplicial complex) and initial conditions of the underlying
hyperbolic spacetime PDE, we construct an unstructured simplicial mesh of the
(d+1)-dimensional spacetime domain M x [0,infinity). Our algorithm uses a
near-optimal number of spacetime elements, each with bounded temporal aspect
ratio for any finite prefix M x [0,T] of spacetime. Our algorithm is an
advancing front procedure that constructs the spacetime mesh incrementally, an
extension of the Tent Pitcher algorithm of Ungor and Sheffer (2000).
  In 2DxTime, our algorithm simultaneously adapts the size and shape of
spacetime tetrahedra to a spacetime error indicator. We are able to incorporate
more general front modification operations, such as edge flips and limited mesh
smoothing. Our algorithm represents recent progress towards a meshing algorithm
in 2DxTime to track moving domain boundaries and other singular surfaces such
as shock fronts."
"Spacetime Discontinuous Galerkin (DG) methods are used to solve hyperbolic
PDEs describing wavelike physical phenomena. When the PDEs are nonlinear, the
speed of propagation of the phenomena, called the wavespeed, at any point in
the spacetime domain is computed as part of the solution. We give an advancing
front algorithm to construct a simplicial mesh of the spacetime domain suitable
for DG solutions. Given a simplicial mesh of a bounded linear or planar space
domain M, we incrementally construct a mesh of the spacetime domain M x
[0,infinity) such that the solution can be computed in constant time per
element. We add a patch of spacetime elements to the mesh at every step. The
boundary of every patch is causal which means that the elements in the patch
can be solved immediately and that the patches in the mesh are partially
ordered by dependence. The elements in a single patch are coupled because they
share implicit faces; however, the number of elements in each patch is bounded.
The main contribution of this paper is sufficient constraints on the progress
in time made by the algorithm at each step which guarantee that a new patch
with causal boundary can be added to the mesh at every step even when the
wavespeed is increasing discontinuously. Our algorithm adapts to the local
gradation of the space mesh as well as the wavespeed that most constrains
progress at each step. Previous algorithms have been restricted at each step by
the maximum wavespeed throughout the entire spacetime domain."
"We propose a variant of Cauchy's Lemma, proving that when a convex chain on
one sphere is redrawn (with the same lengths and angles) on a larger sphere,
the distance between its endpoints increases. The main focus of this work is a
comparison of three alternate proofs, to show the links between Toponogov's
Comparison Theorem, Legendre's Theorem and Cauchy's Arm Lemma."
"Let C be a compact and convex set in the plane that contains the origin in
its interior, and let S be a finite set of points in the plane. The Delaunay
graph DG_C(S) of S is defined to be the dual of the Voronoi diagram of S with
respect to the convex distance function defined by C. We prove that DG_C(S) is
a t-spanner for S, for some constant t that depends only on the shape of the
set C. Thus, for any two points p and q in S, the graph DG_C(S) contains a path
between p and q whose Euclidean length is at most t times the Euclidean
distance between p and q."
"Let S be a set of n points in the plane, and let T be a set of m triangles
with vertices in S. Then there exists a point in the plane contained in
Omega(m^3 / (n^6 log^2 n)) triangles of T. Eppstein (1993) gave a proof of this
claim, but there is a problem with his proof. Here we provide a correct proof
by slightly modifying Eppstein's argument."
"This paper studies the straight skeleton of polyhedra in three dimensions. We
first address voxel-based polyhedra (polycubes), formed as the union of a
collection of cubical (axis-aligned) voxels. We analyze the ways in which the
skeleton may intersect each voxel of the polyhedron, and show that the skeleton
may be constructed by a simple voxel-sweeping algorithm taking constant time
per voxel. In addition, we describe a more complex algorithm for straight
skeletons of voxel-based polyhedra, which takes time proportional to the area
of the surfaces of the straight skeleton rather than the volume of the
polyhedron. We also consider more general polyhedra with axis-parallel edges
and faces, and show that any n-vertex polyhedron of this type has a straight
skeleton with O(n^2) features. We provide algorithms for constructing the
straight skeleton, with running time O(min(n^2 log n, k log^{O(1)} n)) where k
is the output complexity. Next, we discuss the straight skeleton of a general
nonconvex polyhedron. We show that it has an ambiguity issue, and suggest a
consistent method to resolve it. We prove that the straight skeleton of a
general polyhedron has a superquadratic complexity in the worst case. Finally,
we report on an implementation of a simple algorithm for the general case."
"We describe a new approach to fit the polyhedron describing a 3D building
model to the point cloud of a Digital Elevation Model (DEM). We introduce a new
kinetic framework that hides to its user the combinatorial complexity of
determining or maintaining the polyhedron topology, allowing the design of a
simple variational optimization. This new kinetic framework allows the
manipulation of a bounded polyhedron with simple faces by specifying the target
plane equations of each of its faces. It proceeds by evolving continuously from
the polyhedron defined by its initial topology and its initial plane equations
to a polyhedron that is as topologically close as possible to the initial
polyhedron but with the new plane equations. This kinetic framework handles
internally the necessary topological changes that may be required to keep the
faces simple and the polyhedron bounded. For each intermediate configurations
where the polyhedron looses the simplicity of its faces or its boundedness, the
simplest topological modification that is able to reestablish the simplicity
and the boundedness is performed."
"We describe an efficient method for drawing any n-vertex simple graph G in
the hyperbolic plane. Our algorithm produces greedy drawings, which support
greedy geometric routing, so that a message M between any pair of vertices may
be routed geometrically, simply by having each vertex that receives M pass it
along to any neighbor that is closer in the hyperbolic metric to the message's
eventual destination. More importantly, for networking applications, our
algorithm produces succinct drawings, in that each of the vertex positions in
one of our embeddings can be represented using O(log n) bits and the
calculation of which neighbor to send a message to may be performed efficiently
using these representations. These properties are useful, for example, for
routing in sensor networks, where storage and bandwidth are limited."
"A highway H is a line in the plane on which one can travel at a greater speed
than in the remaining plane. One can choose to enter and exit H at any point.
The highway time distance between a pair of points is the minimum time required
to move from one point to the other, with optional use of H.
  The highway hull HH(S,H) of a point set S is the minimal set containing S as
well as the shortest paths between all pairs of points in HH(S,H), using the
highway time distance.
  We provide a Theta(n log n) worst-case time algorithm to find the highway
hull under the L_1 metric, as well as an O(n log^2 n) time algorithm for the
L_2 metric which improves the best known result of O(n^2).
  We also define and construct the useful region of the plane: the region that
a highway must intersect in order that the shortest path between at least one
pair of points uses the highway."
"A surface embedded in space, in such a way that each point has a neighborhood
within which the surface is a terrain, projects to an immersed surface in the
plane, the boundary of which is a self-intersecting curve. Under what
circumstances can we reverse these mappings algorithmically? Shor and van Wyk
considered one such problem, determining whether a curve is the boundary of an
immersed disk; they showed that the self-overlapping curves defined in this way
can be recognized in polynomial time. We show that several related problems are
more difficult: it is NP-complete to determine whether an immersed disk is the
projection of a surface embedded in space, or whether a curve is the boundary
of an immersed surface in the plane that is not constrained to be a disk.
However, when a casing is supplied with a self-intersecting curve, describing
which component of the curve lies above and which below at each crossing, we
may determine in time linear in the number of crossings whether the cased curve
forms the projected boundary of a surface in space. As a related result, we
show that an immersed surface with a single boundary curve that crosses itself
n times has at most 2^{n/2} combinatorially distinct spatial embeddings, and we
discuss the existence of fixed-parameter tractable algorithms for related
problems."
"We are given a finite set of n points (guards) G in the plane R^2 and an
angle 0 < theta < 2 pi. A theta-cone is a cone with apex angle theta. We call a
theta-cone empty (with respect to G) if it does not contain any point of G. A
point p in R^2 is called theta-guarded if every theta-cone with its apex
located at p is non-empty. Furthermore, the set of all theta-guarded points is
called the theta-guarded region, or the theta-region for short.
  We present several results on this topic. The main contribution of our work
is to describe the theta-region with O(n/theta) circular arcs, and we give an
algorithm to compute it. We prove a tight O(n) worst-case bound on the
complexity of the theta-region for theta >= pi/2. In case theta is bounded from
below by a positive constant, we prove an almost linear bound O(n^(1+epsilon))
for any epsilon > 0 on the complexity. Moreover, we show that there is a
sequence of inputs such that the asymptotic bound on the complexity of their
theta-region is Omega(n^2). In addition we point out gaps in the proofs of a
recent publication that claims an O(n) bound on the complexity for any constant
angle theta."
"There exists a surface of a convex polyhedron P and a partition L of P into
geodesic convex polygons such that there are no connected ""edge"" unfoldings of
P without self-intersections (whose spanning tree is a subset of the edge
skeleton of L)."
"Let $\mathcal{P}$ be an $\mathcal{H}$-polytope in $\mathbb{R}^d$ with vertex
set $V$. The vertex centroid is defined as the average of the vertices in $V$.
We prove that computing the vertex centroid of an $\mathcal{H}$-polytope is
#P-hard. Moreover, we show that even just checking whether the vertex centroid
lies in a given halfspace is already #P-hard for $\mathcal{H}$-polytopes. We
also consider the problem of approximating the vertex centroid by finding a
point within an $\epsilon$ distance from it and prove this problem to be
#P-easy by showing that given an oracle for counting the number of vertices of
an $\mathcal{H}$-polytope, one can approximate the vertex centroid in
polynomial time. We also show that any algorithm approximating the vertex
centroid to \emph{any} ``sufficiently'' non-trivial (for example constant)
distance, can be used to construct a fully polynomial approximation scheme for
approximating the centroid and also an output-sensitive polynomial algorithm
for the Vertex Enumeration problem. Finally, we show that for unbounded
polyhedra the vertex centroid can not be approximated to a distance of
$d^{{1/2}-\delta}$ for any fixed constant $\delta>0$."
"For a set P of n points in R^2, the Euclidean 2-center problem computes a
pair of congruent disks of the minimal radius that cover P. We extend this to
the (2,k)-center problem where we compute the minimal radius pair of congruent
disks to cover n-k points of P. We present a randomized algorithm with O(n k^7
log^3 n) expected running time for the (2,k)-center problem. We also study the
(p,k)-center problem in R}^2 under the \ell_\infty-metric. We give solutions
for p=4 in O(k^{O(1)} n log n) time and for p=5 in O(k^{O(1)} n log^5 n) time."
"A {\em Steiner star} for a set $P$ of $n$ points in $\RR^d$ connects an
arbitrary center point to all points of $P$, while a {\em star} connects a
point $p\in P$ to the remaining $n-1$ points of $P$. All connections are
realized by straight line segments. Fekete and Meijer showed that the minimum
star is at most $\sqrt{2}$ times longer than the minimum Steiner star for any
finite point configuration in $\RR^d$. The maximum ratio between them, over all
finite point configurations in $\RR^d$, is called the {\em star Steiner ratio}
in $\RR^d$. It is conjectured that this ratio is $4/\pi = 1.2732...$ in the
plane and $4/3=1.3333...$ in three dimensions. Here we give upper bounds of
1.3631 in the plane, and 1.3833 in 3-space, thereby substantially improving
recent upper bounds of 1.3999, and $\sqrt{2}-10^{-4}$, respectively. Our
results also imply improved bounds on the maximum ratios between the minimum
star and the maximum matching in two and three dimensions."
"Deciding whether the union of two convex polyhedra is itself a convex
polyhedron is a basic problem in polyhedral computations; having important
applications in the field of constrained control and in the synthesis,
analysis, verification and optimization of hardware and software systems. In
such application fields though, general convex polyhedra are just one among
many, so-called, numerical abstractions, which range from restricted families
of (not necessarily closed) convex polyhedra to non-convex geometrical objects.
We thus tackle the problem from an abstract point of view: for a wide range of
numerical abstractions that can be modeled as bounded join-semilattices --that
is, partial orders where any finite set of elements has a least upper bound--,
we show necessary and sufficient conditions for the equivalence between the
lattice-theoretic join and the set-theoretic union. For the case of closed
convex polyhedra --which, as far as we know, is the only one already studied in
the literature-- we improve upon the state-of-the-art by providing a new
algorithm with a better worst-case complexity. The results and algorithms
presented for the other numerical abstractions are new to this paper. All the
algorithms have been implemented, experimentally validated, and made available
in the Parma Polyhedra Library."
"Given a planar point set and an integer $k$, we wish to color the points with
$k$ colors so that any axis-aligned strip containing enough points contains all
colors. The goal is to bound the necessary size of such a strip, as a function
of $k$. We show that if the strip size is at least $2k{-}1$, such a coloring
can always be found. We prove that the size of the strip is also bounded in any
fixed number of dimensions. In contrast to the planar case, we show that
deciding whether a 3D point set can be 2-colored so that any strip containing
at least three points contains both colors is NP-complete.
  We also consider the problem of coloring a given set of axis-aligned strips,
so that any sufficiently covered point in the plane is covered by $k$ colors.
We show that in $d$ dimensions the required coverage is at most $d(k{-}1)+1$.
  Lower bounds are given for the two problems. This complements recent
impossibility results on decomposition of strip coverings with arbitrary
orientations. Finally, we study a variant where strips are replaced by wedges."
"This survey gives a brief overview of theoretically and practically relevant
algorithms to compute geodesic paths and distances on three-dimensional
surfaces. The survey focuses on polyhedral three-dimensional surfaces."
"We construct partitions of rectangles into smaller rectangles from an input
consisting of a planar dual graph of the layout together with restrictions on
the orientations of edges and junctions of the layout. Such an
orientation-constrained layout, if it exists, may be constructed in polynomial
time, and all orientation-constrained layouts may be listed in polynomial time
per layout."
"We show that a $k$-fold covering using translates of an arbitrary convex
polygon can be decomposed into $\Omega(k)$ covers (using an efficient
algorithm). We generalize this result to obtain a constant factor approximation
to the sensor cover problem where the ranges of the sensors are translates of a
given convex polygon. The crucial ingredient in this generalization is a
constant factor approximation algorithm for a one-dimensional version of the
sensor cover problem, called the Restricted Strip Cover (RSC) problem, where
sensors are intervals of possibly different lengths. Our algorithm for RSC
improves on the previous $O(\log \log \log n)$ approximation."
"We extend (and somewhat simplify) the algebraic proof technique of Guth and
Katz \cite{GK}, to obtain several sharp bounds on the number of incidences
between lines and points in three dimensions. Specifically, we show: (i) The
maximum possible number of incidences between $n$ lines in $\reals^3$ and $m$
of their joints (points incident to at least three non-coplanar lines) is
$\Theta(m^{1/3}n)$ for $m\ge n$, and $\Theta(m^{2/3}n^{2/3}+m+n)$ for $m\le n$.
(ii) In particular, the number of such incidences cannot exceed $O(n^{3/2})$.
(iii) The bound in (i) also holds for incidences between $n$ lines and $m$
arbitrary points (not necessarily joints), provided that no plane contains more
than O(n) points and each point is incident to at least three lines. As a
preliminary step, we give a simpler proof of (an extension of) the bound
$O(n^{3/2})$, established by Guth and Katz, on the number of joints in a set of
$n$ lines in $\reals^3$. We also present some further extensions of these
bounds, and give a proof of Bourgain's conjecture on incidences between points
and lines in 3-space, which constitutes a simpler alternative to the proof of
\cite{GK}."
"We bound the time it takes for a group of birds to reach steady state in a
standard flocking model. We prove that (i) within single exponential time
fragmentation ceases and each bird settles on a fixed flying direction; (ii)
the flocking network converges only after a number of steps that is an iterated
exponential of height logarithmic in the number of birds. We also prove the
highly surprising result that this bound is optimal. The model directs the
birds to adjust their velocities repeatedly by averaging them with their
neighbors within a fixed radius. The model is deterministic, but we show that
it can tolerate a reasonable amount of stochastic or even adversarial noise.
Our methods are highly general and we speculate that the results extend to a
wider class of models based on undirected flocking networks, whether defined
metrically or topologically. This work introduces new techniques of broader
interest, including the ""flight net,"" the ""iterated spectral shift,"" and a
certain ""residue-clearing"" argument in circuit complexity."
"Let $k$ and $n$ be positive integers. Define $R(n,k)$ to be the minimum
positive value of $$ | e_i \sqrt{s_1} + e_2 \sqrt{s_2} + ... + e_k \sqrt{s_k}
-t | $$ where $ s_1, s_2, ..., s_k$ are positive integers no larger than $n$,
$t$ is an integer and $e_i\in \{1,0, -1\}$ for all $1\leq i\leq k$. It is
important in computational geometry to determine a good lower and upper bound
of $ R(n,k)$. In this paper we show that this problem is closely related to the
shortest vector problem in certain integral lattices and present an algorithm
to find lower bounds based on lattice reduction algorithms. Although we can
only prove an exponential time upper bound for the algorithm, it is efficient
for large $k$ when an exhaustive search for the minimum value is clearly
infeasible. It produces lower bounds much better than the root separation
technique does. Based on numerical data, we formulate a conjecture on the
length of the shortest nonzero vector in the lattice, whose validation implies
that our algorithm runs in polynomial time and the problem of comparing two
sums of square roots of small integers can be solved in polynomial time. As a
side result, we obtain constructive upper bounds for $R(n,k)$ when $ n$ is much
smaller than $2^{2k}$."
"Let $L$ be a set of $n$ lines in $\reals^d$, for $d\ge 3$. A {\em joint} of
$L$ is a point incident to at least $d$ lines of $L$, not all in a common
hyperplane. Using a very simple algebraic proof technique, we show that the
maximum possible number of joints of $L$ is $\Theta(n^{d/(d-1)})$. For $d=3$,
this is a considerable simplification of the orignal algebraic proof of Guth
and Katz~\cite{GK}, and of the follow-up simpler proof of Elekes et al.
\cite{EKS}."
"The study of (minimally) rigid graphs is motivated by numerous applications,
mostly in robotics and bioinformatics. A major open problem concerns the number
of embeddings of such graphs, up to rigid motions, in Euclidean space. We
capture embeddability by polynomial systems with suitable structure, so that
their mixed volume, which bounds the number of common roots, to yield
interesting upper bounds on the number of embeddings. We focus on $\RR^2$ and
$\RR^3$, where Laman graphs and 1-skeleta of convex simplicial polyhedra,
respectively, admit inductive Henneberg constructions. We establish the first
lower bound in $\RR^3$ of about $2.52^n$, where $n$ denotes the number of
vertices. Moreover, our implementation yields upper bounds for $n \le 10$ in
$\RR^2$ and $\RR^3$, which reduce the existing gaps, and tight bounds up to
$n=7$ in $\RR^3$."
"We present a general framework for computing two-dimensional Voronoi diagrams
of different classes of sites under various distance functions. The framework
is sufficiently general to support diagrams embedded on a family of
two-dimensional parametric surfaces in $R^3$. The computation of the diagrams
is carried out through the construction of envelopes of surfaces in 3-space
provided by CGAL (the Computational Geometry Algorithm Library). The
construction of the envelopes follows a divide-and-conquer approach. A
straightforward application of the divide-and-conquer approach for computing
Voronoi diagrams yields algorithms that are inefficient in the worst case. We
prove that through randomization the expected running time becomes near-optimal
in the worst case. We show how to employ our framework to realize various types
of Voronoi diagrams with different properties by providing implementations for
a vast collection of commonly used Voronoi diagrams. We also show how to apply
the new framework and other existing tools from CGAL to compute minimum-width
annuli of sets of disks, which requires the computation of two Voronoi diagrams
of two different types, and of the overlay of the two diagrams. We do not
assume general position. Namely, we handle degenerate input, and produce exact
results."
"A line L is a transversal to a family F of convex objects in R^d if it
intersects every member of F. In this paper we show that for every integer d>2
there exists a family of 2d-1 pairwise disjoint unit balls in R^d with the
property that every subfamily of size 2d-2 admits a transversal, yet any line
misses at least one member of the family. This answers a question of Danzer
from 1957."
"We study the following general stabbing problem from a parameterized
complexity point of view: Given a set $\mathcal S$ of $n$ translates of an
object in $\Rd$, find a set of $k$ lines with the property that every object in
$\mathcal S$ is ''stabbed'' (intersected) by at least one line.
  We show that when $S$ consists of axis-parallel unit squares in $\Rtwo$ the
(decision) problem of stabbing $S$ with axis-parallel lines is W[1]-hard with
respect to $k$ (and thus, not fixed-parameter tractable unless FPT=W[1]) while
it becomes fixed-parameter tractable when the squares are disjoint. We also
show that the problem of stabbing a set of disjoint unit squares in $\Rtwo$
with lines of arbitrary directions is W[1]--hard with respect to $k$. Several
generalizations to other types of objects and lines with arbitrary directions
are also presented. Finally, we show that deciding whether a set of unit balls
in $\Rd$ can be stabbed by one line is W[1]--hard with respect to the dimension
$d$."
"Clarksons algorithm is a two-staged randomized algorithm for solving linear
programs. This algorithm has been simplified and adapted to fit the framework
of LP-type problems. In this framework we can tackle a number of non-linear
problems such as computing the smallest enclosing ball of a set of points in
R^d . In 2006, it has been shown that the algorithm in its original form works
for violator spaces too, which are a proper general- ization of LP-type
problems. It was not clear, however, whether previous simplifications of the
algorithm carry over to the new setting. In this paper we show the following
theoretical results: (a) It is shown, for the first time, that Clarksons second
stage can be simplified. (b) The previous simplifications of Clarksons first
stage carry over to the violator space setting. (c) Furthermore, we show the
equivalence of violator spaces and partitions of the hypercube by hypercubes."
"We prove that the pleated hyperbolic paraboloid, a familiar origami model
known since 1927, in fact cannot be folded with the standard crease pattern in
the standard mathematical model of zero-thickness paper. In contrast, we show
that the model can be folded with additional creases, suggesting that real
paper ""folds"" into this model via small such creases. We conjecture that the
circular version of this model, consisting simply of concentric circular
creases, also folds without extra creases.
  At the heart of our results is a new structural theorem characterizing
uncreased intrinsically flat surfaces--the portions of paper between the
creases. Differential geometry has much to say about the local behavior of such
surfaces when they are sufficiently smooth, e.g., that they are torsal ruled.
But this classic result is simply false in the context of the whole surface.
Our structural characterization tells the whole story, and even applies to
surfaces with discontinuities in the second derivative. We use our theorem to
prove fundamental properties about how paper folds, for example, that straight
creases on the piece of paper must remain piecewise-straight (polygonal) by
folding."
"We introduce an algorithm that embeds a given 3-connected planar graph as a
convex 3-polytope with integer coordinates. The size of the coordinates is
bounded by $O(2^{7.55n})=O(188^{n})$. If the graph contains a triangle we can
bound the integer coordinates by $O(2^{4.82n})$. If the graph contains a
quadrilateral we can bound the integer coordinates by $O(2^{5.46n})$. The
crucial part of the algorithm is to find a convex plane embedding whose edges
can be weighted such that the sum of the weighted edges, seen as vectors,
cancel at every point. It is well known that this can be guaranteed for the
interior vertices by applying a technique of Tutte. We show how to extend
Tutte's ideas to construct a plane embedding where the weighted vector sums
cancel also on the vertices of the boundary face."
"We show that there is no $(1+\eps)$-approximation algorithm for the problem
of covering points in the plane by minimum number of fat triangles of similar
size (with the minimum angle of the triangles being close to 45 degrees). Here,
the available triangles are prespecified in advance. Since a constant factor
approximation algorithm is known for this problem \cite{cv-iaags-07}, this
settles the approximability of this problem.
  We also investigate some related problems, including cover by friendly fat
shapes, and independent set of triangles in three dimensions."
"In this paper, we analyze the time complexity of finding regular polygons in
a set of n points. We combine two different approaches to find regular
polygons, depending on their number of edges. Our result depends on the
parameter alpha, which has been used to bound the maximum number of isosceles
triangles that can be formed by n points. This bound has been expressed as
O(n^{2+2alpha+epsilon}), and the current best value for alpha is ~0.068.
  Our algorithm finds polygons with O(n^alpha) edges by sweeping a line through
the set of points, while larger polygons are found by random sampling. We can
find all regular polygons with high probability in O(n^{2+alpha+epsilon})
expected time for every positive epsilon. This compares well to the
O(n^{2+2alpha+epsilon}) deterministic algorithm of Brass."
"The minimum feature size of a crossing-free straight line drawing is the
minimum distance between a vertex and a non-incident edge. This quantity
measures the resolution needed to display a figure or the tool size needed to
mill the figure. The spread is the ratio of the diameter to the minimum feature
size. While many algorithms (particularly in meshing) depend on the spread of
the input, none explicitly consider finding a mesh whose spread is similar to
the input. When a polygon is partitioned into smaller regions, such as
triangles or quadrangles, the degradation is the ratio of original to final
spread (the final spread is always greater).
  Here we present an algorithm to quadrangulate a simple n-gon, while achieving
constant degradation. Note that although all faces have a quadrangular shape,
the number of edges bounding each face may be larger. This method uses Theta(n)
Steiner points and produces Theta(n) quadrangles. In fact to obtain constant
degradation, Omega(n) Steiner points are required by any algorithm.
  We also show that, for some polygons, a constant factor cannot be achieved by
any triangulation, even with an unbounded number of Steiner points. The
specific lower bounds depend on whether Steiner vertices are used or not."
We survey several results known on sampling in computational geometry.
"Many problems in computational geometry are not stated in graph-theoretic
terms, but can be solved efficiently by constructing an auxiliary graph and
performing a graph-theoretic algorithm on it. Often, the efficiency of the
algorithm depends on the special properties of the graph constructed in this
way. We survey the art gallery problem, partition into rectangles,
minimum-diameter clustering, rectilinear cartogram construction, mesh
stripification, angle optimization in tilings, and metric embedding from this
perspective."
"In a typical range emptiness searching (resp., reporting) problem, we are
given a set $P$ of $n$ points in $\reals^d$, and wish to preprocess it into a
data structure that supports efficient range emptiness (resp., reporting)
queries, in which we specify a range $\sigma$, which, in general, is a
semi-algebraic set in $\reals^d$ of constant description complexity, and wish
to determine whether $P\cap\sigma=\emptyset$, or to report all the points in
$P\cap\sigma$. Range emptiness searching and reporting arise in many
applications, and have been treated by Matou\v{s}ek \cite{Ma:rph} in the
special case where the ranges are halfspaces bounded by hyperplanes. As shown
in \cite{Ma:rph}, the two problems are closely related, and have solutions (for
the case of halfspaces) with similar performance bounds. In this paper we
extend the analysis to general semi-algebraic ranges, and show how to adapt
Matou\v{s}ek's technique, without the need to {\em linearize} the ranges into a
higher-dimensional space. This yields more efficient solutions to several
useful problems, and we demonstrate the new technique in four applications."
"In this paper we consider finding a geometric minimum-sum dipolar spanning
tree in R^3, and present an algorithm that takes O(n^2 log^2 n) time using
O(n^2) space, thus almost matching the best known results for the planar case.
Our solution uses an interesting result related to the complexity of the common
intersection of n balls in R^3, of possible different radii, that are all
tangent to a given point p. The problem has applications in communication
networks, when the goal is to minimize the distance between two hubs or servers
as well as the distance from any node in the network to the closer of the two
hubs. The approach used in this paper also provides a solution to the discrete
2-center problem in R^3 within the same time and space bounds."
"We show that the number of geometric permutations of an arbitrary collection
of $n$ pairwise disjoint convex sets in $\mathbb{R}^d$, for $d\geq 3$, is
$O(n^{2d-3}\log n)$, improving Wenger's 20 years old bound of $O(n^{2d-2})$."
"We consider a generalization of the Gabriel graph, the witness Gabriel graph.
Given a set of vertices P and a set of witnesses W in the plane, there is an
edge ab between two points of P in the witness Gabriel graph GG-(P,W) if and
only if the closed disk with diameter ab does not contain any witness point
(besides possibly a and/or b). We study several properties of the witness
Gabriel graph, both as a proximity graph and as a new tool in graph drawing."
"Proximity graphs are used in several areas in which a neighborliness
relationship for input data sets is a useful tool in their analysis, and have
also received substantial attention from the graph drawing community, as they
are a natural way of implicitly representing graphs. However, as a tool for
graph representation, proximity graphs have some limitations that may be
overcome with suitable generalizations. We introduce a generalization, witness
graphs, that encompasses both the goal of more power and flexibility for graph
drawing issues and a wider spectrum for neighborhood analysis. We study in
detail two concrete examples, both related to Delaunay graphs, and consider as
well some problems on stabbing geometric objects and point set discrimination,
that can be naturally described in terms of witness graphs."
"In fields ranging from computer vision to signal processing and statistics,
increasing computational power allows a move from classical linear models to
models that incorporate non-linear phenomena. This shift has created interest
in computational aspects of differential geometry, and solving optimization
problems that incorporate non-linear geometry constitutes an important
computational task. In this paper, we develop methods for numerically solving
optimization problems over spaces of geodesics using numerical integration of
Jacobi fields and second order derivatives of geodesic families. As an
important application of this optimization strategy, we compute exact Principal
Geodesic Analysis (PGA), a non-linear version of the PCA dimensionality
reduction procedure. By applying the exact PGA algorithm to synthetic data, we
exemplify the differences between the linearized and exact algorithms caused by
the non-linear geometry. In addition, we use the numerically integrated Jacobi
fields to determine sectional curvatures and provide upper bounds for
injectivity radii."
"This paper addresses the problem of finding an orientation and a minimum
radius for directional antennas of a fixed angle placed at the points of a
planar set S, that induce a strongly connected communication graph. We consider
problem instances in which antenna angles are fixed at 90 and 180 degrees, and
establish upper and lower bounds for the minimum radius necessary to guarantee
strong connectivity. In the case of 90-degree angles, we establish a lower
bound of 2 and an upper bound of 7. In the case of 180-degree angles, we
establish a lower bound of sqrt(3) and an upper bound of 1+sqrt(3). Underlying
our results is the assumption that the unit disk graph for S is connected."
"Let $B$ be a point robot moving in the plane, whose path is constrained to
forward motions with curvature at most one, and let $P$ be a convex polygon
with $n$ vertices. Given a starting configuration (a location and a direction
of travel) for $B$ inside $P$, we characterize the region of all points of $P$
that can be reached by $B$, and show that it has complexity $O(n)$. We give an
$O(n^2)$ time algorithm to compute this region. We show that a point is
reachable only if it can be reached by a path of type CCSCS, where C denotes a
unit circle arc and S denotes a line segment."
"We introduce the notion of a normal gallery, a gallery in which any
configuration of guards that visually covers the walls covers the entire
gallery. We show that any star gallery is normal and any gallery with at most
two reflex corners is normal. A polynomial time algorithm is provided deciding
if, for a given polygon and a finite set of positions, there exists a
configuration of guards in some of these positions that visually covers the
walls but not the entire gallery."
"This paper concerns the forward kinematics and tension distribution of
sinking winches mechanism, which is a type of four-cable-driven partly
constrained parallel robot. Conventional studies on forward kinematics of
cable-driven parallel robot assumed that all cables are taut. Actually, given
the lengths of four cables, some cables may be slack when the platform is in
static equilibrium. Therefore, in this paper, the tension state (tautness or
slackness) of cables is considered in the forward kinematics model. We propose
Traversal-Solving-Algorithm, which can indicate the tension state of cables,
and further determine the pose of the platform, if the lengths of four cables
are given. The effectiveness of the algorithm is verified by four examples. The
results of this paper can be used to control sinking winches mechanism to
achieve the level and stable motion of the platform, and to make the tension
distribution of cables as uniform as possible."
"We define the notion of affine rigidity of a hypergraph and prove a variety
of fundamental results for this notion. First, we show that affine rigidity can
be determined by the rank of a specific matrix which implies that affine
rigidity is a generic property of the hypergraph.Then we prove that if a graph
is is $(d+1)$-vertex-connected, then it must be ""generically neighborhood
affinely rigid"" in $d$-dimensional space. This implies that if a graph is
$(d+1)$-vertex-connected then any generic framework of its squared graph must
be universally rigid.
  Our results, and affine rigidity more generally, have natural applications in
point registration and localization, as well as connections to manifold
learning."
"In this paper we consider an isoperimetric inequality for the ""free
perimeter"" of a planar shape inside a rectangular domain, the free perimeter
being the length of the shape boundary that does not touch the border of the
domain."
"We present an algorithm to find an {\it Euclidean Shortest Path} from a
source vertex $s$ to a sink vertex $t$ in the presence of obstacles in $\Re^2$.
Our algorithm takes $O(T+m(\lg{m})(\lg{n}))$ time and $O(n)$ space. Here,
$O(T)$ is the time to triangulate the polygonal region, $m$ is the number of
obstacles, and $n$ is the number of vertices. This bound is close to the known
lower bound of $O(n+m\lg{m})$ time and $O(n)$ space. Our approach involve
progressing shortest path wavefront as in continuous Dijkstra-type method, and
confining its expansion to regions of interest."
"This paper presents an approximation algorithm for finding a shortest path
between two points $s$ and $t$ in a weighted planar subdivision $\PS$. Each
face $f$ of $\PS$ is associated with a weight $w_f$, and the cost of travel
along a line segment on $f$ is $w_f$ multiplied by the Euclidean norm of that
line segment. The cost of a path which traverses across several faces of the
subdivision is the sum of the costs of travel along each face. Our algorithm
progreeses the discretized shortest path wavefront from source $s$, and takes
polynomial time in finding an $\epsilon$-approximate shortest path."
"Motivated by the desire to cope with data imprecision, we study methods for
taking advantage of preliminary information about point sets in order to speed
up the computation of certain structures associated with them.
  In particular, we study the following problem: given a set L of n lines in
the plane, we wish to preprocess L such that later, upon receiving a set P of n
points, each of which lies on a distinct line of L, we can construct the convex
hull of P efficiently. We show that in quadratic time and space it is possible
to construct a data structure on L that enables us to compute the convex hull
of any such point set P in O(n alpha(n) log* n) expected time. If we further
assume that the points are ""oblivious"" with respect to the data structure, the
running time improves to O(n alpha(n)). The analysis applies almost verbatim
when L is a set of line-segments, and yields similar asymptotic bounds. We
present several extensions, including a trade-off between space and query time
and an output-sensitive algorithm. We also study the ""dual problem"" where we
show how to efficiently compute the (<= k)-level of n lines in the plane, each
of which lies on a distinct point (given in advance).
  We complement our results by Omega(n log n) lower bounds under the algebraic
computation tree model for several related problems, including sorting a set of
points (according to, say, their x-order), each of which lies on a given line
known in advance. Therefore, the convex hull problem under our setting is
easier than sorting, contrary to the ""standard"" convex hull and sorting
problems, in which the two problems require Theta(n log n) steps in the worst
case (under the algebraic computation tree model)."
"We describe conditions under which an appropriately-defined anisotropic
Voronoi diagram of a set of sites in Euclidean space is guaranteed to be
composed of connected cells in any number of dimensions. These conditions are
natural for problems in optimization and approximation, and algorithms already
exist to produce sets of sites that satisfy them."
"We show that, under mild conditions on the underlying metric, duals of
appropriately defined anisotropic Voronoi diagrams are embedded triangulations.
Furthermore, they always triangulate the convex hull of the vertices, and have
other properties that parallel those of ordinary Delaunay triangulations. These
results apply to the duals of anisotropic Voronoi diagrams of any set of
vertices, so long as the diagram is orphan-free."
"Distance function to a compact set plays a central role in several areas of
computational geometry. Methods that rely on it are robust to the perturbations
of the data by the Hausdorff noise, but fail in the presence of outliers. The
recently introduced distance to a measure offers a solution by extending the
distance function framework to reasoning about the geometry of probability
measures, while maintaining theoretical guarantees about the quality of the
inferred information. A combinatorial explosion hinders working with distance
to a measure as an ordinary (power) distance function. In this paper, we
analyze an approximation scheme that keeps the representation linear in the
size of the input, while maintaining the guarantees on the inference quality
close to those for the exact (but costly) representation."
"We study computing geometric problems on uncertain points. An uncertain point
is a point that does not have a fixed location, but rather is described by a
probability distribution. When these probability distributions are restricted
to a finite number of locations, the points are called indecisive points. In
particular, we focus on geometric shape-fitting problems and on building
compact distributions to describe how the solutions to these problems vary with
respect to the uncertainty in the points. Our main results are: (1) a simple
and efficient randomized approximation algorithm for calculating the
distribution of any statistic on uncertain data sets; (2) a polynomial,
deterministic and exact algorithm for computing the distribution of answers for
any LP-type problem on an indecisive point set; and (3) the development of
shape inclusion probability (SIP) functions which captures the ambient
distribution of shapes fit to uncertain or indecisive point sets and are
admissible to the two algorithmic constructions."
"We extend the notion of a source unfolding of a convex polyhedron P to be
based on a closed polygonal curve Q in a particular class rather than based on
a point. The class requires that Q ""lives on a cone"" to both sides; it includes
simple, closed quasigeodesics. Cutting a particular subset of the cut locus of
Q (in P) leads to a non-overlapping unfolding of the polyhedron. This gives a
new general method to unfold the surface of any convex polyhedron to a simple,
planar polygon."
"We consider the problem of approximating the majority depth (Liu and Singh,
1993) of a point q with respect to an n-point set, S, by random sampling. At
the heart of this problem is a data structures question: How can we preprocess
a set of n lines so that we can quickly test whether a randomly selected vertex
in the arrangement of these lines is above or below the median level. We
describe a Monte-Carlo data structure for this problem that can be constructed
in O(nlog n) time, can answer queries O((log n)^{4/3}) expected time, and
answers correctly with high probability."
"In this paper we study a facility location problem in the plane in which a
single point (facility) and a rapid transit line (highway) are simultaneously
located in order to minimize the total travel time from the clients to the
facility, using the $L_1$ or Manhattan metric. The rapid transit line is given
by a segment with any length and orientation, and is an alternative
transportation line that can be used by the clients to reduce their travel time
to the facility. We study the variant of the problem in which clients can enter
and exit the highway at any point. We provide an $O(n^3)$-time algorithm that
solves this variant, where $n$ is the number of clients. We also present a
detailed characterization of the solutions, which depends on the speed given in
the highway."
"We study a variation of the 1-center problem in which, in addition to a
single supply facility, we are allowed to locate a highway. This highway
increases the transportation speed between any demand point and the facility.
That is, given a set $S$ of points and $v>1$, we are interested in locating the
facility point $f$ and the highway $h$ that minimize the expression $\max_{p\in
S}d_{h}(p,f)$, where $d_h$ is the time needed to travel between $p$ and $f$. We
consider two types of highways ({\em freeways} and {\em turnpikes}) and study
the cases in which the highway's length is fixed by the user (or can be
modified to further decrease the transportation time)."
"We address the unsolved problem of unfolding prismatoids in a new context,
viewing a ""topless prismatoid"" as a convex patch---a polyhedral subset of the
surface of a convex polyhedron homeomorphic to a disk. We show that several
natural strategies for unfolding a prismatoid can fail, but obtain a positive
result for ""petal unfolding"" topless prismatoids. We also show that the natural
extension to a convex patch consisting of a face of a polyhedron and all its
incident faces, does not always have a nonoverlapping petal unfolding. However,
we obtain a positive result by excluding the problematical patches. This then
leads a positive result for restricted prismatoids. Finally, we suggest suggest
studying the unfolding of convex patches in general, and offer some possible
lines of investigation."
"Given two triangulations of a convex polygon, computing the minimum number of
flips required to transform one to the other is a long-standing open problem.
It is not known whether the problem is in P or NP-complete. We prove that two
natural generalizations of the problem are NP-complete, namely computing the
minimum number of flips between two triangulations of (1) a polygon with holes;
(2) a set of points in the plane."
"We show that Delaunay triangulations and compressed quadtrees are equivalent
structures. More precisely, we give two algorithms: the first computes a
compressed quadtree for a planar point set, given the Delaunay triangulation;
the second finds the Delaunay triangulation, given a compressed quadtree. Both
algorithms run in deterministic linear time on a pointer machine. Our work
builds on and extends previous results by Krznaric and Levcopolous and Buchin
and Mulzer. Our main tool for the second algorithm is the well-separated pair
decomposition(WSPD), a structure that has been used previously to find
Euclidean minimum spanning trees in higher dimensions (Eppstein). We show that
knowing the WSPD (and a quadtree) suffices to compute a planar Euclidean
minimum spanning tree (EMST) in linear time. With the EMST at hand, we can find
the Delaunay triangulation in linear time.
  As a corollary, we obtain deterministic versions of many previous algorithms
related to Delaunay triangulations, such as splitting planar Delaunay
triangulations, preprocessing imprecise points for faster Delaunay computation,
and transdichotomous Delaunay triangulations."
"Given an arrangement of lines in the plane, what is the minimum number $c$ of
colors required to color the lines so that no cell of the arrangement is
monochromatic? In this paper we give bounds on the number c both for the above
question, as well as some of its variations. We redefine these problems as
geometric hypergraph coloring problems. If we define $\Hlinecell$ as the
hypergraph where vertices are lines and edges represent cells of the
arrangement, the answer to the above question is equal to the chromatic number
of this hypergraph. We prove that this chromatic number is between $\Omega
(\log n / \log\log n)$. and $O(\sqrt{n})$.
  Similarly, we give bounds on the minimum size of a subset $S$ of the
intersections of the lines in $\mathcal{A}$ such that every cell is bounded by
at least one of the vertices in $S$. This may be seen as a problem on guarding
cells with vertices when the lines act as obstacles. The problem can also be
defined as the minimum vertex cover problem in the hypergraph $\Hvertexcell$,
the vertices of which are the line intersections, and the hyperedges are
vertices of a cell. Analogously, we consider the problem of touching the lines
with a minimum subset of the cells of the arrangement, which we identify as the
minimum vertex cover problem in the $\Hcellzone$ hypergraph."
"In this paper we present a novel non-parametric method of simplifying
piecewise linear curves and we apply this method as a statistical approximation
of structure within sequential data in the plane. We consider the problem of
minimizing the average length of sequences of consecutive input points that lie
on any one side of the simplified curve. Specifically, given a sequence $P$ of
$n$ points in the plane that determine a simple polygonal chain consisting of
$n-1$ segments, we describe algorithms for selecting an ordered subset $Q
\subset P$ (including the first and last points of $P$) that determines a
second polygonal chain to approximate $P$, such that the number of crossings
between the two polygonal chains is maximized, and the cardinality of $Q$ is
minimized among all such maximizing subsets of $P$. Our algorithms have
respective running times $O(n^2\log n)$ when $P$ is monotonic and $O(n^2\log^2
n)$ when $P$ is an arbitrary simple polyline. Finally, we examine the
application of our algorithms iteratively in a bootstrapping technique to
define a smooth robust non-parametric approximation of the original sequence."
"Flips in triangulations have received a lot of attention over the past
decades. However, the problem of tracking where particular edges go during the
flipping process has not been addressed. We examine this question by attaching
unique labels to the triangulation edges. We introduce the concept of the orbit
of an edge $e$, which is the set of all edges reachable from $e$ via flips.
  We establish the first upper and lower bounds on the diameter of the flip
graph in this setting. Specifically, we prove tight $\Theta(n \log n)$ bounds
for edge-labelled triangulations of $n$-vertex convex polygons and
combinatorial triangulations, contrasting with the $\Theta(n)$ bounds in their
respective unlabelled settings. The $\Omega(n \log n)$ lower bound for the
convex polygon setting might be of independent interest, as it generalizes
lower bounds on certain sorting models. When simultaneous flips are allowed,
the upper bound for convex polygons decreases to $O(\log^2 n)$, although we no
longer have a matching lower bound.
  Moving beyond convex polygons, we show that edge-labelled triangulated
polygons with a single reflex vertex can have a disconnected flip graph. This
is in sharp contrast with the unlabelled case, where the flip graph is
connected for any triangulated polygon. For spiral polygons, we provide a
complete characterization of the orbits. This allows us to decide connectivity
of the flip graph of a spiral polygon in linear time. We also prove an upper
bound of $O(n^2)$ on the diameter of each connected component, which is optimal
in the worst case. We conclude with an example of a non-spiral polygon whose
flip graph has diameter $\Omega(n^3)$."
"The development of laser scanning techniques has popularized the
representation of 3D shapes by triangular meshes with a large number of
vertices. Compression techniques dedicated to such meshes have emerged, which
exploit the idea that the connectivity of a dense mesh does not deviate much
from the connectivity that can be constructed automatically from the vertex
positions (while possibly being guided by additional codes). The edge flip is
one of the tools that can encode the differences between two meshes, and it is
important to control the length of a sequence of flips that transform one
triangulation into another. This paper provides a practical solution to this
problem. Indeed, the problem of determining a minimal sequence of edge flips
between two triangulations is NP-complete for some types of triangulations
including manifold triangulations of surfaces, so that it is necessary to
develop heuristics. Moreover, it is sometimes difficult to establish a first
sequence of flips between two meshes, and we advocate a solution based on the
reduction of an existing sequence. The new approach we propose is founded on
the assignment of labels to identify the edges, with a property of label
transfer during a flip. This gives a meaning to the tracking of an edge in a
sequence of flips and offers the exploitation of very simple combinatorial
properties. All the operations are performed directly on the sequence of labels
denoting the edges to be flipped, almost regardless of the underlying surface,
since only local temporary connectivity is involved."
"Let D be a set of n disks in the plane. We present a data structure of size
O(n) that can compute, for any query point q, the largest disk in D that
contains q, in O(log n) time. The structure can be constructed in O(n log^3 n)
time. The optimal storage and query time of the structure improve several
recent solutions by Augustine et al. and by Kaplan and Sharir."
"We define a new class of orthogonal polyhedra, called orthogrids, that can be
unfolded without overlap with constant refinement of the gridded surface."
"The \emph{Fr\'echet distance} is a well studied similarity measures between
curves. The \emph{discrete Fr\'echet distance} is an analogous similarity
measure, defined for a sequence $A$ of $m$ points and a sequence $B$ of $n$
points, where the points are usually sampled from input curves. In this paper
we consider a variant, called the \emph{discrete Fr\'echet distance with
shortcuts}, which captures the similarity between (sampled) curves in the
presence of outliers. For the \emph{two-sided} case, where shortcuts are
allowed in both curves, we give an $O((m^{2/3}n^{2/3}+m+n)\log^3 (m+n))$-time
algorithm for computing this distance. When shortcuts are allowed only in one
noise-containing curve, we give an even faster randomized algorithm that runs
in $O((m+n)^{6/5+\varepsilon})$ expected time, for any $\varepsilon>0$.
  Our techniques are novel and may find further applications. One of the main
new technical results is: Given two sets of points $A$ and $B$ and an interval
$I$, we develop an algorithm that decides whether the number of pairs $(x,y)\in
A\times B$ whose distance ${\rm dist}(x,y)$ is in $I$, is less than some given
threshold $L$. The running time of this algorithm decreases as $L$ increases.
In case there are more than $L$ pairs of points whose distance is in $I$, we
can get a small sample of pairs that contains a pair at approximate median
distance (i.e., we can approximately ""bisect"" $I$). We combine this procedure
with additional ideas to search, with a small overhead, for the optimal
one-sided Fr\'echet distance with shortcuts, using a very fast decision
procedure. We also show how to apply this technique for approximating distance
selection (with respect to rank), and for computing the semi-continuous
Fr\'echet distance with one-sided shortcuts."
"Let $\mathcal{C}=\{C_1,\ldots,C_n\}$ be a set of $n$ pairwise-disjoint convex
sets of constant description complexity, and let $\pi$ be a probability density
function (pdf for short) over the non-negative reals. For each $i$, let $K_i$
be the Minkowski sum of $C_i$ with a disk of radius $r_i$, where each $r_i$ is
a random non-negative number drawn independently from the distribution
determined by $\pi$. We show that the expected complexity of the union of $K_1,
\ldots, K_n$ is $O(n^{1+\varepsilon})$ for any $\varepsilon > 0$; here the
constant of proportionality depends on $\varepsilon$ and on the description
complexity of the sets in $\mathcal{C}$, but not on $\pi$. If each $C_i$ is a
convex polygon with at most $s$ vertices, then we show that the expected
complexity of the union is $O(s^2n\log n)$.
  Our bounds hold in the stronger model in which we are given an arbitrary
multi-set $R=\{r_1,\ldots,r_n\}$ of expansion radii, each a non-negative real
number. We assign them to the members of $\mathcal{C}$ by a random permutation,
where all permutations are equally likely to be chosen; the expectations are
now with respect to these permutations.
  We also present an application of our results to a problem that arises in
analyzing the vulnerability of a network to a physical attack. %"
"We describe a randomized algorithm that, given a set $P$ of points in the
plane, computes the best location to insert a new point $p$, such that the
Delaunay triangulation of $P\cup\{p\}$ has the largest possible minimum angle.
The expected running time of our algorithm is at most cubic, improving the
roughly quartic time of the best previously known algorithm. It slows down to
slightly super-cubic if we also specify a set of non-crossing segments with
endpoints in $P$ and insist that the triangulation respect these segments,
i.e., is the constrained Delaunay triangulation of the points and segments."
"Volumetric parameterization problem refers to parameterization of both the
interior and boundary of a 3D model. It is a much harder problem compared to
surface parameterization where a parametric representation is worked out only
for the boundary of a 3D model (which is a surface). Volumetric
parameterization is typically helpful in solving complicated geometric problems
pertaining to shape matching, morphing, path planning of robots, and
isogeometric analysis etc. A novel method is proposed in which a volume
parameterization is developed by mapping a general non-convex (genus-0) domain
to its topologically equivalent convex domain. In order to achieve a continuous
and bijective mapping of a domain, first we use the harmonic function to
establish a potential field over the domain. The gradients of the potential
values are used to track the streamlines which originate from the boundary and
converge to a single point, referred to as the shape center. Each streamline
approaches the shape center at a unique polar angle and an azimuthal angle .
Once all the three parameters (potential value, polar angle, azimuthal angle)
necessary to represent any point in the given domain are available, the domain
is said to be parameterized. Using our method, given a 3D non-convex domain, we
can parameterize the surface as well as the interior of the domain. The
proposed method is implemented and the algorithm is tested on many standard
cases to demonstrate the effectiveness."
"In this paper we consider the problem of computing the weak visibility
polygon of any query line segment $pq$ (or $WVP(pq)$) inside a given polygon
$P$. Our first non-trivial algorithm runs in simple polygons and needs $O(n^3
\log n)$ time and $O(n^3)$ space in the preprocessing phase to report $WVP(pq)$
of any query line segment $pq$ in time $O(\log n + |WVP(pq)|)$. We also give an
algorithm to compute the weak visibility polygon of a query line segment in a
non-simple polygon with $h$ pairwise-disjoint polygonal obstacles with a total
of $n$ vertices. Our algorithm needs $O(n^2 \log n)$ time and $O(n^2)$ space in
the preprocessing phase and computes $WVP(pq)$ in query time of $O(n\hbar \log
n + k)$, in which $\hbar$ is an output sensitive parameter of at most
$\min(h,k)$, and $k = O(n^2h^2)$ is the output size. This is the best
query-time result on this problem so far."
"We present an algorithm that takes as input a finite point set in Euclidean
space, and performs a perturbation that guarantees that the Delaunay
triangulation of the resulting perturbed point set has quantifiable stability
with respect to the metric and the point positions. There is also a guarantee
on the quality of the simplices: they cannot be too flat. The algorithm
provides an alternative tool to the weighting or refinement methods to remove
poorly shaped simplices in Delaunay triangulations of arbitrary dimension, but
in addition it provides a guarantee of stability for the resulting
triangulation."
"The Discrete Morse Theory of Forman appeared to be useful for providing
filtration-preserving reductions of complexes in the study of persistent
homology. So far, the algorithms computing discrete Morse matchings have only
been used for one-dimensional filtrations. This paper is perhaps the first
attempt in the direction of extending such algorithms to multidimensional
filtrations. Initial framework related to Morse matchings for the
multidimensional setting is proposed, and a matching algorithm given by King,
Knudson, and Mramor is extended in this direction. The correctness of the
algorithm is proved, and its complexity analyzed. The algorithm is used for
establishing a reduction of a simplicial complex to a smaller but not
necessarily optimal cellular complex. First experiments with filtrations of
triangular meshes are presented."
"The Frechet distance is a well-studied and very popular measure of similarity
of two curves. Many variants and extensions have been studied since Alt and
Godau introduced this measure to computational geometry in 1991. Their original
algorithm to compute the Frechet distance of two polygonal curves with n
vertices has a runtime of O(n^2 log n). More than 20 years later, the state of
the art algorithms for most variants still take time more than O(n^2 / log n),
but no matching lower bounds are known, not even under reasonable complexity
theoretic assumptions.
  To obtain a conditional lower bound, in this paper we assume the Strong
Exponential Time Hypothesis or, more precisely, that there is no
O*((2-delta)^N) algorithm for CNF-SAT for any delta > 0. Under this assumption
we show that the Frechet distance cannot be computed in strongly subquadratic
time, i.e., in time O(n^{2-delta}) for any delta > 0. This means that finding
faster algorithms for the Frechet distance is as hard as finding faster CNF-SAT
algorithms, and the existence of a strongly subquadratic algorithm can be
considered unlikely.
  Our result holds for both the continuous and the discrete Frechet distance.
We extend the main result in various directions. Based on the same assumption
we (1) show non-existence of a strongly subquadratic 1.001-approximation, (2)
present tight lower bounds in case the numbers of vertices of the two curves
are imbalanced, and (3) examine realistic input assumptions (c-packed curves)."
"Let $(X,\mathbf{d})$ be a metric space, $V\subseteq X$ a finite set, and $E
\subseteq V \times V$. We call the graph $G(E,V)$ a {\em metric} graph if each
edge $(u,v) \in E$ has weight $d(u,v)$. In particular edge $(u,u)$ is in the
graph and have distance $0$. We call $G$ a {\em proximal navigation graph} or
$PN$-graph if for each edge $(u,v) \in E$ either $u=v$ or there is a node $u_1$
such that $(u,u_1) \in E$ and $\mathbf{d}(u,v) > \mathbf{d}(u_1,v)$. In such
graph it is possible to navigate greedily from an arbitrary source node to an
arbitrary target node by reducing the distance between the current node and the
target node in each step. The complete graph, the Delaunay triangulation and
the Half Space Proximal (HSP) graph (defined below in the paper) are examples
of $PN$-graphs.
  In this paper we study the relationship between $PN$-graphs and $t$-spanners
and prove that there are $PN$-graphs that are not $t$-spanners for any $t$. On
the positive side we give sufficient conditions for a $PN$-graph to be a
$t$-spanner and prove that any $PN$-graph over $\mathbb{R}^n$ under the
euclidean distance is a $t$-spanner."
"Over the past decade, we have designed six typefaces based on mathematical
theorems and open problems, specifically computational geometry. These
typefaces expose the general public in a unique way to intriguing results and
hard problems in hinged dissections, geometric tours, origami design,
computer-aided glass design, physical simulation, and protein folding. In
particular, most of these typefaces include puzzle fonts, where reading the
intended message requires solving a series of puzzles which illustrate the
challenge of the underlying algorithmic problem."
"We present an extension of Voronoi diagrams where when considering which site
a client is going to use, in addition to the site distances, other site
attributes are also considered (for example, prices or weights). A cell in this
diagram is then the locus of all clients that consider the same set of sites to
be relevant. In particular, the precise site a client might use from this
candidate set depends on parameters that might change between usages, and the
candidate set lists all of the relevant sites. The resulting diagram is
significantly more expressive than Voronoi diagrams, but naturally has the
drawback that its complexity, even in the plane, might be quite high.
Nevertheless, we show that if the attributes of the sites are drawn from the
same distribution (note that the locations are fixed), then the expected
complexity of the candidate diagram is near linear.
  To this end, we derive several new technical results, which are of
independent interest. In particular, we provide a high-probability,
asymptotically optimal bound on the number of Pareto optima points in a point
set uniformly sampled from the $d$-dimensional hypercube. To do so we revisit
the classical backward analysis technique, both simplifying and improving
relevant results in order to achieve the high-probability bounds."
"We revisit two NP-hard geometric partitioning problems - convex decomposition
and surface approximation. Building on recent developments in geometric
separators, we present quasi-polynomial time algorithms for these problems with
improved approximation guarantees."
"The purpose of this note is to describe of a new set of numerical invariants,
the relevant level persistence numbers, and make explicit their relationship
with the four types of bar codes, a more familiar set of complete invariants
for level persistence. The paper provides the opportunity to compare level
persistence with the persistence introduced by Edelsbrunner-
Letscher-Zomorodian called in this paper, as sub-level persistence."
"A function $f\colon\mathbb R\to\mathbb R$ is called \emph{$k$-monotone} if it
is $(k-2)$-times differentiable and its $(k-2)$nd derivative is convex. A point
set $P\subset\mathbb R^2$ is \emph{$k$-monotone interpolable} if it lies on a
graph of a $k$-monotone function. These notions have been studied in analysis,
approximation theory etc. since the 1940s.
  We show that 3-monotone interpolability is very non-local: we exhibit an
arbitrarily large finite $P$ for which every proper subset is $3$-monotone
interpolable but $P$ itself is not. On the other hand, we prove a Ramsey-type
result: for every $n$ there exists $N$ such that every $N$-point $P$ with
distinct $x$-coordinates contains an $n$-point $Q$ such that $Q$ or its
vertical mirror reflection are $3$-monotone interpolable. The analogs for
$k$-monotone interpolability with $k=1$ and $k=2$ are classical theorems of
Erd\H{o}s and Szekeres, while the cases with $k\ge4$ remain open.
  We also investigate the computational complexity of deciding $3$-monotone
interpolability of a given point set. Using a known characterization, this
decision problem can be stated as an instance of polynomial optimization and
reformulated as a semidefinite program. We exhibit an example for which this
semidefinite program has only doubly exponentially large feasible solutions,
and thus known algorithms cannot solve it in polynomial time. While such
phenomena have been well known for semidefinite programming in general, ours
seems to be the first such example in polynomial optimization, and it involves
only univariate quadratic polynomials."
"We present improved upper and lower bounds on the spanning ratio of
$\theta$-graphs with at least six cones. Given a set of points in the plane, a
$\theta$-graph partitions the plane around each vertex into $m$ disjoint cones,
each having aperture $\theta=2\pi/m$, and adds an edge to the `closest' vertex
in each cone. We show that for any integer $k \geq 1$, $\theta$-graphs with
$4k+2$ cones have a spanning ratio of $1+2\sin(\theta/2)$ and we provide a
matching lower bound, showing that this spanning ratio tight.
  Next, we show that for any integer $k \geq 1$, $\theta$-graphs with $4k+4$
cones have spanning ratio at most
$1+2\sin(\theta/2)/(\cos(\theta/2)-\sin(\theta/2))$. We also show that
$\theta$-graphs with $4k+3$ and $4k+5$ cones have spanning ratio at most
$\cos(\theta/4)/(\cos(\theta/2)-\sin(3\theta/4))$. This is a significant
improvement on all families of $\theta$-graphs for which exact bounds are not
known. For example, the spanning ratio of the $\theta$-graph with 7 cones is
decreased from at most 7.5625 to at most 3.5132. These spanning proofs also
imply improved upper bounds on the competitiveness of the $\theta$-routing
algorithm. In particular, we show that the $\theta$-routing algorithm is
$(1+2\sin(\theta/2)/(\cos(\theta/2)-\sin(\theta/2)))$-competitive on
$\theta$-graphs with $4k+4$ cones and that this ratio is tight.
  Finally, we present improved lower bounds on the spanning ratio of these
graphs. Using these bounds, we provide a partial order on these families of
$\theta$-graphs. In particular, we show that $\theta$-graphs with $4k+4$ cones
have spanning ratio at least $1+2\tan(\theta/2)+2\tan^2(\theta/2)$. This is
somewhat surprising since, for equal values of $k$, the spanning ratio of
$\theta$-graphs with $4k+4$ cones is greater than that of $\theta$-graphs with
$4k+2$ cones, showing that increasing the number of cones can make the spanning
ratio worse."
"Project ILATO focuses on Improving Limited Angle computed Tomography by
Optical data integration in order to enhance image quality and shorten
acquisition times in X-ray based industrial quality inspection. Limited angle
computed tomography is indicated whenever specimen dimensions exceed cone beam
limits or the object is impenetrable from certain angles. Thus, acquiring only
a subset of a full circle CT scan poses problems for reconstruction algorithms
due to incomplete data which introduces blurred edges and other artifacts. To
support volumetric data reconstruction algorithm a surface mesh of the object
obtained via structured light optical scan acts as a mask defining boundaries
of the reconstructed image. The registration of optically acquired surfaces
with data acquired from computed tomography is our current challenge. This
article presents our setup, the methods applied and discusses the problems
arising from registration of data sets created with considerably different
imaging techniques."
"In this paper, we show that the $\theta$-graph with three cones is connected.
We also provide an alternative proof of the connectivity of the Yao graph with
three cones."
"An $st$-path in a drawing of a graph is self-approaching if during the
traversal of the corresponding curve from $s$ to any point $t'$ on the curve
the distance to $t'$ is non-increasing. A path has increasing chords if it is
self-approaching in both directions. A drawing is self-approaching
(increasing-chord) if any pair of vertices is connected by a self-approaching
(increasing-chord) path.
  We study self-approaching and increasing-chord drawings of triangulations and
3-connected planar graphs. We show that in the Euclidean plane, triangulations
admit increasing-chord drawings, and for planar 3-trees we can ensure
planarity. We prove that strongly monotone (and thus increasing-chord) drawings
of trees and binary cactuses require exponential resolution in the worst case,
answering an open question by Kindermann et al. [GD'14]. Moreover, we provide a
binary cactus that does not admit a self-approaching drawing. Finally, we show
that 3-connected planar graphs admit increasing-chord drawings in the
hyperbolic plane and characterize the trees that admit such drawings."
"In this paper we focus on the map matching problem where the goal is to find
a path through a planar graph such that the path through the vertices closely
matches a given polygonal curve. The map matching problem is usually approached
with the Fr\'echet distance matching the edges of the path as well. Here, we
formally define the discrete map matching problem based on the discrete
Fr\'echet distance. We then look at the complexities of some variations of the
problem which allow for vertices in the graph to be unique or reused, and
whether there is a bound on the length of the path or the number of vertices
from the graph used in the path. We prove several of these problems to be
NP-complete, and then conclude the paper with some open questions."
"The problem of efficiently computing and visualizing the structural
resemblance between a pair of protein backbones in 3D has led Bereg et al. to
pose the Chain Pair Simplification problem (CPS). In this problem, given two
polygonal chains $A$ and $B$ of lengths $m$ and $n$, respectively, one needs to
simplify them simultaneously, such that each of the resulting simplified
chains, $A'$ and $B'$, is of length at most $k$ and the discrete \frechet\
distance between $A'$ and $B'$ is at most $\delta$, where $k$ and $\delta$ are
given parameters.
  In this paper we study the complexity of CPS under the discrete \frechet\
distance (CPS-3F), i.e., where the quality of the simplifications is also
measured by the discrete \frechet\ distance. Since CPS-3F was posed in 2008,
its complexity has remained open. However, it was believed to be \npc, since
CPS under the Hausdorff distance (CPS-2H) was shown to be \npc. We first prove
that the weighted version of CPS-3F is indeed weakly \npc\, even on the line,
based on a reduction from the set partition problem. Then, we prove that CPS-3F
is actually polynomially solvable, by presenting an $O(m^2n^2\min\{m,n\})$ time
algorithm for the corresponding minimization problem. In fact, we prove a
stronger statement, implying, for example, that if weights are assigned to the
vertices of only one of the chains, then the problem remains polynomially
solvable. We also study a few less rigid variants of CPS and present efficient
solutions for them.
  Finally, we present some experimental results that suggest that (the
minimization version of) CPS-3F is significantly better than previous
algorithms for the motivating biological application."
"In this note we prove that, if $S_n$ is the greatest area of a rectangle
which can be covered with $n$ unit disks, then $2\leq S_n/n<3 \sqrt{3}/2$, and
these are the best constants; moreover, for $\Delta(n):=(3\sqrt{3}/2)n-S_n$, we
have $0.727384<\liminf\Delta(n)/\sqrt{n}<2.121321$ and
$0.727384<\limsup\Delta(n)/\sqrt{n}<4.165064$."
"The article analyzes similarity of closed polygonal curves in Frechet metric,
which is stronger than the well-known Hausdorff metric and therefore is more
appropriate in some applications. An algorithm that determines whether the
Frechet distance between two closed polygonal curves with m and n vertices is
less than a given number is described. The described algorithm takes O(mn) time
whereas the previously known algorithms take O(mn log(mn)) time."
"The art gallery problem enquires about the least number of guards that are
sufficient to ensure that an art gallery, represented by a polygon $P$, is
fully guarded. In 1998, the problems of finding the minimum number of point
guards, vertex guards, and edge guards required to guard $P$ were shown to be
APX-hard by Eidenbenz, Widmayer and Stamm. In 1987, Ghosh presented
approximation algorithms for vertex guards and edge guards that achieved a
ratio of $\mathcal{O}(\log n)$, which was improved upto $\mathcal{O}(\log\log
OPT)$ by King and Kirkpatrick in 2011. It has been conjectured that
constant-factor approximation algorithms exist for these problems. We settle
the conjecture for the special class of polygons that are weakly visible from
an edge and contain no holes by presenting a 6-approximation algorithm for
finding the minimum number of vertex guards that runs in $\mathcal{O}(n^2)$
time. On the other hand, for weak visibility polygons with holes, we present a
reduction from the Set Cover problem to show that there cannot exist a
polynomial time algorithm for the vertex guard problem with an approximation
ratio better than $((1 - \epsilon)/12)\ln n$ for any $\epsilon>0$, unless NP=P.
We also show that, for the special class of polygons without holes that are
orthogonal as well as weakly visible from an edge, the approximation ratio can
be improved to 3. Finally, we consider the Point Guard problem and show that it
is NP-hard in the case of polygons weakly visible from an edge."
"We consider an extension of the triangular-distance Delaunay graphs
(TD-Delaunay) on a set $P$ of points in the plane. In TD-Delaunay, the convex
distance is defined by a fixed-oriented equilateral triangle $\triangledown$,
and there is an edge between two points in $P$ if and only if there is an empty
homothet of $\triangledown$ having the two points on its boundary. We consider
higher-order triangular-distance Delaunay graphs, namely $k$-TD, which contains
an edge between two points if the interior of the homothet of $\triangledown$
having the two points on its boundary contains at most $k$ points of $P$. We
consider the connectivity, Hamiltonicity and perfect-matching admissibility of
$k$-TD. Finally we consider the problem of blocking the edges of $k$-TD."
"We present a deterministic local routing algorithm that is guaranteed to find
a path between any pair of vertices in a half-$\theta_6$-graph (the
half-$\theta_6$-graph is equivalent to the Delaunay triangulation where the
empty region is an equilateral triangle). The length of the path is at most
$5/\sqrt{3} \approx 2.887$ times the Euclidean distance between the pair of
vertices. Moreover, we show that no local routing algorithm can achieve a
better routing ratio, thereby proving that our routing algorithm is optimal.
This is somewhat surprising because the spanning ratio of the
half-$\theta_6$-graph is 2, meaning that even though there always exists a path
whose lengths is at most twice the Euclidean distance, we cannot always find
such a path when routing locally.
  Since every triangulation can be embedded in the plane as a
half-$\theta_6$-graph using $O(\log n)$ bits per vertex coordinate via
Schnyder's embedding scheme (SODA 1990), our result provides a competitive
local routing algorithm for every such embedded triangulation. Finally, we show
how our routing algorithm can be adapted to provide a routing ratio of
$15/\sqrt{3} \approx 8.660$ on two bounded degree subgraphs of the
half-$\theta_6$-graph."
"We provide a general framework for getting expected linear time constant
factor approximations (and in many cases FPTAS's) to several well known
problems in Computational Geometry, such as $k$-center clustering and farthest
nearest neighbor. The new approach is robust to variations in the input
problem, and yet it is simple, elegant and practical. In particular, many of
these well studied problems which fit easily into our framework, either
previously had no linear time approximation algorithm, or required rather
involved algorithms and analysis. A short list of the problems we consider
include farthest nearest neighbor, $k$-center clustering, smallest disk
enclosing $k$ points, $k$th largest distance, $k$th smallest $m$-nearest
neighbor distance, $k$th heaviest edge in the MST and other spanning forest
type problems, problems involving upward closed set systems, and more. Finally,
we show how to extend our framework such that the linear running time bound
holds with high probability."
"We address the following problem: Given a complete $k$-partite geometric
graph $K$ whose vertex set is a set of $n$ points in $\mathbb{R}^d$, compute a
spanner of $K$ that has a ``small'' stretch factor and ``few'' edges. We
present two algorithms for this problem. The first algorithm computes a
$(5+\epsilon)$-spanner of $K$ with O(n) edges in $O(n \log n)$ time. The second
algorithm computes a $(3+\epsilon)$-spanner of $K$ with $O(n \log n)$ edges in
$O(n \log n)$ time. The latter result is optimal: We show that for any $2 \leq
k \leq n - \Theta(\sqrt{n \log n})$, spanners with $O(n \log n)$ edges and
stretch factor less than 3 do not exist for all complete $k$-partite geometric
graphs."
"We prove that any finite collection of polygons of equal area has a common
hinged dissection. That is, for any such collection of polygons there exists a
chain of polygons hinged at vertices that can be folded in the plane
continuously without self-intersection to form any polygon in the collection.
This result settles the open problem about the existence of hinged dissections
between pairs of polygons that goes back implicitly to 1864 and has been
studied extensively in the past ten years. Our result generalizes and indeed
builds upon the result from 1814 that polygons have common dissections (without
hinges). We also extend our common dissection result to edge-hinged dissections
of solid 3D polyhedra that have a common (unhinged) dissection, as determined
by Dehn's 1900 solution to Hilbert's Third Problem. Our proofs are
constructive, giving explicit algorithms in all cases. For a constant number of
planar polygons, both the number of pieces and running time required by our
construction are pseudopolynomial. This bound is the best possible, even for
unhinged dissections. Hinged dissections have possible applications to
reconfigurable robotics, programmable matter, and nanomanufacturing."
"This paper studies real-world road networks from an algorithmic perspective,
focusing on empirical studies that yield useful properties of road networks
that can be exploited in the design of fast algorithms that deal with
geographic data. Unlike previous approaches, our study is not based on the
assumption that road networks are planar graphs. Indeed, based on the a number
of experiments we have performed on the road networks of the 50 United States
and District of Columbia, we provide strong empirical evidence that road
networks are quite non-planar. Our approach therefore instead is directed at
finding algorithmically-motivated properties of road networks as non-planar
geometric graphs, focusing on alternative properties of road networks that can
still lead to efficient algorithms for such problems as shortest paths and
Voronoi diagrams. In particular, we study road networks as multiscale-dispersed
graphs, which is a concept we formalize in terms of disk neighborhood systems.
This approach allows us to develop fast algorithms for road networks without
making any additional assumptions about the distribution of edge weights. In
fact, our algorithms can allow for non-metric weights."
"We describe a new methodology for studying persistence of topological
features across a family of spaces or point-cloud data sets, called zigzag
persistence. Building on classical results about quiver representations, zigzag
persistence generalises the highly successful theory of persistent homology and
addresses several situations which are not covered by that theory. In this
paper we develop theoretical and algorithmic foundations with a view towards
applications in topological statistics."
"We study Voronoi diagrams for distance functions that add together two convex
functions, each taking as its argument the difference between Cartesian
coordinates of two planar points. When the functions do not grow too quickly,
then the Voronoi diagram has linear complexity and can be constructed in
near-linear randomized expected time. Additionally, the level sets of the
distances from the sites form a family of pseudocircles in the plane, all cells
in the Voronoi diagram are connected, and the set of bisectors separating any
one cell in the diagram from each of the others forms an arrangement of
pseudolines in the plane. We apply these results to the smoothed distance or
biotope transform metric, a geometric analogue of the Jaccard distance whose
Voronoi diagrams can be used to determine the dilation of a star network with a
given hub. For sufficiently closely spaced points in the plane, the Voronoi
diagram of smoothed distance has linear complexity and can be computed
efficiently. We also experiment with a variant of Lloyd's algorithm, adapted to
smoothed distance, to find uniformly spaced point samples with exponentially
decreasing density around a given point."
"We extend the notion of a star unfolding to be based on a simple
quasigeodesic loop Q rather than on a point. This gives a new general method to
unfold the surface of any convex polyhedron P to a simple, planar polygon:
shortest paths from all vertices of P to Q are cut, and all but one segment of
Q is cut."
"A typical computational geometry problem begins: Consider a set P of n points
in R^d. However, many applications today work with input that is not precisely
known, for example when the data is sensed and has some known error model. What
if we do not know the set P exactly, but rather we have a probability
distribution mu_p governing the location of each point p in P?
  Consider a set of (non-fixed) points P, and let mu_P be the probability
distribution of this set. We study several measures (e.g. the radius of the
smallest enclosing ball, or the area of the smallest enclosing box) with
respect to mu_P. The solutions to these problems do not, as in the traditional
case, consist of a single answer, but rather a distribution of answers. We
describe several data structures that approximate distributions of answers for
shape fitting problems.
  We provide simple and efficient randomized algorithms for computing all of
these data structures, which are easy to implement and practical. We provide
some experimental results to assert this. We also provide more involved
deterministic algorithms for some of these data structures that run in time
polynomial in n and 1/eps, where eps is the approximation factor."
"In this paper we show that the (co)chain complex associated with a
decomposition of the computational domain, commonly called a mesh in
computational science and engineering, can be represented by a block-bidiagonal
matrix that we call the Hasse matrix. Moreover, we show that
topology-preserving mesh refinements, produced by the action of (the simplest)
Euler operators, can be reduced to multilinear transformations of the Hasse
matrix representing the complex. Our main result is a new representation of the
(co)chain complex underlying field computations, a representation that provides
new insights into the transformations induced by local mesh refinements. Our
approach is based on first principles and is general in that it applies to most
representational domains that can be characterized as cell complexes, without
any restrictions on their type, dimension, codimension, orientability,
manifoldness, connectedness."
"In greedy geometric routing, messages are passed in a network embedded in a
metric space according to the greedy strategy of always forwarding messages to
nodes that are closer to the destination. We show that greedy geometric routing
schemes exist for the Euclidean metric in R^2, for 3-connected planar graphs,
with coordinates that can be represented succinctly, that is, with O(log n)
bits, where n is the number of vertices in the graph. Moreover, our embedding
strategy introduces a coordinate system for R^2 that supports distance
comparisons using our succinct coordinates. Thus, our scheme can be used to
significantly reduce bandwidth, space, and header size over other recently
discovered greedy geometric routing implementations for R^2."
"Alexandrov's Theorem states that every metric with the global topology and
local geometry required of a convex polyhedron is in fact the intrinsic metric
of a unique convex polyhedron. Recent work by Bobenko and Izmestiev describes a
differential equation whose solution leads to the polyhedron corresponding to a
given metric. We describe an algorithm based on this differential equation to
compute the polyhedron to arbitrary precision given the metric, and prove a
pseudopolynomial bound on its running time. Along the way, we develop
pseudopolynomial algorithms for computing shortest paths and weighted Delaunay
triangulations on a polyhedral surface, even when the surface edges are not
shortest paths."
"Algorithms for determining quality/cost/price tradeoffs in saturated markets
are considered. A product is modeled by $d$ real-valued qualities whose sum
determines the unit cost of producing the product. This leads to the following
optimization problem: given a set of $n$ customers, each of whom has certain
minimum quality requirements and a maximum price they are willing to pay,
design a new product and select a price for that product in order to maximize
the resulting profit. An $O(n\log n)$ time algorithm is given for the case,
$d=1$, of linear products, and $O(n(\log n)^{d+1})$ time approximation
algorithms are given for products with any constant number, $d$, of qualities.
To achieve the latter result, an $O(nk^{d-1})$ bound on the complexity of an
arrangement of homothetic simplices in $\R^d$ is given, where $k$ is the
maximum number of simplices that all contain a single points."
"Reconstructing a finite set of curves from an unordered set of sample points
is a well studied topic. There has been less effort that considers how much
better the reconstruction can be if tangential information is given as well. We
show that if curves are separated from each other by a distance D, then the
sampling rate need only be O(sqrt(D)) for error-free reconstruction. For the
case of point data alone, O(D) sampling is required."
"We present a simple framework to compute hyperbolic Voronoi diagrams of
finite point sets as affine diagrams. We prove that bisectors in Klein's
non-conformal disk model are hyperplanes that can be interpreted as power
bisectors of Euclidean balls. Therefore our method simply consists in computing
an equivalent clipped power diagram followed by a mapping transformation
depending on the selected representation of the hyperbolic space (e.g.,
Poincar\'e conformal disk or upper-plane representations). We discuss on
extensions of this approach to weighted and $k$-order diagrams, and describe
their dual triangulations. Finally, we consider two useful primitives on the
hyperbolic Voronoi diagrams for designing tailored user interfaces of an image
catalog browsing application in the hyperbolic disk: (1) finding nearest
neighbors, and (2) computing smallest enclosing balls."
"We present a simple randomized incremental algorithm for building compressed
quadtrees. The resulting algorithm seems to be simpler than previously known
algorithms for this task."
"We consider the problem of tracking $n$ targets in the plane using $2n$
cameras. We can use two cameras to estimate the location of a target. We are
then interested in forming $n$ camera pairs where each camera belongs to
exactly one pair, followed by forming a matching between the targets and camera
pairs so as to best estimate the locations of each of the targets. We consider
a special case of this problem where each of the cameras are placed along a
horizontal line $l$, and we consider two objective functions which have been
shown to give good estimates of the locations of the targets when the distances
between the targets and the cameras are sufficiently large. In the first
objective, the value of an assignment of a camera pair to a target is the
tracking angle formed by the assignment. Here, we are interested in maximizing
the sum of these tracking angles. A polynomial time 2-approximation is known
for this problem. We give a quasi-polynomial time algorithm that returns a
solution whose value is at least a $(1-\epsilon)$ factor of the value of an
optimal solution for any $\epsilon > 0$. In the second objective, the cost of
an assignment of a camera pair to a target is the ratio of the vertical
distance between the target and $l$ to the horizontal distance between the
cameras in the camera pair. In this setting, we are interested in minimizing
the sum of these ratios. A polynomial time 2-approximation is known for this
problem. We give a quasi-polynomial time algorithm that returns a solution
whose value is at most a $(1+\epsilon)$ factor of the value of an optimal
solution for any $\epsilon > 0$."
"We present a linear programming based algorithm for computing a spanning tree
$T$ of a set $P$ of $n$ points in $\Re^d$, such that its crossing number is
$O(\min(t \log n, n^{1-1/d}))$, where $t$ the minimum crossing number of any
spanning tree of $P$. This is the first guaranteed approximation algorithm for
this problem. We provide a similar approximation algorithm for the more general
settings of building a spanning tree for a set system with bounded \VC
dimension. Our approach is an alternative to the reweighting technique
previously used in computing such spanning trees.
  Our approach is an alternative to the reweighting technique previously used
in computing such spanning trees."
"A set $G$ of points on a 1.5-dimensional terrain, also known as an
$x$-monotone polygonal chain, is said to guard the terrain if any point on the
terrain is 'seen' by a point in $G$. Two points on the terrain see each other
if and only if the line segment between them is never strictly below the
terrain. The minimum terrain guarding problem asks for a minimum guarding set
for the given input terrain. We prove that the decision version of this problem
is NP-hard. This solves a significant open problem and complements recent
positive approximability results for the optimization problem.
  Our proof uses a reduction from PLANAR 3-SAT. We build gadgets capable of
'mirroring' a consistent variable assignment back and forth across a main
valley. The structural simplicity of 1.5-dimensional terrains makes it
difficult to build general clause gadgets that do not destroy this assignment
when they are evaluated. However, we exploit the structure in instances of
PLANAR 3-SAT to find very specific operations involving only 'adjacent'
variables. For these restricted operations we can construct gadgets that allow
a full reduction to work."
"In this note we shall introduce a simple, effective numerical method for
solving partial differential equations for scalar and vector-valued data
defined on surfaces. Even though we shall follow the traditional way to
approximate the regular surfaces under consideration by triangular meshes, the
key idea of our algorithm is to develop an intrinsic and unified way to compute
directly the partial derivatives of functions defined on triangular meshes. We
shall present examples in computer graphics and image processing applications."
"Information visualization is essential in making sense out of large data
sets. Often, high-dimensional data are visualized as a collection of points in
2-dimensional space through dimensionality reduction techniques. However, these
traditional methods often do not capture well the underlying structural
information, clustering, and neighborhoods. In this paper, we describe GMap: a
practical tool for visualizing relational data with geographic-like maps. We
illustrate the effectiveness of this approach with examples from several
domains All the maps referenced in this paper can be found in
http://www.research.att.com/~yifanhu/GMap"
"Two graphs $G_1=(V,E_1)$ and $G_2=(V,E_2)$ admit a geometric simultaneous
embedding if there exists a set of points P and a bijection M: P -> V that
induce planar straight-line embeddings both for $G_1$ and for $G_2$. While it
is known that two caterpillars always admit a geometric simultaneous embedding
and that two trees not always admit one, the question about a tree and a path
is still open and is often regarded as the most prominent open problem in this
area. We answer this question in the negative by providing a counterexample.
Additionally, since the counterexample uses disjoint edge sets for the two
graphs, we also negatively answer another open question, that is, whether it is
possible to simultaneously embed two edge-disjoint trees. As a final result, we
study the same problem when some constraints on the tree are imposed. Namely,
we show that a tree of depth 2 and a path always admit a geometric simultaneous
embedding. In fact, such a strong constraint is not so far from closing the gap
with the instances not admitting any solution, as the tree used in our
counterexample has depth 4."
"This paper studies the geodesic diameter of polygonal domains having h holes
and n corners. For simple polygons (i.e., h = 0), the geodesic diameter is
determined by a pair of corners of a given polygon and can be computed in
linear time, as known by Hershberger and Suri. For general polygonal domains
with h >= 1, however, no algorithm for computing the geodesic diameter was
known prior to this paper. In this paper, we present the first algorithms that
compute the geodesic diameter of a given polygonal domain in worst-case time
O(n^7.73) or O(n^7 (log n + h)). The main difficulty unlike the simple polygon
case relies on the following observation revealed in this paper: two interior
points can determine the geodesic diameter and in that case there exist at
least five distinct shortest paths between the two."
"For a finite set of points $X$ on the unit hypersphere in $\mathbb{R}^d$ we
consider the iteration $u_{i+1}=u_i+\chi_i$, where $\chi_i$ is the point of $X$
farthest from $u_i$. Restricting to the case where the origin is contained in
the convex hull of $X$ we study the maximal length of $u_i$. We give sharp
upper bounds for the length of $u_i$ independently of $X$. Precisely, this
upper bound is infinity for $d\ge 3$ and $\sqrt2$ for $d=2$."
"Let $G$ be a (possibly disconnected) planar subdivision and let $D$ be a
probability measure over $\R^2$. The current paper shows how to preprocess
$(G,D)$ into an O(n) size data structure that can answer planar point location
queries over $G$. The expected query time of this data structure, for a query
point drawn according to $D$, is $O(H+1)$, where $H$ is a lower bound on the
expected query time of any linear decision tree for point location in $G$. This
extends the results of Collette et al (2008, 2009) from connected planar
subdivisions to disconnected planar subdivisions. A version of this structure,
when combined with existing results on succinct point location, provides a
succinct distribution-sensitive point location structure."
"Given a family of k disjoint connected polygonal sites in general position
and of total complexity n, we consider the farthest-site Voronoi diagram of
these sites, where the distance to a site is the distance to a closest point on
it. We show that the complexity of this diagram is O(n), and give an O(n log^3
n) time algorithm to compute it. We also prove a number of structural
properties of this diagram. In particular, a Voronoi region may consist of k-1
connected components, but if one component is bounded, then it is equal to the
entire region."
"We provide an O(log log OPT)-approximation algorithm for the problem of
guarding a simple polygon with guards on the perimeter. We first design a
polynomial-time algorithm for building epsilon-nets of size O(1/epsilon log log
1/epsilon) for the instances of Hitting Set associated with our guarding
problem. We then apply the technique of Bronnimann and Goodrich to build an
approximation algorithm from this epsilon-net finder. Along with a simple
polygon P, our algorithm takes as input a finite set of potential guard
locations that must include the polygon's vertices. If a finite set of
potential guard locations is not specified, e.g. when guards may be placed
anywhere on the perimeter, we use a known discretization technique at the cost
of making the algorithm's running time potentially linear in the ratio between
the longest and shortest distances between vertices. Our algorithm is the first
to improve upon O(log OPT)-approximation algorithms that use generic net
finders for set systems of finite VC-dimension."
"We show that the number of unit-area triangles determined by a set of $n$
points in the plane is $O(n^{9/4+\epsilon})$, for any $\epsilon>0$, improving
the recent bound $O(n^{44/19})$ of Dumitrescu et al."
"We address the point-to-face approximate shortest path problem in R: Given a
set of polyhedral obstacles with a total of n vertices, a source point s, an
obstacle face f, and a real positive parameter epsilon, compute a path from s
to f that avoids the interior of the obstacles and has length at most
(1+epsilon) times the length of the shortest obstacle avoiding path from s to
f. We present three approximation algorithms that take by extending three
well-known ""point-to-point"" shortest path algorithms."
"We discuss two versions of the Fr\'echet distance problem in weighted planar
subdivisions. In the first one, the distance between two points is the weighted
length of the line segment joining the points. In the second one, the distance
between two points is the length of the shortest path between the points. In
both cases, we give algorithms for finding a (1+epsilon)-factor approximation
of the Fr\'echet distance between two polygonal curves. We also consider the
Fr\'echet distance between two polygonal curves among polyhedral obstacles in
R^3 and present a (1+epsilon)-factor approximation algorithm."
"Let B be a centrally symmetric convex polygon of R^2 and || p - q || be the
distance between two points p,q in R^2 in the normed plane whose unit ball is
B. For a set T of n points (terminals) in R^2, a B-Manhattan network on T is a
network N(T) = (V,E) with the property that its edges are parallel to the
directions of B and for every pair of terminals t_i and t_j, the network N(T)
contains a shortest B-path between them, i.e., a path of length || t_i - t_j
||. A minimum B-Manhattan network on T is a B-Manhattan network of minimum
possible length. The problem of finding minimum B-Manhattan networks has been
introduced by Gudmundsson, Levcopoulos, and Narasimhan (APPROX'99) in the case
when the unit ball B is a square (and hence the distance || p - q || is the l_1
or the l_infty-distance between p and q) and it has been shown recently by
Chin, Guo, and Sun (SoCG'09) to be strongly NP-complete. Several approximation
algorithms (with factors 8, 4 ,3 , and 2) for minimum Manhattan problem are
known. In this paper, we propose a factor 2.5 approximation algorithm for
minimum B-Manhattan network problem. The algorithm employs a simplified version
of the strip-staircase decomposition proposed in our paper (APPROX'05) and
subsequently used in other factor 2 approximation algorithms for minimum
Manhattan problem."
"In the point set embeddability problem, we are given a plane graph $G$ with
$n$ vertices and a point set $S$ with $n$ points. Now the goal is to answer the
question whether there exists a straight-line drawing of $G$ such that each
vertex is represented as a distinct point of $S$ as well as to provide an
embedding if one does exist. Recently, in \cite{DBLP:conf/gd/NishatMR10}, a
complete characterization for this problem on a special class of graphs known
as the plane 3-trees was presented along with an efficient algorithm to solve
the problem. In this paper, we use the same characterization to devise an
improved algorithm for the same problem. Much of the efficiency we achieve
comes from clever uses of the triangular range search technique. We also study
a generalized version of the problem and present improved algorithms for this
version of the problem as well."
"Motivated by an open problem from graph drawing, we study several
partitioning problems for line and hyperplane arrangements. We prove a
ham-sandwich cut theorem: given two sets of n lines in R^2, there is a line l
such that in both line sets, for both halfplanes delimited by l, there are
n^{1/2} lines which pairwise intersect in that halfplane, and this bound is
tight; a centerpoint theorem: for any set of n lines there is a point such that
for any halfplane containing that point there are (n/3)^{1/2} of the lines
which pairwise intersect in that halfplane. We generalize those results in
higher dimension and obtain a center transversal theorem, a same-type lemma,
and a positive portion Erdos-Szekeres theorem for hyperplane arrangements. This
is done by formulating a generalization of the center transversal theorem which
applies to set functions that are much more general than measures. Back to
Graph Drawing (and in the plane), we completely solve the open problem that
motivated our search: there is no set of n labelled lines that are universal
for all n-vertex labelled planar graphs. As a side note, we prove that every
set of n (unlabelled) lines is universal for all n-vertex (unlabelled) planar
graphs."
"This paper considers the problem of finding a quickest path between two
points in the Euclidean plane in the presence of a transportation network. A
transportation network consists of a planar network where each road (edge) has
an individual speed. A traveller may enter and exit the network at any point on
the roads. Along any road the traveller moves with a fixed speed depending on
the road, and outside the network the traveller moves at unit speed in any
direction. We give an exact algorithm for the basic version of the problem:
given a transportation network of total complexity n in the Euclidean plane, a
source point s and a destination point t, and the quickest path between s and
t. We also show how the transportation network can be preprocessed in time
O(n^2 log n) into a data structure of size O(n^2) such that (1 +
\epsilon)-approximate cheapest path cost queries between any two points in the
plane can be answered in time O(1\epsilon^4 log n)."
"Given two sets of points in the plane, $P$ of $n$ terminals and $S$ of $m$
Steiner points, a Steiner tree of $P$ is a tree spanning all points of $P$ and
some (or none or all) points of $S$. A Steiner tree with length of longest edge
minimized is called a bottleneck Steiner tree. In this paper, we study the
Euclidean bottleneck Steiner tree problem: given two sets, $P$ and $S$, and a
positive integer $k \le m$, find a bottleneck Steiner tree of $P$ with at most
$k$ Steiner points. The problem has application in the design of wireless
communication networks.
  We first show that the problem is NP-hard and cannot be approximated within
factor $\sqrt{2}$, unless $P=NP$. Then, we present a polynomial-time
approximation algorithm with performance ratio 2."
"Treemaps are a popular technique to visualize hierarchical data. The input is
a weighted tree $\tree$ where the weight of each node is the sum of the weights
of its children. A treemap for $\tree$ is a hierarchical partition of a
rectangle into simply connected regions, usually rectangles. Each region
represents a node of $\tree$ and its area is proportional to the weight of the
corresponding node. An important quality criterion for treemaps is the aspect
ratio of its regions. One cannot bound the aspect ratio if the regions are
restricted to be rectangles. In contrast, \emph{polygonal partitions}, that use
convex polygons, have bounded aspect ratio. We are the first to obtain convex
partitions with optimal aspect ratio $O(\depth(\tree))$. However,
$\depth(\tree)$ still depends on the input tree. Hence we introduce a new type
of treemaps, namely \emph{orthoconvex treemaps}, where regions representing
leaves are rectangles, L-, and S-shapes, and regions representing internal
nodes are orthoconvex polygons. We prove that any input tree, irrespective of
the weights of the nodes and the depth of the tree, admits an orthoconvex
treemap of constant aspect ratio. We also obtain several specialized results
for single-level treemaps, that is, treemaps where the input tree has depth~1."
"Let P be a set of n points in R^3. The 2-center problem for P is to find two
congruent balls of minimum radius whose union covers P. We present two
randomized algorithms for computing a 2-center of P. The first algorithm runs
in O(n^3 log^5 n) expected time, and the second algorithm runs in O((n^2 log^5
n) /(1-r*/r_0)^3) expected time, where r* is the radius of the 2-center balls
of P and r_0 is the radius of the smallest enclosing ball of P. The second
algorithm is faster than the first one as long as r* is not too close to r_0,
which is equivalent to the condition that the centers of the two covering balls
be not too close to each other."
"We consider the problem of finding a door along a wall with a blind robot
that neither knows the distance to the door nor the direction towards of the
door. This problem can be solved with the well-known doubling strategy yielding
an optimal competitive factor of 9 with the assumption that the robot does not
make any errors during its movements. We study the case that the robot's
movement is erroneous. In this case the doubling strategy is no longer optimal.
We present optimal competitive strategies that take the error assumption into
account. The analysis technique can be applied to different error models."
"We present approximation algorithms for maximum independent set of
pseudo-disks in the plane, both in the weighted and unweighted cases. For the
unweighted case, we prove that a local search algorithm yields a \PTAS. For the
weighted case, we suggest a novel rounding scheme based on an \LP relaxation of
the problem, which leads to a constant-factor approximation. Most previous
algorithms for maximum independent set (in geometric settings) relied on
packing arguments that are not applicable in this case. As such, the analysis
of both algorithms requires some new combinatorial ideas, which we believe to
be of independent interest."
"Computing the Fr\'{e}chet distance for surfaces is a surprisingly hard
problem and the only known algorithm is limited to computing it between flat
surfaces. We adapt this algorithm to create one for computing the Fr\'{e}chet
distance for a class of non-flat surfaces which we call folded polygons.
Unfortunately, the original algorithm cannot be extended directly. We present
three different methods to adapt it. The first of which is a fixed-parameter
tractable algorithm. The second is a polynomial-time approximation algorithm.
Finally, we present a restricted class of folded polygons for which we can
compute the Fr\'{e}chet distance in polynomial time."
"Improving the best known examples, two planar straight-line graphs which
cause the non-termination of Ruppert's algorithm for a minimum angle threshold
as low as 29.06 degrees are given."
"Given a finite set S of points in the plane and a real value d > 0, the
d-radius disk graph G^d contains all edges connecting pairs of points in S that
are within distance d of each other. For a given graph G with vertex set S, the
Yao subgraph Y_k[G] with integer parameter k > 0 contains, for each point p in
S, a shortest edge pq from G (if any) in each of the k sectors defined by k
equally-spaced rays with origin p. Motivated by communication issues in mobile
networks with directional antennas, we study the connectivity properties of
Y_k[G^d], for small values of k and d. In particular, we derive lower and upper
bounds on the minimum radius d that renders Y_k[G^d] connected, relative to the
unit radius assumed to render G^d connected. We show that d=sqrt(2) is
necessary and sufficient for the connectivity of Y_4[G^d]. We also show that,
for d <= ~1.056, the graph Y_3[G^d] can be disconnected, but for d >=
2/sqrt(3), Y_3[G^d] is always connected. Finally, we show that Y_2[G^d] can be
disconnected, for any d >= 1."
"Let $S$ be a finite set of points in the Euclidean plane. Let $D$ be a
Delaunay triangulation of $S$. The {\em stretch factor} (also known as {\em
dilation} or {\em spanning ratio}) of $D$ is the maximum ratio, among all
points $p$ and $q$ in $S$, of the shortest path distance from $p$ to $q$ in $D$
over the Euclidean distance $||pq||$. Proving a tight bound on the stretch
factor of the Delaunay triangulation has been a long standing open problem in
computational geometry.
  In this paper we prove that the stretch factor of the Delaunay triangulation
of a set of points in the plane is less than $\rho = 1.998$, improving the
previous best upper bound of 2.42 by Keil and Gutwin (1989). Our bound 1.998 is
better than the current upper bound of 2.33 for the special case when the point
set is in convex position by Cui, Kanj and Xia (2009). This upper bound breaks
the barrier 2, which is significant because previously no family of plane
graphs was known to have a stretch factor guaranteed to be less than 2 on any
set of points."
"Discrepancy measures how uniformly distributed a point set is with respect to
a given set of ranges. There are two notions of discrepancy, namely continuous
discrepancy and combinatorial discrepancy. Depending on the ranges, several
possible variants arise, for example star discrepancy, box discrepancy, and
discrepancy of half-spaces. In this paper, we investigate the hardness of these
problems with respect to the dimension d of the underlying space.
  All these problems are solvable in time {n^O(d)}, but such a time dependency
quickly becomes intractable for high-dimensional data. Thus it is interesting
to ask whether the dependency on d can be moderated.
  We answer this question negatively by proving that the canonical decision
problems are W[1]-hard with respect to the dimension. This is done via a
parameterized reduction from the Clique problem. As the parameter stays linear
in the input parameter, the results moreover imply that these problems require
{n^\Omega(d)} time, unless 3-Sat can be solved in {2^o(n)} time.
  Further, we derive that testing whether a given set is an {\epsilon}-net with
respect to half-spaces takes {n^\Omega(d)} time under the same assumption. As
intermediate results, we discover the W[1]-hardness of other well known
problems, such as determining the largest empty star inside the unit cube. For
this, we show that it is even hard to approximate within a factor of {2^n}."
"We provide a simple proof of the existence of a planar separator by showing
that it is an easy consequence of the circle packing theorem. We also reprove
other results on separators, including:
  (A) There is a simple cycle separator if the planar graph is triangulated.
Furthermore, if each face has at most $d$ edges on its boundary, then there is
a cycle separator of size O(sqrt{d n}).
  (B) For a set of n balls in R^d, that are k-ply, there is a separator, in the
intersection graph of the balls, of size O(k^{1/d}n^{1-1/d}).
  (C) The k nearest neighbor graph of a set of n points in R^d contains a
separator of size O(k^{1/d} n^{1-1/d}).
  The new proofs are (arguably) significantly simpler than previous proofs."
"We study the online problem of assigning a moving point to a base-station
region that contains it. For instance, the moving object could represent a
cellular phone and the base station could represent the coverage zones of cell
towers. Our goal is to minimize the number of handovers that occur when the
point moves outside its assigned region and must be assigned to a new region.
We study this problem in terms of competitive analysis and we measure the
competitive ratio of our algorithms as a function of the ply of the system of
regions, that is, the maximum number of regions that cover any single point. In
the offline version of this problem, when object motions are known in advance,
a simple greedy strategy suffices to determine an optimal assignment of objects
to base stations, with as few handovers as possible. For the online version of
this problem for moving points in one dimension, we present a deterministic
algorithm that achieves a competitive ratio of O(log ply) with respect to the
optimal algorithm, and we show that no better ratio is possible. For two or
more dimensions, we present a randomized online algorithm that achieves a
competitive ratio of O(log ply) with respect to the optimal algorithm, and a
deterministic algorithm that achieves a competitive ratio of O(ply); again, we
show that no better ratio is possible."
"Rectangular layouts, subdivisions of an outer rectangle into smaller
rectangles, have many applications in visualizing spatial information, for
instance in rectangular cartograms in which the rectangles represent geographic
or political regions. A spatial treemap is a rectangular layout with a
hierarchical structure: the outer rectangle is subdivided into rectangles that
are in turn subdivided into smaller rectangles. We describe algorithms for
transforming a rectangular layout that does not have this hierarchical
structure, together with a clustering of the rectangles of the layout, into a
spatial treemap that respects the clustering and also respects to the extent
possible the adjacencies of the input layout."
"We revisit a new type of a Voronoi diagram, in which distance is measured
from a point to a pair of points. We consider a few more such distance
functions, based on geometric primitives, and analyze the structure and
complexity of the nearest- and furthest-neighbor Voronoi diagrams of a point
set with respect to these distance functions."
"For the analysis of systems consisting of small, regular objects, the methods
of mathematical morphology applied to images of these systems are well-suited.
One of these methods is the use of Voronoi polygons. It was found that the
Voronoi tessellation method represents a powerful tool for the analysis of thin
film morphology and provides nanostructural information to many multi-particle
assemblies. In these notes, several morphological algorithms are analyzed and
we study how to join all of them to design a graphical user interface (GUI)
that provides as input for the system the ""AFM image"" and interprets the output
of the system in terms of errors and generators coordinates."
"In this paper, we review a method for computing and parameterizing the set of
homotopy classes of chain maps between two chain complexes. This is then
applied to finding topologically meaningful maps between simplicial complexes,
which in the context of topological data analysis, can be viewed as an
extension of conventional unsupervised learning methods to simplicial
complexes."
"In the bidirected minimum Manhattan network problem, given a set T of n
terminals in the plane, we need to construct a network N(T) of minimum total
length with the property that the edges of N(T) are axis-parallel and oriented
in a such a way that every ordered pair of terminals is connected in N(T) by a
directed Manhattan path. In this paper, we present a polynomial factor 2
approximation algorithm for the bidirected minimum Manhattan network problem."
"The similarity of two polygonal curves can be measured using the Fr\'echet
distance. We introduce the notion of a more robust Fr\'echet distance, where
one is allowed to shortcut between vertices of one of the curves. This is a
natural approach for handling noise, in particular batched outliers. We compute
a (3+\eps)-approximation to the minimum Fr\'echet distance over all possible
such shortcuts, in near linear time, if the curve is c-packed and the number of
shortcuts is either small or unbounded.
  To facilitate the new algorithm we develop several new tools:
  (A) A data structure for preprocessing a curve (not necessarily c-packed)
that supports (1+\eps)-approximate Fr\'echet distance queries between a
subcurve (of the original curve) and a line segment.
  (B) A near linear time algorithm that computes a permutation of the vertices
of a curve, such that any prefix of 2k-1 vertices of this permutation, form an
optimal approximation (up to a constant factor) to the original curve compared
to any polygonal curve with k vertices, for any k > 0.
  (C) A data structure for preprocessing a curve that supports approximate
Fr\'echet distance queries between a subcurve and query polygonal curve. The
query time depends quadratically on the complexity of the query curve, and only
(roughly) logarithmically on the complexity of the original curve.
  To our knowledge, these are the first data structures to support these kind
of queries efficiently."
"We study the problem of discrete geometric packing. Here, given weighted
regions (say in the plane) and points (with capacities), one has to pick a
maximum weight subset of the regions such that no point is covered more than
its capacity. We provide a general framework and an algorithm for approximating
the optimal solution for packing in hypergraphs arising out of such geometric
settings. Using this framework we get a flotilla of results on this problem
(and also on its dual, where one wants to pick a maximum weight subset of the
points when the regions have capacities). For example, for the case of fat
triangles of similar size, we show an O(1)-approximation and prove that no
\PTAS is possible."
"In this paper we study the most-demanding predicate for computing the
Euclidean Voronoi diagram of axes-aligned line segments, namely the Incircle
predicate. Our contribution is two-fold: firstly, we describe, in algorithmic
terms, how to compute the Incircle predicate for axes-aligned line segments,
and secondly we compute its algebraic degree. Our primary aim is to minimize
the algebraic degree, while, at the same time, taking into account the amount
of operations needed to compute our predicate of interest.
  In our predicate analysis we show that the Incircle predicate can be answered
by evaluating the signs of algebraic expressions of degree at most 6; this is
half the algebraic degree we get when we evaluate the Incircle predicate using
the current state-of-the-art approach. In the most demanding cases of our
predicate evaluation, we reduce the problem of answering the Incircle predicate
to the problem of computing the sign of the value of a linear polynomial (in
one variable), when evaluated at a known specific root of a quadratic
polynomial (again in one variable). Another important aspect of our approach is
that, from a geometric point of view, we answer the most difficult case of the
predicate via implicitly performing point locations on an appropriately defined
subdivision of the place induced by the Voronoi circle implicated in the
Incircle predicate."
"We study the computation of the flow of water on imprecise terrains. We
consider two approaches to modeling flow on a terrain: one where water flows
across the surface of a polyhedral terrain in the direction of steepest
descent, and one where water only flows along the edges of a predefined graph,
for example a grid or a triangulation. In both cases each vertex has an
imprecise elevation, given by an interval of possible values, while its
(x,y)-coordinates are fixed. For the first model, we show that the problem of
deciding whether one vertex may be contained in the watershed of another is
NP-hard. In contrast, for the second model we give a simple O(n log n) time
algorithm to compute the minimal and the maximal watershed of a vertex, where n
is the number of edges of the graph. On a grid model, we can compute the same
in O(n) time."
"Divide and Conquer is a well known algorithmic procedure for solving many
kinds of problem. In this procedure, the problem is partitioned into two parts
until the problem is trivially solvable. Finding the distance of the closest
pair is an interesting topic in computer science. With divide and conquer
algorithm we can solve closest pair problem. Here also the problem is
partitioned into two parts until the problem is trivially solvable. But it is
theoretically and practically observed that sometimes partitioning the problem
space into more than two parts can give better performances. In this paper, a
new proposal is given that dividing the problem space into (n) number of parts
can give better result while divide and conquer algorithm is used for solving
the closest pair of point's problem."
"A new class of geometric query problems are studied in this paper. We are
required to preprocess a set of geometric objects $P$ in the plane, so that for
any arbitrary query point $q$, the largest circle that contains $q$ but does
not contain any member of $P$, can be reported efficiently. The geometric sets
that we consider are point sets and boundaries of simple polygons."
"In this paper we present several results on the expected complexity of a
convex hull of $n$ points chosen uniformly and independently from a convex
shape.
  (i) We show that the expected number of vertices of the convex hull of $n$
points, chosen uniformly and independently from a disk is $O(n^{1/3})$, and
$O(k \log{n})$ for the case a convex polygon with $k$ sides. Those results are
well known (see \cite{rs-udkhv-63,r-slcdn-70,ps-cgi-85}), but we believe that
the elementary proof given here are simpler and more intuitive.
  (ii) Let $\D$ be a set of directions in the plane, we define a generalized
notion of convexity induced by $\D$, which extends both rectilinear convexity
and standard convexity.
  We prove that the expected complexity of the $\D$-convex hull of a set of $n$
points, chosen uniformly and independently from a disk, is $O(n^{1/3} +
\sqrt{n\alpha(\D)})$, where $\alpha(\D)$ is the largest angle between two
consecutive vectors in $\D$. This result extends the known bounds for the cases
of rectilinear and standard convexity.
  (iii) Let $\B$ be an axis parallel hypercube in $\Re^d$. We prove that the
expected number of points on the boundary of the quadrant hull of a set $S$ of
$n$ points, chosen uniformly and independently from $\B$ is $O(\log^{d-1}n)$.
Quadrant hull of a set of points is an extension of rectilinear convexity to
higher dimensions. In particular, this number is larger than the number of
maxima in $S$, and is also larger than the number of points of $S$ that are
vertices of the convex hull of $S$.
  Those bounds are known \cite{bkst-anmsv-78}, but we believe the new proof is
simpler."
"Recently, simple conditions for well-behaved-ness of anisotropic Voronoi
diagrams have been proposed. While these conditions ensure well-behaved-ness of
two types of practical anisotropic Voronoi diagrams, as well as the
geodesic-distance one, in any dimension, they are both prohibitively expensive
to evaluate, and not well-suited for typical problems in approximation or
optimization. We propose simple conditions that can be efficiently evaluated,
and are better suited to practical problems of approximation and optimization.
The practical utility of this analysis is enhanced by the fact that orphan-free
anisotropic Voronoi diagrams have embedded triangulations as duals."
"Let $P$ be a set of $2n$ points in the plane, and let $M_{\rm C}$ (resp.,
$M_{\rm NC}$) denote a bottleneck matching (resp., a bottleneck non-crossing
matching) of $P$. We study the problem of computing $M_{\rm NC}$. We first
prove that the problem is NP-hard and does not admit a PTAS. Then, we present
an $O(n^{1.5}\log^{0.5} n)$-time algorithm that computes a non-crossing
matching $M$ of $P$, such that $bn(M) \le 2\sqrt{10} \cdot bn(M_{\rm NC})$,
where $bn(M)$ is the length of a longest edge in $M$. An interesting
implication of our construction is that $bn(M_{\rm NC})/bn(M_{\rm C}) \le
2\sqrt{10}$."
"In this paper, we consider a facility location problem to find a minimum-sum
coverage of n points by disks centered at a fixed line. The cost of a disk with
radius r has a form of a non-decreasing function f(r) = r^a for any a >= 1. The
goal is to find a set of disks under Lp metric such that the disks are centered
on the x-axis, their union covers n points, and the sum of the cost of the
disks is minimized. Alt et al. [1] presented an algorithm in O(n^4 log n) time
for any a > 1 under any Lp metric. We present a faster algorithm for this
problem in O(n^2 log n) time for any a > 1 and any Lp metric."
"In this paper we determine the stretch factor of the $L_1$-Delaunay and
$L_\infty$-Delaunay triangulations, and we show that this stretch is
$\sqrt{4+2\sqrt{2}} \approx 2.61$. Between any two points $x,y$ of such
triangulations, we construct a path whose length is no more than
$\sqrt{4+2\sqrt{2}}$ times the Euclidean distance between $x$ and $y$, and this
bound is best possible. This definitively improves the 25-year old bound of
$\sqrt{10}$ by Chew (SoCG '86). To the best of our knowledge, this is the first
time the stretch factor of the well-studied $L_p$-Delaunay triangulations, for
any real $p\ge 1$, is determined exactly."
"Given two simplicial complexes in R^d, and start and end vertices in each
complex, we show how to compute curves (in each complex) between these
vertices, such that the Fr\'echet distance between these curves is minimized.
As a polygonal curve is a complex, this generalizes the regular notion of weak
Fr\'echet distance between curves. We also generalize the algorithm to handle
an input of k simplicial complexes.
  Using this new algorithm we can solve a slew of new problems, from computing
a mean curve for a given collection of curves, to various motion planning
problems. Additionally, we show that for the mean curve problem, when the k
input curves are c-packed, one can (1+epsilon)-approximate the mean curve in
near linear time, for fixed k and epsilon.
  Additionally, we present an algorithm for computing the strong Fr\'echet
distance between two curves, which is simpler than previous algorithms, and
avoids using parametric search."
"For a set $R$ of $n$ red points and a set $B$ of $n$ blue points, a
$BR$-matching is a non-crossing geometric perfect matching where each segment
has one endpoint in $B$ and one in $R$. Two $BR$-matchings are compatible if
their union is also non-crossing. We prove that, for any two distinct
$BR$-matchings $M$ and $M'$, there exists a sequence of $BR$-matchings $M =
M_1, ..., M_k = M'$ such that $M_{i-1} $ is compatible with $M_i$. This implies
the connectivity of the compatible bichromatic matching graph containing one
node for each bichromatic matching and an edge joining each pair of compatible
matchings, thereby answering the open problem posed by Aichholzer et al. in
""Compatible matchings for bichromatic plane straight-line graphs"""
"With a large number of software tools dedicated to the visualisation and/or
demonstration of properties of geometric constructions and also with the
emerging of repositories of geometric constructions, there is a strong need of
linking them, and making them and their corpora, widely usable. A common
setting for interoperable interactive geometry was already proposed, the i2g
format, but, in this format, the conjectures and proofs counterparts are
missing. A common format capable of linking all the tools in the field of
geometry is missing. In this paper an extension of the i2g format is proposed,
this extension is capable of describing not only the geometric constructions
but also the geometric conjectures. The integration of this format into the
Web-based GeoThms, TGTP and Web Geometry Laboratory systems is also discussed."
"A rectangular partition is the partition of an (axis-aligned) rectangle into
interior-disjoint rectangles. We ask whether a rectangular partition permits a
""nice"" drawing of its dual, that is, a straight-line embedding of it such that
each dual vertex is placed into the rectangle that it represents. We show that
deciding whether such a drawing exists is NP-complete. Moreover, we consider
the drawing where a vertex is placed in the center of the represented rectangle
and consider sufficient conditions for this drawing to be nice. This question
is studied both in the plane and for the higher-dimensional generalization of
rectangular partitions."
"Determining if a point is in a polygon or not is used by a lot of
applications in computer graphics, computer games and geoinformatics.
Implementing this check is error-prone since there are many special cases to be
considered. This holds true in particular for complex polygons whose edges
intersect each other creating holes. In this paper we present a simple even-odd
algorithm to solve this problem for complex polygons in linear time and prove
its correctness for all possible points and polygons. We furthermore provide
examples and implementation notes for this algorithm."
"We describe a family of quadrilateral meshes based on diamonds, rhombi with
60 and 120 degree angles, and kites with 60, 90, and 120 degree angles, that
can be adapted to a local size function by local subdivision operations. Our
meshes use a number of elements that is within a constant factor of the minimum
possible for any mesh of bounded aspect ratio elements, graded by the same
local size function, and is invariant under Laplacian smoothing. The vertices
of our meshes form the centers of the circles in a pair of dual circle
packings. The same vertex placement algorithm but a different mesh topology
gives a pair of dual well-centered meshes adapted to the given size function."
"Let T be a triangulation of a simple polygon. A flip in T is the operation of
removing one diagonal of T and adding a different one such that the resulting
graph is again a triangulation. The flip distance between two triangulations is
the smallest number of flips required to transform one triangulation into the
other. For the special case of convex polygons, the problem of determining the
shortest flip distance between two triangulations is equivalent to determining
the rotation distance between two binary trees, a central problem which is
still open after over 25 years of intensive study. We show that computing the
flip distance between two triangulations of a simple polygon is NP-complete.
This complements a recent result that shows APX-hardness of determining the
flip distance between two triangulations of a planar point set."
"Given a set P of points in the plane, an Euclidean t-spanner for P is a
geometric graph that preserves the Euclidean distances between every pair of
points in P up to a constant factor t. The weight of a geometric graph refers
to the total length of its edges. In this paper we show that the problem of
deciding whether there exists an Euclidean t-spanner, for a given set of points
in the plane, of weight at most w is NP-hard for every real constant t > 1,
both whether planarity of the t-spanner is required or not."
"Recently, a new way of avoiding crossings in straight-line drawings of
non-planar graphs has been investigated. The idea of partial edge drawings
(PED) is to drop the middle part of edges and rely on the remaining edge parts
called stubs. We focus on a symmetric model (SPED) that requires the two stubs
of an edge to be of equal length. In this way, the stub at the other endpoint
of an edge assures the viewer of the edge's existence. We also consider an
additional homogeneity constraint that forces the stub lengths to be a given
fraction $\delta$ of the edge lengths ($\delta$-SHPED). Given length and
direction of a stub, this model helps to infer the position of the opposite
stub.
  We show that, for a fixed stub--edge length ratio $\delta$, not all graphs
have a $\delta$-SHPED. Specifically, we show that $K_{241}$ does not have a
1/4-SHPED, while bandwidth-$k$ graphs always have a $\Theta(1/\sqrt{k})$-SHPED.
We also give bounds for complete bipartite graphs. Further, we consider the
problem \textsc{MaxSPED} where the task is to compute the SPED of maximum total
stub length that a given straight-line drawing contains. We present an
efficient solution for 2-planar drawings and a 2-approximation algorithm for
the dual problem."
"Given a set of line segments in the plane, not necessarily finite, what is a
convex region of smallest area that contains a translate of each input segment?
This question can be seen as a generalization of Kakeya's problem of finding a
convex region of smallest area such that a needle can be rotated through 360
degrees within this region. We show that there is always an optimal region that
is a triangle, and we give an optimal \Theta(n log n)-time algorithm to compute
such a triangle for a given set of n segments. We also show that, if the goal
is to minimize the perimeter of the region instead of its area, then placing
the segments with their midpoint at the origin and taking their convex hull
results in an optimal solution. Finally, we show that for any compact convex
figure G, the smallest enclosing disk of G is a smallest-perimeter region
containing a translate of every rotated copy of G."
"Given a set ${\cal D}$ of unit disks in the Euclidean plane, we consider (i)
the {\it discrete unit disk cover} (DUDC) problem and (ii) the {\it rectangular
region cover} (RRC) problem. In the DUDC problem, for a given set ${\cal P}$ of
points the objective is to select minimum cardinality subset ${\cal D}^*
\subseteq {\cal D}$ such that each point in ${\cal P}$ is covered by at least
one disk in ${\cal D}^*$. On the other hand, in the RRC problem the objective
is to select minimum cardinality subset ${\cal D}^{**} \subseteq {\cal D}$ such
that each point of a given rectangular region ${\cal R}$ is covered by a disk
in ${\cal D}^{**}$. For the DUDC problem, we propose an $(9+\epsilon)$-factor
($0 < \epsilon \leq 6$) approximation algorithm. The previous best known
approximation factor was 15 \cite{FL12}. For the RRC problem, we propose (i) an
$(9 + \epsilon)$-factor ($0 < \epsilon \leq 6$) approximation algorithm, (ii)
an 2.25-factor approximation algorithm in reduce radius setup, improving
previous 4-factor approximation result in the same setup \cite{FKKLS07}.
  The solution of DUDC problem is based on a PTAS for the subproblem LSDUDC,
where all the points in ${\cal P}$ are on one side of a line and covered by the
disks centered on the other side of that line."
"A set P of points in R^2 is n-universal, if every planar graph on n vertices
admits a plane straight-line embedding on P. Answering a question by Kobourov,
we show that there is no n-universal point set of size n, for any n>=15.
Conversely, we use a computer program to show that there exist universal point
sets for all n<=10 and to enumerate all corresponding order types. Finally, we
describe a collection G of 7'393 planar graphs on 35 vertices that do not admit
a simultaneous geometric embedding without mapping, that is, no set of 35
points in the plane supports a plane straight-line embedding of all graphs in
G."
"The local search framework for obtaining PTASs for NP-hard geometric
optimization problems was introduced, independently, by Chan and Har-Peled
(2009) and Mustafa and Ray (2010). In this paper, we generalize the framework
by extending its analysis to additional families of graphs, beyond the family
of planar graphs. We then present several applications of the generalized
framework, some of which are very different from those presented to date (using
the original framework). These applications include PTASs for finding a maximum
l-shallow set of a set of fat objects, for finding a maximum triangle matching
in an l-shallow unit disk graph, and for vertex-guarding a
(not-necessarily-simple) polygon under an appropriate shallowness assumption.
  We also present a PTAS (using the original framework) for the important
problem where one has to find a minimum-cardinality subset of a given set of
disks (of varying radii) that covers a given set of points, and apply it to a
class cover problem (studied in Bereg et al., 2012) to obtain an improved
solution."
"Given a small polygon S, a big simple polygon B and a positive integer k, it
is shown to be NP-hard to determine whether k copies of the small polygon
(allowing translation and rotation) can be placed in the big polygon without
overlap. Previous NP-hardness results were only known in the case where the big
polygon is allowed to be non-simple. A novel reduction from Planar-Circuit-SAT
is presented where a small polygon is constructed to encode the entire circuit."
"Motivated by an application in cell biology, we consider spatial sorting
processes defined by particles moving from an initial to a final configuration.
We describe an algorithm for constructing a cell complex in space-time, called
the medusa, that measures topological properties of the sorting process. The
algorithm requires an extension of the kinetic data structures framework from
Delaunay triangulations to fixed-radius alpha complexes. We report on several
techniques to accelerate the computation."
"We present robust algorithms for set operations and Euclidean transformations
of curved shapes in the plane using approximate geometric primitives. We use a
refinement algorithm to ensure consistency. Its computational complexity is
$\bigo(n\log n+k)$ for an input of size $n$ with $k=\bigo(n^2)$ consistency
violations. The output is as accurate as the geometric primitives. We validate
our algorithms in floating point using sequences of six set operations and
Euclidean transforms on shapes bounded by curves of algebraic degree~1 to~6. We
test generic and degenerate inputs.
  Keywords: robust computational geometry, plane subdivisions, set operations."
"We present the plane-sweep incremental algorithm, a hybrid approach for
computing Delaunay tessellations of large point sets whose size exceeds the
computer's main memory. This approach unites the simplicity of the incremental
algorithms with the comparatively low memory requirements of plane-sweep
approaches. The procedure is to first sort the point set along the first
principal component and then to sequentially insert the points into the
tessellation, essentially simulating a sweeping plane. The part of the
tessellation that has been passed by the sweeping plane can be evicted from
memory and written to disk, limiting the memory requirement of the program to
the ""thickness"" of the data set along its first principal component. We
implemented the algorithm and used it to compute the Delaunay tessellation and
Voronoi partition of the Sloan Digital Sky Survey magnitude space consisting of
287 million points."
"We consider the \emph{smallest superpolyomino problem}: given a set of
colored polyominoes, find the smallest polyomino containing each input
polyomino as a subshape. This problem is shown to be NP-hard, even when
restricted to a set of polyominoes using a single common color. Moreover, for
sets of polyominoes using two or more colors, the problem is shown to be
NP-hard to approximate within a $O(n^{1/3-\varepsilon})$-factor for any
$\varepsilon > 0$."
"Let $S$ be a set of $n$ points in 3-dimensional space. A tetrahedralization
$\mathcal{T}$ of $S$ is a set of interior disjoint tetrahedra with vertices on
$S$, not containing points of $S$ in their interior, and such that their union
is the convex hull of $S$. Given $\mathcal{T}$, $D_\mathcal{T}$ is defined as
the graph having as vertex set the tetrahedra of $\mathcal{T}$, two of which
are adjacent if they share a face. We say that $\mathcal{T}$ is Hamiltonian if
$D_\mathcal{T}$ has a Hamiltonian path. Let $m$ be the number of convex hull
vertices of $S$. We prove that by adding at most $\lfloor \frac{m-2}{2}
\rfloor$ Steiner points to interior of the convex hull of $S$, we can obtain a
point set that admits a Hamiltonian tetrahedralization. An $O(m^{3/2}) + O(n
\log n)$ time algorithm to obtain these points is given. We also show that all
point sets with at most 20 convex hull points admit a Hamiltonian
tetrahedralization without the addition of any Steiner points. Finally we
exhibit a set of 84 points that does not admit a Hamiltonian tetrahedralization
in which all tetrahedra share a vertex."
"By defining grids as graphs, geometric graphs can be represented in a very
concise way."
"We introduce a fully written programmed code with a supervised method for
generating Steiner trees. Our choice of the programming language, and the use
of well-known theorems from Geometry and Complex Analysis, allowed this method
to be implemented with only 747 lines of effective source code. This eases the
understanding and the handling of this beta version for future developments."
"The Opaque Cover Problem (OCP), also known as the Beam Detector Problem, is
the problem of finding, for a set S in Euclidean space, the minimum-length set
F which intersects every straight line passing through S. In spite of its
simplicity, the problem remains remarkably intractable. The aim of this paper
is to establish a framework and fundamental results for minimum opaque covers
where S is a polygonal region in two-dimensional space. We begin by giving some
general results about opaque covers, and describe the close connection that the
OCP has with the Point Goalie Problem. We then consider properties of graphical
solutions to the OCP when S is a convex polygonal region in the plane."
"We show that in the Klein projective ball model of hyperbolic space, the
hyperbolic Voronoi diagram is affine and amounts to clip a corresponding power
diagram, requiring however algebraic arithmetic. By considering the
lesser-known Beltrami hemisphere model of hyperbolic geometry, we overcome the
arithmetic limitations of Klein construction. Finally, we characterize the
bisectors and geodesics in the other Poincar\' e upper half-space, the
Poincar\'e ball, and the Lorentz hyperboloid models, and discusses on
degenerate cases for which the dual hyperbolic Delaunay complex is not a
triangulation."
"Given a set of points in the plane, we show that the $\theta$-graph with 5
cones is a geometric spanner with spanning ratio at most $\sqrt{50 + 22
\sqrt{5}} \approx 9.960$. This is the first constant upper bound on the
spanning ratio of this graph. The upper bound uses a constructive argument that
gives a (possibly self-intersecting) path between any two vertices, of length
at most $\sqrt{50 + 22 \sqrt{5}}$ times the Euclidean distance between the
vertices. We also give a lower bound on the spanning ratio of
$\frac{1}{2}(11\sqrt{5} -17) \approx 3.798$."
"We present improved upper bounds for the size of relative
(p,Epsilon)-approximation for range spaces with the following property: For any
(finite) range space projected onto (that is, restricted to) a ground set of
size n and for any parameter 1 <= k <= n, the number of ranges of size at most
k is only nearly-linear in n and polynomial in k. Such range spaces are called
""well behaved"". Our bound is an improvement over the bound O(\log{(1/p)/\eps^2
p) introduced by Li etal. for the general case (where this bound has been shown
to be tight in the worst case), when p << Epsilon. We also show that such small
size relative (p,Epsilon)-approximations can be constructed in expected
polynomial time.
  Our bound also has an interesting interpretation in the context of ""p-nets"":
As observed by Har-Peled and Sharir, p-nets are special cases of relative
(p,Epsilon)-approximations. Specifically, when Epsilon is a constant smaller
than 1, their analysis implies that there are p-nets of size O(\log{(1/p)}/p)
that are \emph{also} relative approximations. In this context our construction
significantly improves this bound for well-behaved range spaces. Despite the
progress in the theory of p-nets and the existence of improved bounds
corresponding to the cases that we study, these bounds do not necessarily
guarantee a bounded relative error.
  Lastly, we present several geometric scenarios of well-behaved range spaces,
and show the resulting bound for each of these cases obtained as a consequence
of our analysis. In particular, when Epsilon is a constant smaller than 1, our
bound for points and axis-parallel boxes in two and three dimensions, as well
as points and ""fat"" triangles in the plane, matches the optimal bound for
p-nets."
"We prove that for any point set P in the plane, a triangle T, and a positive
integer k, there exists a coloring of P with k colors such that any homothetic
copy of T containing at least ck^8 points of P, for some constant c, contains
at least one of each color. This is the first polynomial bound for range spaces
induced by homothetic polygons. The only previously known bound for this
problem applies to the more general case of octants in R^3, but is doubly
exponential."
"We present a novel approach named TBase for smoothing planar and surface
quadrilateral meshes. Our motivation is that the best shape of quadrilateral
element (square) can be virtually divided into a pair of equilateral right
triangles by any of its diagonals. When move a node to smooth a quadrilateral,
it is optimal to make a pair of triangles divided by a diagonal be equilateral
right triangles separately. The finally smoothed position is obtained by
weighting all individual optimal positions. Three variants are produced
according to the determination of weights. Tests by the TBase are given and
compared with Laplacian smoothing: The Vari.1 of TBase is effectively identical
to Laplacian smoothing for planar quad meshes, while Vari.2 is the best. For
the quad mesh on underlying parametric surface and interpolation surface,
Vari.2 and Vari.1 are best, respectively."
"A basic and an improved ear clipping based algorithm for triangulating simple
polygons and polygons with holes are presented. In the basic version, the ear
with smallest interior angle is always selected to be cut in order to create
fewer sliver triangles. To reduce sliver triangles in further, a bound of angle
is set to determine whether a newly formed triangle has sharp angles, and edge
swapping is accepted when the triangle is sharp. To apply the two algorithms on
polygons with holes, ""Bridge"" edges are created to transform a polygon with
holes to a degenerate polygon which can be triangulated by the two algorithms.
Applications show that the basic algorithm can avoid creating sliver triangles
and obtain better triangulations than the traditional ear clipping algorithm,
and the improved algorithm can in further reduce sliver triangles effectively.
Both of the algorithms run in O(n2) time and O(n) space."
"This paper presents an alternate choice of computing the convex hulls (CHs)
for planar point sets. We firstly discard the interior points and then sort the
remaining vertices by x- / y- coordinates separately, and later create a group
ofquadrilaterals (e-Quads) recursively according to the sequences ofthe sorted
lists of points. Finally, the desired CH is built based on a simple polygon
derived from all e-Quads. Besides the preprocessing for original planar point
sets, this algorithm has another mechanism of discarding interior point when
form e-Quads and assemble the simple polygon. Compared with three popular CH
algorithms, the proposed algorithm can generate CHs faster thanthe three but
has a penalty in space cost."
"Modified Direct Method (MDM) is an iterative scheme based on Jacobi
iterations for smoothing planar meshes [4]. The basic idea behind MDM is to
make any triangular element be as close to an equilateral triangle as possible.
Basedon the MDM, a length-weighted MDM is proposed and then combined with edge
swapping. In length-weighted MDM, weights of each neighboring node of a
smoothed node are determined by the length of its opposite edge. Also, the MDM,
Laplacian smoothing and length-weighted MDM are all combined with edge
swapping, and then implemented and compared on both structured and unstructured
triangular meshes. Examples show that length-weighted MDM is better than the
MDM and Laplacian smoothing for structured mesh but worse for unstructured
mesh. The hybrid approach of combining length-weighted MDM and edge swapping is
much better and can obtain more even optimized meshes than other two hybrid
approaches."
"A novel 2-D method for computing the convex hull of a sufficiently dense set
of n integer points is introduced. The approach employs a ranking function that
avoids sorting the points directly thus reducing the overall time complexity.
The ranked points create a simple polygonal chain from which the Convex Hull
can be found using a suitable O(n) method. The result is achieved by placing a
bound on the density (or ratio) of points to m, where m is the maximum value of
the ranking function required to represent the set of points yielding an O(n+m)
method. A fast method is then developed based on the bit length, p, of the data
set which reduces this time further. The required conditions are easily
satisfied by image processing methods which determine the Hulls of polygonal
regions where the densities are in the range of 3%. Our experiments on a range
problem domains show that this is not atypical. Since the complexity of the
method is related to the bit size p for current machines (p=32, p=64) the
method is for all practical purposes O(n). A short proof is provided."
"Klee's Measure Problem (KMP) asks for the volume of the union of n
axis-aligned boxes in d-space. Omitting logarithmic factors, the best algorithm
has runtime O*(n^{d/2}) [Overmars,Yap'91]. There are faster algorithms known
for several special cases: Cube-KMP (where all boxes are cubes), Unitcube-KMP
(where all boxes are cubes of equal side length), Hypervolume (where all boxes
share a vertex), and k-Grounded (where the projection onto the first k
dimensions is a Hypervolume instance).
  In this paper we bring some order to these special cases by providing
reductions among them. In addition to the trivial inclusions, we establish
Hypervolume as the easiest of these special cases, and show that the runtimes
of Unitcube-KMP and Cube-KMP are polynomially related. More importantly, we
show that any algorithm for one of the special cases with runtime T(n,d)
implies an algorithm for the general case with runtime T(n,2d), yielding the
first non-trivial relation between KMP and its special cases. This allows to
transfer W[1]-hardness of KMP to all special cases, proving that no n^{o(d)}
algorithm exists for any of the special cases under reasonable complexity
theoretic assumptions. Furthermore, assuming that there is no improved
algorithm for the general case of KMP (no algorithm with runtime O(n^{d/2 -
eps})) this reduction shows that there is no algorithm with runtime
O(n^{floor(d/2)/2 - eps}) for any of the special cases. Under the same
assumption we show a tight lower bound for a recent algorithm for 2-Grounded
[Yildiz,Suri'12]."
"In this paper we study the Airspace Sectorization Problem (ASP) where the
goal is to find an optimal partition (sectorization) of the airspace into a
certain number of sectors, each managed by an air traffic controller. The
objective of the ASP is to find a ""well-balanced"" sectorization that
distributes the workload evenly among the controllers. We formulate the ASP as
a partitioning problem of a set of moving points in a polygonal domain. In
addition to the requirement of balancing the workload, we introduce
restrictions on the geometry of the sectorization which come from the Air
Traffic Management aspects. We investigate several versions of the problem that
arise from different definitions of the notion of the workload and various
choices of geometric restrictions on the sectorization. We conclude that most
of the formulations of the problem, except maybe in some trivial cases, are
NP-hard. Finally, we propose a Local Redesigning Method (LRM), a heuristic
algorithm that rebalances a given sectorization by adjusting the boundaries of
the sectors. We evaluate LRM experimentally on synthetically generated
scenarios as well as on the real historical traffic data. We demonstrate that
the sectorizations produced by our method are superior in comparison with the
current sectorizations of the US airspace."
"We prove a conjecture of Aanjaneya, Bishnu, and Pal that the minimum number
of diffuse reflections sufficient to illuminate the interior of any simple
polygon with $n$ walls from any interior point light source is $\lfloor n/2
\rfloor - 1$. Light reflecting diffusely leaves a surface in all directions,
rather than at an identical angle as with specular reflections."
"A path or a polygonal domain is C-oriented if the orientations of its edges
belong to a set of C given orientations; this is a generalization of the
notable rectilinear case (C = 2). We study exact and approximation algorithms
for minimum-link C-oriented paths and paths with unrestricted orientations,
both in C-oriented and in general domains. Our two main algorithms are as
follows:
  A subquadratic-time algorithm with a non-trivial approximation guarantee for
general (unrestricted-orientation) minimum-link paths in general domains.
  An algorithm to find a minimum-link C-oriented path in a C-oriented domain.
Our algorithm is simpler and more time-space efficient than the prior
algorithm.
  We also obtain several related results:
  - 3SUM-hardness of determining the link distance with unrestricted
orientations (even in a rectilinear domain).
  - An optimal algorithm for finding a minimum-link rectilinear path in a
rectilinear domain. The algorithm and its analysis are simpler than the
existing ones.
  - An extension of our methods to find a C-oriented minimum-link path in a
general (not necessarily C-oriented) domain.
  - A more efficient algorithm to compute a 2-approximate C-oriented
minimum-link path.
  - A notion of ""robust"" paths. We show how minimum-link C-oriented paths
approximate the robust paths with unrestricted orientations to within an
additive error of 1."
"For a given point set $S$ in a plane, we develop a distributed algorithm to
compute the $\alpha-$shape of $S$. $\alpha-$shapes are well known geometric
objects which generalize the idea of a convex hull, and provide a good
definition for the shape of $S$. We assume that the distances between pairs of
points which are closer than a certain distance $r>0$ are provided, and we show
constructively that this information is sufficient to compute the alpha shapes
for a range of parameters, where the range depends on $r$.
  Such distributed algorithms are very useful in domains such as sensor
networks, where each point represents a sensing node, the location of which is
not necessarily known.
  We also introduce a new geometric object called the Delaunay-\v{C}ech shape,
which is geometrically more appropriate than an $\alpha-$shape for some cases,
and show that it is topologically equivalent to $\alpha-$shapes."
"Given n red and n blue points in general position in the plane, it is
well-known that there is a perfect matching formed by non-crossing line
segments. We characterize the bichromatic point sets which admit exactly one
non-crossing matching. We give several geometric descriptions of such sets, and
find an O(nlogn) algorithm that checks whether a given bichromatic set has this
property."
"Let $\mathcal{D}$ be a set of $n$ pairwise disjoint unit disks in the plane.
We describe how to build a data structure for $\mathcal{D}$ so that for any
point set $P$ containing exactly one point from each disk, we can quickly find
the onion decomposition (convex layers) of $P$.
  Our data structure can be built in $O(n \log n)$ time and has linear size.
Given $P$, we can find its onion decomposition in $O(n \log k)$ time, where $k$
is the number of layers. We also provide a matching lower bound. Our solution
is based on a recursive space decomposition, combined with a fast algorithm to
compute the union of two disjoint onion"
"Let S be a planar point set. Krznaric and Levcopoulos proved that given the
Delaunay triangulation DT(S) for S, one can find the greedy triangulation GT(S)
in linear time. We provide a (partial) converse of this result: given GT(S), it
is possible to compute DT(S) in linear expected time. Thus, these structures
are basically equivalent.
  To obtain our result, we generalize another algorithm by Krznaric and
Levcopoulos to find a hierarchical clustering for S in linear time, once DT(S)
is known. We show that their algorithm remains (almost) correct for any
triangulation of bounded dilation, i.e., any triangulation in which the
shortest path distance between any two points approximates their Euclidean
distance. In general, however, the resulting running time may be superlinear.
Nonetheless, we can show that the properties of the greedy triangulation
suffice to guarantee a linear time bound."
"With recent advances in sensing and tracking technology, trajectory data is
becoming increasingly pervasive and analysis of trajectory data is becoming
exceedingly important. A fundamental problem in analyzing trajectory data is
that of identifying common patterns between pairs or among groups of
trajectories. In this paper, we consider the problem of identifying similar
portions between a pair of trajectories, each observed as a sequence of points
sampled from it.
  We present new measures of trajectory similarity --- both local and global
--- between a pair of trajectories to distinguish between similar and
dissimilar portions. Our model is robust under noise and outliers, it does not
make any assumptions on the sampling rates on either trajectory, and it works
even if they are partially observed. Additionally, the model also yields a
scalar similarity score which can be used to rank multiple pairs of
trajectories according to similarity, e.g. in clustering applications. We also
present efficient algorithms for computing the similarity under our measures;
the worst-case running time is quadratic in the number of sample points.
  Finally, we present an extensive experimental study evaluating the
effectiveness of our approach on real datasets, comparing with it with earlier
approaches, and illustrating many issues that arise in trajectory data. Our
experiments show that our approach is highly accurate in distinguishing similar
and dissimilar portions as compared to earlier methods even with sparse
sampling."
"Let $P$ be an orthogonal polygon. Consider a sliding camera that travels back
and forth along an orthogonal line segment $s\in P$ as its \emph{trajectory}.
The camera can see a point $p\in P$ if there exists a point $q\in s$ such that
$pq$ is a line segment normal to $s$ that is completely inside $P$. In the
\emph{minimum-cardinality sliding cameras problem}, the objective is to find a
set $S$ of sliding cameras of minimum cardinality to guard $P$ (i.e., every
point in $P$ can be seen by some sliding camera) while in the
\emph{minimum-length sliding cameras problem} the goal is to find such a set
$S$ so as to minimize the total length of trajectories along which the cameras
in $S$ travel.
  In this paper, we first settle the complexity of the minimum-length sliding
cameras problem by showing that it is polynomial tractable even for orthogonal
polygons with holes, answering a question asked by Katz and Morgenstern (2011).
We next show that the minimum-cardinality sliding cameras problem is
\textsc{NP}-hard when $P$ is allowed to have holes, which partially answers
another question asked by Katz and Morgenstern (2011)."
"We consider the situation where one is given a set S of points in the plane
and a collection D of unit disks embedded in the plane. We show that finding a
minimum cardinality subset of D such that any path between any two points in S
is intersected by at least one disk is NP-complete. This settles an open
problem raised by Matt Gibson et al[1]. Using a similar reduction, we show that
finding a minimum cardinality subset D' of D such that R^2 - (D - D') consists
of a single connected region is also NP-complete. Lastly, we show that the
Multiterminal Cut Problem remains NP-complete when restricted to unit disk
graphs."
"Ruled surface is widely used in engineering design such as parting surface
design of injection mold and checking surface design of checking fixture, which
are usually generated by offsetting 3D curves. However, in 3D curve offset,
there often exist break,interaction and overlapping problems which can't be
solved by current CAD software automatically. This paper is targeted at
developing a 3D curve offsetting algorithm for ruled surface generation, and
three key technologies are introduced in details: An improved curve division
method is proposed to reduce the offset accuracy error resulted from different
offset distances and curvatures; An offsetting curve overlapping detection and
elimination method is proposed; And then, a curve transition method is
presented to improve curve offsetting quality for the break and
intersection/overlapping regions, where a new algorithm for generating positive
weights spherical rational quartic Bezier curve is proposed to bridge the
breaks of offset curves to create a smooth ruled surface. Finally, two
practical design cases, parting surface and checking surface generation, show
that the proposed approach can enhance the efficiency and quality for ruled
surface generation in engineering design."
"We propose a linear time and constant space algorithm for computing Euclidean
projections onto sets on which a normalized sparseness measure attains a
constant value. These non-convex target sets can be characterized as
intersections of a simplex and a hypersphere. Some previous methods required
the vector to be projected to be sorted, resulting in at least quasilinear time
complexity and linear space complexity. We improve on this by adaptation of a
linear time algorithm for projecting onto simplexes. In conclusion, we propose
an efficient algorithm for computing the product of the gradient of the
projection with an arbitrary vector."
"Consider a set P of N random points on the unit sphere of dimension $d-1$,
and the symmetrized set S = P union (-P). The halving polyhedron of S is
defined as the convex hull of the set of centroids of N distinct points in S.
We prove that after appropriate rescaling this halving polyhedron is Hausdorff
close to the unit ball with high probability, as soon as the number of points
grows like $Omega(d log(d))$. From this result, we deduce probabilistic lower
bounds on the complexity of approximations of the distance to the empirical
measure on the point set by distance-like functions."
"In this paper we show that the \theta-graph with 4 cones has constant stretch
factor, i.e., there is a path between any pair of vertices in this graph whose
length is at most a constant times the Euclidean distance between that pair of
vertices. This is the last \theta-graph for which it was not known whether its
stretch factor was bounded."
"We present a new algorithm for computing motorcycle graphs that runs in
O(n^(4/3+e)) time for any e>0, improving on all previously known algorithms.
The main application of this result is to computing the straight skeleton of a
polygon. It allows us to compute the straight skeleton of a non-degenerate
polygon with h holes in O(n.sqrt(h+1)log^2(n)+n^(4/3+e)) expected time. If all
input coordinates are O(log n)-bit rational numbers, we can compute the
straight skeleton of a (possibly degenerate) polygon with h holes in
O(n.sqrt(h+1)log^3(n)) expected time.
  In particular, it means that we can compute the straight skeleton of a simple
polygon in O(n.log^3(n)) expected time if all input coordinates are O(\log
n)-bit rationals, while all previously known algorithms have worst-case running
time larger than n^(3/2)."
"The collective motion of a set of moving entities like people, birds, or
other animals, is characterized by groups arising, merging, splitting, and
ending. Given the trajectories of these entities, we define and model a
structure that captures all of such changes using the Reeb graph, a concept
from topology. The trajectory grouping structure has three natural parameters
that allow more global views of the data in group size, group duration, and
entity inter-distance. We prove complexity bounds on the maximum number of
maximal groups that can be present, and give algorithms to compute the grouping
structure efficiently. We also study how the trajectory grouping structure can
be made robust, that is, how brief interruptions of groups can be disregarded
in the global structure, adding a notion of persistence to the structure.
Furthermore, we showcase the results of experiments using data generated by the
NetLogo flocking model and from the Starkey project. The Starkey data describe
the movement of elk, deer, and cattle. Although there is no ground truth for
the grouping structure in this data, the experiments show that the trajectory
grouping structure is plausible and has the desired effects when changing the
essential parameters. Our research provides the first complete study of
trajectory group evolvement, including combinatorial, algorithmic, and
experimental results."
"We describe an algorithm to construct an intrinsic Delaunay triangulation of
a smooth closed submanifold of Euclidean space. Using results established in a
companion paper on the stability of Delaunay triangulations on $\delta$-generic
point sets, we establish sampling criteria which ensure that the intrinsic
Delaunay complex coincides with the restricted Delaunay complex and also with
the recently introduced tangential Delaunay complex. The algorithm generates a
point set that meets the required criteria while the tangential complex is
being constructed. In this way the computation of geodesic distances is
avoided, the runtime is only linearly dependent on the ambient dimension, and
the Delaunay complexes are guaranteed to be triangulations of the manifold."
"Measuring the similarity of curves is a fundamental problem arising in many
application fields. There has been considerable interest in several such
measures, both in Euclidean space and in more general setting such as curves on
Riemannian surfaces or curves in the plane minus a set of obstacles. However,
so far, efficiently computable similarity measures for curves on general
surfaces remain elusive. This paper aims at developing a natural curve
similarity measure that can be easily extended and computed for curves on
general orientable 2-manifolds. Specifically, we measure similarity between
homotopic curves based on how hard it is to deform one curve into the other one
continuously, and define this ""hardness"" as the minimum possible surface area
swept by a homotopy between the curves. We consider cases where curves are
embedded in the plane or on a triangulated orientable surface with genus $g$,
and we present efficient algorithms (which are either quadratic or near linear
time, depending on the setting) for both cases."
"We consider the following problem: Given a point set in space find a largest
subset that is in convex position and whose convex hull is empty. We show that
the (decision version of the) problem is W[1]-hard."
"We investigate the classes of functions whose minimization diagrams can be
approximated efficiently in \Re^d. We present a general framework and a
data-structure that can be used to approximate the minimization diagram of such
functions. The resulting data-structure has near linear size and can answer
queries in logarithmic time. Applications include approximating the Voronoi
diagram of (additively or multiplicatively) weighted points. Our technique also
works for more general distance functions, such as metrics induced by convex
bodies, and the nearest furthest-neighbor distance to a set of point sets.
Interestingly, our framework works also for distance functions that do not
comply with the triangle inequality. For many of these functions no near-linear
size approximation was known before."
"We present a new algorithm that produces a well-spaced superset of points
conforming to a given input set in any dimension with guaranteed optimal output
size. We also provide an approximate Delaunay graph on the output points. Our
algorithm runs in expected time $O(2^{O(d)}(n\log n + m))$, where $n$ is the
input size, $m$ is the output point set size, and $d$ is the ambient dimension.
The constants only depend on the desired element quality bounds.
  To gain this new efficiency, the algorithm approximately maintains the
Voronoi diagram of the current set of points by storing a superset of the
Delaunay neighbors of each point. By retaining quality of the Voronoi diagram
and avoiding the storage of the full Voronoi diagram, a simple exponential
dependence on $d$ is obtained in the running time. Thus, if one only wants the
approximate neighbors structure of a refined Delaunay mesh conforming to a set
of input points, the algorithm will return a size $2^{O(d)}m$ graph in
$2^{O(d)}(n\log n + m)$ expected time. If $m$ is superlinear in $n$, then we
can produce a hierarchically well-spaced superset of size $2^{O(d)}n$ in
$2^{O(d)}n\log n$ expected time."
"Consider the continuum of points along the edges of a network, i.e., an
undirected graph with positive edge weights. We measure distance between these
points in terms of the shortest path distance along the network, known as the
network distance. Within this metric space, we study farthest points.
  We introduce network farthest-point diagrams, which capture how the farthest
points---and the distance to them---change as we traverse the network. We
preprocess a network G such that, when given a query point q on G, we can
quickly determine the farthest point(s) from q in G as well as the farthest
distance from q in G. Furthermore, we introduce a data structure supporting
queries for the parts of the network that are farther away from q than some
threshold R > 0, where R is part of the query.
  We also introduce the minimum eccentricity feed-link problem defined as
follows. Given a network G with geometric edge weights and a point p that is
not on G, connect p to a point q on G with a straight line segment pq, called a
feed-link, such that the largest network distance from p to any point in the
resulting network is minimized. We solve the minimum eccentricity feed-link
problem using eccentricity diagrams. In addition, we provide a data structure
for the query version, where the network G is fixed and a query consists of the
point p."
"Convex hulls are a fundamental geometric tool used in a number of algorithms.
As a side-effect of exhaustive tests for an algorithm for which a convex hull
computation was the first step, interesting experimental results were found and
are the sunject of this paper. They establish that the number of convex
vertices of natural datasets can be predicted, if not precisely at least within
a defined range. Namely it was found that the number of convex vertices of a
dataset of N points lies in the range 2.35 N^0.091 <= h <= 19.19 N^0.091. This
range obviously does not describe neither natural nor artificial worst-cases
but corresponds to the distributions of natural data. This can be used for
instance to define a starting size for pre-allocated arrays or to evaluate
output-sensitive algorithms. A further consequence of these results is that the
random models of data used to test convex hull algorithms should be bounded by
rectangles and not as they usually are by circles if they want to represent
accurately natural datasets"
"Convex hulls are a fundamental geometric tool used in a number of algorithms.
A famous paper by Akl and Toussaint in 1978 described a way to reduce the
number of points involved in the computation, which is since known as the
Akl-Toussaint heuristics. This paper first studies what this heurstics really
represents in terms of reduction of points and demonstrates that the optimum
selection is reached using an octogon as the remaining number of points is in
O(sqrt(N)) rather than the usual O(N). Then it focuses on optimising the
overall computational efficiency in a convex hull computation. Although the
heuristics is usually used as a first step in computations one can obtain the
convex hull directly from the heuristics's basis. First a simple incremental
implementation is described, and if the number of characteristic points of the
Akl-Toussaint heuristics p is taken as a parametre the convex hull is then
computed in a O(N(p+h/p)) average complexity or O(Nh) asymptotic complexity.
Given the relative constant factor of 1/p however experimental results show
that this algorithm should be considered linear in average. Worst-case
complexity is in O(N^2) and space complexity is O(h) but could be O(1) if the
required output is the array of convex vertices's indexes. Then a remark on why
the basic incremental method should be preferred for average cases is made.
Finally an optimal linear algorithm both in average and worst-case and using a
minimal space complexity in O(sqrt(N)) in average (or O(1) if in-place
computation is allowed) is presented."
"We introduce a parametrized notion of genericity for Delaunay triangulations
which, in particular, implies that the Delaunay simplices of $\delta$-generic
point sets are thick. Equipped with this notion, we study the stability of
Delaunay triangulations under perturbations of the metric and of the vertex
positions. We quantify the magnitude of the perturbations under which the
Delaunay triangulation remains unchanged."
"Let $P$ be a collection of $n$ points moving along pseudo-algebraic
trajectories in the plane. One of the hardest open problems in combinatorial
and computational geometry is to obtain a nearly quadratic upper bound, or at
least a subcubic bound, on the maximum number of discrete changes that the
Delaunay triangulation $\DT(P)$ of $P$ experiences during the motion of the
points of $P$.
  In this paper we obtain an upper bound of $O(n^{2+\eps})$, for any $\eps>0$,
under the assumptions that (i) any four points can be co-circular at most
twice, and (ii) either no triple of points can be collinear more than twice, or
no ordered triple of points can be collinear more than once."
"Let $s$ be a source point and $t$ be a destination point inside an $n$-vertex
simple polygon $P$. Euclidean shortest paths and minimum-link paths between $s$
and $t$ inside $P$ have been well studied. Both these kinds of paths are simple
and piecewise-convex. However, computing optimal paths in the context of
diffuse or specular reflections does not seem to be an easy task. A path from a
light source $s$ to $t$ inside $P$ is called a diffuse reflection path if the
turning points of the path lie in the interiors of the boundary edges of $P$. A
diffuse reflection path is said to be optimal if it has the minimum number of
turning points amongst all diffuse reflection paths between $s$ and $t$. The
minimum diffuse reflection path may not be simple. The problem of computing the
minimum diffuse reflection path in low degree polynomial time has remained
open.
  In our quest for understanding the geometric structure of the minimum diffuse
reflection paths vis-a-vis shortest paths and minimum link paths, we define a
new kind of diffuse reflection path called a constrained diffuse reflection
path where (i) the path is simple, (ii) it intersects only the eaves of the
Euclidean shortest path between $s$ and $t$, and (iii) it intersects each eave
exactly once. For computing a minimum constrained diffuse reflection path from
$s$ to $t$, we present an $O(n(n+\beta))$ time algorithm, where $\beta =\Theta
(n^2)$ in the worst case. Here, $\beta$ depends on the shape of the polygon. We
also establish some properties relating minimum constrained diffuse reflection
paths and minimum diffuse reflection paths. Constrained diffuse reflection
paths introduced in this paper provide new geometric insights into the hitherto
unknown structures and shapes of optimal reflection paths."
"This paper dwells in developing a general algorithm for constructing a
piecewise Ball Curve with curvature continuity (GC2). The proposed algorithm
requires GC2 data in which the designer must define unit tangent vectors and
signed curvatures at each interpolating points. As a numerical example, a vase
is constructed using GC2 piecewise Ball Curve"
"Generating Hilbert curves in Z^2 using L-systems appears to be efficient and
easy"
"The persistent homology with coefficients in a field F coincides with the
same for cohomology because of duality. We propose an implementation of a
recently introduced algorithm for persistent cohomology that attaches
annotation vectors with the simplices. We separate the representation of the
simplicial complex from the representation of the cohomology groups, and
introduce a new data structure for maintaining the annotation matrix, which is
more compact and reduces substancially the amount of matrix operations. In
addition, we propose heuristics to simplify further the representation of the
cohomology groups and improve both time and space complexities. The paper
provides a theoretical analysis, as well as a detailed experimental study of
our implementation and comparison with state-of-the-art software for persistent
homology and cohomology."
"We study the \LowerBoundedCenter (\lbc) problem, which is a clustering
problem that can be viewed as a variant of the \kCenter problem. In the \lbc
problem, we are given a set of points P in a metric space and a lower bound
\lambda, and the goal is to select a set C \subseteq P of centers and an
assignment that maps each point in P to a center of C such that each center of
C is assigned at least \lambda points. The price of an assignment is the
maximum distance between a point and the center it is assigned to, and the goal
is to find a set of centers and an assignment of minimum price. We give a
constant factor approximation algorithm for the \lbc problem that runs in O(n
\log n) time when the input points lie in the d-dimensional Euclidean space
R^d, where d is a constant. We also prove that this problem cannot be
approximated within a factor of 1.8-\epsilon unless P = \NP even if the input
points are points in the Euclidean plane R^2."
"We consider the problem of computing the time-convex hull of a point set
under the general $L_p$ metric in the presence of a straight-line highway in
the plane. The traveling speed along the highway is assumed to be faster than
that off the highway, and the shortest time-path between a distant pair may
involve traveling along the highway. The time-convex hull ${TCH}(P)$ of a point
set $P$ is the smallest set containing both $P$ and \emph{all} shortest
time-paths between any two points in ${TCH}(P)$. In this paper we give an
algorithm that computes the time-convex hull under the $L_p$ metric in optimal
$O(n\log n)$ time for a given set of $n$ points and a real number $p$ with
$1\le p \le \infty$."
"Let $P$ and $S$ be two disjoint sets of $n$ and $m$ points in the plane,
respectively. We consider the problem of computing a Steiner tree whose Steiner
vertices belong to $S$, in which each point of $P$ is a leaf, and whose longest
edge length is minimum. We present an algorithm that computes such a tree in
$O((n+m)\log m)$ time, improving the previously best result by a logarithmic
factor. We also prove a matching lower bound in the algebraic computation tree
model."
"In the Boundary Labeling problem, we are given a set of $n$ points, referred
to as sites, inside an axis-parallel rectangle $R$, and a set of $n$ pairwise
disjoint rectangular labels that are attached to $R$ from the outside. The task
is to connect the sites to the labels by non-intersecting rectilinear paths,
so-called leaders, with at most one bend.
  In this paper, we study the Multi-Sided Boundary Labeling problem, with
labels lying on at least two sides of the enclosing rectangle. We present a
polynomial-time algorithm that computes a crossing-free leader layout if one
exists. So far, such an algorithm has only been known for the cases in which
labels lie on one side or on two opposite sides of $R$ (here a crossing-free
solution always exists). The case where labels may lie on adjacent sides is
more difficult. We present efficient algorithms for testing the existence of a
crossing-free leader layout that labels all sites and also for maximizing the
number of labeled sites in a crossing-free leader layout. For two-sided
boundary labeling with adjacent sides, we further show how to minimize the
total leader length in a crossing-free layout."
"The Discretizable Molecular Distance Geometry Problem (DMDGP) consists in a
subclass of the Molecular Distance Geometry Problem for which an embedding in
${\mathbb{R}^3}$ can be found using a Branch & Prune (BP) algorithm in a
discrete search space. We propose a Clifford Algebra model of the DMDGP with an
accompanying version of the BP algorithm."
"While well-known methods to list the intersections of either a list of
segments or a complex polygon aim at achieving optimal time-complexity they
often do so at the cost of memory comsumption and complex code. Real-life
software optimisation however lies in optimising at the same time speed and
memory usage as well as keeping code simple. This paper first presents some
thoughts on the available algorithms in terms of memory usage leading to a very
simple scan-line-based algorithm aiming at answering that challenge. Although
sub-optimal in terms of speed it is optimal if both speed and memory space are
taken together and is very easy to implement. For N segments and k
intersections it uses only N additional integers and lists the intersections in
O(N^1.26) or corrects them in O((N+k) N^0.26) at most in average, with a high
probability of a much lower exponent around 0.16 and even as low as 0.1. It is
therefore well adapted for inclusion in larger software and seems like a good
compromise. Worst-case is in O(N^2). Then the paper will focus on differences
between available methods and the brute-force algorithm and a solution is
proposed. Although sub-optimal its applications could mainly be to answer in a
fast way a number of scattered unrelated intersection queries using minimal
complexity and additional resources."
"Let S be a subdivision of the plane into polygonal regions, where each region
has an associated positive weight. The weighted region shortest path problem is
to determine a shortest path in S between two points s, t in R^2, where the
distances are measured according to the weighted Euclidean metric-the length of
a path is defined to be the weighted sum of (Euclidean) lengths of the
sub-paths within each region. We show that this problem cannot be solved in the
Algebraic Computation Model over the Rational Numbers (ACMQ). In the ACMQ, one
can compute exactly any number that can be obtained from the rationals Q by
applying a finite number of operations from +, -, \times, \div, \sqrt[k]{}, for
any integer k >= 2. Our proof uses Galois theory and is based on Bajaj's
technique."
"In this work, we propose a detailed computational framework for modelling the
envelope of the swept volume, that is the boundary of the volume obtained by
sweeping an input solid along a trajectory of rigid motions. Our framework is
adapted to the well-established industry-standard brep format to enable its
implementation in modern CAD systems. This is achieved via a ""local analysis"",
which covers parametrization and singularities, as well as a ""global theory""
which tackles face-boundaries, self-intersections and trim curves. Central to
the local analysis is the ""funnel"" which serves as a natural parameter space
for the basic surfaces constituting the sweep. The trimming problem is reduced
to the problem of surface-surface intersections of these basic surfaces. Based
on the complexity of these intersections, we introduce a novel classification
of sweeps as either decomposable or non-decomposable. Further, we construct an
{\em invariant} function $\theta$ on the funnel which efficiently separates
decomposable and non-decomposable sweeps. Through a geometric theorem we also
show intimate connections between $\theta$, local curvatures and the inverse
trajectory used in earlier works as an approach towards trimming. In contrast
to the inverse trajectory approach, $\theta$ is robust and is the key to a
complete structural understanding, and an efficient computation of both, the
singular locus and the trim curves, which are central to a stable
implementation. Several illustrative outputs of a pilot implementation are
included."
"We extend the notion of the distance to a measure from Euclidean space to
probability measures on general metric spaces as a way to do topological data
analysis in a way that is robust to noise and outliers. We then give an
efficient way to approximate the sub-level sets of this function by a union of
metric balls and extend previous results on sparse Rips filtrations to this
setting. This robust and efficient approach to topological data analysis is
illustrated with several examples from an implementation."
"We propose an algorithm for tracing polylines on a triangle mesh such that:
they are aligned with a N-symmetry direction field, and two such polylines
cannot cross or merge. This property is fundamental for mesh segmentation and
is very difficult to enforce with numerical integration of vector fields. We
propose an alternative solution based on ""stream-mesh"", a new combinatorial
data structure that defines, for each point of a triangle edge, where the
corresponding polyline leaves the triangle. It makes it possible to trace
polylines by iteratively crossing triangles. Vector field singularities and
polyline/vertex crossing are characterized and consistently handled. The
polylines inherits the cross-free property of the stream-mesh, except inside
triangles where avoiding local overlaps would require higher order polycurves."
"We consider an arrangement $\A$ of $n$ hyperplanes in $\R^d$ and the zone
$\Z$ in $\A$ of the boundary of an arbitrary convex set in $\R^d$ in such an
arrangement. We show that, whereas the combinatorial complexity of $\Z$ is
known only to be $O<n^{d-1}\log n>$ \cite{APS}, the outer part of the zone has
complexity $O<n^{d-1}>$ (without the logarithmic factor). Whether this bound
also holds for the complexity of the inner part of the zone is still an open
question (even for $d=2$)."
"We study a map matching problem, the task of finding in an embedded graph a
path that has low distance to a given curve in R^2. The Fr\'echet distance is a
common measure for this problem. Efficient methods exist to compute the best
path according to this measure. However, these methods cannot guarantee that
the result is simple (i.e. it does not intersect itself) even if the given
curve is simple. In this paper, we prove that it is in fact NP-complete to
determine the existence a simple cycle in a planar straight-line embedding of a
graph that has at most a given Fr\'echet distance to a given simple closed
curve. We also consider the implications of our proof on some variants of the
problem."
"Let $P$ be a $d$-dimensional $n$-point set. A partition $T$ of $P$ is called
a Tverberg partition if the convex hulls of all sets in $T$ intersect in at
least one point. We say $T$ is $t$-tolerant if it remains a Tverberg partition
after deleting any $t$ points from $P$. Sober\'{o}n and Strausz proved that
there is always a $t$-tolerant Tverberg partition with $\lceil n / (d+1)(t+1)
\rceil$ sets. However, so far no nontrivial algorithms for computing or
approximating such partitions have been presented.
  For $d \leq 2$, we show that the Sober\'{o}n-Strausz bound can be improved,
and we show how the corresponding partitions can be found in polynomial time.
For $d \geq 3$, we give the first polynomial-time approximation algorithm by
presenting a reduction to the Tverberg problem with no tolerance. Finally, we
show that it is coNP-complete to determine whether a given Tverberg partition
is t-tolerant."
"This paper focuses on a variation of the Art Gallery problem that considers
open edge guards and open mobile guards. A mobile guard can be placed on edges
and diagonals of a polygon, and the ""open"" prefix means that the endpoints of
such edge or diagonal are not taken into account for visibility purposes. This
paper studies the number of guards that are sufficient and sometimes necessary
to guard some classes of simple polygons for both open edge and open mobile
guards. This problem is also considered for planar triangulation graphs using
open edge guards."
"Greedy embedding (or drawing) is a simple and efficient strategy to route
messages in wireless sensor networks. For each source-destination pair of nodes
s, t in a greedy embedding there is always a neighbor u of s that is closer to
t according to some distance metric. The existence of greedy embeddings in the
Euclidean plane R^2 is known for certain graph classes such as 3-connected
planar graphs. We completely characterize the trees that admit a greedy
embedding in R^2. This answers a question by Angelini et al. (Graph Drawing
2009) and is a further step in characterizing the graphs that admit Euclidean
greedy embeddings."
"In this paper we introduce self-approaching graph drawings. A straight-line
drawing of a graph is self-approaching if, for any origin vertex s and any
destination vertex t, there is an st-path in the graph such that, for any point
q on the path, as a point p moves continuously along the path from the origin
to q, the Euclidean distance from p to q is always decreasing. This is a more
stringent condition than a greedy drawing (where only the distance between
vertices on the path and the destination vertex must decrease), and guarantees
that the drawing is a 5.33-spanner. We study three topics: (1) recognizing
self-approaching drawings; (2) constructing self-approaching drawings of a
given graph; (3) constructing a self-approaching Steiner network connecting a
given set of points. We show that: (1) there are efficient algorithms to test
if a polygonal path is self-approaching in R^2 and R^3, but it is NP-hard to
test if a given graph drawing in R^3 has a self-approaching uv-path; (2) we can
characterize the trees that have self-approaching drawings; (3) for any given
set of terminal points in the plane, we can find a linear sized network that
has a self-approaching path between any ordered pair of terminals."
"All known algorithms for the Fr\'echet distance between curves proceed in two
steps: first, they construct an efficient oracle for the decision version;
second, they use this oracle to find the optimum from a finite set of critical
values. We present a novel approach that avoids the detour through the decision
version. This gives the first quadratic time algorithm for the Fr\'echet
distance between polygonal curves in $R^d$ under polyhedral distance functions
(e.g., $L_1$ and $L_\infty$). We also get a $(1+\varepsilon)$-approximation of
the Fr\'echet distance under the Euclidean metric, in quadratic time for any
fixed $\varepsilon > 0$. For the exact Euclidean case, our framework currently
yields an algorithm with running time $O(n^2 \log^2 n)$. However, we conjecture
that it may eventually lead to a faster exact algorithm."
"CAT(0) metric spaces and hyperbolic spaces play an important role in
combinatorial and geometric group theory. In this paper, we present efficient
algorithms for distance problems in CAT(0) planar complexes. First of all, we
present an algorithm for answering single-point distance queries in a CAT(0)
planar complex. Namely, we show that for a CAT(0) planar complex K with n
vertices, one can construct in O(n^2 log n) time a data structure D of size
O(n^2) so that, given a point x in K, the shortest path gamma(x,y) between x
and the query point y can be computed in linear time. Our second algorithm
computes the convex hull of a finite set of points in a CAT(0) planar complex.
This algorithm is based on Toussaint's algorithm for computing the convex hull
of a finite set of points in a simple polygon and it constructs the convex hull
of a set of k points in O(n^2 log n + nk log k) time, using a data structure of
size O(n^2 + k)."
"In the Hausdorff Voronoi diagram of a set of clusters of points in the plane,
the distance between a point t and a cluster P is the maximum Euclidean
distance between t and a point in P. This diagram has direct applications in
VLSI design. We consider so-called ""non-crossing"" clusters. The complexity of
the Hausdorff diagram of m such clusters is linear in the total number n of
points in the convex hulls of all clusters. We present randomized incremental
constructions for computing efficiently the diagram, improving considerably
previous results. Our best complexity algorithm runs in expected time O((n +
m(log log(n))^2)log^2(n)) and worst-case space O(n). We also provide a more
practical algorithm whose expected running time is O((n + m log(n))log^2(n))
and expected space complexity is O(n). To achieve these bounds, we augment the
randomized incremental paradigm for the construction of Voronoi diagrams with
the ability to efficiently handle non-standard characteristics of generalized
Voronoi diagrams, such as sites of non-constant complexity, sites that are not
enclosed in their Voronoi regions, and empty Voronoi regions."
"We describe a linear-time algorithm that finds a planar drawing of every
graph of a simple line or pseudoline arrangement within a grid of area
O(n^{7/6}). No known input causes our algorithm to use area
\Omega(n^{1+\epsilon}) for any \epsilon>0; finding such an input would
represent significant progress on the famous k-set problem from discrete
geometry. Drawing line arrangement graphs is the main task in the Planarity
puzzle."
"Most CAD or other spatial data models, in particular boundary representation
models, are called ""topological"" and represent spatial data by a structured
collection of ""topological primitives"" like edges, vertices, faces, and
volumes. These then represent spatial objects in geo-information- (GIS) or CAD
systems or in building information models (BIM). Volume objects may then either
be represented by their 2D boundary or by a dedicated 3D-element, the ""solid"".
The latter may share common boundary elements with other solids, just as
2D-polygon topologies in GIS share common boundary edges. Despite the frequent
reference to ""topology"" in publications on spatial modelling the formal link
between mathematical topology and these ""topological"" models is hardly
described in the literature. Such link, for example, cannot be established by
the often cited nine-intersections model which is too elementary for that
purpose. Mathematically, the link between spatial data and the modelled ""real
world"" entities is established by a chain of ""continuous functions"" - a very
important topological notion, yet often overlooked by spatial data modellers.
This article investigates how spatial data can actually be considered
topological spaces, how continuous functions between them are defined, and how
CAD systems can make use of them. Having found examples of applications of
continuity in CAD data models it turns out that of continuity has much
practical relevance for CAD systems."
"We present a symbolic perturbation scheme for arbitrary polynomial geometric
predicates which combines the benefits of Emiris and Canny's simple randomized
linear perturbation scheme with Yap's multiple infinitesimal scheme for general
predicates. Like the randomized scheme, our method accepts black box polynomial
functions as input. For nonmaliciously chosen predicates, our method is as fast
as the linear scheme, scaling reasonably with the degree of the polynomial even
for fully degenerate input. Like Yap's scheme, the computed sign is
deterministic, never requiring an algorithmic restart (assuming a high quality
pseudorandom generator), and works for arbitrary predicates with no knowledge
of their structure. We also apply our technique to exactly or nearly exactly
rounded constructions that work correctly for degenerate input, using
l'Hopital's rule to compute the necessary singular limits. We provide an open
source prototype implementation including example algorithms for Delaunay
triangulation and Boolean operations on polygons and circular arcs in the
plane."
"Consider a sliding camera that travels back and forth along an orthogonal
line segment $s$ inside an orthogonal polygon $P$ with $n$ vertices. The camera
can see a point $p$ inside $P$ if and only if there exists a line segment
containing $p$ that crosses $s$ at a right angle and is completely contained in
$P$. In the minimum sliding cameras (MSC) problem, the objective is to guard
$P$ with the minimum number of sliding cameras. In this paper, we give an
$O(n^{5/2})$-time $(7/2)$-approximation algorithm to the MSC problem on any
simple orthogonal polygon with $n$ vertices, answering a question posed by Katz
and Morgenstern (2011). To the best of our knowledge, this is the first
constant-factor approximation algorithm for this problem."
"In this paper, we consider the problem of improving 2D triangle meshes
tessellating planar regions. We propose a new variational principle for
improving 2D triangle meshes where the energy functional is a convex function
over the angle structures whose maximizer is unique and consists only of
equilateral triangles. This energy functional is related to hyperbolic volume
of ideal 3-simplex. Even with extra constraints on the angles for embedding the
mesh into the plane and preserving the boundary, the energy functional remains
well-behaved. We devise an efficient algorithm for maximizing the energy
functional over these extra constraints. We apply our algorithm to various
datasets and compare its performance with that of CVT. The experimental results
show that our algorithm produces the meshes with both the angles and the aspect
ratios of triangles lying in tighter intervals."
"It is an open problem, posed in \cite{SoCG}, to determine the minimal $k$
such that an open flexible $k$-chain can interlock with a flexible 2-chain. It
was first established in \cite{GLOSZ} that there is an open 16-chain in a
trapezoid frame that achieves interlocking. This was subsequently improved in
\cite{GLOZ} to establish interlocking between a 2-chain and an open 11-chain.
Here we improve that result once more, establishing interlocking between a
2-chain and a 10-chain. We present arguments that indicate that 10 is likely
the minimum."
"Boolean operations of geometric models is an essential issue in computational
geometry. In this paper, we develop a simple and robust approach to perform
Boolean operations on closed and open triangulated surfaces. Our method mainly
has two stages: (1) We firstly find out candidate intersected-triangles pairs
based on Octree and then compute the inter-section lines for all pairs of
triangles with parallel algorithm; (2) We form closed or open
intersection-loops, sub-surfaces and sub-blocks quite robustly only according
to the cleared and updated topology of meshes while without coordinate
computations for geometric enti-ties. A novel technique instead of
inside/outside classification is also proposed to distinguish the resulting
union, subtraction and intersection. Several examples have been given to
illus-trate the effectiveness of our approach."
"A major open problem in the field of metric embedding is the existence of
dimension reduction for $n$-point subsets of Euclidean space, such that both
distortion and dimension depend only on the {\em doubling constant} of the
pointset, and not on its cardinality. In this paper, we negate this possibility
for $\ell_p$ spaces with $p>2$. In particular, we introduce an $n$-point subset
of $\ell_p$ with doubling constant O(1), and demonstrate that any embedding of
the set into $\ell_p^d$ with distortion $D$ must have
$D\ge\Omega\left(\left(\frac{c\log
n}{d}\right)^{\frac{1}{2}-\frac{1}{p}}\right)$."
"Given a tesselation of the plane, defined by a planar straight-line graph
$G$, we want to find a minimal set $S$ of points in the plane, such that the
Voronoi diagram associated with $S$ ""fits"" \ $G$. This is the Generalized
Inverse Voronoi Problem (GIVP), defined in \cite{Trin07} and rediscovered
recently in \cite{Baner12}. Here we give an algorithm that solves this problem
with a number of points that is linear in the size of $G$, assuming that the
smallest angle in $G$ is constant."
"There are numerous styles of planar graph drawings, notably straight-line
drawings, poly-line drawings, orthogonal graph drawings and visibility
representations. In this note, we show that many of these drawings can be
transformed from one style to another without changing the height of the
drawing. We then give some applications of these transformations."
"We initiate the study of the following problem: Given a non-planar graph G
and a planar subgraph S of G, does there exist a straight-line drawing {\Gamma}
of G in the plane such that the edges of S are not crossed in {\Gamma} by any
edge of G? We give positive and negative results for different kinds of
connected spanning subgraphs S of G. Moreover, in order to enlarge the subset
of instances that admit a solution, we consider the possibility of bending the
edges of G not in S; in this setting we discuss different trade-offs between
the number of bends and the required drawing area."
"We define strict confluent drawing, a form of confluent drawing in which the
existence of an edge is indicated by the presence of a smooth path through a
system of arcs and junctions (without crossings), and in which such a path, if
it exists, must be unique. We prove that it is NP-complete to determine whether
a given graph has a strict confluent drawing but polynomial to determine
whether it has an outerplanar strict confluent drawing with a fixed vertex
ordering (a drawing within a disk, with the vertices placed in a given order on
the boundary)."
"We present an algorithm for producing Delaunay triangulations of manifolds.
The algorithm can accommodate abstract manifolds that are not presented as
submanifolds of Euclidean space. Given a set of sample points and an atlas on a
compact manifold, a manifold Delaunay complex is produced provided the
transition functions are bi-Lipschitz with a constant close to 1, and the
sample points meet a local density requirement; no smoothness assumptions are
required. If the transition functions are smooth, the output is a triangulation
of the manifold.
  The output complex is naturally endowed with a piecewise flat metric which,
when the original manifold is Riemannian, is a close approximation of the
original Riemannian metric. In this case the ouput complex is also a Delaunay
triangulation of its vertices with respect to this piecewise flat metric."
"The order type of a point set in $R^d$ maps each $(d{+}1)$-tuple of points to
its orientation (e.g., clockwise or counterclockwise in $R^2$). Two point sets
$X$ and $Y$ have the same order type if there exists a mapping $f$ from $X$ to
$Y$ for which every $(d{+}1)$-tuple $(a_1,a_2,\ldots,a_{d+1})$ of $X$ and the
corresponding tuple $(f(a_1),f(a_2),\ldots,f(a_{d+1}))$ in $Y$ have the same
orientation. In this paper we investigate the complexity of determining whether
two point sets have the same order type. We provide an $O(n^d)$ algorithm for
this task, thereby improving upon the $O(n^{\lfloor{3d/2}\rfloor})$ algorithm
of Goodman and Pollack (1983). The algorithm uses only order type queries and
also works for abstract order types (or acyclic oriented matroids). Our
algorithm is optimal, both in the abstract setting and for realizable points
sets if the algorithm only uses order type queries."
"For a set of $n$ points in the plane, this paper presents simple kinetic data
structures (KDS's) for solutions to some fundamental proximity problems,
namely, the all nearest neighbors problem, the closest pair problem, and the
Euclidean minimum spanning tree (EMST) problem. Also, the paper introduces
KDS's for maintenance of two well-studied sparse proximity graphs, the Yao
graph and the Semi-Yao graph.
  We use sparse graph representations, the Pie Delaunay graph and the
Equilateral Delaunay graph, to provide new solutions for the proximity
problems. Then we design KDS's that efficiently maintain these sparse graphs on
a set of $n$ moving points, where the trajectory of each point is assumed to be
an algebraic function of constant maximum degree $s$. We use the kinetic Pie
Delaunay graph and the kinetic Equilateral Delaunay graph to create KDS's for
maintenance of the Yao graph, the Semi-Yao graph, all the nearest neighbors,
the closest pair, and the EMST. Our KDS's use $O(n)$ space and $O(n\log n)$
preprocessing time.
  We provide the first KDS's for maintenance of the Semi-Yao graph and the Yao
graph. Our KDS processes $O(n^2\beta_{2s+2}(n))$ (resp.
$O(n^3\beta_{2s+2}^2(n)\log n)$) events to maintain the Semi-Yao graph (resp.
the Yao graph); each event can be processed in time $O(\log n)$ in an amortized
sense. Here, $\beta_s(n)$ is an extremely slow-growing function.
  Our KDS for maintenance of all the nearest neighbors and the closest pair
processes $O(n^2\beta^2_{2s+2}(n)\log n)$ events. For maintenance of the EMST,
our KDS processes $O(n^3\beta_{2s+2}^2(n)\log n)$ events. For all three of
these problems, each event can be handled in time $O(\log n)$ in an amortized
sense.
  We improve the previous randomized kinetic algorithm for maintenance of all
the nearest neighbors by Agarwal, Kaplan, and Sharir, and the previous EMST KDS
by Rahmati and Zarei."
"Multidimensional scaling (MDS) is a family of methods that embed a given set
of points into a simple, usually flat, domain. The points are assumed to be
sampled from some metric space, and the mapping attempts to preserve the
distances between each pair of points in the set. Distances in the target space
can be computed analytically in this setting. Generalized MDS is an extension
that allows mapping one metric space into another, that is, multidimensional
scaling into target spaces in which distances are evaluated numerically rather
than analytically. Here, we propose an efficient approach for computing such
mappings between surfaces based on their natural spectral decomposition, where
the surfaces are treated as sampled metric-spaces. The resulting spectral-GMDS
procedure enables efficient embedding by implicitly incorporating smoothness of
the mapping into the problem, thereby substantially reducing the complexity
involved in its solution while practically overcoming its non-convex nature.
The method is compared to existing techniques that compute dense correspondence
between shapes. Numerical experiments of the proposed method demonstrate its
efficiency and accuracy compared to state-of-the-art approaches."
"We study a geometric representation problem, where we are given a set $\cal
R$ of axis-aligned rectangles with fixed dimensions and a graph with vertex set
$\cal R$. The task is to place the rectangles without overlap such that two
rectangles touch if and only if the graph contains an edge between them. We
call this problem Contact Representation of Word Networks (CROWN). It
formalizes the geometric problem behind drawing word clouds in which
semantically related words are close to each other. Here, we represent words by
rectangles and semantic relationships by edges. We show that CROWN is strongly
NP-hard even restricted trees and weakly NP-hard if restricted stars. We
consider the optimization problem Max-CROWN where each adjacency induces a
certain profit and the task is to maximize the sum of the profits. For this
problem, we present constant-factor approximations for several graph classes,
namely stars, trees, planar graphs, and graphs of bounded degree. Finally, we
evaluate the algorithms experimentally and show that our best method improves
upon the best existing heuristic by 45%."
"We consider the problem of taking an opaque forest and determining the
regions that are covered by it. We provide a tight upper bound on the
complexity of this problem, and an algorithm for computing this area, which is
worst-case optimal."
"Let $\phi$ be a function that maps any non-empty subset $A$ of $\mathbb{R}^2$
to a non-empty subset $\phi(A)$ of $\mathbb{R}^2$. A $\phi$-cover of a set
$T=\{T_1, T_2, \dots, T_m\}$ of pairwise non-crossing trees in the plane is a
set of pairwise disjoint connected regions such that each tree $T_i$ is
contained in some region of the cover, and each region of the cover is either
(1) $\phi(T_i)$ for some $i$, or (2) $\phi(A \cup B)$, where $A$ and $B$ are
constructed by either (1) or (2), and $A \cap B \neq \emptyset$.
  We present two properties for the function $\phi$ that make the $\phi$-cover
well-defined. Examples for such functions $\phi$ are the convex hull and the
axis-aligned bounding box. For both of these functions $\phi$, we show that the
$\phi$-cover can be computed in $O(n\log^2n)$ time, where $n$ is the total
number of vertices of the trees in $T$."
"A bottleneck plane perfect matching of a set of $n$ points in $\mathbb{R}^2$
is defined to be a perfect non-crossing matching that minimizes the length of
the longest edge; the length of this longest edge is known as {\em bottleneck}.
The problem of computing a bottleneck plane perfect matching has been proved to
be NP-hard. We present an algorithm that computes a bottleneck plane matching
of size at least $\frac{n}{5}$ in $O(n \log^2 n)$-time. Then we extend our idea
toward an $O(n\log n)$-time approximation algorithm which computes a plane
matching of size at least $\frac{2n}{5}$ whose edges have length at most
$\sqrt{2}+\sqrt{3}$ times the bottleneck."
"We show how to reduce a general, strictly-feasible LP problem, into a min-max
problem, which can be solved by the algorithm from the third section of my
thesis."
"Given a polygon $P$, for two points $s$ and $t$ contained in the polygon,
their \emph{geodesic distance} is the length of the shortest $st$-path within
$P$. A \emph{geodesic disk} of radius $r$ centered at a point $v \in P$ is the
set of points in $P$ whose geodesic distance to $v$ is at most $r$. We present
a polynomial time $2$-approximation algorithm for finding a densest geodesic
unit disk packing in $P$. Allowing arbitrary radii but constraining the number
of disks to be $k$, we present a $4$-approximation algorithm for finding a
packing in $P$ with $k$ geodesic disks whose minimum radius is maximized. We
then turn our focus on \emph{coverings} of $P$ and present a $2$-approximation
algorithm for covering $P$ with $k$ geodesic disks whose maximal radius is
minimized. Furthermore, we show that all these problems are $\mathsf{NP}$-hard
in polygons with holes. Lastly, we present a polynomial time exact algorithm
which covers a polygon with two geodesic disks of minimum maximal radius."
"Given N points in the plane $P_1 P_2...P_N$ and a location $\Omega$, the
union of discs with diameters $[\Omega P_i], i = 1, 2,...N$ covers the convex
hull of the points. The location $\Omega_s$ minimizing the area covered by the
union of discs, is shown to be the Steiner center of the convex hull of the
points. Similar results for $d$-dimensional Euclidean space are conjectured."
"The Euclidean TSP with neighborhoods (TSPN) is the following problem: Given a
set R of k regions, find a shortest tour that visits at least one point from
each region. We study the special cases of disjoint, connected, alpha-fat
regions (i.e., every region P contains a disk of diameter diam(P)/alpha) and
disjoint unit disks.
  For the latter, Dumitrescu and Mitchell proposed an algorithm based on
Mitchell's guillotine subdivision approach for the Euclidean TSP and claimed it
to be a PTAS. However, their proof contains a severe gap, which we will close
in the following. Bodlaender et al. remark that their techniques for the
minimum corridor connection problem carry over to the TSPN and yield an
alternative PTAS for this problem.
  For disjoint connected alpha-fat regions of varying size, Mitchell proposed a
slightly different PTAS candidate. We will expose several further problems and
gaps in this approach. Some of them we can close, but overall, for alpha-fat
regions, the existence of a PTAS for the TSPN remains open."
"Let $P$ be a set of $n$ points in $\mathbb{R}^d$ and $\mathcal{F}$ be a
family of geometric objects. We call a point $x \in P$ a strong centerpoint of
$P$ w.r.t $\mathcal{F}$ if $x$ is contained in all $F \in \mathcal{F}$ that
contains more than $cn$ points from $P$, where $c$ is a fixed constant. A
strong centerpoint does not exist even when $\mathcal{F}$ is the family of
halfspaces in the plane. We prove the existence of strong centerpoints with
exact constants for convex polytopes defined by a fixed set of orientations. We
also prove the existence of strong centerpoints for abstract set systems with
bounded intersection."
"A set of points and a positive integer $m$ are given and our goal is to cover
the maximum number of these point with $m$ disks. We devise the first output
sensitive algorithm for this problem. We introduce a parameter $\rho$ as the
maximum number of points that one disk can cover. In this paper first we solve
the problem for $m=2$ in $O({n\rho} + {\rho ^3}\log \rho ))$ time. The previous
algorithm for this problem runs in $O({n^3}\log n)$ time. Our algorithm
outperforms the previous algorithm because $\rho$ is much smaller than $n$ in
many cases. Then we extend the algorithm for any value of $m$ and we solve the
problem in $O(m{n\rho} + {(m\rho )^{2m - 1}}\log m\rho )$ time. The previous
algorithm for this problem runs in $O({n^{2m - 1}}\log n)$ time. Our algorithm
runs faster than the previous algorithm because $m\rho$ is smaller than $n$ in
many cases. Our technique to obtain an output sensitive algorithm is to use a
greedy algorithm to confine the areas that we should search to obtain the
result. Our technique in this paper may be applicable in other set covering
problems that deploy a greedy algorithm, to obtain faster solutions."
"Nonlinear dimensionality reduction methods have demonstrated top-notch
performance in many pattern recognition and image classification tasks. Despite
their popularity, they suffer from highly expensive time and memory
requirements, which render them inapplicable to large-scale datasets. To
leverage such cases we propose a new method called ""Path-Based Isomap"". Similar
to Isomap, we exploit geodesic paths to find the low-dimensional embedding.
However, instead of preserving pairwise geodesic distances, the low-dimensional
embedding is computed via a path-mapping algorithm. Due to the much fewer
number of paths compared to number of data points, a significant improvement in
time and memory complexity without any decline in performance is achieved. The
method demonstrates state-of-the-art performance on well-known synthetic and
real-world datasets, as well as in the presence of noise."
"Let $P=B\cup R$ be a set of $2n$ points in general position, where $B$ is a
set of $n$ blue points and $R$ a set of $n$ red points. A \emph{$BR$-matching}
is a plane geometric perfect matching on $P$ such that each edge has one red
endpoint and one blue endpoint. Two $BR$-matchings are compatible if their
union is also plane.
  The \emph{transformation graph of $BR$-matchings} contains one node for each
$BR$-matching and an edge joining two such nodes if and only if the
corresponding two $BR$-matchings are compatible. In SoCG 2013 it has been shown
by Aloupis, Barba, Langerman, and Souvaine that this transformation graph is
always connected, but its diameter remained an open question. In this paper we
provide an alternative proof for the connectivity of the transformation graph
and prove an upper bound of $2n$ for its diameter, which is asymptotically
tight."
"For a polyhedron $P$ in $\mathbb{R}^d$, denote by $|P|$ its combinatorial
complexity, i.e., the number of faces of all dimensions of the polyhedra. In
this paper, we revisit the classic problem of preprocessing polyhedra
independently so that given two preprocessed polyhedra $P$ and $Q$ in
$\mathbb{R}^d$, each translated and rotated, their intersection can be tested
rapidly.
  For $d=3$ we show how to perform such a test in $O(\log |P| + \log |Q|)$ time
after linear preprocessing time and space. This running time is the best
possible and improves upon the last best known query time of $O(\log|P|
\log|Q|)$ by Dobkin and Kirkpatrick (1990).
  We then generalize our method to any constant dimension $d$, achieving the
same optimal $O(\log |P| + \log |Q|)$ query time using a representation of size
$O(|P|^{\lfloor d/2\rfloor + \varepsilon})$ for any $\varepsilon>0$ arbitrarily
small. This answers an even older question posed by Dobkin and Kirkpatrick 30
years ago.
  In addition, we provide an alternative $O(\log |P| + \log |Q|)$ algorithm to
test the intersection of two convex polygons $P$ and $Q$ in the plane."
"Mixed elements meshes based on the modified octree approach contain several
co-spherical point configurations. While generating Delaunay tessellations to
be used together with the finite volume method, it is not necessary to
partition them into tetrahedra; co-spherical elements can be used as final
elements. This paper presents a study of all co-spherical elements that appear
while tessellating a 1-irregular cuboid (cuboid with at most one Steiner point
on its edges) with different aspect ratio. Steiner points can be located at any
position between the edge endpoints. When Steiner points are located at edge
midpoints, 24 co-spherical elements appear while tessellating 1-irregular
cubes. By inserting internal faces and edges to these new elements, this number
is reduced to 13. When 1-irregular cuboids with aspect ratio equal to
$\sqrt{2}$ are tessellated, 10 co-spherical elements are required. If
1-irregular cuboids have aspect ratio between 1 and $\sqrt{2}$, all the
tessellations are adequate for the finite volume method. When Steiner points
are located at any position, the study was done for a specific Steiner point
distribution on a cube. 38 co-spherical elements were required to tessellate
all the generated 1-irregular cubes. Statistics about the impact of each new
element in the tessellations of 1-irregular cuboids are also included. This
study was done by developing an algorithm that construct Delaunay tessellations
by starting from a Delaunay tetrahedral mesh built by Qhull."
"We describe how to approximate, in quasi-polynomial time, the largest
independent set of polygons, in a given set of polygons. Our algorithm works by
extending the result of Adamaszek and Wiese \cite{aw-asmwi-13, aw-qmwis-14} to
polygons of arbitrary complexity. Surprisingly, the algorithm also works or
computing the largest subset of the given set of polygons that has some
sparsity condition. For example, we show that one can approximate the largest
subset of polygons, such that the intersection graph of the subset does not
contain a cycle of length $4$ (i.e., $K_{2,2}$)."
"In this note, we introduce a family of bipartite graphs called path
restricted ordered bipartite graphs and present it as an abstract
generalization of some well known geometric graphs like unit distance graphs on
convex point sets. In the framework of convex point sets, we also focus on a
generalized version of Gabriel graphs known as locally Gabriel graphs or
$LGGs$. $LGGs$ can also be seen as the generalization of unit distance graphs.
The path restricted ordered bipartite graph is also a generalization of $LGGs$.
We study some structural properties of the path restricted ordered bipartite
graphs and also show that such graphs have the maximum edge complexity of
$\theta(n \log n)$. It gives an alternate proof to the well known result that
$UDGs$ and $LGGs$ on convex points have $O(n \log n)$ edges."
"Given a set $P$ of $n$ points in the plane, {\sc Covering Points by Lines} is
the problem of finding a minimum-cardinality set $\L$ of lines such that every
point $p \in P$ is incident to some line $\ell \in \L$. As a geometric variant
of {\sc Set Cover}, {\sc Covering Points by Lines} is still NP-hard. Moreover,
it has been proved to be APX-hard, and hence does not admit any polynomial-time
approximation scheme unless P $=$ NP\@. In contrast to the small constant
approximation lower bound implied by APX-hardness, the current best
approximation ratio for {\sc Covering Points by Lines} is still $O(\log n)$,
namely the ratio achieved by the greedy algorithm for {\sc Set Cover}.
  In this paper, we give a lower bound of $\Omega(\log n)$ on the approximation
ratio of the greedy algorithm for {\sc Covering Points by Lines}. We also study
several related problems including {\sc Maximum Point Coverage by Lines}, {\sc
Minimum-Link Covering Tour}, {\sc Minimum-Link Spanning Tour}, and {\sc
Min-Max-Turn Hamiltonian Tour}. We show that all these problems are either
APX-hard or at least NP-hard. In particular, our proof of APX-hardness of {\sc
Min-Max-Turn Hamiltonian Tour} sheds light on the difficulty of {\sc
Bounded-Turn-Minimum-Length Hamiltonian Tour}, a problem proposed by Aggarwal
et al.\ at SODA 1997."
"In \emph{smooth orthogonal layouts} of planar graphs, every edge is an
alternating sequence of axis-aligned segments and circular arcs with common
axis-aligned tangents. In this paper, we study the problem of finding smooth
orthogonal layouts of low \emph{edge complexity}, that is, with few segments
per edge. We say that a graph has \emph{smooth complexity} k---for short, an
SC_k-layout---if it admits a smooth orthogonal drawing of edge complexity at
most $k$.
  Our main result is that every 4-planar graph has an SC_2-layout. While our
drawings may have super-polynomial area, we show that, for 3-planar graphs,
cubic area suffices. Further, we show that every biconnected 4-outerplane graph
admits an SC_1-layout. On the negative side, we demonstrate an infinite family
of biconnected 4-planar graphs that requires exponential area for an
SC_1-layout. Finally, we present an infinite family of biconnected 4-planar
graphs that does not admit an SC_1-layout."
"A closed-form solution for the boundary of the flat state of an orthogonal
cross section of contiguous surface geometry formed by the intersection of two
cylinders of equal radii oriented in dual directions of rotation about their
intersecting axes."
"In this paper, we show that the $L_1$ geodesic diameter and center of a
simple polygon can be computed in linear time. For the purpose, we focus on
revealing basic geometric properties of the $L_1$ geodesic balls, that is, the
metric balls with respect to the $L_1$ geodesic distance. More specifically, in
this paper we show that any family of $L_1$ geodesic balls in any simple
polygon has Helly number two, and the $L_1$ geodesic center consists of
midpoints of shortest paths between diametral pairs. These properties are
crucial for our linear-time algorithms, and do not hold for the Euclidean case."
"In the Hausdorff Voronoi diagram of a family of \emph{clusters of points} in
the plane, the distance between a point $t$ and a cluster $P$ is measured as
the maximum distance between $t$ and any point in $P$, and the diagram is
defined in a nearest-neighbor sense for the input clusters. In this paper we
consider %El.""non-crossing"" \emph{non-crossing} clusters in the plane, for
which the combinatorial complexity of the Hausdorff Voronoi diagram is linear
in the total number of points, $n$, on the convex hulls of all clusters. We
present a randomized incremental construction, based on point location, that
computes this diagram in expected $O(n\log^2{n})$ time and expected $O(n)$
space. Our techniques efficiently handle non-standard characteristics of
generalized Voronoi diagrams, such as sites of non-constant complexity, sites
that are not enclosed in their Voronoi regions, and empty Voronoi regions. The
diagram finds direct applications in VLSI computer-aided design."
"Let $P$ be a set of $n$ points in ${\mathbb R}^{d}$. A point $p \in P$ is
$k$\emph{-shallow} if it lies in a halfspace which contains at most $k$ points
of $P$ (including $p$). We show that if all points of $P$ are $k$-shallow, then
$P$ can be partitioned into $\Theta(n/k)$ subsets, so that any hyperplane
crosses at most $O((n/k)^{1-1/(d-1)} \log^{2/(d-1)}(n/k))$ subsets. Given such
a partition, we can apply the standard construction of a spanning tree with
small crossing number within each subset, to obtain a spanning tree for the
point set $P$, with crossing number $O(n^{1-1/(d-1)}k^{1/d(d-1)}
\log^{2/(d-1)}(n/k))$. This allows us to extend the construction of Har-Peled
and Sharir \cite{hs11} to three and higher dimensions, to obtain, for any set
of $n$ points in ${\mathbb R}^{d}$ (without the shallowness assumption), a
spanning tree $T$ with {\em small relative crossing number}. That is, any
hyperplane which contains $w \leq n/2$ points of $P$ on one side, crosses
$O(n^{1-1/(d-1)}w^{1/d(d-1)} \log^{2/(d-1)}(n/w))$ edges of $T$. Using a
similar mechanism, we also obtain a data structure for halfspace range
counting, which uses $O(n \log \log n)$ space (and somewhat higher
preprocessing cost), and answers a query in time $O(n^{1-1/(d-1)}k^{1/d(d-1)}
(\log (n/k))^{O(1)})$, where $k$ is the output size."
"Consider an orthogonal polyhedron, i.e., a polyhedron where (at least after a
suitable rotation) all faces are perpendicular to a coordinate axis, and hence
all edges are parallel to a coordinate axis. Clearly, any facial angle and any
dihedral angle is a multiple of $\pi/2$.
  In this note we explore the converse: if the facial and/or dihedral angles
are all multiples of $\pi /2$, is the polyhedron necessarily orthogonal? The
case of facial angles was answered previously. In this note we show that if
both the facial and dihedral angles are multiples of $\pi /2$ then the
polyhedron is orthogonal (presuming connectivity), and we give examples to show
that the condition for dihedral angles alone does not suffice."
"A {\beta}-skeleton is a proximity graphs with node neighbourhood defined by
continuous-valued parameter {\beta}. Two nodes in a {\beta}-skeleton are
connected by an edge if their lune-based neighbourhood contains no other nodes.
With increase of {\beta} some edges a skeleton are disappear. We study how a
number of edges in {\beta}-skeleton depends on {\beta}. We speculate how this
dependence can be used to discriminate between random and non-random planar
sets. We also analyse stability of {\beta}-skeletons and their sensitivity to
perturbations."
"A space-filling function is a bijection from the unit line segment to the
unit square, cube, or hypercube. The function from the unit line segment is
continuous. The inverse function, while well-defined, is not continuous.
Space-filling curves, the finite approximations to space-filling functions,
have found application in global optimization, database indexing, and dimension
reduction among others. For these applications the desired transforms are
mapping a scalar to multidimensional coordinates and mapping multidimensional
coordinates to a scalar.
  Presented are recurrences which produce space-filling functions and curves of
any rank $d\ge2$ based on serpentine Hamiltonian paths on $({\bf Z}\bmod s)^d$
where $s\ge2$. The recurrences for inverse space-filling functions are also
presented. Both Peano and Hilbert curves and functions and their
generalizations to higher dimensions are produced by these recurrences. The
computations of these space-filling functions and their inverse functions are
absolutely convergent geometric series.
  The space-filling functions are constructed as limits of integer recurrences
and equivalently as non-terminating real recurrences. Scaling relations are
given which enable the space-filling functions and curves and their inverses to
extend beyond the unit area or volume and even to all of $d$-space.
  This unification of pandimensional space-filling curves facilitates
quantitative comparison of curves generated from different Hamiltonian paths.
The isotropy and performance in dimension reduction of a variety of
space-filling curves are analyzed.
  For dimension reduction it is found that Hilbert curves perform somewhat
better than Peano curves and their isotropic variants."
"The existing combinatorial methods for iso-surface computation are efficient
for pure visualization purposes, but it is known that the resulting
iso-surfaces can have holes, and topological problems like missing or wrong
connectivity can appear. To avoid such problems, we introduce a
graph-theoretical method for the computation of iso-surfaces on cuboid meshes
in $\mathbb{R}^3$. The method for the generation of iso-surfaces employs
labeled cuboid graphs $G(V,E,\mathcal{F})$ such that $V$ is the set of vertices
of a cuboid $C\subset\mathbb{R}^3$, $E$ is the set of edges of $C$ and
$\mathcal{F}\,:\,V\rightarrow [0,1]$. The nodes of $G$ are weighted by the
values of $\mathcal{F}$ which represents the volumetric information, e.g.\ from
a Volume of Fluid method. Using a given iso-level $c\in (0,1)$, we first obtain
all iso-points, i.e.\ points where the value $c$ is attained by the
edge-interpolated $\mathcal{F}$-field. The iso-surface is then built from
iso-elements which are composed of triangles and are such that their polygonal
boundary has only iso-points as vertices. All vertices lie on the faces of a
single mesh cell.
  We give a proof that the generated iso-surface is connected up to the
boundary of the domain and it can be decomposed into different oriented
components. Two different components may have discrete points or line segments
in common. The graph-theoretical method for the computation of iso-surfaces
developed in this paper enables to recover local information of the iso-surface
that can be used e.g.\ to compute discrete mean curvature and to solve surface
PDEs. Concerning the computational effort, the resulting algorithm is as
efficient as existing combinatorial methods."
"We study the problem of reconstructing a convex body using only a finite
number of measurements of outer normal vectors. More precisely, we suppose that
the normal vectors are measured at independent random locations uniformly
distributed along the boundary of our convex set. Given a desired Hausdorff
error eta, we provide an upper bounds on the number of probes that one has to
perform in order to obtain an eta-approximation of this convex set with high
probability. Our result rely on the stability theory related to Minkowski's
theorem."
"Map construction methods automatically produce and/or update road network
datasets using vehicle tracking data. Enabled by the ubiquitous generation of
georeferenced tracking data, there has been a recent surge in map construction
algorithms coming from different computer science domains. A cross-comparison
of the various algorithms is still very rare, since (i) algorithms and
constructed maps are generally not publicly available and (ii) there is no
standard approach to assess the result quality, given the lack of benchmark
data and quantitative evaluation methods. This work represents a first
comprehensive attempt to benchmark map construction algorithms. We provide an
evaluation and comparison of seven algorithms using four datasets and four
different evaluation measures. In addition to this comprehensive comparison, we
make our datasets, source code of map construction algorithms and evaluation
measures publicly available on mapconstruction.org. This site has been
established as a repository for map con- struction data and algorithms and we
invite other researchers to contribute by uploading code and benchmark data
supporting their contributions to map construction algorithms."
"It is shown that every simple polygon in general position with $n$ walls can
be illuminated from a single point light source $s$ after at most $\lfloor
(n-2)/4\rfloor$ diffuse reflections, and this bound is the best possible. A
point $s$ with this property can be computed in $O(n\log n)$ time. It is also
shown that the minimum number of diffuse reflections needed to illuminate a
given simple polygon from a single point can be approximated up to an additive
constant in polynomial time."
"Let $p$ and $q$ be two imprecise points, given as probability density
functions on $\mathbb R^2$, and let $\cal R$ be a set of $n$ line segments
(obstacles) in $\mathbb R^2$. We study the problem of approximating the
probability that $p$ and $q$ can see each other; that is, that the segment
connecting $p$ and $q$ does not cross any segment of $\cal R$. To solve this
problem, we approximate each density function by a weighted set of polygons; a
novel approach to dealing with probability density functions in computational
geometry."
"For a common class of 2D mechanisms called 1-dof tree decomposable linkages,
we present a software CayMos which uses new theoretical results to implement
efficient algorithmic solutions for: (a) meaningfully representing and
visualizing the connected components in the Euclidean realization space; (b)
finding a path of continuous motion between two realizations in the same
connected component, with or without restricting the realization type
(sometimes called orientation type); (c) finding two ``closest'' realizations
in different connected components."
"Weighted geometric set-cover problems arise naturally in several geometric
and non-geometric settings (e.g. the breakthrough of Bansal-Pruhs (FOCS 2010)
reduces a wide class of machine scheduling problems to weighted geometric
set-cover). More than two decades of research has succeeded in settling the
$(1+\epsilon)$-approximability status for most geometric set-cover problems,
except for four basic scenarios which are still lacking. One is that of
weighted disks in the plane for which, after a series of papers, Varadarajan
(STOC 2010) presented a clever \emph{quasi-sampling} technique, which together
with improvements by Chan \etal~(SODA 2012), yielded a $O(1)$-approximation
algorithm. Even for the unweighted case, a PTAS for a fundamental class of
objects called pseudodisks (which includes disks, unit-height rectangles,
translates of convex sets etc.) is currently unknown. Another fundamental case
is weighted halfspaces in $\Re^3$, for which a PTAS is currently lacking. In
this paper, we present a QPTAS for all of these remaining problems. Our results
are based on the separator framework of Adamaszek-Wiese (FOCS 2013, SODA 2014),
who recently obtained a QPTAS for weighted independent set of polygonal
regions. This rules out the possibility that these problems are APX-hard,
assuming $\textbf{NP} \nsubseteq \textbf{DTIME}(2^{polylog(n)})$. Together with
the recent work of Chan-Grant (CGTA 2014), this settles the APX-hardness status
for all natural geometric set-cover problems."
"In the original Art Gallery Problem (AGP), one seeks the minimum number of
guards required to cover a polygon $P$. We consider the Chromatic AGP (CAGP),
where the guards are colored. As long as $P$ is completely covered, the number
of guards does not matter, but guards with overlapping visibility regions must
have different colors. This problem has applications in landmark-based mobile
robot navigation: Guards are landmarks, which have to be distinguishable (hence
the colors), and are used to encode motion primitives, \eg, ""move towards the
red landmark"". Let $\chi_G(P)$, the chromatic number of $P$, denote the minimum
number of colors required to color any guard cover of $P$. We show that
determining, whether $\chi_G(P) \leq k$ is \NP-hard for all $k \geq 2$. Keeping
the number of colors minimal is of great interest for robot navigation, because
less types of landmarks lead to cheaper and more reliable recognition."
"It is proved that the total length of any set of countably many rectifiable
curves, whose union meets all straight lines that intersect the unit square U,
is at least 2.00002. This is the first improvement on the lower bound of 2
established by Jones in 1964. A similar bound is proved for all convex sets U
other than a triangle."
"Determining visibility in planar polygons and arrangements is an important
subroutine for many algorithms in computational geometry. In this paper, we
report on new implementations, and corresponding experimental evaluations, for
two established and one novel algorithm for computing visibility polygons.
These algorithms will be released to the public shortly, as a new package for
the Computational Geometry Algorithms Library (CGAL)."
"Let E be the complete Euclidean graph on a set of points embedded in the
plane. Given a constant t >= 1, a spanning subgraph G of E is said to be a
t-spanner, or simply a spanner, if for any pair of vertices u,v in E the
distance between u and v in G is at most t times their distance in E. A spanner
is plane if its edges do not cross.
  This paper considers the question: ""What is the smallest maximum degree that
can always be achieved for a plane spanner of E?"" Without the planarity
constraint, it is known that the answer is 3 which is thus the best known lower
bound on the degree of any plane spanner. With the planarity requirement, the
best known upper bound on the maximum degree is 6, the last in a long sequence
of results improving the upper bound. In this paper we show that the complete
Euclidean graph always contains a plane spanner of maximum degree at most 4 and
make a big step toward closing the question. Our construction leads to an
efficient algorithm for obtaining the spanner from Chew's L1-Delaunay
triangulation."
"We present a fast algorithm for global rigid symmetry detection with
approximation guarantees. The algorithm is guaranteed to find the best
approximate symmetry of a given shape, to within a user-specified threshold,
with very high probability. Our method uses a carefully designed sampling of
the transformation space, where each transformation is efficiently evaluated
using a sub-linear algorithm. We prove that the density of the sampling depends
on the total variation of the shape, allowing us to derive formal bounds on the
algorithm's complexity and approximation quality. We further investigate
different volumetric shape representations (in the form of truncated distance
transforms), and in such a way control the total variation of the shape and
hence the sampling density and the runtime of the algorithm. A comprehensive
set of experiments assesses the proposed method, including an evaluation on the
eight categories of the COSEG data-set. This is the first large-scale
evaluation of any symmetry detection technique that we are aware of."
"A recent result of Chepoi, Estellon and Vaxes [DCG '07] states that any
planar graph of diameter at most 2R can be covered by a constant number of
balls of size R; put another way, there are a constant-sized subset of vertices
within which every other vertex is distance half the diameter. We generalize
this result to graphs embedded on surfaces of fixed genus with a fixed number
of apices, making progress toward the conjecture that graphs excluding a fixed
minor can also be covered by a constant number of balls. To do so, we develop
two tools which may be of independent interest. The first gives a bound on the
density of graphs drawn on a surface of genus $g$ having a limit on the number
of pairwise-crossing edges. The second bounds the size of a non-contractible
cycle in terms of the Euclidean norm of the degree sequence of a graph embedded
on surface."
"Given two point sets in the plane, we study the minimization of the
bottleneck distance between a point set B and an equally-sized subset of a
point set A under translations. We relate this problem to a Voronoi-type
diagram and derive polynomial bounds for its complexity that are optimal in the
size of A. We devise efficient algorithms for the construction of such a
diagram and its lexicographic variant, which generalize to higher dimensions.
We use the diagram to find an optimal bottleneck matching under translations,
to compute a connecting path of minimum bottleneck cost between two positions
of B, and to determine the maximum bottleneck cost in a convex polygon."
"We consider the following geometric optimization problem: find a maximum-area
rectangle and a maximum-perimeter rectangle contained in a given convex polygon
with $n$ vertices. We give exact algorithms that solve these problems in time
$O(n^3)$. We also give $(1-\varepsilon)$-approximation algorithms that take
time $O(\varepsilon^{-3/2}+ \varepsilon^{-1/2} \log n)$."
"Let $\D$ be a set of $n$ pairwise disjoint unit balls in $\R^d$ and $P$ the
set of their center points. A hyperplane $\Hy$ is an \emph{$m$-separator} for
$\D$ if each closed halfspace bounded by $\Hy$ contains at least $m$ points
from $P$. This generalizes the notion of halving hyperplanes, which correspond
to $n/2$-separators. The analogous notion for point sets has been well studied.
Separators have various applications, for instance, in divide-and-conquer
schemes. In such a scheme any ball that is intersected by the separating
hyperplane may still interact with both sides of the partition. Therefore it is
desirable that the separating hyperplane intersects a small number of balls
only. We present three deterministic algorithms to bisect or approximately
bisect a given set of disjoint unit balls by a hyperplane: Firstly, we present
a simple linear-time algorithm to construct an $\alpha n$-separator for balls
in $\R^d$, for any $0<\alpha<1/2$, that intersects at most $cn^{(d-1)/d}$
balls, for some constant $c$ that depends on $d$ and $\alpha$. The number of
intersected balls is best possible up to the constant $c$. Secondly, we present
a near-linear time algorithm to construct an $(n/2-o(n))$-separator in $\R^d$
that intersects $o(n)$ balls. Finally, we give a linear-time algorithm to
construct a halving line in $\R^2$ that intersects $O(n^{(5/6)+\epsilon})$
disks.
  Our results improve the runtime of a disk sliding algorithm by Bereg,
Dumitrescu and Pach. In addition, our results improve and derandomize an
algorithm to construct a space decomposition used by L{\""o}ffler and Mulzer to
construct an onion (convex layer) decomposition for imprecise points (any point
resides at an unknown location within a given disk)."
"Can folding a piece of paper flat make it larger? We explore whether a shape
$S$ must be scaled to cover a flat-folded copy of itself. We consider both
single folds and arbitrary folds (continuous piecewise isometries $S\rightarrow
R^2$). The underlying problem is motivated by computational origami, and is
related to other covering and fixturing problems, such as Lebesgue's universal
cover problem and force closure grasps. In addition to considering special
shapes (squares, equilateral triangles, polygons and disks), we give upper and
lower bounds on scale factors for single folds of convex objects and arbitrary
folds of simply connected objects."
"An effective strategy for accelerating the calculation of convex hulls for
point sets is to filter the input points by discarding interior points. In this
paper, we present such a straightforward and efficient preprocessing approach
by exploiting the GPU. The basic idea behind our approach is to discard the
points that locate inside a convex polygon formed by 16 extreme points. Due to
the fact that the extreme points of a point set do not alter when all points
are rotated in the same angle, four groups of extreme points with min or max x
or y coordinates can be found in the original point set and three rotated point
sets. These 16 extreme points are then used to form a convex polygon. We check
all input points and discard the points that locate inside the convex polygon.
We use the remaining points to calculate the expected convex hull. Experimental
results show that: when employing the proposed preprocessing algorithm, it
achieves the speedups of about 4x ~5x on average and 5x ~ 6x in the best cases
over the cases where the proposed approach is not used. In addition, more than
99% input points can be discarded in most experimental tests."
"Surprisingly, the order-$k$ Voronoi diagram of line segments had received no
attention in the computational-geometry literature. It illustrates properties
surprisingly different from its counterpart for points; for example, a single
order-$k$ Voronoi region may consist of $\Omega(n)$ disjoint faces. We analyze
the structural properties of this diagram and show that its combinatorial
complexity for $n$ non-crossing line segments is $O(k(n-k))$, despite the
disconnected regions. The same bound holds for $n$ intersecting line segments,
when $k\geq n/2$. We also consider the order-$k$ Voronoi diagram of line
segments that form a planar straight-line graph, and augment the definition of
an order-$k$ Voronoi diagram to cover non-disjoint sites, addressing the issue
of non-uniqueness for $k$-nearest sites. Furthermore, we enhance the iterative
approach to construct this diagram. All bounds are valid in the general $L_p$
metric, $1\leq p\leq \infty$. For non-crossing segments in the $L_\infty$ and
$L_1$ metrics, we show a tighter $O((n-k)^2)$ bound for $k>n/2$."
"We present a new algorithm for computing the straight skeleton of a polygon.
For a polygon with $n$ vertices, among which $r$ are reflex vertices, we give a
deterministic algorithm that reduces the straight skeleton computation to a
motorcycle graph computation in $O(n (\log n)\log r)$ time. It improves on the
previously best known algorithm for this reduction, which is randomized, and
runs in expected $O(n \sqrt{h+1}\log^2 n)$ time for a polygon with $h$ holes.
Using known motorcycle graph algorithms, our result yields improved time bounds
for computing straight skeletons. In particular, we can compute the straight
skeleton of a non-degenerate polygon in $O(n (\log n) \log r +
r^{4/3+\varepsilon})$ time for any $\varepsilon>0$. On degenerate input, our
time bound increases to $O(n (\log n) \log r + r^{17/11+\varepsilon})$."
"We give an algorithm that reduces the straight skeleton to the motorcycle
graph in $O(n\log n)$ time for simple polygons and $O(n(\log n)\log m)$ time
for a planar straight line graph (PSLG) with $m$ connected components. This
improves on the previous best of $O(n(\log n)\log r)$ for polygons with $r$
reflex vertices (possibly with holes) and $O(n^2\log n)$ for general planar
straight line graphs. This allows us to speed up the straight skeleton
algorithm for polygons and PSLGs. For a polygon with $h$ holes and $r$ reflex
vertices we achieve a speedup from $O(n(\log n)\log r + r^{4/3+\epsilon})$ time
to $O(n(\log n)\log h + r^{4/3 + \epsilon})$ time in the non-degenerate case
and from $O(n(\log n)\log r + r^{17/11 + \epsilon})$ to $O(n(\log n)\log h +
r^{17/11 + \epsilon})$ in degenerate cases. For a PSLG with $m$ connected
components and $r$ reflex vertices, we gain a speed up from $O(n^{1 + \epsilon}
+ n^{8/11 + \epsilon}r^{9/11+\epsilon})$ to $O(n(\log n)\log m + r^{4/3 +
\epsilon})$ in the non-degenerate case and from $O(n^{1 + \epsilon} + n^{8/11 +
\epsilon}r^{9/11+\epsilon})$ to $O(n(\log n)\log m + r^{17/11 + \epsilon})$ in
the degenerate case."
"In the continuous 1.5-dimensional terrain guarding problem we are given an
$x$-monotone chain (the \emph{terrain} $T$) and ask for the minimum number of
point guards (located anywhere on $T$), such that all points of $T$ are covered
by at least one guard. It has been shown that the 1.5-dimensional terrain
guarding problem is \NP-hard. The currently best known approximation algorithm
achieves a factor of $4$. For the discrete problem version with a finite set of
guard candidates and a finite set of points on the terrain that need to be
monitored, a polynomial time approximation scheme (PTAS) has been presented
[10]. We show that for the general problem we can construct finite guard and
witness sets, $G$ and $W$, such that there exists an optimal guard cover $G^*
\subseteq G$ that covers $T$, and when these guards monitor all points in $W$
the entire terrain is guarded. This leads to a PTAS as well as an (exact) IP
formulation for the continuous terrain guarding problem."
"We show that the problem of finding the simplex of largest volume in the
convex hull of $n$ points in $\mathbb{Q}^d$ can be approximated with a factor
of $O(\log d)^{d/2}$ in polynomial time. This improves upon the previously best
known approximation guarantee of $d^{(d-1)/2}$ by Khachiyan. On the other hand,
we show that there exists a constant $c>1$ such that this problem cannot be
approximated with a factor of $c^d$, unless $P=NP$. % This improves over the
$1.09$ inapproximability that was previously known. Our hardness result holds
even if $n = O(d)$, in which case there exists a $\bar c\,^{d}$-approximation
algorithm that relies on recent sampling techniques, where $\bar c$ is again a
constant. We show that similar results hold for the problem of finding the
largest absolute value of a subdeterminant of a $d\times n$ matrix."
"Given a point set P in 2D, the problem of finding the smallest set of unit
disks that cover all of P is NP-hard. We present a simple algorithm for this
problem with an approximation factor of 25/6 in the Euclidean norm and 2 in the
max norm, by restricting the disk centers to lie on parallel lines. The run
time and space of this algorithm is O(n log n) and O(n) respectively. This
algorithm extends to any Lp norm and is asymptotically faster than known
alternative approximation algorithms for the same approximation factor."
"We consider maintaining the contour tree $\mathbb{T}$ of a piecewise-linear
triangulation $\mathbb{M}$ that is the graph of a time varying height function
$h: \mathbb{R}^2 \rightarrow \mathbb{R}$. We carefully describe the
combinatorial change in $\mathbb{T}$ that happen as $h$ varies over time and
how these changes relate to topological changes in $\mathbb{M}$. We present a
kinetic data structure that maintains the contour tree of $h$ over time. Our
data structure maintains certificates that fail only when $h(v)=h(u)$ for two
adjacent vertices $v$ and $u$ in $\mathbb{M}$, or when two saddle vertices lie
on the same contour of $\mathbb{M}$. A certificate failure is handled in
$O(\log(n))$ time. We also show how our data structure can be extended to
handle a set of general update operations on $\mathbb{M}$ and how it can be
applied to maintain topological persistence pairs of time varying functions."
"A \emph{pier fractal} is a discrete self-similar fractal whose generator
contains at least one \emph{pier}, that is, a member of the generator with
exactly one adjacent point. Tree fractals and pinch-point fractals are special
cases of pier fractals. In this paper, we study \emph{scaled pier fractals},
where a \emph{scaled fractal} is the shape obtained by replacing each point in
the original fractal by a $c \times c$ block of points, for some $c \in
\mathbb{Z}^+$. We prove that no scaled discrete self-similar pier fractal
strictly self-assembles, at any temperature, in Winfree's abstract Tile
Assembly Model."
"In a \emph{fan-planar drawing} of a graph an edge can cross only edges with a
common end-vertex. Fan-planar drawings have been recently introduced by
Kaufmann and Ueckerdt, who proved that every $n$-vertex fan-planar drawing has
at most $5n-10$ edges, and that this bound is tight for $n \geq 20$. We extend
their result, both from the combinatorial and the algorithmic point of view. We
prove tight bounds on the density of constrained versions of fan-planar
drawings and study the relationship between fan-planarity and $k$-planarity.
Furthermore, we prove that deciding whether a graph admits a fan-planar drawing
in the variable embedding setting is NP-complete."
"This paper provides the first solution to the kinetic reverse $k$-nearest
neighbor (\rknn) problem in $\mathbb{R}^d$, which is defined as follows: Given
a set $P$ of $n$ moving points in arbitrary but fixed dimension $d$, an integer
$k$, and a query point $q\notin P$ at any time $t$, report all the points $p\in
P$ for which $q$ is one of the $k$-nearest neighbors of $p$."
"Let $P$ and $Q$ be two simple polygons in the plane of total complexity $n$,
each of which can be decomposed into at most $k$ convex parts. We present an
$(1-\varepsilon)$-approximation algorithm, for finding the translation of $Q$,
which maximizes its area of overlap with $P$. Our algorithm runs in $O(c n)$
time, where $c$ is a constant that depends only on $k$ and $\varepsilon$.
  This suggest that for polygons that are ""close"" to being convex, the problem
can be solved (approximately), in near linear time."
"We study the convex-hull problem in a probabilistic setting, motivated by the
need to handle data uncertainty inherent in many applications, including sensor
databases, location-based services and computer vision. In our framework, the
uncertainty of each input site is described by a probability distribution over
a finite number of possible locations including a \emph{null} location to
account for non-existence of the point. Our results include both exact and
approximation algorithms for computing the probability of a query point lying
inside the convex hull of the input, time-space tradeoffs for the membership
queries, a connection between Tukey depth and membership queries, as well as a
new notion of $\some$-hull that may be a useful representation of uncertain
hulls."
"A fundamental problem in wireless sensor networks is to connect a given set
of sensors while minimizing the \emph{receiver interference}. This is modeled
as follows: each sensor node corresponds to a point in $\mathbb{R}^d$ and each
\emph{transmission range} corresponds to a ball. The receiver interference of a
sensor node is defined as the number of transmission ranges it lies in. Our
goal is to choose transmission radii that minimize the maximum interference
while maintaining a strongly connected asymmetric communication graph.
  For the two-dimensional case, we show that it is NP-complete to decide
whether one can achieve a receiver interference of at most $5$. In the
one-dimensional case, we prove that there are optimal solutions with nontrivial
structural properties. These properties can be exploited to obtain an exact
algorithm that runs in quasi-polynomial time. This generalizes a result by Tan
et al. to the asymmetric case."
"We consider the problem of covering the boundary of a simple polygon on n
vertices using the minimum number of geodesic unit disks. We present an O(n
\log^2 n+k) time 2-approximation algorithm for finding the centers of the
disks, with k denoting the number centers found by the algorithm."
"Let $P$ be a set of $n$ points in $\mathbb{R}^d$. In the projective
clustering problem, given $k, q$ and norm $\rho \in [1,\infty]$, we have to
compute a set $\mathcal{F}$ of $k$ $q$-dimensional flats such that $(\sum_{p\in
P}d(p, \mathcal{F})^\rho)^{1/\rho}$ is minimized; here $d(p, \mathcal{F})$
represents the (Euclidean) distance of $p$ to the closest flat in
$\mathcal{F}$. We let $f_k^q(P,\rho)$ denote the minimal value and interpret
$f_k^q(P,\infty)$ to be $\max_{r\in P}d(r, \mathcal{F})$. When $\rho=1,2$ and
$\infty$ and $q=0$, the problem corresponds to the $k$-median, $k$-mean and the
$k$-center clustering problems respectively.
  For every $0 < \epsilon < 1$, $S\subset P$ and $\rho \ge 1$, we show that the
orthogonal projection of $P$ onto a randomly chosen flat of dimension
$O(((q+1)^2\log(1/\epsilon)/\epsilon^3) \log n)$ will $\epsilon$-approximate
$f_1^q(S,\rho)$. This result combines the concepts of geometric coresets and
subspace embeddings based on the Johnson-Lindenstrauss Lemma. As a consequence,
an orthogonal projection of $P$ to an $O(((q+1)^2 \log
((q+1)/\epsilon)/\epsilon^3) \log n)$ dimensional randomly chosen subspace
$\epsilon$-approximates projective clusterings for every $k$ and $\rho$
simultaneously. Note that the dimension of this subspace is independent of the
number of clusters~$k$.
  Using this dimension reduction result, we obtain new approximation and
streaming algorithms for projective clustering problems. For example, given a
stream of $n$ points, we show how to compute an $\epsilon$-approximate
projective clustering for every $k$ and $\rho$ simultaneously using only
$O((n+d)((q+1)^2\log ((q+1)/\epsilon))/\epsilon^3 \log n)$ space. Compared to
standard streaming algorithms with $\Omega(kd)$ space requirement, our approach
is a significant improvement when the number of input points and their
dimensions are of the same order of magnitude."
"A closed curve in the plane is weakly simple if it is the limit (in the
Fr\'echet metric) of a sequence of simple closed curves. We describe an
algorithm to determine whether a closed walk of length n in a simple plane
graph is weakly simple in O(n log n) time, improving an earlier O(n^3)-time
algorithm of Cortese et al. [Discrete Math. 2009]. As an immediate corollary,
we obtain the first efficient algorithm to determine whether an arbitrary
n-vertex polygon is weakly simple; our algorithm runs in O(n^2 log n) time. We
also describe algorithms that detect weak simplicity in O(n log n) time for two
interesting classes of polygons. Finally, we discuss subtle errors in several
previously published definitions of weak simplicity."
"We study the spanning properties of Theta-Theta graphs. Similar in spirit
with the Yao-Yao graphs, Theta-Theta graphs partition the space around each
vertex into a set of k cones, for some fixed integer k > 1, and select at most
one edge per cone. The difference is in the way edges are selected. Yao-Yao
graphs select an edge of minimum length, whereas Theta-Theta graphs select an
edge of minimum orthogonal projection onto the cone bisector. It has been
established that the Yao-Yao graphs with parameter k = 6k' have spanning ratio
11.67, for k' >= 6. In this paper we establish a first spanning ratio of $7.82$
for Theta-Theta graphs, for the same values of $k$. We also extend the class of
Theta-Theta spanners with parameter 6k', and establish a spanning ratio of
$16.76$ for k' >= 5. We surmise that these stronger results are mainly due to a
tighter analysis in this paper, rather than Theta-Theta being superior to
Yao-Yao as a spanner. We also show that the spanning ratio of Theta-Theta
graphs decreases to 4.64 as k' increases to 8. These are the first results on
the spanning properties of Theta-Theta graphs."
"We consider variants of the following multi-covering problem with disks. We
are given two point sets $Y$ (servers) and $X$ (clients) in the plane, a
coverage function $\kappa :X \rightarrow \mathcal{N}$, and a constant $\alpha
\geq 1$. Centered at each server is a single disk whose radius we are free to
set. The requirement is that each client $x \in X$ be covered by at least
$\kappa(x)$ of the server disks. The objective function we wish to minimize is
the sum of the $\alpha$-th powers of the disk radii. We present a polynomial
time algorithm for this problem achieving an $O(1)$ approximation."
"We consider offsets of a union of convex objects. We aim for a filtration, a
sequence of nested cell complexes, that captures the topological evolution of
the offsets for increasing radii. We describe methods to compute a filtration
based on the Voronoi partition with respect to the given convex objects. We
prove that, in two and three dimensions, the size of the filtration is
proportional to the size of the Voronoi diagram. Our algorithm runs in
$\Theta(n \log{n})$ in the $2$-dimensional case and in expected time $O(n^{3 +
\epsilon})$, for any $\epsilon > 0$, in the $3$-dimensional case. Our approach
is inspired by alpha-complexes for point sets, but requires more involved
machinery and analysis primarily since Voronoi regions of general convex
objects do not form a good cover. We show by experiments that our approach
results in a similarly fast and topologically more stable method for computing
a filtration compared to approximating the input by point samples."
"The change in the normal between any two nearby points on a closed, smooth
surface is bounded with respect to the local feature size (distance to the
medial axis). An incorrect proof of this lemma appeared as part of the analysis
of the ""crust"" algorithm of Amenta and Bern."
"Many well-known graph drawing techniques, including force directed drawings,
spectral graph layouts, multidimensional scaling, and circle packings, have
algebraic formulations. However, practical methods for producing such drawings
ubiquitously use iterative numerical approximations rather than constructing
and then solving algebraic expressions representing their exact solutions. To
explain this phenomenon, we use Galois theory to show that many variants of
these problems have solutions that cannot be expressed by nested radicals or
nested roots of low-degree polynomials. Hence, such solutions cannot be
computed exactly even in extended computational models that include such
operations."
"For Euclidean space ($\ell_2$), there exists the powerful dimension reduction
transform of Johnson and Lindenstrauss, with a host of known applications.
Here, we consider the problem of dimension reduction for all $\ell_p$ spaces $1
\le p \le 2$. Although strong lower bounds are known for dimension reduction in
$\ell_1$, Ostrovsky and Rabani successfully circumvented these by presenting an
$\ell_1$ embedding that maintains fidelity in only a bounded distance range,
with applications to clustering and nearest neighbor search. However, their
embedding techniques are specific to $\ell_1$ and do not naturally extend to
other norms.
  In this paper, we apply a range of advanced techniques and produce bounded
range dimension reduction embeddings for all of $1 \le p \le 2$, thereby
demonstrating that the approach initiated by Ostrovsky and Rabani for $\ell_1$
can be extended to a much more general framework. We also obtain improved
bounds in terms of the intrinsic dimensionality. As a result we achieve
improved bounds for proximity problems including snowflake embeddings and
clustering."
"Motivated by applications to graph morphing, we consider the following
\emph{compatible connectivity-augmentation problem}: We are given a labelled
$n$-vertex planar graph, $\mathcal{G}$, that has $r\ge 2$ connected components,
and $k\ge 2$ isomorphic planar straight-line drawings, $G_1,\ldots,G_k$, of
$\mathcal{G}$. We wish to augment $\mathcal G$ by adding vertices and edges to
make it connected in such a way that these vertices and edges can be added to
$G_1,\ldots,G_k$ as points and straight-line segments, respectively, to obtain
$k$ planar straight-line drawings isomorphic to the augmentation of $\mathcal
G$. We show that adding $\Theta(nr^{1-1/k})$ edges and vertices to
$\mathcal{G}$ is always sufficient and sometimes necessary to achieve this
goal. The upper bound holds for all $r\in\{2,\ldots,n\}$ and $k\ge 2$ and is
achievable by an algorithm whose running time is $O(nr^{1-1/k})$ for $k=O(1)$
and whose running time is $O(kn^2)$ for general values of $k$. The lower bound
holds for all $r\in\{2,\ldots,n/4\}$ and $k\ge 2$."
"In this paper, we introduce a variation of the well-studied Yao graphs. Given
a set of points $S\subset \mathbb{R}^2$ and an angle $0 < \theta \leq 2\pi$, we
define the continuous Yao graph $cY(\theta)$ with vertex set $S$ and angle
$\theta$ as follows. For each $p,q\in S$, we add an edge from $p$ to $q$ in
$cY(\theta)$ if there exists a cone with apex $p$ and aperture $\theta$ such
that $q$ is the closest point to $p$ inside this cone.
  We study the spanning ratio of $cY(\theta)$ for different values of $\theta$.
Using a new algebraic technique, we show that $cY(\theta)$ is a spanner when
$\theta \leq 2\pi /3$. We believe that this technique may be of independent
interest. We also show that $cY(\pi)$ is not a spanner, and that $cY(\theta)$
may be disconnected for $\theta > \pi$."
"We study balanced circle packings and circle-contact representations for
planar graphs, where the ratio of the largest circle's diameter to the smallest
circle's diameter is polynomial in the number of circles. We provide a number
of positive and negative results for the existence of such balanced
configurations."
"A directed path whose edges are assigned labels ""up"", ""down"", ""right"", or
""left"" is called \emph{four-directional}, and \emph{three-directional} if at
most three out of the four labels are used. A \emph{direction-consistent
embedding} of an \mbox{$n$-vertex} four-directional path $P$ on a set $S$ of
$n$ points in the plane is a straight-line drawing of $P$ where each vertex of
$P$ is mapped to a distinct point of $S$ and every edge points to the direction
specified by its label. We study planar direction-consistent embeddings of
three- and four-directional paths and provide a complete picture of the problem
for convex point sets."
"In their recent article (2010), Levy and Liu introduced a generalization of
Centroidal Voronoi Tessellation (CVT) - namely the Lp-CVT - that allows the
computation of an anisotropic CVT over a sound mathematical framework. In this
article a new objective function is defined, and both this function and its
gradient are derived in closed-form for surfaces and volumes. This method opens
a wide range of possibilities, also described in the paper, such as
quad-dominant surface remeshing, hex-dominant volume meshing or fully-automated
capturing of sharp features. However, in the same paper, the derivations of the
gradient and of the new objective function are only partially expanded in the
appendices, and some relevant requisites on the anisotropy field are left
implicit. In order to better harness the possibilities described there, in this
work the entire derivation process is made explicit. In the authors' opinion,
this also helps understanding the working conditions of the method and its
possible applications."
"We study the simultaneous embeddability of a pair of partitions of the same
underlying set into disjoint blocks. Each element of the set is mapped to a
point in the plane and each block of either of the two partitions is mapped to
a region that contains exactly those points that belong to the elements in the
block and that is bounded by a simple closed curve. We establish three main
classes of simultaneous embeddability (weak, strong, and full embeddability)
that differ by increasingly strict well-formedness conditions on how different
block regions are allowed to intersect. We show that these simultaneous
embeddability classes are closely related to different planarity concepts of
hypergraphs. For each embeddability class we give a full characterization. We
show that (i) every pair of partitions has a weak simultaneous embedding, (ii)
it is NP-complete to decide the existence of a strong simultaneous embedding,
and (iii) the existence of a full simultaneous embedding can be tested in
linear time."
"The Voronoi Covariance Measure of a compact set K of R^d is a tensor-valued
measure that encodes geometric information on K and which is known to be
resilient to Hausdorff noise but sensitive to outliers. In this article, we
generalize this notion to any distance-like function delta and define the
delta-VCM. We show that the delta-VCM is resilient to Hausdorff noise and to
outliers, thus providing a tool to estimate robustly normals from a point cloud
approximation. We present experiments showing the robustness of our approach
for normal and curvature estimation and sharp feature detection."
"Given a set $S$ of points in the plane, the $k$-Gabriel graph of $S$ is the
geometric graph with vertex set $S$, where $p_i,p_j\in S$ are connected by an
edge if and only if the closed disk having segment $\bar{p_ip_j}$ as diameter
contains at most $k$ points of $S \setminus \{p_i,p_j\}$. We consider the
following question: What is the minimum value of $k$ such that the $k$-Gabriel
graph of every point set $S$ contains a Hamiltonian cycle? For this value, we
give an upper bound of 10 and a lower bound of 2. The best previously known
values were 15 and 1, respectively."
"Given a set $P$ of $n$ points in the plane, the order-$k$ Gabriel graph on
$P$, denoted by $k$-$GG$, has an edge between two points $p$ and $q$ if and
only if the closed disk with diameter $pq$ contains at most $k$ points of $P$,
excluding $p$ and $q$. We study matching problems in $k$-$GG$ graphs. We show
that a Euclidean bottleneck perfect matching of $P$ is contained in $10$-$GG$,
but $8$-$GG$ may not have any Euclidean bottleneck perfect matching. In
addition we show that $0$-$GG$ has a matching of size at least $\frac{n-1}{4}$
and this bound is tight. We also prove that $1$-$GG$ has a matching of size at
least $\frac{2(n-1)}{5}$ and $2$-$GG$ has a perfect matching. Finally we
consider the problem of blocking the edges of $k$-$GG$."
"In Euclidean geometry, it is well-known that the $k$-order Voronoi diagram in
$\mathbb{R}^d$ can be computed from the vertical projection of the $k$-level of
an arrangement of hyperplanes tangent to a convex potential function in
$\mathbb{R}^{d+1}$: the paraboloid. Similarly, we report for the Klein ball
model of hyperbolic geometry such a {\em concave} potential function: the
northern hemisphere. Furthermore, we also show how to build the hyperbolic
$k$-order diagrams as equivalent clipped power diagrams in $\mathbb{R}^d$. We
investigate the hyperbolic Voronoi diagram in the hyperboloid model and show
how it reduces to a Klein-type model using central projections."
"We consider approximation of diameter of a set $S$ of $n$ points in dimension
$m$. E$\tilde{g}$ecio$\tilde{g}$lu and Kalantari \cite{kal} have shown that
given any $p \in S$, by computing its farthest in $S$, say $q$, and in turn the
farthest point of $q$, say $q'$, we have ${\rm diam}(S) \leq \sqrt{3} d(q,q')$.
Furthermore, iteratively replacing $p$ with an appropriately selected point on
the line segment $pq$, in at most $t \leq n$ additional iterations, the
constant bound factor is improved to $c_*=\sqrt{5-2\sqrt{3}} \approx 1.24$.
Here we prove when $m=2$, $t=1$. This suggests in practice a few iterations may
produce good solutions in any dimension. Here we also propose a randomized
version and present large scale computational results with these algorithm for
arbitrary $m$. The algorithms outperform many existing algorithms. On sets of
data as large as $1,000,000$ points, the proposed algorithms compute solutions
to within an absolute error of $10^{-4}$."
"Given a set $P$ of $n$ points in $\mathbb{R}^3$, we show that, for any
$\varepsilon >0$, there exists an $\varepsilon$-net of $P$ for halfspace
ranges, of size $O(1/\varepsilon)$. We give five proofs of this result, which
are arguably simpler than previous proofs \cite{msw-hnlls-90, cv-iaags-07,
pr-nepen-08}. We also consider several related variants of this result,
including the case of points and pseudo-disks in the plane."
"We present randomized versions of the {\it triangle algorithm} introduced in
\cite{kal14}. The triangle algorithm tests membership of a distinguished point
$p \in \mathbb{R} ^m$ in the convex hull of a given set $S$ of $n$ points in
$\mathbb{R}^m$. Given any {\it iterate} $p' \in conv(S)$, it searches for a
{\it pivot}, a point $v \in S$ so that $d(p',v) \geq d(p,v)$. It replaces $p'$
with the point on the line segment $p'v$ closest to $p$ and repeats this
process. If a pivot does not exist, $p'$ certifies that $p \not \in conv(S)$.
Here we propose two random variations of the triangle algorithm that allow
relaxed steps so as to take more effective steps possible in subsequent
iterations. One is inspired by the {\it chaos game} known to result in the
Sierpinski triangle. The incentive is that randomized iterates together with a
property of Sierpinski triangle would result in effective pivots. Bounds on
their expected complexity coincides with those of the deterministic version
derived in \cite{kal14}."
"A variation on the classical polygon illumination problem was introduced in
[Aichholzer et. al. EuroCG'09]. In this variant light sources are replaced by
wireless devices called k-modems, which can penetrate a fixed number k, of
""walls"". A point in the interior of a polygon is ""illuminated"" by a k-modem if
the line segment joining them intersects at most k edges of the polygon. It is
easy to construct polygons of n vertices where the number of k-modems required
to illuminate all interior points is Omega(n/k). However, no non-trivial upper
bound is known. In this paper we prove that the number of k-modems required to
illuminate any polygon of n vertices is at most O(n/k). For the cases of
illuminating an orthogonal polygon or a set of disjoint orthogonal segments, we
give a tighter bound of 6n/k + 1. Moreover, we present an O(n log n) time
algorithm to achieve this bound."
"Given a planar map of $n$ segments in which we wish to efficiently locate
points, we present the first randomized incremental construction of the
well-known trapezoidal-map search-structure that only requires expected $O(n
\log n)$ preprocessing time while deterministically guaranteeing worst-case
linear storage space and worst-case logarithmic query time. This settles a long
standing open problem; the best previously known construction time of such a
structure, which is based on a directed acyclic graph, so-called the history
DAG, and with the above worst-case space and query-time guarantees, was
expected $O(n \log^2 n)$. The result is based on a deeper understanding of the
structure of the history DAG, its depth in relation to the length of its
longest search path, as well as its correspondence to the trapezoidal search
tree. Our results immediately extend to planar maps induced by finite
collections of pairwise interior disjoint well-behaved curves."
"The Art Gallery Problem (AGP) is one of the classical problems in
computational geometry. It asks for the minimum number of guards required to
achieve visibility coverage of a given polygon. The AGP is well-known to be
NP-hard even in restricted cases. In this paper, we consider the Art Gallery
Problem with Fading (AGPF): A polygonal region is to be illuminated with light
sources such that every point is illuminated with at least a global threshold,
light intensity decreases over distance, and we seek to minimize the total
energy consumption. Choosing fading exponents of zero, one, and two are
equivalent to the AGP, laser scanner applications, and natural light,
respectively. We present complexity results as well as a negative solvability
result. Still, we propose two practical algorithms for AGPF with fixed light
positions (e.g. vertex guards) independent of the fading exponent, which we
demonstrate to work well in practice. One is based on a discrete approximation,
the other on non-linear programming by means of simplex-partitioning
strategies. The former approach yields a fully polynomial-time approximation
scheme for AGPF with fixed light positions. The latter approach obtains better
results in our experimental evaluation."
"Given a sheet of paper and a prescribed folding of its boundary, is there a
way to fold the paper's interior without stretching so that the boundary lines
up with the prescribed boundary folding? For polygonal boundaries
nonexpansively folded at finitely many points, we prove that a consistent
isometric mapping of the polygon interior always exists and is computable in
polynomial time."
"In this paper, we give the first algorithm that outputs a faithful
reconstruction of a submanifold of Euclidean space without maintaining or even
constructing complicated data structures such as Voronoi diagrams or Delaunay
complexes. Our algorithm uses the witness complex and relies on the stability
of power protection, a notion introduced in this paper. The complexity of the
algorithm depends exponentially on the intrinsic dimension of the manifold,
rather than the dimension of ambient space, and linearly on the dimension of
the ambient space. Another interesting feature of this work is that no explicit
coordinates of the points in the point sample is needed. The algorithm only
needs the distance matrix as input, i.e., only distance between points in the
point sample as input."
"The Art Gallery Problem is one of the most well-known problems in
Computational Geometry, with a rich history in the study of algorithms,
complexity, and variants. Recently there has been a surge in experimental work
on the problem. In this survey, we describe this work, show the chronology of
developments, and compare current algorithms, including two unpublished
versions, in an exhaustive experiment. Furthermore, we show what core
algorithmic ingredients have led to recent successes."
"$\newcommand{\MatA}{\mathcal{M}}$ $\newcommand{\eps}{\varepsilon}$
$\newcommand{\NSize}{\mathsf{N}{}}$ $\newcommand{\MatB}{\mathcal{B}}$
$\newcommand{\Fnorm}[1]{\left\| {#1} \right\|_F}$
$\newcommand{\PrcOpt}[2]{\mu_{\mathrm{opt}}\pth{#1, #2}}$
$\newcommand{\pth}[1]{\left(#1\right)}$
  Given a matrix $\MatA$ with $n$ rows and $d$ columns, and fixed $k$ and
$\eps$, we present an algorithm that in linear time (i.e., $O(\NSize )$)
computes a $k$-rank matrix $\MatB$ with approximation error $\Fnorm{\MatA -
\MatB}^2 \leq (1+\eps) \PrcOpt{\MatA}{k}$, where $\NSize = n d$ is the input
size, and $\PrcOpt{\MatA}{k}$ is the minimum error of a $k$-rank approximation
to $\MatA$.
  This algorithm succeeds with constant probability, and to our knowledge it is
the first linear-time algorithm to achieve multiplicative approximation."
"The number of triangulations of a planar n point set is known to be $c^n$,
where the base $c$ lies between $2.43$ and $30.$ The fastest known algorithm
for counting triangulations of a planar n point set runs in $O^*(2^n)$ time.
The fastest known arbitrarily close approximation algorithm for the base of the
number of triangulations of a planar n point set runs in time subexponential in
$n.$ We present the first quasi-polynomial approximation scheme for the base of
the number of triangulations of a planar point set."
"Reeb graphs are structural descriptors that capture shape properties of a
topological space from the perspective of a chosen function. In this work we
define a combinatorial metric for Reeb graphs of orientable surfaces in terms
of the cost necessary to transform one graph into another by edit operations.
The main contributions of this paper are the stability property and the
optimality of this edit distance. More precisely, the stability result states
that changes in the functions, measured by the maximum norm, imply not greater
changes in the corresponding Reeb graphs, measured by the edit distance. The
optimality result states that our edit distance discriminates Reeb graphs
better than any other metric for Reeb graphs of surfaces satisfying the
stability property."
"We revisit the classical problem of computing the \emph{contour tree} of a
scalar field $f:\mathbb{M} \to \mathbb{R}$, where $\mathbb{M}$ is a
triangulated simplicial mesh in $\mathbb{R}^d$. The contour tree is a
fundamental topological structure that tracks the evolution of level sets of
$f$ and has numerous applications in data analysis and visualization.
  All existing algorithms begin with a global sort of at least all critical
values of $f$, which can require (roughly) $\Omega(n\log n)$ time. Existing
lower bounds show that there are pathological instances where this sort is
required. We present the first algorithm whose time complexity depends on the
contour tree structure, and avoids the global sort for non-pathological inputs.
If $C$ denotes the set of critical points in $\mathbb{M}$, the running time is
roughly $O(\sum_{v \in C} \log \ell_v)$, where $\ell_v$ is the depth of $v$ in
the contour tree. This matches all existing upper bounds, but is a significant
improvement when the contour tree is short and fat. Specifically, our approach
ensures that any comparison made is between nodes in the same descending path
in the contour tree, allowing us to argue strong optimality properties of our
algorithm.
  Our algorithm requires several novel ideas: partitioning $\mathbb{M}$ in
well-behaved portions, a local growing procedure to iteratively build contour
trees, and the use of heavy path decompositions for the time complexity
analysis."
"We present a new type of polyominoes that can have transparent squares
(holes). We show how these polyominoes can tile rectangles and we categorise
them according to their tiling ability. We were able to categorise all but 6
polyominoes with 5 or fewer visible squares."
"We present linear-time algorithms to construct tree-like Voronoi diagrams
with disconnected regions, after the sequence of their faces along an enclosing
boundary (or at infinity) is known. We focus on Voronoi diagrams of line
segments including the farthest-segment Voronoi diagram, the order-(k+1)
subdivision within an order-k Voronoi region, and deleting a site from a
nearest-neighbor segment Voronoi diagram. Although tree-structured, these
diagrams illustrate properties surprisingly different from their counterparts
for points. The sequence of their faces along the relevant boundary forms a
Davenport-Schinzel sequence of order >=2. Once this sequence is known, we show
how to compute the corresponding Voronoi diagram in linear time, expected or
deterministic, augmenting the existing frameworks for points in convex position
with the ability to handle non-point sites and multiple Voronoi faces. Our
techniques contribute towards the question of linear-time construction
algorithms for Voronoi diagrams whose graph structure is a tree."
"In this paper, we show that any scaled-up version of any discrete
self-similar {\it tree} fractal does not strictly self-assemble, at any
temperature, in Winfree's abstract Tile Assembly Model."
"We extend the results of Adcock, Carlsson, and Carlsson by constructing
numeric invariants from the computation of a multidimensional persistence
module as given by Carlsson, Singh, and Zomorodian."
"Persistent homology and zigzag persistent homology are techniques which track
the homology over a sequence of spaces, outputting a set of intervals
corresponding to birth and death times of homological features in the sequence.
This paper presents a method for choosing a homology class to correspond to
each of the intervals at each time point. For each homology class a specific
representative cycle is stored, with the choice of homology class and
representative cycle being both geometrically relevant and compatible with the
birth-death interval decomposition. After describing the method in detail and
proving its correctness, we illustrate the utility of the method by applying it
to the study of coverage holes in time-varying sensor networks."
"$\beta$-skeletons, a prominent member of the neighborhood graph family, have
interesting geometric properties and various applications ranging from
geographic networks to archeology. This paper focuses on developing a new, more
general than the present one, definition of $\beta$-skeletons based only on the
distance criterion. It allows us to consider them in many different cases, e.g.
for weighted graphs or objects other than points. Two types of
$\beta$-skeletons are especially well-known: the Gabriel Graph (for $\beta =
1$) and the Relative Neighborhood Graph (for $\beta = 2$). The new definition
retains relations between those graphs and the other well-known ones (minimum
spanning tree and Delaunay triangulation). We also show several new algorithms
finding $\beta$-skeletons."
"$\beta$-skeletons are well-known neighborhood graphs for a set of points. We
extend this notion to sets of line segments in the Euclidean plane and present
algorithms computing such skeletons for the entire range of $\beta$ values. The
main reason of such extension is the possibility to study $\beta$-skeletons for
points moving along given line segments. We show that relations between
$\beta$-skeletons for $\beta > 1$, $1$-skeleton (Gabriel Graph), and the
Delaunay triangulation for sets of points hold also for sets of segments. We
present algorithms for computing circle-based and lune-based $\beta$-skeletons.
We describe an algorithm that for $\beta \geq 1$ computes the $\beta$-skeleton
for a set $S$ of $n$ segments in the Euclidean plane in $O(n^2 \alpha (n) \log
n)$ time in the circle-based case and in $O(n^2 \lambda_4(n))$ in the
lune-based one, where the construction relies on the Delaunay triangulation for
$S$, $\alpha$ is a functional inverse of Ackermann function and $\lambda_4(n)$
denotes the maximum possible length of a $(n,4)$ Davenport-Schinzel sequence.
When $0 < \beta < 1$, the $\beta$-skeleton can be constructed in a $O(n^3
\lambda_4(n))$ time. In the special case of $\beta = 1$, which is a
generalization of Gabriel Graph, the construction can be carried out in a $O(n
\log n)$ time."
"We present a new algorithm for lune-based $\beta$-skeletons for sets of $n$
points in the plane, for $\beta \in (2,\infty]$, the only case when optimal
algorithms are not known. The running time of the algorithm is $O(n^{3/2}
\log^{1/2} n)$, which is the best known and is an improvement of Rao and
Mukhopadhyay \cite{rm97} result. The method is based on point location in
monotonic subdivisions of arrangements of curve segments."
"The $\beta$-skeleton $\{G_{\beta}(V)\}$ for a point set V is a family of
geometric graphs, defined by the notion of neighborhoods parameterized by real
number $0 < \beta < \infty$. By using the distance-based version definition of
$\beta$-skeletons we study those graphs for a set of points in $\mathbb{R}^d$
space with $l_1$ and $l_{\infty}$ metrics. We present algorithms for the entire
spectrum of $\beta$ values and we discuss properties of lens-based and
circle-based $\beta$-skeletons in those metrics.
  Let $V \in \mathbb{R}^d$ in $L_{\infty}$ metric be a set of $n$ points in
general position. Then, for $\beta<2$ lens-based $\beta$-skeleton
$G_{\beta}(V)$ can be computed in $O(n^2 \log^d n)$ time. For $\beta \geq 2$
there exists an $O(n \log^{d-1} n)$ time algorithm that constructs
$\beta$-skeleton for the set $V$. We show that in $\mathbb{R}^d$ with
$L_{\infty}$ metric, for $\beta<2$ $\beta$-skeleton $G_{\beta}(V)$ for $n$
points can be computed in $O(n^2 \log^d n)$ time. For $\beta \geq 2$ there
exists an $O(n \log^{d-1} n)$ time algorithm. In $\mathbb{R}^d$ with $L_1$
metric for a set of $n$ points in arbitrary position $\beta$-skeleton
$G_{\beta}(V)$ can be computed in $O(n^2 \log^{d+2} n)$ time."
"Alamdari et al. showed that given two straight-line planar drawings of a
graph, there is a morph between them that preserves planarity and consists of a
polynomial number of steps where each step is a \emph{linear morph} that moves
each vertex at constant speed along a straight line. An important step in their
proof consists of converting a \emph{pseudo-morph} (in which contractions are
allowed) to a true morph. Here we introduce the notion of \emph{unidirectional
morphing} step, where the vertices move along lines that all have the same
direction. Our main result is to show that any planarity preserving
pseudo-morph consisting of unidirectional steps and contraction of low degree
vertices can be turned into a true morph without increasing the number of
steps. Using this, we strengthen Alamdari et al.'s result to use only
unidirectional morphs, and in the process we simplify the proof."
"We show that in the hierarchical tile assembly model, if there is a
producible assembly that overlaps a nontrivial translation of itself
consistently (i.e., the pattern of tile types in the overlap region is
identical in both translations), then arbitrarily large assemblies are
producible. The significance of this result is that tile systems intended to
controllably produce finite structures must avoid pattern repetition in their
producible assemblies that would lead to such overlap. This answers an open
question of Chen and Doty (SODA 2012), who showed that so-called
""partial-order"" systems producing a unique finite assembly *and"" avoiding such
overlaps must require time linear in the assembly diameter. An application of
our main result is that any system producing a unique finite assembly is
automatically guaranteed to avoid such overlaps, simplifying the hypothesis of
Chen and Doty's main theorem."
"We present a novel set of methods for analyzing coverage properties in
dynamic sensor networks. The dynamic sensor network under consideration is
studied through a series of snapshots, and is represented by a sequence of
simplicial complexes, built from the communication graph at each time point. A
method from computational topology called zigzag persistent homology takes this
sequence of simplicial complexes as input, and returns a `barcode' containing
the birth and death times of homological features in this sequence. We derive
useful statistics from this output for analyzing time-varying coverage
properties. Further, we propose a method which returns specific representative
cycles for these homological features, at each point along the birth-death
intervals. These representative cycles are then used to track coverage holes in
the network, and obtain size estimates for individual holes at each time point.
A weighted barcode, incorporating the size information, is then used as a
visual and quantitative descriptor of the dynamic network coverage."
"Deterministically generating near-uniform point samplings of the motion
groups like SO(3), SE(3) and their n-wise products SO(3)^n, SE(3)^n is
fundamental to numerous applications in computational and data sciences. The
natural measure of sampling quality is discrepancy. In this work, our main goal
is construct low discrepancy deterministic samplings in product spaces of the
motion groups. To this end, we develop a novel strategy (using a two-step
discrepancy construction) that leads to an almost exponential improvement in
size (from the trivial direct product). To the best of our knowledge, this is
the first nontrivial construction for SO(3)^n, SE(3)^n and the hypertorus T^n.
  We also construct new low discrepancy samplings of S^2 and SO(3). The central
component in our construction for SO(3) is an explicit construction of N points
in S^2 with discrepancy \tilde{\O}(1/\sqrt{N}) with respect to convex sets,
matching the bound achieved for the special case of spherical caps in
\cite{ABD_12}. We also generalize the discrepancy of Cartesian product sets
\cite{Chazelle04thediscrepancy} to the discrepancy of local Cartesian product
sets.
  The tools we develop should be useful in generating low discrepancy samplings
of other complicated geometric spaces."
"Teramoto et al. defined a new measure called the gap ratio that measures the
uniformity of a finite point set sampled from $\cal S$, a bounded subset of
$\mathbb{R}^2$. We generalize this definition of measure over all metric spaces
by appealing to covering and packing radius. The definition of gap ratio needs
only a metric unlike discrepancy, a widely used uniformity measure, that
depends on the notion of a range space and its volume. We also show some
interesting connections of gap ratio to Delaunay triangulation and discrepancy
in the Euclidean plane. The major focus of this work is on solving optimization
related questions about selecting uniform point samples from metric spaces; the
uniformity being measured using gap ratio. We consider discrete spaces like
graph and set of points in the Euclidean space and continuous spaces like the
unit square and path connected spaces. We deduce lower bounds, prove hardness
and approximation hardness results. We show that a general approximation
algorithm framework gives different approximation ratios for different metric
spaces based on the lower bound we deduce. Apart from the above, we show
existence of coresets for sampling uniform points from the Euclidean space --
for both the static and the streaming case. This leads to a $\left( 1+\epsilon
\right)$-approximation algorithm for uniform sampling from the Euclidean space."
"Let $P$ be a set of $n$ vertices in the plane and $S$ a set of non-crossing
line segments between vertices in $P$, called constraints. Two vertices are
visible if the straight line segment connecting them does not properly
intersect any constraints. The constrained $\Theta_m$-graph is constructed by
partitioning the plane around each vertex into $m$ disjoint cones, each with
aperture $\theta = 2 \pi/m$, and adding an edge to the `closest' visible vertex
in each cone. We consider how to route on the constrained $\Theta_6$-graph. We
first show that no deterministic 1-local routing algorithm is
$o(\sqrt{n})$-competitive on all pairs of vertices of the constrained
$\Theta_6$-graph. After that, we show how to route between any two visible
vertices of the constrained $\Theta_6$-graph using only 1-local information.
Our routing algorithm guarantees that the returned path is 2-competitive.
Additionally, we provide a 1-local 18-competitive routing algorithm for visible
vertices in the constrained half-$\Theta_6$-graph, a subgraph of the
constrained $\Theta_6$-graph that is equivalent to the Delaunay graph where the
empty region is an equilateral triangle. To the best of our knowledge, these
are the first local routing algorithms in the constrained setting with
guarantees on the length of the returned path."
"$\renewcommand{\Re}{{\rm I\!\hspace{-0.025em} R}}
\newcommand{\SetX}{\mathsf{X}} \newcommand{\VorX}[1]{\mathcal{V} \pth{#1}}
\newcommand{\Polygon}{\mathsf{P}} \newcommand{\Space}{\overline{\mathsf{m}}}
\newcommand{\pth}[2][\!]{#1\left({#2}\right)}$ We resolve an open problem due
to Tetsuo Asano, showing how to compute the shortest path in a polygon, given
in a read only memory, using sublinear space and subquadratic time.
Specifically, given a simple polygon $\Polygon$ with $n$ vertices in a read
only memory, and additional working memory of size $\Space$, the new algorithm
computes the shortest path (in $\Polygon$) in $O( n^2 /\, \Space )$ expected
time. This requires several new tools, which we believe to be of independent
interest."
"The SINR model for the quality of wireless connections has been the subject
of extensive recent study. It attempts to predict whether a particular
transmitter is heard at a specific location, in a setting consisting of $n$
simultaneous transmitters and background noise. The SINR model gives rise to a
natural geometric object, the SINR diagram, which partitions the space into $n$
regions where each of the transmitters can be heard and the remaining space
where no transmitter can be heard.
  Efficient point location in the SINR diagram, i.e., being able to build a
data structure that facilitates determining, for a query point, whether any
transmitter is heard there, and if so, which one, has been recently
investigated in several papers. These planar data structures are constructed in
time at least quadratic in $n$ and support logarithmic-time approximate
queries. Moreover, the performance of some of the proposed structures depends
strongly not only on the number $n$ of transmitters and on the approximation
parameter $\varepsilon$, but also on some geometric parameters that cannot be
bounded a priori as a function of $n$ or $\varepsilon$.
  We address the question of batched point location queries, i.e., answering
many queries simultaneously. Specifically, in one dimension, we can answer $n$
queries exactly in amortized polylogarithmic time per query, while in the plane
we can do it approximately.
  We also show how to answer $n^2$ queries exactly in amortized polylogarithmic
time per query, assuming the queries are located on a possibly non-uniform $n
\times n$ grid.
  All these results can handle arbitrary power assignments to the transmitters.
Moreover, the amortized query time in these results depends only on $n$ and
$\varepsilon$.
  These results demonstrate the (so far underutilized) power of combining
algebraic tools with those of computational geometry and other fields."
"We analyze the correctness of an O(n log n) time divide-and-conquer algorithm
for the convex hull problem when each input point is a location determined by a
normal distribution. We show that the algorithm finds the convex hull of such
probabilistic points to precision within some expected correctness determined
by a user-given confidence value. In order to precisely explain how correct the
resulting structure is, we introduce a new certificate error model for
calculating and understanding approximate geometric error based on the
fundamental properties of a geometric structure. We show that this new error
model implies correctness under a robust statistical error model, in which each
point lies within the hull with probability at least that of the user-given
confidence value, for the convex hull problem."
"We investigate what computational tasks can be performed on a point set in
$\Re^d$, if we are only given black-box access to it via nearest-neighbor
search. This is a reasonable assumption if the underlying point set is either
provided implicitly, or it is stored in a data structure that can answer such
queries. In particular, we show the following: (A) One can compute an
approximate bi-criteria $k$-center clustering of the point set, and more
generally compute a greedy permutation of the point set. (B) One can decide if
a query point is (approximately) inside the convex-hull of the point set.
  We also investigate the problem of clustering the given point set, such that
meaningful proximity queries can be carried out on the centers of the clusters,
instead of the whole point set."
"Given a real-valued function $f$ defined over a manifold $M$ embedded in
$\mathbb{R}^d$, we are interested in recovering structural information about
$f$ from the sole information of its values on a finite sample $P$. Existing
methods provide approximation to the persistence diagram of $f$ when geometric
noise and functional noise are bounded. However, they fail in the presence of
aberrant values, also called outliers, both in theory and practice.
  We propose a new algorithm that deals with outliers. We handle aberrant
functional values with a method inspired from the k-nearest neighbors
regression and the local median filtering, while the geometric outliers are
handled using the distance to a measure. Combined with topological results on
nested filtrations, our algorithm performs robust topological analysis of
scalar fields in a wider range of noise models than handled by current methods.
We provide theoretical guarantees and experimental results on the quality of
our approximation of the sampled scalar field."
"The approximate nearest neighbor problem ($\epsilon$-ANN) in high dimensional
Euclidean space has been mainly addressed by Locality Sensitive Hashing (LSH),
which has polynomial dependence in the dimension, sublinear query time, but
subquadratic space requirement. In this paper, we introduce a new definition of
""low-quality"" embeddings for metric spaces. It requires that, for some query
point $q$, there exists an approximate nearest neighbor among the pre-images of
the $k>1$ approximate nearest neighbors in the target space. Focusing on
Euclidean spaces, we employ random projections in order to reduce the original
problem to one in a space of dimension inversely proportional to $k$.
  The $k$ approximate nearest neighbors can be efficiently retrieved by a data
structure such as BBD-trees. The same approach is applied to the problem of
computing an approximate near neighbor, where we obtain a data structure
requiring linear space, and query time in $O(d n^{\rho})$, for $\rho\approx
1-\epsilon^2/\log(1/\epsilon)$. This directly implies a solution for
$\epsilon$-ANN, while achieving a better exponent in the query time than the
method based on BBD-trees. Better bounds are obtained in the case of doubling
subsets of $\ell_2$, by combining our method with $r$-nets.
  We implement our method in C++, and present experimental results in dimension
up to $500$ and $10^6$ points, which show that performance is better than
predicted by the analysis. In addition, we compare our ANN approach to E2LSH,
which implements LSH, and we show that the theoretical advantages of each
method are reflected on their actual performance."
"Let $P_1,...,P_{d+1} \subset \mathbb{R}^d$ be $d$-dimensional point sets such
that the convex hull of each $P_i$ contains the origin. We call the sets $P_i$
color classes, and we think of the points in $P_i$ as having color $i$. A
colorful choice is a set with at most one point of each color. The colorful
Caratheodory theorem guarantees the existence of a colorful choice whose convex
hull contains the origin. So far, the computational complexity of finding such
a colorful choice is unknown.
  We approach this problem from two directions. First, we consider
approximation algorithms: an $m$-colorful choice is a set that contains at most
$m$ points from each color class. We show that for any constant $\varepsilon >
0$, an $\lceil \varepsilon(d+1)\rceil$-colorful choice containing the origin in
its convex hull can be found in polynomial time. This notion of approximation
has not been studied before, and it is motivated through the applications of
the colorful Caratheodory theorem in the literature. In the second part, we
present a natural generalization of the colorful Caratheodory problem: in the
Nearest Colorful Polytope problem (NCP), we are given sets $P_1,...,P_n \subset
\mathbb{R}^d$ that do not necessarily contain the origin in their convex hulls.
The goal is to find a colorful choice whose convex hull minimizes the distance
to the origin. We show that computing local optima for the NCP problem is
PLS-complete, while computing a global optimum is NP-hard."
"A lower bound for the interleaving distance on persistence vector spaces is
given in terms of rank invariants. This offers an alternative proof of the
stability of rank invariants."
"We address recently proposed chromatic versions of the classic Art Gallery
Problem. Assume a simple polygon $P$ is guarded by a finite set of point guards
and each guard is assigned one of $t$ colors. Such a chromatic guarding is said
to be conflict-free if each point $p\in P$ sees at least one guard with a
unique color among all guards visible from $p$. The goal is to establish bounds
on the function $\chi_{cf}(n)$ of the number of colors sufficient to guarantee
the existence of a conflict-free chromatic guarding for any $n$-vertex polygon.
B\""artschi and Suri showed $\chi_{cf}(n)\in O(\log n)$ (Algorithmica, 2014) for
simple orthogonal polygons and the same bound applies to general simple
polygons (B\""artschi et al., SoCG 2014). In this paper, we assume the
r-visibility model instead of standard line visibility. Points $p$ and $q$ in
an orthogonal polygon are r-visible to each other if the rectangle spanned by
the points is contained in $P$. For this model we show $\chi_{cf}(n)\in
O(\log\log n)$ and $\chi_{cf}(n)\in \Omega(\log\log n /\log\log\log n)$. Most
interestingly, we can show that the lower bound proof extends to guards with
line visibility. To this end we introduce and utilize a novel discrete
combinatorial structure called multicolor tableau. This is the first
non-trivial lower bound for this problem setting.Furthermore, for the strong
chromatic version of the problem, where all guards r-visible from a point must
have distinct colors, we prove a $\Theta(\log n)$-bound. Our results can be
interpreted as coloring results for special geometric hypergraphs."
"We describe new methods for the construction of spiral toolpaths for
high-speed machining. In the simplest case, our method takes a polygon as input
and a number $\delta>0$ and returns a spiral starting at a central point in the
polygon, going around towards the boundary while morphing to the shape of the
polygon. The spiral consists of linear segments and circular arcs, it is $G^1$
continuous, it has no self-intersections, and the distance from each point on
the spiral to each of the neighboring revolutions is at most $\delta$. Our
method has the advantage over previously described methods that it is easily
adjustable to the case where there is an island in the polygon to be avoided by
the spiral. In that case, the spiral starts at the island and morphs the island
to the outer boundary of the polygon. It is shown how to apply that method to
make significantly shorter spirals in polygons with no islands. Finally, we
show how to make a spiral in a polygon with multiple islands by connecting the
islands into one island."
"Given a set $P$ of $n$ points in the plane, we study the computation of the
probability distribution function of both the area and perimeter of the convex
hull of a random subset $S$ of $P$. The random subset $S$ is formed by drawing
each point $p$ of $P$ independently with a given rational probability $\pi_p$.
For both measures of the convex hull, we show that it is \#P-hard to compute
the probability that the measure is at least a given bound $w$. For
$\varepsilon\in(0,1)$, we provide an algorithm that runs in
$O(n^{6}/\varepsilon)$ time and returns a value that is between the probability
that the area is at least $w$, and the probability that the area is at least
$(1-\varepsilon)w$. For the perimeter, we show a similar algorithm running in
$O(n^{6}/\varepsilon)$ time. Finally, given $\varepsilon,\delta\in(0,1)$ and
for any measure, we show an $O(n\log n+ (n/\varepsilon^2)\log(1/\delta))$-time
Monte Carlo algorithm that returns a value that, with probability of success at
least $1-\delta$, differs at most $\varepsilon$ from the probability that the
measure is at least $w$."
"This paper introduces a new proximity graph, called the $k$-Semi-Yao graph
($k$-SYG), on a set $P$ of points in $\mathbb{R}^d$, which is a supergraph of
the $k$-nearest neighbor graph ($k$-NNG) of $P$. We provide a kinetic data
structure (KDS) to maintain the $k$-SYG on moving points, where the trajectory
of each point is a polynomial function whose degree is bounded by some
constant. Our technique gives the first KDS for the theta graph (\ie, $1$-SYG)
in $\mathbb{R}^d$. It generalizes and improves on previous work on maintaining
the theta graph in $\mathbb{R}^2$.
  As an application, we use the kinetic $k$-SYG to provide the first KDS for
maintenance of all the $k$-nearest neighbors in $\mathbb{R}^d$, for any $k\geq
1$. Previous works considered the $k=1$ case only. Our KDS for all the
$1$-nearest neighbors is deterministic. The best previous KDS for all the
$1$-nearest neighbors in $ \mathbb{R}^d$ is randomized. Our structure and
analysis are simpler and improve on this work for the $k=1$ case. We also
provide a KDS for all the $(1+\epsilon)$-nearest neighbors, which in fact gives
better performance than previous KDS's for maintenance of all the exact
$1$-nearest neighbors.
  As another application, we present the first KDS for answering reverse
$k$-nearest neighbor queries on moving points in $ \mathbb{R}^d$, for any
$k\geq 1$."
"Suppose that a circular fire spreads in the plane at unit speed. A single
fire fighter can build a barrier at speed $v>1$. How large must $v$ be to
ensure that the fire can be contained, and how should the fire fighter proceed?
We contribute two results.
  First, we analyze the natural curve $\mbox{FF}_v$ that develops when the
fighter keeps building, at speed $v$, a barrier along the boundary of the
expanding fire. We prove that the behavior of this spiralling curve is governed
by a complex function $(e^{w Z} - s \, Z)^{-1}$, where $w$ and $s$ are real
functions of $v$. For $v>v_c=2.6144 \ldots$ all zeroes are complex conjugate
pairs. If $\phi$ denotes the complex argument of the conjugate pair nearest to
the origin then, by residue calculus, the fire fighter needs $\Theta( 1/\phi)$
rounds before the fire is contained. As $v$ decreases towards $v_c$ these two
zeroes merge into a real one, so that argument $\phi$ goes to~0. Thus, curve
$\mbox{FF}_v$ does not contain the fire if the fighter moves at speed $v=v_c$.
(That speed $v>v_c$ is sufficient for containing the fire has been proposed
before by Bressan et al. [7], who constructed a sequence of logarithmic spiral
segments that stay strictly away from the fire.)
  Second, we show that any curve that visits the four coordinate half-axes in
cyclic order, and in inreasing distances from the origin, needs speed
$v>1.618\ldots$, the golden ratio, in order to contain the fire.
  Keywords: Motion Planning, Dynamic Environments, Spiralling strategies, Lower
and upper bounds"
"A simple linear search algorithm running in $O(n+mk)$ time is proposed for
constructing the lower envelope of $k$ vertices from $m$ monotone polygonal
chains in 2D with $n$ vertices in total. This can be applied to
output-sensitive construction of lower envelopes for arbitrary line segments in
optimal $O(n\log k)$ time, where $k$ is the output size. Compared to existing
output-sensitive algorithms for lower envelopes, this is simpler to implement,
does not require complex data structures, and is a constant factor faster."
"We devise a polynomial-time approximation scheme for the classical geometric
problem of finding an approximate short path amid weighted regions. In this
problem, a triangulated region P comprising of n vertices, a positive weight
associated with each triangle, and two points s and t that belong to P are
given as the input. The objective is to find a path whose cost is at most
(1+epsilon)OPT where OPT is the cost of an optimal path between s and t. Our
algorithm initiates a discretized-Dijkstra wavefront from source s and
progresses the wavefront till it strikes t. This result is about a cubic factor
(in n) improvement over the Mitchell and Papadimitriou '91 result, which is the
only known polynomial time algorithm for this problem to date. Further, with
polynomial time preprocessing of P, a set of data structures are computed which
allow answering approximate weighted shortest path queries in polynomial time."
"Given two points in a simple polygon $P$ of $n$ vertices, its geodesic
distance is the length of the shortest path that connects them among all paths
that stay within $P$. The geodesic center of $P$ is the unique point in $P$
that minimizes the largest geodesic distance to all other points of $P$. In
1989, Pollack, Sharir and Rote [Disc. \& Comput. Geom. 89] showed an $O(n\log
n)$-time algorithm that computes the geodesic center of $P$. Since then, a
longstanding question has been whether this running time can be improved
(explicitly posed by Mitchell [Handbook of Computational Geometry, 2000]). In
this paper we affirmatively answer this question and present a linear time
algorithm to solve this problem."
"We study the family of intersection graphs of low density objects in low
dimensional Euclidean space. This family is quite general, and includes planar
graphs. We prove that such graphs have small separators. Next, we present
efficient $(1+\varepsilon)$-approximation algorithms for these graphs, for
Independent Set, Set Cover, and Dominating Set problems, among others. We also
prove corresponding hardness of approximation for some of these optimization
problems, providing a characterization of their intractability in terms of
density."
"Given a set $P$ of terminals in the plane and a partition of $P$ into $k$
subsets $P_1, ..., P_k$, a two-level rectilinear Steiner tree consists of a
rectilinear Steiner tree $T_i$ connecting the terminals in each set $P_i$
($i=1,...,k$) and a top-level tree $T_{top}$ connecting the trees $T_1, ...,
T_k$. The goal is to minimize the total length of all trees. This problem
arises naturally in the design of low-power physical implementations of parity
functions on a computer chip.
  For bounded $k$ we present a polynomial time approximation scheme (PTAS) that
is based on Arora's PTAS for rectilinear Steiner trees after lifting each
partition into an extra dimension. For the general case we propose an algorithm
that predetermines a connection point for each $T_i$ and $T_{top}$
($i=1,...,k$).
  Then, we apply any approximation algorithm for minimum rectilinear Steiner
trees in the plane to compute each $T_i$ and $T_{top}$ independently.
  This gives us a $2.37$-factor approximation with a running time of
$\mathcal{O}(|P|\log|P|)$ suitable for fast practical computations. The
approximation factor reduces to $1.63$ by applying Arora's approximation scheme
in the plane."
"An important task in trajectory analysis is clustering. The results of a
clustering are often summarized by a single representative trajectory and an
associated size of each cluster. We study the problem of computing a suitable
representative of a set of similar trajectories. To this end we define a
central trajectory $\mathcal{C}$, which consists of pieces of the input
trajectories, switches from one entity to another only if they are within a
small distance of each other, and such that at any time $t$, the point
$\mathcal{C}(t)$ is as central as possible. We measure centrality in terms of
the radius of the smallest disk centered at $\mathcal{C}(t)$ enclosing all
entities at time $t$, and discuss how the techniques can be adapted to other
measures of centrality. We first study the problem in $\mathbb{R}^1$, where we
show that an optimal central trajectory $\mathcal{C}$ representing $n$
trajectories, each consisting of $\tau$ edges, has complexity $\Theta(\tau
n^2)$ and can be computed in $O(\tau n^2 \log n)$ time. We then consider
trajectories in $\mathbb{R}^d$ with $d\geq 2$, and show that the complexity of
$\mathcal{C}$ is at most $O(\tau n^{5/2})$ and can be computed in $O(\tau n^3)$
time."
"The geometric hitting set problem is one of the basic geometric combinatorial
optimization problems: given a set $P$ of points, and a set $\mathcal{D}$ of
geometric objects in the plane, the goal is to compute a small-sized subset of
$P$ that hits all objects in $\mathcal{D}$. In 1994, Bronniman and Goodrich
made an important connection of this problem to the size of fundamental
combinatorial structures called $\epsilon$-nets, showing that small-sized
$\epsilon$-nets imply approximation algorithms with correspondingly small
approximation ratios. Very recently, Agarwal and Pan showed that their scheme
can be implemented in near-linear time for disks in the plane. Altogether this
gives $O(1)$-factor approximation algorithms in $\tilde{O}(n)$ time for hitting
sets for disks in the plane.
  This constant factor depends on the sizes of $\epsilon$-nets for disks;
unfortunately, the current state-of-the-art bounds are large -- at least
$24/\epsilon$ and most likely larger than $40/\epsilon$. Thus the approximation
factor of the Agarwal and Pan algorithm ends up being more than $40$. The best
lower-bound is $2/\epsilon$, which follows from the Pach-Woeginger construction
for halfspaces in two dimensions. Thus there is a large gap between the
best-known upper and lower bounds. Besides being of independent interest,
finding precise bounds is important since this immediately implies an improved
linear-time algorithm for the hitting-set problem.
  The main goal of this paper is to improve the upper-bound to $13.4/\epsilon$
for disks in the plane. The proof is constructive, giving a simple algorithm
that uses only Delaunay triangulations. We have implemented the algorithm,
which is available as a public open-source module. Experimental results show
that the sizes of $\epsilon$-nets for a variety of data-sets is lower, around
$9/\epsilon$."
"The discrete Fr\'echet distance is a useful similarity measure for comparing
two sequences of points $P=(p_1,\ldots, p_m)$ and $Q=(q_1,\ldots,q_n)$. In many
applications, the quality of the matching can be improved if we let $Q$ undergo
some transformation relative to $P$. In this paper we consider the problem of
finding a translation of $Q$ that brings the discrete Fr\'echet distance
between $P$ and $Q$ to a minimum. We devise an algorithm that computes the
minimum discrete Fr\'echet distance under translation in $\mathbb{R}^2$, and
runs in $O(m^3n^2(1+\log(n/m))\log(m+n))$ time, assuming $m\leq n$. This
improves a previous algorithm of Jiang et al.~\cite{JXZ08}, which runs in
$O(m^3n^3 \log(m + n))$ time."
"The Reeb graph is a construction which originated in Morse theory to study a
real valued function defined on a topological space. More recently, it has been
used in various applications to study noisy data which creates a desire to
define a measure of similarity between these structures. Here, we exploit the
fact that the category of Reeb graphs is equivalent to the category of a
particular class of cosheaf. Using this equivalency, we can define an
`interleaving' distance between Reeb graphs which is stable under the
perturbation of a function. Along the way, we obtain a natural construction for
smoothing a Reeb graph to reduce its topological complexity. The smoothed Reeb
graph can be constructed in polynomial time."
"The one-round discrete Voronoi game, with respect to a $n$-point user set
$U$, consists of two players Player 1 ($\mathcal{P}_1$) and Player 2
($\mathcal{P}_2$). At first, $\mathcal{P}_1$ chooses a set of facilities $F_1$
following which $\mathcal{P}_2$ chooses another set of facilities $F_2$,
disjoint from $F_1$. The payoff of $\mathcal{P}_2$ is defined as the
cardinality of the set of points in $U$ which are closer to a facility in $F_2$
than to every facility in $F_1$, and the payoff of $\mathcal{P}_1$ is the
difference between the number of users in $U$ and the payoff of
$\mathcal{P}_2$. The objective of both the players in the game is to maximize
their respective payoffs. In this paper we study the one-round discrete Voronoi
game where $\mathcal{P}_1$ places $k$ facilities and $\mathcal{P}_2$ places one
facility and we have denoted this game as $VG(k,1)$. Although the optimal
solution of this game can be found in polynomial time, the polynomial has a
very high degree. In this paper, we focus on achieving approximate solutions to
$VG(k,1)$ with significantly better running times. We provide a constant-factor
approximate solution to the optimal strategy of $\mathcal{P}_1$ in $VG(k,1)$ by
establishing a connection between $VG(k,1)$ and weak $\epsilon$-nets. To the
best of our knowledge, this is the first time that Voronoi games are studied
from the point of view of $\epsilon$-nets."
"Point feature map labeling is a geometric problem, in which a set of input
points must be labeled with a set of disjoint rectangles (the bounding boxes of
the label texts). Typically, labeling models either use internal labels, which
must touch their feature point, or external (boundary) labels, which are placed
on one of the four sides of the input points' bounding box and which are
connected to their feature points by crossing-free leader lines. In this paper
we study polynomial-time algorithms for maximizing the number of internal
labels in a mixed labeling model that combines internal and external labels.
The model requires that all leaders are parallel to a given orientation $\theta
\in [0,2\pi)$, whose value influences the geometric properties and hence the
running times of our algorithms."
"We show that the union of $n$ translates of a convex body in $\mathbb{R}^3$
can have $\Theta(n^3)$ holes in the worst case, where a hole in a set $X$ is a
connected component of $\mathbb{R}^3 \setminus X$. This refutes a 20-year-old
conjecture. As a consequence, we also obtain improved lower bounds on the
complexity of motion planning problems and of Voronoi diagrams with convex
distance functions."
"For rooted trees, an ideal drawing is one that is planar, straight-line,
strictly-upward, and order-preserving. This paper considers ideal drawings of
rooted trees with the objective of keeping the width of such drawings small. It
is not known whether finding the minimum-possible width is NP-hard or
polynomial. This paper gives a 2-approximation for this problem, and a
$2\Delta$-approximation (for $\Delta$-ary trees) where additionally the height
is $O(n)$. For trees with $\Delta\leq 3$, the former algorithm finds ideal
drawings with minimum-possible width."
"Given a set of points in the plane each colored either red or blue, we find
non-self-intersecting geometric spanning cycles of the red points and of the
blue points such that each edge of the red spanning cycle is crossed at most
three times by the blue spanning cycle and vice-versa."
"We study the problem of assigning transmission ranges to radio stations
placed arbitrarily in a $d$-dimensional ($d$-D) Euclidean space in order to
achieve a strongly connected communication network with minimum total power
consumption. The power required for transmitting in range $r$ is proportional
to $r^\alpha$, where $\alpha$ is typically between $1$ and $6$, depending on
various environmental factors. While this problem can be solved optimally in
$1$D, in higher dimensions it is known to be $NP$-hard for any $\alpha \geq 1$.
  For the $1$D version of the problem, i.e., radio stations located on a line
and $\alpha \geq 1$, we propose an optimal $O(n^2)$-time algorithm. This
improves the running time of the best known algorithm by a factor of $n$.
Moreover, we show a polynomial-time algorithm for finding the minimum cost
range assignment in $1$D whose induced communication graph is a $t$-spanner,
for any $t \geq 1$.
  In higher dimensions, finding the optimal range assignment is $NP$-hard;
however, it can be approximated within a constant factor. The best known
approximation ratio is for the case $\alpha=1$, where the approximation ratio
is $1.5$. We show a new approximation algorithm with improved approximation
ratio of $1.5-\epsilon$, where $\epsilon>0$ is a small constant."
"We are given a set of weighted unit disks and a set of points in Euclidean
plane. The minimum weight unit disk cover (\UDC) problem asks for a subset of
disks of minimum total weight that covers all given points. \UDC\ is one of the
geometric set cover problems, which have been studied extensively for the past
two decades (for many different geometric range spaces, such as (unit) disks,
halfspaces, rectangles, triangles). It is known that the unweighted \UDC\
problem is NP-hard and admits a polynomial-time approximation scheme (PTAS).
For the weighted \UDC\ problem, several constant approximations have been
developed. However, whether the problem admits a PTAS has been an open
question. In this paper, we answer this question affirmatively by presenting
the first PTAS for \UDC. Our result implies the first PTAS for the minimum
weight dominating set problem in unit disk graphs. Combining with existing
ideas, our result can also be used to obtain the first PTAS for the maxmimum
lifetime coverage problem and an improved constant approximation ratio for the
connected dominating set problem in unit disk graphs."
"It is well-known that the number of non-crossing perfect matchings of $2k$
points in convex position in the plane is $C_k$, the $k$th Catalan number.
Garc\'ia, Noy, and Tejel proved in 2000 that for any set of $2k$ points in
general position, the number of such matchings is at least $C_k$. We show that
the equality holds only for sets of points in convex position, and for one
exceptional configuration of $6$ points."
"For $\cal C$ a collection of $n$ objects in $R^d$, let the packing and
piercing numbers of $\cal C$, denoted by $Pack({\cal C})$, and $Pierce({\cal
C})$, respectively, be the largest number of pairwise disjoint objects in
${\cal C}$, and the smallest number of points in $R^d$ that are common to all
elements of ${\cal C}$, respectively. When elements of $\cal C$ are fat objects
of arbitrary sizes, we derive sub-exponential time algorithms for the NP-hard
problems of computing ${Pack}({\cal C})$ and $Pierce({\cal C})$, respectively,
that run in $n^{O_d({{Pack}({\cal C})}^{d-1\over d})}$ and
$n^{O_d({{Pierce}({\cal C})}^{d-1\over d})}$ time, respectively, and $O(n\log
n)$ storage. Our main tool which is interesting in its own way, is a new
separation theorem. The algorithms readily give rise to polynomial time
approximation schemes (PTAS) that run in $n^{O({({1\over\epsilon})}^{d-1})}$
time and $O(n\log n)$ storage. The results favorably compare with many related
best known results. Specifically, our separation theorem significantly improves
the splitting ratio of the previous result of Chan, whereas, the
sub-exponential time algorithms significantly improve upon the running times of
very recent algorithms of Fox and Pach for packing of spheres."
"We present an efficient algorithm that computes the Minkowski sum of two
polygons, which may have holes. The new algorithm is based on the convolution
approach. Its efficiency stems in part from a property for Minkowski sums of
polygons with holes, which in fact holds in any dimension: Given two polygons
with holes, for each input polygon we can fill up the holes that are relatively
small compared to the other polygon. Specifically, we can always fill up all
the holes of at least one polygon, transforming it into a simple polygon, and
still obtain exactly the same Minkowski sum. Obliterating holes in the input
summands speeds up the computation of Minkowski sums.
  We introduce a robust implementation of the new algorithm, which follows the
Exact Geometric Computation paradigm and thus guarantees exact results. We also
present an empirical comparison of the performance of Minkowski sum
construction of various input examples, where we show that the implementation
of the new algorithm exhibits better performance than several other
implementations in many cases. In particular, we compared the implementation of
the new algorithm, an implementation of the standard convolution algorithm, and
an implementation of the decomposition approach using various convex
decomposition methods, including two new methods that handle polygons with
holes - one is based on vertical decomposition and the other is based on
triangulation.
  The software has been developed as an extension of the ""2D Minkowski Sums""
package of CGAL (Computational Geometry Algorithms Library). Additional
information and supplementary material is available at our project page
http://acg.cs.tau.ac.il/projects/rc"
"Several researchers proposed using non-Euclidean metrics on point sets in
Euclidean space for clustering noisy data. Almost always, a distance function
is desired that recognizes the closeness of the points in the same cluster,
even if the Euclidean cluster diameter is large. Therefore, it is preferred to
assign smaller costs to the paths that stay close to the input points.
  In this paper, we consider the most natural metric with this property, which
we call the nearest neighbor metric. Given a point set P and a path $\gamma$,
our metric charges each point of $\gamma$ with its distance to P. The total
charge along $\gamma$ determines its nearest neighbor length, which is formally
defined as the integral of the distance to the input points along the curve. We
describe a $(3+\varepsilon)$-approximation algorithm and a
$(1+\varepsilon)$-approximation algorithm to compute the nearest neighbor
metric. Both approximation algorithms work in near-linear time. The former uses
shortest paths on a sparse graph using only the input points. The latter uses a
sparse sample of the ambient space, to find good approximate geodesic paths."
"In this paper we investigate the computational power of the polygonal tile
assembly model (polygonal TAM) at temperature 1, i.e. in non-cooperative
systems. The polygonal TAM is an extension of Winfree's abstract tile assembly
model (aTAM) which not only allows for square tiles (as in the aTAM) but also
allows for tile shapes that are polygons. Although a number of self-assembly
results have shown computational universality at temperature 1, these are the
first results to do so by fundamentally relying on tile placements in
continuous, rather than discrete, space. With the square tiles of the aTAM, it
is conjectured that the class of temperature 1 systems is not computationally
universal. Here we show that the class of systems whose tiles are composed of a
regular polygon P with n > 6 sides is computationally universal. On the other
hand, we show that the class of systems whose tiles consist of a regular
polygon P with n <= 6 cannot compute using any known techniques. In addition,
we show a number of classes of systems whose tiles consist of a non-regular
polygon with n >= 3 sides are computationally universal."
"We consider a generalization of the concept of $d$-flattenability of graphs -
introduced for the $l_2$ norm by Belk and Connelly - to general $l_p$ norms,
with integer $P$, $1 \le p < \infty$, though many of our results work for
$l_\infty$ as well. The following results are shown for graphs $G$, using
notions of genericity, rigidity, and generic $d$-dimensional rigidity matroid
introduced by Kitson for frameworks in general $l_p$ norms, as well as the
cones of vectors of pairwise $l_p^p$ distances of a finite point configuration
in $d$-dimensional, $l_p$ space: (i) $d$-flattenability of a graph $G$ is
equivalent to the convexity of $d$-dimensional, inherent Cayley configurations
spaces for $G$, a concept introduced by the first author; (ii)
$d$-flattenability and convexity of Cayley configuration spaces over specified
non-edges of a $d$-dimensional framework are not generic properties of
frameworks (in arbitrary dimension); (iii) $d$-flattenability of $G$ is
equivalent to all of $G$'s generic frameworks being $d$-flattenable; (iv)
existence of one generic $d$-flattenable framework for $G$ is equivalent to the
independence of the edges of $G$, a generic property of frameworks; (v) the
rank of $G$ equals the dimension of the projection of the $d$-dimensional
stratum of the $l_p^p$ distance cone. We give stronger results for specific
norms for $d = 2$: we show that (vi) 2-flattenable graphs for the $l_1$-norm
(and $l_\infty$-norm) are a larger class than 2-flattenable graphs for
Euclidean $l_2$-norm case and finally (vii) prove further results towards
characterizing 2-flattenability in the $l_1$-norm. A number of conjectures and
open problems are posed."
"Given a hypergraph $H$ with $m$ hyperedges and a set $X$ of $m$ \emph{pins},
i.e.\ globally fixed subspaces in Euclidean space $\mathbb{R}^d$, a
\emph{pinned subspace-incidence system} is the pair $(H, X)$, with the
constraint that each pin in $X$ lies on the subspace spanned by the point
realizations in $\mathbb{R}^d$ of vertices of the corresponding hyperedge of
$H$. We are interested in combinatorial characterization of pinned
subspace-incidence systems that are \emph{minimally rigid}, i.e.\ those systems
that are guaranteed to generically yield a locally unique realization. As is
customary, this is accompanied by a characterization of generic independence as
well as rigidity.
  In a previous paper \cite{sitharam2014incidence}, we used pinned
subspace-incidence systems towards solving the \emph{fitted dictionary
learning} problem, i.e.\ dictionary learning with specified underlying
hypergraph, and gave a combinatorial characterization of minimal rigidity for a
more restricted version of pinned subspace-incidence system, with $H$ being a
uniform hypergraph and pins in $X$ being 1-dimension subspaces. Moreover in a
recent paper \cite{Baker2015}, the special case of pinned line incidence
systems was used to model biomaterials such as cellulose and collagen fibrils
in cell walls. In this paper, we extend the combinatorial characterization to
general pinned subspace-incidence systems, with $H$ being a non-uniform
hypergraph and pins in $X$ being subspaces with arbitrary dimension. As there
are generally many data points per subspace in a dictionary learning problem,
which can only be modeled with pins of dimension larger than $1$, such an
extension enables application to a much larger class of fitted dictionary
learning problems."
"The linear-time ham-sandwich cut algorithm of Lo, Matou\v{s}ek, and Steiger
for bi-chromatic finite point sets in the plane works by appropriately
selecting crossings of the lines in the dual line arrangement with a set of
well-chosen vertical lines. We consider the setting where we are not given the
coordinates of the point set, but only the orientation of each point triple
(the order type) and give a deterministic linear-time algorithm for the
mentioned sub-algorithm. This yields a linear-time ham-sandwich cut algorithm
even in our restricted setting. We also show that our methods are applicable to
abstract order types."
"We resolve an open problem posed by Joswig et al. by providing an
$\tilde{O}(N)$ time, $O(\log^2(N))$-factor approximation algorithm for the
min-Morse unmatched problem (MMUP) Let $\Lambda$ be the no. of critical cells
of the optimal discrete Morse function and $N$ be the total no. of cells of a
regular cell complex K. The goal of MMUP is to find $\Lambda$ for a given
complex K. To begin with, we apply an approx. preserving graph reduction on
MMUP to obtain a new problem namely the min-partial order problem (min-POP)(a
strict generalization of the min-feedback arc set problem). The reduction
involves introduction of rigid edges which are edges that demand strict
inclusion in output solution. To solve min-POP, we use the Leighton- Rao
divide-&-conquer paradigm that provides solutions to SDP-formulated instances
of min-directed balanced cut with rigid edges (min-DBCRE). Our first algorithm
for min-DBCRE extends Agarwal et al.'s rounding procedure for digraph
formulation of ARV-algorithm to handle rigid edges. Our second algorithm to
solve min-DBCRE SDP, adapts Arora et al.'s primal dual MWUM. In terms of
applications, under the mild assumption1 of the size of topological features
being significantly smaller compared to the size of the complex, we obtain an
(a) $\tilde{O}(N)$ algorithm for computing homology groups $H_i(K,A)$ of a
simplicial complex K, (where A is an arbitrary Abelian group.) (b) an
$\tilde{O}(N^2)$ algorithm for computing persistent homology and (c) an
$\tilde{O}(N)$ algorithm for computing the optimal discrete Morse-Witten
function compatible with input scalar function as simple consequences of our
approximation algorithm for MMUP thereby giving us the best known complexity
bounds for each of these applications under the aforementioned assumption. Such
an assumption is realistic in applied settings, and often a characteristic of
modern massive datasets."
"Let $\mathcal L$ be a set of $n$ lines in the plane, and let $C$ be a convex
curve in the plane, like a circle or a parabola. The ""zone"" of $C$ in $\mathcal
L$, denoted $\mathcal Z(C,\mathcal L)$, is defined as the set of all cells in
the arrangement $\mathcal A(\mathcal L)$ that are intersected by $C$.
Edelsbrunner et al. (1992) showed that the complexity (total number of edges or
vertices) of $\mathcal Z(C,\mathcal L)$ is at most $O(n\alpha(n))$, where
$\alpha$ is the inverse Ackermann function. They did this by translating the
sequence of edges of $\mathcal Z(C,\mathcal L)$ into a sequence $S$ that avoids
the subsequence $ababa$. Whether the worst-case complexity of $\mathcal
Z(C,\mathcal L)$ is only linear is a longstanding open problem.
  Since the relaxation of the problem to pseudolines does have a
$\Theta(n\alpha(n))$ bound, any proof of $O(n)$ for the case of straight lines
must necessarily use geometric arguments.
  In this paper we present some such geometric arguments. We show that, if $C$
is a circle, then certain configurations of straight-line segments with
endpoints on $C$ are impossible. In particular, we show that there exists a
Hart-Sharir sequence that cannot appear as a subsequence of $S$.
  The Hart-Sharir sequences are essentially the only known way to construct
$ababa$-free sequences of superlinear length. Hence, if it could be shown that
every family of $ababa$-free sequences of superlinear-length eventually
contains all Hart-Sharir sequences, it would follow that the complexity of
$\mathcal Z(C,\mathcal L)$ is $O(n)$ whenever $C$ is a circle."
"We study a generalization of the classical problem of the illumination of
polygons. Instead of modeling a light source we model a wireless device whose
radio signal can penetrate a given number $k$ of walls. We call these objects
$k$-modems and study the minimum number of $k$-modems sufficient and sometimes
necessary to illuminate monotone and monotone orthogonal polygons. We show that
every monotone polygon with $n$ vertices can be illuminated with $\big\lceil
\frac{n-2}{2k+3} \big\rceil$ $k$-modems. In addition, we exhibit examples of
monotone polygons requiring at least $\lceil \frac {n-2} {2k+3}\rceil$
$k$-modems to be illuminated.
  For monotone orthogonal polygons with $n$ vertices we show that for $k=1$ and
for even $k$, every such polygon can be illuminated with $\big\lceil
\frac{n-2}{2k+4} \big\rceil$ $k$-modems, while for odd $k\geq3$, $\big\lceil
\frac{n-2}{2k+6} \big\rceil$ $k$-modems are always sufficient. Further, by
presenting according examples of monotone orthogonal polygons, we show that
both bounds are tight."
"We consider a generalization of the classical Art Gallery Problem, where
instead of a light source, the guards, called $k$-transmitters, model a
wireless device with a signal that can pass through at most $k$ walls. We show
it is NP-hard to compute a minimum cover of point 2-transmitters, point
$k$-transmitters, and edge 2-transmitters in a simple polygon. The point
2-transmitter result extends to orthogonal polygons. In addition, we give
necessity and sufficiency results for the number of edge 2-transmitters in
general, monotone, orthogonal monotone, and orthogonal polygons."
"Metric graphs are ubiquitous in science and engineering. For example, many
data are drawn from hidden spaces that are graph-like, such as the cosmic web.
A metric graph offers one of the simplest yet still meaningful ways to
represent the non-linear structure hidden behind the data. In this paper, we
propose a new distance between two finite metric graphs, called the
persistence-distortion distance, which draws upon a topological idea. This
topological perspective along with the metric space viewpoint provide a new
angle to the graph matching problem. Our persistence-distortion distance has
two properties not shared by previous methods: First, it is stable against the
perturbations of the input graph metrics. Second, it is a continuous distance
measure, in the sense that it is defined on an alignment of the underlying
spaces of input graphs, instead of merely their nodes. This makes our
persistence-distortion distance robust against, for example, different
discretizations of the same underlying graph. Despite considering the input
graphs as continuous spaces, that is, taking all points into account, we show
that we can compute the persistence-distortion distance in polynomial time. The
time complexity for the discrete case where only graph nodes are considered is
much faster. We also provide some preliminary experimental results to
demonstrate the use of the new distance measure."
"The goal of Point Distance Solving Problems is to find 2D or 3D placements of
points knowing distances between some pairs of points. The common guideline is
to solve them by a numerical iterative method (\emph{e.g.} Newton-Raphson
method). A sole solution is obtained whereas many exist. However the number of
solutions can be exponential and methods should provide solutions close to a
sketch drawn by the user.Geometric reasoning can help to simplify the
underlying system of equations by changing a few equations and triangularizing
it.This triangularization is a geometric construction of solutions, called
construction plan. We aim at finding several solutions close to the sketch on a
one-dimensional path defined by a global parameter-homotopy using a
construction plan. Some numerical instabilities may be encountered due to
specific geometric configurations. We address this problem by changing
on-the-fly the construction plan.Numerical results show that this hybrid method
is efficient and robust."
"Let $\Poly$ be a simple polygon with $n$ vertices. The \emph{dual graph}
$\triang^*$ of a triangulation~$\triang$ of~$\Poly$ is the graph whose vertices
correspond to the bounded faces of $\triang$ and whose edges connect those
faces of~$\triang$ that share an edge. We consider triangulations of~$\Poly$
that minimize or maximize the diameter of their dual graph. We show that both
triangulations can be constructed in $O(n^3\log n)$ time using dynamic
programming. If $\Poly$ is convex, we show that any minimizing triangulation
has dual diameter exactly $2\cdot\lceil\log_2(n/3)\rceil$ or
$2\cdot\lceil\log_2(n/3)\rceil -1$, depending on~$n$. Trivially, in this case
any maximizing triangulation has dual diameter $n-2$. Furthermore, we
investigate the relationship between the dual diameter and the number of
\emph{ears} (triangles with exactly two edges incident to the boundary of
$\Poly$) in a triangulation. For convex $\Poly$, we show that there is always a
triangulation that simultaneously minimizes the dual diameter and maximizes the
number of ears. In contrast, we give examples of general simple polygons where
every triangulation that maximizes the number of ears has dual diameter that is
quadratic in the minimum possible value. We also consider the case of point
sets in general position in the plane. We show that for any such set of $n$
points there are triangulations with dual diameter in~$O(\log n)$ and
in~$\Omega(\sqrt n)$."
"Let $G=(V(G), E(G))$ be an undirected graph with a measure function $\mu$
assigning non-negative values to subgraphs $H$ so that $\mu(H)$ does not exceed
the clique cover number of $H$. When $\mu$ satisfies some additional natural
conditions, we study the problem of separating $G$ into two subgraphs, each
with a measure of at most $2\mu(G)/3$ by removing a set of vertices that can be
covered with a small number of cliques $G$. When $E(G)=E(G_1)\cap E(G_2)$,
where $G_1=(V(G_1),E(G_1))$ is a graph with $V(G_1)=V(G)$, and $G_2=(V(G_2),
E(G_2))$ is a chordal graph with $V(G_2)=V(G)$, we prove that there is a
separator $S$ that can be covered with $O(\sqrt{l\mu(G)})$ cliques in $G$,
where $l=l(G,G_1)$ is a parameter similar to the bandwidth, which arises from
the linear orderings of cliques covers in $G_1$. The results and the methods
are then used to obtain exact and approximate algorithms which significantly
improve some of the past results for several well known NP-hard geometric
problems. In addition, the methods involve introducing new concepts and hence
may be of an independent interest."
"In this paper, we give a characterization of the visibility graphs of
pseudo-polygons. We first identify some key combinatorial properties of
pseudo-polygons, and we then give a set of five necessary conditions based off
our identified properties. We then prove that these necessary conditions are
also sufficient via a reduction to a characterization of vertex-edge visibility
graphs given by O'Rourke and Streinu."
"We investigate straight-line drawings of topological graphs that consist of a
planar graph plus one edge, also called almost-planar graphs. We present a
characterization of such graphs that admit a straight-line drawing. The
characterization enables a linear-time testing algorithm to determine whether
an almost-planar graph admits a straight-line drawing, and a linear-time
drawing algorithm that constructs such a drawing, if it exists. We also show
that some almost-planar graphs require exponential area for a straight-line
drawing."
"The task of approximation of points with circular arcs is performed in many
applications, such as polyline compression, noise filtering, and feature
recognition. However, development of algorithms that perform a significant
amount of circular arcs fitting require an efficient way of fitting circular
arcs with complexity O(1). The elegant solution to this task based on an
eigenvector problem for a square nonsymmetrical matrix is described in [1]. For
the compression algorithm described in [2], it is necessary to solve this task
when two points on the arc are known. This paper describes a different approach
to efficiently fitting the arcs and solves the task when one or two points are
known."
"There are many practical applications that require simplification of
polylines. Some of the goals are to reduce the amount of information necessary
to store, improve processing time, or simplify editing. The simplification is
usually done by removing some of the vertices, making the resultant polyline go
through a subset of the source polyline vertices. However, such approaches do
not necessarily produce a new polyline with the minimum number of vertices. The
approximate solution to find a polyline, within a specified tolerance, with the
minimum number of vertices is described in this paper."
"Our concern is the digitalization of line segments in two dimensions as
considered by Chun et al.[Discrete Comput. Geom., 2009] and Christ et
al.[Discrete Comput. Geom., 2012]. The key property that differentiates the
research of Chun et al. and Christ et al. from other research in digital line
segment construction is that the intersection of any two segments must be
connected. Such a system of segments is called a consistent digital line
segments system (CDS). Chun et al. give a construction for all segments in
higher dimensions that share a common endpoint (called consistent digital rays
(CDR)) that has asymptotically optimal Hausdorff distance, and Christ et al.
give a complete CDS in two dimensions with optimal Hausdorff distance. Christ
et al. also give a characterization of CDRs in two dimensions, and they leave
open the question on how to characterize CDSes in two dimensions. In this
paper, we answer the most important open question regarding CDSes in two
dimensions by giving the characterization asked for by Christ et al. We obtain
the characterization by giving a set of necessary and sufficient conditions
that a CDS must satisfy."
"$\renewcommand{\Re}{{\rm I\!\hspace{-0.025em} R}}
\newcommand{\eps}{{\varepsilon}} \newcommand{\SetX}{\mathsf{X}}
\newcommand{\VorX}[1]{\mathcal{V} \pth{#1}} \newcommand{\Polygon}{\mathsf{P}}
\newcommand{\Space}{\overline{\mathsf{m}}}
\newcommand{\pth}[2][\!]{#1\left({#2}\right)}$ We revisit the problem of
computing Fr\'echet distance between polygonal curves under $L_1$, $L_2$, and
$L_\infty$ norms, focusing on discrete Fr\'echet distance, where only distance
between vertices is considered. We develop efficient algorithms for two natural
classes of curves. In particular, given two polygonal curves of $n$ vertices
each, a $\eps$-approximation of their discrete Fr\'echet distance can be
computed in roughly $O(n\kappa^3\log n/\eps^3)$ time in three dimensions, if
one of the curves is \emph{$\kappa$-bounded}. Previously, only a
$\kappa$-approximation algorithm was known. If both curves are the so-called
\emph{\backbone~curves}, which are widely used to model protein backbones in
molecular biology, we can $\eps$-approximate their Fr\'echet distance in near
linear time in two dimensions, and in roughly $O(n^{4/3}\log nm)$ time in three
dimensions. In the second part, we propose a pseudo--output-sensitive algorithm
for computing Fr\'echet distance exactly. The complexity of the algorithm is a
function of a quantity we call the \emph{\bwnumber{}}, which is quadratic in
the worst case, but tends to be much smaller in practice."
"We give a $O(n)$-time algorithm for determining whether translations of a
polyomino with $n$ edges can tile the plane. The algorithm is also a
$O(n)$-time algorithm for enumerating all such tilings that are also regular,
and we prove that at most $\Theta(n)$ such tilings exist."
"The steady development of motor vehicle technology will enable cars of the
near future to assume an ever increasing role in the decision making and
control of the vehicle itself. In the foreseeable future, cars will have the
ability to communicate with one another in order to better coordinate their
motion. This motivates a number of interesting algorithmic problems. One of the
most challenging aspects of traffic coordination involves traffic
intersections. In this paper we consider two formulations of a simple and
fundamental geometric optimization problem involving coordinating the motion of
vehicles through an intersection.
  We are given a set of $n$ vehicles in the plane, each modeled as a unit
length line segment that moves monotonically, either horizontally or
vertically, subject to a maximum speed limit. Each vehicle is described by a
start and goal position and a start time and deadline. The question is whether,
subject to the speed limit, there exists a collision-free motion plan so that
each vehicle travels from its start position to its goal position prior to its
deadline.
  We present three results. We begin by showing that this problem is
NP-complete with a reduction from 3-SAT. Second, we consider a constrained
version in which cars traveling horizontally can alter their speeds while cars
traveling vertically cannot. We present a simple algorithm that solves this
problem in $O(n \log n)$ time. Finally, we provide a solution to the discrete
version of the problem and prove its asymptotic optimality in terms of the
maximum delay of a vehicle."
"The convex hull describes the extent or shape of a set of data and is used
ubiquitously in computational geometry. Common algorithms to construct the
convex hull on a finite set of n points (x,y) range from O(nlogn) time to O(n)
time. However, it is often the case that a heuristic procedure is applied to
reduce the original set of n points to a set of s < n points which contains the
hull and so accelerates the final hull finding procedure. We present an
algorithm to precondition data before building a 2D convex hull with integer
coordinates, with three distinct advantages. First, for all practical purposes,
it is linear; second, no explicit sorting of data is required and third, the
reduced set of s points is constructed such that it forms an ordered set that
can be directly pipelined into an O(n) time convex hull algorithm. Under these
criteria a fast (or O(n)) pre-conditioner in principle creates a fast convex
hull (approximately O(n)) for an arbitrary set of points. The paper empirically
evaluates and quantifies the acceleration generated by the method against the
most common convex hull algorithms. An extra acceleration of at least four
times when compared to previous existing preconditioning methods is found from
experiments on a dataset."
"A crossing-free straight-line drawing of a graph is monotone if there is a
monotone path between any pair of vertices with respect to some direction. We
show how to construct a monotone drawing of a tree with $n$ vertices on an
$O(n^{1.5}) \times O(n^{1.5})$ grid whose angles are close to the best possible
angular resolution. Our drawings are convex, that is, if every edge to a leaf
is substituted by a ray, the (unbounded) faces form convex regions. It is known
that convex drawings are monotone and, in the case of trees, also
crossing-free.
  A monotone drawing is strongly monotone if, for every pair of vertices, the
direction that witnesses the monotonicity comes from the vector that connects
the two vertices. We show that every tree admits a strongly monotone drawing.
For biconnected outerplanar graphs, this is easy to see. On the other hand, we
present a simply-connected graph that does not have a strongly monotone drawing
in any embedding."
"In this work, we design a nearly linear time discrete Morse theory based
algorithm for computing homology groups of 2-manifolds, thereby establishing
the fact that computing homology groups of 2-manifolds is remarkably easy.
Unlike previous algorithms of similar flavor, our method works with
coefficients from arbitrary abelian groups. Another advantage of our method
lies in the fact that our algorithm actually elucidates the topological reason
that makes computation on 2-manifolds easy. This is made possible owing to a
new simple homotopy based construct that is referred to as \emph{expansion
frames}. To being with we obtain an optimal discrete gradient vector field
using expansion frames. This is followed by a pseudo-linear time dynamic
programming based computation of discrete Morse boundary operator. The
efficient design of optimal gradient vector field followed by fast computation
of boundary operator affords us near linearity in computation of homology
groups. Moreover, we define a new criterion for nearly optimal Morse functions
called pseudo-optimality. A Morse function is pseudo-optimal if we can obtain
an optimal Morse function from it, simply by means of critical cell
cancellations. Using expansion frames, we establish the surprising fact that an
arbitrary discrete Morse function on 2-manifolds is pseudo-optimal."
"We study approximation algorithms for the following geometric version of the
maximum coverage problem: Let P be a set of n weighted points in the plane. We
want to place m a * b rectangles such that the sum of the weights of the points
in P covered by these rectangles is maximized.For any fixed > 0, we present
efficient approximation schemes that can find (1-{\epsilon})-approximation to
the optimal solution.In particular, for m = 1, our algorithm runs in linear
time O(n log( 1/{\epsilon})), improving over the previous result. For m > 1, we
present an algorithm that runs in
O(n/{\epsilon}log(1/{\epsilon})+m(1/{\epsilon})^(O(min(sqrt(m),1/{\epsilon})))
time."
"Motivated by biological questions, we study configurations of equal-sized
disks in the Euclidean plane that neither pack nor cover. Measuring the quality
by the probability that a random point lies in exactly one disk, we show that
the regular hexagonal grid gives the maximum among lattice configurations."
"Given the coordinates of four terminals in the Euclidean plane we present
explicit formulas for Steiner point coordinates for Steiner minimal tree
problem. We utilize the obtained formulas for evaluation of the influence of
terminal coordinates on the loci of Steiner points."
"It has long been known that $d$-dimensional Euclidean point sets admit
$(1+\epsilon)$-stretch spanners with lightness $W_E = \epsilon^{-O(d)}$, that
is total edge weight at most $W_E$ times the weight of the minimum spaning tree
of the set [DHN93]. Whether or not a similar result holds for metric spaces
with low doubling dimension has remained an important open problem, and has
resisted numerous attempts at resolution. In this paper, we resolve the
question in the affirmative, and show that doubling spaces admit
$(1+\epsilon)$-stretch spanners with lightness $W_D =
(ddim/\epsilon)^{O(ddim)}$.
  Important in its own right, our result also implies a much faster
polynomial-time approximation scheme for the traveling salesman problemin
doubling metric spaces, improving upon the bound presented in [BGK-12]."
"The article analyzes similarity of closed polygonal curves with respect to
the Frechet metric, which is stronger than the well-known Hausdorff metric and
therefore is more appropriate in some applications. An algorithm is described
that determines whether the Frechet distance between two closed polygonal
curves with m and n vertices is less than a given number. The algorithm takes
O(mn) time whereas the previously known algorithms take O(mn log(mn)) time."
"Let $\langle G_r,G_b \rangle$ be a pair of plane $st$-graphs with the same
vertex set $V$. A simultaneous visibility representation with L-shapes of
$\langle G_r,G_b \rangle$ is a pair of bar visibility representations
$\langle\Gamma_r,\Gamma_b\rangle$ such that, for every vertex $v \in V$,
$\Gamma_r(v)$ and $\Gamma_b(v)$ are a horizontal and a vertical segment, which
share an end-point. In other words, every vertex is drawn as an $L$-shape,
every edge of $G_r$ is a vertical visibility segment, and every edge of $G_b$
is a horizontal visibility segment. Also, no two L-shapes intersect each other.
An L-shape has four possible rotations, and we assume that each vertex is given
a rotation for its L-shape as part of the input. Our main results are: (i) a
characterization of those pairs of plane $st$-graphs admitting such a
representation, (ii) a cubic time algorithm to recognize them, and (iii) a
linear time drawing algorithm if the test is positive."
"The Morton- or z-curve is one example for a space filling curve: Given a
level of refinement L, it maps the interval [0, 2**dL) one-to-one to a set of
d-dimensional cubes of edge length 2**-L that form a subdivision of the unit
cube. Similar curves have been proposed for triangular and tetrahedral unit
domains. In contrast to the Hilbert curve that is continuous, the Morton-type
curves produce jumps.
  We prove that any contiguous subinterval of the curve divides the domain into
a bounded number of face-connected subdomains. For the hypercube case and
arbitrary dimension, the subdomains are star-shaped and the bound is indeed
two. For the simplicial case in dimensions 2 and 3, the bound is proportional
to the depth of refinement L. We supplement the paper with theoretical and
computational studies on the frequency of jumps for a quantitative assessment."
"We establish tight bounds for beacon-based coverage problems, and improve the
bounds for beacon-based routing problems in simple rectilinear polygons.
Specifically, we show that $\lfloor \frac{n}{6} \rfloor$ beacons are always
sufficient and sometimes necessary to cover a simple rectilinear polygon $P$
with $n$ vertices. We also prove tight bounds for the case where $P$ is
monotone, and we present an optimal linear-time algorithm that computes the
beacon-based kernel of $P$. For the routing problem, we show that $\lfloor
\frac{3n-4}{8} \rfloor - 1$ beacons are always sufficient, and $\lceil
\frac{n}{4}\rceil-1$ beacons are sometimes necessary to route between all pairs
of points in $P$."
"Computing Delaunay triangulations in $\mathbb{R}^d$ involves evaluating the
so-called in\_sphere predicate that determines if a point $x$ lies inside, on
or outside the sphere circumscribing $d+1$ points $p_0,\ldots ,p_d$. This
predicate reduces to evaluating the sign of a multivariate polynomial of degree
$d+2$ in the coordinates of the points $x, \, p_0,\, \ldots,\, p_d$. Despite
much progress on exact geometric computing, the fact that the degree of the
polynomial increases with $d$ makes the evaluation of the sign of such a
polynomial problematic except in very low dimensions. In this paper, we propose
a new approach that is based on the witness complex, a weak form of the
Delaunay complex introduced by Carlsson and de Silva. The witness complex
$\mathrm{Wit} (L,W)$ is defined from two sets $L$ and $W$ in some metric space
$X$: a finite set of points $L$ on which the complex is built, and a set $W$ of
witnesses that serves as an approximation of $X$. A fundamental result of de
Silva states that $\mathrm{Wit}(L,W)=\mathrm{Del} (L)$ if $W=X=\mathbb{R}^d$.
In this paper, we give conditions on $L$ that ensure that the witness complex
and the Delaunay triangulation coincide when $W$ is a finite set, and we
introduce a new perturbation scheme to compute a perturbed set $L'$ close to
$L$ such that $\mathrm{Del} (L')= \mathrm{wit} (L', W)$. Our perturbation
algorithm is a geometric application of the Moser-Tardos constructive proof of
the Lov\'asz local lemma. The only numerical operations we use are (squared)
distance comparisons (i.e., predicates of degree 2). The time-complexity of the
algorithm is sublinear in $|W|$. Interestingly, although the algorithm does not
compute any measure of simplex quality, a lower bound on the thickness of the
output simplices can be guaranteed."
"The Gilbert-Johnson-Keerthi (GJK) algorithm is an iterative improvement
technique for finding the minimum distance between two convex objects. It can
easily be extended to work with concave objects and return the pair of closest
points. [4] The key operation of GJK is testing whether a Voronoi region of a
simplex contains the origin or not. In this paper we show that, in the context
where one is interested only in the Boolean value of whether two convex objects
intersect, and not in the actual distance between them, the number of test
cases in GJK can be significantly reduced. This results in a simpler and more
efficient algorithm that can be used in many computational geometry
applications."
"Given a set \mathcal{P} of non-intersecting polygonal obstacles in
\mathbb{R}^2 defined with n vertices, we compute a sketch \Omega of \mathcal{P}
whose size is independent of n. We utilize \Omega to devise an algorithm to
compute a (1+\epsilon)-approximate Euclidean shortest path between two points
given with the description of \mathcal{P}. When \mathcal{P} comprises of convex
polygonal obstacles, we devise a (2+\epsilon)-approximation algorithm to
efficiently answer two-point Euclidean distance queries."
"An upward drawing of a tree is a drawing such that no parents are below their
children. It is order-preserving if the edges to children appear in prescribed
order around each node. Chan showed that any tree has an upward
order-preserving drawing with width O(log n). In this paper, we present
linear-time algorithms that finds upward with instance-optimal width, i.e., the
width is the minimum-possible for the input tree.
  We study two different models. In the first model, the drawings need not be
order-preserving; a very simple algorithm then finds straight-line drawings of
optimal width. In the second model, the drawings must be order-preserving; and
we give an algorithm that finds optimum-width poly-line drawings, i.e., edges
are allowed to have bends. We also briefly study order-preserving upward
straight-line drawings, and show that some trees require larger width if
drawings must be straight-line."
"Minimizing the number of probes is one of the main challenges in
reconstructing geometric objects with probing devices. In this paper, we
investigate the problem of using an $\omega$-wedge probing tool to determine
the exact shape and orientation of a convex polygon. An $\omega$-wedge consists
of two rays emanating from a point called the apex of the wedge and the two
rays forming an angle $\omega$. To probe with an $\omega$-wedge, we set the
direction that the apex of the probe has to follow, the line $\overrightarrow
L$, and the initial orientation of the two rays. A valid $\omega$-probe of a
convex polygon $O$ contains $O$ within the $\omega$-wedge and its outcome
consists of the coordinates of the apex, the orientation of both rays and the
coordinates of the closest (to the apex) points of contact between $O$ and each
of the rays.
  We present algorithms minimizing the number of probes and prove their
optimality. In particular, we show how to reconstruct a convex $n$-gon (with
all internal angles of size larger than $\omega$) using $2n-2$ $\omega$-probes;
if $\omega = \pi/2$, the reconstruction uses $2n-3$ $\omega$-probes. We show
that both results are optimal. Let $N_B$ be the number of vertices of $O$ whose
internal angle is at most $\omega$, (we show that $0 \leq N_B \leq 3$). We
determine the shape and orientation of a general convex $n$-gon with $N_B=1$
(respectively $N_B=2$, $N_B=3$) using $2n-1$ (respectively $2n+3$, $2n+5$)
$\omega$-probes. We prove optimality for the first case. Assuming the algorithm
knows the value of $N_B$ in advance, the reconstruction of $O$ with $N_B=2$ or
$N_B=3$ can be achieved with $2n+2$ probes,- which is optimal."
"We present a geometric perspective on sparse filtrations used in topological
data analysis. This new perspective leads to much simpler proofs, while also
being more general, applying equally to Rips filtrations and Cech filtrations
for any convex metric. We also give an algorithm for finding the simplices in
such a filtration and prove that the vertex removal can be implemented as a
sequence of elementary edge collapses."
"This paper presents a counterexample for the approximation algorithm proposed
by Durocher and Mehrabi [1] for the general problem of finding a rectangular
partition of a rectilinear polygon with minimum stabbing number."
"We introduce the discrete Fr\'echet gap and its variants as an alternative
measure of similarity between polygonal curves. We believe that for some
applications the new measure (and its variants) may better reflect our
intuitive notion of similarity than the discrete Fr\'echet distance (and its
variants), since the latter measure is indifferent to (matched) pairs of points
that are relatively close to each other. Referring to the frogs analogy by
which the discrete Fr\'echet distance is often described, the discrete
Fr\'echet gap is the minimum difference between the longest and shortest
positions of the leash needed for the frogs to traverse their point sequences.
  We present an optimization scheme, which is suitable for any monotone
function defined for pairs of distances such as the gap and ratio functions. We
apply this scheme to two variants of the discrete Fr\'echet gap, namely, the
one-sided discrete Fr\'echet gap with shortcuts and the weak discrete Fr\'echet
gap, to obtain $O(n^2 \log^2 n)$-time algorithms for computing them."
"In 1978 Erd\H os asked if every sufficiently large set of points in general
position in the plane contains the vertices of a convex $k$-gon, with the
additional property that no other point of the set lies in its interior.
Shortly after, Horton provided a construction---which is now called the Horton
set---with no such $7$-gon. In this paper we show that the Horton set of $n$
points can be realized with integer coordinates of absolute value at most
$\frac{1}{2} n^{\frac{1}{2} \log (n/2)}$. We also show that any set of points
with integer coordinates combinatorially equivalent (with the same order type)
to the Horton set, contains a point with a coordinate of absolute value at
least $c \cdot n^{\frac{1}{24}\log (n/2)}$, where $c$ is a positive constant."
"The Morse-Smale complex is an important tool for global topological analysis
in various problems of computational geometry and topology. Algorithms for
Morse-Smale complexes have been presented in case of piecewise linear
manifolds. However, previous research in this field is incomplete in the case
of smooth functions. In the current paper we address the following question:
Given an arbitrarily complex Morse-Smale system on a planar domain, is it
possible to compute its certified (topologically correct) Morse-Smale complex?
Towards this, we develop an algorithm using interval arithmetic to compute
certified critical points and separatrices forming the Morse-Smale complexes of
smooth functions on bounded planar domain. Our algorithm can also compute
geometrically close Morse-Smale complexes."
"We revisit the traveling salesman problem with neighborhoods (TSPN) and
present the first constant-ratio approximation for disks in the plane: Given a
set of $n$ disks in the plane, a TSP tour whose length is at most $O(1)$ times
the optimal can be computed in time that is polynomial in $n$. Our result is
the first constant-ratio approximation for a class of planar convex bodies of
arbitrary size and arbitrary intersections. In order to achieve a
$O(1)$-approximation, we reduce the traveling salesman problem with disks, up
to constant factors, to a minimum weight hitting set problem in a geometric
hypergraph. The connection between TSPN and hitting sets in geometric
hypergraphs, established here, is likely to have future applications."
"We describe an algorithm that builds a plane spanner with a maximum degree of
8 and a spanning ratio of approximately 4.414 with respect to the complete
graph. This is the best currently known spanning ratio for a plane spanner with
a maximum degree of less than 14."
"Optimal recursive decomposition (or DR-planning) is crucial for analyzing,
designing, solving or finding realizations of geometric constraint sytems.
While the optimal DR-planning problem is NP-hard even for general 2D bar-joint
constraint systems, we describe an O(n^3) algorithm for a broad class of
constraint systems that are isostatic or underconstrained. The algorithm
achieves optimality by using the new notion of a canonical DR-plan that also
meets various desirable, previously studied criteria. In addition, we leverage
recent results on Cayley configuration spaces to show that the indecomposable
systems---that are solved at the nodes of the optimal DR-plan by recombining
solutions to child systems---can be minimally modified to become decomposable
and have a small DR-plan, leading to efficient realization algorithms. We show
formal connections to well-known problems such as completion of
underconstrained systems. Well suited to these methods are classes of
constraint systems that can be used to efficiently model, design and analyze
quasi-uniform (aperiodic) and self-similar, layered material structures. We
formally illustrate by modeling silica bilayers as body-hyperpin systems and
cross-linking microfibrils as pinned line-incidence systems. A software
implementation of our algorithms and videos demonstrating the software are
publicly available online (visit http://cise.ufl.edu/~tbaker/drp/index.html.)"
"$\newcommand{\eps}{\varepsilon}$ In this paper, we consider two important
problems defined on finite metric spaces, and provide efficient new algorithms
and approximation schemes for these problems on inputs given as graph shortest
path metrics or high-dimensional Euclidean metrics. The first of these problems
is the greedy permutation (or farthest-first traversal) of a finite metric
space: a permutation of the points of the space in which each point is as far
as possible from all previous points. We describe randomized algorithms to find
$(1+\eps)$-approximate greedy permutations of any graph with $n$ vertices and
$m$ edges in expected time $O(\eps^{-1}(m+n)\log n\log(n/\eps))$, and to find
$(1+\eps)$-approximate greedy permutations of points in high-dimensional
Euclidean spaces in expected time $O(\eps^{-2} n^{1+1/(1+\eps)^2 + o(1)})$.
Additionally we describe a deterministic algorithm to find exact greedy
permutations of any graph with $n$ vertices and treewidth $O(1)$ in worst-case
time $O(n^{3/2}\log^{O(1)} n)$. The second of the two problems we consider is
distance selection: given $k \in [ \binom{n}{2} ]$, we are interested in
computing the $k$th smallest distance in the given metric space. We show that
for planar graph metrics one can approximate this distance, up to a constant
factor, in near linear time."
"Fix positive integers $a$ and $b$ such that $a> b\geq 2$ and a positive real
$\delta>0$. Let $S$ be a planar set of diameter $\delta$ having the following
property: for every $a$ points in $S$, at least $b$ of them have pairwise
distances that are all less than or equal to $2$. What is the maximum Lebesgue
measure of $S$? In this paper we investigate this problem. We discuss the,
devious, motivation that leads to its formulation and provide upper bounds on
the Lebesgue measure of $S$. Our main result is based on a generalisation of a
theorem that is due to Heinrich Jung. In certain instances we are able to find
the extremal set but the general case seems elusive."
"A plane tiling consisting of congruent copies of a shape is isohedral
provided that for any pair of copies, there exists a symmetry of the tiling
mapping one copy to the other. We give a $O(n\log^2{n})$-time algorithm for
deciding if a polyomino with $n$ edges can tile the plane isohedrally. This
improves on the $O(n^{18})$-time algorithm of Keating and Vince and generalizes
recent work by Brlek, Proven\c{c}al, F\'{e}dou, and the second author."
"Several aspects of managing a sensor network (e.g., motion planning for data
mules, serial data fusion and inference) benefit once the network is linearized
to a path. The linearization is often achieved by constructing a space filling
curve in the domain. However, existing methods cannot handle networks
distributed on surfaces of complex topology. This paper presents a novel method
for generating space filling curves for 3D sensor networks that are distributed
densely on some two-dimensional geometric surface. Our algorithm is completely
distributed and constructs a path which gets uniformly, progressively denser as
it becomes longer. We analyze the algorithm mathematically and prove that the
curve we obtain is dense. Our method is based on the Hodge decomposition
theorem and uses holomorphic differentials on Riemann surfaces. The underlying
high genus surface is conformally mapped to a union of flat tori and then a
proportionally-dense space filling curve on this union is constructed. The
pullback of this curve to the original network gives us the desired curve."
"Beacon attraction is a movement system whereby a robot (modeled as a point in
2D) moves in a free space so as to always locally minimize its Euclidean
distance to an activated beacon (which is also a point). This results in the
robot moving directly towards the beacon when it can, and otherwise sliding
along the edge of an obstacle. When a robot can reach the activated beacon by
this method, we say that the beacon attracts the robot. A beacon routing from
$p$ to $q$ is a sequence $b_1, b_2,$ ..., $b_{k}$ of beacons such that
activating the beacons in order will attract a robot from $p$ to $b_1$ to $b_2$
... to $b_{k}$ to $q$, where $q$ is considered to be a beacon. A routing set of
beacons is a set $B$ of beacons such that any two points $p, q$ in the free
space have a beacon routing with the intermediate beacons $b_1, b_2,$ ...,
$b_{k}$ all chosen from $B$. Here we address the question of ""how large must
such a $B$ be?"" in orthogonal polygons, and show that the answer is ""sometimes
as large as $[(n-4)/3]$, but never larger."""
"Computational mathematics plays an increasingly important role in
computational fluid dynamics (CFD). The aeronautics and aerospace re- search
community is working on next generation of CFD capacity that is accurate,
automatic, and fast. A key component of the next generation of CFD is a greatly
enhanced capacity for mesh generation and adaptivity of the mesh according to
solution and geometry. In this paper, we propose a new method that generates
triangular meshes on domains of curved boundary. The method deforms a Cartesian
mesh that covers the domain to generate a mesh with prescribed boundary nodes.
The deformation fields are generated by a system of divergence and curl
equations which are solved effectively by the least square finite element
method."
"Adaptive grid generation is an active research topic for numer- ical solution
of differential equations. In this paper, we propose a variational method which
generates transformations with prescribed Jacobian determinant and curl. Then
we use this transformation to achieve adaptive grid generation task, and show
the importance of curl in a transformation."
"In this paper we study similarity measures for moving curves which can, for
example, model changing coastlines or retreating glacier termini. Points on a
moving curve have two parameters, namely the position along the curve as well
as time. We therefore focus on similarity measures for surfaces, specifically
the Fr\'echet distance between surfaces. While the Fr\'echet distance between
surfaces is not even known to be computable, we show for variants arising in
the context of moving curves that they are polynomial-time solvable or
NP-complete depending on the restrictions imposed on how the moving curves are
matched. We achieve the polynomial-time solutions by a novel approach for
computing a surface in the so-called free-space diagram based on max-flow
min-cut duality."
"Let $P$ be a convex polyhedron in $\mathbb{R}^3$. The skeleton of $P$ is the
graph whose vertices and edges are the vertices and edges of $P$, respectively.
We prove that, if these vertices are on the unit-sphere, the skeleton is a
$(0.999 \cdot \pi)$-spanner. If the vertices are very close to this sphere,
then the skeleton is not necessarily a spanner. For the case when the boundary
of $P$ is between two concentric spheres of radii $1$ and $R>1$, and the angles
in all faces are at least $\theta$, we prove that the skeleton is a
$t$-spanner, where $t$ depends only on $R$ and $\theta$. One of the ingredients
in the proof is a tight upper bound on the geometric dilation of a convex cycle
that is contained in an annulus."
"The regular polyhedra have the highest order of 3D symmetries and are
exceptionally at- tractive templates for (self)-assembly using minimal types of
building blocks, from nano-cages and virus capsids to large scale constructions
like glass domes. However, they only represent a small number of possible
spherical layouts which can serve as templates for symmetric assembly. In this
paper, we formalize the necessary and sufficient conditions for symmetric
assembly using exactly one type of building block. All such assemblies
correspond to spherical polyhedra which are edge-transitive and
face-transitive, but not necessarily vertex-transitive. This describes a new
class of polyhedra outside of the well-studied Platonic, Archimedean, Catalan
and and Johnson solids. We show that this new family, dubbed almost-regular
polyhedra, can be pa- rameterized using only two variables and provide an
efficient algorithm to generate an infinite series of such polyhedra.
Additionally, considering the almost-regular polyhedra as templates for the
assembly of 3D spherical shell structures, we developed an efficient polynomial
time shell assembly approximation algorithm for an otherwise NP-hard geometric
optimization problem."
"An IC-plane graph is a topological graph where every edge is crossed at most
once and no two crossed edges share a vertex. We show that every IC-plane graph
has a visibility drawing where every vertex is an L-shape, and every edge is
either a horizontal or vertical segment. As a byproduct of our drawing
technique, we prove that an IC-plane graph has a RAC drawing in quadratic area
with at most two bends per edge."
"The straight skeleton of a polygon is the geometric graph obtained by tracing
the vertices during a mitered offsetting process. It is known that the straight
skeleton of a simple polygon is a tree, and one can naturally derive directions
on the edges of the tree from the propagation of the shrinking process.
  In this paper, we ask the reverse question: Given a tree with directed edges,
can it be the straight skeleton of a polygon? And if so, can we find a suitable
simple polygon? We answer these questions for all directed trees where the
order of edges around each node is fixed."
"We study computational aspects of the General Position Subset Selection
problem defined as follows: Given a set of points in the plane, find a
maximum-cardinality subset of points in general position. We prove that General
Position Subset Selection is NP-hard, APX-hard, and give several
fixed-parameter tractability results as well as a subexponential running time
lower bound based on the Exponential Time Hypothesis."
"R-trees can be used to store and query sets of point data in two or more
dimensions. An easy way to construct and maintain R-trees for two-dimensional
points, due to Kamel and Faloutsos, is to keep the points in the order in which
they appear along the Hilbert curve. The R-tree will then store bounding boxes
of points along contiguous sections of the curve, and the efficiency of the
R-tree depends on the size of the bounding boxes---smaller is better. Since
there are many different ways to generalize the Hilbert curve to higher
dimensions, this raises the question which generalization results in the
smallest bounding boxes. Familiar methods, such as the one by Butz, can result
in curve sections whose bounding boxes are a factor $\Omega(2^{d/2})$ larger
than the volume traversed by that section of the curve. Most of the volume
bounded by such bounding boxes would not contain any data points. In this paper
we present a new way of generalizing Hilbert's curve to higher dimensions,
which results in much tighter bounding boxes: they have at most 4 times the
volume of the part of the curve covered, independent of the number of
dimensions. Moreover, we prove that a factor 4 is asymptotically optimal."
"An important problem in geometric computing is defining and computing
similarity between two geometric shapes, e.g. point sets, curves and surfaces,
etc. Important geometric and topological information of many shapes can be
captured by defining a tree structure on them (e.g. medial axis and contour
trees). Hence, it is natural to study the problem of comparing similarity
between trees. We study gapped edit distance between two ordered labeled trees,
first proposed by Touzet \cite{Touzet2003}.
  Given two binary trees $T_{1}$ and $T_{2}$ with $m$ and $n$ nodes. We compute
the general gap edit distance in $O(m^{3}n^{2} + m^{2}n^{3})$ time. The
computation of this distance in the case of arbitrary trees has shown to be
NP-hard \cite{Touzet2003}. We also give an algorithm for computing the complete
subtree gap edit distance, which can be applied to comparing contour trees of
terrains in $\mathbb{R}^{3}$."
"This paper presents a practical GPU-accelerated convex hull algorithm and a
novel Sorting-based Preprocessing Approach (SPA) for planar point sets. The
proposed algorithm consists of two stages: (1) two rounds of preprocessing
performed on the GPU and (2) the finalization of calculating the expected
convex hull on the CPU. We first discard the interior points that locate inside
a quadrilateral formed by four extreme points, and then distribute the
remaining points into several (typically four) sub regions. For each subset of
points, we first sort them in parallel, then perform the second round of
discarding using SPA, and finally form a simple chain for the current remaining
points. A simple polygon can be easily generated by directly connecting all the
chains in sub regions. We at last obtain the expected convex hull of the input
points by calculating the convex hull of the simple polygon. We use the library
Thrust to realize the parallel sorting, reduction, and partitioning for better
efficiency and simplicity. Experimental results show that our algorithm
achieves 5x ~ 6x speedups over the Qhull implementation for 20M points. Thus,
this algorithm is competitive in practical applications for its simplicity and
satisfied efficiency."
"A point set $S \subseteq \mathbb{R}^2$ is universal for a class $\cal G$ if
every graph of ${\cal G}$ has a planar straight-line embedding on $S$. It is
well-known that the integer grid is a quadratic-size universal point set for
planar graphs, while the existence of a sub-quadratic universal point set for
them is one of the most fascinating open problems in Graph Drawing. Motivated
by the fact that outerplanarity is a key property for the existence of small
universal point sets, we study 2-outerplanar graphs and provide for them a
universal point set of size $O(n \log n)$."
"This paper presents a fast implementation of the Graham scan on the GPU. The
proposed algorithm is composed of two stages: (1) two rounds of preprocessing
performed on the GPU and (2) the finalization of finding the convex hull on the
CPU. We first discard the interior points that locate inside a quadrilateral
formed by four extreme points, sort the remaining points according to the
angles, and then divide them into the left and the right regions. For each
region, we perform a second round of filtering using the proposed preprocessing
approach to discard the interior points in further. We finally obtain the
expected convex hull by calculating the convex hull of the remaining points on
the CPU. We directly employ the parallel sorting, reduction, and partitioning
provided by the library Thrust for better efficiency and simplicity.
Experimental results show that our implementation achieves 6x ~ 7x speedups
over the Qhull implementation for 20M points."
"Set membership of points in the plane can be visualized by connecting
corresponding points via graphical features, like paths, trees, polygons,
ellipses. In this paper we study the \emph{bus embeddability problem} (BEP):
given a set of colored points we ask whether there exists a planar realization
with one horizontal straight-line segment per color, called bus, such that all
points with the same color are connected with vertical line segments to their
bus. We present an ILP and an FPT algorithm for the general problem. For
restricted versions of this problem, such as when the relative order of buses
is predefined, or when a bus must be placed above all its points, we provide
efficient algorithms. We show that another restricted version of the problem
can be solved using 2-stack pushall sorting. On the negative side we prove the
NP-completeness of a special case of BEP."
"We study a turn-based game in a simply connected polygonal environment $Q$
between a pursuer $P$ and an adversarial evader $E$. Both players can move in a
straight line to any point within unit distance during their turn. The pursuer
$P$ wins by capturing the evader, meaning that their distance satisfies $d(P,
E) \leq 1$, while the evader wins by eluding capture forever. Both players have
a map of the environment, but they have different sensing capabilities. The
evader $E$ always knows the location of $P$. Meanwhile, $P$ only has
line-of-sight visibility: $P$ observes the evader's position only when the line
segment connecting them lies entirely within the polygon. Therefore $P$ must
search for $E$ when the evader is hidden from view.
  We provide a winning strategy for $P$ in the family of strictly sweepable
polygons, meaning that a straight line $L$ can be moved continuously over $Q$
so that (1) $L \cap Q$ is always convex and (2) every point on the boundary
$\partial Q$ is swept exactly once. This strict sweeping requires that $L$
moves along $Q$ via a sequence of translations and rotations. We develop our
main result by first considering pursuit in the subfamilies of monotone
polygons (where $L$ moves by translation) and scallop polygons (where $L$ moves
by a single rotation). Our algorithm uses rook strategy during its active
pursuit phase, rather than the well-known lion strategy. The rook strategy is
crucial for obtaining a capture time that is linear in the area of $Q$. For
monotone and scallop polygons, our algorithm has a capture time of $O(n(Q) +
\mbox{area}(Q))$, where $n(Q)$ is the number of polygon vertices. The capture
time bound for strictly sweepable polygons is $O( n(Q) \cdot \mbox{area}(Q) )$."
"Boundary labeling deals with annotating features in images such that labels
are placed outside of the image and are connected by curves (so-called leaders)
to the corresponding features. While boundary labeling has been extensively
investigated from an algorithmic perspective, the research on its readability
has been neglected. In this paper we present the first formal user study on the
readability of boundary labeling. We consider the four most studied leader
types with respect to their performance, i.e., whether and how fast a viewer
can assign a feature to its label and vice versa. We give a detailed analysis
of the results regarding the readability of the four models and discuss their
aesthetic qualities based on the users' preference judgments and interviews."
"IC-planar graphs are those graphs that admit a drawing where no two crossed
edges share an end-vertex and each edge is crossed at most once. They are a
proper subfamily of the 1-planar graphs. Given an embedded IC-planar graph $G$
with $n$ vertices, we present an $O(n)$-time algorithm that computes a
straight-line drawing of $G$ in quadratic area, and an $O(n^3)$-time algorithm
that computes a straight-line drawing of $G$ with right-angle crossings in
exponential area. Both these area requirements are worst-case optimal. We also
show that it is NP-complete to test IC-planarity both in the general case and
in the case in which a rotation system is fixed for the input graph.
Furthermore, we describe a polynomial-time algorithm to test whether a set of
matching edges can be added to a triangulated planar graph such that the
resulting graph is IC-planar."
"Disk contact representations realize graphs by mapping vertices bijectively
to interior-disjoint disks in the plane such that two disks touch each other if
and only if the corresponding vertices are adjacent in the graph. Deciding
whether a vertex-weighted planar graph can be realized such that the disks'
radii coincide with the vertex weights is known to be NP-hard. In this work, we
reduce the gap between hardness and tractability by analyzing the problem for
special graph classes. We show that it remains NP-hard for outerplanar graphs
with unit weights and for stars with arbitrary weights, strengthening the
previous hardness results. On the positive side, we present constructive
linear-time recognition algorithms for caterpillars with unit weights and for
embedded stars with arbitrary weights."
"We consider arrangements of axis-aligned rectangles in the plane. A geometric
arrangement specifies the coordinates of all rectangles, while a combinatorial
arrangement specifies only the respective intersection type in which each pair
of rectangles intersects. First, we investigate combinatorial contact
arrangements, i.e., arrangements of interior-disjoint rectangles, with a
triangle-free intersection graph. We show that such rectangle arrangements are
in bijection with the 4-orientations of an underlying planar multigraph and
prove that there is a corresponding geometric rectangle contact arrangement.
Moreover, we prove that every triangle-free planar graph is the contact graph
of such an arrangement. Secondly, we introduce the question whether a given
rectangle arrangement has a combinatorially equivalent square arrangement. In
addition to some necessary conditions and counterexamples, we show that
rectangle arrangements pierced by a horizontal line are squarable under certain
sufficient conditions."
"The k-means problem consists of finding k centers in the d-dimensional
Euclidean space that minimize the sum of the squared distances of all points in
an input set P to their closest respective center. Awasthi et. al. recently
showed that there exists a constant c > 1 such that it is NP-hard to
approximate the k-means objective within a factor of c. We establish that the
constant c is at least 1.0013."
"In this thesis, we study two different graph problems.
  The first problem revolves around geometric spanners. Here, we have a set of
points in the plane and we want to connect them with straight line segments,
such that there is a path between each pair of points that does not make a
large detour. If we achieve this, the resulting graph is called a spanner. We
focus our attention on $\Theta$-graphs, which are constructed by connecting
each point with its nearest neighbour in a fixed number of cones. Although this
construction is very straight-forward, it has proven challenging to fully
determine the properties of the resulting graphs. We show that if the
construction uses 5 cones, the resulting graphs are still spanners. This was
the only number of cones for which this question remained unanswered. We also
present a routing strategy on the half-$\Theta_6$-graph, a variant of the graph
with 6 cones. We show that our routing strategy finds a path whose length is at
most a constant factor from the straight-line distance between the endpoints.
Moreover, we show that this routing strategy is optimal.
  In the second part, we turn our attention to flips in triangulations. A flip
is a simple operation that transforms one triangulation into another. It turns
out that with enough flips, we can transform any triangulation into any other.
But how many flips is enough? We present an improved upper bound of $5.2n -
33.6$ on the maximum flip distance between any pair of triangulations with n
vertices. Along the way, we prove matching lower bounds on each step in the
current algorithm, including a tight bound of $(3n - 9)/5$ flips needed to make
a triangulation 4-connected. In addition, we prove tight $\Theta(n \log n)$
bounds on the number of flips required in several settings where the edges have
unique labels."
"We study the problem of computing the upper bound of the discrete Fr\'{e}chet
distance for imprecise input, and prove that the problem is NP-hard. This
solves an open problem posed in 2010 by Ahn \emph{et al}. If shortcuts are
allowed, we show that the upper bound of the discrete Fr\'{e}chet distance with
shortcuts for imprecise input can be computed in polynomial time and we present
several efficient algorithms."
"We study the $O_\beta$-hull of a planar point set, a generalization of the
Orthogonal Convex Hull where the coordinate axes form an angle $\beta$. Given a
set $P$ of $n$ points in the plane, we show how to maintain the $O_\beta$-hull
of $P$ while $\beta$ runs from $0$ to $\pi$ in $O(n \log n)$ time and $O(n)$
space. With the same complexity, we also find the values of $\beta$ that
maximize the area and the perimeter of the $O_\beta$-hull and, furthermore, we
find the value of $\beta$ achieving the best fitting of the point set $P$ with
a two-joint chain of alternate interior angle $\beta$."
"Let $P$ be a planar set of $n$ points in general position. We consider the
problem of computing an orientation of the plane for which the Rectilinear
Convex Hull of $P$ has minimum area. Bae et al. (Computational Geometry: Theory
and Applications, Vol. 42, 2009) solved the problem in quadratic time and
linear space. We describe an algorithm that reduces this time complexity to
$\Theta(n \log n)$."
"We establish a one-to-one correspondence between 1-planar graphs and general
and hole-free 4-map graphs and show that 1-planar graphs can be recognized in
polynomial time if they are crossing-augmented, fully triangulated, and maximal
1-planar, respectively, with a polynomial of degree 120, 3, and 5,
respectively."
"An opaque set (or a barrier) for $U \subseteq \mathbb{R}^2$ is a set $B$ of
finite-length curves such that any line intersecting $U$ also intersects $B$.
In this paper, we consider the lower bound for the shortest barrier when $U$ is
the unit equilateral triangle. The known best lower bound for triangles is the
classic one by Jones [Jones,1964], which exhibits that the length of the
shortest barrier for any convex polygon is at least the half of its perimeter.
That is, for the unit equilateral triangle, it must be at least $3/2$. Very
recently, this lower bounds are improved for convex $k$-gons for any $k\geq 4$
[Kawamura et al. 2014], but the case of triangles still lack the bound better
than Jones' one. The main result of this paper is to fill this missing piece:
We give the lower bound of $3/2 + 5 \cdot 10^{-13}$ for the unit-size
equilateral triangle. The proof is based on two new ideas, angle-restricted
barriers and a weighted sum of projection-cover conditions, which may be of
independently interest."
"We show that the size-sensitive packing lemma follows from a simple
modification of the standard proof, due to Haussler and simplified by Chazelle,
of the packing lemma."
"Topological simplification of scalar and vector fields is well-established as
an effective method for analysing and visualising complex data sets. For
multi-field data, topological analysis requires simultaneous advances both
mathematically and computationally. We propose a robust multivariate topology
simplification method based on ``lip''-pruning from the Reeb Space.
Mathematically, we show that the projection of the Jacobi Set of multivariate
data into the Reeb Space produces a Jacobi Structure that separates the Reeb
Space into simple components. We also show that the dual graph of these
components gives rise to a Reeb Skeleton that has properties similar to the
scalar contour tree and Reeb Graph, for topologically simple domains. We then
introduce a range measure to give a scaling-invariant total ordering of the
components or features that can be used for simplification. Computationally, we
show how to compute Jacobi Structure, Reeb Skeleton, Range and Geometric
Measures in the Joint Contour Net (an approximation of the Reeb Space) and that
these can be used for visualisation similar to the contour tree or Reeb Graph."
"We introduce a force-directed algorithm, called Sync-and-Burst, which falls
into the category of classical force-directed graph drawing algorithms. A
distinct feature in Sync-and-Burst is the use of simplified forces of
attraction and repulsion whose magnitude does not depend on the distance
between vertices. Instead, magnitudes are uniform throughout the graph at each
iteration and monotonically increase as the number of iterations grows. The
Sync-and-Burst layouts are always circular in shape with relatively even
distribution of vertices throughout the drawing area. We demonstrate that
aesthetically pleasing layouts are achieved in O(n) iterations."
"A greedily routable region (GRR) is a closed subset of $\mathbb R^2$, in
which each destination point can be reached from each starting point by
choosing the direction with maximum reduction of the distance to the
destination in each point of the path.
  Recently, Tan and Kermarrec proposed a geographic routing protocol for dense
wireless sensor networks based on decomposing the network area into a small
number of interior-disjoint GRRs. They showed that minimum decomposition is
NP-hard for polygons with holes.
  We consider minimum GRR decomposition for plane straight-line drawings of
graphs. Here, GRRs coincide with self-approaching drawings of trees, a drawing
style which has become a popular research topic in graph drawing. We show that
minimum decomposition is still NP-hard for graphs with cycles, but can be
solved optimally for trees in polynomial time. Additionally, we give a
2-approximation for simple polygons, if a given triangulation has to be
respected."
"Given a set of $k$-colored points in the plane, we consider the problem of
finding $k$ trees such that each tree connects all points of one color class,
no two trees cross, and the total edge length of the trees is minimized. For
$k=1$, this is the well-known Euclidean Steiner tree problem. For general $k$,
a $k\rho$-approximation algorithm is known, where $\rho \le 1.21$ is the
Steiner ratio.
  We present a PTAS for $k=2$, a $(5/3+\varepsilon)$-approximation algorithm
for $k=3$, and two approximation algorithms for general~$k$, with ratios
$O(\sqrt n \log k)$ and $k+\varepsilon$."
"The Gromov-Hausdorff (GH) distance is a natural way to measure distance
between two metric spaces. We prove that it is $\mathrm{NP}$-hard to
approximate the Gromov-Hausdorff distance better than a factor of $3$ for
geodesic metrics on a pair of trees. We complement this result by providing a
polynomial time $O(\min\{n, \sqrt{rn}\})$-approximation algorithm for computing
the GH distance between a pair of metric trees, where $r$ is the ratio of the
longest edge length in both trees to the shortest edge length. For metric trees
with unit length edges, this yields an $O(\sqrt{n})$-approximation algorithm."
"We present an algorithm that computes the geodesic center of a given
polygonal domain. The running time of our algorithm is $O(n^{12+\epsilon})$ for
any $\epsilon>0$, where $n$ is the number of corners of the input polygonal
domain. Prior to our work, only the very special case where a simple polygon is
given as input has been intensively studied in the 1980s, and an $O(n \log
n)$-time algorithm is known by Pollack et al. Our algorithm is the first one
that can handle general polygonal domains having one or more polygonal holes."
"An $s$-workspace algorithm is an algorithm that has read-only access to the
values of the input, write-only access to the output, and only uses $O(s)$
additional words of space. We present a randomized $s$-workspace algorithm for
triangulating a simple polygon $P$ of $n$ vertices that runs in $O(n^2/s+n \log
n \log^{5} (n/s))$ expected time using $O(s)$ variables, for any $s \leq n$. In
particular, when $s \leq \frac{n}{\log n\log^{5}\log n}$ the algorithm runs in
$O(n^2/s)$ expected time."
"In the NP-hard continuous 1.5D Terrain Guarding Problem (TGP) we are given an
$x$-monotone chain of line segments in $\mathbb{R}^2$ (the terrain $T$) and ask
for the minimum number of guards (located anywhere on $T$) required to guard
all of $T$. We construct guard candidate and witness sets $G, W \subset T$ of
polynomial size such that any feasible (optimal) guard cover $G^* \subseteq G$
for $W$ is also feasible (optimal) for the continuous TGP. This discretization
allows us to (1) settle NP-completeness for the continuous TGP, (2) provide a
Polynomial Time Approximation Scheme (PTAS) for the continuous TGP using the
PTAS for the discrete TGP by Gibson et al., and (3) formulate the continuous
TGP as an Integer Linear Program (IP). Furthermore, we propose several
filtering techniques reducing the size of our discretization, allowing us to
devise an efficient IP-based algorithm that reliably provides optimal guard
placements for terrains with up to $10^6$ vertices within minutes on a standard
desktop computer."
"Given a set of N points, we have discovered an algorithm that can separate
these points from one another by n-dimensional planes. Each point is chosen at
random and put into a set S and planes which separate them are determined and
put into S. The algorithm gives a method of choosing points and planes which
separate them, till all the points are separated. A proof is provided with a
worked example.
  The algorithm is non iterative and always halts successfully and the
algorithm strictly follows Shannon's principle of making optimal use of
information as it advances stage by stage. It also has a restart facility and
can take care of new points from where it left off.At some later stage if the
dimension of the data is increased from n to n+r, the algorithm can still
continue from where it left off, after some simple adjustments, and tackle the
new data points which are of a higher dimension. and separate them. The
computational complexity is O(n.N log(N)) + O(n3 log(N)), where N is the given
number of points and n3 is the cube of n - the dimension of space. The
algorithm is made possible because a new concept called Orientation Vector is
used. This vector is a Hamming vector and is associated with each point and has
been so devised that it has all the information necessary to ascertain if two
points are separate or not when among a collection of planes.Its application to
data retrieval problems in very large medical data bases is also given."
"Let $P$ be a set of $n$ points in general position in the plane, $r$ of which
are red and $b$ of which are blue. In this paper we prove that there exist: for
every $\alpha \in \left [ 0,\frac{1}{2} \right ]$, a convex set containing
exactly $\lceil \alpha r\rceil$ red points and exactly $\lceil \alpha b \rceil$
blue points of $P$; a convex set containing exactly $\left \lceil
\frac{r+1}{2}\right \rceil$ red points and exactly $\left \lceil
\frac{b+1}{2}\right \rceil$ blue points of $P$. Furthermore, we present
polynomial time algorithms to find these convex sets. In the first case we
provide an $O(n^4)$ time algorithm and an $O(n^2\log n)$ time algorithm in the
second case. Finally, if $\lceil \alpha r\rceil+\lceil \alpha b\rceil$ is
small, that is, not much larger than $\frac{1}{3}n$, we improve the running
time to $O(n \log n)$."
"We study two variants of the problem of contact representation of planar
graphs with axis-aligned boxes. In a cube-contact representation we realize
each vertex with a cube, while in a proportional box-contact representation
each vertex is an axis-aligned box with a prespecified volume. We present
algorithms for constructing cube-contact representation and proportional
box-contact representation for several classes of planar graphs."
"Given a planar graph $G(V,E)$ and a partition of the neighbors of each vertex
$v \in V$ in four sets $UR(v)$, $UL(v)$, $DL(v)$, and $DR(v)$, the problem
Windrose Planarity asks to decide whether $G$ admits a windrose-planar drawing,
that is, a planar drawing in which (i) each neighbor $u \in UR(v)$ is above and
to the right of $v$, (ii) each neighbor $u \in UL(v)$ is above and to the left
of $v$, (iii) each neighbor $u \in DL(v)$ is below and to the left of $v$, (iv)
each neighbor $u \in DR(v)$ is below and to the right of $v$, and (v) edges are
represented by curves that are monotone with respect to each axis. By
exploiting both the horizontal and the vertical relationship among vertices,
windrose-planar drawings allow to simultaneously visualize two partial orders
defined by means of the edges of the graph.
  Although the problem is NP-hard in the general case, we give a
polynomial-time algorithm for testing whether there exists a windrose-planar
drawing that respects a combinatorial embedding that is given as part of the
input. This algorithm is based on a characterization of the plane
triangulations admitting a windrose-planar drawing. Furthermore, for any
embedded graph admitting a windrose-planar drawing we show how to construct one
with at most one bend per edge on an $O(n) \times O(n)$ grid. The latter result
contrasts with the fact that straight-line windrose-planar drawings may require
exponential area."
"The Delaunay triangulation (DT) is one of the most common and useful
triangulations of point sets $P$ in the plane. DT is not unique when $P$ is
degenerate, specifically when it contains quadruples of co-circular points. One
way to achieve uniqueness is by applying a small (or infinitesimal)
perturbation to $P$.
  We consider a specific perturbation of such degenerate sets, in which the
coordinates of each point are independently perturbed by normally distributed
small quantities, and investigate the effect of such perturbations on the DT of
the set. We focus on two special configurations, where (1) the points of $P$
form a uniform grid, and (2) the points of $P$ are vertices of a regular
polygon.
  We present interesting (and sometimes surprising) empirical findings and
properties of the perturbed DTs for these cases, and give theoretical
explanations to some of them."
"In a book embedding of a graph G, the vertices of G are placed in order along
a straight-line called spine of the book, and the edges of G are drawn on a set
of half-planes, called the pages of the book, such that two edges drawn on a
page do not cross each other. The minimum number of pages in which a graph can
be embedded is called the book-thickness or the page-number of the graph. It is
known that every planar graph has a book embedding on at most four pages. Here
we investigate the book-embeddings of 1-planar graphs. A graph is 1-planar if
it can be drawn in the plane such that each edge is crossed at most once. We
prove that every 1-planar graph has a book embedding on at most 16 pages and
every 3-connected 1-planar graph has a book embedding on at most 12 pages. The
drawings can be computed in linear time from any given 1-planar embedding of
the graph."
"Hyperspectral unmixing (HU) is a crucial signal processing procedure to
identify the underlying materials (or endmembers) and their corresponding
proportions (or abundances) from an observed hyperspectral scene. A well-known
blind HU criterion, advocated by Craig in early 1990's, considers the vertices
of the minimum-volume enclosing simplex of the data cloud as good endmember
estimates, and it has been empirically and theoretically found effective even
in the scenario of no pure pixels. However, such kind of algorithms may suffer
from heavy simplex volume computations in numerical optimization, etc. In this
work, without involving any simplex volume computations, by exploiting a convex
geometry fact that a simplest simplex of N vertices can be defined by N
associated hyperplanes, we propose a fast blind HU algorithm, for which each of
the N hyperplanes associated with the Craig's simplex of N vertices is
constructed from N-1 affinely independent data pixels, together with an
endmember identifiability analysis for its performance support. Without
resorting to numerical optimization, the devised algorithm searches for the
N(N-1) active data pixels via simple linear algebraic computations, accounting
for its computational efficiency. Monte Carlo simulations and real data
experiments are provided to demonstrate its superior efficacy over some
benchmark Craig-criterion-based algorithms in both computational efficiency and
estimation accuracy."
"We show that there exists a geodesic spanner with almost linear number of
edges."
"We describe an algorithm for computing the separating common tangents of two
simple polygons using linear time and only constant workspace. A tangent of a
polygon is a line touching the polygon such that all of the polygon lies to the
same side of the line. A separating common tangent of two polygons is a tangent
of both polygons where the polygons are lying on different sides of the
tangent. Each polygon is given as a read-only array of its corners. If a
separating common tangent does not exist, the algorithm reports that.
Otherwise, two corners defining a separating common tangent are returned. The
algorithm is simple and implies an optimal algorithm for deciding if the convex
hulls of two polygons are disjoint or not. This was not known to be possible in
linear time and constant workspace prior to this paper.
  An outer common tangent is a tangent of both polygons where the polygons are
on the same side of the tangent. In the case where the convex hulls of the
polygons are disjoint, we give an algorithm for computing the outer common
tangents in linear time using constant workspace."
"Given a polygonal region containing a target point (which we assume is the
origin), it is not hard to see that there are two points on the perimeter that
are antipodal, i.e., whose midpoint is the origin. We prove three
generalizations of this fact. (1) For any polygon (or any bounded closed region
with connected boundary) containing the origin, it is possible to place a given
set of weights on the boundary so that their barycenter (center of mass)
coincides with the origin, provided that the largest weight does not exceed the
sum of the other weights. (2) On the boundary of any $3$-dimensional bounded
polyhedron containing the origin, there exist three points that form an
equilateral triangle centered at the origin. (3) On the $1$-skeleton of any
$3$-dimensional bounded convex polyhedron containing the origin, there exist
three points whose center of mass coincides with the origin."
"An algorithm is presented that constructs an acyclic partial matching on the
cells of a given simplicial complex from a vector-valued function defined on
the vertices and extended to each simplex by taking the least common upper
bound of the values on its vertices. The resulting acyclic partial matching may
be used to construct a reduced filtered complex with the same multidimensional
persistent homology as the original simplicial complex filtered by the sublevel
sets of the function. Numerical tests show that in practical cases the rate of
reduction in the number of cells achieved by the algorithm is substantial. This
promises to be useful for the computation of multidimensional persistent
homology of simplicial complexes filtered by sublevel sets of vector-valued
functions."
"In many data analysis applications the following scenario is commonplace: we
are given a point set that is supposed to sample a hidden ground truth $K$ in a
metric space, but it got corrupted with noise so that some of the data points
lie far away from $K$ creating outliers also termed as {\em ambient noise}. One
of the main goals of denoising algorithms is to eliminate such noise so that
the curated data lie within a bounded Hausdorff distance of $K$. Popular
denoising approaches such as deconvolution and thresholding often require the
user to set several parameters and/or to choose an appropriate noise model
while guaranteeing only asymptotic convergence. Our goal is to lighten this
burden as much as possible while ensuring theoretical guarantees in all cases.
  Specifically, first, we propose a simple denoising algorithm that requires
only a single parameter but provides a theoretical guarantee on the quality of
the output on general input points. We argue that this single parameter cannot
be avoided. We next present a simple algorithm that avoids even this parameter
by paying for it with a slight strengthening of the sampling condition on the
input points which is not unrealistic. We also provide some preliminary
empirical evidence that our algorithms are effective in practice."
"We introduce a new variant of the nearest neighbor search problem, which
allows for some coordinates of the dataset to be arbitrarily corrupted or
unknown. Formally, given a dataset of $n$ points $P=\{ x_1,\ldots, x_n\}$ in
high-dimensions, and a parameter $k$, the goal is to preprocess the dataset,
such that given a query point $q$, one can compute quickly a point $x \in P$,
such that the distance of the query to the point $x$ is minimized, when
ignoring the ""optimal"" $k$ coordinates. Note, that the coordinates being
ignored are a function of both the query point and the point returned.
  We present a general reduction from this problem to answering ANN queries,
which is similar in spirit to LSH (locality sensitive hashing) [IM98].
Specifically, we give a sampling technique which achieves a bi-criterion
approximation for this problem. If the distance to the nearest neighbor after
ignoring $k$ coordinates is $r$, the data-structure returns a point that is
within a distance of $O(r)$ after ignoring $O(k)$ coordinates. We also present
other applications and further extensions and refinements of the above result.
  The new data-structures are simple and (arguably) elegant, and should be
practical -- specifically, all bounds are polynomial in all relevant parameters
(including the dimension of the space, and the robustness parameter $k$)."
"A rectangle visibility representation (RVR) of a graph consists of an
assignment of axis-aligned rectangles to vertices such that for every edge
there exists a horizontal or vertical line of sight between the rectangles
assigned to its endpoints. Testing whether a graph has an RVR is known to be
NP-hard. In this paper, we study the problem of finding an RVR under the
assumption that an embedding in the plane of the input graph is fixed and we
are looking for an RVR that reflects this embedding. We show that in this case
the problem can be solved in polynomial time for general embedded graphs and in
linear time for 1-plane graphs (i.e., embedded graphs having at most one
crossing per edge). The linear time algorithm uses a precise list of forbidden
configurations, which extends the set known for straight-line drawings of
1-plane graphs. These forbidden configurations can be tested for in linear
time, and so in linear time we can test whether a 1-plane graph has an RVR and
either compute such a representation or report a negative witness. Finally, we
discuss some extensions of our study to the case when the embedding is not
fixed but the RVR can have at most one crossing per edge."
"We show that if you represent all primes with less than n-digits as points in
n-dimensional space, then they can be stored and retrieved conveniently using
n-dimensional geometry. Also once you have calculated all the prime numbers
less than n digits, it is very easy to find out if a given number having less
than n-digits is or is not a prime. We do this by separating all the primes
which are represented by points in n-dimension space by planes. It so turns out
that the number of planes q, required to separate all the points represented by
primes less than n-digit, are very few in number. Thus we obtain a very
efficient storage and retrieval system in n-dimensional space. In addition the
storage and retieval repository has the property that when new primes are added
there is no need to start all over, we can begin where we last left off and add
the new primes in the repository and add new planes that separate them as and
when necessary. Also we can arrange matters such that the repository can begin
to accept larger primes which has more digits say n' where n' > n. The
algorithm does not make use of any property of prime numbers or of integers in
general,except for the fact that any n-digit integer can be represented as a
point in n-dimension space. Therefore the method can serve to be a storage and
retrieval repository of any set of given integers, in practical cases they can
represent information. Thus the algorithm can be used to devise a very
efficient storage and retrieval system for large amounts of digital data."
"For a graph $G$, a function $\psi$ is called a \emph{bar visibility
representation} of $G$ when for each vertex $v \in V(G)$, $\psi(v)$ is a
horizontal line segment (\emph{bar}) and $uv \in E(G)$ iff there is an
unobstructed, vertical, $\varepsilon$-wide line of sight between $\psi(u)$ and
$\psi(v)$. Graphs admitting such representations are well understood (via
simple characterizations) and recognizable in linear time. For a directed graph
$G$, a bar visibility representation $\psi$ of $G$, additionally, puts the bar
$\psi(u)$ strictly below the bar $\psi(v)$ for each directed edge $(u,v)$ of
$G$. We study a generalization of the recognition problem where a function
$\psi'$ defined on a subset $V'$ of $V(G)$ is given and the question is whether
there is a bar visibility representation $\psi$ of $G$ with $\psi(v) =
\psi'(v)$ for every $v \in V'$. We show that for undirected graphs this problem
together with closely related problems are \NP-complete, but for certain cases
involving directed graphs it is solvable in polynomial time."
"Given $n$ non-vertical lines in 3-space, their vertical depth (above/below)
relation can contain cycles. We show that the lines can be cut into
$O(n^{3/2}\mathop{\mathrm{polylog}} n)$ pieces, such that the depth relation
among these pieces is now a proper partial order. This bound is nearly tight in
the worst case.
  Previous results on this topic could only handle restricted cases of the
problem (such as handling only triangular cycles, by Aronov, Koltun, and Sharir
(2005), or only cycles in grid-like patterns, by Chazelle et al. (1992)), and
the bounds were considerably weaker---much closer to the trivial quadratic
bound.
  Our proof uses a recent variant of the polynomial partitioning technique, due
to Guth, and some simple tools from algebraic geometry. It is much more
straightforward than the previous ""purely combinatorial"" methods.
  Our technique can be extended to eliminating all cycles in the depth relation
among segments, and among constant-degree algebraic arcs. We hope that a
suitable extension of this technique could be used to handle the much more
difficult case of pairwise-disjoint triangles.
  We also discuss several algorithms for constructing a small set of cuts so as
to eliminate all depth-relation cycles among the lines (minimizing such a set,
for the case of line segments, is known to be NP-complete). The performance of
these algorithms improves due to our new bound, but, so far, none of them both
(a) produce close to $n^{3/2}$ cuts, and (b) run in time close to $n^{3/2}$, in
the worst case.
  Our results almost completely settle a 35-year-old open problem in
computational geometry, motivated by hidden-surface removal in computer
graphics."
"We show that $O(n^2)$ exchanging flips suffice to transform any edge-labelled
pointed pseudo-triangulation into any other with the same set of labels. By
using insertion, deletion and exchanging flips, we can transform any
edge-labelled pseudo-triangulation into any other with $O(n \log c + h \log h)$
flips, where $c$ is the number of convex layers and $h$ is the number of points
on the convex hull."
"Pretropisms are candidates for the leading exponents of Puiseux series that
represent solutions of polynomial systems. To find pretropisms, we propose an
exact gift wrapping algorithm to prune the tree of edges of a tuple of Newton
polytopes. We prefer exact arithmetic not only because of the exact input and
the degrees of the output, but because of the often unpredictable growth of the
coordinates in the face normals, even for polytopes in generic position. We
provide experimental results with our preliminary implementation in Sage that
compare favorably with the pruning method that relies only on cone
intersections."
"A k-transmitter in a simple orthogonal polygon P is a mobile guard that
travels back and forth along an orthogonal line segment s inside P. The
k-transmitter can see a point p in P if there exists a point q on s such that
the line segment pq is normal to s and pq intersects the boundary of P in at
most k points. In this paper, we give a 2-approximation algorithm for the
problem of guarding a monotone orthogonal polygon with the minimum number of
2-transmitters."
"We show that the hypercube has a face-unfolding that tiles space, and that
unfolding has an edge-unfolding that tiles the plane. So the hypercube is a
""dimension-descending tiler."" We also show that the hypercube cross unfolding
made famous by Dali tiles space, but we leave open the question of whether or
not it has an edge-unfolding that tiles the plane."
"We seek to augment a geometric network in the Euclidean plane with shortcuts
to minimize its continuous diameter, i.e., the largest network distance between
any two points on the augmented network. Unlike in the discrete setting where a
shortcut connects two vertices and the diameter is measured between vertices,
we take all points along the edges of the network into account when placing a
shortcut and when measuring distances in the augmented network.
  We study this network augmentation problem for paths and cycles. For paths,
we determine an optimal shortcut in linear time. For cycles, we show that a
single shortcut never decreases the continuous diameter and that two shortcuts
always suffice to reduce the continuous diameter. Furthermore, we characterize
optimal pairs of shortcuts for convex and non-convex cycles. Finally, we
develop a linear time algorithm that produces an optimal pair of shortcuts for
convex cycles. Apart from the algorithms, our results extend to rectifiable
curves.
  Our work reveals some of the underlying challenges that must be overcome when
addressing the discrete version of this network augmentation problem, where we
minimize the discrete diameter of a network with shortcuts that connect only
vertices."
"Given a convex polygon $P$ with $n$ vertices, the two-center problem is to
find two congruent closed disks of minimum radius such that they completely
cover $P$. We propose an algorithm for this problem in the streaming setup,
where the input stream is the vertices of the polygon in clockwise order. It
produces a radius $r$ satisfying $r\leq2r_{opt}$ using $O(1)$ space, where
$r_{opt}$ is the optimum solution. Next, we show that in non-streaming setup,
we can improve the approximation factor by $r\leq 1.84 r_{opt}$, maintaining
the time complexity of the algorithm to $O(n)$, and using $O(1)$ extra space in
addition to the space required for storing the input."
"Let $R$ and $B$ be two disjoint sets of points in the plane such that
$|B|\leqslant |R|$, and no three points of $R\cup B$ are collinear. We show
that the geometric complete bipartite graph $K(R,B)$ contains a non-crossing
spanning tree whose maximum degree is at most $\max\left\{3, \left\lceil
\frac{|R|-1}{|B|}\right\rceil + 1\right\}$; this is the best possible upper
bound on the maximum degree. This solves an open problem posed by Abellanas et
al. at the Graph Drawing Symposium, 1996."
"Zonotopes are becoming an increasingly popular set representation for formal
verification techniques. This is mainly due to their efficient representation
and their favorable computational complexity of important operations in
high-dimensional spaces. In particular, zonotopes are closed under Minkowski
addition and linear maps, which can be very efficiently implemented.
Unfortunately, zonotopes are not closed under Minkowski difference for
dimensions greater than two. However, we present an algorithm that efficiently
computes a halfspace representation of the Minkowski difference of two
zonotopes. In addition, we present an efficient algorithm that computes an
approximation of the Minkowski difference in generator representation. The
efficiency of the proposed solution is demonstrated by numerical experiments.
These experiments show a reduced computation time in comparison to that when
first the halfspace representation of zonotopes is obtained and the Minkowski
difference is performed subsequently."
"A pseudo-polynomial time $(1 + \varepsilon)$-approximation algorithm is
presented for computing the integral and average Fr\'{e}chet distance between
two given polygonal curves $T_1$ and $T_2$. In particular, the running time is
upper-bounded by $\mathcal{O}( \zeta^{4}n^4/\varepsilon^{2})$ where $n$ is the
complexity of $T_1$ and $T_2$ and $\zeta$ is the maximal ratio of the lengths
of any pair of segments from $T_1$ and $T_2$. The Fr\'{e}chet distance captures
the minimal cost of a continuous deformation of $T_1$ into $T_2$ and vice versa
and defines the cost of a deformation as the maximal distance between two
points that are related. The integral Fr\'{e}chet distance defines the cost of
a deformation as the integral of the distances between points that are related.
The average Fr\'{e}chet distance is defined as the integral Fr\'{e}chet
distance divided by the lengths of $T_1$ and $T_2$.
  Furthermore, we give relations between weighted shortest paths inside a
single parameter cell $C$ and the monotone free space axis of $C$. As a result
we present a simple construction of weighted shortest paths inside a parameter
cell. Additionally, such a shortest path provides an optimal solution for the
partial Fr\'{e}chet similarity of segments for all leash lengths. These two
aspects are related to each other and are of independent interest."
"Given a two-dimensional space endowed with a divergence function that is
convex in the first argument, continuously differentiable in the second, and
satisfies suitable regularity conditions at Voronoi vertices, we show that
orphan-freedom (the absence of disconnected Voronoi regions) is sufficient to
ensure that Voronoi edges and vertices are also connected, and that the dual is
a simple planar graph. We then prove that the straight-edge dual of an
orphan-free Voronoi diagram (with sites as the first argument of the
divergence) is always an embedded triangulation.
  Among the divergences covered by our proofs are Bregman divergences,
anisotropic divergences, as well as all distances derived from strictly convex
$\mathcal{C}^1$ norms (including the $L_p$ norms with $1< p < \infty$). While
Bregman diagrams of the {first kind} are simply affine diagrams, and their
duals ({weighted} Delaunay triangulations) are always embedded, we show that
duals of orphan-free Bregman diagrams of the \emph{second kind} are always
embedded."
"Given a convex polygon $P$ with $n$ edges, we consider the geometric
optimization problem of computing the parallelograms in $P$ with maximal area.
We design an $O(n\log^2n)$ time algorithm for computing all these
parallelograms, which improves over a previous known quadratic time algorithm.
To this end, we propose a novel geometric structure, called $\Nest(P)$, which
is induced by $P$ and is an arrangement of $\Theta(n^2)$ segments, each of
which is parallel to an edge of $P$. This structure admits several interesting
properties, which follow from two fundamental properties in geometry, namely,
convexity and parallelism. Structure $\Nest(P)$ captures the essential nature
of the maximal area parallelograms, and the original optimization problem can
be reduced to answering $O(n)$ location queries on $\Nest(P)$. Moreover,
avoiding an explicit construction of $\Nest(P)$, which would take $\Omega(n^2)$
time, we answer each of these queries in $O(\log^2n)$ time."
"Random sampling is a classical tool in constrained optimization. Under
favorable conditions, the optimal solution subject to a small subset of
randomly chosen constraints violates only a small subset of the remaining
constraints. Here we study the following variant that we call random sampling
with removal: suppose that after sampling the subset, we remove a fixed number
of constraints from the sample, according to an arbitrary rule. Is it still
true that the optimal solution of the reduced sample violates only a small
subset of the constraints?
  The question naturally comes up in situations where the solution subject to
the sampled constraints is used as an approximate solution to the original
problem. In this case, it makes sense to improve cost and volatility of the
sample solution by removing some of the constraints that appear most
restricting. At the same time, the approximation quality (measured in terms of
violated constraints) should remain high.
  We study random sampling with removal in a generalized, completely abstract
setting where we assign to each subset $R$ of the constraints an arbitrary set
$V(R)$ of constraints disjoint from $R$; in applications, $V(R)$ corresponds to
the constraints violated by the optimal solution subject to only the
constraints in R. Furthermore, our results are parametrized by the dimension
$\delta$.
  In this setting, we prove matching upper and lower bounds for the expected
number of constraints violated by a random sample, after the removal of $k$
elements. For a large range of values of $k$, the new upper bounds improve the
previously best bounds for LP-type problems, which moreover had only been known
in special cases. We show that this bound on special LP-type problems, can be
derived in the much more general setting of violator spaces, and with very
elementary proofs."
"The Fr\'echet distance is a popular distance measure for curves. We study the
problem of clustering time series under the Fr\'echet distance. In particular,
we give $(1+\varepsilon)$-approximation algorithms for variations of the
following problem with parameters $k$ and $\ell$. Given $n$ univariate time
series $P$, each of complexity at most $m$, we find $k$ time series, not
necessarily from $P$, which we call \emph{cluster centers} and which each have
complexity at most $\ell$, such that (a) the maximum distance of an element of
$P$ to its nearest cluster center or (b) the sum of these distances is
minimized. Our algorithms have running time near-linear in the input size for
constant $\varepsilon$, $k$ and $\ell$. To the best of our knowledge, our
algorithms are the first clustering algorithms for the Fr\'echet distance which
achieve an approximation factor of $(1+\varepsilon)$ or better.
  Keywords: time series, longitudinal data, functional data, clustering,
Fr\'echet distance, dynamic time warping, approximation algorithms."
"We present a sampling theory for a class of binary images with finite rate of
innovation (FRI). Every image in our model is the restriction of
$\mathds{1}_{\{p\leq0\}}$ to the image plane, where $\mathds{1}$ denotes the
indicator function and $p$ is some real bivariate polynomial. This particularly
means that the boundaries in the image form a subset of an algebraic curve with
the implicit polynomial $p$. We show that the image parameters --i.e., the
polynomial coefficients-- satisfy a set of linear annihilation equations with
the coefficients being the image moments. The inherent sensitivity of the
moments to noise makes the reconstruction process numerically unstable and
narrows the choice of the sampling kernels to polynomial reproducing kernels.
As a remedy to these problems, we replace conventional moments with more stable
\emph{generalized moments} that are adjusted to the given sampling kernel. The
benefits are threefold: (1) it relaxes the requirements on the sampling
kernels, (2) produces annihilation equations that are robust at numerical
precision, and (3) extends the results to images with unbounded boundaries. We
further reduce the sensitivity of the reconstruction process to noise by taking
into account the sign of the polynomial at certain points, and sequentially
enforcing measurement consistency. We consider various numerical experiments to
demonstrate the performance of our algorithm in reconstructing binary images,
including low to moderate noise levels and a range of realistic sampling
kernels."
"Let $P$ be a set of $n$ points in $d$-dimensions. The simplicial depth,
$\sigma_P(q)$ of a point $q$ is the number of $d$-simplices with vertices in
$P$ that contain $q$ in their convex hulls. The simplicial depth is a notion of
data depth with many applications in robust statistics and computational
geometry. Computing the simplicial depth of a point is known to be a
challenging problem. The trivial solution requires $O(n^{d+1})$ time whereas it
is generally believed that one cannot do better than $O(n^{d-1})$. In this
paper, we consider approximation algorithms for computing the simplicial depth
of a point. For $d=2$, we present a new data structure that can approximate the
simplicial depth in polylogarithmic time, using polylogarithmic query time. In
3D, we can approximate the simplicial depth of a given point in near-linear
time, which is clearly optimal up to polylogarithmic factors. For higher
dimensions, we consider two approximation algorithms with different worst-case
scenarios. By combining these approaches, we compute a
$(1+\varepsilon)$-approximation of the simplicial depth in time
$\tilde{O}(n^{d/2 + 1})$ ignoring polylogarithmic factor. All of these
algorithms are Monte Carlo algorithms. Furthermore, we present a simple
strategy to compute the simplicial depth exactly in $O(n^d \log n)$ time, which
provides the first improvement over the trivial $O(n^{d+1})$ time algorithm for
$d>4$. Finally, we show that computing the simplicial depth exactly is
#P-complete and W[1]-hard if the dimension is part of the input."
"We prove that it is NP-hard to dissect one simple orthogonal polygon into
another using a given number of pieces, as is approximating the fewest pieces
to within a factor of $1+1/1080-\varepsilon$."
"We consider the $k$-center problem in which the centers are constrained to
lie on two lines. Given a set of $n$ weighted points in the plane, we want to
locate up to $k$ centers on two parallel lines. We present an $O(n\log^2 n)$
time algorithm, which minimizes the weighted distance from any point to a
center. We then consider the unweighted case, where the centers are constrained
to be on two perpendicular lines. Our algorithms run in $O(n\log^2 n)$ time
also in this case."
"Given a set of $n$ weighted points on the $x$-$y$ plane, we want to find a
step function consisting of $k$ horizontal steps such that the maximum vertical
weighted distance from any point to a step is minimized. We solve this problem
in $O(n)$ time when $k$ is a constant. Our approach relies on the
prune-and-search technique, and can be adapted to design similar linear time
algorithms to solve the line-constrained k-center problem and the size-$k$
histogram construction problem as well."
"A terrain T is an x-monotone polygonal chain in the plane; T is orthogonal if
each edge of T is either horizontal or vertical. In this paper, we give an
exact algorithm for the problem of guarding the convex vertices of an
orthogonal terrain with the minimum number of reflex vertices."
"Voronoi and related diagrams have technological applications, for example, in
motion planning and surface reconstruction, and also find significant use in
materials science, molecular biology, and crystallography. Apollonius diagrams
arguably provide the most natural division of space for many materials and
technology problems, but compared to Voronoi and power diagrams, their use has
been limited, presumably by the complexity of their calculation. In this work,
we report explicit equations for the vertices of the Apollonius diagram in a
d-dimensional Euclidean space. We show that there are special lines that
contain vertices of more than one type of diagram and this property can be
exploited to develop simple vertex expressions for the Apollonius diagram.
Finding the Apollonius vertices is not significantly more difficult or
expensive than computing those of the power diagram and have application beyond
their use in calculating the diagram. The expressions reported here lend
themselves to the use of standard vector and matrix libraries and the stability
and precision their use implies. They can also be used in algorithms with
multi-precision numeric types and those adhering to the exact algorithms
paradigm. The results have been coded in C++ for the 2-d and 3-d cases and an
example of their use in characterizing the shape of a void in a molecular
crystal is given."
"Geometric data summarization has become an essential tool in both geometric
approximation algorithms and where geometry intersects with big data problems.
In linear or near-linear time large data sets can be compressed into a summary,
and then more intricate algorithms can be run on the summaries whose results
approximate those of the full data set. Coresets and sketches are the two most
important classes of these summaries. We survey five types of coresets and
sketches: shape-fitting, density estimation, high-dimensional vectors,
high-dimensional point sets / matrices, and clustering."
"We study versions of cop and robber pursuit-evasion games on the visibility
graphs of polygons, and inside polygons with straight and curved sides. Each
player has full information about the other player's location, players take
turns, and the robber is captured when the cop arrives at the same point as the
robber. In visibility graphs we show the cop can always win because visibility
graphs are dismantlable, which is interesting as one of the few results
relating visibility graphs to other known graph classes. We extend this to show
that the cop wins games in which players move along straight line segments
inside any polygon and, more generally, inside any simply connected planar
region with a reasonable boundary. Essentially, our problem is a type of
pursuit-evasion using the link metric rather than the Euclidean metric, and our
result provides an interesting class of infinite cop-win graphs."
"A straight-line drawing of a graph is a monotone drawing if for each pair of
vertices there is a path which is monotonically increasing in some direction,
and it is called a strongly monotone drawing if the direction of monotonicity
is given by the direction of the line segment connecting the two vertices.
  We present algorithms to compute crossing-free strongly monotone drawings for
some classes of planar graphs; namely, 3-connected planar graphs, outerplanar
graphs, and 2-trees. The drawings of 3-connected planar graphs are based on
primal-dual circle packings. Our drawings of outerplanar graphs are based on a
new algorithm that constructs strongly monotone drawings of trees which are
also convex. For irreducible trees, these drawings are strictly convex."
"We describe the first algorithm to compute the outer common tangents of two
disjoint simple polygons using linear time and only constant workspace. A
tangent of a polygon is a line touching the polygon such that all of the
polygon lies on the same side of the line. An outer common tangent of two
polygons is a tangent of both polygons such that the polygons lie on the same
side of the tangent. Each polygon is given as a read-only array of its corners
in cyclic order. The algorithm detects if an outer common tangent does not
exist, which is the case if and only if the convex hull of one of the polygons
is contained in the convex hull of the other. Otherwise, two corners defining
an outer common tangent are returned."
"We investigate the problem of computing a minimal-volume container for the
non-overlapping packing of a given set of three-dimensional convex objects.
Already the simplest versions of the problem are NP-hard so that we cannot
expect to find exact polynomial time algorithms. We give constant ratio
approximation algorithms for packing axis-parallel (rectangular) cuboids under
translation into an axis-parallel (rectangular) cuboid as container, for
cuboids under rigid motions into an axis-parallel cuboid or into an arbitrary
convex container, and for packing convex polyhedra under rigid motions into an
axis-parallel cuboid or arbitrary convex container. This work gives the first
approximability results for the computation of minimal volume containers for
the objects described."
"$\renewcommand{\Re}{{\rm I\!\hspace{-0.025em} R}}
\newcommand{\SetX}{\mathsf{X}} \newcommand{\eps}{\varepsilon}
\newcommand{\VorX}[1]{\mathcal{V} \pth{#1}} \newcommand{\Polygon}{\mathsf{P}}
\newcommand{\IntRange}[1]{[ #1 ]} \newcommand{\Space}{\ovebarline{\mathsf{m}}}
\newcommand{\pth}[2][\!]{#1\left({#2}\right)} \newcommand{\Arr}{{\cal A}}$
  Let $H$ be a set of $n$ planes in three dimensions, and let $r \leq n$ be a
parameter. We give a simple alternative proof of the existence of a
$(1/r)$-cutting of the first $n/r$ levels of $\Arr(H)$, which consists of
$O(r)$ semi-unbounded vertical triangular prisms. The same construction yields
an approximation of the $(n/r)$-level by a terrain consisting of $O(r/\eps^3)$
triangular faces, which lies entirely between the levels $(1\pm\eps)n/r$. The
proof does not use sampling, and exploits techniques based on planar separators
and various structural properties of levels in three-dimensional arrangements
and of planar maps. The proof is constructive, and leads to a simple randomized
algorithm, with expected near-linear running time. An application of this
technique allows us to mimic Matousek's construction of cuttings in the plane,
to obtain a similar construction of ""layered"" $(1/r)$-cutting of the entire
arrangement $\Arr(H)$, of optimal size $O(r^3)$. Another application is a
simplified optimal approximate range counting algorithm in three dimensions,
competing with that of Afshani and Chan."
"Inspired by the Japanese game Pachinko, we study simple (perfectly
""inelastic"" collisions) dynamics of a unit ball falling amidst point obstacles
(pins) in the plane. A classic example is that a checkerboard grid of pins
produces the binomial distribution, but what probability distributions result
from different pin placements? In the 50-50 model, where the pins form a subset
of this grid, not all probability distributions are possible, but surprisingly
the uniform distribution is possible for $\{1,2,4,8,16\}$ possible drop
locations. Furthermore, every probability distribution can be approximated
arbitrarily closely, and every dyadic probability distribution can be divided
by a suitable power of $2$ and then constructed exactly (along with extra
""junk"" outputs). In a more general model, if a ball hits a pin off center, it
falls left or right accordingly. Then we prove a universality result: any
distribution of $n$ dyadic probabilities, each specified by $k$ bits, can be
constructed using $O(n k^2)$ pins, which is close to the information-theoretic
lower bound of $\Omega(n k)$."
"Modeling folding surfaces with nonzero thickness is of practical interest for
mechanical engineering. There are many existing approaches that account for
material thickness in folding applications. We propose a new systematic and
broadly applicable algorithm to transform certain flat-foldable crease patterns
into new crease patterns with similar folded structure but with a
facet-separated folded state. We provide conditions on input crease patterns
for the algorithm to produce a thickened crease pattern avoiding local self
intersection, and provide bounds for the maximum thickness that the algorithm
can produce for a given input. We demonstrate these results in parameterized
numerical simulations and physical models."
"A weak pseudoline arrangement is a topological generalization of a line
arrangement, consisting of curves topologically equivalent to lines that cross
each other at most once. We consider arrangements that are outerplanar---each
crossing is incident to an unbounded face---and simple---each crossing point is
the crossing of only two curves. We show that these arrangements can be
represented by chords of a circle, by convex polygonal chains with only two
bends, or by hyperbolic lines. Simple but non-outerplanar arrangements
(non-weak) can be represented by convex polygonal chains or convex smooth
curves of linear complexity."
"Let $P \subset \mathbb{R}^d$ be a set of $n$ points in the $d$ dimensions
such that each point $p \in P$ has an associated radius $r_p > 0$. The
transmission graph $G$ for $P$ is the directed graph with vertex set $P$ such
that there is an edge from $p$ to $q$ if and only if $d(p, q) \leq r_p$, for
any $p, q \in P$.
  A reachability oracle is a data structure that decides for any two vertices
$p, q \in G$ whether $G$ has a path from $p$ to $q$. The quality of the oracle
is measured by the space requirement $S(n)$, the query time $Q(n)$, and the
preprocessing time. For transmission graphs of one-dimensional point sets, we
can construct in $O(n \log n)$ time an oracle with $Q(n) = O(1)$ and $S(n) =
O(n)$. For planar point sets, the ratio $\Psi$ between the largest and the
smallest associated radius turns out to be an important parameter. We present
three data structures whose quality depends on $\Psi$: the first works only for
$\Psi < \sqrt{3}$ and achieves $Q(n) = O(1)$ with $S(n) = O(n)$ and
preprocessing time $O(n\log n)$; the second data structure gives $Q(n) =
O(\Psi^3 \sqrt{n})$ and $S(n) = O(\Psi^5 n^{3/2})$; the third data structure is
randomized with $Q(n) = O(n^{2/3}\log^{1/3} \Psi \log^{2/3} n)$ and $S(n) =
O(n^{5/3}\log^{1/3} \Psi \log^{2/3} n)$ and answers queries correctly with high
probability."
"Let $P \subset \mathbb{R}^2$ be a planar $n$-point set such that each point
$p \in P$ has an associated radius $r_p > 0$. The transmission graph $G$ for
$P$ is the directed graph with vertex set $P$ such that for any $p, q \in P$,
there is an edge from $p$ to $q$ if and only if $d(p, q) \leq r_p$.
  Let $t > 1$ be a constant. A $t$-spanner for $G$ is a subgraph $H \subseteq
G$ with vertex set $P$ so that for any two vertices $p,q \in P$, we have
$d_H(p, q) \leq t d_G(p, q)$, where $d_H$ and $d_G$ denote the shortest path
distance in $H$ and $G$, respectively (with Euclidean edge lengths). We show
how to compute a $t$-spanner for $G$ with $O(n)$ edges in $O(n (\log n + \log
\Psi))$ time, where $\Psi$ is the ratio of the largest and smallest radius of a
point in $P$. Using more advanced data structures, we obtain a construction
that runs in $O(n \log^6 n)$ time, independent of $\Psi$.
  We give two applications for our spanners. First, we show how to use our
spanner to find a BFS tree from any given start vertex in $O(n \log n)$ time
(in addition to the time it takes to build the spanner). Second, we show how to
use our spanner to extend a reachability oracle to answer geometric
reachability queries. In a geometric reachability query we ask whether a vertex
$p$ in $G$ can ""reach"" a target $q$ which is an arbitrary point the plane
(rather than restricted to be another vertex $q$ of $G$ in a standard
reachability query). Our spanner allows the reachability oracle to answer
geometric reachability queries with an additive overhead of $O(\log n\log
\Psi)$ to the query time and $O(n \log \Psi)$ to the space."
"We present tight bounds on the spanning ratio of a large family of ordered
$\theta$-graphs. A $\theta$-graph partitions the plane around each vertex into
$m$ disjoint cones, each having aperture $\theta = 2 \pi/m$. An ordered
$\theta$-graph is constructed by inserting the vertices one by one and
connecting each vertex to the closest previously-inserted vertex in each cone.
We show that for any integer $k \geq 1$, ordered $\theta$-graphs with $4k + 4$
cones have a tight spanning ratio of $1 + 2 \sin(\theta/2) / (\cos(\theta/2) -
\sin(\theta/2))$. We also show that for any integer $k \geq 2$, ordered
$\theta$-graphs with $4k + 2$ cones have a tight spanning ratio of $1 / (1 - 2
\sin(\theta/2))$. We provide lower bounds for ordered $\theta$-graphs with $4k
+ 3$ and $4k + 5$ cones. For ordered $\theta$-graphs with $4k + 2$ and $4k + 5$
cones these lower bounds are strictly greater than the worst case spanning
ratios of their unordered counterparts. These are the first results showing
that ordered $\theta$-graphs have worse spanning ratios than unordered
$\theta$-graphs. Finally, we show that, unlike their unordered counterparts,
the ordered $\theta$-graphs with 4, 5, and 6 cones are not spanners."
"Let $\mathcal{S}$ be a connected planar polygonal subdivision with $n$ edges
that we want to preprocess for point-location queries, and where we are given
the probability $\gamma_i$ that the query point lies in a polygon $P_i$ of
$\mathcal{S}$. We show how to preprocess $\mathcal{S}$ such that the query time
for a point~$p\in P_i$ depends on~$\gamma_i$ and, in addition, on the distance
from $p$ to the boundary of~$P_i$---the further away from the boundary, the
faster the query. More precisely, we show that a point-location query can be
answered in time $O\left(\min \left(\log n, 1 + \log
\frac{\mathrm{area}(P_i)}{\gamma_i \Delta_{p}^2}\right)\right)$, where
$\Delta_{p}$ is the shortest Euclidean distance of the query point~$p$ to the
boundary of $P_i$. Our structure uses $O(n)$ space and $O(n \log n)$
preprocessing time. It is based on a decomposition of the regions of
$\mathcal{S}$ into convex quadrilaterals and triangles with the following
property: for any point $p\in P_i$, the quadrilateral or triangle
containing~$p$ has area $\Omega(\Delta_{p}^2)$. For the special case where
$\mathcal{S}$ is a subdivision of the unit square and
$\gamma_i=\mathrm{area}(P_i)$, we present a simpler solution that achieves a
query time of $O\left(\min \left(\log n, \log
\frac{1}{\Delta_{p}^2}\right)\right)$. The latter solution can be extended to
convex subdivisions in three dimensions."
"We show that c-planarity is solvable in quadratic time for flat clustered
graphs with three clusters if the combinatorial embedding of the underlying
graph is fixed. In simpler graph-theoretical terms our result can be viewed as
follows. Given a graph $G$ with the vertex set partitioned into three parts
embedded on a 2-sphere, our algorithm decides if we can augment $G$ by adding
edges without creating an edge-crossing so that in the resulting spherical
graph the vertices of each part induce a connected sub-graph. We proceed by a
reduction to the problem of testing the existence of a perfect matching in
planar bipartite graphs. We formulate our result in a slightly more general
setting of cyclic clustered graphs, i.e., the simple graph obtained by
contracting each cluster, where we disregard loops and multi-edges, is a cycle."
"This report presents a new, algorithmic approach to the distributions of the
distance between two points distributed uniformly at random in various
polygons, based on the extended Kinematic Measure (KM) from integral geometry.
We first obtain such random Point Distance Distributions (PDDs) associated with
arbitrary triangles (i.e., triangle-PDDs), including the PDD within a triangle,
and that between two triangles sharing either a common side or a common vertex.
For each case, we provide an algorithmic procedure showing the mathematical
derivation process, based on which either the closed-form expressions or the
algorithmic results can be obtained. The obtained triangle-PDDs can be utilized
for modeling and analyzing the wireless communication networks associated with
triangle geometries, such as sensor networks with triangle-shaped clusters and
triangle-shaped cellular systems with highly directional antennas. Furthermore,
based on the obtained triangle-PDDs, we then show how to obtain the PDDs
associated with arbitrary polygons through the decomposition and recursion
approach, since any polygons can be triangulated, and any geometry shapes can
be approximated by polygons with a needed precision. Finally, we give the PDDs
associated with ring geometries. The results shown in this report can enrich
and expand the theory and application of the probabilistic distance models for
the analysis of wireless communication networks."
"In the metric multi-cover problem (MMC), we are given two point sets $Y$
(servers) and $X$ (clients) in an arbitrary metric space $(X \cup Y, d)$, a
positive integer $k$ that represents the coverage demand of each client, and a
constant $\alpha \geq 1$. Each server can have a single ball of arbitrary
radius centered on it. Each client $x \in X$ needs to be covered by at least
$k$ such balls centered on servers. The objective function that we wish to
minimize is the sum of the $\alpha$-th powers of the radii of the balls.
  In this article, we consider the MMC problem as well as some non-trivial
generalizations, such as (a) the non-uniform MMC, where we allow
client-specific demands, and (b) the $t$-MMC, where we require the number of
open servers to be at most some given integer $t$. For each of these problems,
we present an efficient algorithm that reduces the problem to several instances
of the corresponding $1$-covering problem, where the coverage demand of each
client is $1$. Our reductions preserve optimality up to a multiplicative
constant factor.
  Applying known constant factor approximation algorithms for $1$-covering, we
obtain the first constant approximations for the MMC and these generalizations."
"This paper presents a new O(nlog(n)) algorithm for computing the convex hull
of a set of 3 dimensional points. The algorithm first sorts the point in
(x,y,z) then incrementally adds sorted points to the convex hull using the
constraint that each new point added to the hull can 'see' at least one facet
touching the last point added. The reduces the search time for adding new
points. The algorithm belongs to the family of swept hull algorithms.
  While slower than q-hull for the general case it significantly outperforms
q-hull for the pathological case where all of the points are on the 3D hull (as
is the case for Delaunay triangulation). The algorithm has been named the
'Newton Apple Wrapper algorithm' and has been released under GPL in C++.
keywords: Delaunay triangulation, 3D convex hull."
"Given an even number of points in a plane, we are interested in matching all
the points by straight line segments so that the segments do not cross.
Bottleneck matching is a matching that minimizes the length of the longest
segment. For points in convex position, we present a quadratic-time algorithm
for finding a bottleneck non-crossing matching, improving upon the best
previously known algorithm of cubic time complexity."
"We establish an upper bound of 4.94 on the stretch factor of the Yao graph
$Y_4^\infty$ defined in the $L_\infty$-metric, improving upon the best
previously known upper bound of 6.31. We also establish an upper bound of 54.62
on the stretch factor of the Yao graph $Y_4$ defined in the Euclidean metric,
improving upon the best previously known upper bound of 662.16."
"We propose a flexible and multi-scale method for organizing, visualizing, and
understanding datasets sampled from or near stratified spaces. The first part
of the algorithm produces a cover tree using adaptive thresholds based on a
combination of multi-scale local principal component analysis and topological
data analysis. The resulting cover tree nodes consist of points within or near
the same stratum of the stratified space. They are then connected to form a
\emph{scaffolding} graph, which is then simplified and collapsed down into a
\emph{spine} graph. From this latter graph the stratified structure becomes
apparent. We demonstrate our technique on several synthetic point cloud
examples and we use it to understand song structure in musical audio data."
"There is a graph reduction system so that every optimal 1-planar graph can be
reduced to an irreducible extended wheel graph, provided the reductions are
applied such that the given graph class is preserved. A graph is optimal
1-planar if it can be drawn in the plane with at most one crossing per edge and
is optimal if it has the maximum of 4n-8 edges.
  We show that the reduction system is context-sensitive so that the
preservation of the graph class can be granted by local conditions which can be
tested in constant time. Every optimal 1-planar graph G can be reduced to every
extended wheel graph whose size is in a range from the (second) smallest one to
some upper bound that depends on G. There is a reduction to the smallest
extended wheel graph if G is not 5-connected, but not conversely. The reduction
system has side effects and is non-deterministic and non-confluent.
Nevertheless, reductions can be computed in linear time."
"The cyclic n-roots problem is an important benchmark problem for polynomial
system solvers. We consider the pruning of cone intersections for a polyhedral
method to compute series for the solution curves."
"We look at generalized Delaunay graphs in the constrained setting by
introducing line segments which the edges of the graph are not allowed to
cross. Given an arbitrary convex shape $C$, a constrained Delaunay graph is
constructed by adding an edge between two vertices $p$ and $q$ if and only if
there exists a homothet of $C$ with $p$ and $q$ on its boundary that does not
contain any other vertices visible to $p$ and $q$. We show that, regardless of
the convex shape $C$ used to construct the constrained Delaunay graph, there
exists a constant $t$ (that depends on $C$) such that it is a plane
$t$-spanner. Furthermore, we reduce the upper bound on the spanning ratio for
the special case where the empty convex shape is an arbitrary rectangle to
$\sqrt{2} \cdot \left( 2 l/s + 1 \right)$, where $l$ and $s$ are the length of
the long and short side of the rectangle."
"The thickness of a graph $G=(V,E)$ with $n$ vertices is the minimum number of
planar subgraphs of $G$ whose union is $G$. A polyline drawing of $G$ in
$\mathbb{R}^2$ is a drawing $\Gamma$ of $G$, where each vertex is mapped to a
point and each edge is mapped to a polygonal chain. Bend and layer complexities
are two important aesthetics of such a drawing. The bend complexity of $\Gamma$
is the maximum number of bends per edge in $\Gamma$, and the layer complexity
of $\Gamma$ is the minimum integer $r$ such that the set of polygonal chains in
$\Gamma$ can be partitioned into $r$ disjoint sets, where each set corresponds
to a planar polyline drawing. Let $G$ be a graph of thickness $t$. By
F\'{a}ry's theorem, if $t=1$, then $G$ can be drawn on a single layer with bend
complexity $0$. A few extensions to higher thickness are known, e.g., if $t=2$
(resp., $t>2$), then $G$ can be drawn on $t$ layers with bend complexity 2
(resp., $3n+O(1)$). However, allowing a higher number of layers may reduce the
bend complexity, e.g., complete graphs require $\Theta(n)$ layers to be drawn
using 0 bends per edge.
  In this paper we present an elegant extension of F\'{a}ry's theorem to draw
graphs of thickness $t>2$. We first prove that thickness-$t$ graphs can be
drawn on $t$ layers with $2.25n+O(1)$ bends per edge. We then develop another
technique to draw thickness-$t$ graphs on $t$ layers with bend complexity,
i.e., $O(\sqrt{2}^{t} \cdot n^{1-(1/\beta)})$, where $\beta = 2^{\lceil (t-2)/2
\rceil }$. Previously, the bend complexity was not known to be sublinear for
$t>2$. Finally, we show that graphs with linear arboricity $k$ can be drawn on
$k$ layers with bend complexity $\frac{3(k-1)n}{(4k-2)}$."
"We study an algorithmic problem that is motivated by ink minimization for
sparse set visualizations. Our input is a set of points in the plane which are
either blue, red, or purple. Blue points belong exclusively to the blue set,
red points belong exclusively to the red set, and purple points belong to both
sets. A \emph{red-blue-purple spanning graph} (RBP spanning graph) is a set of
edges connecting the points such that the subgraph induced by the red and
purple points is connected, and the subgraph induced by the blue and purple
points is connected.
  We study the geometric properties of minimum RBP spanning graphs and the
algorithmic problems associated with computing them. Specifically, we show that
the general problem can be solved in polynomial time using matroid techniques.
In addition, we discuss more efficient algorithms for the case in which points
are located on a line or a circle, and also describe a fast $(\frac
12\rho+1)$-approximation algorithm, where $\rho$ is the Steiner ratio."
"We describe an algorithm for solving an important geometric problem arising
in computer-aided manufacturing. When machining a pocket in a solid piece of
material such as steel using a rough tool in a milling machine, sharp convex
corners of the pocket cannot be done properly, but have to be left for finer
tools that are more expensive to use. We want to determine a tool path that
maximizes the use of the rough tool. Mathematically, this boils down to the
following problem. Given a simply-connected set of points $P$ in the plane such
that the boundary $\partial P$ is a curvilinear polygon consisting of $n$ line
segments and circular arcs of arbitrary radii, compute the maximum subset
$Q\subseteq P$ consisting of simply-connected sets where the boundary of each
set is a curve with bounded convex curvature. A closed curve has bounded convex
curvature if, when traversed in counterclockwise direction, it turns to the
left with curvature at most $1$. There is no bound on the curvature where it
turns to the right. The difference in the requirement to left- and
right-curvature is a natural consequence of different conditions when machining
convex and concave areas of the pocket. We devise an algorithm to compute the
unique maximum such set $Q$. The algorithm runs in $O(n\log n)$ time and uses
$O(n)$ space.
  For the correctness of our algorithm, we prove a new generalization of the
Pestov-Ionin Theorem. This is needed to show that the output $Q$ of our
algorithm is indeed maximum in the sense that if $Q'$ is any subset of $P$ with
a boundary of bounded convex curvature, then $Q'\subseteq Q$."
"We study the problem of $k$-visibility in the memory-constrained model. In
this model, the input resides in a randomly accessible read-only memory of
$O(n)$ words with $O(\log{n})$ bits each. An algorithm can read and write
$O(s)$ additional words of workspace during its execution, and it writes its
output to write-only memory. In a given polygon $P$ and for a given point $q
\in P$, we say that a point $p$ is inside the $k$-visibility region of $q$ if
and only if the line segment $pq$ intersects the boundary of $P$ at most $k$
times. Given a simple $n$-vertex polygon $P$ stored in a read-only input array
and a point $q \in P$, we give a time-space trade-off algorithm which reports a
suitable representation of the $k$-visibility region for $q$ in $P$ in
$O(cn/s+n\log{s}+ \min\{{kn/s,n \log{\log_s{n}}}\})$ expected time using $O(s)$
words of workspace. Here $c\leq n$ is the number of critical vertices for $q$,
i.e., the vertices of $P$ where the visibility region changes. We also show how
to generalize this result for non-simple polygons and for sets of line
segments."
"This write-up contains some minor results and notes related to our work
[HQ15] (some of them already known in the literature). In particular, it shows
the following:
  - We show that a graph with polynomial expansion have sublinear separators.
  - We show that hereditary sublinear separators imply that a graph have small
divisions.
  - We show a natural condition on a set of segments, such that they have low
density. This might be of independent interest in trying to define a realistic
input model for a set of segments. Unlike the previous two results, this is
new.
  For context and more details, see the main paper."
"An approach is shown that proves various theorems of plane geometry in an
algorithmic manner. The approach affords transparent proofs of a generalization
of the Theorem of Morley and other well known results by casting them in terms
of constraint satisfaction."
"Let ${\cal P}$ be a set of $n$ points embedded in the plane, and let ${\cal
C}$ be the complete Euclidean graph whose point-set is ${\cal P}$. Each edge in
${\cal C}$ between two points $p, q$ is realized as the line segment $[pq]$,
and is assigned a weight equal to the Euclidean distance $|pq|$. In this paper,
we show how to construct in $O(n\lg{n})$ time a plane spanner of ${\cal C}$ of
maximum degree at most 4 and stretch factor at most 20. This improves a long
sequence of results on the construction of plane spanners of ${\cal C}$. Our
result matches the smallest known upper bound of 4 by Bonichon et al. on the
maximum degree of plane spanners of ${\cal C}$, while significantly improving
their stretch factor upper bound from 156.82 to 20. The construction of our
spanner is based on Delaunay triangulations defined with respect to the
equilateral-triangle distance, and uses a different approach than that used by
Bonichon et al. Our approach leads to a simple and intuitive construction of a
well-structured spanner, and reveals useful structural properties of the
Delaunay triangulations defined with respect to the equilateral-triangle
distance.
  The structure of the constructed spanner implies that when ${\cal P}$ is in
convex position, the maximum degree of this spanner is at most 3. Combining the
above degree upper bound with the fact that 3 is a lower bound on the maximum
degree of any plane spanner of ${\cal C}$ when the point-set ${\cal P}$ is in
convex position, the results in this paper give a tight bound of 3 on the
maximum degree of plane spanners of ${\cal C}$ for point-sets in convex
position."
"Given a hypergraph $H$ with $m$ hyperedges and a set $Q$ of $m$ \emph{pinning
subspaces}, i.e.\ globally fixed subspaces in Euclidean space $\mathbb{R}^d$, a
\emph{pinned subspace-incidence system} is the pair $(H, Q)$, with the
constraint that each pinning subspace in $Q$ is contained in the subspace
spanned by the point realizations in $\mathbb{R}^d$ of vertices of the
corresponding hyperedge of $H$. This paper provides a combinatorial
characterization of pinned subspace-incidence systems that are \emph{minimally
rigid}, i.e.\ those systems that are guaranteed to generically yield a locally
unique realization.
  Pinned subspace-incidence systems have applications in the \emph{Dictionary
Learning (aka sparse coding)} problem, i.e.\ the problem of obtaining a sparse
representation of a given set of data vectors by learning \emph{dictionary
vectors} upon which the data vectors can be written as sparse linear
combinations. Viewing the dictionary vectors from a geometry perspective as the
spanning set of a subspace arrangement, the result gives a tight bound on the
number of dictionary vectors for sufficiently randomly chosen data vectors, and
gives a way of constructing a dictionary that meets the bound. For less
stringent restrictions on data, but a natural modification of the dictionary
learning problem, a further dictionary learning algorithm is provided. Although
there are recent rigidity based approaches for low rank matrix completion, we
are unaware of prior application of combinatorial rigidity techniques in the
setting of Dictionary Learning. We also provide a systematic classification of
problems related to dictionary learning together with various algorithms, their
assumptions and performance."
"We give lower bounds for various natural node- and edge-based local
strategies for exploring a graph. We consider this problem both in the setting
of an arbitrary graph as well as the abstraction of a geometric exploration of
a space by a robot, both of which have been extensively studied. We consider
local exploration policies that use time-of-last- visit or alternatively
least-frequently-visited local greedy strategies to select the next step in the
exploration path. Both of these strategies were previously considered by Cooper
et al. (2011) for a scenario in which counters for the last visit or visit
frequency are attached to the edges. In this work we consider the case in which
the counters are associated with the nodes, which for the case of dual graphs
of geometric spaces could be argued to be intuitively more natural and likely
more efficient. Surprisingly, these alternate strategies give worst-case
superpolynomial/ exponential time for exploration, whereas the least-frequently
visited strategy for edges has a polynomially bounded exploration time, as
shown by Cooper et al. (2011)."
"We study several natural instances of the geometric hitting set problem for
input consisting of sets of line segments (and rays, lines) having a small
number of distinct slopes. These problems model path monitoring (e.g., on road
networks) using the fewest sensors (the ""hitting points""). We give
approximation algorithms for cases including (i) lines of 3 slopes in the
plane, (ii) vertical lines and horizontal segments, (iii) pairs of
horizontal/vertical segments. We give hardness and hardness of approximation
results for these problems. We prove that the hitting set problem for vertical
lines and horizontal rays is polynomially solvable."
"We present algorithms and data structures that support the interactive
analysis of the grouping structure of one-, two-, or higher-dimensional
time-varying data while varying all defining parameters. Grouping structures
characterise important patterns in the temporal evaluation of sets of
time-varying data. We follow Buchin et al. [JoCG 2015] who define groups using
three parameters: group-size, group-duration, and inter-entity distance. We
give upper and lower bounds on the number of maximal groups over all parameter
values, and show how to compute them efficiently. Furthermore, we describe data
structures that can report changes in the set of maximal groups in an
output-sensitive manner. Our results hold in $\mathbb{R}^d$ for fixed $d$."
"We revisit the minimum-link path problem: Given a polyhedral domain and two
points in it, connect the points by a polygonal path with minimum number of
edges. We consider settings where the vertices and/or the edges of the path are
restricted to lie on the boundary of the domain, or can be in its interior. Our
results include bit complexity bounds, a novel general hardness construction,
and a polynomial-time approximation scheme. We fully characterize the situation
in 2D, and provide first results in dimensions 3 and higher for several
variants of the problem.
  Concretely, our results resolve several open problems. We prove that
computing the minimum-link diffuse reflection path, motivated by ray tracing in
computer graphics, is NP-hard, even for two-dimensional polygonal domains with
holes. This has remained an open problem [1] despite a large body of work on
the topic. We also resolve the open problem from [2] mentioned in the handbook
[3] (see Chapter 27.5, Open problem 3) and The Open Problems Project [4] (see
Problem 22): ""What is the complexity of the minimum-link path problem in
3-space?"" Our results imply that the problem is NP-hard even on terrains (and
hence, due to discreteness of the answer, there is no FPTAS unless P=NP), but
admits a PTAS."
"In this paper, we study the linear separability problem for stochastic
geometric objects under the well-known unipoint/multipoint uncertainty models.
Let $S=S_R \cup S_B$ be a given set of stochastic bichromatic points, and
define $n = \min\{|S_R|, |S_B|\}$ and $N = \max\{|S_R|, |S_B|\}$. We show that
the separable-probability (SP) of $S$ can be computed in $O(nN^{d-1})$ time for
$d \geq 3$ and $O(\min\{nN \log N, N^2\})$ time for $d=2$, while the expected
separation-margin (ESM) of $S$ can be computed in $O(nN^{d})$ time for $d \geq
2$. In addition, we give an $\Omega(nN^{d-1})$ witness-based lower bound for
computing SP, which implies the optimality of our algorithm among all those in
this category. Also, a hardness result for computing ESM is given to show the
difficulty of further improving our algorithm. As an extension, we generalize
the same problems from points to general geometric objects, i.e., polytopes
and/or balls, and extend our algorithms to solve the generalized SP and ESM
problems in $O(nN^{d})$ and $O(nN^{d+1})$ time, respectively. Finally, we
present some applications of our algorithms to stochastic convex-hull related
problems."
"Congruence between two n-point sets in 4 dimension can be checked in O(n log
n) time. On the way to establishing this result, we revisit several parts of
4-dimensional geometry, such as angles and distances between planes, Hopf
fibrations, and Coxeter groups."
"In the Line Cover problem a set of n points is given and the task is to cover
the points using either the minimum number of lines or at most k lines. In
Curve Cover, a generalization of Line Cover, the task is to cover the points
using curves with d degrees of freedom. Another generalization is the
Hyperplane Cover problem where points in d-dimensional space are to be covered
by hyperplanes. All these problems have kernels of polynomial size, where the
parameter is the minimum number of lines, curves, or hyperplanes needed. First
we give a non-parameterized algorithm for both problems in O*(2^n) (where the
O*(.) notation hides polynomial factors of n) time and polynomial space,
beating a previous exponential-space result. Combining this with incidence
bounds similar to the famous Szemeredi-Trotter bound, we present a Curve Cover
algorithm with running time O*((Ck/log k)^((d-1)k)), where C is some constant.
Our result improves the previous best times O*((k/1.35)^k) for Line Cover
(where d=2), O*(k^(dk)) for general Curve Cover, as well as a few other bounds
for covering points by parabolas or conics. We also present an algorithm for
Hyperplane Cover in R^3 with running time O*((Ck^2/log^(1/5) k)^k), improving
on the previous time of O*((k^2/1.3)^k)."
"We present an $O(n\log n)$-time algorithm that determines whether a given
planar $n$-gon is weakly simple. This improves upon an $O(n^2\log n)$-time
algorithm by Chang, Erickson, and Xu (2015). Weakly simple polygons are
required as input for several geometric algorithms. As such, how to recognize
simple or weakly simple polygons is a fundamental question."
"Packing graphs is a combinatorial problem where several given graphs are
being mapped into a common host graph such that every edge is used at most
once. In the planar tree packing problem we are given two trees T1 and T2 on n
vertices and have to find a planar graph on n vertices that is the
edge-disjoint union of T1 and T2. A clear exception that must be made is the
star which cannot be packed together with any other tree. But according to a
conjecture of Garc\'ia et al. from 1997 this is the only exception, and all
other pairs of trees admit a planar packing. Previous results addressed various
special cases, such as a tree and a spider tree, a tree and a caterpillar, two
trees of diameter four, two isomorphic trees, and trees of maximum degree
three. Here we settle the conjecture in the affirmative and prove its general
form, thus making it the planar tree packing theorem. The proof is constructive
and provides a polynomial time algorithm to obtain a packing for two given
nonstar trees."
"Given a simple polygon $\mathcal{P}$ on $n$ vertices, two points $x,y$ in
$\mathcal{P}$ are said to be visible to each other if the line segment between
$x$ and $y$ is contained in $\mathcal{P}$. The Point Guard Art Gallery problem
asks for a minimum set $S$ such that every point in $\mathcal{P}$ is visible
from a point in $S$. The Vertex Guard Art Gallery problem asks for such a set
$S$ subset of the vertices of $\mathcal{P}$. A point in the set $S$ is referred
to as a guard. For both variants, we rule out any $n^{o(k / \log k)}$
algorithm, where $k := |S|$ is the number of guards, unless the Exponential
Time Hypothesis fails. These lower bounds almost match the $n^{O(k)}$
algorithms that exist for both problems."
"We propose a new data-structure, the generalized randomized kd forest, or
kgeraf, for approximate nearest neighbor searching in high dimensions. In
particular, we introduce new randomization techniques to specify a set of
independently constructed trees where search is performed simultaneously, hence
increasing accuracy. We omit backtracking, and we optimize distance
computations, thus accelerating queries. We release public domain software
geraf and we compare it to existing implementations of state-of-the-art methods
including BBD-trees, Locality Sensitive Hashing, randomized kd forests, and
product quantization. Experimental results indicate that our method would be
the method of choice in dimensions around 1,000, and probably up to 10,000, and
pointsets of cardinality up to a few hundred thousands or even one million;
this range of inputs is encountered in many critical applications today. For
instance, we handle a real dataset of $10^6$ images represented in 960
dimensions with a query time of less than $1$sec on average and 90\% responses
being true nearest neighbors."
"In the polytope membership problem, a convex polytope $K$ in $\mathbb{R}^d$
is given, and the objective is to preprocess $K$ into a data structure so that,
given any query point $q \in \mathbb{R}^d$, it is possible to determine
efficiently whether $q \in K$. We consider this problem in an approximate
setting. Given an approximation parameter $\varepsilon$, the query can be
answered either way if the distance from $q$ to $K$'s boundary is at most
$\varepsilon$ times $K$'s diameter. We assume that the dimension $d$ is fixed,
and $K$ is presented as the intersection of $n$ halfspaces. Previous solutions
to approximate polytope membership were based on straightforward applications
of classic polytope approximation techniques by Dudley (1974) and Bentley et
al. (1982). The former is optimal in the worst-case with respect to space, and
the latter is optimal with respect to query time.
  We present four main results. First, we show how to combine the two above
techniques to obtain a simple space-time trade-off. Second, we present an
algorithm that dramatically improves this trade-off. In particular, for any
constant $\alpha \ge 4$, this data structure achieves query time
$O(1/\varepsilon^{(d-1)/\alpha})$ and space roughly $O(1/\varepsilon^{(d-1)(1 -
O(\log \alpha)/\alpha)})$. We do not know whether this space bound is tight,
but our third result shows that there is a convex body such that our algorithm
achieves a space of at least $\Omega(
1/\varepsilon^{(d-1)(1-O(\sqrt{\alpha})/\alpha} )$. Our fourth result shows
that it is possible to reduce approximate Euclidean nearest neighbor searching
to approximate polytope membership queries. Combined with the above results,
this provides significant improvements to the best known space-time trade-offs
for approximate nearest neighbor searching in $\mathbb{R}^d$."
"A new O(nlog(n)) algorithm is presented for performing Delaunay triangulation
of sets of 2D points. The novel component of the algorithm is a radially
propagating \emph{sweep-hull} (sequentially created from the radially sorted
set of 2D points, giving a non-overlapping triangulation), paired with a final
triangle flipping step to give the Delaunay triangluation.
  In empirical tests the algorithm runs in approximately half the time of
q-hull for 2D Delaunay triangulation on randomly generated point sets."
"We trace movements of certain points in space-time along their corresponding
continuous path. We partition the space at every moment of time using
alpha-Complexes, Voronoi medusa is then the collection or union of restricted
Voronoi cells at every moment in time. We can imagine them as a four
dimensional structure formed when three dimensional restricted Voronoi cells
sweeps continuously through the extra dimension of time. Similarly Delaunay
medusa is the collection of the corresponding Delaunay triangulations at each
moment in time. In this article we prove that these two structures are
homotopic."
"We introduce additively-weighted straight skeletons as a new generalization
of straight skeletons. An additively-weighted straight skeleton is the result
of a wavefront-propagation process where, unlike in previous variants of
straight skeletons, wavefront edges do not necessarily start to move at the
begin of the propagation process but at later points in time. We analyze the
properties of additively-weighted straight skeletons and show how to compute
straight skeletons with both additive and multiplicative weights.
  We then show how to use additively-weighted and multiplicatively-weighted
straight skeletons to generate roofs and terrains for polygonal shapes such as
the footprints of buildings or river networks. As a result, we get an automated
generation of roofs and terrains where the individual facets have different
inclinations and may start at different heights."
"In this paper, we prove that the Max-Morse Matching Problem is approximable,
thus resolving an open problem posed by Joswig and Pfetsch. We describe two
different approximation algorithms for the Max-Morse Matching Problem. For
$D$-dimensional simplicial complexes, we obtain a
$\frac{(D+1)}{(D^2+D+1)}$-factor approximation ratio using a simple edge
reorientation algorithm that removes cycles. Our second result is an algorithm
that provides a $\frac{2}{D}$-factor approximation for simplicial manifolds by
processing the simplices in increasing order of dimension. One application of
these algorithms is towards efficient homology computation of simplicial
complexes. Experiments using a prototype implementation on several datasets
indicate that the algorithm computes near optimal results."
"We describe a framework for counting and enumerating various types of
crossing-free geometric graphs on a planar point set. The framework generalizes
ideas of Alvarez and Seidel, who used them to count triangulations in time
$O(2^nn^2)$ where $n$ is the number of points. The main idea is to reduce the
problem of counting geometric graphs to counting source-sink paths in a
directed acyclic graph.
  The following new results will emerge. The number of all crossing-free
geometric graphs can be computed in time $O(c^nn^4)$ for some $c < 2.83929$.
The number of crossing-free convex partitions can be computed in time
$O(2^nn^4)$. The number of crossing-free perfect matchings can be computed in
time $O(2^nn^4)$. The number of convex subdivisions can be computed in time
$O(2^nn^4)$. The number of crossing-free spanning trees can be computed in time
$O(c^nn^4)$ for some $c < 7.04313$. The number of crossing-free spanning cycles
can be computed in time $O(c^nn^4)$ for some $c < 5.61804$.
  With the same bounds on the running time we can construct data structures
which allow fast enumeration of the respective classes. For example, after
$O(2^nn^4)$ time of preprocessing we can enumerate the set of all crossing-free
perfect matchings using polynomial time per enumerated object. For
crossing-free perfect matchings and convex partitions we further obtain
enumeration algorithms where the time delay for each (in particular, the first)
output is bounded by a polynomial in $n$.
  All described algorithms are comparatively simple, both in terms of their
analysis and implementation."
"A sliding camera inside an orthogonal polygon $P$ is a point guard that
travels back and forth along an orthogonal line segment $\gamma$ in $P$. The
sliding camera $g$ can see a point $p$ in $P$ if the perpendicular from $p$
onto $\gamma$ is inside $P$. In this paper, we give the first constant-factor
approximation algorithm for the problem of guarding $P$ with the minimum number
of sliding cameras. Next, we show that the sliding guards problem is
linear-time solvable if the (suitably defined) dual graph of the polygon has
bounded treewidth. Finally, we study art gallery theorems for sliding cameras,
thus, give upper and lower bounds in terms of the number of guards needed
relative to the number of vertices $n$."
"Guarding a polygon with few guards is an old and well-studied problem in
computational geometry. Here we consider the following variant: We assume that
the polygon is orthogonal and thin in some sense, and we consider a point $p$
to guard a point $q$ if and only if the minimum axis-aligned rectangle spanned
by $p$ and $q$ is inside the polygon. A simple proof shows that this problem is
NP-hard on orthogonal polygons with holes, even if the polygon is thin. If
there are no holes, then a thin polygon becomes a tree polygon in the sense
that the so-called dual graph of the polygon is a tree. It was known that
finding the minimum set of $r$-guards is polynomial for tree polygons, but the
run-time was $\tilde{O}(n^{17})$. We show here that with a different approach
the running time becomes linear, answering a question posed by Biedl et al.
(SoCG 2011). Furthermore, the approach is much more general, allowing to
specify subsets of points to guard and guards to use, and it generalizes to
polygons with $h$ holes or thickness $K$, becoming fixed-parameter tractable in
$h+K$."
"This paper describes an efficient approach to constructing a resultant
polyline with a minimum number of segments and arcs. While fitting an arc can
be done with complexity O(1) (see [1] and [2]), the main complexity is in
checking that the resultant arc is within the specified tolerance. There are
additional tests to check for the ends and for changes in direction (see [3,
section 3] and [4, sections II.C and II.D]). However, the most important part
in reducing complexity is the ability to subdivide the polyline in order to
limit the number of arc fittings [2]. The approach described in this paper
finds a compressed polyline with a minimum number of segments and arcs."
"A new algorithm for the determination of the relative convex hull in the
plane of a simple polygon A with respect to another simple polygon B which
contains A, is proposed. The relative convex hull is also known as geodesic
convex hull, and the problem of its determination in the plane is equivalent to
find the shortest curve among all Jordan curves lying in the difference set of
B and A and encircling A. Algorithms solving this problem known from
Computational Geometry are based on the triangulation or similar decomposition
of that difference set. The algorithm presented here does not use such
decomposition, but it supposes that A and B are given as ordered sequences of
vertices. The algorithm is based on convex hull calculations of A and B and of
smaller polygons and polylines, it produces the output list of vertices of the
relative convex hull from the sequence of vertices of the convex hull of A."
"An ortho-polygon visibility representation of an $n$-vertex embedded graph
$G$ (OPVR of $G$) is an embedding-preserving drawing of $G$ that maps every
vertex to a distinct orthogonal polygon and each edge to a vertical or
horizontal visibility between its end-vertices. The vertex complexity of an
OPVR of $G$ is the minimum $k$ such that every polygon has at most $k$ reflex
corners. We present polynomial time algorithms that test whether $G$ has an
OPVR and, if so, compute one of minimum vertex complexity. We argue that the
existence and the vertex complexity of an OPVR of $G$ are related to its number
of crossings per edge and to its connectivity. More precisely, we prove that if
$G$ has at most one crossing per edge (i.e., $G$ is a 1-plane graph), an OPVR
of $G$ always exists while this may not be the case if two crossings per edge
are allowed. Also, if $G$ is a 3-connected 1-plane graph, we can compute an
OPVR of $G$ whose vertex complexity is bounded by a constant in $O(n)$ time.
However, if $G$ is a 2-connected 1-plane graph, the vertex complexity of any
OPVR of $G$ may be $\Omega(n)$. In contrast, we describe a family of
2-connected 1-plane graphs for which an embedding that guarantees constant
vertex complexity can be computed in $O(n)$ time. Finally, we present the
results of an experimental study on the vertex complexity of ortho-polygon
visibility representations of 1-plane graphs."
"The described works have been carried out in the framework of a mid-term
study initiated by the Centre Electronique de l'Armement, then by an advanced
study launched by the Direction de la Recherche et des Etudes Technologiques in
France in the aim to develop new techniques for multidimensional hierarchical
modeling and to port them on parallel architecture computers for satisfying the
future needs in processing huge numerical data bases. Following the first tome
describing the modeling principles, the second tome details the way used for
developing the modeling software and for porting it on different computers,
especially on parallel architecture computers. In addition to these works, it
is gone through new algorithms that have been developed after those that have
been presented in the former tome and that are described in pseudo-code in
annex of the present document: - operators for constructive geometry (building
simple shapes, Boolean operators, slice handling); - integral transformations
(epigraph, hypograph, convex hull) ; - homotopic transformations (boundary,
erosion, dilation, opening, closing) ; - median transformations (median
filtering, thinning, median set, intrinsic dimension) ; - transformations of
(hyper-)surface manifolds (median filtering, extension, polynomial fitting of a
simple function). The present publication is ending with the software porting
on two distributed memory parallel computers: - a thin-grained synchronous
computer ; - a coarse-grained asynchronous computer."
"In this paper, we study two classic optimization problems: minimum geometric
dominating set and set cover. Both the problems have been studied for different
types of objects for a long time. These problems become APX-hard when the
objects are axis-parallel rectangles, ellipses, $\alpha$-fat objects of
constant description complexity, and convex polygons. On the other hand, PTAS
(polynomial time approximation scheme) is known for them when the objects are
disks or unit squares. Surprisingly, PTAS was unknown even for arbitrary
squares. For homothetic set of convex objects, an $O(k^4)$ approximation
algorithm is known for dominating set problem, where $k$ is the number of
corners in a convex object. On the other hand, QPTAS (quasi polynomial time
approximation scheme) is known very recently for the covering problem when the
objects are pseudodisks. For both problems obtaining a PTAS remains open for a
large class of objects.
  For the dominating set problems, we prove that the popular local search
algorithm leads to an $(1+\varepsilon)$ approximation when objects are
homothetic set of convex objects (which includes arbitrary squares, $k$-regular
polygons, translated and scaled copies of a convex set etc.) in
$n^{O(1/\varepsilon^2)}$ time. On the other hand, the same technique leads to a
PTAS for geometric covering problem when the objects are convex pseudodisks
(which includes disks, unit height rectangles, homothetic convex objects etc.).
As a consequence, we obtain an easy to implement approximation algorithm for
both problems for a large class of objects, significantly improving the best
known approximation guarantees."
"Terrain Guarding Problem(TGP), which is known to be NP-complete, asks to find
a smallest set of guard locations on a terrain $T$ such that every point on $T$
is visible by a guard. Here, we study this problem on 1.5D orthogonal terrains
where the edges are bound to be horizontal or vertical. We propose a
2-approximation algorithm that runs in O($n \log m$) time, where $n$ and $m$
are the sizes of input and output, respectively. This is an improvement over
the previous best algorithm, which is a 2-approximation with O($n^2$) running
time."
"Given a set $S$ of $n$ disjoint line segments in $\mathbb{R}^{2}$, the
visibility counting problem (VCP) is to preprocess $S$ such that the number of
segments in $S$ visible from any query point $p$ can be computed quickly. This
problem can trivially be solved in logarithmic query time using $O(n^{4})$
preprocessing time and space. Gudmundsson and Morin proposed a 2-approximation
algorithm for this problem with a tradeoff between the space and the query
time. They answer any query in $O_{\epsilon}(n^{1-\alpha})$ with
$O_{\epsilon}(n^{2+2\alpha})$ of preprocessing time and space, where $\alpha$
is a constant $0\leq \alpha\leq 1$, $\epsilon > 0$ is another constant that can
be made arbitrarily small, and $O_{\epsilon}(f(n))=O(f(n)n^{\epsilon})$.
  In this paper, we propose a randomized approximation algorithm for VCP with a
tradeoff between the space and the query time. We will show that for an
arbitrary constants $0\leq \beta\leq \frac{2}{3}$ and $0<\delta <1$, the
expected preprocessing time, the expected space, and the query time of our
algorithm are $O(n^{4-3\beta}\log n)$, $O(n^{4-3\beta})$, and
$O(\frac{1}{\delta^3}n^{\beta}\log n)$, respectively. The algorithm computes
the number of visible segments from $p$, or $m_p$, exactly if $m_p\leq
\frac{1}{\delta^3}n^{\beta}\log n$. Otherwise, it computes a
$(1+\delta)$-approximation $m'_p$ with the probability of at least
$1-\frac{1}{\log n}$, where $m_p\leq m'_p\leq (1+\delta)m_p$."
"In this paper, we indicate a new way to define coordinates for the tiles of
the tilings $\{p,3\}$ and $\{p$$-$$2,4\}$ where the natural number $p$
satisfies $p\geq 7$."
"In this paper, we propose a linear method for $C^{(r,s)}$ approximation of
rational B\'{e}zier curve with arbitrary degree polynomial curve. Based on
weighted least-squares, the problem be converted to an approximation between
two polynomial curves. Then applying Bernstein-Jacobi hybrid polynomials, we
obtain the resulting curve. In order to reduce error, degree reduction method
for B\'{e}zier curve is used. A error bound between rational B\'{e}zier curve
and B\'{e}zier curve is presented. Finally, some examples and figures were
offered to demonstrate the efficiency, simplicity, and stability of our
methods."
"Given an unlabeled road map, we consider, from an algorithmic perspective,
the cartographic problem to place non-overlapping road labels embedded in their
roads. We first decompose the road network into logically coherent road
sections, e.g., parts of roads between two junctions. Based on this
decomposition, we present and implement a new and versatile framework for
placing labels in road maps such that the number of labeled road sections is
maximized. In an experimental evaluation with road maps of 11 major cities we
show that our proposed labeling algorithm is both fast in practice and that it
reaches near-optimal solution quality, where optimal solutions are obtained by
mixed-integer linear programming. In comparison to the standard OpenStreetMap
renderer Mapnik, our algorithm labels 31% more road sections in average."
"When using the convex hull approach in the boundary modeling process,
Model-Based Calibration (MBC) software suites -- such as Model-Based
Calibration Toolbox from MathWorks -- can be computationally intensive
depending on the amount of data modeled. The reason for this is that the
half-space representation of the convex hull is used. We discuss here another
representation of the convex hull, the vertex representation, which proves
capable to reduce the computational cost. Numerical comparisons in this article
are executed in MATLAB by using MBC Toolbox commands, and show that for certain
conditions, the vertex representation outperforms the half-space
representation."
"In this paper, we characterize planar point sets that can be partitioned into
disjoint polygons of arbitrarily specified sizes. We provide an algorithm to
construct such a partition, if it exists, in polynomial time. We show that this
problem is equivalent to finding a specified $2$-factor in the visibility graph
of the point set. The characterization for the case where all cycles have
length $3$ also translates to finding a $K_3$-factor of the visibility graph of
the point set. We show that the generalized problem of finding a $K_k$-factor
of the visibility graph of a given point set for $k \geq 5$ is NP-hard."
"To support exactly tracking a neutron moving along a given line segment
through a CAD model with quadric surfaces, this paper considers the arithmetic
precision required to compute the order of intersection points of two quadrics
along the line segment. When the orders of all but one pair of intersections
are known, we show that a resultant can resolve the order of the remaining pair
using only half the precision that may be required to eliminate radicals by
repeated squaring. We compare the time and accuracy of our technique with
converting to extended precision to calculate roots."
"An Euclidean greedy embedding of a graph is a straight-line embedding in the
plane, such that for every pair of vertices $s$ and $t$, the vertex $s$ has a
neighbor $v$ with smaller distance to $t$ than $s$. This drawing style is
motivated by greedy geometric routing in wireless sensor networks.
  A Christmas cactus is a connected graph in which every two simple cycles have
at most one vertex in common and in which every cutvertex is part of at most
two biconnected blocks. It has been proved that Christmas cactus graphs have an
Euclidean greedy embedding. This fact has played a crucial role in proving that
every 3-connected planar graph has an Euclidean greedy embedding. The proofs
construct greedy embeddings of Christmas cactuses of exponential size, and it
has been an open question whether exponential area is necessary in the worst
case for greedy embeddings of Christmas cactuses. We prove that this is indeed
the case."
"Segmentations are often necessary for the analysis of image data. They are
used to identify different objects, for example cell nuclei, mitochondria, or
complete cells in microscopic images. There might be features in the data, that
cannot be detected by segmentation approaches directly, because they are not
characterized by their texture of boundaries, which are properties most
segmentation techniques rely on, but morphologically. In this report we will
introduce our algorithm for the extraction of suchlike morphological features
of segmented objects from segmentations of neuromuscular junctions and its
interface for informed parameter tuning."
"Nearest-neighbor search, which returns the nearest neighbor of a query point
in a set of points, is an important and widely studied problem in many fields,
and it has wide range of applications. In many of them, such as sensor
databases, location-based services, face recognition, and mobile data, the
location of data is imprecise. We therefore study nearest-neighbor queries in a
probabilistic framework in which the location of each input point is specified
as a probability distribution function. We present efficient algorithms for
  - computing all points that are nearest neighbors of a query point with
nonzero probability; and
  - estimating the probability of a point being the nearest neighbor of a query
point, either exactly or within a specified additive error."
"Given an $n$-vertex graph and two straight-line planar drawings of the graph
that have the same faces and the same outer face, we show that there is a morph
(i.e., a continuous transformation) between the two drawings that preserves
straight-line planarity and consists of $O(n)$ steps, which we prove is optimal
in the worst case. Each step is a unidirectional linear morph, which means that
every vertex moves at constant speed along a straight line, and the lines are
parallel although the vertex speeds may differ. Thus we provide an efficient
version of Cairns' 1944 proof of the existence of straight-line
planarity-preserving morphs for triangulated graphs, which required an
exponential number of steps."
"A non-aligned drawing of a graph is a drawing where no two vertices are in
the same row or column. Auber et al. showed that not all planar graphs have
non-aligned drawings that are straight-line, planar, and in the
minimal-possible $n\times n$-grid. They also showed that such drawings exist if
up to $n-3$ edges may have a bend. In this paper, we give algorithms for
non-aligned planar drawings that improve on the results by Auber et al. In
particular, we give such drawings in an $n\times n$-grid with significantly
fewer bends, and we study what grid-size can be achieved if we insist on having
straight-line drawings."
"In this paper, we study how to draw trees so that they are planar,
straight-line and respect a given order of edges around each node. We focus on
minimizing the height, and show that we can always achieve a height of at most
2pw(T)+1, where pw(T) (the so-called pathwidth) is a known lower bound on the
height. Hence we give an asymptotic 2-approximation algorithm. We also create a
drawing whose height is at most 3pw(T ), but where the width can be bounded by
the number of nodes. Finally we construct trees that require height 2pw(T)+1 in
all planar order-preserving straight-line drawings."
"In 2001, K\'arolyi, Pach and T\'oth introduced a family of point sets to
solve an Erd\H{o}s-Szekeres type problem; which have been used to solve several
other Ed\H{o}s-Szekeres type problems. In this paper we refer to these sets as
nested almost convex sets. A nested almost convex set $\mathcal{X}$ has the
property that the interior of every triangle determined by three points in the
same convex layer of $\mathcal{X}$, contains exactly one point of
$\mathcal{X}$. In this paper, we introduce a characterization of nested almost
convex sets. Our characterization implies that there exists at most one (up to
order type) nested almost convex set of $n$ points. We use our characterization
to obtain a linear time algorithm to construct nested almost convex sets of $n$
points, with integer coordinates of absolute values at most $O(n^{\log_2 5})$.
Finally, we use our characterization to obtain an $O(n\log n)$-time algorithm
to determine whether a set of points is a nested almost convex set."
"Exploiting geometric structure to improve the asymptotic complexity of
discrete assignment problems is a well-studied subject. In contrast, the
practical advantages of using geometry for such problems have not been
explored. We implement geometric variants of the Hopcroft--Karp algorithm for
bottleneck matching (based on previous work by Efrat el al.) and of the auction
algorithm by Bertsekas for Wasserstein distance computation. Both
implementations use k-d trees to replace a linear scan with a geometric
proximity query. Our interest in this problem stems from the desire to compute
distances between persistence diagrams, a problem that comes up frequently in
topological data analysis. We show that our geometric matching algorithms lead
to a substantial performance gain, both in running time and in memory
consumption, over their purely combinatorial counterparts. Moreover, our
implementation significantly outperforms the only other implementation
available for comparing persistence diagrams."
"Let $OWRN = \left\langle W_x,W_y \right\rangle$ be a One Way Road Network
where $W_x$ and $W_y$ are the sets of directed horizontal and vertical roads
respectively. $OWRN$ can be considered as a variation of directed grid graph.
The intersections of the horizontal and vertical roads are the vertices of
$OWRN$ and any two consecutive vertices on a road are connected by an edge. In
this work, we analyze the problem of collision free traffic configuration in a
$OWRN$. A traffic configuration is a two-tuple $TC=\left\langle OWRN,
C\right\rangle$, where $C$ is a set of cars travelling on a pre-defined path.
We prove that finding a maximum cardinality subset $C_{sub}\subseteq C$ such
that $TC=\left\langle OWRN, C_{sub}\right\rangle$ is collision-free, is
NP-hard. Lastly we investigate the properties of connectedness, shortest paths
in a $OWRN$."
"To produce cartographic maps, simplification is typically used to reduce
complexity of the map to a legible level. With schematic maps, however, this
simplification is pushed far beyond the legibility threshold and is instead
constrained by functional need and resemblance. Moreover, stylistic geometry is
often used to convey the schematic nature of the map. In this paper we explore
discretized approaches to computing a schematic shape $S$ for a simple polygon
$P$. We do so by overlaying a plane graph $G$ on $P$ as the solution space for
the schematic shape. Topological constraints imply that $S$ should describe a
simple polygon. We investigate two approaches, simple map matching and
connected face selection, based on commonly used similarity metrics.
  With the former, $S$ is a simple cycle $C$ in $G$ and we quantify resemblance
via the Fr\'echet distance. We prove that it is NP-hard to compute a cycle that
approximates the minimal Fr\'echet distance over all simple cycles in a plane
graph $G$. This result holds even if $G$ is a partial grid graph, if area
preservation is required and if we assume a given sequence of turns is
specified.
  With the latter, $S$ is a connected face set in $G$, quantifying resemblance
via the symmetric difference. Though the symmetric difference seems a less
strict measure, we prove that it is NP-hard to compute the optimal face set.
This result holds even if $G$ is full grid graph or a triangular or hexagonal
tiling, and if area preservation is required. Moreover, it is independent of
whether we allow the set of faces to have holes or not."
"We present fast and accurate ways to normalize two and three dimensional
vectors and quaternions and compute their length. Our approach is an adaptation
of ideas used in the linear algebra library LAPACK, and we believe that the
computational geometry and computer aided design communities are not aware of
the possibility of speeding up these fundamental operations in the robust way
proposed here."
"We show how to represent a simple polygon $P$ by a grid (pixel-based) polygon
$Q$ that is simple and whose Hausdorff or Fr\'echet distance to $P$ is small.
For any simple polygon $P$, a grid polygon exists with constant Hausdorff
distance between their boundaries and their interiors. Moreover, we show that
with a realistic input assumption we can also realize constant Fr\'echet
distance between the boundaries. We present algorithms accompanying these
constructions, heuristics to improve their output while keeping the distance
bounds, and experiments to assess the output."
"A rectilinear polygon is a polygon whose edges are axis-aligned. Walking
counterclockwise on the boundary of such a polygon yields a sequence of left
turns and right turns. The number of left turns always equals the number of
right turns plus 4. It is known that any such sequence can be realized by a
rectilinear polygon. In this paper, we consider the problem of finding
realizations that minimize the perimeter or the area of the polygon or the area
of the bounding box of the polygon. We show that all three problems are NP-hard
in general. Then we consider the special cases of $x$-monotone and
$xy$-monotone rectilinear polygons. For these, we can optimize the three
objectives efficiently."
"We study a new search problem on the plane involving a robot and an immobile
treasure, initially placed at distance $1$ from each other. The length $\beta$
of an arc (a fence) within the perimeter of the corresponding circle, as well
as the promise that the treasure is outside the fence, is given as part of the
input. The goal is to device movement trajectories so that the robot locates
the treasure in minimum time. Notably, although the presence of the fence
limits searching uncertainty, the location of the fence is unknown, and in the
worst case analysis is determined adversarially. Nevertheless, the robot has
the ability to move in the interior of the circle. In particular the robot can
attempt a number of chord-jump moves if it happens to be within the fence or if
an endpoint of the fence is discovered.
  The optimal solution to our question can be obtained as a solution to a
complicated optimization problem, which involves trigonometric functions, and
trigonometric equations that do not admit closed form solutions. For the 1-Jump
Algorithm, we fully describe the optimal trajectory, and provide an analysis of
the associated cost as a function of $\beta$. Our analysis indicates that the
optimal k-Jump Algorithm requires that the robot has enough memory and
computation power to compute the optimal chord-jumps. Motivated by this, we
give an abstract performance analysis for every k-Jump Algorithm. Subsequently,
we present a highly efficient Halving Heuristic k-Jump Algorithm that can
effectively approximate the optimal k-Jump Algorithm, with very limited memory
and computation requirements."
"Let $S$ be a finite set of points in the plane that are in convex position.
We present an algorithm that constructs a plane $\frac{3+4\pi}{3}$-spanner of
$S$ whose vertex degree is at most 3. Let $\Lambda$ be the vertex set of a
finite non-uniform rectangular lattice in the plane. We present an algorithm
that constructs a plane $3\sqrt{2}$-spanner for $\Lambda$ whose vertex degree
is at most 3. For points that are in the plane and in general position, we show
how to compute plane degree-3 spanners with a linear number of Steiner points."
"Given a positive real value $\delta$, a set $P$ of points along a line and a
distance function $d$, in the movement to independence problem, we wish to move
the points to new positions on the line such that for every two points
$p_{i},p_{j} \in P$, we have $d(p_{i},p_{j}) \geq \delta$ while minimizing the
sum of movements of all points. This measure of the cost for moving the points
was previously unsolved in this setting. However for different cost measures
there are algorithms of $O(n \log(n))$ or of $O(n)$. We present an $O(n
\log(n))$ algorithm for the points on a line and thus conclude the setting in
one dimension."
"An example of reversible (or hinge inside-out transformable) figures is the
Dudeney's Haberdasher's puzzle in which an equilateral triangle is dissected
into four pieces, then hinged like a chain, and then is transformed into a
square by rotating the hinged pieces. Furthermore, the entire boundary of each
figure goes into the inside of the other figure and becomes the dissection
lines of the other figure. Many intriguing results on reversibilities of
figures have been found in prior research, but most of them are results on
polygons. This paper generalizes those results to a wider range of general
connected figures. It is shown that two nets obtained by cutting the surface of
an arbitrary convex polyhedron along non-intersecting dissection trees are
reversible. Moreover, a condition for two nets of an isotetrahedron to be both
reversible and tessellative is given."
"Given a plane forest $F = (V, E)$ of $|V| = n$ points, we find the minimum
set $S \subseteq E$ of edges such that the edge-constrained minimum spanning
tree over the set $V$ of vertices and the set $S$ of constraints contains $F$.
We present an $O(n \log n )$-time algorithm that solves this problem. We
generalize this to other proximity graphs in the constraint setting, such as
the relative neighbourhood graph, Gabriel graph, $\beta$-skeleton and Delaunay
triangulation. We present an algorithm that identifies the minimum set
$S\subseteq E$ of edges of a given plane graph $I=(V,E)$ such that $I \subseteq
CG_\beta(V, S)$ for $1 \leq \beta \leq 2$, where $CG_\beta(V, S)$ is the
constraint $\beta$-skeleton over the set $V$ of vertices and the set $S$ of
constraints. The running time of our algorithm is $O(n)$, provided that the
constrained Delaunay triangulation of $I$ is given."
"This paper presents a fast an robust mesh generation procedure that is able
to generate meshes of the earth system (ocean and continent) in matters of
seconds. Our algorithm takes as input a standard shape-file i.e. geospatial
vector data format for geographic information system (GIS) software. The input
is initially coarsened in order to automatically remove unwanted channels that
are under a desired resolution. A valid non-overlapping 1D mesh is then created
on the sphere using the Euclidian coordinates system $x,y,z$. A modified
Delaunay kernel is then proposed that enables to generate meshes on the sphere
in a straightforward manner without parametrization. One of the main difficulty
in dealing with geographical data is the over-sampled nature of coastline
representations. We propose here an algorithm that automatically unrefines
coastline data. Small features are automatically removed while always keeping a
valid (non-overlapping) geometrical representation of the domain. A Delaunay
refinement procedure is subsequently applied to the domain. The refinement
scheme is also multi-threaded at a fine grain level, allowing to generate about
a million points per second on 8 threads. Examples of meshes of the Baltic sea
as well as of the global ocean are presented."
"In a multi-robot system, a number of autonomous robots would sense,
communicate, and decide to move within a given domain to achieve a common goal.
In this paper, we consider a new variant of the pursuit-evasion problem in
which the robots (pursuers) each move back and forth along an orthogonal line
segment inside a simple orthogonal polygon $P$. A point $p$ can be covered by a
sliding robot that moves along a line segment s, if there exists a point $q\in
s$ such that $\overline{pq}$ is a line segment perpendicular to $s$. In the
pursuit-evasion problem, a polygonal region is given and a robot called a
pursuer tries to find some mobile targets called evaders. The goal of this
problem is to design a motion strategy for the pursuer such that it can detect
all the evaders. We assume that $P$ includes unpredictable, moving evaders that
have unbounded speed. We propose a motion-planning algorithm for a group of
sliding robots, assuming that they move along the pre-located line segments
with a constant speed to detect all the evaders with unbounded speed."
"We study the construction of the minimum cost spanning geometric graph of a
given rooted point set $P$ where each point of $P$ is connected to the root by
a path that satisfies a given property. We focus on two properties, namely the
monotonicity w.r.t. a single direction ($y$-monotonicity) and the monotonicity
w.r.t. a single pair of orthogonal directions ($xy$-monotonicity). We propose
algorithms that compute the rooted $y$-monotone ($xy$-monotone) minimum
spanning tree of $P$ in $O(|P|\log^2 |P|)$ (resp. $O(|P|\log^3 |P|)$) time when
the direction (resp. pair of orthogonal directions) of monotonicity is given,
and in $O(|P|^2\log|P|)$ time when the optimum direction (resp. pair of
orthogonal directions) has to be determined. We also give simple algorithms
which, given a rooted connected geometric graph, decide if the root is
connected to every other vertex by paths that are all monotone w.r.t. the same
direction (pair of orthogonal directions)."
"We study the problem of determining optimal coordinated motions for two disc
robots in an otherwise obstacle-free plane. Using the total path length traced
by the two disc centres as a measure of distance, we give an exact
characterization of a shortest collision-avoiding motion for all initial and
final configurations of the robots. The individual paths are composed of at
most six (straight or circular-arc) segments, and their total length can be
expressed as a simple integral with a closed form solution depending only on
the initial and final configuration of the robots. Furthermore, the paths can
be parametrized in such a way that (i) only one robot is moving at any given
time (decoupled motion), or (ii) the angle between the two robots' centres
changes monotonically."
"Following a recent improvement of Cardinal et al. on the complexity of a
linear decision tree for $k$-SUM, resulting in $O(n^3 \log^3{n})$ linear
queries, we present a further improvement to $O(n^2 \log^2{n})$ such queries."
"The construction of $r$-nets offers a powerful tool in computational and
metric geometry. We focus on high-dimensional spaces and present a new
randomized algorithm which efficiently computes approximate $r$-nets with
respect to Euclidean distance. For any fixed $\epsilon>0$, the approximation
factor is $1+\epsilon$ and the complexity is polynomial in the dimension and
subquadratic in the number of points. The algorithm succeeds with high
probability. More specifically, the best previously known LSH-based
construction of Eppstein et al.\ \cite{EHS15} is improved in terms of
complexity by reducing the dependence on $\epsilon$, provided that $\epsilon$
is sufficiently small. Our method does not require LSH but, instead, follows
Valiant's \cite{Val15} approach in designing a sequence of reductions of our
problem to other problems in different spaces, under Euclidean distance or
inner product, for which $r$-nets are computed efficiently and the error can be
controlled. Our result immediately implies efficient solutions to a number of
geometric problems in high dimension, such as finding the
$(1+\epsilon)$-approximate $k$th nearest neighbor distance in time subquadratic
in the size of the input."
"Solving geometric optimization problems over uncertain data have become
increasingly important in many applications and have attracted a lot of
attentions in recent years. In this paper, we study two important geometric
optimization problems, the $k$-center problem and the $j$-flat-center problem,
over stochastic/uncertain data points in Euclidean spaces. For the stochastic
$k$-center problem, we would like to find $k$ points in a fixed dimensional
Euclidean space, such that the expected value of the $k$-center objective is
minimized. For the stochastic $j$-flat-center problem, we seek a $j$-flat
(i.e., a $j$-dimensional affine subspace) such that the expected value of the
maximum distance from any point to the $j$-flat is minimized. We consider both
problems under two popular stochastic geometric models, the existential
uncertainty model, where the existence of each point may be uncertain, and the
locational uncertainty model, where the location of each point may be
uncertain. We provide the first PTAS (Polynomial Time Approximation Scheme) for
both problems under the two models. Our results generalize the previous results
for stochastic minimum enclosing ball and stochastic enclosing cylinder."
"Motivated by automated junction recognition in tracking data, we study a
problem of placing a square or disc of fixed size in an arrangement of lines or
line segments in the plane. We let distances among the intersection points of
the lines and line segments with the square or circle define a clustering, and
study the complexity of \emph{critical} placements for this clustering. Here
critical means that arbitrarily small movements of the placement change the
clustering.
  A parameter $\varepsilon$ defines the granularity of the clustering. Without
any assumptions on $\varepsilon$, the critical placements have a trivial
$O(n^4)$ upper bound. When the square or circle has unit size and $0 <
\varepsilon < 1$ is given, we show a refined $O(n^2/\varepsilon^2)$ bound,
which is tight in the worst case.
  We use our combinatorial bounds to design efficient algorithms to compute
junctions. As a proof of concept for our algorithms we have a prototype
implementation that showcases their application in a basic visualization of a
set of $n$ trajectories and their $k$ most important junctions."
"We consider the problem of augmenting an n-vertex graph embedded in a metric
space, by inserting one additional edge in order to minimize the diameter of
the resulting graph. We present exact algorithms for the cases when (i) the
input graph is a path, running in O(n \log^3 n) time, and (ii) the input graph
is a tree, running in O(n^2 \log n) time. We also present an algorithm that
computes a (1+\eps)-approximation in O(n + 1/\eps^3) time, for paths in R^d,
where d is a constant."
"We investigate the problem of determining if a given graph corresponds to the
dual of a triangulation of a simple polygon. This is a graph recognition
problem, where in our particular case we wish to recognize a graph which
corresponds to the dual of a triangulation of a simple polygon with or without
holes and interior points. We show that the difficulty of this problem depends
critically on the amount of information given and we give a sharp boundary
between the various tractable and intractable versions of the problem."
"We study the problem of detecting zeros of continuous functions that are
known only up to an error bound, extending the earlier theoretical work with
explicit algorithms and experiments with an implementation. More formally, the
robustness of zero of a continuous map $f: X\to \mathbb{R}^n$ is the maximal
$r>0$ such that each $g:X\to\mathbb{R}^n$ with $\|f-g\|_\infty\le r$ has a
zero. We develop and implement an efficient algorithm approximating the
robustness of zero. Further, we show how to use the algorithm for approximating
worst-case optima in optimization problems in which the feasible domain is
defined by equations that are only known approximately.
  An important ingredient is an algorithm for deciding the topological
extension problem based on computing cohomological obstructions to
extendability and their persistence. We describe an explicit algorithm for the
primary and secondary obstruction, two stages of a sequence of algorithms with
increasing complexity. We provide experimental evidence that for random
Gaussian fields, the primary obstruction---a much less computationally
demanding test than the secondary obstruction---is typically sufficient for
approximating robustness of zero."
"In this paper we present an interesting gadget based on the chain pair
simplification problem under the discrete Fr\'echet distance (CPS-3F), which
allows the construction of arbitrarily long paths that must be chosen in the
simplification of the two curves. A pseudopolynomial time reduction from set
partition is given as an example. For clarification, CPS-3F was recently shown
to be in \textbf{P}, and the reduction is merely to show how the gadget works."
"We study several variations of line segment covering problem with
axis-parallel unit squares in $I\!\!R^2$. A set $S$ of $n$ line segments is
given. The objective is to find the minimum number of axis-parallel unit
squares which cover at least one end-point of each segment. The variations
depend on the orientation and length of the input segments. We prove some of
these problems to be NP-complete, and give constant factor approximation
algorithms for those problems. For some variations, we have polynomial time
exact algorithms. For the general version of the problem, where the segments
are of arbitrary length and orientation, and the squares are given as input, we
propose a factor 16 approximation result based on multilevel linear programming
relaxation technique, which may be useful for solving some other problems.
Further, we show that our problems have connections with the problems studied
by Arkin et al. 2015 on conflict-free covering problem. Our NP-completeness
results hold for more simplified types of objects than those of Arkin et al.
2015."
"A sliding k-transmitter in an orthogonal polygon P is a mobile guard that
travels back and forth along an orthogonal line segment s inside P. It can see
a point p in P if the perpendicular from p onto s intersects the boundary of P
at most k times. We show that guarding an orthogonal polygon P with the minimum
number of k-transmitters is NP-hard, for any fixed k>0, even if P is simple and
monotone. Moreover, we give an O(1)-approximation algorithm for this problem."
"We study the Unique Set Cover problem on unit disks and unit squares. For a
given set $P$ of $n$ points and a set $D$ of $m$ geometric objects both in the
plane, the objective of the Unique Set Cover problem is to select a subset
$D'\subseteq D$ of objects such that every point in $P$ is covered by at least
one object in $D'$ and the number of points covered uniquely is maximized,
where a point is covered uniquely if the point is covered by exactly one object
in $D'$. In this paper, (i) we show that the Unique Set Cover is NP-hard on
both unit disks and unit squares, and (ii) we give a PTAS for this problem on
unit squares by applying the mod-one approach of Chan and Hu (Comput. Geom.
48(5), 2015)."
"A notion of ""radially monotone"" cut paths is introduced as an effective
choice for finding a non-overlapping edge-unfolding of a convex polyhedron.
These paths have the property that the two sides of the cut avoid overlap
locally as the cut is infinitesimally opened by the curvature at the vertices
along the path. It is shown that a class of planar, triangulated convex domains
always have a radially monotone spanning forest, a forest that can be found by
an essentially greedy algorithm. This algorithm can be mimicked in 3D and
applied to polyhedra inscribed in a sphere. Although the algorithm does not
provably find a radially monotone cut tree, it in fact does find such a tree
with high frequency, and after cutting unfolds without overlap. This
performance of a greedy algorithm leads to the conjecture that spherical
polyhedra always have a radially monotone cut tree and unfold without overlap."
"In tolerancing analysis, geometrical or contact specifications can be
represented by polytopes. Due to the degrees of invariance of surfaces and that
of freedom of joints, these operand polytopes are originally unbounded in most
of the cases (i.e. polyhedra). Homri et al. proposed the introduction of
virtual boundaries (called cap half-spaces) over the unbounded displacements of
each polyhedron to turn them into 6-polytopes. This decision was motivated by
the complexity that operating on polyhedra in R6 supposes. However, that
strategy has to face the multiplication of the number of cap half-spaces during
the computation of Minkowski sums. In general, the time for computing cap
facets is greater than for computing facets representing real limits of bounded
displacements. In order to deal with that, this paper proposes the use of the
theory of screws to determine the set of displacements that defines the
positioning of one surface in relation to another. This set of displacements
defines the subspace of R6 in which the polytopes of the respective surfaces
have to be projected and operated to avoid calculating facets and vertices
along the directions of unbounded displacements. With this new strategy it is
possible to decrease the complexity of the Minkowski sums by reducing the
dimension of the operands and consequently reducing the computation time. An
example illustrates the method and shows the time reduction during the
computations."
"We show how to design a universal shape replicator in a self-assembly system
with both attractive and repulsive forces. More precisely, we show that there
is a universal set of constant-size objects that, when added to any unknown
hole-free polyomino shape, produces an unbounded number of copies of that shape
(plus constant-size garbage objects). The constant-size objects can be easily
constructed from a constant number of individual tile types using a constant
number of preprocessing self-assembly steps. Our construction uses the
well-studied 2-Handed Assembly Model (2HAM) of tile self-assembly, in the
simple model where glues interact only with identical glues, allowing glue
strengths that are either positive (attractive) or negative (repulsive), and
constant temperature (required glue strength for parts to hold together). We
also require that the given shape has specified glue types on its surface, and
that the feature size (smallest distance between nonincident edges) is bounded
below by a constant. Shape replication necessarily requires a self-assembly
model where parts can both attach and detach, and this construction is the
first to do so using the natural model of negative/repulsive glues (also
studied before for other problems such as fuel-efficient computation); previous
replication constructions require more powerful global operations such as an
""enzyme"" that destroys a subset of the tile types."
"The algorithmic self-assembly of shapes has been considered in several models
of self-assembly. For the problem of \emph{shape construction}, we consider an
extended version of the Two-Handed Tile Assembly Model (2HAM), which contains
positive (attractive) and negative (repulsive) interactions. As a result,
portions of an assembly can become unstable and detach. In this model, we
utilize fuel-efficient computation to perform Turing machine simulations for
the construction of the shape. In this paper, we show how an arbitrary shape
can be constructed using an asymptotically optimal number of distinct tile
types (based on the shape's Kolmogorov complexity). We achieve this at $O(1)$
scale factor in this straightforward model, whereas all previous results with
sublinear scale factors utilize powerful self-assembly models containing
features such as staging, tile deletion, chemical reaction networks, and tile
activation/deactivation. Furthermore, the computation and construction in our
result only creates constant-size garbage assemblies as a byproduct of
assembling the shape."
"Zigzag persistent homology is a powerful generalisation of persistent
homology that allows one not only to compute persistence diagrams with less
noise and using less memory, but also to use persistence in new fields of
application. However, due to the increase in complexity of the algebraic
treatment of the theory, most algorithmic results in the field have remained of
theoretical nature.
  This article describes an efficient algorithm to compute zigzag persistence,
emphasising on its practical interest. The algorithm is a zigzag persistent
cohomology algorithm, based on the dualisation of reflections and
transpositions transformations within the zigzag sequence.
  We provide an extensive experimental study of the algorithm. We study the
algorithm along two directions. First, we compare its performance with zigzag
persistent homology algorithm and show the interest of cohomology in zigzag
persistence. Second, we illustrate the interest of zigzag persistence in
topological data analysis by comparing it to state of the art methods in the
field, specifically optimised algorithm for standard persistent homology and
sparse filtrations. We compare the memory and time complexities of the
different algorithms, as well as the quality of the output persistence
diagrams."
"We study a problem proposed by Hurtado et al. (2016) motivated by sparse set
visualization. Given $n$ points in the plane, each labeled with one or more
primary colors, a \emph{colored spanning graph} (CSG) is a graph such that for
each primary color, the vertices of that color induce a connected subgraph. The
\textsc{Min-CSG} problem asks for the minimum sum of edge lengths in a colored
spanning graph. We show that the problem is NP-hard for $k$ primary colors when
$k\ge 3$ and provide a $(2-\frac{1}{3+2\varrho})$-approximation algorithm for
$k=3$ that runs in polynomial time, where $\varrho$ is the Steiner ratio.
Further, we give a $O(n)$ time algorithm in the special case that the input
points are collinear and $k$ is constant."
"A graph is 1-planar if it has a drawing where each edge is crossed at most
once. A drawing is RAC (Right Angle Crossing) if the edges cross only at right
angles. The relationships between 1-planar graphs and RAC drawings have been
partially studied in the literature. It is known that there are both 1-planar
graphs that are not straight-line RAC drawable and graphs that have a
straight-line RAC drawing but that are not 1-planar. Also, straight-line RAC
drawings always exist for IC-planar graphs, a subclass of 1-planar graphs. One
of the main questions still open is whether every 1-planar graph has a RAC
drawing with at most one bend per edge. We positively answer this question."
"It is proved that every series-parallel digraph whose maximum vertex-degree
is $\Delta$ admits an upward planar drawing with at most one bend per edge such
that each edge segment has one of $\Delta$ distinct slopes. This is shown to be
worst-case optimal in terms of the number of slopes. Furthermore, our
construction gives rise to drawings with optimal angular resolution
$\frac{\pi}{\Delta}$. A variant of the proof technique is used to show that
(non-directed) reduced series-parallel graphs and flat series-parallel graphs
have a (non-upward) one-bend planar drawing with $\lceil\frac{\Delta}{2}\rceil$
distinct slopes if biconnected, and with $\lceil\frac{\Delta}{2}\rceil+1$
distinct slopes if connected."
"A drawing of a graph $G$ is radial if the vertices of $G$ are placed on
concentric circles $C_1, \ldots, C_k$ with common center $c$, and edges are
drawn radially: every edge intersects every circle centered at $c$ at most
once. $G$ is radial planar if it has a radial embedding, that is, a
crossing-free radial drawing. If the vertices of $G$ are ordered or partitioned
into ordered levels (as they are for leveled graphs), we require that the
assignment of vertices to circles corresponds to the given ordering or
leveling. A pair of edges $e$ and $f$ in a graph is independent if $e$ and $f$
do not share a vertex.
  We show that a graph $G$ is radial planar if $G$ has a radial drawing in
which every two independent edges cross an even number of times; the radial
embedding has the same leveling as the radial drawing. In other words, we
establish the strong Hanani-Tutte theorem for radial planarity. This
characterization yields a very simple algorithm for radial planarity testing."
"In geographic information systems and in the production of digital maps for
small devices with restricted computational resources one often wants to round
coordinates to a rougher grid. This removes unnecessary detail and reduces
space consumption as well as computation time. This process is called snapping
to the grid and has been investigated thoroughly from a computational-geometry
perspective. In this paper we investigate the same problem for given drawings
of planar graphs under the restriction that their combinatorial embedding must
be kept and edges are drawn straight-line. We show that the problem is NP-hard
for several objectives and provide an integer linear programming formulation.
Given a plane graph G and a positive integer w, our ILP can also be used to
draw G straight-line on a grid of width w and minimum height (if possible)."
"A geometric graph is angle-monotone if every pair of vertices has a path
between them that---after some rotation---is $x$- and $y$-monotone.
Angle-monotone graphs are $\sqrt 2$-spanners and they are increasing-chord
graphs. Dehkordi, Frati, and Gudmundsson introduced angle-monotone graphs in
2014 and proved that Gabriel triangulations are angle-monotone graphs. We give
a polynomial time algorithm to recognize angle-monotone geometric graphs. We
prove that every point set has a plane geometric graph that is generalized
angle-monotone---specifically, we prove that the half-$\theta_6$-graph is
generalized angle-monotone. We give a local routing algorithm for Gabriel
triangulations that finds a path from any vertex $s$ to any vertex $t$ whose
length is within $1 + \sqrt 2$ times the Euclidean distance from $s$ to $t$.
Finally, we prove some lower bounds and limits on local routing algorithms on
Gabriel triangulations."
"We initiate the study of 2.5D box visibility representations (2.5D-BR) where
vertices are mapped to 3D boxes having the bottom face in the plane $z=0$ and
edges are unobstructed lines of sight parallel to the $x$- or $y$-axis. We
prove that: $(i)$ Every complete bipartite graph admits a 2.5D-BR; $(ii)$ The
complete graph $K_n$ admits a 2.5D-BR if and only if $n \leq 19$; $(iii)$ Every
graph with pathwidth at most $7$ admits a 2.5D-BR, which can be computed in
linear time. We then turn our attention to 2.5D grid box representations
(2.5D-GBR) which are 2.5D-BRs such that the bottom face of every box is a unit
square at integer coordinates. We show that an $n$-vertex graph that admits a
2.5D-GBR has at most $4n - 6 \sqrt{n}$ edges and this bound is tight. Finally,
we prove that deciding whether a given graph $G$ admits a 2.5D-GBR with a given
footprint is NP-complete. The footprint of a 2.5D-BR $\Gamma$ is the set of
bottom faces of the boxes in $\Gamma$."
"In this paper, we consider the problem of computing the algebraic parametric
equation of the Euclidean 1-center function in $\mathbb{R}^d$, $d \geq 2$, for
a system of $n$ static points and $m$ mobile points having motion defined by
rational parametric functions. We have shown that the corresponding Euclidean
1-center function is a piecewise differentiable function and have derived its
exact parametric algebraic equation. If the positions of the static points and
the rational parametric equations of the motion of the mobile points are given,
we have proposed an algorithm that computes the parametric equation of the
Euclidean 1-center function."
"Let $S$ be a set of $n$ points in general poisition in the plane. Let
$\overline{cr}(S)$ be number of pairs of edges that cross in a rectilinear
drawing of the complete graph with $S$ as its vertex set. Suppose that this
number is known. In this paper we consider the problem computing
$\overline{cr}(S')$, where $S'$ comes from adding, removing or moving a point
from $S$."
"In this paper, we discuss the algorithm engineering aspects of an O(n^2)-time
algorithm [6] for computing a minimum-area convex polygon that intersects a set
of n isothetic line segments."
"In this paper, we study different variations of minimum width color-spanning
annulus problem among a set of points $P=\{p_1,p_2,\ldots,p_n\}$ in $I\!\!R^2$,
where each point is assigned with a color in $\{1, 2, \ldots, k\}$. We present
algorithms for finding a minimum width color-spanning axis parallel square
annulus $(CSSA)$, minimum width color spanning axis parallel rectangular
annulus $(CSRA)$, and minimum width color-spanning equilateral triangular
annulus of fixed orientation $(CSETA)$. The time complexities of computing (i)
a $CSSA$ is $O(n^3+n^2k\log k)$ which is an improvement by a factor $n$ over
the existing result on this problem, (ii) that for a $CSRA$ is $O(n^4\log n)$,
and for (iii) a $CSETA$ is $O(n^3k)$. The space complexity of all the
algorithms is $O(k)$."
"Geographic routing is a routing paradigm, which uses geographic coordinates
of network nodes to determine routes. Greedy routing, the simplest form of
geographic routing forwards a packet to the closest neighbor towards the
destination. A greedy embedding is a embedding of a graph on a geometric space
such that greedy routing always guarantees delivery. A Schnyder drawing is a
classical way to draw a planar graph. In this manuscript, we show that every
Schnyder drawing is a greedy embedding, based on a generalized definition of
greedy routing."
"We show that clustered planarity with overlapping clusters as introduced by
Didimo et al. can be solved in polynomial time if each cluster induces a
connected subgraph. It can be solved in linear time if the set of clusters is
the union of two partitions of the vertex set such that, for each cluster, both
the cluster and its complement, induce connected subgraphs. Clustered planarity
with overlapping clusters is NP-complete, even if restricted to instances where
the underlying graph is 2-connected, the set of clusters is the union of two
partitions and each cluster contains at most two connected components while
their complements contain at most three connected components."
"The metric sketching problem is defined as follows. Given a metric on $n$
points, and $\epsilon>0$, we wish to produce a small size data structure
(sketch) that, given any pair of point indices, recovers the distance between
the points up to a $1+\epsilon$ distortion. In this paper we consider metrics
induced by $\ell_2$ and $\ell_1$ norms whose spread (the ratio of the diameter
to the closest pair distance) is bounded by $\Phi>0$. A well-known
dimensionality reduction theorem due to Johnson and Lindenstrauss yields a
sketch of size $O(\epsilon^{-2} \log (\Phi n) n\log n)$, i.e., $O(\epsilon^{-2}
\log (\Phi n) \log n)$ bits per point. We show that this bound is not optimal,
and can be substantially improved to $O(\epsilon^{-2}\log(1/\epsilon) \cdot
\log n + \log\log \Phi)$ bits per point. Furthermore, we show that our bound is
tight up to a factor of $\log(1/\epsilon)$.
  We also consider sketching of general metrics and provide a sketch of size
$O(n\log(1/\epsilon)+ \log\log \Phi)$ bits per point, which we show is optimal."
"Let $H$ be a set of $n$ halfplanes in $\mathbb{R}^2$ in general position, and
let $k<n$ be a given parameter. We show that the number of vertices of the
arrangement of $H$ that lie at depth exactly $k$ (i.e., that are contained in
the interiors of exactly $k$ halfplanes of $H$) is $O(nk^{1/3} +
n^{2/3}k^{4/3})$. The bound is tight when $k=\Theta(n)$. This generalizes the
study of Dey [Dey98], concerning the complexity of a single level in an
arrangement of lines, and coincides with it for $k=O(n^{1/3})$."
"This paper considers 1-string representations of planar graphs that are
order-preserving in the sense that the order of crossings along the curve
representing vertex $v$ is the same as the order of edges in the clockwise
order around $v$ in the planar embedding. We show that this does not exist for
all planar graphs (not even for all planar 3-trees), but show existence for
some subclasses of planar partial 3-trees. In particular, for outer-planar
graphs it can be order-preserving and outer-string in the sense that all ends
of strings are on the outside of the representation."
"In the Sparse Linear Regression (SLR) problem, given a $d \times n$ matrix
$M$ and a $d$-dimensional vector $q$, we want to compute a $k$-sparse vector
$\tau$ such that the error $||M \tau-q||$ is minimized. In this paper, we
present algorithms and conditional lower bounds for several variants of this
problem. In particular, we consider (i) the Affine SLR where we add the
constraint that $\sum_i \tau_i=1$ and (ii) the Convex SLR where we further add
the constraint that $\tau \geq 0$. Furthermore, we consider (i) the batched
(offline) setting, where the matrix $M$ and the vector $q$ are given as inputs
in advance, and (ii) the query(online) setting, where an algorithm preprocesses
the matrix $M$ to quickly answer such queries. All of the aforementioned
variants have been well-studied and have many applications in statistics,
machine learning and sparse recovery.
  We consider the approximate variants of these problems in the ""low sparsity
regime"" where the value of the sparsity bound $k$ is low. In particular, we
show that the online variant of all three problems can be solved with query
time $\tilde O(n^{k-1})$. This provides non-trivial improvements over the naive
algorithm that exhaustively searches all ${ n \choose k}$ subsets $B$. We also
show that solving the offline variant of all three problems, would require an
exponential dependence of the form $\tilde \Omega(n^{k/2}/e^{k})$, under a
natural complexity-theoretic conjecture. Improving this lower bound for the
case of $k=4$ would imply a nontrivial lower bound for the famous Hopcroft's
problem. Moreover, solving the offline variant of affine SLR in $o(n^{k-1})$
would imply an upper bound of $o(n^d)$ for the problem of testing whether a
given set of $n$ points in a $d$-dimensional space is degenerate. However, this
is conjectured to require $\Omega(n^d)$ time."
"Hilbert's two-dimensional space-filling curve is appreciated for its good
locality-preserving properties and easy implementation for many applications.
However, Hilbert did not describe how to generalize his construction to higher
dimensions. In fact, the number of ways in which this may be done ranges from
zero to infinite, depending on what properties of the Hilbert curve one
considers to be essential.
  In this work we take the point of view that a Hilbert curve should at least
be self-similar and traverse cubes octant by octant. We organize and explore
the space of possible three-dimensional Hilbert curves and the potentially
useful properties which they may have. We discuss a notation system that allows
us to distinguish the curves from one another and enumerate them. This system
has been implemented in a software prototype, available from the author's
website. Several examples of possible three-dimensional Hilbert curves are
presented, including a curve that visits the points on most sides of the unit
cube in the order of the two-dimensional Hilbert curve; curves of which not
only the eight octants are similar to each other, but also the four quarters; a
curve with excellent locality-preserving properties and endpoints that are not
vertices of the cube; a curve in which all but two octants are each other's
images with respect to reflections in axis-parallel planes; and curves that can
be sketched on a grid without using vertical line segments. In addition, we
discuss several four-dimensional Hilbert curves."
"Given $n$ line segments in the plane, do they form the edge set of a
\emph{weakly simple polygon}; that is, can the segment endpoints be perturbed
by at most $\varepsilon$, for any $\varepsilon>0$, to obtain a simple polygon?
While the analogous question for \emph{simple polygons} can easily be answered
in $O(n\log n)$ time, we show that it is NP-complete for weakly simple
polygons. We give $O(n)$-time algorithms in two special cases: when all
segments are collinear, or the segment endpoints are in general position. These
results extend to the variant in which the segments are \emph{directed}, and
the counterclockwise traversal of a polygon should follow the orientation.
  We study related problems for the case that the union of the $n$ input
segments is connected. (i) If each segment can be subdivided into several
segments, find the minimum number of subdivision points to form a weakly simple
polygon. (ii) If new line segments can be added, find the minimum total length
of new segments that creates a weakly simple polygon. We give worst-case upper
and lower bounds for both problems."
"In this paper we show that it can be decided in polynomial time whether or
not the visibility graph of a given point set is 4-colourable, and such a
4-colouring, if it exists, can also be constructed in polynomial time. We show
that the problem of deciding whether the visibility graph of a point set is
5-colourable, is NP-complete. We give an example of a point visibility graph
that has chromatic number 6 while its clique number is only 4."
"We consider the problem of finding a small hitting set in an {\it infinite}
range space $\cF=(Q,\cR)$ of bounded VC-dimension. We show that, under
reasonably general assumptions, the infinite dimensional convex relaxation can
be solved (approximately) efficiently by multiplicative weight updates. As a
consequence, we get an algorithm that finds, for any $\delta>0$, a set of size
$O(s_{\cF}(z^*_\cF))$ that hits $(1-\delta)$-fraction of $\cR$ (with respect to
a given measure) in time proportional to $\log(\frac{1}{\delta})$, where
$s_{\cF}(\frac{1}{\epsilon})$ is the size of the smallest $\epsilon$-net the
range space admits, and $z^*_{\cF}$ is the value of the {\it fractional}
optimal solution. This {\it exponentially} improves upon previous results which
achieve the same approximation guarantees with running time proportional to
$\poly(\frac{1}{\delta})$. Our assumptions hold, for instance, in the case when
the range space represents the {\it visibility} regions of a polygon in
$\RR^2$, giving thus a deterministic polynomial time $O(\log
z^*_{\cF})$-approximation algorithm for guarding $(1-\delta)$-fraction of the
area of any given simple polygon, with running time proportional to
$\polylog(\frac{1}{\delta})$."
"A geometric graph is a graph whose vertex set is a set of points in the plane
and whose edge set contains straight-line segments. A matching in a graph is a
subset of edges of the graph with no shared vertices. A matching is called
perfect if it matches all the vertices of the underling graph. A geometric
matching is a matching in a geometric graph. In this thesis, we study matching
problems in various geometric graphs. Among the family of geometric graphs we
look at complete graphs, complete bipartite graphs, complete multipartite
graphs, Delaunay graphs, Gabriel graphs, and $\Theta$-graphs. The classical
matching problem is to find a matching of maximum size in a given graph. We
study this problem as well as some of its variants on geometric graphs. The
bottleneck matching problem is to find a maximum matching that minimizes the
length of the longest edge. The plane matching problem is to find a maximum
matching so that the edges in the matching are pairwise non-crossing. A
geometric matching is strong with respect to a given shape $S$ if we can assign
to each edge in the matching a scaled version of $S$ such that the shapes
representing the edges are pairwise disjoint. The strong matching problem is to
find a maximum strong matching with respect to a given shape. The matching
packing problem is to pack as many edge-disjoint perfect matchings as possible
into a geometric graph. We study these problems and establish lower and upper
bounds on the size of different kinds of matchings in various geometric graphs.
We also present algorithms for computing such matchings. Some of the presented
bounds are tight, while the others need to be sharpened."
"A drawing in the plane ($\mathbb{R}^2$) of a graph $G=(V,E)$ equipped with a
function $\gamma: V \rightarrow \mathbb{N}$ is \emph{$x$-bounded} if (i) $x(u)
<x(v)$ whenever $\gamma(u)<\gamma(v)$ and (ii) $\gamma(u)\leq\gamma(w)\leq
\gamma(v)$, where $uv\in E$ and $\gamma(u)\leq \gamma(v)$, whenever $x(w)\in
x(uv)$, where $x(.)$ denotes the projection to the $x$-axis. We prove a
characterization of isotopy classes of graph embeddings in the plane containing
an $x$-bounded embedding. Then we present an efficient algorithm, that relies
on our result, for testing the existence of an $x$-bounded embedding if the
given graph is a tree or generalized $\Theta$-graph. This partially answers a
question raised recently by Angelini et al. and Chang et al., and proves that
c-planarity testing of flat clustered graphs with three clusters is tractable
if each connected component of the underlying abstract graph is a tree."
"In this paper, we study the problem of computing the diameter of a set of $n$
points in $d$-dimensional Euclidean space for a fixed dimension $d$, and
propose a new $(1+\varepsilon)$-approximation algorithm with $O(n+
1/\varepsilon^{d-2})$ time and $O(n)$ space, where $0 < \varepsilon\leqslant
1$. We also show that the proposed algorithm can be modified to a
$(1+O(\varepsilon))$-approximation algorithm with $O(n+
1/\varepsilon^{\frac{2d}{3}-\frac{1}{2}})$ running time. These results improve
the required time to solve this problem in comparison with previous algorithms."
"We show that every orthogonal polyhedron of genus at most 2 can be unfolded
without overlap while using only a linear number of orthogonal cuts (parallel
to the polyhedron edges). This is the first result on unfolding general
orthogonal polyhedra beyond genus-0. Our unfolding algorithm relies on the
existence of at most 2 special leaves in what we call the ""unfolding tree""
(which ties back to the genus), so unfolding polyhedra of genus 3 and beyond
requires new techniques."
"Let $R$ and $B$ be two disjoint sets of points in the plane where the points
of $R$ are colored red and the points of $B$ are colored blue, and let
$n=|R\cup B|$. A bichromatic spanning tree is a spanning tree in the complete
bipartite geometric graph with bipartition $(R,B)$. The minimum (respectively
maximum) bichromatic spanning tree problem is the problem of computing a
bichromatic spanning tree of minimum (respectively maximum) total edge length.
  1. We present a simple algorithm that solves the minimum bichromatic spanning
tree problem in $O(n\log^3 n)$ time. This algorithm can easily be extended to
solve the maximum bichromatic spanning tree problem within the same time bound.
It also can easily be generalized to multicolored point sets.
  2. We present $\Theta(n\log n)$-time algorithms that solve the minimum and
the maximum bichromatic spanning tree problems.
  3. We extend the bichromatic spanning tree algorithms and solve the
multicolored version of these problems in $O(n\log n\log k)$ time, where $k$ is
the number of different colors (or the size of the multipartition in a complete
multipartite geometric graph)."
"This paper addresses the equal circle packing problem, and proposes an
efficient Quasi-physical Quasi-human (QPQH) algorithm. QPQH is based on a
modified Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm which we call the
local BFGS and a new basin hopping strategy based on a Chinese idiom: alternate
tension with relaxation. Starting from a random initial layout, we apply the
local BFGS algorithm to reach a local minimum layout. The local BFGS algorithm
fully utilizes the neighborhood information of each circle to considerably
speed up the running time of the gradient descent process, and the efficiency
is very apparent for large scale instances. When yielding a local minimum
layout, the new basin-hopping strategy is to shrink the container size to
different extent to generate several new layouts. Experimental results indicate
that the new basin-hopping strategy is very efficient, especially for a type of
layout with comparatively dense packing in the center and comparatively sparse
packing around the boundary of the container. We test QPQH on the instances of
n = 1,2,...,320, and obtain 66 new layouts which have smaller container sizes
than the current best-known results reported in literature."
"We show that every triangulation (maximal planar graph) on $n\ge 6$ vertices
can be flipped into a Hamiltonian triangulation using a sequence of less than
$n/2$ combinatorial edge flips. The previously best upper bound uses
$4$-connectivity as a means to establish Hamiltonicity. But in general about
$3n/5$ flips are necessary to reach a $4$-connected triangulation. Our result
improves the upper bound on the diameter of the flip graph of combinatorial
triangulations on $n$ vertices from $5.2n-33.6$ to $5n-23$. We also show that
for every triangulation on $n$ vertices there is a simultaneous flip of less
than $2n/3$ edges to a $4$-connected triangulation. The bound on the number of
edges is tight, up to an additive constant. As another application we show that
every planar graph on $n$ vertices admits an arc diagram with less than $n/2$
biarcs, that is, after subdividing less than $n/2$ (of potentially $3n-6$)
edges the resulting graph admits a $2$-page book embedding."
"Maximum independent set from a given set $D$ of unit disks intersecting a
horizontal line can be solved in $O(n^2)$ time and $O(n^2)$ space. As a
corollary, we design a factor 2 approximation algorithm for the maximum
independent set problem on unit disk graph which takes both time and space of
$O(n^2)$. The best known factor 2 approximation algorithm for this problem runs
in $O(n^2 \log n)$ time and takes $O(n^2)$ space [Jallu and Das 2016, Das et
al. 2016]."
"We address the question if a point inside a domain bounded by a simple closed
arc spline is circularly visible from a specified arc from the boundary. We
provide a simple and numerically stable linear time algorithm that solves this
problem. In particular, we present an easy to check criterion that implies that
a point is not visible from a specified boundary arc."
"Averaging diffeomorphisms is a challenging problem, and it has great
applications in areas like medical image atlases. The simple Euclidean average
can neither guarantee the averaged transformation is a diffeomorphism, nor get
reasonable result when there is a local rotation. The goal of this paper is to
propose a new approach to averaging diffeomorphisms based on the Jacobian
determinant and the curl vector of the diffeomorphisms. Instead of averaging
the diffeomorphisms directly, we average the Jacobian determinants and the curl
vectors, and then construct a diffeomorphism based on the averaged Jacobian
determinant and averaged curl vector as the average of diffeomorphisms.
Numerical examples with convincible results are presented to demonstrate the
method."
"We propose a space-efficient algorithm for hidden surface removal that
combines one of the fastest previous algorithms for that problem with
techniques based on bit manipulation. Such techniques had been successfully
used in other settings, for example to reduce working space for several graph
algorithms. However, bit manipulation is not usually employed in geometric
algorithms because the standard model of computation (the real RAM) does not
support it. For this reason, we first revisit our model of computation to have
a reasonable theoretical framework. Under this framework we show how the use of
a bit representation for the union of triangles, in combination with
rank-select data structures, allows us to implicitly compute the union of $n$
triangles with roughly $O(1)$ bits per union boundary vertex. This results in
an algorithm that uses at most as much space as the previous one, and depending
on the input, can give a reduction of up to a factor $\Theta(\log n)$, while
maintaining the running time."
"We study when an arrangement of axis-aligned rectangles can be transformed
into an arrangement of axis-aligned squares in $\mathbb{R}^2$ while preserving
its structure. We found a counterexample to the conjecture of J. Klawitter, M.
N\""ollenburg and T. Ueckerdt whether all arrangements without crossing and
side-piercing can be squared. Our counterexample also works in a more general
case when we only need to preserve the intersection graph and we forbid
side-piercing between squares. We also show counterexamples for transforming
box arrangements into combinatorially equivalent hypercube arrangements.
Finally, we introduce a linear program deciding whether an arrangement of
rectangles can be squared in a more restrictive version where the order of all
sides is preserved."
"Multidimensional-scaling (MDS) is a dimensionality reduction tool used for
information analysis, data visualization and manifold learning. Most MDS
procedures find embedding of data points in low dimensional Euclidean (flat)
domains, such that distances between the points are as close as possible to
given inter-points dissimilarities. We present an efficient solver for
Classical Scaling, a specific MDS model, by extrapolating the information
provided by distances measured from a subset of the points to the rest. The
computational and space complexities of the new MDS procedures are be thereby
reduced from quadratic to quasi-linear in the number of data points.
Incorporating both local and global information about the data allows us to
construct a low rank approximation to the inter-geodesic distances between the
data points. As a by-product, the proposed method allows for efficient
computation of geodesic distances. Finally, we show how to apply our method to
two geometric analysis applications and obtain state of the art results."
"The Gromov-Hausdorff (GH) distance is traditionally used for measuring
distances between metric spaces. It is defined as the minimal distortion of
embedding one surface into the other, while the optimal correspondence can be
described as the map that minimizes this distortion. Solving such a
minimization is a hard combinatorial problem that requires pre-computation and
storing of all pairwise geodesic distances for the matched surfaces. A popular
way for compact representation of functions on surfaces is by projecting them
into the leading eigenfunctions of the Laplace-Beltrami Operator (LBO). When
truncated, The basis of the LBO is known to be the optimal for representing
functions with bounded gradient in a min-max sense. Methods such as
Spectral-GMDS exploit this idea to simplify and efficiently approximate a
minimization related to the GH distance by operating in the truncated spectral
domain, and obtain state of the art results for matching of nearly isometric
shapes. However, when considering only a specific set of functions on the
surface, such as geodesic distances, an optimized basis could be considered as
an even better alternative. Here, we define the geodesic distance basis, which
is optimal for compact approximation of geodesic distances, in terms of
Frobenius norm. We use the suggested basis to extract the Geodesic Distance
Descriptor (GDD), which encodes the geodesic distances information as a linear
combination of the basis functions. We then show how these ideas can be used to
efficiently and accurately approximate the metric spaces matching problem with
almost no loss of information. These observations are used to construct a very
simple and efficient procedure for shape correspondence. Experimental results
show that the GDD improves both accuracy and efficiency of state of the art
shape matching procedures."
"A unit disk graph is the intersection graph of a set of unit diameter disks
in the plane. In this paper we consider liar's domination problem on unit disk
graphs, a variant of dominating set problem. We call this problem as {\it
Euclidean liar's domination problem}. In the Euclidean liar's domination
problem, a set ${\cal P}=\{p_1,p_2,\ldots,p_n\}$ of $n$ points (disk centers)
are given in the Euclidean plane. For $p \in {\cal P}$, $N[p]$ is a subset of
${\cal P}$ such that for any $q \in N[p]$, the Euclidean distance between $p$
and $q$ is less than or equal to 1, i.e., the corresponding unit diameter disks
intersect. The objective of the Euclidean liar's domination problem is to find
a subset $D\; (\subseteq {\cal P})$ of minimum size having the following
properties : (i) $|N[p_i] \cap D| \geq 2$ for $1 \leq i \leq n$, and (ii)
$|(N[p_i] \cup N[p_j]) \cap D| \geq 3$ for $i\neq j, 1\leq i,j \leq n$. This
article aims to prove the Euclidean liar's domination problem is NP-complete."
"We provide a spectrum of results for the Universal Guard Problem, in which
one is to obtain a small set of points (""guards"") that are ""universal"" in their
ability to guard any of a set of possible polygonal domains in the plane. We
give upper and lower bounds on the number of universal guards that are always
sufficient to guard all polygons having a given set of n vertices, or to guard
all polygons in a given set of k polygons on an n-point vertex set. Our upper
bound proofs include algorithms to construct universal guard sets of the
respective cardinalities."
"We consider a problem of dispersing points on disjoint intervals on a line.
Given n pairwise disjoint intervals sorted on a line, we want to find a point
in each interval such that the minimum pairwise distance of these points is
maximized. Based on a greedy strategy, we present a linear time algorithm for
the problem. Further, we also solve in linear time the cycle version of the
problem where the intervals are given on a cycle."
"An efficient algorithm to enumerate the vertices of a two-dimensional (2D)
projection of a polytope, is presented in this paper. The proposed algorithm
uses the support function of the polytope to be projected and enumerated for
vertices. The complexity of our algorithm is linear in the number of vertices
of the projected polytope and we show empirically that the performance is
significantly better in comparison to some known efficient algorithms of
projection and enumeration."
"This paper applies the randomized incremental construction (RIC) framework to
computing the Hausdorff Voronoi diagram of a family of k clusters of points in
the plane. The total number of points is n. The diagram is a generalization of
Voronoi diagrams based on the Hausdorff distance function. The combinatorial
complexity of the Hausdorff Voronoi diagram is O(n+m), where m is the total
number of crossings between pairs of clusters. For non-crossing clusters (m=0),
our algorithm works in expected O(n log n + k log n log k) time and
deterministic
  O(n) space. For arbitrary clusters (m=O(n^2)), the algorithm runs in expected
O((m+n log k) log n) time and O(m +n log k) space. When clusters cross,
bisectors are disconnected curves resulting in disconnected Voronoi regions
that challenge the incremental construction. This paper applies the RIC
paradigm to a Voronoi diagram with disconnected regions and disconnected
bisectors, for the first time."
"We augment a tree $T$ with a shortcut $pq$ to minimize the largest distance
between any two points along the resulting augmented tree $T+pq$. We study this
problem in a continuous and geometric setting where $T$ is a geometric tree in
the Euclidean plane, where a shortcut is a line segment connecting any two
points along the edges of $T$, and we consider all points on $T+pq$ (i.e.,
vertices and points along edges) when determining the largest distance along
$T+pq$. We refer to the largest distance between any two points along edges as
the continuous diameter to distinguish it from the discrete diameter, i.e., the
largest distance between any two vertices.
  We establish that a single shortcut is sufficient to reduce the continuous
diameter of a geometric tree $T$ if and only if the intersection of all
diametral paths of $T$ is neither a line segment nor a single point. We
determine an optimal shortcut for a geometric tree with $n$ straight-line edges
in $O(n \log n)$ time. Apart from the running time, our results extend to
geometric trees whose edges are rectifiable curves. The algorithm for trees
generalizes our algorithm for paths."
"In the polytope membership problem, a convex polytope $K$ in $R^d$ is given,
and the objective is to preprocess $K$ into a data structure so that, given a
query point $q \in R^d$, it is possible to determine efficiently whether $q \in
K$. We consider this problem in an approximate setting and assume that $d$ is a
constant. Given an approximation parameter $\varepsilon > 0$, the query can be
answered either way if the distance from $q$ to $K$'s boundary is at most
$\varepsilon$ times $K$'s diameter. Previous solutions to the problem were on
the form of a space-time trade-off, where logarithmic query time demands
$O(1/\varepsilon^{d-1})$ storage, whereas storage $O(1/\varepsilon^{(d-1)/2})$
admits roughly $O(1/\varepsilon^{(d-1)/8})$ query time. In this paper, we
present a data structure that achieves logarithmic query time with storage of
only $O(1/\varepsilon^{(d-1)/2})$, which matches the worst-case lower bound on
the complexity of any $\varepsilon$-approximating polytope. Our data structure
is based on a new technique, a hierarchy of ellipsoids defined as
approximations to Macbeath regions.
  As an application, we obtain major improvements to approximate Euclidean
nearest neighbor searching. Notably, the storage needed to answer
$\varepsilon$-approximate nearest neighbor queries for a set of $n$ points in
$O(\log \frac{n}{\varepsilon})$ time is reduced to $O(n/\varepsilon^{d/2})$.
This halves the exponent in the $\varepsilon$-dependency of the existing space
bound of roughly $O(n/\varepsilon^d)$, which has stood for 15 years (Har-Peled,
2001)."
"Let $P$ be a set of $n$ points in the plane. We show how to find, for a given
integer $k>0$, the smallest-area axis-parallel rectangle that covers $k$ points
of $P$ in $O(nk^2 \log n+ n\log^2 n)$ time. We also consider the problem of,
given a value $\alpha>0$, covering as many points of $P$ as possible with an
axis-parallel rectangle of area at most $\alpha$. For this problem we give a
randomized $(1-\varepsilon)$-approximation that works in near-linear time: in
$O((n/\varepsilon^4)\log^3 n \log (1/\varepsilon))$ time we find an
axis-parallel rectangle of area at most $\alpha$ that covers at least
$(1-\varepsilon)\kappa^*$ points, where $\kappa^*$ is the maximum possible
number of points that could be covered."
"We consider the problem of digitalizing Euclidean line segments from
$\mathbb{R}^d$ to $\mathbb{Z}^d$. Christ {\em et al.} (DCG, 2012) showed how to
construct a set of {\em consistent digital segment} (CDS) for $d=2$: a
collection of segments connecting any two points in $\mathbb{Z}^2$ that
satisfies the natural extension of the Euclidean axioms to $\mathbb{Z}^d$. In
this paper we study the construction of CDSs in higher dimensions.
  We show that any total order can be used to create a set of {\em consistent
digital rays} CDR in $\mathbb{Z}^d$ (a set of rays emanating from a fixed point
$p$ that satisfies the extension of the Euclidean axioms). We fully
characterize for which total orders the construction holds and study their
Hausdorff distance, which in particular positively answers the question posed
by Christ {\em et al.}."
"We introduce the dune-curvilineargrid module. The module provides the
self-contained, parallel grid manager, as well as the underlying elementary
curvilinear geometry module dune-curvilineargeometry. This work is motivated by
the need for reliable and scalable electromagnetic design of nanooptical
devices. Curvilinear geometries improve both the accuracy of modeling smooth
material boundaries, and the h/p-convergence rate of PDE solutions, reducing
the necessary computational effort. dune-curvilineargrid provides a large
spectrum of features for scalable parallel implementations of Finite Element
and Boundary Integral methods over curvilinear tetrahedral geometries,
including symbolic polynomial mappings and operations, recursive integration,
sparse and dense grid communication, parallel timing and memory footprint
diagnostics utilities. It is written in templated C++ using MPI for
parallelization and ParMETIS for grid partitioning, and is provided as a module
for the DUNE interface. The dune-curvilineargrid grid manager is continuously
developed and improved, and so is this documentation. For the most recent
version of the documentation, as well as the source code, please refer to the
provided repositories and our website."
"The crossing number is the smallest number of pairwise edge-crossings when
drawing a graph into the plane. There are only very few graph classes for which
the exact crossing number is known or for which there at least exist constant
approximation ratios. Furthermore, up to now, general crossing number
computations have never been successfully tackled using bounded width of graph
decompositions, like treewidth or pathwidth.
  In this paper, we for the first time show that crossing number is tractable
(even in linear time) for maximal graphs of bounded pathwidth~3. The technique
also shows that the crossing number and the rectilinear (a.k.a. straight-line)
crossing number are identical for this graph class, and that we require only an
$O(n)\times O(n)$-grid to achieve such a drawing.
  Our techniques can further be extended to devise a 2-approximation for
general graphs with pathwidth 3, and a $4w^3$-approximation for maximal graphs
of pathwidth $w$. This is a constant approximation for bounded pathwidth
graphs."
"We consider edge insertion and deletion operations that increase the
connectivity of a given planar straight-line graph (PSLG), while minimizing the
total edge length of the output. We show that every connected PSLG $G=(V,E)$ in
general position can be augmented to a 2-connected PSLG $(V,E\cup E^+)$ by
adding new edges of total Euclidean length $\|E^+\|\leq 2\|E\|$, and this bound
is the best possible. An optimal edge set $E^+$ can be computed in $O(|V|^4)$
time; however the problem becomes NP-hard when $G$ is disconnected. Further,
there is a sequence of edge insertions and deletions that transforms a
connected PSLG $G=(V,E)$ into a planar straight-line cycle $G'=(V,E')$ such
that $\|E'\|\leq 2\|{\rm MST}(V)\|$, and the graph remains connected with edge
length below $\|E\|+\|{\rm MST}(V)\|$ at all stages. These bounds are the best
possible."
"Let $T$ be a tree space (or tree network) represented by a weighted tree with
$t$ vertices, and $S$ be a set of $n$ stochastic points in $T$, each of which
has a fixed location with an independent existence probability. We investigate
two fundamental problems under such a stochastic setting, the closest-pair
problem and the nearest-neighbor search. For the former, we study the
computation of the $\ell$-threshold probability and the expectation of the
closest-pair distance of a realization of $S$. We propose the first algorithm
to compute the $\ell$-threshold probability in $O(t+n\log n+ \min\{tn,n^2\})$
time for any given threshold $\ell$, which immediately results in an
$O(t+\min\{tn^3,n^4\})$-time algorithm for computing the expected closest-pair
distance. Based on this, we further show that one can compute a
$(1+\varepsilon)$-approximation for the expected closest-pair distance in
$O(t+\varepsilon^{-1}\min\{tn^2,n^3\})$ time, by arguing that the expected
closest-pair distance can be approximated via $O(\varepsilon^{-1}n)$ threshold
probability queries. For the latter, we study the $k$ most-likely
nearest-neighbor search ($k$-LNN) via a notion called $k$ most-likely Voronoi
Diagram ($k$-LVD). We show that the size of the $k$-LVD $\varPsi_T^S$ of $S$ on
$T$ is bounded by $O(kn)$ if the existence probabilities of the points in $S$
are constant-far from 0. Furthermore, we establish an $O(kn)$ average-case
upper bound for the size of $\varPsi_T^S$, by regarding the existence
probabilities as i.i.d. random variables drawn from some fixed distribution.
Our results imply the existence of an LVD data structure which answers $k$-LNN
queries in $O(\log n+k)$ time using average-case $O(t+k^2n)$ space, and
worst-case $O(t+kn^2)$ space if the existence probabilities are constant-far
from 0. Finally, we also give an $O(t+ n^2\log n+n^2k)$-time algorithm to
construct the LVD data structure."
"We consider the problem of planning a collision-free path of a robot in the
presence of risk zones. The robot is allowed to travel in these zones but is
penalized in a super-linear fashion for consecutive accumulative time spent
there. We recently suggested a natural cost function that balances path length
and risk-exposure time. When no risk zones exists, our problem resorts to
computing minimal-length paths which is known to be computationally hard in the
number of dimensions. It is well known that in two-dimensions computing
minimal-length paths can be done efficiently. Thus, a natural question we pose
is ""Is our problem computationally hard or not?"" If the problem is hard, we
wish to find an approximation algorithm to compute a near-optimal path. If not,
then a polynomial-time algorithm should be found."
"In this paper, we study the dominance relation under a stochastic setting.
Let $\mathcal{S}$ be a set of $n$ colored stochastic points in $\mathbb{R}^d$,
each of which is associated with an existence probability. We investigate the
problem of computing the probability that a realization of $\mathcal{S}$
contains inter-color dominances, which we call the \textit{colored stochastic
dominance} (CSD) problem. We propose the first algorithm to solve the CSD
problem for $d=2$ in $O(n^2 \log^2 n)$ time. On the other hand, we prove that,
for $d \geq 3$, even the CSD problem with a restricted color pattern is
#P-hard. In addition, even if the existence probabilities are restricted to be
$\frac{1}{2}$, the problem remains #P-hard for $d \geq 7$. A simple FPRAS is
then provided to approximate the desired probability in any dimension. We also
study a variant of the CSD problem in which the dominance relation is
considered with respect to not only the standard basis but any orthogonal basis
of $\mathbb{R}^d$. Specifically, this variant, which we call the {\em
free-basis colored stochastic dominance} (FBCSD) problem, considers the
probability that a realization of $\mathcal{S}$ contains inter-color dominances
with respect to any orthogonal basis of $\mathbb{R}^d$. We show that the CSD
problem is polynomial-time reducible to the FBCSD problem in the same
dimension, which proves the #P-hardness of the latter for $d \geq 3$.
Conversely, we reduce the FBCSD problem in $\mathbb{R}^2$ to the CSD problem in
$\mathbb{R}^2$, by which an $O(n^4 \log^2 n)$ time algorithm for the former is
obtained."
"The $c$-approximate Near Neighbor problem in high dimensional spaces has been
mainly addressed by Locality Sensitive Hashing (LSH), which offers polynomial
dependence on the dimension, query time sublinear in the size of the dataset,
and subquadratic space requirement. For practical applications, linear space is
typically imperative. Most previous work in the linear space regime focuses on
the case that $c$ exceeds $1$ by a constant term. In a recently accepted paper,
optimal bounds have been achieved for any $c>1$ \cite{ALRW17}.
  Towards practicality, we present a new and simple data structure using linear
space and sublinear query time for any $c>1$ including $c\to 1^+$. Given an LSH
family of functions for some metric space, we randomly project points to the
Hamming cube of dimension $\log n$, where $n$ is the number of input points.
The projected space contains strings which serve as keys for buckets containing
the input points. The query algorithm simply projects the query point, then
examines points which are assigned to the same or nearby vertices on the
Hamming cube. We analyze in detail the query time for some standard LSH
families.
  To illustrate our claim of practicality, we offer an open-source
implementation in {\tt C++}, and report on several experiments in dimension up
to 1000 and $n$ up to $10^6$. Our algorithm is one to two orders of magnitude
faster than brute force search. Experiments confirm the sublinear dependence on
$n$ and the linear dependence on the dimension. We have compared against
state-of-the-art LSH-based library {\tt FALCONN}: our search is somewhat
slower, but memory usage and preprocessing time are significantly smaller."
"A graph drawing is $\textit{greedy}$ if, for every ordered pair of vertices
$(x,y)$, there is a path from $x$ to $y$ such that the Euclidean distance to
$y$ decreases monotonically at every vertex of the path. Greedy drawings
support a simple geometric routing scheme, in which any node that has to send a
packet to a destination ""greedily"" forwards the packet to any neighbor that is
closer to the destination than itself, according to the Euclidean distance in
the drawing. In a greedy drawing such a neighbor always exists and hence this
routing scheme is guaranteed to succeed.
  In 2004 Papadimitriou and Ratajczak stated two conjectures related to greedy
drawings. The $\textit{greedy embedding conjecture}$ states that every
$3$-connected planar graph admits a greedy drawing. The $\textit{convex greedy
embedding conjecture}$ asserts that every $3$-connected planar graph admits a
planar greedy drawing in which the faces are delimited by convex polygons. In
2008 the greedy embedding conjecture was settled in the positive by Leighton
and Moitra.
  In this paper we prove that every $3$-connected planar graph admits a
$\textit{planar}$ greedy drawing. Apart from being a strengthening of Leighton
and Moitra's result, this theorem constitutes a natural intermediate step
towards a proof of the convex greedy embedding conjecture."
"More than 25 years ago Chazelle~\emph{et al.} (FOCS 1991) studied the
following question: Is it possible to cut any set of $n$ lines in ${\Bbb R}^3$
into a subquadratic number of fragments such that the resulting fragments admit
a depth order? They proved an $O(n^{9/4})$ bound for the very special case of
bipartite weavings of lines. Since then only little progress was made, until a
recent breakthrough by Aronov and Sharir (STOC 2016) who showed that
$O(n^{3/2}\mathrm{polylog}\; n)$ fragments suffice for any set of lines. In a
follow-up paper Aronov, Miller and Sharir (SODA 2017) proved an
$O(n^{3/2+\varepsilon})$ bound for triangles, but their method results in
pieces with curved boundaries. Moreover, their method uses polynomial
partitions, for which currently no algorithm is known. Thus the most natural
version of the problem is still wide open: Can we cut any collection of $n$
disjoint triangles in ${\Bbb R}^3$ into a subquadratic number of triangular
fragments that admit a depth order? And if so, can we compute the cuts
efficiently?
  We answer this question by presenting an algorithm that cuts any set of $n$
disjoint triangles in ${\Bbb R}^3$ into $O(n^{7/4}\mathrm{polylog}\; n)$
triangular fragments that admit a depth order. The running time of our
algorithm is $O(n^{3.69})$. We also prove a refined bound that depends on the
number, $K$, of intersections between the projections of the triangle edges
onto the $xy$-plane: we show that $O(n^{1+\varepsilon} + n^{1/4}
K^{3/4}\mathrm{polylog}\; n)$ fragments suffice to obtain a depth order. This
result extends to $xy$-monotone surface patches bounded by a constant number of
bounded-degree algebraic arcs, constituting the first subquadratic bound for
surface patches. As a byproduct of our approach we obtain a faster algorithm to
cut a set of lines into $O(n^{3/2}\mathrm{polylog}\; n)$ fragments that admit a
depth order."
"Readability criteria have been addressed as a measurement of the quality of
graph visualizations. In this paper, we argue that readability criteria are
necessary but not sufficient. We propose a new kind of criteria, namely
faithfulness, to evaluate the quality of graph layouts. We introduce a general
model for quantify faithfulness, and contrast it with the well established
readability criteria. We show examples of common visualization techniques, such
as multidimensional scaling, edge bundling and several other visualization
metaphors for the study of faithfulness."
"We study the following local-to-global phenomenon: Let $B$ and $R$ be two
finite sets of (blue and red) points in the Euclidean plane $\mathbb{R}^2$.
Suppose that in each ""neighborhood"" of a red point, the number of blue points
is at least as large as the number of red points. We show that in this case the
total number of blue points is at least one fifth of the total number of red
points. We also show that this bound is optimal and we generalize the result to
arbitrary dimension and arbitrary norm using results from Minkowski
arrangements."
"Consider a pair of plane straight-line graphs, whose edges are colored red
and blue, respectively, and let n be the total complexity of both graphs. We
present a O(n log n)-time O(n)-space technique to preprocess such pair of
graphs, that enables efficient searches among the red-blue intersections along
edges of one of the graphs. Our technique has a number of applications to
geometric problems. This includes: (1) a solution to the batched red-blue
search problem [Dehne et al. 2006] in O(n log n) queries to the oracle; (2) an
algorithm to compute the maximum vertical distance between a pair of 3D
polyhedral terrains one of which is convex in O(n log n) time, where n is the
total complexity of both terrains; (3) an algorithm to construct the Hausdorff
Voronoi diagram of a family of point clusters in the plane in O((n+m) log^3 n)
time and O(n+m) space, where n is the total number of points in all clusters
and m is the number of crossings between all clusters; (4) an algorithm to
construct the farthest-color Voronoi diagram of the corners of n axis-aligned
rectangles in O(n log^2 n) time; (5) an algorithm to solve the stabbing circle
problem for n parallel line segments in the plane in optimal O(n log n) time.
All these results are new or improve on the best known algorithms."
"We introduce the dynamic conflict-free coloring problem for a set $S$ of
intervals in $\mathbb{R}^1$ with respect to points, where the goal is to
maintain a conflict-free coloring for $S$ under insertions and deletions. We
investigate trade-offs between the number of colors used and the number of
intervals that are recolored upon insertion or deletion of an interval. Our
results include:
  - a lower bound on the number of recolorings as a function of the number of
colors, which implies that with $O(1)$ recolorings per update the worst-case
number of colors is $\Omega(\log n/\log\log n)$, and that any strategy using
$O(1/\varepsilon)$ colors needs $\Omega(\varepsilon n^{\varepsilon})$
recolorings;
  - a coloring strategy that uses $O(\log n)$ colors at the cost of $O(\log n)$
recolorings, and another strategy that uses $O(1/\varepsilon)$ colors at the
cost of $O(n^{\varepsilon}/\varepsilon)$ recolorings;
  - stronger upper and lower bounds for special cases.
  We also consider the kinetic setting where the intervals move continuously
(but there are no insertions or deletions); here we show how to maintain a
coloring with only four colors at the cost of three recolorings per event and
show this is tight."
"Locality-sensitive hashing (LSH) is a fundamental technique for similarity
search and similarity estimation in high-dimensional spaces. The basic idea is
that similar objects should produce hash collisions with probability
significantly larger than objects with low similarity. We consider LSH for
objects that can be represented as point sets in either one or two dimensions.
To make the point sets finite size we consider the subset of points on a grid.
Directly applying LSH (e.g. min-wise hashing) to these point sets would require
time proportional to the number of points. We seek to achieve time that is much
lower than direct approaches.
  Technically, we introduce new primitives for range-efficient consistent
sampling (of independent interest), and show how to turn such samples into LSH
values. Another application of our technique is a data structure for quickly
estimating the size of the intersection or union of a set of preprocessed
polygons. Curiously, our consistent sampling method uses transformation to a
geometric problem."
"We study biplane graphs drawn on a finite planar point set $S$ in general
position. This is the family of geometric graphs whose vertex set is $S$ and
can be decomposed into two plane graphs. We show that two maximal biplane
graphs---in the sense that no edge can be added while staying biplane---may
differ in the number of edges, and we provide an efficient algorithm for adding
edges to a biplane graph to make it maximal. We also study extremal properties
of maximal biplane graphs such as the maximum number of edges and the largest
maximum connectivity over $n$-element point sets."
"We study biplane graphs drawn on a finite point set $S$ in the plane in
general position. This is the family of geometric graphs whose vertex set is
$S$ and which can be decomposed into two plane graphs. We show that every
sufficiently large point set admits a 5-connected biplane graph and that there
are arbitrarily large point sets that do not admit any 6-connected biplane
graph. Furthermore, we show that every plane graph (other than a wheel or a
fan) can be augmented into a 4-connected biplane graph. However, there are
arbitrarily large plane graphs that cannot be augmented to a 5-connected
biplane graph by adding pairwise noncrossing edges."
"In the Any-Angle Pathfinding problem, the goal is to find the shortest path
between a pair of vertices on a uniform square grid, that is not constrained to
any fixed number of possible directions over the grid. Visibility Graphs are a
known optimal algorithm for solving the problem with the use of pre-processing.
However, Visibility Graphs are known to perform poorly in terms of running
time, especially on large, complex maps. In this paper, we introduce two
improvements over the Visibility Graph Algorithm to compute optimal paths.
Sparse Visibility Graphs (SVGs) are constructed by pruning unnecessary edges
from the original Visibility Graph. Edge N-Level Sparse Visibility Graphs
(ENLSVGs) is a hierarchical SVG built by iteratively pruning non-taut paths. We
also introduce Line-of-Sight Scans, a faster algorithm for building Visibility
Graphs over a grid. SVGs run much faster than Visibility Graphs by reducing the
average vertex degree. ENLSVGs, a hierarchical algorithm, improves this
further, especially on larger maps. On large maps, with the use of
pre-processing, these algorithms are orders of magnitude faster than existing
algorithms like Visibility Graphs and Theta*."
"The usefulness of technical drawings as well as scientific illustrations such
as medical drawings of human anatomy essentially depends on the placement of
labels that describe all relevant parts of the figure. In order to not spoil or
clutter the figure with text, the labels are often placed around the figure and
are associated by thin connecting lines to their features, respectively. This
labeling technique is known as external label placement.
  In this paper we introduce a flexible and general approach for external label
placement assuming a contour of the figure prescribing the possible positions
of the labels. While much research on external label placement aims for fast
labeling procedures for interactive systems, we focus on highest-quality
illustrations. Based on interviews with domain experts and a semi-automatic
analysis of 202 handmade anatomical drawings, we identify a set of 18 layout
quality criteria, naturally not all of equal importance. We design a new
geometric label placement algorithm that is based only on the most important
criteria. Yet, other criteria can flexibly be included in the algorithm, either
as hard constraints not to be violated or as soft constraints whose violation
is penalized by a general cost function. We formally prove that our approach
yields labelings that satisfy all hard constraints and have minimum overall
cost. Introducing several speedup techniques, we further demonstrate how to
deploy our approach in practice. In an experimental evaluation on real-world
anatomical drawings we show that the resulting labelings are of high quality
and can be produced in adequate time."
"We study approximation algorithms for the following geometric version of the
maximum coverage problem: Let $\mathcal{P}$ be a set of $n$ weighted points in
the plane. Let $D$ represent a planar object, such as a rectangle, or a disk.
We want to place $m$ copies of $D$ such that the sum of the weights of the
points in $\mathcal{P}$ covered by these copies is maximized. For any fixed
$\varepsilon>0$, we present efficient approximation schemes that can find a
$(1-\varepsilon)$-approximation to the optimal solution. In particular, for
$m=1$ and for the special case where $D$ is a rectangle, our algorithm runs in
time $O(n\log (\frac{1}{\varepsilon}))$, improving on the previous result. For
$m>1$ and the rectangular case, our algorithm runs in
$O(\frac{n}{\varepsilon}\log (\frac{1}{\varepsilon})+\frac{m}{\varepsilon}\log
m +m(\frac{1}{\varepsilon})^{O(\min(\sqrt{m},\frac{1}{\varepsilon}))})$ time.
For a more general class of shapes (including disks, polygons with $O(1)$
edges), our algorithm runs in
$O(n(\frac{1}{\varepsilon})^{O(1)}+\frac{m}{\epsilon}\log m +
m(\frac{1}{\varepsilon})^{O(\min(m,\frac{1}{\varepsilon^2}))})$ time."
"We present an implementation of a recent algorithm to compute shortest-path
trees in unit disk graphs in $O(n\log n)$ worst-case time, where $n$ is the
number of disks.
  In the minimum-separation problem, we are given $n$ unit disks and two points
$s$ and $t$, not contained in any of the disks, and we want to compute the
minimum number of disks one needs to retain so that any curve connecting $s$ to
$t$ intersects some of the retained disks. We present a new algorithm solving
this problem in $O(n^2\log^3 n)$ worst-case time and its implementation."
"The credit on {\it reduction theory} goes back to the work of Lagrange,
Gauss, Hermite, Korkin, Zolotarev, and Minkowski. Modern reduction theory is
voluminous and includes the work of A. Lenstra, H. Lenstra and L. Lovasz who
created the well known LLL algorithm, and many other researchers such as L.
Babai and C. P. Schnorr who created significant new variants of basis reduction
algorithms. In this paper, we propose and investigate the efficacy of new
optimization techniques to be used along with LLL algorithm. The techniques we
have proposed are: i) {\it hill climbing (HC)}, ii) {\it lattice diffusion-sub
lattice fusion (LDSF)}, and iii) {\it multistage hybrid LDSF-HC}. The first
technique relies on the sensitivity of LLL to permutations of the input basis
$B$, and optimization ideas over the symmetric group $S_m$ viewed as a metric
space. The second technique relies on partitioning the lattice into
sublattices, performing basis reduction in the partition sublattice blocks,
fusing the sublattices, and repeating. We also point out places where parallel
computation can reduce run-times achieving almost linear speedup. The
multistage hybrid technique relies on the lattice diffusion and sublattice
fusion and hill climbing algorithms."
"A shape visibility representation displays a graph so that each vertex is
represented by an orthogonal polygon of a particular shape and for each edge
there is a horizontal or vertical line of sight between the polygons assigned
to its endvertices. Special shapes are rectangles, L, T, E and H-shapes, and
caterpillars. A flat rectangle is a horizontal bar of height $\epsilon>0$. A
graph is 1-planar if there is a drawing in the plane such that each edge is
crossed at most once and is IC-planar if in addition no two crossing edges
share a vertex.
  We show that every IC-planar graph has a flat rectangle visibility
representation and that every 1-planar graph has a T-shape visibility
representation. The representations use quadratic area and can be computed in
linear time from a given embedding."
"A graph $G$ is called B$_k$-VPG (resp., B$_k$-EPG), for some constant $k\geq
0$, if it has a string representation on a grid such that each vertex is an
orthogonal path with at most $k$ bends and two vertices are adjacent in $G$ if
and only if the corresponding strings intersect (resp., the corresponding
strings share at least one grid edge). If two adjacent strings of a B$_k$-VPG
graph intersect exactly once, then the graph is called a one-string B$_k$-VPG
graph.
  In this paper, we study the Maximum Independent Set and Minimum Dominating
Set problems on B$_1$-VPG and B$_1$-EPG graphs. We first give a simple $O(\log
n)$-approximation algorithm for the Maximum Independent Set problem on
B$_1$-VPG graphs, improving the previous $O((\log n)^2)$-approximation
algorithm of Lahiri et al. (COCOA 2015). Then, we consider the Minimum
Dominating Set problem. We give an $O(1)$-approximation algorithm for this
problem on one-string B$_1$-VPG graphs, providing the first constant-factor
approximation algorithm for this problem. Moreover, we show that the Minimum
Dominating Set problem is APX-hard on B$_1$-EPG graphs, ruling out the
possibility of a PTAS unless P=NP. Finally, we give constant-factor
approximation algorithms for this problem on two non-trivial subclasses of
B$_1$-EPG graphs. To our knowledge, these are the first results for the Minimum
Dominating Set problem on B$_1$-EPG graphs, partially answering a question
posed by Epstein et al. (WADS 2013)."
"We introduce a new geometric spanner, $\delta$-Greedy, whose construction is
based on a generalization of the known Path-Greedy and Gap-Greedy spanners. The
$\delta$-Greedy spanner combines the most desirable properties of geometric
spanners both in theory and in practice. More specifically, it has the same
theoretical and practical properties as the Path-Greedy spanner: a natural
definition, small degree, linear number of edges, low weight, and strong
$(1+\varepsilon)$-spanner for every $\varepsilon>0$. The $\delta$-Greedy
algorithm is an improvement over the Path-Greedy algorithm with respect to the
number of shortest path queries and hence with respect to its construction
time. We show how to construct such a spanner for a set of $n$ points in the
plane in $O(n^2 \log n)$ time.
  The $\delta$-Greedy spanner has an additional parameter, $\delta$, which
indicates how close it is to the Path-Greedy spanner on the account of the
number of shortest path queries. For $\delta = t$ the output spanner is
identical to the Path-Greedy spanner, while the number of shortest path queries
is, in practice, linear.
  Finally, we show that for a set of $n$ points placed independently at random
in a unit square the expected construction time of the $\delta$-Greedy
algorithm is $O(n \log n)$. Our analysis indicates that the $\delta$-Greedy
spanner gives the best results among the known spanners of expected $O(n \log
n)$ time for random point sets. Moreover, the analysis implies that by setting
$\delta = t$, the $\delta$-Greedy algorithm provides a spanner identical to the
Path-Greedy spanner in expected $O(n \log n)$ time."
"For a distribution function $F$ on $\mathbb{R}^d$ and a point $q\in
\mathbb{R}^d$, the \emph{spherical depth} $\SphD(q;F)$ is defined to be the
probability that a point $q$ is contained inside a random closed hyper-ball
obtained from a pair of points from $F$. The spherical depth $\SphD(q;S)$ is
also defined for an arbitrary data set $S\subseteq \mathbb{R}^d$ and $q\in
\mathbb{R}^d$. This definition is based on counting all of the closed
hyper-balls, obtained from pairs of points in $S$, that contain $q$. The
significant advantage of using the spherical depth in multivariate data
analysis is related to its complexity of computation. Unlike most other data
depths, the time complexity of the spherical depth grows linearly rather than
exponentially in the dimension $d$. The straightforward algorithm for computing
the spherical depth in dimension $d$ takes $O(dn^2)$. The main result of this
paper is an optimal algorithm that we present for computing the bivariate
spherical depth. The algorithm takes $O(n \log n)$ time. By reducing the
problem of \textit{Element Uniqueness}, we prove that computing the spherical
depth requires $\Omega(n \log n)$ time. Some geometric properties of spherical
depth are also investigated in this paper. These properties indicate that
\emph{simplicial depth} ($\SD$) (Liu, 1990) is linearly bounded by spherical
depth (in particular, $\SphD\geq \frac{2}{3}SD$). To illustrate this
relationship between the spherical depth and the simplicial depth, some
experimental results are provided. The obtained experimental bound ($\SphD\geq
2\SD$) indicates that, perhaps, a stronger theoretical bound can be achieved."
"For a set of points in the plane, a \emph{crossing family} is a set of line
segments, each joining two of the points, such that any two line segments
cross. We investigate the following generalization of crossing families: a
\emph{spoke set} is a set of lines drawn through a point set such that each
unbounded region of the induced line arrangement contains at least one point of
the point set. We show that every point set has a spoke set of size
$\sqrt{\frac{n}{8}}$. We also characterize the matchings obtained by selecting
exactly one point in each unbounded region and connecting every such point to
the point in the antipodal unbounded region."
"A straight-line drawing $\Gamma$ of a graph $G=(V,E)$ is a drawing of $G$ in
the Euclidean plane, where every vertex in $G$ is mapped to a distinct point,
and every edge in $G$ is mapped to a straight line segment between their
endpoints. A path $P$ in $\Gamma$ is called increasing-chord if for every four
points (not necessarily vertices) $a,b,c,d$ on $P$ in this order, the Euclidean
distance between $b,c$ is at most the Euclidean distance between $a,d$. A
spanning tree $T$ rooted at some vertex $r$ in $\Gamma$ is called
increasing-chord if $T$ contains an increasing-chord path from $r$ to every
vertex in $T$. In this paper we prove that given a vertex $r$ in a
straight-line drawing $\Gamma$, it is NP-complete to determine whether $\Gamma$
contains an increasing-chord spanning tree rooted at $r$. We conjecture that
finding an increasing-chord path between a pair of vertices in $\Gamma$, which
is an intriguing open problem posed by Alamdari et al., is also NP-complete,
and show a (non-polynomial) reduction from the 3-SAT problem."
"We present a new algorithm for the widely used density-based clustering
method DBscan. Our algorithm computes the DBscan-clustering in $O(n\log n)$
time in $\mathbb{R}^2$, irrespective of the scale parameter $\varepsilon$ (and
assuming the second parameter MinPts is set to a fixed constant, as is the case
in practice). Experiments show that the new algorithm is not only fast in
theory, but that a slightly simplified version is competitive in practice and
much less sensitive to the choice of $\varepsilon$ than the original DBscan
algorithm. We also present an $O(n\log n)$ randomized algorithm for HDBscan in
the plane---HDBscan is a hierarchical version of DBscan introduced
recently---and we show how to compute an approximate version of HDBscan in
near-linear time in any fixed dimension."
"In the General Position Subset Selection (GPSS) problem, the goal is to find
the largest possible subset of a set of points, such that no three of its
members are collinear. If $s_{\textrm{GPSS}}$ is the size the optimal solution,
$\sqrt{s_{\textrm{GPSS}}}$ is the current best guarantee for the size of the
solution obtained using a polynomial time algorithm. In this paper we present
an algorithm for GPSS to improve this bound based on the number of collinear
pairs of points. We experimentally evaluate this and few other GPSS algorithms;
the result of these experiments suggests further opportunities for obtaining
tighter lower bounds for GPSS."
"A graph is $k$-planar $(k \geq 1)$ if it can be drawn in the plane such that
no edge is crossed more than $k$ times. A graph is $k$-quasi planar $(k \geq
2)$ if it can be drawn in the plane with no $k$ pairwise crossing edges. The
families of $k$-planar and $k$-quasi planar graphs have been widely studied in
the literature, and several bounds have been proven on their edge density.
Nonetheless, only trivial results are known about the relationship between
these two graph families. In this paper we prove that, for $k \geq 3$, every
$k$-planar graph is $(k+1)$-quasi planar."
"Given a set $S$ of $n$ static points and a free point $p$ in the Euclidean
plane, we study a new variation of the minimum enclosing circle problem, in
which a dynamic weight that equals to the reciprocal of the distance from the
free point $p$ to the undetermined circle center is included. In this work, we
prove the optimal solution of the new problem is unique and lies on the
boundary of the farthest-point Voronoi diagram of $S$, once $p$ does not
coincide with any vertex of the convex hull of $S$. We propose a tree structure
constructed from the boundary of the farthest-point Voronoi diagram and use the
hierarchical relationship between edges to locate the optimal solution. The
plane could be divide into at most $3n-4$ non-overlapping regions. When $p$
lies in one of the regions, the optimal solution locates at one node or lies on
the interior of one edge in the boundary of the farthest-point Voronoi diagram.
Moreover, we apply the new variation to calculate the maximum displacement of
one point $p$ under the condition that the displacements of points in $S$ are
restricted in 2D rigid motion."
"In an $\mathsf{L}$-embedding of a graph, each vertex is represented by an
$\mathsf{L}$-segment, and two segments intersect each other if and only if the
corresponding vertices are adjacent in the graph. If the corner of each
$\mathsf{L}$-segment in an $\mathsf{L}$-embedding lies on a straight line, we
call it a monotone $\mathsf{L}$-embedding. In this paper we give a full
characterization of monotone $\mathsf{L}$-embeddings by introducing a new class
of graphs which we call ""non-jumping"" graphs. We show that a graph admits a
monotone $\mathsf{L}$-embedding if and only if the graph is a non-jumping
graph. Further, we show that outerplanar graphs, convex bipartite graphs,
interval graphs, 3-leaf power graphs, and complete graphs are subclasses of
non-jumping graphs. Finally, we show that distance-hereditary graphs and
$k$-leaf power graphs ($k\le 4$) admit $\mathsf{L}$-embeddings."
"The Euclidean TSP with neighborhoods (TSPN) problem seeks a shortest tour
that visits a given collection of $n$ regions ({\em neighborhoods}). We present
the first polynomial-time approximation scheme for TSPN for a set of regions
given by arbitrary disjoint fat regions in the plane. This improves
substantially upon the known approximation algorithms, and is the first PTAS
for TSPN on regions of non-comparable sizes. Our result is based on a novel
extension of the $m$-guillotine method. The result applies to regions that are
""fat"" in a very weak sense: each region $P_i$ has area $\Omega([diam(P_i)]^2)$,
but is otherwise arbitrary."
"We define the emph{visual complexity of a plane graph drawing to be the
number of basic geometric objects needed to represent all its edges. In
particular, one object may represent multiple edges (e.g., one needs only one
line segment to draw two collinear edges of the same vertex). Let $n$ denote
the number of vertices of a graph. We show that trees can be drawn with $3n/4$
straight-line segments on a polynomial grid, and with $n/2$ straight-line
segments on a quasi-polynomial grid. Further, we present an algorithm for
drawing planar 3-trees with $(8n-17)/3$ segments on an $O(n)\times O(n^2)$
grid. This algorithm can also be used with a small modification to draw maximal
outerplanar graphs with $3n/2$ edges on an $O(n)\times O(n^2)$ grid. We also
study the problem of drawing maximal planar graphs with circular arcs and
provide an algorithm to draw such graphs using only $(5n - 11)/3$ arcs. This
provides a significant improvement over the lower bound of $2n$ for line
segments for a nontrivial graph class."
"The notion of 1-planarity is among the most natural and most studied
generalizations of graph planarity. A graph is 1-planar if it has an embedding
where each edge is crossed by at most another edge. The study of 1-planar
graphs dates back to more than fifty years ago and, recently, it has driven
increasing attention in the areas of graph theory, graph algorithms, graph
drawing, and computational geometry. This annotated bibliography aims to
provide a guiding reference to researchers who want to have an overview of the
large body of literature about 1-planar graphs. It reviews the current
literature covering various research streams about 1-planarity, such as
characterization and recognition, combinatorial properties, and geometric
representations. As an additional contribution, we offer a list of open
problems on 1-planar graphs."
"We study the complexity of symmetric assembly puzzles: given a collection of
simple polygons, can we translate, rotate, and possibly flip them so that their
interior-disjoint union is line symmetric? On the negative side, we show that
the problem is strongly NP-complete even if the pieces are all polyominos. On
the positive side, we show that the problem can be solved in polynomial time if
the number of pieces is a fixed constant."
"We describe a set of $\Delta -1$ slopes that are universal for 1-bend planar
drawings of planar graphs of maximum degree $\Delta \geq 4$; this establishes a
new upper bound of $\Delta-1$ on the 1-bend planar slope number. By universal
we mean that every planar graph of degree $\Delta$ has a planar drawing with at
most one bend per edge and such that the slopes of the segments forming the
edges belong to the given set of slopes. This improves over previous results in
two ways: Firstly, the best previously known upper bound for the 1-bend planar
slope number was $\frac{3}{2} (\Delta -1)$ (the known lower bound being
$\frac{3}{4} (\Delta -1)$); secondly, all the known algorithms to construct
1-bend planar drawings with $O(\Delta)$ slopes use a different set of slopes
for each graph and can have bad angular resolution, while our algorithm uses a
universal set of slopes, which also guarantees that the minimum angle between
any two edges incident to a vertex is $\frac{\pi}{(\Delta-1)}$."
"Given a set $S$ of $n$ line segments in the plane, we say that a region
$\mathcal{R}\subseteq \mathbb{R}^2$ is a {\em stabber} for $S$ if $\mathcal{R}$
contains exactly one endpoint of each segment of $S$. In this paper we provide
optimal or near-optimal algorithms for reporting all combinatorially different
stabbers for several shapes of stabbers. Specifically, we consider the case in
which the stabber can be described as the intersection of axis-parallel
halfplanes (thus the stabbers are halfplanes, strips, quadrants, $3$-sided
rectangles, or rectangles). The running times are $O(n)$ (for the halfplane
case), $O(n\log n)$ (for strips, quadrants, and 3-sided rectangles), and $O(n^2
\log n)$ (for rectangles)."
"We present an $(1+\varepsilon)$-approximation algorithm with quasi-polynomial
running time for computing the maximum weight independent set of polygons out
of a given set of polygons in the plane (specifically, the running time is
$n^{O( \mathrm{poly}( \log n, 1/\varepsilon))}$). Contrasting this, the best
known polynomial time algorithm for the problem has an approximation ratio
of~$n^{\varepsilon}$. Surprisingly, we can extend the algorithm to the problem
of computing the maximum weight subset of the given set of polygons whose
intersection graph fulfills some sparsity condition. For example, we show that
one can approximate the maximum weight subset of polygons, such that the
intersection graph of the subset is planar or does not contain a cycle of
length $4$ (i.e., $K_{2,2}$). Our algorithm relies on a recursive partitioning
scheme, whose backbone is the existence of balanced cuts with small complexity
that intersect polygons from the optimal solution of a small total weight.
  For the case of large axis-parallel rectangles, we provide a polynomial time
$(1+\varepsilon)$-approximation for the maximum weight independent set.
Specifically, we consider the problem where each rectangle has one edge whose
length is at least a constant fraction of the length of the corresponding edge
of the bounding box of all the input elements. This is now the most general
case for which a PTAS is known, and it requires a new and involved partitioning
scheme, which should be of independent interest."
"Graphs and network data are ubiquitous across a wide spectrum of scientific
and application domains. Often in practice, an input graph can be considered as
an observed snapshot of a (potentially continuous) hidden domain or process.
Subsequent analysis, processing, and inferences are then performed on this
observed graph. In this paper we advocate the perspective that an observed
graph is often a noisy version of some discretized 1-skeleton of a hidden
domain, and specifically we will consider the following natural network model:
We assume that there is a true graph ${G^*}$ which is a certain proximity graph
for points sampled from a hidden domain $\mathcal{X}$; while the observed graph
$G$ is an Erd$\""{o}$s-R$\'{e}$nyi type perturbed version of ${G^*}$.
  Our network model is related to, and slightly generalizes, the
much-celebrated small-world network model originally proposed by Watts and
Strogatz. However, the main question we aim to answer is orthogonal to the
usual studies of network models (which often focuses on characterizing /
predicting behaviors and properties of real-world networks). Specifically, we
aim to recover the metric structure of ${G^*}$ (which reflects that of the
hidden space $\mathcal{X}$ as we will show) from the observed graph $G$. Our
main result is that a simple filtering process based on the \emph{Jaccard
index} can recover this metric within a multiplicative factor of $2$ under our
network model. Our work makes one step towards the general question of
inferring structure of a hidden space from its observed noisy graph
representation. In addition, our results also provide a theoretical
understanding for Jaccard-Index-based denoising approaches."
"Let $P$ be a set of $n$ points in the plane. We consider the problem of
partitioning $P$ into two subsets $P_1$ and $P_2$ such that the sum of the
perimeters of $\text{CH}(P_1)$ and $\text{CH}(P_2)$ is minimized, where
$\text{CH}(P_i)$ denotes the convex hull of $P_i$. The problem was first
studied by Mitchell and Wynters in 1991 who gave an $O(n^2)$ time algorithm.
Despite considerable progress on related problems, no subquadratic time
algorithm for this problem was found so far. We present an exact algorithm
solving the problem in $O(n \log^4 n)$ time and a
$(1+\varepsilon)$-approximation algorithm running in $O(n +
1/\varepsilon^2\cdot\log^4(1/\varepsilon))$ time."
"Given a set of points in the plane, we want to establish a connection network
between these points that consists of several disjoint layers. Motivated by
sensor networks, we want that each layer is spanning and plane, and that no
edge is very long (when compared to the minimum length needed to obtain a
spanning graph).
  We consider two different approaches: first we show an almost optimal
centralized approach to extract two graphs. Then we show a constant factor
approximation for a distributed model in which each point can compute its
adjacencies using only local information. In both cases the obtained layers are
plane"
"We study self-approaching paths that are contained in a simple polygon. A
self-approaching path is a directed curve connecting two points such that the
Euclidean distance between a point moving along the path and any future
position does not increase, that is, for all points $a$, $b$, and $c$ that
appear in that order along the curve, $|ac| \ge |bc|$. We analyze the
properties, and present a characterization of shortest self-approaching paths.
In particular, we show that a shortest self-approaching path connecting two
points inside a polygon can be forced to use a general class of non-algebraic
curves. While this makes it difficult to design an exact algorithm, we show how
to find a self-approaching path inside a polygon connecting two points under a
model of computation which assumes that we can calculate involute curves of
high order. Lastly, we provide an algorithm to test if a given simple polygon
is self-approaching, that is, if there exists a self-approaching path for any
two points inside the polygon."
"The concept of derivative coordinate functions proved useful in the
formulation of analytic fractal functions to represent smooth symmetric binary
fractal trees [1]. In this paper we introduce a new geometry that defines the
fractal space around these fractal trees. We present the canonical and
degenerate form of this fractal space and extend the fractal geometrical space
to R3 specifically and Rn by a recurrence relation. We also discuss the usage
of such fractal geometry."
"The construction of anisotropic triangulations is desirable for various
applications, such as the numerical solving of partial differential equations
and the representation of surfaces in graphics. To solve this notoriously
difficult problem in a practical way, we introduce the discrete Riemannian
Voronoi diagram, a discrete structure that approximates the Riemannian Voronoi
diagram. This structure has been implemented and was shown to lead to good
triangulations in $\mathbb{R}^2$ and on surfaces embedded in $\mathbb{R}^3$ as
detailed in our experimental companion paper.
  In this paper, we study theoretical aspects of our structure. Given a finite
set of points $\cal P$ in a domain $\Omega$ equipped with a Riemannian metric,
we compare the discrete Riemannian Voronoi diagram of $\cal P$ to its
Riemannian Voronoi diagram. Both diagrams have dual structures called the
discrete Riemannian Delaunay and the Riemannian Delaunay complex. We provide
conditions that guarantee that these dual structures are identical. It then
follows from previous results that the discrete Riemannian Delaunay complex can
be embedded in $\Omega$ under sufficient conditions, leading to an anisotropic
triangulation with curved simplices. Furthermore, we show that, under similar
conditions, the simplices of this triangulation can be straightened."
"The computation of (i) $\varepsilon$-kernels, (ii) approximate diameter, and
(iii) approximate bichromatic closest pair are fundamental problems in
geometric approximation. In this paper, we describe new algorithms that offer
significant improvements to their running times. In each case the input is a
set of $n$ points in $R^d$ for a constant dimension $d \geq 3$ and an
approximation parameter $\varepsilon > 0$. We reduce the respective running
times (i) from $O((n + 1/\varepsilon^{d-2})\log(1/\varepsilon))$ to $O(n
\log(1/\varepsilon) + 1/\varepsilon^{(d-1)/2+\alpha})$, (ii) from $O((n +
1/\varepsilon^{d-2})\log(1/\varepsilon))$ to $O(n \log(1/\varepsilon) +
1/\varepsilon^{(d-1)/2+\alpha})$, and (iii) from $O(n / \varepsilon^{d/3})$ to
$O(n / \varepsilon^{d/4+\alpha}),$ for an arbitrarily small constant $\alpha >
0$. Result (i) is nearly optimal since the size of the output
$\varepsilon$-kernel is $\Theta(1/\varepsilon^{(d-1)/2})$ in the worst case.
  These results are all based on an efficient decomposition of a convex body
using a hierarchy of Macbeath regions, and contrast to previous solutions that
decompose space using quadtrees and grids. By further application of these
techniques, we also show that it is possible to obtain near-optimal
preprocessing time for the most efficient data structures to approximately
answer queries for (iv) nearest-neighbor searching, (v) directional width, and
(vi) polytope membership."
"We study the minimum diameter problem for a set of inexact points. By
inexact, we mean that the precise location of the points is not known. Instead,
the location of each point is restricted to a contineus region ($\impre$ model)
or a finite set of points ($\indec$ model). Given a set of inexact points in
one of $\impre$ or $\indec$ models, we wish to provide a lower-bound on the
diameter of the real points.
  In the first part of the paper, we focus on $\indec$ model. We present an
$O(2^{\frac{1}{\epsilon^d}} \cdot \epsilon^{-2d} \cdot n^3 )$ time
approximation algorithm of factor $(1+\epsilon)$ for finding minimum diameter
of a set of points in $d$ dimensions. This improves the previously proposed
algorithms for this problem substantially.
  Next, we consider the problem in $\impre$ model. In $d$-dimensional space, we
propose a polynomial time $\sqrt{d}$-approximation algorithm. In addition, for
$d=2$, we define the notion of $\alpha$-separability and use our algorithm for
$\indec$ model to obtain $(1+\epsilon)$-approximation algorithm for a set of
$\alpha$-separable regions in time $O(2^{\frac{1}{\epsilon^2}}\allowbreak .
\frac{n^3}{\epsilon^{10} .\sin(\alpha/2)^3} )$."
"In this paper we propose a novel algorithm to combine two or more cellular
complexes, providing a minimal fragmentation of the cells of the resulting
complex. We introduce here the idea of arrangement generated by a collection of
cellular complexes, producing a cellular decomposition of the embedding space.
The algorithm that executes this computation is called Merge of complexes. The
arrangements of segment lines in 2D and polygons in 3D are special cases, like
the combination of closed triangulated surfaces or meshed models. This
algorithm has several important applications, including Boolean and other set
operations over large geometric models, the extraction of solid models of
biomedical structures at the cellular scale, the detailed geometric modeling of
buildings, the combination of 3D meshes, and the repair of graphical models.
The algorithm operates over the Linear Algebraic Representation (LAR) of
argument complexes, i.e., on sparse representation of binary characteristic
matrices of d-cell bases, well-suited for implementation in last generation
accelerators and GPGPU applications."
"A lattice (d, k)-polytope is the convex hull of a set of points in dimension
d whose coordinates are integers between 0 and k. Let {\delta}(d, k) be the
largest diameter over all lattice (d, k)-polytopes. We develop a computational
framework to determine {\delta}(d, k) for small instances. We show that
{\delta}(3, 4) = 7 and {\delta}(3, 5) = 9; that is, we verify for (d, k) = (3,
4) and (3, 5) the conjecture whereby {\delta}(d, k) is at most (k + 1)d/2 and
is achieved, up to translation, by a Minkowski sum of lattice vectors."
"The problem of constrained $k$-center clustering has attracted significant
attention in the past decades. In this paper, we study balanced $k$-center
cluster where the size of each cluster is constrained by the given lower and
upper bounds. The problem is motivated by the applications in processing and
analyzing large-scale data in high dimension. We provide a simple nearly linear
time $4$-approximation algorithm when the number of clusters $k$ is assumed to
be a constant. Comparing with existing method, our algorithm improves the
approximation ratio and significantly reduces the time complexity. Moreover,
our result can be easily extended to any metric space."
"LSH (locality sensitive hashing) had emerged as a powerful technique in
nearest-neighbor search in high dimensions [IM98, HIM12]. Given a point set $P$
in a metric space, and given parameters $r$ and $\varepsilon > 0$, the task is
to preprocess the point set, such that given a query point $q$, one can quickly
decide if $q$ is in distance at most $\leq r$ or $\geq (1+\varepsilon)r$ from
the point set $P$. Once such a near-neighbor data-structure is available, one
can reduce the general nearest-neighbor search to logarithmic number of queries
in such structures [IM98, Har01, HIM12].
  In this note, we revisit the most basic settings, where $P$ is a set of
points in the binary hypercube $\{0,1\}^d$, under the $L_1$/Hamming metric, and
present a short description of the LSH scheme in this case. We emphasize that
there is no new contribution in this note, except (maybe) the presentation
itself, which is inspired by the authors recent work [HM17]."
"It is a long standing open problem whether Yao-Yao graphs $\mathsf{YY}_{k}$
are all spanners. Bauer and Damian \cite{bauer2013infinite} showed that all
$\mathsf{YY}_{6k}$ for $k \geq 6$ are spanners. Li and Zhan \cite{li2016almost}
generalized their result and proved that all even Yao-Yao graphs
$\mathsf{YY}_{2k}$ are spanners (for $k\geq 42$). However, their technique
cannot be extended to odd Yao-Yao graphs, and whether they are spanners are
still elusive. In this paper, we show that, surprisingly, for any integer $k
\geq 1$, there exist odd Yao-Yao graph $\mathsf{YY}_{2k+1}$ instances, which
are not spanners."
"Let $P$ be a finite set of points in the plane and $S$ a set of non-crossing
line segments with endpoints in $P$. The visibility graph of $P$ with respect
to $S$, denoted $Vis(P,S)$, has vertex set $P$ and an edge for each pair of
vertices $u,v$ in $P$ for which no line segment of $S$ properly intersects
$uv$. We show that the constrained half-$\theta_6$-graph (which is identical to
the constrained Delaunay graph whose empty visible region is an equilateral
triangle) is a plane 2-spanner of $Vis(P,S)$. We then show how to construct a
plane 6-spanner of $Vis(P,S)$ with maximum degree $6+c$, where $c$ is the
maximum number of segments of $S$ incident to a vertex."
"This article introduces a theory of proximal nerve complexes and nerve
spokes, restricted to the triangulation of finite regions in the Euclidean
plane. A nerve complex is a collection of filled triangles with a common
vertex, covering a finite region of the plane. Structures called $k$-spokes,
$k\geq 1$, are a natural extension of nerve complexes. A $k$-spoke is the union
of a collection of filled triangles that pairwise either have a common edge or
a common vertex. A consideration of the closeness of nerve complexes leads to a
proximal view of simplicial complexes. A practical application of proximal
nerve complexes is given, briefly, in terms of object shape geometry in digital
images."
"The dispersion problem has been widely studied in computational geometry and
facility location, and is closely related to the packing problem. The goal is
to locate n points (e.g., facilities or persons) in a k-dimensional polytope,
so that they are far away from each other and from the boundary of the
polytope. In many real-world scenarios however, the points arrive and depart at
different times, and decisions must be made without knowing future events.
Therefore we study, for the first time in the literature, the online dispersion
problem in Euclidean space.
  There are two natural objectives when time is involved: the all-time
worst-case (ATWC) problem tries to maximize the minimum distance that ever
appears at any time; and the cumulative distance (CD) problem tries to maximize
the integral of the minimum distance throughout the whole time interval.
Interestingly, the online problems are highly non-trivial even on a segment.
For cumulative distance, this remains the case even when the problem is
time-dependent but offline, with all the arriving and departure times given in
advance.
  For the online ATWC problem on a segment, we construct a deterministic
polynomial-time algorithm which is (2ln2+epsilon)-competitive, where epsilon>0
can be arbitrarily small and the algorithm's running time is polynomial in
1/epsilon. We show this algorithm is actually optimal. For the same problem in
a square, we provide a 1.591-competitive algorithm and a 1.183 lower-bound.
Furthermore, for arbitrary k-dimensional polytopes with k>=2, we provide a
2/(1-epsilon)-competitive algorithm and a 7/6 lower-bound. Interestingly, for
the offline CD problem in arbitrary k-dimensional polytopes, we provide a
polynomial-time black-box reduction to the online ATWC problem, and the
resulting competitive ratio increases by a factor of at most 2."
"We prove that the art gallery problem is equivalent under polynomial time
reductions to deciding whether a system of polynomial equations over the real
numbers has a solution. The art gallery problem is a classical problem in
computational geometry. Given a simple polygon $P$ and an integer $k$, the goal
is to decide if there exists a set $G$ of $k$ guards within $P$ such that every
point $p\in P$ is seen by at least one guard $g\in G$. Each guard corresponds
to a point in the polygon $P$, and we say that a guard $g$ sees a point $p$ if
the line segment $pg$ is contained in $P$.
  The art gallery problem has stimulated a myriad of research in geometry and
in algorithms. However, the complexity status of the art gallery problem has
not been resolved. It has long been known that the problem is $\text{NP}$-hard,
but no one has been able to show that it lies in $\text{NP}$. Recently, the
computational geometry community became more aware of the complexity class
$\exists \mathbb{R}$. The class $\exists \mathbb{R}$ consists of problems that
can be reduced in polynomial time to the problem of deciding whether a system
of polynomial equations with integer coefficients and any number of real
variables has a solution. It can be easily seen that $\text{NP}\subseteq
\exists \mathbb{R}$. We prove that the art gallery problem is $\exists
\mathbb{R}$-complete, implying that (1) any system of polynomial equations over
the real numbers can be encoded as an instance of the art gallery problem, and
(2) the art gallery problem is not in the complexity class $\text{NP}$ unless
$\text{NP}=\exists \mathbb{R}$. As a corollary of our construction, we prove
that for any real algebraic number $\alpha$ there is an instance of the art
gallery problem where one of the coordinates of the guards equals $\alpha$ in
any guard set of minimum cardinality. That rules out many geometric approaches
to the problem."
"We investigate several computational problems related to the stochastic
convex hull (SCH). Given a stochastic dataset consisting of $n$ points in
$\mathbb{R}^d$ each of which has an existence probability, a SCH refers to the
convex hull of a realization of the dataset, i.e., a random sample including
each point with its existence probability. We are interested in computing
certain expected statistics of a SCH, including diameter, width, and
combinatorial complexity. For diameter, we establish the first deterministic
1.633-approximation algorithm with a time complexity polynomial in both $n$ and
$d$. For width, two approximation algorithms are provided: a deterministic
$O(1)$-approximation running in $O(n^{d+1} \log n)$ time, and a fully
polynomial-time randomized approximation scheme (FPRAS). For combinatorial
complexity, we propose an exact $O(n^d)$-time algorithm. Our solutions exploit
many geometric insights in Euclidean space, some of which might be of
independent interest."
"Motivated by map labeling, we study the problem in which we are given a
collection of $n$ disks $D_1, \dots, D_n$ in the plane that grow at possibly
different speeds. Whenever two disks meet, the one with the lower index
disappears. This problem was introduced by Funke, Krumpe, and Storandt [IWOCA
2016]. We provide the first general subquadratic algorithm for computing the
times and the order of disappearance. This algorithm also works for other
shapes (such as rectangles) and in any fixed dimension.
  Using quadtrees, we provide an alternative algorithm that runs in near linear
time, although this second algorithm has a logarithmic dependence on either the
ratio of the fastest speed to the slowest speed of disks or the spread of disk
centers (the ratio of the maximum to the minimum distance between them). Our
result improves the running times of previous algorithms by Funke, Krumpe, and
Storandt [IWOCA 2016], Bahrdt et al. [ALENEX 2017] and Funke and Storandt [EWCG
2017]. Finally, we give an $\Omega(n\log n)$ lower bound on the problem,
showing that our quadtree algorithms are almost tight."
"We devise the following dynamic algorithms for both maintaining as well as
querying for the visibility and weak visibility polygons amid vertex insertions
and/or deletions to the simple polygon.
  * A fully-dynamic algorithm for maintaining the visibility polygon of a fixed
point located interior to the simple polygon amid vertex insertions and
deletions to the simple polygon. The time complexity to update the visibility
polygon of a point $q$ due to the insertion (resp. deletion) of vertex $v$ to
(resp. from) the current simple polygon is expressed in terms of the number of
combinatorial changes needed to the visibility polygon of $q$ due to the
insertion (resp. deletion) of $v$.
  * An output-sensitive query algorithm to answer the visibility polygon query
corresponding to any point $p$ in $\mathbb{R}^2$ amid vertex insertions and
deletions to the simple polygon. If $p$ is not exterior to the current simple
polygon then the visibility polygon of $p$ is computed. Otherwise, our
algorithm outputs the visibility polygon corresponding to the exterior
visibility of $p$.
  * An output-sensitive algorithm to compute the weak visibility polygon
corresponding to any query line segment located interior to the simple polygon
amid both the vertex insertions and deletions to the simple polygon.
  Each of these algorithms require preprocessing the initial simple polygon.
And, the algorithms that maintain the visibility polygon (resp. weak visibility
polygon) compute the visibility polygon (resp. weak visibility polygon) with
respect to the initial simple polygon during the preprocessing phase."
"This paper intends to lay the theoretical foundation for the method of
functional maps, first presented in 2012 by Ovsjanikov, Ben-Chen, Solomon,
Butscher and Guibas in the field of the theory and numerics of maps between
shapes. We show how to analyze this method by looking at it as an application
of the theories of composition operators, of matrix representa- tion of
operators on separable Hilbert spaces, and of the theory of the Finite Section
Method. These are three well known fruitful topics in functional analysis. When
applied to the task of modelling of correspondences of shapes in
three-dimensional space, these concepts lead directly to functional maps and
its associated functional matrices. Mathematically spoken, functional maps are
composition operators between two-dimensional manifolds, and functional
matrices are infinite matrix representations of such maps. We present an
introduction into the notion and theoretical foundation of the functional
analytic framework of the theory of matrix repre- sentation, especially of
composition operators. We will also discuss two numerical methods for solving
equations with such operators, namely, two variants of the Rectangular Finite
Section Method. While one of these, which is well known, leads to an
overdetermined system of linear equations, in the second one the minimum-norm
solution of an underdetermined system has to be computed. We will present the
main convergence results related to these methods."
"We derive properties and a characterization of discrete composition matrices
which are useful in the field of numerical computation of shape
correspondences."
"In the circle packing problem for triangular containers, one asks whether a
given set of circles can be packed into a given triangle. Packing problems like
this have been shown to be $\mathsf{NP}$-hard. In this paper, we present a new
sufficient condition for packing circles into any right or obtuse triangle
using only the circles' combined area: It is possible to pack any circle
instance whose combined area does not exceed the triangle's incircle. This area
condition is tight, in the sense that for any larger area, there are instances
which cannot be packed.
  A similar result for square containers has been established earlier this
year, using the versatile, divide-and-conquer-based Split Packing algorithm. In
this paper, we present a generalized, weighted version of this approach,
allowing us to construct packings of circles into asymmetric triangles. It
seems crucial to the success of these results that Split Packing does not
depend on an orthogonal subdivision structure. Beside realizing all packings
below the critical density bound, our algorithm can also be used as a
constant-factor approximation algorithm when looking for the smallest non-acute
triangle of a given side ratio in which a given set of circles can be packed.
  An interactive visualization of the Split Packing approach and other related
material can be found at https://morr.cc/split-packing/."
"The VC-dimension plays an important role for the algorithmic problem of
guarding art galleries efficiently. We prove that inside a simple polygon at
most $5$ points can be shattered by $L_1$-visibility polygons and give an
example where 5 points are shattered. The VC-dimension is exactly $5$. The
proof idea for the upper bound is different from previous approaches.
  Keywords: Art gallery, VC-dimension, $L_1$-visibility, polygons"
"Indirect hex-dominant meshing relies on the detection of adjacent tetrahedra
that may be combined to form hexahedra. In this paper we introduce an algorithm
that performs this identification and builds the largest set of potential
hexahedra from an input tetrahedral mesh. The new method does not rely on a
predefined set of patterns of tetrahedral decompositions of hexahedra but
allows to look for all combinations of eight vertices whose connectivity
matches the connectivity of a hexahedron. The method relies on a backtracking
algorithm where connectivity checks and minimum bound quality checks are used
to discard invalid hexahedra. The subset of tetrahedra of triangulating each
potential hexahedron is determined once its eight vertices are computed. The
new algorithm computes around 3 millions potential hexahedra in 10 seconds and
ensures that each potential hexahedron is computed only once. We finally
demonstrate that the set of potential hexes built by our algorithm is
significantly larger than those built by pattern based methods."