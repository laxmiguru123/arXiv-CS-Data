summary
"In this note, we fill a gap in the proof of the heuristic GCD in the
multivariate case made by Char, Geddes and Gonnet (JSC 1989) and give some
additionnal information on this method."
"This paper developed a systematic strategy establishing RBF on the wavelet
analysis, which includes continuous and discrete RBF orthonormal wavelet
transforms respectively in terms of singular fundamental solutions and
nonsingular general solutions of differential operators. In particular, the
harmonic Bessel RBF transforms were presented for high-dimensional data
processing. It was also found that the kernel functions of convection-diffusion
operator are feasible to construct some stable ridgelet-like RBF transforms. We
presented time-space RBF transforms based on non-singular solution and
fundamental solution of time-dependent differential operators. The present
methodology was further extended to analysis of some known RBFs such as the MQ,
Gaussian and pre-wavelet kernel RBFs."
"Recently, Bert, Wang and Striz [1, 2] applied the differential quadrature
(DQ) and harmonic differential quadrature (HDQ) methods to analyze static and
dynamic behaviors of anisotropic plates. Their studies showed that the methods
were conceptually simple and computationally efficient in comparison to other
numerical techniques. Based on some recent work by the present author [3, 4],
the purpose of this note is to further simplify the formulation effort and
improve computing efficiency in applying the DQ and HDQ methods for these
cases."
"The theme of symbolic computation in algebraic categories has become of
utmost importance in the last decade since it enables the automatic modeling of
modern algebra theories. On this theoretical background, the present paper
reveals the utility of the parameterized categorical approach by deriving a
multivariate polynomial category (over various coefficient domains), which is
used by our Mathematica implementation of Buchberger's algorithms for
determining the Groebner basis. These implementations are designed according to
domain and category parameterization principles underlining their advantages:
operation protection, inheritance, generality, easy extendibility. In
particular, such an extension of Mathematica, a widely used symbolic
computation system, with a new type system has a certain practical importance.
The approach we propose for Mathematica is inspired from D. Gruntz and M.
Monagan's work in Gauss, for Maple."
"A method is presented that reduces the number of terms of systems of linear
equations (algebraic, ordinary and partial differential equations). As a
byproduct these systems have a tendency to become partially decoupled and are
more likely to be factorizable or integrable. A variation of this method is
applicable to non-linear systems. Modifications to improve efficiency are given
and examples are shown. This procedure can be used in connection with the
computation of the radical of a differential ideal (differential Groebner
basis)."
"Inevitability properties in branching temporal logics are of the syntax
forall eventually \phi, where \phi is an arbitrary (timed) CTL formula. In the
sense that ""good things will happen"", they are parallel to the ""liveness""
properties in linear temporal logics. Such inevitability properties in
dense-time logics can be analyzed with greatest fixpoint calculation. We
present algorithms to model-check inevitability properties both with and
without requirement of non-Zeno computations. We discuss a technique for early
decision on greatest fixpoints in the temporal logics, and experiment with the
effect of non-Zeno computations on the evaluation of greatest fixpoints. We
also discuss the TCTL subclass with only universal path quantifiers which
allows for the safe abstraction analysis of inevitability properties. Finally,
we report our implementation and experiments to show the plausibility of our
ideas."
"Fast algorithms for arithmetic on real or complex polynomials are well-known
and have proven to be not only asymptotically efficient but also very
practical. Based on Fast Fourier Transform (FFT), they for instance multiply
two polynomials of degree up to N or multi-evaluate one at N points
simultaneously within quasi-linear time O(N.polylog N). An extension to (and in
fact the mere definition of) polynomials over the skew-field H of quaternions
is promising but still missing. The present work proposes three such
definitions which in the commutative case coincide but for H turn out to
differ, each one satisfying some desirable properties while lacking others. For
each notion we devise algorithms for according arithmetic; these are
quasi-optimal in that their running times match lower complexity bounds up to
polylogarithmic factors."
"We present a novel scheme to the coverage problem, introducing a quantitative
way to estimate the interaction between a block and its enviroment.This is
achieved by setting a discrete version of Green`s theorem, specially adapted
for Model Checking based verification of integrated circuits.This method is
best suited for the coverage problem since it enables one to quantify the
incompleteness or, on the other hand, the redundancy of a set of rules,
describing the model under verification.Moreover this can be done continuously
throughout the verification process, thus enabling the user to pinpoint the
stages at which incompleteness/redundancy occurs. Although the method is
presented locally on a small hardware example, we additionally show its
possibility to provide precise coverage estimation also for large scale
systems. We compare this method to others by checking it on the same
test-cases."
"We want to achieve efficiency for the exact computation of the dot product of
two vectors over word-size finite fields. We therefore compare the practical
behaviors of a wide range of implementation techniques using different
representations. The techniques used include oating point representations,
discrete logarithms, tabulations, Montgomery reduction, delayed modulus."
"The aim of this work is to show how we can decompose a module (if
decomposable) into an indecomposable module with the help of the minimization
process."
"A prototype for an extensible interactive graphical term manipulation system
is presented that combines pattern matching and nondeterministic evaluation to
provide a convenient framework for doing tedious algebraic manipulations that
so far had to be done manually in a semi-automatic fashion."
"We give a brief introduction to FORM, a symbolic programming language for
massive batch operations, designed by J.A.M. Vermaseren. In particular, we
stress various methods to efficiently use FORM under the UNIX operating system.
Several scripts and examples are given, and suggestions on how to use the vim
editor as development platform."
"In this paper we present our recent work in developing a computer-algebra
tool for systems of partial differential equations (PDEs), termed ""Kranc"". Our
work is motivated by the problem of finding solutions of the Einstein equations
through numerical simulations. Kranc consists of Mathematica based
computer-algebra packages, that facilitate the task of dealing with symbolic
tensorial calculations and realize the conversion of systems of partial
differential evolution equations into parallelized C or Fortran code."
"In this paper, we present a determinist Jordan normal form algorithms based
on the Fadeev formula: \[(\lambda \cdot I-A) \cdot B(\lambda)=P(\lambda) \cdot
I\] where $B(\lambda)$ is $(\lambda \cdot I-A)$'s comatrix and $P(\lambda)$ is
$A$'s characteristic polynomial. This rational Jordan normal form algorithm
differs from usual algorithms since it is not based on the Frobenius/Smith
normal form but rather on the idea already remarked in Gantmacher that the
non-zero column vectors of $B(\lambda_0)$ are eigenvectors of $A$ associated to
$\lambda_0$ for any root $\lambda_0$ of the characteristical polynomial. The
complexity of the algorithm is $O(n^4)$ field operations if we know the
factorization of the characteristic polynomial (or $O(n^5 \ln(n))$ operations
for a matrix of integers of fixed size). This algorithm has been implemented
using the Maple and Giac/Xcas computer algebra systems."
"This article deals with the computation of the characteristic polynomial of
dense matrices over small finite fields and over the integers. We first present
two algorithms for the finite fields: one is based on Krylov iterates and
Gaussian elimination. We compare it to an improvement of the second algorithm
of Keller-Gehrig. Then we show that a generalization of Keller-Gehrig's third
algorithm could improve both complexity and computational time. We use these
results as a basis for the computation of the characteristic polynomial of
integer matrices. We first use early termination and Chinese remaindering for
dense matrices. Then a probabilistic approach, based on integer minimal
polynomial and Hensel factorization, is particularly well suited to sparse
and/or structured matrices."
"GPL Maxima is an open-source computer algebra system based on DOE-MACSYMA.
GPL Maxima included two tensor manipulation packages from DOE-MACSYMA, but
these were in various states of disrepair. One of the two packages, CTENSOR,
implemented component-based tensor manipulation; the other, ITENSOR, treated
tensor symbols as opaque, manipulating them based on their index properties.
The present paper describes the state in which these packages were found, the
steps that were needed to make the packages fully functional again, and the new
functionality that was implemented to make them more versatile. A third
package, ATENSOR, was also implemented; fully compatible with the identically
named package in the commercial version of MACSYMA, ATENSOR implements abstract
tensor algebras."
"We propose a new diagrammatic modeling language, DML. The paradigm used is
that of the category theory and in particular of the pushout tool. We show that
most of the object-oriented structures can be described with this tool and have
many examples in C++, ranging from virtual inheritance and polymorphism to
template genericity. With this powerful tool, we propose a quite simple
description of the C++ LinBox library. This library has been designed for
efficiency and genericity and therefore makes heavy usage of complex template
and polymorphic mecanism. Be reverse engineering, we are able to describe in a
simple manner the complex structure of archetypes in LinBox."
"We report on the status of our project of parallelization of the symbolic
manipulation program FORM. We have now parallel versions of FORM running on
Cluster- or SMP-architectures. These versions can be used to run arbitrary FORM
programs in parallel."
"For a linearly recurrent vector sequence P[n+1] = A(n) * P[n], consider the
problem of calculating either the n-th term P[n] or L<=n arbitrary terms
P[n_1],...,P[n_L], both for the case of constant coefficients A(n)=A and for a
matrix A(n) with entries polynomial in n. We improve and extend known
algorithms for this problem and present new applications for it. Specifically
it turns out that for instance * any family (p_n) of classical orthogonal
polynomials admits evaluation at given x within O(n^{1/2} log n) operations
INDEPENDENT of the family (p_n) under consideration. * For any L indices
n_1,...,n_L <= n, the values p_{n_i}(x) can be calculated simultaneously using
O(n^{1/2} log n + L log(n/L)) arithmetic operations; again this running time
bound holds uniformly. * Every hypergeometric (or, more generally, holonomic)
function admits approximate evaluation up to absolute error e>0 within
O((log(1/e)^{1/2} loglog(1/e)) -- as opposed to O(log(1/e)) -- arithmetic
steps. * Given m and a polynomial p of degree d over a field of characteristic
zero, the coefficient of p^m to term X^n can be computed within O(d^2
M(n^{1/2})) steps where M(n) denotes the cost of multiplying two degree-n
polynomials. * The same time bound holds for the joint calculation of any
L<=n^{1/2} desired coefficients of p^m to terms X^{n_i}, n_1,...,n_L <= n."
"We present an algorithm computing the determinant of an integer matrix A. The
algorithm is introspective in the sense that it uses several distinct
algorithms that run in a concurrent manner. During the course of the algorithm
partial results coming from distinct methods can be combined. Then, depending
on the current running time of each method, the algorithm can emphasize a
particular variant. With the use of very fast modular routines for linear
algebra, our implementation is an order of magnitude faster than other existing
implementations. Moreover, we prove that the expected complexity of our
algorithm is only O(n^3 log^{2.5}(n ||A||)) bit operations in the dense case
and O(Omega n^{1.5} log^2(n ||A||) + n^{2.5}log^3(n||A||)) in the sparse case,
where ||A|| is the largest entry in absolute value of the matrix and Omega is
the cost of matrix-vector multiplication in the case of a sparse matrix."
"In the past two decades, some major efforts have been made to reduce exact
(e.g. integer, rational, polynomial) linear algebra problems to matrix
multiplication in order to provide algorithms with optimal asymptotic
complexity. To provide efficient implementations of such algorithms one need to
be careful with the underlying arithmetic. It is well known that modular
techniques such as the Chinese remainder algorithm or the p-adic lifting allow
very good practical performance, especially when word size arithmetic are used.
Therefore, finite field arithmetic becomes an important core for efficient
exact linear algebra libraries. In this paper, we study high performance
implementations of basic linear algebra routines over word size prime fields:
specially the matrix multiplication; our goal being to provide an exact
alternate to the numerical BLAS library. We show that this is made possible by
a carefull combination of numerical computations and asymptotically faster
algorithms. Our kernel has several symbolic linear algebra applications enabled
by diverse matrix multiplication reductions: symbolic triangularization, system
solving, determinant and matrix inverse implementations are thus studied."
"In this paper, a set of programs enhancing the Kenzo system is presented.
Kenzo is a Common Lisp program designed for computing in Algebraic Topology, in
particular it allows the user to calculate homology and homotopy groups of
complicated spaces. The new programs presented here entirely compute Serre and
Eilenberg-Moore spectral sequences, in particular the groups and differential
maps for arbitrary r. They also determine when the spectral sequence has
converged and describe the filtration of the target homology groups induced by
the spectral sequence."
"We propose a new algorithm to solve sparse linear systems of equations over
the integers. This algorithm is based on a $p$-adic lifting technique combined
with the use of block matrices with structured blocks. It achieves a sub-cubic
complexity in terms of machine operations subject to a conjecture on the
effectiveness of certain sparse projections. A LinBox-based implementation of
this algorithm is demonstrated, and emphasizes the practical benefits of this
new method over the previous state of the art."
"Lie group theory states that knowledge of a $m$-parameters solvable group of
symmetries of a system of ordinary differential equations allows to reduce by
$m$ the number of equation. We apply this principle by finding dilatations and
translations that are Lie point symmetries of considered ordinary differential
system. By rewriting original problem in an invariant coordinates set for these
symmetries, one can reduce the involved number of parameters. This process is
classically call nondimensionalisation in dimensional analysis. We present an
algorithm based on this standpoint and show that its arithmetic complexity is
polynomial in input's size."
"We propose new algorithms for the computation of the first N terms of a
vector (resp. a basis) of power series solutions of a linear system of
differential equations at an ordinary point, using a number of arithmetic
operations which is quasi-linear with respect to N. Similar results are also
given in the non-linear case. This extends previous results obtained by Brent
and Kung for scalar differential equations of order one and two."
"This seminar report is concerned with expressing LPO-termination of term
rewrite systems as a satisfiability problem in propositional logic. After
relevant algorithms are explained, experimental results are reported."
"We consider two kinds of problems: the computation of polynomial and rational
solutions of linear recurrences with coefficients that are polynomials with
integer coefficients; indefinite and definite summation of sequences that are
hypergeometric over the rational numbers. The algorithms for these tasks all
involve as an intermediate quantity an integer $N$ (dispersion or root of an
indicial polynomial) that is potentially exponential in the bit size of their
input. Previous algorithms have a bit complexity that is at least quadratic in
$N$. We revisit them and propose variants that exploit the structure of
solutions and avoid expanding polynomials of degree $N$. We give two
algorithms: a probabilistic one that detects the existence or absence of
nonzero polynomial and rational solutions in $O(\sqrt{N}\log^{2}N)$ bit
operations; a deterministic one that computes a compact representation of the
solution in $O(N\log^{3}N)$ bit operations. Similar speed-ups are obtained in
indefinite and definite hypergeometric summation. We describe the results of an
implementation."
"Consider a system of n polynomial equations and r polynomial inequations in n
indeterminates of degree bounded by d with coefficients in a polynomial ring of
s parameters with rational coefficients of bit-size at most $\sigma$. From the
real viewpoint, solving such a system often means describing some
semi-algebraic sets in the parameter space over which the number of real
solutions of the considered parametric system is constant. Following the works
of Lazard and Rouillier, this can be done by the computation of a discriminant
variety. In this report we focus on the case where for a generic specialization
of the parameters the system of equations generates a radical zero-dimensional
ideal, which is usual in the applications. In this case, we provide a
deterministic method computing the minimal discriminant variety reducing the
problem to a problem of elimination. Moreover, we prove that the degree of the
computed minimal discriminant variety is bounded by $D:=(n+r)d^{(n+1)}$ and
that the complexity of our method is $\sigma^{\mathcal{O}(1)}
D^{\mathcal{O}(n+s)}$ bit-operations on a deterministic Turing machine."
"Let f1, ..., fs be a polynomial family in Q[X1,..., Xn] (with s less than n)
of degree bounded by D. Suppose that f1, ..., fs generates a radical ideal, and
defines a smooth algebraic variety V. Consider a projection P. We prove that
the degree of the critical locus of P restricted to V is bounded by
D^s(D-1)^(n-s) times binomial of n and n-s. This result is obtained in two
steps. First the critical points of P restricted to V are characterized as
projections of the solutions of Lagrange's system for which a bi-homogeneous
structure is exhibited. Secondly we prove a bi-homogeneous B\'ezout Theorem,
which bounds the sum of the degrees of the equidimensional components of the
radical of an ideal generated by a bi-homogeneous polynomial family. This
result is improved when f1,..., fs is a regular sequence. Moreover, we use
Lagrange's system to design an algorithm computing at least one point in each
connected component of a smooth real algebraic set. This algorithm generalizes,
to the non equidimensional case, the one of Safey El Din and Schost. The
evaluation of the output size of this algorithm gives new upper bounds on the
first Betti number of a smooth real algebraic set. Finally, we estimate its
arithmetic complexity and prove that in the worst cases it is polynomial in n,
s, D^s(D-1)^(n-s) and the binomial of n and n-s, and the complexity of
evaluation of f1,..., fs."
"This note presents absolute bounds on the size of the coefficients of the
characteristic and minimal polynomials depending on the size of the
coefficients of the associated matrix. Moreover, we present algorithms to
compute more precise input-dependant bounds on these coefficients. Such bounds
are e.g. useful to perform deterministic chinese remaindering of the
characteristic or minimal polynomial of an integer matrix."
"In this paper we consider systems of partial (multidimensional) linear
difference equations. Specifically, such systems arise in scientific computing
under discretization of linear partial differential equations and in
computational high energy physics as recurrence relations for multiloop Feynman
integrals. The most universal algorithmic tool for investigation of linear
difference systems is based on their transformation into an equivalent Groebner
basis form. We present an algorithm for this transformation implemented in
Maple. The algorithm and its implementation can be applied to automatic
generation of difference schemes for linear partial differential equations and
to reduction of Feynman integrals. Some illustrative examples are given."
"Lie group theory states that knowledge of a $m$-parameters solvable group of
symmetries of a system of ordinary differential equations allows to reduce by
$m$ the number of equations. We apply this principle by finding some
\emph{affine derivations} that induces \emph{expanded} Lie point symmetries of
considered system. By rewriting original problem in an invariant coordinates
set for these symmetries, we \emph{reduce} the number of involved parameters.
We present an algorithm based on this standpoint whose arithmetic complexity is
\emph{quasi-polynomial} in input's size."
"Schur's transforms of a polynomial are used to count its roots in the unit
disk. These are generalized them by introducing the sequence of symmetric
sub-resultants of two polynomials. Although they do have a determinantal
definition, we show that they satisfy a structure theorem which allows us to
compute them with a type of Euclidean division. As a consequence, a fast
algorithm based on a dichotomic process and FFT is designed. We prove also that
these symmetric sub-resultants have a deep link with Toeplitz matrices.
Finally, we propose a new algorithm of inversion for such matrices. It has the
same cost as those already known, however it is fraction-free and consequently
well adapted to computer algebra."
"The currently best known algorithms for the numerical evaluation of
hypergeometric constants such as $\zeta(3)$ to $d$ decimal digits have time
complexity $O(M(d) \log^2 d)$ and space complexity of $O(d \log d)$ or $O(d)$.
Following work from Cheng, Gergel, Kim and Zima, we present a new algorithm
with the same asymptotic complexity, but more efficient in practice. Our
implementation of this algorithm improves slightly over existing programs for
the computation of $\pi$, and we announce a new record of 2 billion digits for
$\zeta(3)$."
"The aim of the present paper is to propose an algorithm for a new ODE--solver
which should improve the abilities of current solvers to handle second order
differential equations. The paper provides also a theoretical result revealing
the relationship between the change of coordinates, that maps the generic
equation to a given target equation, and the symmetry $\D$-groupoid of this
target."
"Formal proof checkers such as Coq are capable of validating proofs of
correction of algorithms for finite field arithmetics but they require
extensive training from potential users. The delayed solution of a triangular
system over a finite field mixes operations on integers and operations on
floating point numbers. We focus in this report on verifying proof obligations
that state that no round off error occurred on any of the floating point
operations. We use a tool named Gappa that can be learned in a matter of
minutes to generate proofs related to floating point arithmetic and hide
technicalities of formal proof checkers. We found that three facilities are
missing from existing tools. The first one is the ability to use in Gappa new
lemmas that cannot be easily expressed as rewriting rules. We coined the second
one ``variable interchange'' as it would be required to validate loop
interchanges. The third facility handles massive loop unrolling and argument
instantiation by generating traces of execution for a large number of cases. We
hope that these facilities may sometime in the future be integrated into
mainstream code validation."
"In this paper we propose several strategies for the exact computation of the
determinant of a rational matrix. First, we use the Chinese Remaindering
Theorem and the rational reconstruction to recover the rational determinant
from its modular images. Then we show a preconditioning for the determinant
which allows us to skip the rational reconstruction process and reconstruct an
integer result. We compare those approaches with matrix preconditioning which
allow us to treat integer instead of rational matrices. This allows us to
introduce integer determinant algorithms to the rational determinant problem.
In particular, we discuss the applicability of the adaptive determinant
algorithm of [9] and compare it with the integer Chinese Remaindering scheme.
We present an analysis of the complexity of the strategies and evaluate their
experimental performance on numerous examples. This experience allows us to
develop an adaptive strategy which would choose the best solution at the run
time, depending on matrix properties. All strategies have been implemented in
LinBox linear algebra library."
"We present an algorithm to perform a simultaneous modular reduction of
several residues. This algorithm is applied fast modular polynomial
multiplication. The idea is to convert the $X$-adic representation of modular
polynomials, with $X$ an indeterminate, to a $q$-adic representation where $q$
is an integer larger than the field characteristic. With some control on the
different involved sizes it is then possible to perform some of the $q$-adic
arithmetic directly with machine integers or floating points. Depending also on
the number of performed numerical operations one can then convert back to the
$q$-adic or $X$-adic representation and eventually mod out high residues. In
this note we present a new version of both conversions: more tabulations and a
way to reduce the number of divisions involved in the process are presented.
The polynomial multiplication is then applied to arithmetic in small finite
field extensions."
"We start with elementary algebraic theory of factorization of linear ordinary
differential equations developed in the period 1880-1930. After exposing these
classical results we sketch more sophisticated algorithmic approaches developed
in the last 20 years.
  The main part of this paper is devoted to modern generalizations of the
notion of factorization to the case of systems of linear partial differential
equations and their relation with explicit solvability of nonlinear partial
differential equations based on some constructions from the theory of abelian
categories."
"Given the implicit equation $F(x,y,t,s)$ of a family of algebraic plane
curves depending on the parameters $t,s$, we provide an algorithm for studying
the topology types arising in the family. For this purpose, the algorithm
computes a finite partition of the parameter space so that the topology type of
the family stays invariant over each element of the partition. The ideas
contained in the paper can be seen as a generalization of the ideas in
\cite{JGRS}, where the problem is solved for families of algebraic curves
depending on one parameter, to the two-parameters case."
"In eye movement research in reading, the amount of data plays a crucial role
for the validation of results. A methodological problem for the analysis of the
eye movement in reading are blinks, when readers close their eyes. Blinking
rate increases with increasing reading time, resulting in high data losses,
especially for older adults or reading impaired subjects. We present a method,
based on the symbolic sequence dynamics of the eye movements, that reconstructs
the horizontal position of the eyes while the reader blinks. The method makes
use of an observed fact that the movements of the eyes before closing or after
opening contain information about the eyes movements during blinks. Test
results indicate that our reconstruction method is superior to methods that use
simpler interpolation approaches. In addition, analyses of the reconstructed
data show no significant deviation from the usual behavior observed in readers."
"We propose to store several integers modulo a small prime into a single
machine word. Modular addition is performed by addition and possibly
subtraction of a word containing several times the modulo. Modular
Multiplication is not directly accessible but modular dot product can be
performed by an integer multiplication by the reverse integer. Modular
multiplication by a word containing a single residue is a also possible.
Therefore matrix multiplication can be performed on such a compressed storage.
We here give bounds on the sizes of primes and matrices for which such a
compression is possible. We also explicit the details of the required
compressed arithmetic routines."
"We have designed a new symbolic-numeric strategy to compute efficiently and
accurately floating point Puiseux series defined by a bivariate polynomial over
an algebraic number field. In essence, computations modulo a well chosen prime
$p$ are used to obtain the exact information required to guide floating point
computations. In this paper, we detail the symbolic part of our algorithm:
First of all, we study modular reduction of Puiseux series and give a good
reduction criterion to ensure that the information required by the numerical
part is preserved. To establish our results, we introduce a simple modification
of classical Newton polygons, that we call ""generic Newton polygons"", which
happen to be very convenient. Then, we estimate the arithmetic complexity of
computing Puiseux series over finite fields and improve known bounds. Finally,
we give bit-complexity bounds for deterministic and randomized versions of the
symbolic part. The details of the numerical part will be described in a
forthcoming paper."
"We present an algorithm for computing a Smith form with multipliers of a
regular matrix polynomial over a field. This algorithm differs from previous
ones in that it computes a local Smith form for each irreducible factor in the
determinant separately and then combines them into a global Smith form, whereas
other algorithms apply a sequence of unimodular row and column operations to
the original matrix. The performance of the algorithm in exact arithmetic is
reported for several test cases."
"We have developed two computer algebra systems, meditor [Jolly:2007] and JAS
[Kredel:2006]. These CAS systems are available as Java libraries. For the
use-case of interactively entering and manipulating mathematical expressions,
there is a need of a scripting front-end for our libraries. Most other CAS
invent and implement their own scripting interface for this purpose. We,
however, do not want to reinvent the wheel and propose to use a contemporary
scripting language with access to Java code. In this paper we discuss the
requirements for a scripting language in computer algebra and check whether the
languages Python, Ruby, Groovy and Scala meet these requirements. We conclude
that, with minor problems, any of these languages is suitable for our purpose."
"We consider solutions to the equation f = h^r for polynomials f and h and
integer r > 1. Given a polynomial f in the lacunary (also called sparse or
super-sparse) representation, we first show how to determine if f can be
written as h^r and, if so, to find such an r. This is a Monte Carlo randomized
algorithm whose cost is polynomial in the number of non-zero terms of f and in
log(deg f), i.e., polynomial in the size of the lacunary representation, and it
works over GF(q)[x] (for large characteristic) as well as Q[x]. We also give
two deterministic algorithms to compute the perfect root h given f and r. The
first is output-sensitive (based on the sparsity of h) and works only over
Q[x]. A sparsity-sensitive Newton iteration forms the basis for the second
approach to computing h, which is extremely efficient and works over both
GF(q)[x] (for large characteristic) and Q[x], but depends on a number-theoretic
conjecture. Work of Erdos, Schinzel, Zannier, and others suggests that both of
these algorithms are unconditionally polynomial-time in the lacunary size of
the input polynomial f. Finally, we demonstrate the efficiency of the
randomized detection algorithm and the latter perfect root computation
algorithm with an implementation in the C++ library NTL."
"We address complexity issues for linear differential equations in
characteristic $p>0$: resolution and computation of the $p$-curvature. For
these tasks, our main focus is on algorithms whose complexity behaves well with
respect to $p$. We prove bounds linear in $p$ on the degree of polynomial
solutions and propose algorithms for testing the existence of polynomial
solutions in sublinear time $\tilde{O}(p^{1/2})$, and for determining a whole
basis of the solution space in quasi-linear time $\tilde{O}(p)$; the
$\tilde{O}$ notation indicates that we hide logarithmic factors. We show that
for equations of arbitrary order, the $p$-curvature can be computed in
subquadratic time $\tilde{O}(p^{1.79})$, and that this can be improved to
$O(\log(p))$ for first order equations and to $\tilde{O}(p)$ for classes of
second order equations."
"We present algorithms and heuristics to compute the characteristic polynomial
of a matrix given its minimal polynomial. The matrix is represented as a
black-box, i.e., by a function to compute its matrix-vector product. The
methods apply to matrices either over the integers or over a large enough
finite field. Experiments show that these methods perform efficiently in
practice. Combined in an adaptive strategy, these algorithms reach significant
speedups in practice for some integer matrices arising in an application from
graph theory."
"We consider the problem of constructing roadmaps of real algebraic sets. The
problem was introduced by Canny to answer connectivity questions and solve
motion planning problems. Given $s$ polynomial equations with rational
coefficients, of degree $D$ in $n$ variables, Canny's algorithm has a Monte
Carlo cost of $s^n\log(s) D^{O(n^2)}$ operations in $\mathbb{Q}$; a
deterministic version runs in time $s^n \log(s) D^{O(n^4)}$. The next
improvement was due to Basu, Pollack and Roy, with an algorithm of
deterministic cost $s^{d+1} D^{O(n^2)}$ for the more general problem of
computing roadmaps of semi-algebraic sets ($d \le n$ is the dimension of an
associated object). We give a Monte Carlo algorithm of complexity
$(nD)^{O(n^{1.5})}$ for the problem of computing a roadmap of a compact
hypersurface $V$ of degree $D$ in $n$ variables; we also have to assume that
$V$ has a finite number of singular points. Even under these extra assumptions,
no previous algorithm featured a cost better than $D^{O(n^2)}$."
"We consider the problem of bounding away from 0 the minimum value m taken by
a polynomial P of Z[X_1,...,X_k] over the standard simplex, assuming that m>0.
Recent algorithmic developments in real algebraic geometry enable us to obtain
a positive lower bound on m in terms of the dimension k, the degree d and the
bitsize of the coefficients of P. The bound is explicit, and obtained without
any extra assumption on P, in contrast with previous results reported in the
literature."
"Examples show that integral forms can be efficiently proved positive
semidefinite by the WDS method, but it was unknown that how many steps of
substitutions are needed, or furthermore, which integral forms is this method
applicable for. In this paper, we give upper bounds of step numbers of WDS
required in proving that an integral form is positive definite, positive
semidefinite, or not positive semidefinite, thus deducing that the WDS method
is complete."
"This paper mainly studies nonnegativity decision of forms based on variable
substitutions. Unlike existing research, the paper regards simplex subdivisions
as new perspectives to study variable substitutions, gives some subdivisions of
the simplex T_n, introduces the concept of convergence of the subdivision
sequence, and presents a sufficient and necessary condition for the convergent
self-similar subdivision sequence. Then the relationships between subdivisions
and their corresponding substitutions are established. Moreover, it is proven
that if the form F is indefinite on T_n and the sequence of the successive
L-substitution sets is convergent, then the sequence of sets {SLS^(m)(F)} is
negatively terminating, and an algorithm for deciding indefinite forms with a
counter-example is obtained. Thus, various effective substitutions for deciding
positive semi-definite forms and indefinite forms are gained, which are beyond
the weighted difference substitutions characterized by ""difference""."
"This article describes the implementation in the software package NumGfun of
classical algorithms that operate on solutions of linear differential equations
or recurrence relations with polynomial coefficients, including what seems to
be the first general implementation of the fast high-precision numerical
evaluation algorithms of Chudnovsky & Chudnovsky. In some cases, our
descriptions contain improvements over existing algorithms. We also provide
references to relevant ideas not currently used in NumGfun."
"We consider two algorithms which can be used for proving positivity of
sequences that are defined by a linear recurrence equation with polynomial
coefficients (P-finite sequences). Both algorithms have in common that while
they do succeed on a great many examples, there is no guarantee for them to
terminate, and they do in fact not terminate for every input. For some
restricted classes of P-finite recurrence equations of order up to three we
provide a priori criteria that assert the termination of the algorithms."
"We investigate which polynomials can possibly occur as factors in the
denominators of rational solutions of a given partial linear difference
equation (PLDE). Two kinds of polynomials are to be distinguished, we call them
/periodic/ and /aperiodic/. The main result is a generalization of a well-known
denominator bounding technique for univariate equations to PLDEs. This
generalization is able to find all the aperiodic factors of the denominators
for a given PLDE."
"Our probabilistic analysis sheds light to the following questions: Why do
random polynomials seem to have few, and well separated real roots, on the
average? Why do exact algorithms for real root isolation may perform
comparatively well or even better than numerical ones? We exploit results by
Kac, and by Edelman and Kostlan in order to estimate the real root separation
of degree $d$ polynomials with i.i.d.\ coefficients that follow two zero-mean
normal distributions: for SO(2) polynomials, the $i$-th coefficient has
variance ${d \choose i}$, whereas for Weyl polynomials its variance is
${1/i!}$. By applying results from statistical physics, we obtain the expected
(bit) complexity of \func{sturm} solver, $\sOB(r d^2 \tau)$, where $r$ is the
number of real roots and $\tau$ the maximum coefficient bitsize. Our bounds are
two orders of magnitude tighter than the record worst case ones. We also derive
an output-sensitive bound in the worst case. The second part of the paper shows
that the expected number of real roots of a degree $d$ polynomial in the
Bernstein basis is $\sqrt{2d}\pm\OO(1)$, when the coefficients are i.i.d.\
variables with moderate standard deviation. Our paper concludes with
experimental results which corroborate our analysis."
"In this paper we derive aggregate separation bounds, named after
Davenport-Mahler-Mignotte (\dmm), on the isolated roots of polynomial systems,
specifically on the minimum distance between any two such roots. The bounds
exploit the structure of the system and the height of the sparse (or toric)
resultant by means of mixed volume, as well as recent advances on aggregate
root bounds for univariate polynomials, and are applicable to arbitrary
positive dimensional systems. We improve upon Canny's gap theorem
\cite{c-crmp-87} by a factor of $\OO(d^{n-1})$, where $d$ bounds the degree of
the polynomials, and $n$ is the number of variables. One application is to the
bitsize of the eigenvalues and eigenvectors of an integer matrix, which also
yields a new proof that the problem is polynomial. We also compare against
recent lower bounds on the absolute value of the root coordinates by Brownawell
and Yap \cite{by-issac-2009}, obtained under the hypothesis there is a
0-dimensional projection. Our bounds are in general comparable, but exploit
sparseness; they are also tighter when bounding the value of a positive
polynomial over the simplex. For this problem, we also improve upon the bounds
in \cite{bsr-arxix-2009,jp-arxiv-2009}. Our analysis provides a precise
asymptotic upper bound on the number of steps that subdivision-based algorithms
perform in order to isolate all real roots of a polynomial system. This leads
to the first complexity bound of Milne's algorithm \cite{Miln92} in 2D."
"The famous F5 algorithm for computing \gr basis was presented by Faug\`ere in
2002. The original version of F5 is given in programming codes, so it is a bit
difficult to understand. In this paper, the F5 algorithm is simplified as F5B
in a Buchberger's style such that it is easy to understand and implement. In
order to describe F5B, we introduce F5-reduction, which keeps the signature of
labeled polynomials unchanged after reduction. The equivalence between F5 and
F5B is also shown. At last, some versions of the F5 algorithm are illustrated."
"We consider the problem of isolating the real roots of a square-free
polynomial with integer coefficients using (variants of) the continued fraction
algorithm (CF). We introduce a novel way to compute a lower bound on the
positive real roots of univariate polynomials. This allows us to derive a worst
case bound of $\sOB(d^6 + d^4\tau^2 + d^3\tau^2)$ for isolating the real roots
of a polynomial with integer coefficients using the classic variant of CF,
where $d$ is the degree of the polynomial and $\tau$ the maximum bitsize of its
coefficients. This improves the previous bound by Sharma \cite{sharma-tcs-2008}
by a factor of $d^3$ and matches the bound derived by Mehlhorn and Ray
\cite{mr-jsc-2009} for another variant of CF; it also matches the worst case
bound of the subdivision-based solvers. We present a new variant of CF, we call
it iCF, that isolates the real roots of a polynomial with integer coefficients
in $\sOB(d^5+d^4\tau)$, thus improving the current known bound for the problem
by a factor of $d$. If the polynomial has only real roots, then our bound
becomes $\sOB(d^4+d^3\tau+ d^2\tau^2)$, thus matching the bound of the
numerical algorithms by Reif \cite{r-focs-1993} and by Ben-Or and Tiwari
\cite{bt-joc-1990}. Actually the latter bound holds in a more general setting,
that is under the rather mild assumption that $\Omega(d/\lg^c{d})$, where
$c\geq 0$ is a constant, roots contribute to the sign variations of the
coefficient list of the polynomial. This is the only bound on exact algorithms
that matches the one of the numerical algorithms by Pan \cite{Pan02jsc} and
Sch\""onhage \cite{Sch82}. To our knowledge the presented bounds are the best
known for the problem of real root isolation for algorithms based on exact
computations."
"We consider the problem of isolating the real roots of a square-free
polynomial with integer coefficients using (variants of) the continued fraction
algorithm (CF). We introduce a novel way to compute a lower bound on the
positive real roots of univariate polynomials. This allows us to derive a worst
case bound of $\sOB(d^6 + d^4\tau^2 + d^3\tau^2)$ for isolating the real roots
of a polynomial with integer coefficients using the classic variant
\cite{Akritas:implementation} of CF, where $d$ is the degree of the polynomial
and $\tau$ the maximum bitsize of its coefficients. This improves the previous
bound of Sharma \cite{sharma-tcs-2008} by a factor of $d^3$ and matches the
bound derived by Mehlhorn and Ray \cite{mr-jsc-2009} for another variant of CF;
it also matches the worst case bound of the subdivision-based solvers."
"We continue to investigate which polynomials can possibly occur as factors in
the denominators of rational solutions of a given partial linear difference
equation.
  In an earlier article we had introduced the distinction between periodic and
aperiodic factors in the denominator, and we gave an algorithm for predicting
the aperiodic ones. Now we extend this technique towards the periodic case and
present a refined algorithm which also finds most of the periodic factors."
"We develop a new symbolic-numeric algorithm for the certification of singular
isolated points, using their associated local ring structure and certified
numerical computations. An improvement of an existing method to compute inverse
systems is presented, which avoids redundant computation and reduces the size
of the intermediate linear systems to solve. We derive a one-step deflation
technique, from the description of the multiplicity structure in terms of
differentials. The deflated system can be used in Newton-based iterative
schemes with quadratic convergence. Starting from a polynomial system and a
small-enough neighborhood, we obtain a criterion for the existence and
uniqueness of a singular root of a given multiplicity structure, applying a
well-chosen symbolic perturbation. Standard verification methods, based eg. on
interval arithmetic and a fixed point theorem, are employed to certify that
there exists a unique perturbed system with a singular root in the domain.
Applications to topological degree computation and to the analysis of real
branches of an implicit curve illustrate the method."
"A generalized criterion for signature related algorithms to compute Gr\""obner
basis is proposed in this paper. Signature related algorithms are a popular
kind of algorithms for computing Gr\""obner basis, including the famous F5
algorithm, the extended F5 algorithm and the GVW algorithm. The main purpose of
current paper is to study in theory what kind of criteria is correct in
signature related algorithms and provide a generalized method to develop new
criteria. For this purpose, a generalized criterion is proposed. The
generalized criterion only relies on a general partial order defined on a set
of polynomials. When specializing the partial order to appropriate specific
orders, the generalized criterion can specialize to almost all existing
criteria of signature related algorithms. For {\em admissible} partial orders,
a complete proof of the correctness of the algorithm based on this generalized
criterion is also presented. This proof has no extra requirements on the
computing order of critical pairs, and is also valid for non-homogeneous
polynomial systems. More importantly, the partial orders implied by existing
criteria are admissible. Besides, one can also check whether a new criterion is
correct in signature related algorithms or even develop new criteria by using
other admissible partial orders in the generalized criterion."
"In this paper, a multiplicity preserving triangular set decomposition
algorithm is proposed for a system of two polynomials. The algorithm decomposes
the variety defined by the polynomial system into unmixed components
represented by triangular sets, which may have negative multiplicities. In the
bivariate case, we give a complete algorithm to decompose the system into
multiplicity preserving triangular sets with positive multiplicities. We also
analyze the complexity of the algorithm in the bivariate case. We implement our
algorithm and show the effectiveness of the method with extensive experiments."
"We consider the problem of approximating all real roots of a square-free
polynomial $f$. Given isolating intervals, our algorithm refines each of them
to a width of $2^{-L}$ or less, that is, each of the roots is approximated to
$L$ bits after the binary point. Our method provides a certified answer for
arbitrary real polynomials, only considering finite approximations of the
polynomial coefficients and choosing a suitable working precision adaptively.
In this way, we get a correct algorithm that is simple to implement and
practically efficient. Our algorithm uses the quadratic interval refinement
method; we adapt that method to be able to cope with inaccuracies when
evaluating $f$, without sacrificing its quadratic convergence behavior. We
prove a bound on the bit complexity of our algorithm in terms of the degree of
the polynomial, the size and the separation of the roots, that is, parameters
exclusively related to the geometric location of the roots. Our bound is near
optimal and significantly improves previous work on integer polynomials.
Furthermore, it essentially matches the best known theoretical bounds on root
approximation which are obtained by very sophisticated algorithms. We also
investigate the practical behavior of the algorithm and demonstrate how closely
the practical performance matches our asymptotic bounds."
"Computing the topology of an algebraic plane curve $\mathcal{C}$ means to
compute a combinatorial graph that is isotopic to $\mathcal{C}$ and thus
represents its topology in $\mathbb{R}^2$. We prove that, for a polynomial of
degree $n$ with coefficients bounded by $2^\rho$, the topology of the induced
curve can be computed with $\tilde{O}(n^8(n+\rho^2))$ bit operations
deterministically, and with $\tilde{O}(n^8\rho^2)$ bit operations with a
randomized algorithm in expectation. Our analysis improves previous best known
complexity bounds by a factor of $n^2$. The improvement is based on new
techniques to compute and refine isolating intervals for the real roots of
polynomials, and by the consequent amortized analysis of the critical fibers of
the algebraic curve."
"A generalized criterion for signature-based algorithms to compute Gr\""obner
bases is proposed in this paper. This criterion is named by ""generalized
criterion"", because it can be specialized to almost all existing criteria for
signature-based algorithms which include the famous F5 algorithm, F5C, extended
F5, G$^2$V and the GVW algorithm. The main purpose of current paper is to study
in theory which kind of criteria is correct in signature-based algorithms and
provide a generalized method to develop new criteria. For this purpose, by
studying some key facts and observations of signature-based algorithms, a
generalized criterion is proposed. The generalized criterion only relies on a
partial order defined on a set of polynomials. When specializing the partial
order to appropriate specific orders, the generalized criterion can specialize
to almost all existing criteria of signature-based algorithms. For {\em
admissible} partial orders, a proof is presented for the correctness of the
algorithm that is based on this generalized criterion. And the partial orders
implied by the criteria of F5 and GVW are also shown to be admissible. More
importantly, the generalized criterion provides an effective method to check
whether a new criterion is correct as well as to develop new criteria for
signature-based algorithms."
"Signature-based algorithms are a popular kind of algorithms for computing
Groebner basis, including the famous F5 algorithm, F5C, extended F5, G2V and
the GVW algorithm. In this paper, an efficient method is proposed to solve the
detachability problem. The new method only uses the outputs of signature-based
algorithms, and no extra Groebner basis computations are needed. When a
Groebner basis is obtained by signature-based algorithms, the detachability
problem can be settled in polynomial time."
"We analyze the differential equations produced by the method of creative
telescoping applied to a hyperexponential term in two variables. We show that
equations of low order have high degree, and that higher order equations have
lower degree. More precisely, we derive degree bounding formulas which allow to
estimate the degree of the output equations from creative telescoping as a
function of the order. As an application, we show how the knowledge of these
formulas can be used to improve, at least in principle, the performance of
creative telescoping implementations, and we deduce bounds on the asymptotic
complexity of creative telescoping for hyperexponential terms."
"We give an algorithm for reversion of formal power series, based on an
efficient way to implement the Lagrange inversion formula. Our algorithm
requires $O(n^{1/2}(M(n) + MM(n^{1/2})))$ operations where $M(n)$ and $MM(n)$
are the costs of polynomial and matrix multiplication respectively. This
matches the asymptotic complexity of an algorithm of Brent and Kung, but we
achieve a constant factor speedup whose magnitude depends on the polynomial and
matrix multiplication algorithms used. Benchmarks confirm that the algorithm
performs well in practice."
"In computer algebra there are different ways of approaching the mathematical
concept of functions, one of which is by defining them as solutions of
differential equations. We compare different such approaches and discuss the
occurring problems. The main focus is on the question of determining possible
branch cuts. We explore the extent to which the treatment of branch cuts can be
rendered (more) algorithmic, by adapting Kahan's rules to the differential
equation setting."
"Let R=F[D;sigma,delta] be the ring of Ore polynomials over a field (or skew
field) F, where sigma is a automorphism of F and delta is a sigma-derivation.
Given a an m by n matrix A over R, we show how to compute the Hermite form H of
A and a unimodular matrix U such that UA=H. The algorithm requires a polynomial
number of operations in F in terms of both the dimensions m and n, and the
degree of the entries in A. When F=k(z) for some field k, it also requires time
polynomial in the degree in z, and if k is the rational numbers Q, it requires
time polynomial in the bit length of the coefficients as well. Explicit
analyses are provided for the complexity, in particular for the important cases
of differential and shift polynomials over Q(z). To accomplish our algorithm,
we apply the Dieudonne determinant and quasideterminant theory for Ore
polynomial rings to get explicit bounds on the degrees and sizes of entries in
H and U."
"We study the complexity of some fundamental operations for triangular sets in
dimension zero. Using Las-Vegas algorithms, we prove that one can perform such
operations as change of order, equiprojectable decomposition, or quasi-inverse
computation with a cost that is essentially that of modular composition. Over
an abstract field, this leads to a subquadratic cost (with respect to the
degree of the underlying algebraic set). Over a finite field, in a boolean RAM
model, we obtain a quasi-linear running time using Kedlaya and Umans' algorithm
for modular composition. Conversely, we also show how to reduce the problem of
modular composition to change of order for triangular sets, so that all these
problems are essentially equivalent. Our algorithms are implemented in Maple;
we present some experimental results."
"We generalize the structural theorem of Lazard in 1985, from 2 variables to 3
variables. We use the Gianni-Kalkbrener result to do this, which implies some
restrictions inside which lies the case of a radical ideal."
"The classical division algorithm for polynomials requires $O(n^2)$ operations
for inputs of size $n$. Using reversal technique and Newton iteration, it can
be improved to $O({M}(n))$, where ${M}$ is a multiplication time. But the
method requires that the degree of the modulo, $x^l$, should be the power of 2.
If $l$ is not a power of 2 and $f(0)=1$, Gathen and Gerhard suggest to compute
the inverse,$f^{-1}$, modulo $x^{\lceil l/2^r\rceil}, x^{\lceil
l/2^{r-1}\rceil},..., x^{\lceil l/2\rceil}, x^l$, separately. But they did not
specify the iterative step. In this note, we show that the original Newton
iteration formula can be directly used to compute $f^{-1}\,{mod}\,x^{l}$
without any additional cost, when $l$ is not a power of 2."
"We study the complexity of solving the \emph{generalized MinRank problem},
i.e. computing the set of points where the evaluation of a polynomial matrix
has rank at most $r$. A natural algebraic representation of this problem gives
rise to a \emph{determinantal ideal}: the ideal generated by all minors of size
$r+1$ of the matrix. We give new complexity bounds for solving this problem
using Gr\""obner bases algorithms under genericity assumptions on the input
matrix. In particular, these complexity bounds allow us to identify families of
generalized MinRank problems for which the arithmetic complexity of the solving
process is polynomial in the number of solutions. We also provide an algorithm
to compute a rational parametrization of the variety of a 0-dimensional and
radical system of bi-degree $(D,1)$. We show that its complexity can be bounded
by using the complexity bounds for the generalized MinRank problem."
"We present an algorithm for tests generation tools based on symbolic
execution. The algorithm is supposed to help in situations, when a tool is
repeatedly failing to cover some code by tests. The algorithm then provides the
tool a necessary condition strongly narrowing space of program paths, which
must be checked for reaching the uncovered code. We also discuss integration of
the algorithm into the tools and we provide experimental results showing a
potential of the algorithm to be valuable in the tools, when properly
implemented there."
"Elementary tutorial on implementation aspects of Gr\""obner bases computation."
"Creative telescoping applied to a bivariate proper hypergeometric term
produces linear recurrence operators with polynomial coefficients, called
telescopers. We provide bounds for the degrees of the polynomials appearing in
these operators. Our bounds are expressed as curves in the (r,d)-plane which
assign to every order r a bound on the degree d of the telescopers. These
curves are hyperbolas, which reflect the phenomenon that higher order
telescopers tend to have lower degree, and vice versa."
"We present here algorithms for efficient computation of linear algebra
problems over finite fields."
"In this paper, a matrix representation for the differential resultant of two
generic ordinary differential polynomials $f_1$ and $f_2$ in the differential
indeterminate $y$ with order one and arbitrary degree is given. That is, a
non-singular matrix is constructed such that its determinant contains the
differential resultant as a factor. Furthermore, the algebraic sparse resultant
of $f_1, f_2, \delta f_1, \delta f_2$ treated as polynomials in $y, y', y""$ is
shown to be a non-zero multiple of the differential resultant of $f_1, f_2$.
Although very special, this seems to be the first matrix representation for a
class of nonlinear generic differential polynomials."
"Power Series Solution method has been used traditionally for to solve Linear
Differential Equations, in Ordinary and Partial form. But this method has been
limited to this kind of problems. We present the solution of problems of Non
Linear Partial Differential equations of Physical Mathematical using power
series."
"Two new concepts, generic regular decomposition and
regular-decomposition-unstable (RDU) variety for generic zero-dimensional
systems, are introduced in this paper and an algorithm is proposed for
computing a generic regular decomposition and the associated RDU variety of a
given generic zero-dimensional system simultaneously. The solutions of the
given system can be expressed by finitely many zero-dimensional regular chains
if the parameter value is not on the RDU variety.
  The so called weakly relatively simplicial decomposition plays a crucial role
in the algorithm, which is based on the theories of subresultant chains.
Furthermore, the algorithm can be naturally adopted to compute a non-redundant
Wu's decomposition and the decomposition is stable at any parameter value that
is not on the RDU variety. The algorithm has been implemented with Maple 15 and
experimented with a number of benchmarks from the literature. Empirical results
are also presented to show the good performance of the algorithm."
"The aim of this article is twofold: on the one hand it is intended to serve
as a gentle introduction to the topic of creative telescoping, from a practical
point of view; for this purpose its application to several problems is
exemplified. On the other hand, this chapter has the flavour of a survey
article: the developments in this area during the last two decades are sketched
and a selection of references is compiled in order to highlight the impact of
creative telescoping in numerous contexts."
"The library \emph{fast\_polynomial} for Sage compiles multivariate
polynomials for subsequent fast evaluation. Several evaluation schemes are
handled, such as H\""orner, divide and conquer and new ones can be added easily.
Notably, a new scheme is introduced that improves the classical divide and
conquer scheme when the number of terms is not a pure power of two. Natively,
the library handles polynomials over gmp big integers, boost intervals, python
numeric types. And any type that supports addition and multiplication can
extend the library thanks to the template design. Finally, the code is
parallelized for the divide and conquer schemes, and memory allocation is
localized and optimized for the different evaluation schemes. This extended
abstract presents the concepts behind the \emph{fast\_polynomial} library. The
sage package can be downloaded at
\url{http://trac.sagemath.org/sage_trac/ticket/13358}."
"A roadmap for a semi-algebraic set $S$ is a curve which has a non-empty and
connected intersection with all connected components of $S$. Hence, this kind
of object, introduced by Canny, can be used to answer connectivity queries
(with applications, for instance, to motion planning) but has also become of
central importance in effective real algebraic geometry, since it is used in
higher-level algorithms. In this paper, we provide a probabilistic algorithm
which computes roadmaps for smooth and bounded real algebraic sets. Its output
size and running time are polynomial in $(nD)^{n\log(d)}$, where $D$ is the
maximum of the degrees of the input polynomials, $d$ is the dimension of the
set under consideration and $n$ is the number of variables. More precisely, the
running time of the algorithm is essentially subquadratic in the output size.
Even under our assumptions, it is the first roadmap algorithm with output size
and running time polynomial in $(nD)^{n\log(d)}$."
"Parameterized telescoping (including telescoping and creative telescoping)
and refined versions of it play a central role in the research area of symbolic
summation. Karr introduced 1981 $\Pi\Sigma$-fields, a general class of
difference fields, that enables one to consider this problem for indefinite
nested sums and products covering as special cases, e.g., the
($q$--)hypergeometric case and their mixed versions. This survey article
presents the available algorithms in the framework of $\Pi\Sigma$-extensions
and elaborates new results concerning efficiency."
"Computing the critical points of a polynomial function $q\in\mathbb
Q[X_1,\ldots,X_n]$ restricted to the vanishing locus $V\subset\mathbb R^n$ of
polynomials $f_1,\ldots, f_p\in\mathbb Q[X_1,\ldots, X_n]$ is of first
importance in several applications in optimization and in real algebraic
geometry. These points are solutions of a highly structured system of
multivariate polynomial equations involving maximal minors of a Jacobian
matrix. We investigate the complexity of solving this problem by using
Gr\""obner basis algorithms under genericity assumptions on the coefficients of
the input polynomials. The main results refine known complexity bounds (which
depend on the maximum $D=\max(deg(f_1),\ldots,deg(f_p),deg(q))$) to bounds
which depend on the list of degrees $(deg(f_1),\ldots,deg(f_p),deg(q))$: we
prove that the Gr\""obner basis computation can be performed in
$\delta^{O(\log(A)/\log(G))}$ arithmetic operations in $\mathbb Q$, where
$\delta$ is the algebraic degree of the ideal vanishing on the critical points,
and $A$ and $G$ are the arithmetic and geometric average of a multiset
constructed from the sequence of degrees. As a by-product, we prove that
solving such generic optimization problems with Gr\""obner bases requires at
most $D^{O(n)}$ arithmetic operations in $\mathbb Q$, which meets the best
known complexity bound for this problem. Finally, we illustrate these
complexity results with experiments, giving evidence that these bounds are
relevant for applications."
"Modular algorithm are widely used in computer algebra systems (CAS), for
example to compute efficiently the gcd of multivariate polynomials. It is known
to work to compute Groebner basis over $\Q$, but it does not seem to be popular
among CAS implementers. In this paper, I will show how to check a candidate
Groebner basis (obtained by reconstruction of several Groebner basis modulo
distinct prime numbers) with a given error probability, that may be 0 if a
certified Groebner basis is desired. This algorithm is now the default
algorithm used by the Giac/Xcas computer algebra system with competitive
timings, thanks to a trick that can accelerate computing Groebner basis modulo
a prime once the computation has been done modulo another prime."
"Symbolic integration is an important module of a typical Computer Algebra
System. As for now, Mathematica, Matlab, Maple and Sage are all mainstream CAS.
They share the same framework for symbolic integration at some points. In this
book first we review the state of the art in the field of CAS. Then we focus on
typical frameworks of the current symbolic integration systems and summarize
the main mathematical theories behind these frameworks. Based on the
open-source computer algebra system maTHmU developed by our team in our
university, we propose a potential framework to improve the performance of the
current symbolic integration system."
"When using cylindrical algebraic decomposition (CAD) to solve a problem with
respect to a set of polynomials, it is likely not the signs of those
polynomials that are of paramount importance but rather the truth values of
certain quantifier free formulae involving them. This observation motivates our
article and definition of a Truth Table Invariant CAD (TTICAD).
  In ISSAC 2013 the current authors presented an algorithm that can efficiently
and directly construct a TTICAD for a list of formulae in which each has an
equational constraint. This was achieved by generalising McCallum's theory of
reduced projection operators. In this paper we present an extended version of
our theory which can be applied to an arbitrary list of formulae, achieving
savings if at least one has an equational constraint. We also explain how the
theory of reduced projection operators can allow for further improvements to
the lifting phase of CAD algorithms, even in the context of a single equational
constraint.
  The algorithm is implemented fully in Maple and we present both promising
results from experimentation and a complexity analysis showing the benefits of
our contributions."
"Certificates to a linear algebra computation are additional data structures
for each output, which can be used by a---possibly randomized---verification
algorithm that proves the correctness of each output. The certificates are
essentially optimal if the time (and space) complexity of verification is
essentially linear in the input size $N$, meaning $N$ times a factor
$N^{o(1)}$, i.e., a factor $N^{\eta(N)}$ with $\lim_{N\to \infty} \eta(N)$ $=$
$0$. We give algorithms that compute essentially optimal certificates for the
positive semidefiniteness, Frobenius form, characteristic and minimal
polynomial of an $n\times n$ dense integer matrix $A$. Our certificates can be
verified in Monte-Carlo bit complexity $(n^2 \lognormA)^{1+o(1)}$, where
$\lognormA$ is the bit size of the integer entries, solving an open problem in
[Kaltofen, Nehring, Saunders, Proc.\ ISSAC 2011] subject to computational
hardness assumptions. Second, we give algorithms that compute certificates for
the rank of sparse or structured $n\times n$ matrices over an abstract field,
whose Monte Carlo verification complexity is $2$ matrix-times-vector products
$+$ $n^{1+o(1)}$ arithmetic operations in the field. For example, if the
$n\times n$ input matrix is sparse with $n^{1+o(1)}$ non-zero entries, our rank
certificate can be verified in $n^{1+o(1)}$ field operations. This extends also
to integer matrices with only an extra $||A||^{1+o(1)}$ factor. All our
certificates are based on interactive verification protocols with the
interaction removed by a Fiat-Shamir identification heuristic. The validity of
our verification procedure is subject to standard computational hardness
assumptions from cryptography."
"We present a new Monte Carlo algorithm for the interpolation of a
straight-line program as a sparse polynomial $f$ over an arbitrary finite field
of size $q$. We assume a priori bounds $D$ and $T$ are given on the degree and
number of terms of $f$. The approach presented in this paper is a hybrid of the
diversified and recursive interpolation algorithms, the two previous fastest
known probabilistic methods for this problem. By making effective use of the
information contained in the coefficients themselves, this new algorithm
improves on the bit complexity of previous methods by a ""soft-Oh"" factor of
$T$, $\log D$, or $\log q$."
"A new projection operator based on cylindrical algebraic decomposition (CAD)
is proposed. The new operator computes the intersection of projection factor
sets produced by different CAD projection orders. In other words, it computes
the gcd of projection polynomials in the same variables produced by different
CAD projection orders. We prove that the new operator still guarantees
obtaining at least one sample point from every connected component of the
highest dimension, and therefore, can be used for testing semi-definiteness of
polynomials. Although the complexity of the new method is still doubly
exponential, in many cases, the new operator does produce smaller projection
factor sets and fewer open cells. Some examples of testing semi-definiteness of
polynomials, which are difficult to be solved by existing tools, have been
worked out efficiently by our program based on the new method."
"It is known that multiplication of linear differential operators over ground
fields of characteristic zero can be reduced to a constant number of matrix
products. We give a new algorithm by evaluation and interpolation which is
faster than the previously-known one by a constant factor, and prove that in
characteristic zero, multiplication of differential operators and of matrices
are computationally equivalent problems. In positive characteristic, we show
that differential operators can be multiplied in nearly optimal time.
Theoretical results are validated by intensive experiments."
"Efficient algorithms are known for many operations on truncated power series
(multiplication, powering, exponential, ...). Composition is a more complex
task. We isolate a large class of power series for which composition can be
performed efficiently. We deduce fast algorithms for converting polynomials
between various bases, including Euler, Bernoulli, Fibonacci, and the
orthogonal Laguerre, Hermite, Jacobi, Krawtchouk, Meixner and
Meixner-Pollaczek."
"We discuss efficient conversion algorithms for orthogonal polynomials. We
describe a known conversion algorithm from an arbitrary orthogonal basis to the
monomial basis, and deduce a new algorithm of the same complexity for the
converse operation."
"We describe an algorithm that takes as input a complex sequence $(u_n)$ given
by a linear recurrence relation with polynomial coefficients along with initial
values, and outputs a simple explicit upper bound $(v_n)$ such that $|u_n| \leq
v_n$ for all $n$. Generically, the bound is tight, in the sense that its
asymptotic behaviour matches that of $u_n$. We discuss applications to the
evaluation of power series with guaranteed precision."
"We extend Zeilberger's approach to special function identities to cases that
are not holonomic. The method of creative telescoping is thus applied to
definite sums or integrals involving Stirling or Bernoulli numbers, incomplete
Gamma function or polylogarithms, which are not covered by the holonomic
framework. The basic idea is to take into account the dimension of appropriate
ideals in Ore algebras. This unifies several earlier extensions and provides
algorithms for summation and integration in classes that had not been
accessible to computer algebra before."
"In this paper, the changes of representations of a group are used in order to
describe its action as algebraic Galois group of an univariate polynomial on
the roots of factors of any Lagrange resolvent. By this way, the Galois group
of resolvent factors are pre-determinated. In follows, different applications
are exposed; in particular, some classical results of algebraic Galois theory."
"The theory part of this paper is sketched as follows. Based on column
stochastic average matrix $T_n$ selected as a basic substitution matrix, the
method of advanced successive difference substitution is established. Then, a
set of necessary and sufficient conditions for deciding positive semi-definite
form on $\R^n_+$ is derived from this method. And furthermore, it is proved
that the sequence of SDS sets of a positive definite form is positively
terminating.
  Worked out according to these results, the Maple program TSDS3 not only
automatically proves the polynomial inequalities, but also outputs counter
examples for the false. Sometimes TSDS3 does not halt, but it is very useful by
experimenting on so many examples."
"We present a new algorithm for isolating the real roots of a system of
multivariate polynomials, given in the monomial basis. It is inspired by
existing subdivision methods in the Bernstein basis; it can be seen as
generalization of the univariate continued fraction algorithm or alternatively
as a fully analog of Bernstein subdivision in the monomial basis. The
representation of the subdivided domains is done through homographies, which
allows us to use only integer arithmetic and to treat efficiently unbounded
regions. We use univariate bounding functions, projection and preconditionning
techniques to reduce the domain of search. The resulting boxes have optimized
rational coordinates, corresponding to the first terms of the continued
fraction expansion of the real roots. An extension of Vincent's theorem to
multivariate polynomials is proved and used for the termination of the
algorithm. New complexity bounds are provided for a simplified version of the
algorithm. Examples computed with a preliminary C++ implementation illustrate
the approach."
"Computer algebra in Java is a promising field of development. It has not yet
reached an industrial strength, in part because of a lack of good user
interfaces. Using a general purpose scripting language can bring a natural
mathematical notation, akin to the one of specialized interfaces included in
most computer algebra systems. We present such an interface for Java computer
algebra libraries, using scripts available in the JSR 223 framework. We
introduce the concept of `symbolic programming' and show its usefulness by
prototypes of symbolic polynomials and polynomial rings."
"A Chebyshev expansion is a series in the basis of Chebyshev polynomials of
the first kind. When such a series solves a linear differential equation, its
coefficients satisfy a linear recurrence equation. We interpret this equation
as the numerator of a fraction of linear recurrence operators. This
interpretation lets us give a simple view of previous algorithms, analyze their
complexity, and design a faster one for large orders."
"Matrix interpretations generalize linear polynomial interpretations and have
been proved useful in the implementation of tools for automatically proving
termination of Term Rewriting Systems. In view of the successful use of
rational coefficients in polynomial interpretations, we have recently
generalized traditional matrix interpretations (using natural numbers in the
matrix entries) to incorporate real numbers. However, existing results which
formally prove that polynomials over the reals are more powerful than
polynomials over the naturals for proving termination of rewrite systems failed
to be extended to matrix interpretations. In this paper we get deeper into this
problem. We show that, under some conditions, it is possible to transform a
matrix interpretation over the rationals satisfying a set of symbolic
constraints into a matrix interpretation over the naturals (using bigger
matrices) which still satisfies the constraints."
"We give bit-size estimates for the coefficients appearing in triangular sets
describing positive-dimensional algebraic sets defined over Q. These estimates
are worst case upper bounds; they depend only on the degree and height of the
underlying algebraic sets. We illustrate the use of these results in the
context of a modular algorithm. This extends results by the first and last
author, which were confined to the case of dimension 0. Our strategy is to get
back to dimension 0 by evaluation and inter- polation techniques. Even though
the main tool (height theory) remains the same, new difficulties arise to
control the growth of the coefficients during the interpolation process."
"We study a variant of the univariate approximate GCD problem, where the
coefficients of one polynomial f(x)are known exactly, whereas the coefficients
of the second polynomial g(x)may be perturbed. Our approach relies on the
properties of the matrix which describes the operator of multiplication by gin
the quotient ring C[x]=(f). In particular, the structure of the null space of
the multiplication matrix contains all the essential information about GCD(f;
g). Moreover, the multiplication matrix exhibits a displacement structure that
allows us to design a fast algorithm for approximate GCD computation with
quadratic complexity w.r.t. polynomial degrees."
"In this paper, a linear univariate representation for the roots of a
zero-dimensional polynomial equation system is presented, where the roots of
the equation system are represented as linear combinations of roots of several
univariate polynomial equations. The main advantage of this representation is
that the precision of the roots can be easily controlled. In fact, based on the
linear univariate representation, we can give the exact precisions needed for
roots of the univariate equations in order to obtain the roots of the equation
system to a given precision. As a consequence, a root isolation algorithm for a
zero-dimensional polynomial equation system can be easily derived from its
linear univariate representation."
"We study tight bounds and fast algorithms for LCLMs of several linear
differential operators with polynomial coefficients. We analyze the arithmetic
complexity of existing algorithms for LCLMs, as well as the size of their
outputs. We propose a new algorithm that recasts the LCLM computation in a
linear algebra problem on a polynomial matrix. This algorithm yields sharp
bounds on the coefficient degrees of the LCLM, improving by one order of
magnitude the best bounds obtained using previous algorithms. The complexity of
the new algorithm is almost optimal, in the sense that it nearly matches the
arithmetic size of the output."
"We provide algorithms computing power series solutions of a large class of
differential or $q$-differential equations or systems. Their number of
arithmetic operations grows linearly with the precision, up to logarithmic
terms."
"This paper studies the unification problem with associative, commutative, and
associative-commutative functions mainly from a viewpoint of the parameterized
complexity on the number of variables. It is shown that both associative and
associative-commutative unification problems are $W[1]$-hard. A fixed-parameter
algorithm and a polynomial-time algorithm are presented for special cases of
commutative unification in which one input term is variable-free and the number
of variables is bounded by a constant, respectively. Related results including
those on the string and tree edit distance problems with variables are shown
too."
"We adapt the rectangular splitting technique of Paterson and Stockmeyer to
the problem of evaluating terms in holonomic sequences that depend on a
parameter. This approach allows computing the $n$-th term in a recurrent
sequence of suitable type using $O(n^{1/2})$ ""expensive"" operations at the cost
of an increased number of ""cheap"" operations.
  Rectangular splitting has little overhead and can perform better than either
naive evaluation or asymptotically faster algorithms for ranges of $n$
encountered in applications. As an example, fast numerical evaluation of the
gamma function is investigated. Our work generalizes two previous algorithms of
Smith."
"The GVW algorithm is a signature-based algorithm for computing Gr\""obner
bases. If the input system is not homogeneous, some J-pairs with higher
signatures but lower degrees are rejected by GVW's Syzygy Criterion, instead,
GVW have to compute some J-pairs with lower signatures but higher degrees.
Consequently, degrees of polynomials appearing during the computations may
unnecessarily grow up higher and the computation become more expensive. In this
paper, a variant of the GVW algorithm, called M-GVW, is proposed and mutant
pairs are introduced to overcome inconveniences brought by inhomogeneous input
polynomials. Some techniques from linear algebra are used to improve the
efficiency. Both GVW and M-GVW have been implemented in C++ and tested by many
examples from boolean polynomial rings. The timings show M-GVW usually performs
much better than the original GVW algorithm when mutant pairs are found.
Besides, M-GVW is also compared with intrinsic Gr\""obner bases functions on
Maple, Singular and Magma. Due to the efficient routines from the M4RI library,
the experimental results show that M-GVW is very efficient."
"Consider the problem: given a real number $x$ and an error bound $\epsilon$,
find an interval such that it contains the $\sqrt[n]{x}$ and its width is less
than $\epsilon$. One way to solve the problem is to start with an initial
interval and to repeatedly update it by applying an interval refinement map on
it until it becomes narrow enough. In this paper, we prove that the well known
Secant-Newton map is optimal among a certain family of natural generalizations."
"We estimate the Boolean complexity of multiplication of structured matrices
by a vector and the solution of nonsingular linear systems of equations with
these matrices. We study four basic most popular classes, that is, Toeplitz,
Hankel, Cauchy and Van-der-monde matrices, for which the cited computational
problems are equivalent to the task of polynomial multiplication and division
and polynomial and rational multipoint evaluation and interpolation. The
Boolean cost estimates for the latter problems have been obtained by Kirrinnis
in \cite{kirrinnis-joc-1998}, except for rational interpolation, which we
supply now. All known Boolean cost estimates for these problems rely on using
Kronecker product. This implies the $d$-fold precision increase for the $d$-th
degree output, but we avoid such an increase by relying on distinct techniques
based on employing FFT. Furthermore we simplify the analysis and make it more
transparent by combining the representation of our tasks and algorithms in
terms of both structured matrices and polynomials and rational functions. This
also enables further extensions of our estimates to cover Trummer's important
problem and computations with the popular classes of structured matrices that
generalize the four cited basic matrix classes."
"Highly efficient and even nearly optimal algorithms have been developed for
the classical problem of univariate polynomial root-finding (see, e.g.,
\cite{P95}, \cite{P02}, \cite{MNP13}, and the bibliography therein), but this
is still an area of active research. By combining some powerful techniques
developed in this area we devise new nearly optimal algorithms, whose
substantial merit is their simplicity, important for the implementation."
"Cylindrical algebraic decompositions (CADs) are a key tool for solving
problems in real algebraic geometry and beyond. We recently presented a new CAD
algorithm combining two advances: truth-table invariance, making the CAD
invariant with respect to the truth of logical formulae rather than the signs
of polynomials; and CAD construction by regular chains technology, where first
a complex decomposition is constructed by refining a tree incrementally by
constraint. We here consider how best to formulate problems for input to this
algorithm. We focus on a choice (not relevant for other CAD algorithms) about
the order in which constraints are presented. We develop new heuristics to help
make this choice and thus allow the best use of the algorithm in practice. We
also consider other choices of problem formulation for CAD, as discussed in
CICM 2013, revisiting these in the context of the new algorithm."
"We investigate the distribution of cells by dimension in cylindrical
algebraic decompositions (CADs). We find that they follow a standard
distribution which seems largely independent of the underlying problem or CAD
algorithm used. Rather, the distribution is inherent to the cylindrical
structure and determined mostly by the number of variables.
  This insight is then combined with an algorithm that produces only
full-dimensional cells to give an accurate method of predicting the number of
cells in a complete CAD. Since constructing only full-dimensional cells is
relatively inexpensive (involving no costly algebraic number calculations) this
leads to heuristics for helping with various questions of problem formulation
for CAD, such as choosing an optimal variable ordering. Our experiments
demonstrate that this approach can be highly effective."
"Given a square, nonsingular matrix of univariate polynomials
$\mathbf{F}\in\mathbb{K}[x]^{n\times n}$ over a field $\mathbb{K}$, we give a
deterministic algorithm for finding the determinant of $\mathbf{F}$. The
complexity of the algorithm is $\bigO \left(n^{\omega}s\right)$ field
operations where $s$ is the average column degree or the average row degree of
$\mathbf{F}$. Here $\bigO$ notation is Big-$O$ with log factors omitted and
$\omega$ is the exponent of matrix multiplication."
"In this paper, we draw a connection between ideal lattices and Gr\""{o}bner
bases in the multivariate polynomial rings over integers. We study extension of
ideal lattices in $\mathbb{Z}[x]/\langle f \rangle$ (Lyubashevsky \&
Micciancio, 2006) to ideal lattices in
$\mathbb{Z}[x_1,\ldots,x_n]/\mathfrak{a}$, the multivariate case, where $f$ is
a polynomial in $\mathbb{Z}[X]$ and $\mathfrak{a}$ is an ideal in
$\mathbb{Z}[x_1,\ldots,x_n]$. Ideal lattices in univariate case are interpreted
as generalizations of cyclic lattices. We introduce a notion of multivariate
cyclic lattices and we show that multivariate ideal lattices are indeed a
generalization of them. We show that the fact that existence of ideal lattice
in univariate case if and only if $f$ is monic translates to short reduced
Gr\""obner basis (Francis \& Dukkipati, 2014) of $\mathfrak{a}$ is monic in
multivariate case. We, thereby, give a necessary and sufficient condition for
residue class polynomial rings over $\mathbb{Z}$ to have ideal lattices. We
also characterize ideals in $\mathbb{Z}[x_1,\ldots,x_n]$ that give rise to full
rank lattices."
"This paper describes and analyzes a method for computing border bases of a
zero-dimensional ideal $I$. The criterion used in the computation involves
specific commutation polynomials and leads to an algorithm and an
implementation extending the one provided in [MT'05]. This general border basis
algorithm weakens the monomial ordering requirement for \grob bases
computations. It is up to date the most general setting for representing
quotient algebras, embedding into a single formalism Gr\""obner bases, Macaulay
bases and new representation that do not fit into the previous categories. With
this formalism we show how the syzygies of the border basis are generated by
commutation relations. We also show that our construction of normal form is
stable under small perturbations of the ideal, if the number of solutions
remains constant. This new feature for a symbolic algorithm has a huge impact
on the practical efficiency as it is illustrated by the experiments on
classical benchmark polynomial systems, at the end of the paper."
"Let $f_1,...,f_s \in \mathbb{K}[x_1,...,x_m]$ be a system of polynomials
generating a zero-dimensional ideal $\I$, where $\mathbb{K}$ is an arbitrary
algebraically closed field. Assume that the factor algebra
$\A=\mathbb{K}[x_1,...,x_m]/\I$ is Gorenstein and that we have a bound
$\delta>0$ such that a basis for $\A$ can be computed from multiples of
$f_1,...,f_s$ of degrees at most $\delta$. We propose a method using Sylvester
or Macaulay type resultant matrices of $f_1,...,f_s$ and $J$, where $J$ is a
polynomial of degree $\delta$ generalizing the Jacobian, to compute moment
matrices, and in particular matrices of traces for $\A$. These matrices of
traces in turn allow us to compute a system of multiplication matrices
$\{M_{x_i}|i=1,...,m\}$ of the radical $\sqrt{\I}$, following the approach in
the previous work by Janovitz-Freireich, R\'{o}nyai and Sz\'ant\'o.
Additionally, we give bounds for $\delta$ for the case when $\I$ has finitely
many projective roots in $\mathbb{P}^m_\CC$."
"The computation of triangular decompositions are based on two fundamental
operations: polynomial GCDs modulo regular chains and regularity test modulo
saturated ideals. We propose new algorithms for these core operations relying
on modular methods and fast polynomial arithmetic. Our strategies take also
advantage of the context in which these operations are performed. We report on
extensive experimentation, comparing our code to pre-existing Maple
implementations, as well as more optimized Magma functions. In most cases, our
new code outperforms the other packages by several orders of magnitude."
"Cylindrical algebraic decomposition is one of the most important tools for
computing with semi-algebraic sets, while triangular decomposition is among the
most important approaches for manipulating constructible sets. In this paper,
for an arbitrary finite set $F \subset {\R}[y_1, ..., y_n]$ we apply
comprehensive triangular decomposition in order to obtain an $F$-invariant
cylindrical decomposition of the $n$-dimensional complex space, from which we
extract an $F$-invariant cylindrical algebraic decomposition of the
$n$-dimensional real space. We report on an implementation of this new approach
for constructing cylindrical algebraic decompositions."
"In this short note, we give simple proofs of several results and conjectures
formulated by Stolarsky and Tran concerning generating functions of some
families of Chebyshev-like polynomials."
"In this paper we present a specialised algebraic manipulation package devoted
to Celestial Mechanics. The system, called Piranha, is built on top of a
generic and extensible framework, which allows to treat efficiently and in a
unified way the algebraic structures most commonly encountered in Celestial
Mechanics (such as multivariate polynomials and Poisson series). In this
contribution we explain the architecture of the software, with special focus on
the implementation of series arithmetics, show its current capabilities, and
present benchmarks indicating that Piranha is competitive, performance-wise,
with other specialised manipulators."
"A new efficient algorithm is proposed for factoring polynomials over an
algebraic extension field. The extension field is defined by a polynomial ring
modulo a maximal ideal. If the maximal ideal is given by its Groebner basis, no
extra Groebner basis computation is needed for factoring a polynomial over this
extension field. Nothing more than linear algebraic technique is used to get a
polynomial over the ground field by a generic linear map. Then this polynomial
is factorized over the ground field. From these factors, the factorization of
the polynomial over the extension field is obtained. The new algorithm has been
implemented and computer experiments indicate that the new algorithm is very
efficient, particularly in complicated examples."
"The aim of this paper is to give a higher dimensional equivalent of the
classical modular polynomials $\Phi_\ell(X,Y)$. If $j$ is the $j$-invariant
associated to an elliptic curve $E_k$ over a field $k$ then the roots of
$\Phi_\ell(j,X)$ correspond to the $j$-invariants of the curves which are
$\ell$-isogeneous to $E_k$. Denote by $X_0(N)$ the modular curve which
parametrizes the set of elliptic curves together with a $N$-torsion subgroup.
It is possible to interpret $\Phi_\ell(X,Y)$ as an equation cutting out the
image of a certain modular correspondence $X_0(\ell) \to X_0(1) \times X_0(1)$
in the product $X_0(1) \times X_0(1)$. Let $g$ be a positive integer and
$\overn \in \N^g$. We are interested in the moduli space that we denote by
$\Mn$ of abelian varieties of dimension $g$ over a field $k$ together with an
ample symmetric line bundle $\pol$ and a symmetric theta structure of type
$\overn$. If $\ell$ is a prime and let $\overl=(\ell, ..., \ell)$, there exists
a modular correspondence $\Mln \to \Mn \times \Mn$. We give a system of
algebraic equations defining the image of this modular correspondence."
"We present a novel method for checking the Hurwitz stability of a polytope of
matrices. First we prove that the polytope matrix is stable if and only if two
homogenous polynomials are positive on a simplex, then through a newly proposed
method, i.e., the weighted difference substitution method, the latter can be
checked in finite steps. Examples show the efficiency of our method."
"We present a complete algorithm for finding an exact minimal polynomial from
its approximate value by using an improved parameterized integer relation
construction method. Our result is superior to the existence of error
controlling on obtaining an exact rational number from its approximation. The
algorithm is applicable for finding exact minimal polynomial of an algebraic
number by its approximate root. This also enables us to provide an efficient
method of converting the rational approximation representation to the minimal
polynomial representation, and devise a simple algorithm to factor multivariate
polynomials with rational coefficients.
  Compared with the subsistent methods, our method combines advantage of high
efficiency in numerical computation, and exact, stable results in symbolic
computation. we also discuss some applications to some transcendental numbers
by approximations. Moreover, the Digits of our algorithm is far less than the
LLL-lattice basis reduction technique in theory. In this paper, we completely
implement how to obtain exact results by numerical approximate computations."
"Solving multihomogeneous systems, as a wide range of structured algebraic
systems occurring frequently in practical problems, is of first importance.
Experimentally, solving these systems with Gr\""obner bases algorithms seems to
be easier than solving homogeneous systems of the same degree. Nevertheless,
the reasons of this behaviour are not clear. In this paper, we focus on
bilinear systems (i.e. bihomogeneous systems where all equations have bidegree
(1,1)). Our goal is to provide a theoretical explanation of the aforementionned
experimental behaviour and to propose new techniques to speed up the Gr\""obner
basis computations by using the multihomogeneous structure of those systems.
The contributions are theoretical and practical. First, we adapt the classical
F5 criterion to avoid reductions to zero which occur when the input is a set of
bilinear polynomials. We also prove an explicit form of the Hilbert series of
bihomogeneous ideals generated by generic bilinear polynomials and give a new
upper bound on the degree of regularity of generic affine bilinear systems.
This leads to new complexity bounds for solving bilinear systems. We propose
also a variant of the F5 Algorithm dedicated to multihomogeneous systems which
exploits a structural property of the Macaulay matrix which occurs on such
inputs. Experimental results show that this variant requires less time and
memory than the classical homogeneous F5 Algorithm."
"The famous F5 algorithm for computing Gr\""obner basis was presented by
Faug\`ere in 2002 without complete proofs for its correctness. The current
authors have simplified the original F5 algorithm into an F5 algorithm in
Buchberger's style (F5B algorithm), which is equivalent to original F5
algorithm and may deduce some F5-like versions. In this paper, the F5B
algorithm is briefly revisited and a new complete proof for the correctness of
F5B algorithm is proposed. This new proof is not limited to homogeneous systems
and does not depend on the strategy of selecting critical pairs (i.e. the
strategy deciding which critical pair is computed first) such that any strategy
could be utilized in F5B (F5) algorithm. From this new proof, we find that the
special reduction procedure (F5-reduction) is the key of F5 algorithm, so
maintaining this special reduction, various variation algorithms become
available. A natural variation of F5 algorithm, which transforms original F5
algorithm to a non-incremental algorithm, is presented and proved in this paper
as well. This natural variation has been implemented over the Boolean ring. The
two revised criteria in this natural variation are also able to reject almost
all unnecessary computations and few polynomials reduce to 0 in most examples."
"Insa and Pauer presented a basic theory of Groebner basis for differential
operators with coefficients in a commutative ring in 1998, and a criterion was
proposed to determine if a set of differential operators is a Groebner basis.
In this paper, we will give a new criterion such that Insa and Pauer's
criterion could be concluded as a special case and one could compute the
Groebner basis more efficiently by this new criterion."
"In this paper we present two algorithms for the multiplication of sparse
Laurent polynomials and Poisson series (the latter being algebraic structures
commonly arising in Celestial Mechanics from the application of perturbation
theories). Both algorithms first employ the Kronecker substitution technique to
reduce multivariate multiplication to univariate multiplication, and then use
the schoolbook method to perform the univariate multiplication. The first
algorithm, suitable for moderately-sparse multiplication, uses the exponents of
the monomials resulting from the univariate multiplication as trivial hash
values in a one dimensional lookup array of coefficients. The second algorithm,
suitable for highly-sparse multiplication, uses a cache-optimised hash table
which stores the coefficient-exponent pairs resulting from the multiplication
using the exponents as keys. Both algorithms have been implemented with
attention to modern computer hardware architectures. Particular care has been
devoted to the efficient exploitation of contemporary memory hierarchies
through cache-blocking techniques and cache-friendly term ordering. The first
algorithm has been parallelised for shared-memory multicore architectures,
whereas the second algorithm is in the process of being parallelised. We
present benchmarks comparing our algorithms to the routines of other computer
algebra systems, both in sequential and parallel mode."
"If g and h are functions over some field, we can consider their composition f
= g(h). The inverse problem is decomposition: given f, determine the ex-
istence of such functions g and h. In this thesis we consider functional decom-
positions of univariate and multivariate polynomials, and rational functions
over a field F of characteristic p. In the polynomial case, ""wild"" behaviour
occurs in both the mathematical and computational theory of the problem if p
divides the degree of g. We consider the wild case in some depth, and deal with
those polynomials whose decompositions are in some sense the ""wildest"": the
additive polynomials. We determine the maximum number of decompositions and
show some polynomial time algorithms for certain classes of polynomials with
wild decompositions. For the rational function case we present a definition of
the problem, a normalised version of the problem to which the general problem
reduces, and an exponential time solution to the normal problem."
This paper collects many results on galoisian ideals and Galois theory.
"We give an approximate algorithm of computing holonomic systems of linear
differential equations for definite integrals with parameters. We show that
this algorithm gives a correct answer in finite steps, but we have no general
stopping condition. We apply the approximate method to find differential
equations for integrals associated to smooth Fano polytopes. They are
interested in the study of K3 surfaces and the toric mirror symmetry. In this
class of integrals, we can apply Stienstra's rank formula to our algorithm,
which gives a stopping condition of the approximate algorithm."
"Lazard and Rouillier in [9], by introducing the concept of discriminant
variety, have described a new and efficient algorithm for solving parametric
polynomial systems. In this paper we modify this algorithm, and we show that
with our improvements the output of our algorithm is always minimal and it does
not need to compute the radical of ideals."
"In this paper we explore the possibility of using computational algebraic
methods to analyze a class of consensus protocols. We state some necessary
conditions for convergence under consensus protocols that are polynomials."
"Loop invariants play a very important role in proving correctness of
programs. In this paper, we address the problem of generating invariants of
polynomial loop programs. We present a new approach, for generating polynomial
equation invariants of polynomial loop programs through computing vanishing
ideals of sample points. We apply rational function interpolation, based on
early termination technique, to generate invariants of loop programs with
symbolic initial values. Our approach avoids first-order quantifier elimination
and cylindrical algebraic decomposition(CAD). An algorithm for generating
polynomial invariants is proposed and some examples are given to illustrate the
algorithm. Furthermore, we demonstrate on a set of loop programs with symbolic
initial values that our algorithm can yield polynomial invariants with degrees
high up to 15."
"We consider the problem of computing critical points of the restriction of a
polynomial map to an algebraic variety. This is of first importance since the
global minimum of such a map is reached at a critical point. Thus, these points
appear naturally in non-convex polynomial optimization which occurs in a wide
range of scientific applications (control theory, chemistry, economics,...).
Critical points also play a central role in recent algorithms of effective real
algebraic geometry. Experimentally, it has been observed that Gr\""obner basis
algorithms are efficient to compute such points. Therefore, recent software
based on the so-called Critical Point Method are built on Gr\""obner bases
engines. Let $f_1,..., f_p$ be polynomials in $ \Q[x_1,..., x_n]$ of degree
$D$, $V\subset\C^n$ be their complex variety and $\pi_1$ be the projection map
$(x_1,.., x_n)\mapsto x_1$. The critical points of the restriction of $\pi_1$
to $V$ are defined by the vanishing of $f_1,..., f_p$ and some maximal minors
of the Jacobian matrix associated to $f_1,..., f_p$. Such a system is
algebraically structured: the ideal it generates is the sum of a determinantal
ideal and the ideal generated by $f_1,..., f_p$. We provide the first
complexity estimates on the computation of Gr\""obner bases of such systems
defining critical points. We prove that under genericity assumptions on
$f_1,..., f_p$, the complexity is polynomial in the generic number of critical
points, i.e. $D^p(D-1)^{n-p}{{n-1}\choose{p-1}}$. More particularly, in the
quadratic case D=2, the complexity of such a Gr\""obner basis computation is
polynomial in the number of variables $n$ and exponential in $p$. We also give
experimental evidence supporting these theoretical results."
"A new algorithm for real root isolation of polynomial equations based on
hybrid computation is presented in this paper. Firstly, the approximate
(complex) zeros of the given polynomial equations are obtained via homotopy
continuation method. Then, for each approximate zero, an initial box relying on
the Kantorovich theorem is constructed, which contains the corresponding
accurate zero. Finally, the Krawczyk interval iteration with interval
arithmetic is applied to the initial boxes so as to check whether or not the
corresponding approximate zeros are real and to obtain the real root isolation
boxes. Meanwhile, an empirical construction of initial box is provided for
higher performance. Our experiments on many benchmarks show that the new hybrid
method is more efficient, compared with the traditional symbolic approaches."
"Due to the elimination property held by the lexicographic monomial order, the
corresponding Groebner bases display strong structural properties from which
meaningful informations can easily be extracted. We study these properties for
radical ideals of (co)dimension zero. The proof presented relies on a
combinatorial decomposition of the finite set of points whereby iterated
Lagrange interpolation formulas permit to reconstruct a minimal Groebner basis.
This is the first fully explicit interpolation formula for polynomials forming
a lexicographic Groebner basis, from which the structure property can easily be
read off. The inductive nature of the proof also yield as a byproduct a
triangular decomposition algorithm from the Groebner basis."
"We state and analyze a generalization of the ""truncation trick"" suggested by
Gourdon and Sebah to improve the performance of power series evaluation by
binary splitting. It follows from our analysis that the values of D-finite
functions (i.e., functions described as solutions of linear differential
equations with polynomial coefficients) may be computed with error bounded by
2^(-p) in time O(p*(lg p)^(3+o(1))) and space O(p). The standard fast algorithm
for this task, due to Chudnovsky and Chudnovsky, achieves the same time
complexity bound but requires \Theta(p*lg p) bits of memory."
"Truncated Fourier Transforms (TFTs), first introduced by Van der Hoeven,
refer to a family of algorithms that attempt to smooth ""jumps"" in complexity
exhibited by FFT algorithms. We present an in-place TFT whose time complexity,
measured in terms of ring operations, is comparable to existing not-in-place
TFT methods. We also describe a transformation that maps between two families
of TFT algorithms that use different sets of evaluation points."
"In this paper, we propose an incremental algorithm for computing cylindrical
algebraic decompositions. The algorithm consists of two parts: computing a
complex cylindrical tree and refining this complex tree into a cylindrical tree
in real space. The incrementality comes from the first part of the algorithm,
where a complex cylindrical tree is constructed by refining a previous complex
cylindrical tree with a polynomial constraint. We have implemented our
algorithm in Maple. The experimentation shows that the proposed algorithm
outperforms existing ones for many examples taken from the literature."
"The series expansion at the origin of the Airy function Ai(x) is alternating
and hence problematic to evaluate for x > 0 due to cancellation. Based on a
method recently proposed by Gawronski, M\""uller, and Reinhard, we exhibit two
functions F and G, both with nonnegative Taylor expansions at the origin, such
that Ai(x) = G(x)/F(x). The sums are now well-conditioned, but the Taylor
coefficients of G turn out to obey an ill-conditioned three-term recurrence. We
use the classical Miller algorithm to overcome this issue. We bound all errors
and our implementation allows an arbitrary and certified accuracy, that can be
used, e.g., for providing correct rounding in arbitrary precision."
"In considering the reliability of numerical programs, it is normal to ""limit
our study to the semantics dealing with numerical precision"" (Martel, 2005). On
the other hand, there is a great deal of work on the reliability of programs
that essentially ignores the numerics. The thesis of this paper is that there
is a class of problems that fall between these two, which could be described as
""does the low-level arithmetic implement the high-level mathematics"". Many of
these problems arise because mathematics, particularly the mathematics of the
complex numbers, is more difficult than expected: for example the complex
function log is not continuous, writing down a program to compute an inverse
function is more complicated than just solving an equation, and many algebraic
simplification rules are not universally valid.
  The good news is that these problems are theoretically capable of being
solved, and are practically close to being solved, but not yet solved, in
several real-world examples. However, there is still a long way to go before
implementations match the theoretical possibilities."
"Desingularization is the problem of finding a left multiple of a given Ore
operator in which some factor of the leading coefficient of the original
operator is removed. An order-degree curve for a given Ore operator is a curve
in the $(r,d)$-plane such that for all points $(r,d)$ above this curve, there
exists a left multiple of order $r$ and degree $d$ of the given operator. We
give a new proof of a desingularization result by Abramov and van Hoeij for the
shift case, and show how desingularization implies order-degree curves which
are extremely accurate in examples."
"We present a new algorithm for computing hyperexponential solutions of
ordinary linear differential equations with polynomial coefficients. The
algorithm relies on interpreting formal series solutions at the singular points
as analytic functions and evaluating them numerically at some common ordinary
point. The numerical data is used to determine a small number of combinations
of the formal series that may give rise to hyperexponential solutions."
"Creative telescoping algorithms compute linear differential equations
satisfied by multiple integrals with parameters. We describe a precise and
elementary algorithmic version of the Griffiths-Dwork method for the creative
telescoping of rational functions. This leads to bounds on the order and degree
of the coefficients of the differential equation, and to the first complexity
result which is simply exponential in the number of variables. One of the
important features of the algorithm is that it does not need to compute
certificates. The approach is vindicated by a prototype implementation."
"We present an algorithm for isolating the roots of an arbitrary complex
polynomial $p$ that also works for polynomials with multiple roots provided
that the number $k$ of distinct roots is given as part of the input. It outputs
$k$ pairwise disjoint disks each containing one of the distinct roots of $p$,
and its multiplicity. The algorithm uses approximate factorization as a
subroutine.
  In addition, we apply the new root isolation algorithm to a recent algorithm
for computing the topology of a real planar algebraic curve specified as the
zero set of a bivariate integer polynomial and for isolating the real solutions
of a bivariate polynomial system. For input polynomials of degree $n$ and
bitsize $\tau$, we improve the currently best running time from
$\tO(n^{9}\tau+n^{8}\tau^{2})$ (deterministic) to $\tO(n^{6}+n^{5}\tau)$
(randomized) for topology computation and from $\tO(n^{8}+n^{7}\tau)$
(deterministic) to $\tO(n^{6}+n^{5}\tau)$ (randomized) for solving bivariate
systems."
"In this paper we show how to find a closed form solution for third order
difference operators in terms of solutions of second order operators. This work
is an extension of previous results on finding closed form solutions of
recurrence equations and a counterpart to existing results on differential
equations. As motivation and application for this work, we discuss the problem
of proving positivity of sequences given merely in terms of their defining
recurrence relation. The main advantage of the present approach to earlier
methods attacking the same problem is that our algorithm provides
human-readable and verifiable, i.e., certified proofs."
"The long-term goal initiated in this work is to obtain fast algorithms and
implementations for definite integration in Almkvist and Zeilberger's framework
of (differential) creative telescoping. Our complexity-driven approach is to
obtain tight degree bounds on the various expressions involved in the method.
To make the problem more tractable, we restrict to bivariate rational
functions. By considering this constrained class of inputs, we are able to
blend the general method of creative telescoping with the well-known Hermite
reduction. We then use our new method to compute diagonals of rational power
series arising from combinatorics."
"Uncoupling algorithms transform a linear differential system of first order
into one or several scalar differential equations. We examine two approaches to
uncoupling: the cyclic-vector method (CVM) and the
Danilevski-Barkatou-Z\""urcher algorithm (DBZ). We give tight size bounds on the
scalar equations produced by CVM, and design a fast variant of CVM whose
complexity is quasi-optimal with respect to the output size. We exhibit a
strong structural link between CVM and DBZ enabling to show that, in the
generic case, DBZ has polynomial complexity and that it produces a single
equation, strongly related to the output of CVM. We prove that algorithm CVM is
faster than DBZ by almost two orders of magnitude, and provide experimental
results that validate the theoretical complexity analyses."
"Let $\K$ be a field and $(f_1, \ldots, f_n)\subset \K[X_1, \ldots, X_n]$ be a
sequence of quasi-homogeneous polynomials of respective weighted degrees $(d_1,
\ldots, d_n)$ w.r.t a system of weights $(w_{1},\dots,w_{n})$. Such systems are
likely to arise from a lot of applications, including physics or cryptography.
We design strategies for computing Gr\""obner bases for quasi-homogeneous
systems by adapting existing algorithms for homogeneous systems to the
quasi-homogeneous case. Overall, under genericity assumptions, we show that for
a generic zero-dimensional quasi-homogeneous system, the complexity of the full
strategy is polynomial in the weighted B\'ezout bound $\prod_{i=1}^{n}d_{i} /
\prod_{i=1}^{n}w_{i}$. We provide some experimental results based on generic
systems as well as systems arising from a cryptography problem. They show that
taking advantage of the quasi-homogeneous structure of the systems allow us to
solve systems that were out of reach otherwise."
"As was initially shown by Brent, exponentials of truncated power series can
be computed using a constant number of polynomial multiplications. This note
gives a relatively simple algorithm with a low constant factor."
"Simplification of fractional powers of positive rational numbers and of sums,
products and powers of such numbers is taught in beginning algebra. Such
numbers can often be expressed in many ways, as this article discusses in some
detail. Since they are such a restricted subset of algebraic numbers, it might
seem that good simplification of them must already be implemented in all widely
used computer algebra systems. However, the algorithm taught in beginning
algebra uses integer factorization, which can consume unacceptable time for the
large numbers that often arise within computer algebra. Therefore some systems
apparently use various ad hoc techniques that can return an incorrect result
because of not simplifying to 0 the difference between two equivalent such
expressions. Even systems that avoid this flaw often do not return the same
result for all equivalent such input forms, or return an unnecessarily bulky
result that does not have any other compensating useful property. This article
identifies some of these deficiencies, then describes the advantages and
disadvantages of various alternative forms and how to overcome the deficiencies
without costly integer factorization."
"We introduce a framework for computer-aided derivation of multi-scale models.
It relies on a combination of an asymptotic method used in the field of partial
differential equations with term rewriting techniques coming from computer
science.
  In our approach, a multi-scale model derivation is characterized by the
features taken into account in the asymptotic analysis. Its formulation
consists in a derivation of a reference model associated to an elementary
nominal model, and in a set of transformations to apply to this proof until it
takes into account the wanted features. In addition to the reference model
proof, the framework includes first order rewriting principles designed for
asymptotic model derivations, and second order rewriting principles dedicated
to transformations of model derivations. We apply the method to generate a
family of homogenized models for second order elliptic equations with periodic
coefficients that could be posed in multi-dimensional domains, with possibly
multi-domains and/or thin domains."
"For a regular chain $R$, we propose an algorithm which computes the
(non-trivial) limit points of the quasi-component of $R$, that is, the set
$\bar{W(R)} \setminus W(R)$. Our procedure relies on Puiseux series expansions
and does not require to compute a system of generators of the saturated ideal
of $R$. We focus on the case where this saturated ideal has dimension one and
we discuss extensions of this work in higher dimensions. We provide
experimental results illustrating the benefits of our algorithms."
"Cylindrical algebraic decomposition (CAD) is an important tool for the
investigation of semi-algebraic sets. Originally introduced by Collins in the
1970s for use in quantifier elimination it has since found numerous
applications within algebraic geometry and beyond. Following from his original
work in 1988, McCallum presented an improved algorithm, CADW, which offered a
huge increase in the practical utility of CAD. In 2009 a team based at the
University of Western Ontario presented a new and quite separate algorithm for
CAD, which was implemented and included in the computer algebra system Maple.
As part of a wider project at Bath investigating CAD and its applications,
Collins and McCallum's CAD algorithms have been implemented in Maple. This
report details these implementations and compares them to Qepcad and the
Ontario algorithm.
  The implementations were originally undertaken to facilitate research into
the connections between the algorithms. However, the ability of the code to
guarantee order-invariant output has led to its use in new research on CADs
which are minimal for certain problems. In addition, the implementation
described here is of interest as the only full implementation of CADW, (since
Qepcad does not currently make use of McCallum's delineating polynomials), and
hence can solve problems not admissible to other CAD implementations."
"Given a zero-dimensional ideal I in K[x1,...,xn] of degree D, the
transformation of the ordering of its Groebner basis from DRL to LEX is a key
step in polynomial system solving and turns out to be the bottleneck of the
whole solving process. Thus it is of crucial importance to design efficient
algorithms to perform the change of ordering.
  The main contributions of this paper are several efficient methods for the
change of ordering which take advantage of the sparsity of multiplication
matrices in the classical FGLM algorithm. Combing all these methods, we propose
a deterministic top-level algorithm that automatically detects which method to
use depending on the input. As a by-product, we have a fast implementation that
is able to handle ideals of degree over 40000. Such an implementation
outperforms the Magma and Singular ones, as shown by our experiments.
  First for the shape position case, two methods are designed based on the
Wiedemann algorithm: the first is probabilistic and its complexity to complete
the change of ordering is O(D(N1+nlog(D))), where N1 is the number of nonzero
entries of a multiplication matrix; the other is deterministic and computes the
LEX Groebner basis of the radical of I via Chinese Remainder Theorem. Then for
the general case, the designed method is characterized by the
Berlekamp-Massey-Sakata algorithm from Coding Theory to handle the
multi-dimensional linearly recurring relations. Complexity analyses of all
proposed methods are also provided.
  Furthermore, for generic polynomial systems, we present an explicit formula
for the estimation of the sparsity of one main multiplication matrix, and prove
its construction is free. With the asymptotic analysis of such sparsity, we are
able to show for generic systems the complexity above becomes $O(\sqrt{6/n \pi}
D^{2+(n-1)/n}})$."
"Let $\RR$ be a real closed field (e.g. the field of real numbers) and
$\mathscr{S} \subset \RR^n$ be a semi-algebraic set defined as the set of
points in $\RR^n$ satisfying a system of $s$ equalities and inequalities of
multivariate polynomials in $n$ variables, of degree at most $D$, with
coefficients in an ordered ring $\ZZ$ contained in $\RR$. We consider the
problem of computing the {\em real dimension}, $d$, of $\mathscr{S}$. The real
dimension is the first topological invariant of interest; it measures the
number of degrees of freedom available to move in the set. Thus, computing the
real dimension is one of the most important and fundamental problems in
computational real algebraic geometry. The problem is ${\rm
NP}_{\mathbb{R}}$-complete in the Blum-Shub-Smale model of computation. The
current algorithms (probabilistic or deterministic) for computing the real
dimension have complexity $(s \, D)^{O(d(n-d))}$, that becomes $(s \,
D)^{O(n^2)}$ in the worst-case. The existence of a probabilistic or
deterministic algorithm for computing the real dimension with single
exponential complexity with a factor better than ${O(n^2)}$ in the exponent in
the worst-case, is a longstanding open problem. We provide a positive answer to
this problem by introducing a probabilistic algorithm for computing the real
dimension of a semi-algebraic set with complexity $(s\, D)^{O(n)}$."
"We give a new probabilistic algorithm for interpolating a ""sparse"" polynomial
f given by a straight-line program. Our algorithm constructs an approximation
f* of f, such that their difference probably has at most half the number of
terms of f, then recurses on their difference. Our approach builds on previous
work by Garg and Schost (2009), and Giesbrecht and Roche (2011), and is
asymptotically more efficient in terms of the total cost of the probes required
than previous methods, in many cases."
"Efficient matrix determinant calculations have been studied since the 19th
century. Computers expand the range of determinants that are practically
calculable to include matrices with symbolic entries. However, the fastest
determinant algorithms for numerical matrices are often not the fastest for
symbolic matrices with many variables. We compare the performance of two
algorithms, fraction-free Gaussian elimination and minor expansion, on symbolic
matrices with many variables. We show that, under a simplified theoretical
model, minor expansion is faster in most situations. We then propose
optimizations for minor expansion and demonstrate their effectiveness with
empirical data."
"It is known that point searching in basic semialgebraic sets and the search
for globally minimal points in polynomial optimization tasks can be carried out
using $(s\,d)^{O(n)}$ arithmetic operations, where $n$ and $s$ are the numbers
of variables and constraints and $d$ is the maximal degree of the polynomials
involved.\spar \noindent We associate to each of these problems an intrinsic
system degree which becomes in worst case of order $(n\,d)^{O(n)}$ and which
measures the intrinsic complexity of the task under consideration.\spar
\noindent We design non-uniformly deterministic or uniformly probabilistic
algorithms of intrinsic, quasi-polynomial complexity which solve these
problems."
"Polynomial system solving is a classical problem in mathematics with a wide
range of applications. This makes its complexity a fundamental problem in
computer science. Depending on the context, solving has different meanings. In
order to stick to the most general case, we consider a representation of the
solutions from which one can easily recover the exact solutions or a certified
approximation of them. Under generic assumption, such a representation is given
by the lexicographical Gr\""obner basis of the system and consists of a set of
univariate polynomials. The best known algorithm for computing the
lexicographical Gr\""obner basis is in $\widetilde{O}(d^{3n})$ arithmetic
operations where $n$ is the number of variables and $d$ is the maximal degree
of the equations in the input system. The notation $\widetilde{O}$ means that
we neglect polynomial factors in $n$. We show that this complexity can be
decreased to $\widetilde{O}(d^{\omega n})$ where $2 \leq \omega < 2.3727$ is
the exponent in the complexity of multiplying two dense matrices. Consequently,
when the input polynomial system is either generic or reaches the B\'ezout
bound, the complexity of solving a polynomial system is decreased from
$\widetilde{O}(D^3)$ to $\widetilde{O}(D^\omega)$ where $D$ is the number of
solutions of the system. To achieve this result we propose new algorithms which
rely on fast linear algebra. When the degree of the equations are bounded
uniformly by a constant we propose a deterministic algorithm. In the unbounded
case we present a Las Vegas algorithm."
"Cylindrical algebraic decomposition (CAD) is an important tool for the study
of real algebraic geometry with many applications both within mathematics and
elsewhere. It is known to have doubly exponential complexity in the number of
variables in the worst case, but the actual computation time can vary greatly.
It is possible to offer different formulations for a given problem leading to
great differences in tractability. In this paper we suggest a new measure for
CAD complexity which takes into account the real geometry of the problem. This
leads to new heuristics for choosing: the variable ordering for a CAD problem,
a designated equational constraint, and formulations for truth-table invariant
CADs (TTICADs). We then consider the possibility of using Groebner bases to
precondition TTICAD and when such formulations constitute the creation of a new
problem."
"We introduce a general algebraic setting for describing linear boundary
problems in a symbolic computation context, with emphasis on the case of
partial differential equations. The general setting is then applied to the
Cauchy problem for completely reducible partial differential equations with
constant coefficients. While we concentrate on the theoretical features in this
paper, the underlying operator ring is implemented and provides a sufficient
basis for all methods presented here."
"This article makes the key observation that when using cylindrical algebraic
decomposition (CAD) to solve a problem with respect to a set of polynomials, it
is not always the signs of those polynomials that are of paramount importance
but rather the truth values of certain quantifier free formulae involving them.
This motivates our definition of a Truth Table Invariant CAD (TTICAD). We
generalise the theory of equational constraints to design an algorithm which
will efficiently construct a TTICAD for a wide class of problems, producing
stronger results than when using equational constraints alone. The algorithm is
implemented fully in Maple and we present promising results from
experimentation."
"We employ computer algebra algorithms to prove a collection of identities
involving Bessel functions with half-integer orders and other special
functions. These identities appear in the famous Handbook of Mathematical
Functions, as well as in its successor, the DLMF, but their proofs were lost.
We use generating functions and symbolic summation techniques to produce new
proofs for them."
"Cylindrical algebraic decomposition (CAD) is an important tool for the
investigation of semi-algebraic sets, with applications within algebraic
geometry and beyond. We recently reported on a new implementation of CAD in
Maple which implemented the original algorithm of Collins and the subsequent
improvement to projection by McCallum. Our implementation was in contrast to
Maple's in-built CAD command, based on a quite separate theory. Although
initially developed as an investigative tool to compare the algorithms, we
found and reported that our code offered functionality not currently available
in any other existing implementations. One particularly important piece of
functionality is the ability to produce order-invariant CADs. This has allowed
us to extend the implementation to produce CADs invariant with respect to
either equational constraints (ECCADs) or the truth-tables of sequences of
formulae (TTICADs). This new functionality is contained in the second release
of our code, along with commands to consider problem formulation which can be a
major factor in the tractability of a CAD. In the report we describe the new
functionality and some theoretical discoveries it prompted. We describe how the
CADs produced using equational constraints are able to take advantage of not
just improved projection but also improvements in the lifting phase. We also
present an extension to the original TTICAD algorithm which increases both the
applicability of TTICAD and its relative benefit over other algorithms. The
code and an introductory Maple worksheet / pdf demonstrating the full
functionality of the package are freely available online."
"There have been some effective tools for solving (constant/parametric)
semi-algebraic systems in Maple's library RegularChains since Maple 13. By
using the functions of the library, e.g., RealRootClassfication, one can prove
and discover polynomial inequalities. This paper is more or less a user guide
on using RealRootClassfication to prove the nonnegativity of polynomials. We
show by examples how to use this powerful tool to prove a polynomial is
nonnegative under some polynomial inequality and/or equation constraints. Some
tricks for using the tool are also provided."
"Students of our department solve algebraic exercises in mathematical logic in
a computerized environment. They construct transformations step by step and the
program checks the syntax, equivalence of expressions and completion of the
task. With our current project, we add a program component for checking
relevance of the steps."
"Signature-based algorithms is a popular kind of algorithms for computing
Gr\""obner bases, and many related papers have been published recently. In this
paper, no new signature-based algorithms and no new proofs are presented.
Instead, a view of signature-based algorithms is given, that is,
signature-based algorithms can be regarded as an extended version of the famous
MMM algorithm. By this view, this paper aims to give an easier way to
understand signature-based Gr\""obner basis algorithms."
"In this paper, we are concerned with the problem of determining the existence
of multiple equilibria in economic models. We propose a general and complete
approach for identifying multiplicities of equilibria in semi-algebraic
economies, which may be expressed as semi-algebraic systems. The approach is
based on triangular decomposition and real solution classification, two
powerful tools of algebraic computation. Its effectiveness is illustrated by
two examples of application."
"We show that the number of digits in the integers of a creative telescoping
relation of expected minimal order for a bivariate proper hypergeometric term
has essentially cubic growth with the problem size. For telescopers of higher
order but lower degree we obtain a quintic bound. Experiments suggest that
these bounds are tight. As applications of our results, we give an improved
bound on the maximal possible integer root of the leading coefficient of a
telescoper, and the first discussion of the bit complexity of creative
telescoping."
"In this paper, we are concerned with the problem of counting the
multiplicities of a zero-dimensional regular set's zeros. We generalize the
squarefree decomposition of univariate polynomials to the so-called pseudo
squarefree decomposition of multivariate polynomials, and then propose an
algorithm for decomposing a regular set into a finite number of simple sets.
From the output of this algorithm, the multiplicities of zeros could be
directly read out, and the real solution isolation with multiplicity can also
be easily produced. Experiments with a preliminary implementation show the
efficiency of our method."
"In this paper, we extend the idea of comprehensive Gr\""{o}bner bases given by
Weispfenning (1992) to border bases for zero dimensional parametric polynomial
ideals. For this, we introduce a notion of comprehensive border bases and
border system, and prove their existence even in the cases where they do not
correspond to any term order. We further present algorithms to compute
comprehensive border bases and border system. Finally, we study the relation
between comprehensive Gr\""{o}bner bases and comprehensive border bases w.r.t. a
term order and give an algorithm to compute such comprehensive border bases
from comprehensive Gr\""{o}bner bases."
"We improve the local generic position method for isolating the real roots of
a zero-dimensional bivariate polynomial system with two polynomials and extend
the method to general zero-dimensional polynomial systems. The method mainly
involves resultant computation and real root isolation of univariate polynomial
equations. The roots of the system have a linear univariate representation. The
complexity of the method is $\tilde{O}_B(N^{10})$ for the bivariate case, where
$N=\max(d,\tau)$, $d$ resp., $\tau$ is an upper bound on the degree, resp., the
maximal coefficient bitsize of the input polynomials. The algorithm is
certified with probability 1 in the multivariate case. The implementation shows
that the method is efficient, especially for bivariate polynomial systems."
"We study the complexity of Gr\""obner bases computation, in particular in the
generic situation where the variables are in simultaneous Noether position with
respect to the system.
  We give a bound on the number of polynomials of degree $d$ in a Gr\""obner
basis computed by Faug\`ere's $F_5$ algorithm~(Fau02) in this generic case for
the grevlex ordering (which is also a bound on the number of polynomials for a
reduced Gr\""obner basis, independently of the algorithm used). Next, we analyse
more precisely the structure of the polynomials in the Gr\""obner bases with
signatures that $F_5$ computes and use it to bound the complexity of the
algorithm.
  Our estimates show that the version of~$F_5$ we analyse, which uses only
standard Gaussian elimination techniques, outperforms row reduction of the
Macaulay matrix with the best known algorithms for moderate degrees, and even
for degrees up to the thousands if Strassen's multiplication is used. The
degree being fixed, the factor of improvement grows exponentially with the
number of variables."
"We consider the problem of counting (stable) equilibriums of an important
family of algebraic differential equations modeling multistable biological
regulatory systems. The problem can be solved, in principle, using real
quantifier elimination algorithms, in particular real root classification
algorithms. However, it is well known that they can handle only very small
cases due to the enormous computing time requirements. In this paper, we
present a special algorithm which is much more efficient than the general
methods. Its efficiency comes from the exploitation of certain interesting
structures of the family of differential equations."
"We present a detailed and simplified version of Hrushovski's algorithm that
determines the Galois group of a linear differential equation. There are three
major ingredients in this algorithm. The first is to look for a degree bound
for proto-Galois groups, which enables one to compute one of them. The second
is to determine the identity component of the Galois group that is the pullback
of a torus to the proto-Galois group. The third is to recover the Galois group
from its identity component and a finite Galois group."
"The Apagodu-Zeilberger algorithm can be used for computing annihilating
operators for definite sums over hypergeometric terms, or for definite
integrals over hyperexponential functions. In this paper, we propose a
generalization of this algorithm which is applicable to arbitrary
$\partial$-finite functions. In analogy to the hypergeometric case, we
introduce the notion of proper $\partial$-finite functions. We show that the
algorithm always succeeds for these functions, and we give a tight a priori
bound for the order of the output operator."
"Toric (or sparse) elimination theory is a framework developped during the
last decades to exploit monomial structures in systems of Laurent polynomials.
Roughly speaking, this amounts to computing in a \emph{semigroup algebra},
\emph{i.e.} an algebra generated by a subset of Laurent monomials. In order to
solve symbolically sparse systems, we introduce \emph{sparse Gr\""obner bases},
an analog of classical Gr\""obner bases for semigroup algebras, and we propose
sparse variants of the $F_5$ and FGLM algorithms to compute them. Our prototype
""proof-of-concept"" implementation shows large speed-ups (more than 100 for some
examples) compared to optimized (classical) Gr\""obner bases software. Moreover,
in the case where the generating subset of monomials corresponds to the points
with integer coordinates in a normal lattice polytope $\mathcal P\subset\mathbb
R^n$ and under regularity assumptions, we prove complexity bounds which depend
on the combinatorial properties of $\mathcal P$. These bounds yield new
estimates on the complexity of solving $0$-dim systems where all polynomials
share the same Newton polytope (\emph{unmixed case}). For instance, we
generalize the bound $\min(n_1,n_2)+1$ on the maximal degree in a Gr\""obner
basis of a $0$-dim. bilinear system with blocks of variables of sizes
$(n_1,n_2)$ to the multilinear case: $\sum n_i - \max(n_i)+1$. We also propose
a variant of Fr\""oberg's conjecture which allows us to estimate the complexity
of solving overdetermined sparse systems."
"The PSLQ algorithm is one of the most popular algorithm for finding
nontrivial integer relations for several real numbers. In the present work, we
present an incremental version of PSLQ. For some applications needing to call
PSLQ many times, such as finding the minimal polynomial of an algebraic number
without knowing the degree, the incremental PSLQ algorithm is more efficient
than PSLQ, both theoretically and practically."
"In this paper we introduce the notion of an Open Non-uniform Cylindrical
Algebraic Decomposition (NuCAD), and present an efficient model-based algorithm
for constructing an Open NuCAD from an input formula. A NuCAD is a
generalization of Cylindrical Algebraic Decomposition (CAD) as defined by
Collins in his seminal work from the early 1970s, and as extended in concepts
like Hong's partial CAD. A NuCAD, like a CAD, is a decomposition of
n-dimensional real space into cylindrical cells. But unlike a CAD, the cells in
a NuCAD need not be arranged cylindrically. It is in this sense that NuCADs are
not uniformly cylindrical. However, NuCADs--- like CADs --- carry a tree-like
structure that relates different cells. It is a very different tree but, as
with the CAD tree structure, it allows some operations to be performed
efficiently, for example locating the containing cell for an arbitrary input
point."
"Fruit of the relationship of our research group with the team coordinated by
the biologist Miguel Morales (http://spineup.es), we have applied different
topo-geometric techniques for neuronal image processing. The images, captured
with a powerful confocal microscope, allow to study the evolution of synaptic
density under the influence of various substances, with the aim of studying
neurodegenerative diseases like Alzheimer.
  In the paper we make a brief review of the techniques that appear in our
bioinformatic problems, including the calculation of ordinary and persistent
homology (for which one can use the program Kenzo for symbolic computation in
algebraic topology ) and classical problems of digital topology as skeleton
location and path tracking. We focus on some particular cases of recent
application, with which we will illustrate the previous techniques."
"We discuss issues of problem formulation for algorithms in real algebraic
geometry, focussing on quantifier elimination by cylindrical algebraic
decomposition. We recall how the variable ordering used can have a profound
effect on both performance and output and summarise what may be done to assist
with this choice. We then survey other questions of problem formulation and
algorithm optimisation that have become pertinent following advances in CAD
theory, including both work that is already published and work that is
currently underway. With implementations now in reach of real world
applications and new theory meaning algorithms are far more sensitive to the
input, our thesis is that intelligently formulating problems for algorithms,
and indeed choosing the correct algorithm variant for a problem, is key to
improving the practical use of both quantifier elimination and symbolic real
algebraic geometry in general."
"An improved characteristic set algorithm for solving Boolean polynomial
systems is pro- posed. This algorithm is based on the idea of converting all
the polynomials into monic ones by zero decomposition, and using additions to
obtain pseudo-remainders. Three important techniques are applied in the
algorithm. The first one is eliminating variables by new gener- ated linear
polynomials. The second one is optimizing the strategy of choosing polynomial
for zero decomposition. The third one is to compute add-remainders to eliminate
the leading variable of new generated monic polynomials. By analyzing the depth
of the zero decompo- sition tree, we present some complexity bounds of this
algorithm, which are lower than the complexity bounds of previous
characteristic set algorithms. Extensive experimental results show that this
new algorithm is more efficient than previous characteristic set algorithms for
solving Boolean polynomial systems."
"En este trabajo se presenta una propuesta para realizar Diferenciaci\'on
Autom\'atica Anidada utilizando cualquier biblioteca de Diferenciaci\'on
Autom\'atica que permita sobrecarga de operadores. Para calcular las derivadas
anidadas en una misma evaluaci\'on de la funci\'on, la cual se asume que sea
anal\'itica, se trabaja con el modo forward utilizando una nueva estructura
llamada SuperAdouble, que garantiza que se aplique correctamente la
Diferenciaci\'on Autom\'atica y se calculen el valor y la derivada que se
requiera.
  This paper proposes a framework to apply Nested Automatic Differentiation
using any library of Automatic Differentiation which allows operator
overloading. To compute nested derivatives of a function while it is being
evaluated, which is assumed to be analytic, a new structure called SuperAdouble
is used in the forward mode. This new class guarantees the correct application
of Automatic Differentiation to calculate the value and derivative of a
function where is required."
"We present an algorithm which computes a cylindrical algebraic decomposition
of a semialgebraic set using projection sets computed for each cell separately.
Such local projection sets can be significantly smaller than the global
projection set used by the Cylindrical Algebraic Decomposition (CAD) algorithm.
This leads to reduction in the number of cells the algorithm needs to
construct. We give an empirical comparison of our algorithm and the classical
CAD algorithm."
"We discuss theoretical and algorithmic questions related to the $p$-curvature
of differential operators in characteristic $p$. Given such an operator $L$,
and denoting by $\Chi(L)$ the characteristic polynomial of its $p$-curvature,
we first prove a new, alternative, description of $\Chi(L)$. This description
turns out to be particularly well suited to the fast computation of $\Chi(L)$
when $p$ is large: based on it, we design a new algorithm for computing
$\Chi(L)$, whose cost with respect to $p$ is $\softO(p^{0.5})$ operations in
the ground field. This is remarkable since, prior to this work, the fastest
algorithms for this task, and even for the subtask of deciding nilpotency of
the $p$-curvature, had merely slightly subquadratic complexity
$\softO(p^{1.79})$."
"Let $V\in\mathbb{Q}(i)(\a_1,\dots,\a_n)(\q_1,\q_2)$ be a rationally
parametrized planar homogeneous potential of homogeneity degree $k\neq -2, 0,
2$. We design an algorithm that computes polynomial \emph{necessary} conditions
on the parameters $(\a_1,\dots,\a_n)$ such that the dynamical system associated
to the potential $V$ is integrable. These conditions originate from those of
the Morales-Ramis-Sim\'o integrability criterion near all Darboux points. The
implementation of the algorithm allows to treat applications that were out of
reach before, for instance concerning the non-integrability of polynomial
potentials up to degree $9$. Another striking application is the first complete
proof of the non-integrability of the \emph{collinear three body problem}."
"En este trabajo se presenta una propuesta para realizar Diferenciaci\'on
Autom\'atica Anidada utilizando cualquier biblioteca de Diferenciaci\'on
Autom\'atica que permita sobrecarga de operadores. Para calcular las derivadas
anidadas en una misma evaluaci\'on de la funci\'on, la cual se asume que sea
anal'itica, se trabaja con el modo forward utilizando una nueva estructura
llamada SuperAdouble, que garantiza que se aplique correctamente la
diferenciaci\'on autom\'atica y se calculen el valor y la derivada que se
requiera. Tambi\'en se presenta un enfoque algebraico de la Diferenciaci\'on
Autom\'atica y en particular del espacio de los SuperAdoubles.
  This paper proposes a framework to apply Nested Automatic Differentiation
using any library of Automatic Differentiation which allows operator
overloading. To compute nested derivatives of a function while it is being
evaluated, which is assumed to be analytic, a new structure called SuperAdouble
is used in the forward mode. This new class guarantees the correct application
of Automatic Differentiation to calculate the value and derivative of a
function where is required. Also, an Automatic Differentiation algebraic point
of view is presented with particular emphasis in Nested Automatic
Differentiation."
"Cylindrical algebraic decomposition (CAD) is a key tool for problems in real
algebraic geometry and beyond. When using CAD there is often a choice over the
variable ordering to use, with some problems infeasible in one ordering but
simple in another. Here we discuss a recent experiment comparing three
heuristics for making this choice on thousands of examples."
"Cylindrical algebraic decomposition (CAD) is an important tool, both for
quantifier elimination over the reals and a range of other applications.
Traditionally, a CAD is built through a process of projection and lifting to
move the problem within Euclidean spaces of changing dimension. Recently, an
alternative approach which first decomposes complex space using triangular
decomposition before refining to real space has been introduced and implemented
within the RegularChains Library of Maple. We here describe a freely available
package ProjectionCAD which utilises the routines within the RegularChains
Library to build CADs by projection and lifting. We detail how the projection
and lifting algorithms were modified to allow this, discuss the motivation and
survey the functionality of the package."
"Cylindrical algebraic decomposition (CAD) is a key tool for solving problems
in real algebraic geometry and beyond. In recent years a new approach has been
developed, where regular chains technology is used to first build a
decomposition in complex space. We consider the latest variant of this which
builds the complex decomposition incrementally by polynomial and produces CADs
on whose cells a sequence of formulae are truth-invariant. Like all CAD
algorithms the user must provide a variable ordering which can have a profound
impact on the tractability of a problem. We evaluate existing heuristics to
help with the choice for this algorithm, suggest improvements and then derive a
new heuristic more closely aligned with the mechanics of the new algorithm."
"The concept of comprehensive triangular decomposition (CTD) was first
introduced by Chen et al. in their CASC'2007 paper and could be viewed as an
analogue of comprehensive Grobner systems for parametric polynomial systems.
The first complete algorithm for computing CTD was also proposed in that paper
and implemented in the RegularChains library in Maple. Following our previous
work on generic regular decomposition for parametric polynomial systems, we
introduce in this paper a so-called hierarchical strategy for computing CTDs.
Roughly speaking, for a given parametric system, the parametric space is
divided into several sub-spaces of different dimensions and we compute CTDs
over those sub-spaces one by one. So, it is possible that, for some benchmarks,
it is difficult to compute CTDs in reasonable time while this strategy can
obtain some ""partial"" solutions over some parametric sub-spaces. The program
based on this strategy has been tested on a number of benchmarks from the
literature. Experimental results on these benchmarks with comparison to
RegularChains are reported and may be valuable for developing more efficient
triangularization tools."
"A summation framework is developed that enhances Karr's difference field
approach. It covers not only indefinite nested sums and products in terms of
transcendental extensions, but it can treat, e.g., nested products defined over
roots of unity. The theory of the so-called $R\Pi\Sigma^*$-extensions is
supplemented by algorithms that support the construction of such difference
rings automatically and that assist in the task to tackle symbolic summation
problems. Algorithms are presented that solve parameterized telescoping
equations, and more generally parameterized first-order difference equations,
in the given difference ring. As a consequence, one obtains algorithms for the
summation paradigms of telescoping and Zeilberger's creative telescoping. With
this difference ring theory one obtains a rigorous summation machinery that has
been applied to numerous challenging problems coming, e.g., from combinatorics
and particle physics."
"A zero-dimensional polynomial ideal may have a lot of complex zeros. But
sometimes, only some of them are needed. In this paper, for a zero-dimensional
ideal $I$, we study its complex zeros that locate in another variety
$\textbf{V}(J)$ where $J$ is an arbitrary ideal.
  The main problem is that for a point in $\textbf{V}(I) \cap
\textbf{V}(J)=\textbf{V}(I+J)$, its multiplicities w.r.t. $I$ and $I+J$ may be
different. Therefore, we cannot get the multiplicity of this point w.r.t. $I$
by studying $I + J$. A straightforward way is that first compute the points of
$\textbf{V}(I + J)$, then study their multiplicities w.r.t. $I$. But the former
step is difficult to realize exactly.
  In this paper, we propose a natural geometric explanation of the localization
of a polynomial ring corresponding to a semigroup order. Then, based on this
view, using the standard basis method and the border basis method, we introduce
a way to compute the complex zeros of $I$ in $\textbf{V}(J)$ with their
multiplicities w.r.t. $I$. As an application, we compute the sum of Milnor
numbers of the singular points on a polynomial hypersurface and work out all
the singular points on the hypersurface with their Milnor numbers."
"Computing the determinant of a matrix with the univariate and multivariate
polynomial entries arises frequently in the scientific computing and
engineering fields. In this paper, an effective algorithm is presented for
computing the determinant of a matrix with polynomial entries using hybrid
symbolic and numerical computation. The algorithm relies on the Newton's
interpolation method with error control for solving Vandermonde systems. It is
also based on a novel approach for estimating the degree of variables, and the
degree homomorphism method for dimension reduction. Furthermore, the
parallelization of the method arises naturally."
"The GVW algorithm, presented by Gao et al., is a signature-based algorithm
for computing Gr\""obner bases. In this paper, a variant of GVW is presented.
This new algorithm is called a monomial-oriented GVW algorithm or mo-GVW
algorithm for short. The mo-GVW algorithm presents a new frame of GVW and
regards {\em labeled monomials} instead of {\em labeled polynomials} as basic
elements of the algorithm. Being different from the original GVW algorithm, for
each labeled monomial, the mo-GVW makes efforts to find the smallest signature
that can generate this monomial. The mo-GVW algorithm also avoids generating
J-pairs, and uses efficient methods of searching reducers and checking
criteria. Thus, the mo-GVW algorithm has a better performance during practical
implementations."
"We present in this paper a canonical form for the elements in the ring of
continuous piecewise polynomial functions. This new representation is based on
the use of a particular class of functions
$$\{C_i(P):P\in\Q[x],i=0,\ldots,\deg(P)\}$$ defined by $$C_i(P)(x)= \left\{
\begin{array}{cll}0 & \mbox{ if } & x \leq \alpha \\ P(x) & \mbox{ if } & x
\geq \alpha \end{array} \right.$$ where $\alpha$ is the $i$-th real root of the
polynomial $P$. These functions will allow us to represent and manipulate
easily every continuous piecewise polynomial function through the use of the
corresponding canonical form.
  It will be also shown how to produce a ""rational"" representation of each
function $C_{i}(P)$ allowing its evaluation by performing only operations in
$\Q$ and avoiding the use of any real algebraic number."
"We present an algebraic framework to represent indefinite nested sums over
hypergeometric expressions in difference rings. In order to accomplish this
task, parts of Karr's difference field theory have been extended to a ring
theory in which also the alternating sign can be expressed. The underlying
machinery relies on algorithms that compute all solutions of a given
parameterized telescoping equation. As a consequence, we can solve the
telescoping and creative telescoping problem in such difference rings."
"This note shows the equivalence of two projection operators which both can be
used in cylindrical algebraic decomposition (CAD) . One is known as Brown's
Projection (C. W. Brown (2001)); the other was proposed by Lu Yang in his
earlier work (L.Yang and S.~H. Xia (2000)) that is sketched as follows: given a
polynomial $f$ in $x_1,\,x_2,\,\cdots$, by $f_1$ denote the resultant of $f$
and its partial derivative with respect to $x_1$ (removing the multiple
factors), by $f_2$ denote the resultant of $f_1$ and its partial derivative
with respect to $x_2$, (removing the multiple factors), $\cdots$, repeat this
procedure successively until the last resultant becomes a univariate
polynomial. Making use of an identity, the equivalence of these two projection
operators is evident."
"Blackbox algorithms for linear algebra problems start with projection of the
sequence of powers of a matrix to a sequence of vectors (Lanczos), a sequence
of scalars (Wiedemann) or a sequence of smaller matrices (block methods). Such
algorithms usually depend on the minimal polynomial of the resulting sequence
being that of the given matrix. Here exact formulas are given for the
probability that this occurs. They are based on the generalized Jordan normal
form (direct sum of companion matrices of the elementary divisors) of the
matrix. Sharp bounds follow from this for matrices of unknown elementary
divisors. The bounds are valid for all finite field sizes and show that a small
blocking factor can give high probability of success for all cardinalities and
matrix dimensions."
"Solving polynomial systems arising from applications is frequently made
easier by the structure of the systems. Weighted homogeneity (or
quasi-homogeneity) is one example of such a structure: given a system of
weights $W=(w\_{1},\dots,w\_{n})$, $W$-homogeneous polynomials are polynomials
which are homogeneous w.r.t the weighted degree
$\deg\_{W}(X\_{1}^{\alpha\_{1}},\dots,X\_{n}^{\alpha\_{n}}) = \sum
w\_{i}\alpha\_{i}$. Gr\""obner bases for weighted homogeneous systems can be
computed by adapting existing algorithms for homogeneous systems to the
weighted homogeneous case. We show that in this case, the complexity estimate
for Algorithm~\F5 $\left(\binom{n+\dmax-1}{\dmax}^{\omega}\right)$ can be
divided by a factor $\left(\prod w\_{i} \right)^{\omega}$. For zero-dimensional
systems, the complexity of Algorithm~\FGLM $nD^{\omega}$ (where $D$ is the
number of solutions of the system) can be divided by the same factor
$\left(\prod w\_{i} \right)^{\omega}$. Under genericity assumptions, for
zero-dimensional weighted homogeneous systems of $W$-degree
$(d\_{1},\dots,d\_{n})$, these complexity estimates are polynomial in the
weighted B\'ezout bound $\prod\_{i=1}^{n}d\_{i} / \prod\_{i=1}^{n}w\_{i}$.
Furthermore, the maximum degree reached in a run of Algorithm \F5 is bounded by
the weighted Macaulay bound $\sum (d\_{i}-w\_{i}) + w\_{n}$, and this bound is
sharp if we can order the weights so that $w\_{n}=1$. For overdetermined
semi-regular systems, estimates from the homogeneous case can be adapted to the
weighted case. We provide some experimental results based on systems arising
from a cryptography problem and from polynomial inversion problems. They show
that taking advantage of the weighted homogeneous structure yields substantial
speed-ups, and allows us to solve systems which were otherwise out of reach."
"Maximum likelihood estimation (MLE) is a fundamental computational problem in
statistics. The problem is to maximize the likelihood function with respect to
given data on a statistical model. An algebraic approach to this problem is to
solve a very structured parameterized polynomial system called likelihood
equations. For general choices of data, the number of complex solutions to the
likelihood equations is finite and called the ML-degree of the model. The only
solutions to the likelihood equations that are statistically meaningful are the
real/positive solutions. However, the number of real/positive solutions is not
characterized by the ML-degree. We use discriminants to classify data according
to the number of real/positive solutions of the likelihood equations. We call
these discriminants data-discriminants (DD). We develop a probabilistic
algorithm for computing DDs. Experimental results show that, for the benchmarks
we have tried, the probabilistic algorithm is more efficient than the standard
elimination algorithm. Based on the computational results, we discuss the real
root classification problem for the 3 by 3 symmetric matrix~model."
"When building a cylindrical algebraic decomposition (CAD) savings can be made
in the presence of an equational constraint (EC): an equation logically implied
by a formula.
  The present paper is concerned with how to use multiple ECs, propagating
those in the input throughout the projection set. We improve on the approach of
McCallum in ISSAC 2001 by using the reduced projection theory to make savings
in the lifting phase (both to the polynomials we lift with and the cells lifted
over). We demonstrate the benefits with worked examples and a complexity
analysis."
"We describe a new incomplete but terminating method for real root finding for
large multivariate polynomials. We take an abstract view of the polynomial as
the set of exponent vectors associated with sign information on the
coefficients. Then we employ linear programming to heuristically find roots.
There is a specialized variant for roots with exclusively positive coordinates,
which is of considerable interest for applications in chemistry and systems
biology. An implementation of our method combining the computer algebra system
Reduce with the linear programming solver Gurobi has been successfully applied
to input data originating from established mathematical models used in these
areas. We have solved several hundred problems with up to more than 800000
monomials in up to 10 variables with degrees up to 12. Our method has failed
due to its incompleteness in less than 8 percent of the cases."
"The row (resp. column) rank profile of a matrix describes the staircase shape
of its row (resp. column) echelon form. In an ISSAC'13 paper, we proposed a
recursive Gaussian elimination that can compute simultaneously the row and
column rank profiles of a matrix as well as those of all of its leading
sub-matrices, in the same time as state of the art Gaussian elimination
algorithms. Here we first study the conditions making a Gaus-sian elimination
algorithm reveal this information. Therefore, we propose the definition of a
new matrix invariant, the rank profile matrix, summarizing all information on
the row and column rank profiles of all the leading sub-matrices. We also
explore the conditions for a Gaussian elimination algorithm to compute all or
part of this invariant, through the corresponding PLUQ decomposition. As a
consequence, we show that the classical iterative CUP decomposition algorithm
can actually be adapted to compute the rank profile matrix. Used, in a Crout
variant, as a base-case to our ISSAC'13 implementation, it delivers a
significant improvement in efficiency. Second, the row (resp. column) echelon
form of a matrix are usually computed via different dedicated triangular
decompositions. We show here that, from some PLUQ decompositions, it is
possible to recover the row and column echelon forms of a matrix and of any of
its leading sub-matrices thanks to an elementary post-processing algorithm."
"Gaussian elimination with no pivoting and block Gaussian elimination are
attractive alternatives to the customary but communication intensive Gaussian
elimination with partial pivoting (hereafter we use the acronyms GENP, BGE, and
GEPP} provided that the computations proceed safely and numerically safely},
that is, run into neither division by 0 nor numerical problems. Empirically,
safety and numerical safety of GENP have been consistently observed in a number
of papers where an input matrix was pre-processed with various structured
multipliers chosen ad hoc. Our present paper provides missing formal support
for this empirical observation and explains why it was elusive so far. Namely
we prove that GENP is numerically unsafe for a specific class of input matrices
in spite of its pre-processing with some well-known and well-tested structured
multipliers, but we also prove that GENP and BGE are safe and numerically safe
for the average input matrix pre-processed with any nonsingular and
well-conditioned multiplier. This should embolden search for sparse and
structured multipliers, and we list and test some new classes of them. We also
seek randomized pre-processing that universally (that is, for all input
matrices) supports (i) safe GENP and BGE with probability 1 and/or (ii)
numerically safe GENP and BGE with a probability close to 1.We achieve goal (i)
with a Gaussian structured multiplier and goal (ii) with a Gaussian
unstructured multiplier and alternatively with Gaussian structured
augmentation. We consistently confirm all these formal results with our tests
of GENP for benchmark inputs. We have extended our approach to other
fundamental matrix computations and keep working on further extensions."
"Univariate polynomial root-finding is a classical subject, still important
for modern computing. Frequently one seeks just the real roots of a polynomial
with real coefficients. They can be approximated at a low computational cost if
the polynomial has no nonreal roots, but for high degree polynomials, nonreal
roots are typically much more numerous than the real ones. The challenge is
known for a long time, and the subject has been intensively studied.
Nevertheless, we produce some novel ideas and techniques and obtain dramatic
acceleration of the known algorithms. In order to achieve our progress we
exploit the correlation between the computations with matrices and polynomials,
randomized matrix computations, and complex plane geometry, extend the
techniques of the matrix sign iterations, and use the structure of the
companion matrix of the input polynomial. The results of our extensive tests
with benchmark polynomials and random matrices are quite encouraging. In
particular in our tests the number of iterations required for convergence of
our algorithms grew very slowly (if at all) as we increased the degree of the
univariate input polynomials and the dimension of the input matrices from 64 to
1024."
"The algorithms of Pan (1995) and(2002) approximate the roots of a complex
univariate polynomial in nearly optimal arithmetic and Boolean time but require
precision of computing that exceeds the degree of the polynomial. This causes
numerical stability problems when the degree is large. We observe, however,
that such a difficulty disappears at the initial stage of the algorithms, and
in our present paper we extend this stage to root-finding within a nearly
optimal arithmetic and Boolean complexity bounds provided that some mild
initial isolation of the roots of the input polynomial has been ensured.
Furthermore our algorithm is nearly optimal for the approximation of the roots
isolated in a fixed disc, square or another region on the complex plane rather
than all complex roots of a polynomial. Moreover the algorithm can be applied
to a polynomial given by a black box for its evaluation (even if its
coefficients are not known); it promises to be of practical value for
polynomial root-finding and factorization, the latter task being of interest on
its own right. We also provide a new support for a winding number algorithm,
which enables extension of our progress to obtaining mild initial
approximations to the roots. We conclude with summarizing our algorithms and
their extension to the approximation of isolated multiple roots and root
clusters."
"Let $H\_0, ..., H\_n$ be $m \times m$ matrices with entries in $\QQ$ and
Hankel structure, i.e. constant skew diagonals. We consider the linear Hankel
matrix $H(\vecx)=H\_0+\X\_1H\_1+...+\X\_nH\_n$ and the problem of computing
sample points in each connected component of the real algebraic set defined by
the rank constraint ${\sf rank}(H(\vecx))\leq r$, for a given integer $r \leq
m-1$. Computing sample points in real algebraic sets defined by rank defects in
linear matrices is a general problem that finds applications in many areas such
as control theory, computational geometry, optimization, etc. Moreover, Hankel
matrices appear in many areas of engineering sciences. Also, since Hankel
matrices are symmetric, any algorithmic development for this problem can be
seen as a first step towards a dedicated exact algorithm for solving
semi-definite programming problems, i.e. linear matrix inequalities. Under some
genericity assumptions on the input (such as smoothness of an incidence
variety), we design a probabilistic algorithm for tackling this problem. It is
an adaptation of the so-called critical point method that takes advantage of
the special structure of the problem. Its complexity reflects this: it is
essentially quadratic in specific degree bounds on an incidence variety. We
report on practical experiments and analyze how the algorithm takes advantage
of this special structure. A first implementation outperforms existing
implementations for computing sample points in general real algebraic sets: it
tackles examples that are out of reach of the state-of-the-art."
"Our overall goal is to unify and extend some results in the literature
related to the approximation of generating functions of finite and infinite
sequences over a field by rational functions. In our approach, numerators play
a significant role. We revisit a theorem of Niederreiter on (i) linear
complexities and (ii) '$n^{th}$ minimal polynomials' of an infinite sequence,
proved using partial quotients. We prove (i) and its converse from first
principles and generalise (ii) to rational functions where the denominator need
not have minimal degree. We prove (ii) in two parts: firstly for geometric
sequences and then for sequences with a jump in linear complexity. The basic
idea is to decompose the denominator as a sum of polynomial multiples of two
polynomials of minimal degree; there is a similar decomposition for the
numerators. The decomposition is unique when the denominator has degree at most
the length of the sequence. The proof also applies to rational functions
related to finite sequences, generalising a result of Massey. We give a number
of applications to rational functions associated to sequences."
"We give an example where the number of elements of a Groebner basis in a
Boolean ring is not polynomially bounded in terms of the bitsize and degrees of
the input."
"We present an algorithm that determines the Galois group of linear difference
equations with rational function coefficients."
"In this paper, a new triangular decomposition algorithm is proposed for
ordinary differential polynomial systems, which has triple exponential
computational complexity. The key idea is to eliminate one algebraic variable
from a set of polynomials in one step using the theory of multivariate
resultant. This seems to be the first differential triangular decomposition
algorithm with elementary computation complexity."
"Cylindrical algebraic decomposition (CAD) is an important tool for the
investigation of semi-algebraic sets, with applications in algebraic geometry
and beyond. We have previously reported on an implementation of CAD in Maple
which offers the original projection and lifting algorithm of Collins along
with subsequent improvements.
  Here we report on new functionality: specifically the ability to build
cylindrical algebraic sub-decompositions (sub-CADs) where only certain cells
are returned. We have implemented algorithms to return cells of a prescribed
dimensions or higher (layered {\scad}s), and an algorithm to return only those
cells on which given polynomials are zero (variety {\scad}s). These offer
substantial savings in output size and computation time.
  The code described and an introductory Maple worksheet / pdf demonstrating
the full functionality of the package are freely available online at
http://opus.bath.ac.uk/43911/."
"Background. It is assumed that the introduction of stochastic in mathematical
model makes it more adequate. But there is virtually no methods of coordinated
(depended on structure of the system) stochastic introduction into
deterministic models. Authors have improved the method of stochastic models
construction for the class of one-step processes and illustrated by models of
population dynamics. Population dynamics was chosen for study because its
deterministic models were sufficiently well explored that allows to compare the
results with already known ones.
  Purpose. To optimize the models creation as much as possible some routine
operations should be automated. In this case, the process of drawing up the
model equations can be algorithmized and implemented in the computer algebra
system. Furthermore, on the basis of these results a set of programs for
numerical experiment can be obtained.
  Method. The computer algebra system Axiom is used for analytical calculations
implementation. To perform the numerical experiment FORTRAN and Julia languages
are used. The method Runge--Kutta method for stochastic differential equations
is used as numerical method.
  Results. The program compex for creating stochastic one-step processes models
is constructed. Its application is illustrated by the predator-prey population
dynamic system.
  Conclusions. Computer algebra systems are very convenient for the purposes of
rapid prototyping in mathematical models design and analysis."
"High index differential algebraic equations (DAEs) are ordinary differential
equations (ODEs) with constraints and arise frequently from many mathematical
models of physical phenomenons and engineering fields. In this paper, we
generalize the idea of differential elimination with Dixon resultant to
polynomially nonlinear DAEs. We propose a new algorithm for index reduction of
DAEs and establish the notion of differential algebraic elimination, which can
provide the differential algebraic resultant of the enlarged system of original
equations. To make use of structure of DAEs, variable pencil technique is given
to determine the termination of differentiation. Moreover, we also provide a
heuristics method for removing the extraneous factors from differential
algebraic resultant. The experimentation shows that the proposed algorithm
outperforms existing ones for many examples taken from the literature."
"There are several efficient methods to solve linear interval polynomial
systems in the context of interval computations, however, the general case of
interval polynomial systems is not yet covered as well. In this paper we
introduce a new elimination method to solve and analyse interval polynomial
systems, in general case. This method is based on computational algebraic
geometry concepts such as polynomial ideals and Groebner basis computation.
Specially, we use the comprehensive Groebner system concept to keep the
dependencies between interval coefficients. At the end of paper, we will state
some applications of our method to evaluate its performance."
"We design an algorithm for computing the $p$-curvature of a differential
system in positive characteristic $p$. For a system of dimension $r$ with
coefficients of degree at most $d$, its complexity is $\softO (p d r^\omega)$
operations in the ground field (where $\omega$ denotes the exponent of matrix
multiplication), whereas the size of the output is about $p d r^2$. Our
algorithm is then quasi-optimal assuming that matrix multiplication is
(\emph{i.e.} $\omega = 2$). The main theoretical input we are using is the
existence of a well-suited ring of series with divided powers for which an
analogue of the Cauchy--Lipschitz Theorem holds."
"In this paper, we suggest a new efficient algorithm in order to compute
S-polynomial reduction rapidly in the known algorithm for computing Grobner
bases, and compare the complexity with others."
"The concept of open weak CAD is introduced. Every open CAD is an open weak
CAD. On the contrary, an open weak CAD is not necessarily an open CAD. An
algorithm for computing open weak CADs is proposed. The key idea is to compute
the intersection of projection factor sets produced by different projection
orders. The resulting open weak CAD often has smaller number of sample points
than open CADs.
  The algorithm can be used for computing sample points for all open connected
components of $ f\ne 0$ for a given polynomial $f$. It can also be used for
many other applications, such as testing semi-definiteness of polynomials and
copositive problems. In fact, we solved several difficult semi-definiteness
problems efficiently by using the algorithm. Furthermore, applying the
algorithm to copositive problems, we find an explicit expression of the
polynomials producing open weak CADs under some conditions, which significantly
improves the efficiency of solving copositive problems."
"We describe a simple method that produces automatically closed forms for the
coefficients of continued fractions expansions of a large number of special
functions. The function is specified by a non-linear differential equation and
initial conditions. This is used to generate the first few coefficients and
from there a conjectured formula. This formula is then proved automatically
thanks to a linear recurrence satisfied by some remainder terms. Extensive
experiments show that this simple approach and its straightforward
generalization to difference and $q$-difference equations capture a large part
of the formulas in the literature on continued fractions."
"A computation method of algebraic local cohomology with parameters,
associated with zero-dimensional ideal with parameter, is introduced. This
computation method gives us in particular a decomposition of the parameter
space depending on the structure of algebraic local cohomology classes. This
decomposition informs us several properties of input ideals and the output of
our algorithm completely describes the multiplicity structure of input ideals.
An efficient algorithm for computing a parametric standard basis of a given
zero-dimensional ideal, with respect to an arbitrary local term order, is also
described as an application of the computation method. The algorithm can always
output ""reduced"" standard basis of a given zero-dimensional ideal, even if the
zero-dimensional ideal has parameters."
"The diagonal of a multivariate power series F is the univariate power series
Diag(F) generated by the diagonal terms of F. Diagonals form an important class
of power series; they occur frequently in number theory, theoretical physics
and enumerative combinatorics. We study algorithmic questions related to
diagonals in the case where F is the Taylor expansion of a bivariate rational
function. It is classical that in this case Diag(F) is an algebraic function.
We propose an algorithm that computes an annihilating polynomial for Diag(F).
Generically, it is its minimal polynomial and is obtained in time quasi-linear
in its size. We show that this minimal polynomial has an exponential size with
respect to the degree of the input rational function. We then address the
related problem of enumerating directed lattice walks. The insight given by our
study leads to a new method for expanding the generating power series of
bridges, excursions and meanders. We show that their first N terms can be
computed in quasi-linear complexity in N, without first computing a very large
polynomial equation."
"The diagonal of a multivariate power series F is the univariate power series
Diag(F) generated by the diagonal terms of F. Diagonals form an important class
of power series; they occur frequently in number theory, theoretical physics
and enumerative combinatorics. We study algorithmic questions related to
diagonals in the case where F is the Taylor expansion of a bivariate rational
function. It is classical that in this case Diag(F) is an algebraic function.
We propose an algorithm that computes an annihilating polynomial for Diag(F).
We give a precise bound on the size of this polynomial and show that
generically, this polynomial is the minimal polynomial and that its size
reaches the bound. The algorithm runs in time quasi-linear in this bound, which
grows exponentially with the degree of the input rational function. We then
address the related problem of enumerating directed lattice walks. The insight
given by our study leads to a new method for expanding the generating power
series of bridges, excursions and meanders. We show that their first N terms
can be computed in quasi-linear complexity in N, without first computing a very
large polynomial equation."
"In this paper, we present an algorithm for computing a fundamental matrix of
formal solutions of completely integrable Pfaffian systems with normal
crossings in several variables. This algorithm is a generalization of a method
developed for the bivariate case based on a combination of several reduction
techniques and is implemented in the computer algebra system Maple."
"Polynomial remainder sequences contain the intermediate results of the
Euclidean algorithm when applied to (non-)commutative polynomials. The running
time of the algorithm is dependent on the size of the coefficients of the
remainders. Different ways have been studied to make these as small as
possible. The subresultant sequence of two polynomials is a polynomial
remainder sequence in which the size of the coefficients is optimal in the
generic case, but when taking the input from applications, the coefficients are
often larger than necessary. We generalize two improvements of the subresultant
sequence to Ore polynomials and derive a new bound for the minimal coefficient
size. Our approach also yields a new proof for the results in the commutative
case, providing a new point of view on the origin of the extraneous factors of
the coefficients."
"We introduce a new problem in the approximate computation of Gr\""{o}bner
bases that allows the algorithm to ignore a constant fraction of the generators
- of the algorithm's choice - then compute a Gr\""{o}bner basis for the
remaining polynomial system. The set ignored is subject to one quite-natural
structural constraint. For lexicographic orders, when the discarded fraction is
less than $(1/4-\epsilon)$, for $\epsilon>0$, we prove that this problem cannot
be solved in polynomial time, even when the original polynomial system has
maximum degree 3 and each polynomial contains at most 3 variables.
Qualitatively, even for sparse systems composed of low-degree polynomials, we
show that Gr\""{o}bner basis computation is robustly hard: even producing a
Gr\""{o}bner basis for a large subset of the generators is NP-hard."
"Ore operators form a common algebraic abstraction of linear ordinary
differential and recurrence equations. Given an Ore operator $L$ with
polynomial coefficients in $x$, it generates a left ideal $I$ in the Ore
algebra over the field $\mathbf{k}(x)$ of rational functions. We present an
algorithm for computing a basis of the contraction ideal of $I$ in the Ore
algebra over the ring $R[x]$ of polynomials, where $R$ may be either
$\mathbf{k}$ or a domain with $\mathbf{k}$ as its fraction field. This
algorithm is based on recent work on desingularization for Ore operators by
Chen, Jaroschek, Kauers and Singer. Using a basis of the contraction ideal, we
compute a completely desingularized operator for $L$ whose leading coefficient
not only has minimal degree in $x$ but also has minimal content. Completely
desingularized operators have interesting applications such as certifying
integer sequences and checking special cases of a conjecture of Krattenthaler."
"An algebraic approach to the maximum likelihood estimation problem is to
solve a very structured parameterized polynomial system called likelihood
equations that have finitely many complex (real or non-real) solutions. The
only solutions that are statistically meaningful are the real solutions with
positive coordinates. In order to classify the parameters (data) according to
the number of real/positive solutions, we study how to efficiently compute the
discriminants, say data-discriminants (DD), of the likelihood equations. We
develop a probabilistic algorithm with three different strategies for computing
DDs. Our implemented probabilistic algorithm based on Maple and FGb is more
efficient than our previous version presented in ISSAC2015, and is also more
efficient than the standard elimination for larger benchmarks. By applying
RAGlib to a DD we compute, we give the real root classification of 3 by 3
symmetric matrix model."
"A Chebyshev curve $\mathcal{C}(a,b,c,\phi)$ has a parametrization of the
form$ x(t)=T\_a(t)$; \ $y(t)=T\_b(t)$; $z(t)= T\_c(t + \phi)$, where $a,b,c$are
integers, $T\_n(t)$ is the Chebyshev polynomialof degree $n$ and $\phi \in
\mathbb{R}$. When $\mathcal{C}(a,b,c,\phi)$ is nonsingular,it defines a
polynomial knot. We determine all possible knot diagrams when $\phi$ varies.
Let $a,b,c$ be integers, $a$ is odd, $(a,b)=1$, we show that one can list all
possible knots $\mathcal{C}(a,b,c,\phi)$ in$\tilde{\mathcal{O}}(n^2)$ bit
operations, with $n=abc$."
"We consider the problem of computing the monic gcd of two polynomials over a
number field L = Q(alpha_1,...,alpha_n). Langemyr and McCallum have already
shown how Brown's modular GCD algorithm for polynomials over Q can be modified
to work for Q(alpha) and subsequently, Langemyr extended the algorithm to L[x].
Encarnacion also showed how to use rational number to make the algorithm for
Q(alpha) output sensitive, that is, the number of primes used depends on the
size of the integers in the gcd and not on bounds based on the input
polynomials.
  Our first contribution is an extension of Encarnacion's modular GCD algorithm
to the case n>1, which, like Encarnacion's algorithm, is is output sensitive.
  Our second contribution is a proof that it is not necessary to test if p
divides the discriminant. This simplifies the algorithm; it is correct without
this test.
  Our third contribution is a modification to the algorithm to treat the case
of reducible extensions. Such cases arise when solving systems of polynomial
equations.
  Our fourth contribution is an implementation of the modular GCD algorithm in
Maple and in Magma. Both implementations use a recursive dense polynomial data
structure for representing polynomials over number fields with multiple field
extensions.
  Our fifth contribution is a primitive fraction-free algorithm. This is the
best non-modular approach. We present timing comparisons of the Maple and Magma
implementations demonstrating various optimizations and comparing them with the
monic Euclidan algorithm and our primitive fraction-free algorithm."
"Hilbert order is widely applied in many areas. However, most of the
algorithms are confined to low dimensional cases. In this paper, algorithms for
encoding and decoding arbitrary dimensional Hilbert order are presented. Eight
algorithms are proposed. Four algorithms are based on arithmetic operations and
the other four algorithms are based on bit operations. For the algorithms
complexities, four of them are linear and the other four are constant for given
inputs. In the end of the paper, algorithms for two dimensional Hilbert order
are presented to demonstrate the usage of the algorithms introduced."
"The row (resp. column) rank profile of a matrix describes the stair-case
shape of its row (resp. column) echelon form.We here propose a new matrix
invariant, the rank profile matrix, summarizing all information on the row
andcolumn rank profiles of all the leading sub-matrices.We show that this
normal form exists and is unique over any ring, provided that the notion of
McCoy's rank is used, in the presence of zero divisors.We then explore the
conditions for a Gaussian elimination algorithm to compute all or part of this
invariant,through the corresponding PLUQ decomposition. This enlarges the set
of known Elimination variants that compute row or column rank profiles.As a
consequence a new Crout base case variant significantly improves the practical
efficiency of previously known implementations over a finite field.With
matrices of very small rank, we also generalize the techniques ofStorjohann and
Yang to the computation of the rank profile matrix, achieving an
$(r^\omega+mn)^{1+o(1)}$ time complexity for an $m \times n$ matrix of rank
$r$, where $\omega$ is the exponent of matrix multiplication. Finally, by give
connections to the Bruhat decomposition, and severalof its variants and
generalizations. Thus, our algorithmicimprovements for the PLUQ factorization,
and their implementations,directly apply to these decompositions.In particular,
we show how a PLUQ decomposition revealing the rankprofile matrix also reveals
both a row and a column echelon form ofthe input matrix or of any of its
leading sub-matrices, by a simplepost-processing made of row and column
permutations."
"We discuss how to decide whether a given C-finite sequence can be written
nontrivially as a product of two other C-finite sequences."
"Several algorithms in computer algebra involve the computation of a power
series solution of a given ordinary differential equation. Over finite fields,
the problem is often lifted in an approximate $p$-adic setting to be
well-posed. This raises precision concerns: how much precision do we need on
the input to compute the output accurately? In the case of ordinary
differential equations with separation of variables, we make use of the recent
technique of differential precision to obtain optimal bounds on the stability
of the Newton iteration. The results apply, for example, to algorithms for
manipulating algebraic numbers over finite fields, for computing isogenies
between elliptic curves or for deterministically finding roots of polynomials
in finite fields. The new bounds lead to significant speedups in practice."
"Continuing a series of articles in the past few years on creative telescoping
using reductions, we develop a new algorithm to construct minimal telescopers
for algebraic functions. This algorithm is based on Trager's Hermite reduction
and on polynomial reduction, which was originally designed for hyperexponential
functions and extended to the algebraic case in this paper."
"We compute minimal bases of solutions for a general interpolation problem,
which encompasses Hermite-Pad\'e approximation and constrained multivariate
interpolation, and has applications in coding theory and security.
  This problem asks to find univariate polynomial relations between $m$ vectors
of size $\sigma$; these relations should have small degree with respect to an
input degree shift. For an arbitrary shift, we propose an algorithm for the
computation of an interpolation basis in shifted Popov normal form with a cost
of $\mathcal{O}\tilde{~}(m^{\omega-1} \sigma)$ field operations, where $\omega$
is the exponent of matrix multiplication and the notation
$\mathcal{O}\tilde{~}(\cdot)$ indicates that logarithmic terms are omitted.
  Earlier works, in the case of Hermite-Pad\'e approximation and in the general
interpolation case, compute non-normalized bases. Since for arbitrary shifts
such bases may have size $\Theta(m^2 \sigma)$, the cost bound
$\mathcal{O}\tilde{~}(m^{\omega-1} \sigma)$ was feasible only with restrictive
assumptions on the shift that ensure small output sizes. The question of
handling arbitrary shifts with the same complexity bound was left open.
  To obtain the target cost for any shift, we strengthen the properties of the
output bases, and of those obtained during the course of the algorithm: all the
bases are computed in shifted Popov form, whose size is always $\mathcal{O}(m
\sigma)$. Then, we design a divide-and-conquer scheme. We recursively reduce
the initial interpolation problem to sub-problems with more convenient shifts
by first computing information on the degrees of the intermediate bases."
"We give a Las Vegas algorithm which computes the shifted Popov form of an $m
\times m$ nonsingular polynomial matrix of degree $d$ in expected
$\widetilde{\mathcal{O}}(m^\omega d)$ field operations, where $\omega$ is the
exponent of matrix multiplication and $\widetilde{\mathcal{O}}(\cdot)$
indicates that logarithmic factors are omitted. This is the first algorithm in
$\widetilde{\mathcal{O}}(m^\omega d)$ for shifted row reduction with arbitrary
shifts.
  Using partial linearization, we reduce the problem to the case $d \le \lceil
\sigma/m \rceil$ where $\sigma$ is the generic determinant bound, with $\sigma
/ m$ bounded from above by both the average row degree and the average column
degree of the matrix. The cost above becomes $\widetilde{\mathcal{O}}(m^\omega
\lceil \sigma/m \rceil)$, improving upon the cost of the fastest previously
known algorithm for row reduction, which is deterministic.
  Our algorithm first builds a system of modular equations whose solution set
is the row space of the input matrix, and then finds the basis in shifted Popov
form of this set. We give a deterministic algorithm for this second step
supporting arbitrary moduli in $\widetilde{\mathcal{O}}(m^{\omega-1} \sigma)$
field operations, where $m$ is the number of unknowns and $\sigma$ is the sum
of the degrees of the moduli. This extends previous results with the same cost
bound in the specific cases of order basis computation and M-Pad\'e
approximation, in which the moduli are products of known linear factors."
"Certificates to a linear algebra computation are additional data structures
for each output, which can be used by a-possibly randomized- verification
algorithm that proves the correctness of each output. In this paper, we give an
algorithm that compute a certificate for the minimal polynomial of sparse or
structured n x n matrices over an abstract field, of sufficiently large
cardinality, whose Monte Carlo verification complexity requires a single
matrix-vector multiplication and a linear number of extra field operations. We
also propose a novel preconditioner that ensures irreducibility of the
characteristic polynomial of the preconditioned matrix. This preconditioner
takes linear time to be applied and uses only two random entries. We then
combine these two techniques to give algorithms that compute certificates for
the determinant, and thus for the characteristic polynomial, whose Monte Carlo
verification complexity is therefore also linear."
"We describe how to solve simultaneous Pad\'e approximations over a power
series ring $K[[x]]$ for a field $K$ using $O~(n^{\omega - 1} d)$ operations in
$K$, where $d$ is the sought precision and $n$ is the number of power series to
approximate. We develop two algorithms using different approaches. Both
algorithms return a reduced sub-bases that generates the complete set of
solutions to the input approximations problem that satisfy the given degree
constraints. Our results are made possible by recent breakthroughs in fast
computations of minimal approximant bases and Hermite Pad\'e approximations."
"The class of quasiseparable matrices is defined by a pair of bounds, called
the quasiseparable orders, on the ranks of the maximal sub-matrices entirely
located in their strictly lower and upper triangular parts. These arise
naturally in applications, as e.g. the inverse of band matrices, and are widely
used for they admit structured representations allowing to compute with them in
time linear in the dimension and quadratic with the quasiseparable order. We
show, in this paper, the connection between the notion of quasisepa-rability
and the rank profile matrix invariant, presented in [Dumas \& al. ISSAC'15].
This allows us to propose an algorithm computing the quasiseparable orders (rL,
rU) in time O(n^2 s^($\omega$--2)) where s = max(rL, rU) and $\omega$ the
exponent of matrix multiplication. We then present two new structured
representations, a binary tree of PLUQ decompositions, and the Bruhat
generator, using respectively O(ns log n/s) and O(ns) field elements instead of
O(ns^2) for the previously known generators. We present algorithms computing
these representations in time O(n^2 s^($\omega$--2)). These representations
allow a matrix-vector product in time linear in the size of their
representation. Lastly we show how to multiply two such structured matrices in
time O(n^2 s^($\omega$--2))."
"Given a square, nonsingular matrix of univariate polynomials $\mathbf{F} \in
\mathbb{K}[x]^{n \times n}$ over a field $\mathbb{K}$, we give a fast,
deterministic algorithm for finding the Hermite normal form of $\mathbf{F}$
with complexity $O^{\sim}\left(n^{\omega}d\right)$ where $d$ is the degree of
$\mathbf{F}$. Here soft-$O$ notation is Big-$O$ with log factors removed and
$\omega$ is the exponent of matrix multiplication. The method relies of a fast
algorithm for determining the diagonal entries of its Hermite normal form,
having as cost $O^{\sim}\left(n^{\omega}s\right)$ operations with $s$ the
average of the column degrees of $\mathbf{F}$."
"Given an ideal $\mathfrak{a}$ in $A[x_1, \ldots, x_n]$, where $A$ is a
Noetherian integral domain, we propose an approach to compute the Krull
dimension of $A[x_1,\ldots,x_n]/\mathfrak{a}$, when the residue class
polynomial ring is a free $A$-module. When $A$ is a field, the Krull dimension
of $A[x_1,\ldots,x_n]/\mathfrak{a}$ has several equivalent algorithmic
definitions by which it can be computed. But this is not true in the case of
arbitrary Noetherian rings. For a Noetherian integral domain, $A$ we introduce
the notion of combinatorial dimension of $A[x_1, \ldots,x_n]/\mathfrak{a}$ and
give a Gr\""obner basis method to compute it for residue class polynomial rings
that have a free $A$-module representation w.r.t. a lexicographic ordering. For
such $A$-algebras, we derive a relation between Krull dimension and
combinatorial dimension of $A[x_1, \ldots, x_n]/\mathfrak{a}$. An immediate
application of this relation is that it gives a uniform method, the first of
its kind, to compute the dimension of $A[x_1, \ldots, x_n]/\mathfrak{a}$
without having to consider individual properties of the ideal. For $A$-algebras
that have a free $A$-module representation w.r.t. degree compatible monomial
orderings, we introduce the concepts of Hilbert function, Hilbert series and
Hilbert polynomials and show that Gr\""obner basis methods can be used to
compute these quantities. We then proceed to show that the combinatorial
dimension of such $A$-algebras is equal to the degree of the Hilbert
polynomial. This enables us to extend the relation between Krull dimension and
combinatorial dimension to $A$-algebras with a free $A$-module representation
w.r.t. a degree compatible ordering as well."
"The Truncated Fourier Transform (TFT) is a variation of the Discrete Fourier
Transform (DFT/FFT) that allows for input vectors that do NOT have length $2^n$
for $n$ a positive integer. We present the univariate version of the TFT,
originally due to Joris van der Hoeven, heavily illustrating the presentation
in order to make these methods accessible to a broader audience."
"This is a system paper about a new GPLv2 open source C library GBLA
implementing and improving the idea of Faug\`ere and Lachartre (GB reduction).
We further exploit underlying structures in matrices generated during Gr\""obner
basis computations in algorithms like F4 or F5 taking advantage of block
patterns by using a special data structure called multilines. Moreover, we
discuss a new order of operations for the reduction process. In various
different experimental results we show that GBLA performs better than GB
reduction or Magma in sequential computations (up to 40% faster) and scales
much better than GB reduction for a higher number of cores: On 32 cores we
reach a scaling of up to 26. GBLA is up to 7 times faster than GB reduction.
Further, we compare different parallel schedulers GBLA can be used with. We
also developed a new advanced storage format that exploits the fact that our
matrices are coming from Gr\""obner basis computations, shrinking storage by a
factor of up to 4. A huge database of our matrices is freely available with
GBLA."
"Given a multi-variant polynomial inequality with a parameter, how to find the
best possible value of this parameter that satisfies the inequality? For
instance, find the greatest number $k$ that satisfies $ a^3+b^3+c^3+
k(a^2b+b^2c+c^2a)-(k+1)(ab^2+bc^2+ca^2)\geq 0 $ for all nonnegative real
numbers $ a,b,c $. Analogues problems often appeared in studies of inequalities
and were dealt with by various methods. In this paper, a general algorithm is
proposed for finding the required best possible constant. The algorithm can be
easily implemented by computer algebra tools such as Maple."
"We consider exact matrix decomposition by Gauss-Bareiss reduction. We
investigate two aspects of the process: common row and column factors and the
influence of pivoting strategies. We identify two types of common factors:
systematic and statistical. Systematic factors depend on the process, while
statistical factors depend on the specific data. We show that existing
fraction-free QR (Gram-Schmidt) algorithms create a common factor in the last
column of Q. We relate the existence of row factors in LU decomposition to
factors appearing in the Smith normal form of the matrix. For statistical
factors, we identify mechanisms and give estimates of the frequency. Our
conclusions are tested by experimental data. For pivoting strategies, we
compare the sizes of output factors obtained by different strategies. We also
comment on timing differences."
"Recently, $R\Pi\Sigma^*$-extensions have been introduced which extend Karr's
$\Pi\Sigma^*$-fields substantially: one can represent expressions not only in
terms of transcendental sums and products, but one can work also with products
over primitive roots of unity. Since one can solve the parameterized
telescoping problem in such rings, covering as special cases the summation
paradigms of telescoping and creative telescoping, one obtains a rather
flexible toolbox for symbolic summation. This article is the continuation of
this work. Inspired by Singer's Galois theory of difference equations we will
work out several alternative characterizations of $R\Pi\Sigma^*$-extensions:
adjoining naively sums and products leads to an $R\Pi\Sigma^*$-extension iff
the obtained difference ring is simple iff the ring can be embedded into the
ring of sequences iff the ring can be given by the interlacing of
$\Pi\Sigma^*$-extensions. From the viewpoint of applications this leads to a
fully automatic machinery to represent indefinite nested sums and products in
such $R\Pi\Sigma^*$-rings. In addition, we work out how the parameterized
telescoping paradigm can be used to prove algebraic independence of indefinite
nested sums. Furthermore, one obtains an alternative reduction tactic to solve
the parameterized telescoping problem in basic $R\Pi\Sigma^*$-extensions
exploiting the interlacing property."
"We treat the interpolation problem $ \{f(x_j)=y_j\}_{j=1}^N $ for polynomial
and rational functions. Developing the approach by C.Jacobi, we represent the
interpolants by virtue of the Hankel polynomials generated by the sequences $
\{\sum_{j=1}^N x_j^ky_j/W^{\prime}(x_j) \}_{k\in \mathbb N} $ and $
\{\sum_{j=1}^N x_j^k/(y_jW^{\prime}(x_j)) \}_{k\in \mathbb N} $; here $
W(x)=\prod_{j=1}^N(x-x_j) $. The obtained results are applied for the error
correction problem, i.e. the problem of reconstructing the polynomial from a
redundant set of its values some of which are probably erroneous. The problem
of evaluation of the resultant of polynomials $ p(x) $ and $ q(x) $ from the
set of values $ \{p(x_j)/q(x_j) \}_{j=1}^N $ is also tackled within the
framework of this approach."
"Clifford algebras have broad applications in science and engineering. The use
of Clifford algebras can be further promoted in these fields by availability of
computational tools that automate tedious routine calculations. We offer an
extensive demonstration of the applications of Clifford algebras in
electromagnetism using the geometric algebra G3 = Cl(3,0) as a computational
model in the Maxima computer algebra system. We compare the geometric
algebra-based approach with conventional symbolic tensor calculations supported
by Maxima, based on the itensor package. The Clifford algebra functionality of
Maxima is distributed as two new packages called clifford - for basic
simplification of Clifford products, outer products, scalar products and
inverses; and cliffordan - for applications of geometric calculus."
"To synthesize Maxwell optics systems, the mathematical apparatus of tensor
and vector analysis is generally employed. This mathematical apparatus implies
executing a great number of simple stereotyped operations, which are adequately
supported by computer algebra systems. In this paper, we distinguish between
two stages of working with a mathematical model: model development and model
usage. Each of these stages implies its own computer algebra system. As a model
problem, we consider the problem of geometrization of Maxwell's equations. Two
computer algebra systems---Cadabra and FORM---are selected for use at different
stages of investigation."
"Control theory has recently been involved in the field of nuclear magnetic
resonance imagery. The goal is to control the magnetic field optimally in order
to improve the contrast between two biological matters on the pictures.
Geometric optimal control leads us here to analyze mero-morphic vector fields
depending upon physical parameters , and having their singularities defined by
a deter-minantal variety. The involved matrix has polynomial entries with
respect to both the state variables and the parameters. Taking into account the
physical constraints of the problem, one needs to classify, with respect to the
parameters, the number of real singularities lying in some prescribed
semi-algebraic set. We develop a dedicated algorithm for real root
classification of the singularities of the rank defects of a polynomial matrix,
cut with a given semi-algebraic set. The algorithm works under some genericity
assumptions which are easy to check. These assumptions are not so restrictive
and are satisfied in the aforementioned application. As more general strategies
for real root classification do, our algorithm needs to compute the critical
loci of some maps, intersections with the boundary of the semi-algebraic
domain, etc. In order to compute these objects, the determinantal structure is
exploited through a stratifi-cation by the rank of the polynomial matrix. This
speeds up the computations by a factor 100. Furthermore, our implementation is
able to solve the application in medical imagery, which was out of reach of
more general algorithms for real root classification. For instance,
computational results show that the contrast problem where one of the matters
is water is partitioned into three distinct classes."
"Cylindrical algebraic decomposition (CAD) is an important tool for working
with polynomial systems, particularly quantifier elimination. However, it has
complexity doubly exponential in the number of variables. The base algorithm
can be improved by adapting to take advantage of any equational constraints
(ECs): equations logically implied by the input. Intuitively, we expect the
double exponent in the complexity to decrease by one for each EC. In ISSAC 2015
the present authors proved this for the factor in the complexity bound
dependent on the number of polynomials in the input. However, the other term,
that dependent on the degree of the input polynomials, remained unchanged.
  In the present paper the authors investigate how CAD in the presence of ECs
could be further refined using the technology of Groebner Bases to move towards
the intuitive bound for polynomial degree."
"Let V $\subset$ C n be an equidimensional algebraic set and g be an n-variate
polynomial with rational coefficients. Computing the critical points of the map
that evaluates g at the points of V is a cornerstone of several algorithms in
real algebraic geometry and optimization. Under the assumption that the
critical locus is finite and that the projective closure of V is smooth, we
provide sharp upper bounds on the degree of the critical locus which depend
only on deg(g) and the degrees of the generic polar varieties associated to V.
Hence, in some special cases where the degrees of the generic polar varieties
do not reach the worst-case bounds, this implies that the number of critical
points of the evaluation map of g is less than the currently known degree
bounds. We show that, given a lifting fiber of V , a slight variant of an
algorithm due to Bank, Giusti, Heintz, Lecerf, Matera and Solern{\'o} computes
these critical points in time which is quadratic in this bound up to
logarithmic factors, linear in the complexity of evaluating the input system
and polynomial in the number of variables and the maximum degree of the input
polynomials."
"Polynomial Systems, or at least their algorithms, have the reputation of
being doubly-exponential in the number of variables [Mayr and Mayer, 1982],
[Davenport and Heintz, 1988]. Nevertheless, the Bezout bound tells us that that
number of zeros of a zero-dimensional system is singly-exponential in the
number of variables. How should this contradiction be reconciled?
  We first note that [Mayr and Ritscher, 2013] shows that the doubly
exponential nature of Gr\""{o}bner bases is with respect to the dimension of the
ideal, not the number of variables. This inspires us to consider what can be
done for Cylindrical Algebraic Decomposition which produces a
doubly-exponential number of polynomials of doubly-exponential degree.
  We review work from ISSAC 2015 which showed the number of polynomials could
be restricted to doubly-exponential in the (complex) dimension using McCallum's
theory of reduced projection in the presence of equational constraints. We then
discuss preliminary results showing the same for the degree of those
polynomials. The results are under primitivity assumptions whose importance we
illustrate."
"Two models were recently proposed to explore the robust hardness of Gr\""obner
basis computation. Given a polynomial system, both models allow an algorithm to
selectively ignore some of the polynomials: the algorithm is only responsible
for returning a Gr\""obner basis for the ideal generated by the remaining
polynomials. For the $q$-Fractional Gr\""obner Basis Problem the algorithm is
allowed to ignore a constant $(1-q)$-fraction of the polynomials (subject to
one natural structural constraint). Here we prove a new strongest-parameter
result: even if the algorithm is allowed to choose a $(3/10-\epsilon)$-fraction
of the polynomials to ignore, and need only compute a Gr\""obner basis with
respect to some lexicographic order for the remaining polynomials, this cannot
be accomplished in polynomial time (unless $P=NP$). This statement holds even
if every polynomial has maximum degree 3. Next, we prove the first robust
hardness result for polynomial systems of maximum degree 2: for the
$q$-Fractional model a $(1/5-\epsilon)$ fraction of the polynomials may be
ignored without losing provable NP-Hardness. Both theorems hold even if every
polynomial contains at most three distinct variables. Finally, for the Strong
$c$-partial Gr\""obner Basis Problem of De Loera et al. we give conditional
results that depend on famous (unresolved) conjectures of Khot and Dinur, et
al."
"Creative telescoping is a powerful computer algebra paradigm -initiated by
Doron Zeilberger in the 90's- for dealing with definite integrals and sums with
parameters. We address the mixed continuous-discrete case, and focus on the
integration of bivariate hypergeometric-hyperexponential terms. We design a new
creative telescoping algorithm operating on this class of inputs, based on a
Hermite-like reduction procedure. The new algorithm has two nice features: it
is efficient and it delivers, for a suitable representation of the input, a
minimal-order telescoper. Its analysis reveals tight bounds on the sizes of the
telescoper it produces."
"B{\'e}zout 's theorem states that dense generic systems of n multivariate
quadratic equations in n variables have 2 n solutions over algebraically closed
fields. When only a small subset M of monomials appear in the equations
(fewnomial systems), the number of solutions may decrease dramatically. We
focus in this work on subsets of quadratic monomials M such that generic
systems with support M do not admit any solution at all. For these systems,
Hilbert's Nullstellensatz ensures the existence of algebraic certificates of
inconsistency. However, up to our knowledge all known bounds on the sizes of
such certificates -including those which take into account the Newton polytopes
of the polynomials- are exponential in n. Our main results show that if the
inequality 2|M| -- 2n $\le$ $\sqrt$ 1 + 8{\nu} -- 1 holds for a quadratic
fewnomial system -- where {\nu} is the matching number of a graph associated
with M, and |M| is the cardinality of M -- then there exists generically a
certificate of inconsistency of linear size (measured as the number of
coefficients in the ground field K). Moreover this certificate can be computed
within a polynomial number of arithmetic operations. Next, we evaluate how
often this inequality holds, and we give evidence that the probability that the
inequality is satisfied depends strongly on the number of squares. More
precisely, we show that if M is picked uniformly at random among the subsets of
n + k + 1 quadratic monomials containing at least $\Omega$(n 1/2+$\epsilon$)
squares, then the probability that the inequality holds tends to 1 as n grows.
Interestingly, this phenomenon is related with the matching number of random
graphs in the Erd{\""o}s-Renyi model. Finally, we provide experimental results
showing that certificates in inconsistency can be computed for systems with
more than 10000 variables and equations."
"The $p$-curvature of a system of linear differential equations in positive
characteristic $p$ is a matrix that measures how far the system is from having
a basis of polynomial solutions. We show that the similarity class of the
$p$-curvature can be determined without computing the $p$-curvature itself.
More precisely, we design an algorithm that computes the invariant factors of
the $p$-curvature in time quasi-linear in $\sqrt p$. This is much less than the
size of the $p$-curvature, which is generally linear in $p$. The new algorithm
allows to answer a question originating from the study of the Ising model in
statistical physics."
"Multi-homogeneous polynomial systems arise in many applications. We provide
bit complexity estimates for solving them which, up to a few extra other
factors, are quadratic in the number of solutions and linear in the height of
the input system under some genericity assumptions. The assumptions essentially
imply that the Jacobian matrix of the system under study has maximal rank at
the solution set and that this solution set if finite. The algorithm is
probabilistic and a probability analysis is provided. Next, we apply these
results to the problem of optimizing a linear map on the real trace of an
algebraic set. Under some genericity assumptions, we provide bit complexity
estimates for solving this polynomial minimization problem."
"We present a new open source implementation in the SageMath computer algebra
system of algorithms for the numerical solution of linear ODEs with polynomial
coefficients. Our code supports regular singular connection problems and
provides rigorous error bounds."
"Algorithms of numeric (in exact arithmetic) deduction of analytical
expressions, proposed and described by Shevchenko and Vasiliev (1993), are
developed and implemented in a computer algebra code. This code is built as a
superstructure for the computer algebra package by Shevchenko and Sokolsky
(1993a) for normalization of Hamiltonian systems of ordinary differential
equations, in order that high complexity problems of normalization could be
solved. As an example, a resonant normal form of a Hamiltonian describing the
hyperboloidal precession of a dynamically symmetric satellite is derived by
means of the numeric deduction technique. The technique provides a considerable
economy, about 30 times in this particular application, in computer memory
consumption. It is naturally parallelizable. Thus the economy of memory
consumption is convertible into a gain in computation speed."
"Given a nonsingular $n \times n$ matrix of univariate polynomials over a
field $\mathbb{K}$, we give fast and deterministic algorithms to compute its
determinant and its Hermite normal form. Our algorithms use
$\widetilde{\mathcal{O}}(n^\omega \lceil s \rceil)$ operations in $\mathbb{K}$,
where $s$ is bounded from above by both the average of the degrees of the rows
and that of the columns of the matrix and $\omega$ is the exponent of matrix
multiplication. The soft-$O$ notation indicates that logarithmic factors in the
big-$O$ are omitted while the ceiling function indicates that the cost is
$\widetilde{\mathcal{O}}(n^\omega)$ when $s = o(1)$. Our algorithms are based
on a fast and deterministic triangularization method for computing the diagonal
entries of the Hermite form of a nonsingular matrix."
"Polynomial multiplication is a key algorithm underlying computer algebra
systems (CAS) and its efficient implementation is crucial for the performance
of CAS. In this paper we design and implement algorithms for polynomial
multiplication using approaches based the fast Fourier transform (FFT) and the
truncated Fourier transform (TFT). We improve on the state-of-the-art in both
theoretical and practical performance. The {\SPIRAL} library generation system
is extended and used to automatically generate and tune the performance of a
polynomial multiplication library that is optimized for memory hierarchy,
vectorization and multi-threading, using new and existing algorithms. The
performance tuning has been aided by the use of automation where many code
choices are generated and intelligent search is utilized to find the ""best""
implementation on a given architecture. The performance of autotuned
implementations is comparable to, and in some cases better than, the best
hand-tuned code."
"Creative telescoping is the method of choice for obtaining information about
definite sums or integrals. It has been intensively studied since the early
1990s, and can now be considered as a classical technique in computer algebra.
At the same time, it is still subject of ongoing research. In this paper, we
present a selection of open problems in this context. We would be curious to
hear about any substantial progress on any of these problems."
"Let P and Q be two polynomials in K[x, y] with degree at most d, where K is a
field. Denoting by R $\in$ K[x] the resultant of P and Q with respect to y, we
present an algorithm to compute R mod x^k in O~(kd) arithmetic operations in K,
where the O~ notation indicates that we omit polylogarithmic factors. This is
an improvement over state-of-the-art algorithms that require to compute R in
O~(d^3) operations before computing its first k coefficients."
"We describe an algorithm to factor sparse multivariate polynomials using O(d)
bivariate factorizations where d is the number of variables. This algorithm is
implemented in the Giac/Xcas computer algebra system."
"We show that a linear-time algorithm for computing Hong's bound for positive
roots of a univariate polynomial, described by K. Mehlhorn and S. Ray in an
article ""Faster algorithms for computing Hong's bound on absolute
positiveness"", is incorrect. We present a corrected version."
"Continuing a series of articles in the past few years on creative telescoping
using reductions, we adapt Trager's Hermite reduction for algebraic functions
to fuchsian D-finite functions and develop a reduction-based creative
telescoping algorithm for this class of functions, thereby generalizing our
recent reduction-based algorithm for algebraic functions, presented at ISSAC
2016."
"Current techniques for formally verifying circuits implemented in Galois
field (GF) arithmetic are limited to those with a known irreducible polynomial
P(x). This paper presents a computer algebra based technique that extracts the
irreducible polynomial P(x) used in the implementation of a multiplier in
GF(2^m). The method is based on first extracting a unique polynomial in Galois
field of each output bit independently. P(x) is then obtained by analyzing the
algebraic expression in GF(2^m) of each output bit. We demonstrate that this
method is able to reverse engineer the irreducible polynomial of an n-bit GF
multiplier in n threads. Experiments were performed on Mastrovito and
Montgomery multipliers with different P (x), including NIST-recommended
polynomials and optimal polynomials for different microprocessor architectures."
"This work is a comprehensive extension of Abu-Salem et al. (2015) that
investigates the prowess of the Funnel Heap for implementing sums of products
in the polytope method for factoring polynomials, when the polynomials are in
sparse distributed representation. We exploit that the work and cache
complexity of an Insert operation using Funnel Heap can be refined to de- pend
on the rank of the inserted monomial product, where rank corresponds to its
lifetime in Funnel Heap. By optimising on the pattern by which insertions and
extractions occur during the Hensel lifting phase of the polytope method, we
are able to obtain an adaptive Funnel Heap that minimises all of the work,
cache, and space complexity of this phase. Additionally, we conduct a detailed
empirical study confirming the superiority of Funnel Heap over the generic
Binary Heap once swaps to external memory begin to take place. We demonstrate
that Funnel Heap is a more efficient merger than the cache oblivious k-merger,
which fails to achieve its optimal (and amortised) cache complexity when used
for performing sums of products. This provides an empirical proof of concept
that the overlapping approach for perform- ing sums of products using one
global Funnel Heap is more suited than the serialised approach, even when the
latter uses the best merging structures available."
"Mahler equations relate evaluations of the same function $f$ at iterated
$b$th powers of the variable. They arise in particular in the study of
automatic sequences and in the complexity analysis of divide-and-conquer
algorithms. Recently, the problem of solving Mahler equations in closed form
has occurred in connection with number-theoretic questions. A difficulty in the
manipulation of Mahler equations is the exponential blow-up of degrees when
applying a Mahler operator to a polynomial. In this work, we present algorithms
for solving linear Mahler equations for series, polynomials, and rational
functions, and get polynomial-time complexity under a mild assumption.
Incidentally, we develop an algorithm for computing the gcrd of a family of
linear Mahler operators with nonzero constant terms."
"The complexity of matrix multiplication (hereafter MM) has been intensively
studied since 1969, when Strassen surprisingly decreased the exponent 3 in the
cubic cost of the straightforward classical MM to log 2 (7) $\approx$ 2.8074.
Applications to some fundamental problems of Linear Algebra and Computer
Science have been immediately recognized, but the researchers in Computer
Algebra keep discovering more and more applications even today, with no sign of
slowdown. We survey the unfinished history of decreasing the exponent towards
its information lower bound 2, recall some important techniques discovered in
this process and linked to other fields of computing, reveal sample surprising
applications to fast computation of the inner products of two vectors and
summation of integers, and discuss the curse of recursion, which separates the
progress in fast MM into its most acclaimed and purely theoretical part and
into valuable acceleration of MM of feasible sizes. Then, in the second part of
our paper, we cover fast MM in realistic symbolic computations and discuss
applications and implementation of fast exact matrix multiplication. We first
review how most of exact linear algebra can be reduced to matrix multiplication
over small finite fields. Then we highlight the differences in the design of
approximate and exact implementations of fast MM, taking into account nowadays
processor and memory hierarchies. In the concluding section we comment on
current perspectives of the study of fast MM."
"We examine several matrix layouts based on space-filling curves that allow
for a cache-oblivious adaptation of parallel TU decomposition for rectangular
matrices over finite fields. The TU algorithm of \cite{Dumas} requires index
conversion routines for which the cost to encode and decode the chosen curve is
significant. Using a detailed analysis of the number of bit operations required
for the encoding and decoding procedures, and filtering the cost of lookup
tables that represent the recursive decomposition of the Hilbert curve, we show
that the Morton-hybrid order incurs the least cost for index conversion
routines that are required throughout the matrix decomposition as compared to
the Hilbert, Peano, or Morton orders. The motivation lies in that cache
efficient parallel adaptations for which the natural sequential evaluation
order demonstrates lower cache miss rate result in overall faster performance
on parallel machines with private or shared caches, on GPU's, or even cloud
computing platforms. We report on preliminary experiments that demonstrate how
the TURBO algorithm in Morton-hybrid layout attains orders of magnitude
improvement in performance as the input matrices increase in size. For example,
when $N = 2^{13}$, the row major TURBO algorithm concludes within about 38.6
hours, whilst the Morton-hybrid algorithm with truncation size equal to $64$
concludes within 10.6 hours."
"The class of quasiseparable matrices is defined by the property that any
submatrix entirely below or above the main diagonal has small rank, namely
below a bound called the order of quasiseparability. These matrices arise
naturally in solving PDE's for particle interaction with the Fast Multi-pole
Method (FMM), or computing generalized eigenvalues. From these application
fields, structured representations and algorithms have been designed in
numerical linear algebra to compute with these matrices in time linear in the
matrix dimension and either quadratic or cubic in the quasiseparability order.
Motivated by the design of the general purpose exact linear algebra library
LinBox, and by algorithmic applications in algebraic computing, we adapt
existing techniques introduce novel ones to use quasiseparable matrices in
exact linear algebra, where sub-cubic matrix arithmetic is available. In
particular, we will show, the connection between the notion of
quasiseparability and the rank profile matrix invariant, that we have
introduced in 2015. It results in two new structured representations, one being
a simpler variation on the hierarchically semiseparable storage, and the second
one exploiting the generalized Bruhat decomposition. As a consequence, most
basic operations, such as computing the quasiseparability orders, applying a
vector, a block vector, multiplying two quasiseparable matrices together,
inverting a quasiseparable matrix, can be at least as fast and often faster
than previous existing algorithms."
"Let $f\in K(t)$ be a univariate rational function. It is well known that any
non-trivial decomposition $g \circ h$, with $g,h\in K(t)$, corresponds to a
non-trivial subfield $K(f(t))\subsetneq L \subsetneq K(t)$ and vice-versa. In
this paper we use the idea of principal subfields and fast
subfield-intersection techniques to compute the subfield lattice of
$K(t)/K(f(t))$. This yields a Las Vegas type algorithm with improved complexity
and better run times for finding all non-equivalent complete decompositions of
$f$."
"In this paper, we give decision criteria for normal binomial difference
polynomial ideals in the univariate difference polynomial ring F{y} to have
finite difference Groebner bases and an algorithm to compute the finite
difference Groebner bases if these criteria are satisfied. The novelty of these
criteria lies in the fact that complicated properties about difference
polynomial ideals are reduced to elementary properties of univariate
polynomials in Z[x]."
"It is well known that the composition of a D-finite function with an
algebraic function is again D-finite. We give the first estimates for the
orders and the degrees of annihilating operators for the compositions. We find
that the analysis of removable singularities leads to an order-degree curve
which is much more accurate than the order-degree curve obtained from the usual
linear algebra reasoning."
"Tensor expression simplification is an ""ancient"" topic in computer algebra, a
representative of which is the canonicalization of Riemann tensor polynomials.
Practically fast algorithms exist for monoterm canonicalization, but not for
multiterm canonicalization. Targeting the multiterm difficulty, in this paper
we establish the extension theory of graph algebra, and propose a
canonicalization algorithm for Riemann tensor polynomials based on this theory."
"In this paper, we give novel certificates for triangular equivalence and rank
profiles. These certificates enable to verify the row or column rank profiles
or the whole rank profile matrix faster than recomputing them, with a
negligible overall overhead. We first provide quadratic time and space
non-interactive certificates saving the logarithmic factors of previously known
ones. Then we propose interactive certificates for the same problems whose
Monte Carlo verification complexity requires a small constant number of
matrix-vector multiplications, a linear space, and a linear number of extra
field operations. As an application we also give an interactive protocol ,
certifying the determinant of dense matrices, faster than the best previously
known one."
"We provide an algorithm for computing semi-Fourier sequences for expressions
constructed from arithmetic operations, exponentiations and integrations. The
semi-Fourier sequence is a relaxed version of Fourier sequence for polynomials
(expressions made of additions and multiplications)."
"The deterministic recursive pivot-free algorithms for the computation of
generalized Bruhat decomposition of the matrix in the field and for the
computation of the inverse matrix are presented. This method has the same
complexity as algorithm of matrix multiplication and it is suitable for the
parallel computer systems."
"Deterministic recursive algorithms for the computation of matrix triangular
decompositions with permutations like LU and Bruhat decomposition are presented
for the case of commutative domains. This decomposition can be considered as a
generalization of LU and Bruhat decompositions, because they both may be easily
obtained from this triangular decomposition. Algorithms have the same
complexity as the algorithm of matrix multiplication."
"Deterministic recursive algorithms for the computation of generalized Bruhat
decomposition of the matrix in commutative domain are presented. This method
has the same complexity as the algorithm of matrix multiplication."
"A characteristic pair is a pair (G,C) of polynomial sets in which G is a
reduced lexicographic Groebner basis, C is the minimal triangular set contained
in G, and C is normal. In this paper, we show that any finite polynomial set P
can be decomposed algorithmically into finitely many characteristic pairs with
associated zero relations, which provide representations for the zero set of P
in terms of those of Groebner bases and those of triangular sets. The algorithm
we propose for the decomposition makes use of the inherent connection between
Ritt characteristic sets and lexicographic Groebner bases and is based
essentially on the structural properties and the computation of lexicographic
Groebner bases. Several nice properties about the decomposition and the
resulting characteristic pairs, in particular relationships between the
Groebner basis and the triangular set in each pair, are established. Examples
are given to illustrate the algorithm and some of the properties."
"We propose and implement an algorithm for solving an overdetermined system of
partial differential equations in one unknown. Our approach relies on
Bour-Mayer method to determine compatibility conditions via Jacobi-Mayer
brackets. We solve compatible systems recursively by imitating what one would
do with pen and paper: Solve one equation, substitute the solution into the
remaining equations and iterate the process until the equations of the system
are exhausted. The method we employ for assessing the consistency of the
underlying system differs from the traditional use of differential Gr\""obner
bases yet seems more efficient and straightforward to implement. We are not
aware of a computer algebra system that adopts the procedure we advocate in
this work."
"We write a procedure for constructing noncommutative Groebner bases.
Reductions are done by particular linear projectors, called reduction
operators. The operators enable us to use a lattice construction to reduce
simultaneously each S-polynomial into a unique normal form. We write an
implementation as well as an example to illustrate our procedure. Moreover, the
lattice construction is done by Gaussian elimination, which relates our
procedure to the F4 algorithm for constructing commutative Groebner bases."
"We show a method for constructing a polynomial interpolating roots'
multiplicities of another polynomial, that does not use companion matrices.
This leads to a modification to Guersenzvaig--Szechtman square-free
decomposition algorithm that is more efficient both in theory and in practice."
"In 1969, V. Strassen improves the classical~2x2 matrix multiplication
algorithm. The current upper bound for 3x3 matrix multiplication was reached by
J.B. Laderman in 1976. This note presents a geometric relationship between
Strassen and Laderman algorithms. By doing so, we retrieve a geometric
formulation of results very similar to those presented by O. Sykora in 1977."
"In this paper, we give new sparse interpolation algorithms for black box
polynomial f whose coefficients are from a finite set. In the univariate case,
we recover f from one evaluation of f(a) for a sufficiently large number a. In
the multivariate case, we introduce the modified Kronecker substitution to
reduce the interpolation of a multivariate polynomial to the univariate case.
Both algorithms have polynomial bit-size complexity."
"We propose an efficient algorithm to compute the real roots of a sparse
polynomial $f\in\mathbb{R}[x]$ having $k$ non-zero real-valued coefficients. It
is assumed that arbitrarily good approximations of the non-zero coefficients
are given by means of a coefficient oracle. For a given positive integer $L$,
our algorithm returns disjoint disks
$\Delta_{1},\ldots,\Delta_{s}\subset\mathbb{C}$, with $s<2k$, centered at the
real axis and of radius less than $2^{-L}$ together with positive integers
$\mu_{1},\ldots,\mu_{s}$ such that each disk $\Delta_{i}$ contains exactly
$\mu_{i}$ roots of $f$ counted with multiplicity. In addition, it is ensured
that each real root of $f$ is contained in one of the disks. If $f$ has only
simple real roots, our algorithm can also be used to isolate all real roots.
  The bit complexity of our algorithm is polynomial in $k$ and $\log n$, and
near-linear in $L$ and $\tau$, where $2^{-\tau}$ and $2^{\tau}$ constitute
lower and upper bounds on the absolute values of the non-zero coefficients of
$f$, and $n$ is the degree of $f$. For root isolation, the bit complexity is
polynomial in $k$ and $\log n$, and near-linear in $\tau$ and
$\log\sigma^{-1}$, where $\sigma$ denotes the separation of the real roots."
"We consider the problem of determining multiple steady states for positive
real values in models of biological networks. Investigating the potential for
these in models of the mitogen-activated protein kinases (MAPK) network has
consumed considerable effort using special insights into the structure of
corresponding models. Here we apply combinations of symbolic computation
methods for mixed equality/inequality systems, specifically virtual
substitution, lazy real triangularization and cylindrical algebraic
decomposition. We determine multistationarity of an 11-dimensional MAPK network
when numeric values are known for all but potentially one parameter. More
precisely, our considered model has 11 equations in 11 variables and 19
parameters, 3 of which are of interest for symbolic treatment, and furthermore
positivity conditions on all variables and parameters."
"We consider linear systems of recurrence equations whose coefficients are
given in terms of indefinite nested sums and products covering, e.g., the
harmonic numbers, hypergeometric products, $q$-hypergeometric products or their
mixed versions. These linear systems are formulated in the setting of
$\Pi\Sigma$-extensions and our goal is to find a denominator bound (also known
as universal denominator) for the solutions; i.e., a non-zero polynomial $d$
such that the denominator of every solution of the system divides $d$. This is
the first step in computing all rational solutions of such a rather general
recurrence system. Once the denominator bound is known, the problem of solving
for rational solutions is reduced to the problem of solving for polynomial
solutions."
"We generalize the notions of singularities and ordinary points from linear
ordinary differential equations to D-finite systems. Ordinary points of a
D-finite system are characterized in terms of its formal power series
solutions. We also show that apparent singularities can be removed like in the
univariate case by adding suitable additional solutions to the system at hand.
Several algorithms are presented for removing and detecting apparent
singularities. In addition, an algorithm is given for computing formal power
series solutions of a D-finite system at apparent singularities."
"Pattern matching is a powerful tool which is part of many functional
programming languages as well as computer algebra systems such as Mathematica.
Among the existing systems, Mathematica offers the most expressive pattern
matching. Unfortunately, no open source alternative has comparable pattern
matching capabilities. Notably, these features include support for associative
and/or commutative function symbols and sequence variables. While those
features have individually been subject of previous research, their
comprehensive combination has not yet been investigated. Furthermore, in many
applications, a fixed set of patterns is matched repeatedly against different
subjects. This many-to-one matching can be sped up by exploiting similarities
between patterns. Discrimination nets are the state-of-the-art solution for
many-to-one matching. In this thesis, a generalized discrimination net which
supports the full feature set is presented. All algorithms have been
implemented as an open-source library for Python. In experiments on real world
examples, significant speedups of many-to-one over one-to-one matching have
been observed."
"In recent years, Karr's difference field theory has been extended to the
so-called $R\Pi\Sigma$-extensions in which one can represent not only
indefinite nested sums and products that can be expressed by transcendental
ring extensions, but one can also handle algebraic products of the form
$\alpha^n$ where $\alpha$ is a root of unity. In this article we supplement
this summation theory substantially by the following building block. We provide
new algorithms that represent a finite number of hypergeometric or mixed
$(q_1,...,q_e)$-multibasic hypergeometric products in such a difference ring.
This new insight provides a complete summation machinery that enables one to
formulate such products and indefinite nested sums defined over such products
in $R\Pi\Sigma$-extensions fully automatically. As a side-product, one obtains
compactified expressions where the products are algebraically independent among
each other, and one can solve the zero-recognition problem for such products."
"Template metaprogramming is a popular technique for implementing compile time
mechanisms for numerical computing. We demonstrate how expression templates can
be used for compile time symbolic differentiation of algebraic expressions in
C++ computer programs. Given a positive integer $N$ and an algebraic function
of multiple variables, the compiler generates executable code for the $N$th
partial derivatives of the function. Compile-time simplification of the
derivative expressions is achieved using recursive templates. A detailed
analysis indicates that current C++ compiler technology is already sufficient
for practical use of our results, and highlights a number of issues where
further improvements may be desirable."
"Analyzing and reasoning about safety properties of software systems becomes
an especially challenging task for programs with complex flow and, in
particular, with loops or recursion. For such programs one needs additional
information, for example in the form of loop invariants, expressing properties
to hold at intermediate program points. In this paper we study program loops
with non-trivial arithmetic, implementing addition and multiplication among
numeric program variables. We present a new approach for automatically
generating all polynomial invariants of a class of such programs. Our approach
turns programs into linear ordinary recurrence equations and computes closed
form solutions of these equations. These closed forms express the most precise
inductive property, and hence invariant. We apply Gr\""obner basis computation
to obtain a basis of the polynomial invariant ideal, yielding thus a finite
representation of all polynomial invariants. Our work significantly extends the
class of so-called P-solvable loops by handling multiplication with the loop
counter variable. We implemented our method in the Mathematica package Aligator
and showcase the practical use of our approach."
"We study the computation of canonical bases of sets of univariate relations
$(p_1,\ldots,p_m) \in \mathbb{K}[x]^{m}$ such that $p_1 f_1 + \cdots + p_m f_m
= 0$; here, the input elements $f_1,\ldots,f_m$ are from a quotient
$\mathbb{K}[x]^n/\mathcal{M}$, where $\mathcal{M}$ is a $\mathbb{K}[x]$-module
of rank $n$ given by a basis $\mathbf{M}\in\mathbb{K}[x]^{n\times n}$ in
Hermite form. We exploit the triangular shape of $\mathbf{M}$ to generalize a
divide-and-conquer approach which originates from fast minimal approximant
basis algorithms. Besides recent techniques for this approach, we rely on
high-order lifting to perform fast modular products of polynomial matrices of
the form $\mathbf{P}\mathbf{F} \bmod \mathbf{M}$.
  Our algorithm uses $O\tilde{~}(m^{\omega-1}D + n^{\omega} D/m)$ operations in
$\mathbb{K}$, where $D = \mathrm{deg}(\det(\mathbf{M}))$ is the
$\mathbb{K}$-vector space dimension of $\mathbb{K}[x]^n/\mathcal{M}$,
$O\tilde{~}(\cdot)$ indicates that logarithmic factors are omitted, and
$\omega$ is the exponent of matrix multiplication. This had previously only
been achieved for a diagonal matrix $\mathbf{M}$. Furthermore, our algorithm
can be used to compute the shifted Popov form of a nonsingular matrix within
the same cost bound, up to logarithmic factors, as the previously fastest known
algorithm, which is randomized."
"We give an algorithm for computing all roots of polynomials over a univariate
power series ring over an exact field $\mathbb{K}$. More precisely, given a
precision $d$, and a polynomial $Q$ whose coefficients are power series in $x$,
the algorithm computes a representation of all power series $f(x)$ such that
$Q(f(x)) = 0 \bmod x^d$. The algorithm works unconditionally, in particular
also with multiple roots, where Newton iteration fails. Our main motivation
comes from coding theory where instances of this problem arise and multiple
roots must be handled.
  The cost bound for our algorithm matches the worst-case input and output size
$d \deg(Q)$, up to logarithmic factors. This improves upon previous algorithms
which were quadratic in at least one of $d$ and $\deg(Q)$. Our algorithm is a
refinement of a divide \& conquer algorithm by Alekhnovich (2005), where the
cost of recursive steps is better controlled via the computation of a factor of
$Q$ which has a smaller degree while preserving the roots."
"In this paper, we give new sparse interpolation algorithms for black box
univariate and multivariate rational functions h=f/g whose coefficients are
integers with an upper bound. The main idea is as follows: choose a proper
integer beta and let h(beta) = a/b with gcd(a,b)=1. Then f and g can be
computed by solving the polynomial interpolation problems f(beta)=ka and
g(beta)=ka for some integer k. It is shown that the univariate interpolation
algorithm is almost optimal and multivariate interpolation algorithm has low
complexity in T but the data size is exponential in n."
"It is well-known that every non-negative univariate real polynomial can be
written as the sum of two polynomial squares with real coefficients. When one
allows a weighted sum of finitely many squares instead of a sum of two squares,
then one can choose all coefficients in the representation to lie in the field
generated by the coefficients of the polynomial.
  In this article, we describe, analyze and compare both from the theoretical
and practical points of view, two algorithms computing such a weighted sums of
squares decomposition for univariate polynomials with rational coefficients.
  The first algorithm, due to the third author relies on real root isolation,
quadratic approximations of positive polynomials and square-free decomposition
but its complexity was not analyzed. We provide bit complexity estimates, both
on runtime and output size of this algorithm. They are exponential in the
degree of the input univariate polynomial and linear in the maximum bitsize of
its complexity. This analysis is obtained using quantifier elimination and root
isolation bounds.
  The second algorithm, due to Chevillard, Harrison, Joldes and Lauter, relies
on complex root isolation and square-free decomposition and has been introduced
for certifying positiveness of polynomials in the context of computer
arithmetics. Again, its complexity was not analyzed. We provide bit complexity
estimates, both on runtime and output size of this algorithm, which are
polynomial in the degree of the input polynomial and linear in the maximum
bitsize of its complexity. This analysis is obtained using Vieta's formula and
root isolation bounds.
  Finally, we report on our implementations of both algorithms. While the
second algorithm is, as expected from the complexity result, more efficient on
most of examples, we exhibit families of non-negative polynomials for which the
first algorithm is better."
"We investigate models of the mitogenactivated protein kinases (MAPK) network,
with the aim of determining where in parameter space there exist multiple
positive steady states. We build on recent progress which combines various
symbolic computation methods for mixed systems of equalities and inequalities.
We demonstrate that those techniques benefit tremendously from a newly
implemented graph theoretical symbolic preprocessing method. We compare
computation times and quality of results of numerical continuation methods with
our symbolic approach before and after the application of our preprocessing."
"Abramov's algorithm enables us to decide whether a univariate rational
function can be written as a difference of another rational function, which has
been a fundamental algorithm for rational summation. In 2014, Chen and Singer
generalized Abramov's algorithm to the case of rational functions in two
($q$-)discrete variables. In this paper we solve the remaining three mixed
cases, which completes our recent project on bivariate extensions of Abramov's
algorithm for rational summation."
"Inspired by Faug\`ere and Mou's sparse FGLM algorithm, we show how using
linear recurrent multi-dimensional sequences can allow one to perform
operations such as the primary decomposition of an ideal, by computing the
annihilator of one or several such sequences."
"A paper by Bruno Salvy and the author introduced measured multiseries and
gave an algorithm to compute these for a large class of elementary functions,
modulo a zero-equivalence method for constants. This gave a theoretical
background for the implementation that Salvy was developing at that time. The
main result of the present article is an algorithm to calculate measured
multiseries for integrals of functions of the form h*sin G, where h and G
belong to a Hardy field. The process can reiterated with the resulting algebra,
and also applied to solutions of a second order differential equation of a
particular form."
"In this article we determine a generating set of rational invariants of
minimal cardinality for the action of the orthogonal group $\mathrm{O}_3$ on
the space $\mathbb{R}[x,y,z]_{2d}$ of ternary forms of even degree $2d$. The
construction relies on two key ingredients: On one hand, the Slice Lemma allows
us to reduce the problem to dermining the invariants for the action on a
subspace of the finite subgroup $\mathrm{B}_3$ of signed permutations. On the
other hand, our construction relies in a fundamental way on specific bases of
harmonic polynomials. These bases provide maps with prescribed
$\mathrm{B}_3$-equivariance properties. Our explicit construction of these
bases should be relevant well beyond the scope of this paper. The expression of
the $\mathrm{B}_3$-invariants can then be given in a compact form as the
composition of two equivariant maps. Instead of providing (cumbersome) explicit
expressions for the $\mathrm{O}_3$-invariants, we provide efficient algorithms
for their evaluation and rewriting. We also use the constructed
$\mathrm{B}_3$-invariants to determine the $\mathrm{O}_3$-orbit locus and
provide an algorithm for the inverse problem of finding an element in
$\mathbb{R}[x,y,z]_{2d}$ with prescribed values for its invariants. These are
the computational issues relevant in brain imaging."
"The advantages of mixed approach with using different kinds of programming
techniques for symbolic manipulation are discussed. The main purpose of
approach offered is merge the methods of object oriented programming that
convenient for presentation data and algorithms for user with advantages of
functional languages for data manipulation, internal presentation, and
portability of software."
"We perform a comparison of the performance and efficiency of four different
function evaluation methods: black-box functions, binary trees, $n$-ary trees
and string parsing. The test consists in evaluating 8 different functions of
two variables $x,y$ over 5000 floating point values of the pair $(x,y)$. The
outcome of the test indicates that the $n$-ary tree representation of algebraic
expressions is the fastest method, closely followed by black-box function
method, then by binary trees and lastly by string parsing."
"A new integration technique is presented for systems of linear partial
differential equations (PDEs) for which syzygies can be formulated that obey
conservation laws. These syzygies come for free as a by-product of the
differential Groebner Basis computation. Compared with the more obvious way of
integrating a single equation and substituting the result in other equations
the new technique integrates more than one equation at once and therefore
introduces temporarily fewer new functions of integration that in addition
depend on fewer variables. Especially for high order PDE systems in many
variables the conventional integration technique may lead to an explosion of
the number of functions of integration which is avoided with the new method. A
further benefit is that redundant free functions in the solution are either
prevented or that their number is at least reduced."
"This article documents the free computer algebra system ""gTybalt"". The
program is build on top of other packages, among others GiNaC, TeXmacs and
Root. It offers the possibility of interactive symbolic calculations within the
C++ programming language. Mathematical formulae are visualized using TeX fonts."
"We give a polynomial time algorithm for computing the Igusa local zeta
function $Z(s,f)$ attached to a polynomial $f(x)\in \QTR{Bbb}{Z}[x]$, in one
variable, with splitting field $\QTR{Bbb}{Q}$, and a prime number $p$. We also
propose a new class of Linear Feedback Shift Registers based on the computation
of Igusa's local zeta function."
"Mace4 is a program that searches for finite models of first-order formulas.
For a given domain size, all instances of the formulas over the domain are
constructed. The result is a set of ground clauses with equality. Then, a
decision procedure based on ground equational rewriting is applied. If
satisfiability is detected, one or more models are printed. Mace4 is a useful
complement to first-order theorem provers, with the prover searching for proofs
and Mace4 looking for countermodels, and it is useful for work on finite
algebras. Mace4 performs better on equational problems than did our previous
model-searching program Mace2."
"OTTER is a resolution-style theorem-proving program for first-order logic
with equality. OTTER includes the inference rules binary resolution,
hyperresolution, UR-resolution, and binary paramodulation. Some of its other
abilities and features are conversion from first-order formulas to clauses,
forward and back subsumption, factoring, weighting, answer literals, term
ordering, forward and back demodulation, evaluable functions and predicates,
Knuth-Bendix completion, and the hints strategy. OTTER is coded in ANSI C, is
free, and is portable to many different kinds of computer."
"Let $\{w_{i,j}\}_{1\leq i\leq n, 1\leq j\leq s} \subset
L_m=F(X_1,...,X_m)[{\partial \over \partial X_1},..., {\partial \over \partial
X_m}]$ be linear partial differential operators of orders with respect to
${\partial \over \partial X_1},..., {\partial \over \partial X_m}$ at most $d$.
We prove an upper bound n(4m^2d\min\{n,s\})^{4^{m-t-1}(2(m-t))} on the leading
coefficient of the Hilbert-Kolchin polynomial of the left $L_m$-module
$<\{w_{1,j}, ..., w_{n,j}\}_{1\leq j \leq s} > \subset L_m^n$ having the
differential type $t$ (also being equal to the degree of the Hilbert-Kolchin
polynomial). The main technical tool is the complexity bound on solving systems
of linear equations over {\it algebras of fractions} of the form
$$L_m(F[X_1,..., X_m, {\partial \over \partial X_1},..., {\partial \over
\partial X_k}])^{-1}.$$"
"We present here algebraic formulas associating a k-automaton to a
k-epsilon-automaton. The existence depends on the definition of the star of
matrices and of elements in the semiring k. For this reason, we present the
theorem which allows the transformation of k-epsilon-automata into k-automata.
The two automata have the same behaviour."
"We present a partial proof of van Hoeij-Abramov conjecture about the
algorithmic possibility of computation of finite sums of rational functions.
The theoretical results proved in this paper provide an algorithm for
computation of a large class of sums $ S(n) = \sum_{k=0}^{n-1}R(k,n)$."
"The article mainly presents some results in using MAPLE platform for computer
algebra and GrTensorII package in doing calculations for theoretical and
numerical cosmology"
"E. Bach, following an idea of T. Itoh, has shown how to build a small set of
numbers modulo a prime p such that at least one element of this set is a
generator of $\pF{p}$\cite{Bach:1997:sppr,Itoh:2001:PPR}. E. Bach suggests also
that at least half of his set should be generators. We show here that a slight
variant of this set can indeed be made to contain a ratio of primitive roots as
close to 1 as necessary. We thus derive several algorithms computing primitive
roots correct with very high probability in polynomial time. In particular we
present an asymptotically $O^{\sim}(\sqrt{\frac{1}{\epsilon}}log^1.5(p) +
\log^2(p))$ algorithm providing primitive roots of $p$ with probability of
correctness greater than $1-\epsilon$ and several $O(log^\alpha(p))$, $\alpha
\leq 5.23$ algorithms computing ""Industrial-strength"" primitive roots with
probabilities e.g. greater than the probability of ""hardware malfunctions""."
"So far, the scope of computer algebra has been needlessly restricted to exact
algebraic methods. Its possible extension to approximate analytical methods is
discussed. The entangled roles of functional analysis and symbolic programming,
especially the functional and transformational paradigms, are put forward. In
the future, algebraic algorithms could constitute the core of extended symbolic
manipulation systems including primitives for symbolic approximations."
"We continue the study of counting complexity begun in [Buergisser, Cucker 04]
and [Buergisser, Cucker, Lotz 05] by proving upper and lower bounds on the
complexity of computing the Hilbert polynomial of a homogeneous ideal. We show
that the problem of computing the Hilbert polynomial of a smooth
equidimensional complex projective variety can be reduced in polynomial time to
the problem of counting the number of complex common zeros of a finite set of
multivariate polynomials. Moreover, we prove that the more general problem of
computing the Hilbert polynomial of a homogeneous ideal is polynomial space
hard. This implies polynomial space lower bounds for both the problems of
computing the rank and the Euler characteristic of cohomology groups of
coherent sheaves on projective space, improving the #P-lower bound of Bach (JSC
1999)."
"We reduce the problem of computing the rank and a nullspace basis of a
univariate polynomial matrix to polynomial matrix multiplication. For an input
n x n matrix of degree d over a field K we give a rank and nullspace algorithm
using about the same number of operations as for multiplying two matrices of
dimension n and degree d. If the latter multiplication is done in
MM(n,d)=softO(n^omega d) operations, with omega the exponent of matrix
multiplication over K, then the algorithm uses softO(MM(n,d)) operations in K.
The softO notation indicates some missing logarithmic factors. The method is
randomized with Las Vegas certification. We achieve our results in part through
a combination of matrix Hensel high-order lifting and matrix minimal fraction
reconstruction, and through the computation of minimal or small degree vectors
in the nullspace seen as a K[x]-module"
"We present the asymptotically fastest known algorithms for some basic
problems on univariate polynomial matrices: rank, nullspace, determinant,
generic inverse, reduced form. We show that they essentially can be reduced to
two computer algebra techniques, minimal basis computations and matrix fraction
expansion/reconstruction, and to polynomial matrix multiplication. Such
reductions eventually imply that all these problems can be solved in about the
same amount of time as polynomial matrix multiplication."
"A Maple package for computing Groebner bases of linear difference ideals is
described. The underlying algorithm is based on Janet and Janet-like monomial
divisions associated with finite difference operators. The package can be used,
for example, for automatic generation of difference schemes for linear partial
differential equations and for reduction of multiloop Feynman integrals. These
two possible applications are illustrated by simple examples of the Laplace
equation and a one-loop scalar integral of propagator type"
"We present two algorithms for the computation of the Kalman form of a linear
control system. The first one is based on the technique developed by
Keller-Gehrig for the computation of the characteristic polynomial. The cost is
a logarithmic number of matrix multiplications. To our knowledge, this improves
the best previously known algebraic complexity by an order of magnitude. Then
we also present a cubic algorithm proven to more efficient in practice."
"We present algorithmic and complexity results concerning computations with
one and two real algebraic numbers, as well as real solving of univariate
polynomials and bivariate polynomial systems with integer coefficients using
Sturm-Habicht sequences.
  Our main results, in the univariate case, concern the problems of real root
isolation (Th. 19) and simultaneous inequalities (Cor.26) and in the bivariate,
the problems of system real solving (Th.42), sign evaluation (Th. 37) and
simultaneous inequalities (Cor. 43)."
"In this article, we study some new characterizations of primitive recursive
functions based on restricted forms of primitive recursion, improving the
pioneering work of R. M. Robinson and M. D. Gladstone in this area. We reduce
certain recursion schemes (mixed/pure iteration without parameters) and we
characterize one-argument primitive recursive functions as the closure under
substitution and iteration of certain optimal sets."
"We describe the implementation of facilities for the communication with
external resources in the Symbolic Manipulation System FORM. This is done
according to the POSIX standards defined for the UNIX operating system. We
present a number of examples that illustrate the increased power due to these
new capabilities."
"We present a method for determining the one-dimensional submodules of a
Laurent-Ore module. The method is based on a correspondence between
hyperexponential solutions of associated systems and one-dimensional
submodules. The hyperexponential solutions are computed recursively by solving
a sequence of first-order ordinary matrix equations. As the recursion proceeds,
the matrix equations will have constant coefficients with respect to the
operators that have been considered."
"We present an algorithm which takes as input a closed semi-algebraic set, $S
\subset \R^k$, defined by \[ P_1 \leq 0, ..., P_\ell \leq 0, P_i \in
\R[X_1,...,X_k], \deg(P_i) \leq 2, \] and computes the Euler-Poincar\'e
characteristic of $S$. The complexity of the algorithm is $k^{O(\ell)}$."
"We prove a general finite convergence theorem for ""upward-guarded"" fixpoint
expressions over a well-quasi-ordered set. This has immediate applications in
regular model checking of well-structured systems, where a main issue is the
eventual convergence of fixpoint computations. In particular, we are able to
directly obtain several new decidability results on lossy channel systems."
"This paper presents two new approaches to prove termination of rewrite
systems with the Knuth-Bendix order efficiently. The constraints for the weight
function and for the precedence are encoded in (pseudo-)propositional logic and
the resulting formula is tested for satisfiability. Any satisfying assignment
represents a weight function and a precedence such that the induced
Knuth-Bendix order orients the rules of the encoded rewrite system from left to
right."
"The notion of differentiation index for DAE systems of arbitrary order with
generic second members is discussed by means of the study of the behavior of
the ranks of certain Jacobian associated sub-matrices. As a by-product, we
obtain upper bounds for the regularity of the Hilbert-Kolchin function and the
order of the ideal associated to the DAE systems under consideration, not
depending on characteristic sets. Some quantitative and algorithmic results
concerning differential transcendence bases and induced equivalent explicit ODE
systems are also established."
"We describe a method of obtaining closed-form complete solutions of certain
second-order linear partial differential equations with more than two
independent variables. This method generalizes the classical method of Laplace
transformations of second-order hyperbolic equations in the plane and is based
on an idea given by Ulisse Dini in 1902."
"Given a lattice basis of n vectors in Z^n, we propose an algorithm using
12n^3+O(n^2) floating point operations for checking whether the basis is
LLL-reduced. If the basis is reduced then the algorithm will hopefully answer
''yes''. If the basis is not reduced, or if the precision used is not
sufficient with respect to n, and to the numerical properties of the basis, the
algorithm will answer ''failed''. Hence a positive answer is a rigorous
certificate. For implementing the certificate itself, we propose a floating
point algorithm for computing (certified) error bounds for the entries of the R
factor of the QR matrix factorization. This algorithm takes into account all
possible approximation and rounding errors. The cost 12n^3+O(n^2) of the
certificate is only six times more than the cost of numerical algorithms for
computing the QR factorization itself, and the certificate may be implemented
using matrix library routines only. We report experiments that show that for a
reduced basis of adequate dimension and quality the certificate succeeds, and
establish the effectiveness of the certificate. This effectiveness is applied
for certifying the output of fastest existing floating point heuristics of LLL
reduction, without slowing down the whole process."
"Block projections have been used, in [Eberly et al. 2006], to obtain an
efficient algorithm to find solutions for sparse systems of linear equations. A
bound of softO(n^(2.5)) machine operations is obtained assuming that the input
matrix can be multiplied by a vector with constant-sized entries in softO(n)
machine operations. Unfortunately, the correctness of this algorithm depends on
the existence of efficient block projections, and this has been conjectured. In
this paper we establish the correctness of the algorithm from [Eberly et al.
2006] by proving the existence of efficient block projections over sufficiently
large fields. We demonstrate the usefulness of these projections by deriving
improved bounds for the cost of several matrix problems, considering, in
particular, ``sparse'' matrices that can be be multiplied by a vector using
softO(n) field operations. We show how to compute the inverse of a sparse
matrix over a field F using an expected number of softO(n^(2.27)) operations in
F. A basis for the null space of a sparse matrix, and a certification of its
rank, are obtained at the same cost. An application to Kaltofen and Villard's
Baby-Steps/Giant-Steps algorithms for the determinant and Smith Form of an
integer matrix yields algorithms requiring softO(n^(2.66)) machine operations.
The derived algorithms are all probabilistic of the Las Vegas type."
"We define a canonical form for piecewise defined functions. We show that this
has a wider range of application as well as better complexity properties than
previous work."
"It is classical that univariate algebraic functions satisfy linear
differential equations with polynomial coefficients. Linear recurrences follow
for the coefficients of their power series expansions. We show that the linear
differential equation of minimal order has coefficients whose degree is cubic
in the degree of the function. We also show that there exists a linear
differential equation of order linear in the degree whose coefficients are only
of quadratic degree. Furthermore, we prove the existence of recurrences of
order and degree close to optimal. We study the complexity of computing these
differential equations and recurrences. We deduce a fast algorithm for the
expansion of algebraic series."
"We provide a proof of a conjecture in (Bailey, Borwein, Borwein, Crandall
2007) on the existence and form of linear recursions for moments of powers of
the Bessel function $K_0$."
"Given a group action, known by its infinitesimal generators, we exhibit a
complete set of syzygies on a generating set of differential invariants. For
that we elaborate on the reinterpretation of Cartan's moving frame by Fels and
Olver (1999). This provides constructive tools for exploring algebras of
differential invariants."
"We give an $O(N\cdot \log N\cdot 2^{O(\log^*N)})$ algorithm for multiplying
two $N$-bit integers that improves the $O(N\cdot \log N\cdot \log\log N)$
algorithm by Sch\""{o}nhage-Strassen. Both these algorithms use modular
arithmetic. Recently, F\""{u}rer gave an $O(N\cdot \log N\cdot 2^{O(\log^*N)})$
algorithm which however uses arithmetic over complex numbers as opposed to
modular arithmetic. In this paper, we use multivariate polynomial
multiplication along with ideas from F\""{u}rer's algorithm to achieve this
improvement in the modular setting. Our algorithm can also be viewed as a
$p$-adic version of F\""{u}rer's algorithm. Thus, we show that the two seemingly
different approaches to integer multiplication, modular and complex arithmetic,
are similar."
"In this paper we present an efficient computational and symbolic algorithms
for solving a backward pentadiagonal linear systems. The implementation of the
algorithms using Computer Algebra Systems (CAS) such as MAPLE, MACSYMA,
MATHEMATICA, and MATLAB are straightforward. An examples are given in order to
illustrate the algorithms. The symbolic algorithm is competitive the other
methods for solving a backward pentadiagonal linear systems."
"How many moves does it take to solve Rubik's Cube? Positions are known that
require 20 moves, and it has already been shown that there are no positions
that require 27 or more moves; this is a surprisingly large gap. This paper
describes a program that is able to find solutions of length 20 or less at a
rate of more than 16 million positions a second. We use this program, along
with some new ideas and incremental improvements in other techniques, to show
that there is no position that requires 26 moves."
"A classical theorem by Ritt states that all the complete decomposition chains
of a univariate polynomial satisfying a certain tameness condition have the
same length. In this paper we present our conclusions about the generalization
of these theorem in the case of finite coefficient fields when the tameness
condition is dropped."
"We present algorithms to perform modular polynomial multiplication or modular
dot product efficiently in a single machine word. We pack polynomials into
integers and perform several modular operations with machine integer or
floating point arithmetic. The modular polynomials are converted into integers
using Kronecker substitution (evaluation at a sufficiently large integer). With
some control on the sizes and degrees, arithmetic operations on the polynomials
can be performed directly with machine integers or floating point numbers and
the number of conversions can be reduced. We also present efficient ways to
recover the modular values of the coefficients. This leads to practical gains
of quite large constant factors for polynomial multiplication, prime field
linear algebra and small extension field arithmetic."
"In some fields such as Mathematics Mechanization, automated reasoning and
Trustworthy Computing etc., exact results are needed. Symbolic computations are
used to obtain the exact results. Symbolic computations are of high complexity.
In order to improve the situation, exactly interpolating methods are often
proposed for the exact results and approximate interpolating methods for the
approximate ones. In this paper, we study how to obtain exact interpolation
polynomial with rational coefficients by approximate interpolating methods."
"Since many years, theoretical concepts of Data Mining have been developed and
improved. Data Mining has become applied to many academic and industrial
situations, and recently, soundings of public opinion about privacy have been
carried out. However, a consistent and standardized definition is still
missing, and the initial explanation given by Frawley et al. has pragmatically
often changed over the years. Furthermore, alternative terms like Knowledge
Discovery have been conjured and forged, and a necessity of a Data Warehouse
has been endeavoured to persuade the users. In this work, we pick up current
definitions and introduce an unified definition that covers existing attempted
explanations. For this, we appeal to the natural original of chemical states of
aggregation."
"For a field k$with an automorphism \sigma and a derivation \delta, we
introduce the notion of liouvillian solutions of linear difference-differential
systems {\sigma(Y) = AY, \delta(Y) = BY} over k and characterize the existence
of liouvillian solutions in terms of the Galois group of the systems. We will
give an algorithm to decide whether such a system has liouvillian solutions
when k = C(x,t), \sigma(x) = x+1, \delta = d/dt$ and the size of the system is
a prime."
"We describe a cache-friendly version of van der Hoeven's truncated FFT and
inverse truncated FFT, focusing on the case of `large' coefficients, such as
those arising in the Schonhage--Strassen algorithm for multiplication in Z[x].
We describe two implementations and examine their performance."
"Kaltofen has proposed a new approach in 1992 for computing matrix
determinants without divisions. The algorithm is based on a baby steps/giant
steps construction of Krylov subspaces, and computes the determinant as the
constant term of a characteristic polynomial. For matrices over an abstract
ring, by the results of Baur and Strassen, the determinant algorithm, actually
a straight-line program, leads to an algorithm with the same complexity for
computing the adjoint of a matrix. However, the latter adjoint algorithm is
obtained by the reverse mode of automatic differentiation, hence somehow is not
""explicit"". We present an alternative (still closely related) algorithm for the
adjoint thatcan be implemented directly, we mean without resorting to an
automatic transformation. The algorithm is deduced by applying program
differentiation techniques ""by hand"" to Kaltofen's method, and is completely
decribed. As subproblem, we study the differentiation of programs that compute
minimum polynomials of lineraly generated sequences, and we use a lazy
polynomial evaluation mechanism for reducing the cost of Strassen's avoidance
of divisions in our case."
"Let $f_1,...,f_s \in \mathbb{K}[x_1,...,x_m]$ be a system of polynomials
generating a zero-dimensional ideal $\I$, where $\mathbb{K}$ is an arbitrary
algebraically closed field. We study the computation of ""matrices of traces""
for the factor algebra $\A := \CC[x_1, ..., x_m]/ \I$, i.e. matrices with
entries which are trace functions of the roots of $\I$. Such matrices of traces
in turn allow us to compute a system of multiplication matrices
$\{M_{x_i}|i=1,...,m\}$ of the radical $\sqrt{\I}$. We first propose a method
using Macaulay type resultant matrices of $f_1,...,f_s$ and a polynomial $J$ to
compute moment matrices, and in particular matrices of traces for $\A$. Here
$J$ is a polynomial generalizing the Jacobian. We prove bounds on the degrees
needed for the Macaulay matrix in the case when $\I$ has finitely many
projective roots in $\mathbb{P}^m_\CC$. We also extend previous results which
work only for the case where $\A$ is Gorenstein to the non-Gorenstein case. The
second proposed method uses Bezoutian matrices to compute matrices of traces of
$\A$. Here we need the assumption that $s=m$ and $f_1,...,f_m$ define an affine
complete intersection. This second method also works if we have higher
dimensional components at infinity. A new explicit description of the
generators of $\sqrt{\I}$ are given in terms of Bezoutians."
"We study the cost of multiplication modulo triangular families of
polynomials. Following previous work by Li, Moreno Maza and Schost, we propose
an algorithm that relies on homotopy and fast evaluation-interpolation
techniques. We obtain a quasi-linear time complexity for substantial families
of examples, for which no such result was known before. Applications are given
to notably addition of algebraic numbers in small characteristic."
"We present an algorithm for decomposing a symmetric tensor, of dimension n
and order d as a sum of rank-1 symmetric tensors, extending the algorithm of
Sylvester devised in 1886 for binary forms. We recall the correspondence
between the decomposition of a homogeneous polynomial in n variables of total
degree d as a sum of powers of linear forms (Waring's problem), incidence
properties on secant varieties of the Veronese Variety and the representation
of linear forms as a linear combination of evaluations at distinct points. Then
we reformulate Sylvester's approach from the dual point of view. Exploiting
this duality, we propose necessary and sufficient conditions for the existence
of such a decomposition of a given rank, using the properties of Hankel (and
quasi-Hankel) matrices, derived from multivariate polynomials and normal form
computations. This leads to the resolution of polynomial equations of small
degree in non-generic cases. We propose a new algorithm for symmetric tensor
decomposition, based on this characterization and on linear algebra
computations with these Hankel matrices. The impact of this contribution is
two-fold. First it permits an efficient computation of the decomposition of any
tensor of sub-generic rank, as opposed to widely used iterative algorithms with
unproved global convergence (e.g. Alternate Least Squares or gradient
descents). Second, it gives tools for understanding uniqueness conditions, and
for detecting the rank."
"Compact closed categories provide a foundational formalism for a variety of
important domains, including quantum computation. These categories have a
natural visualisation as a form of graphs. We present a formalism for
equational reasoning about such graphs and develop this into a generic proof
system with a fixed logical kernel for equational reasoning about compact
closed categories. Automating this reasoning process is motivated by the slow
and error prone nature of manual graph manipulation. A salient feature of our
system is that it provides a formal and declarative account of derived results
that can include `ellipses'-style notation. We illustrate the framework by
instantiating it for a graphical language of quantum computation and show how
this can be used to perform symbolic computation."
"The Riordan group is the semi-direct product of a multiplicative group of
invertible series and a group, under substitution, of non units. The Riordan
near algebra, as introduced in this paper, is the Cartesian product of the
algebra of formal power series and its principal ideal of non units, equipped
with a product that extends the multiplication of the Riordan group. The later
is naturally embedded as a subgroup of units into the former. In this paper, we
prove the existence of a formal calculus on the Riordan algebra. This formal
calculus plays a role similar to those of holomorphic calculi in the Banach or
Fr\'echet algebras setting, but without the constraint of a radius of
convergence. Using this calculus, we define \emph{en passant} a notion of
generalized powers in the Riordan group."
"In this paper we present algorithms for computing the topology of planar and
space rational curves defined by a parametrization. The algorithms given here
work directly with the parametrization of the curve, and do not require to
compute or use the implicit equation of the curve (in the case of planar
curves) or of any projection (in the case of space curves). Moreover, these
algorithms have been implemented in Maple; the examples considered and the
timings obtained show good performance skills."
"Given a family of rational curves depending on a real parameter, defined by
its parametric equations, we provide an algorithm to compute a finite partition
of the parameter space (${\Bbb R}$, in general) so that the shape of the family
stays invariant along each element of the partition. So, from this partition
the topology types in the family can be determined. The algorithm is based on a
geometric interpretation of previous work (\cite{JGRS}) for the implicit case.
However, in our case the algorithm works directly with the parametrization of
the family, and the implicit equation does not need to be computed. Timings
comparing the algorithm in the implicit and the parametric cases are given;
these timings show that the parametric algorithm developed here provides in
general better results than the known algorithm for the implicit case."
"In this paper we study the local behavior of an algebraic curve under a
geometric construction which is a variation of the usual offsetting
construction, namely the {\it generalized} offsetting process (\cite {SS99}).
More precisely, here we discuss when and how this geometric construction may
cause local changes in the shape of an algebraic curve, and we compare our
results with those obtained for the case of classical offsets (\cite{JGS07}).
For these purposes, we use well-known notions of Differential Geometry, and
also the notion of {\it local shape} introduced in \cite{JGS07}."
"We prove explicit bounds on the radius of a ball centered at the origin which
is guaranteed to contain all bounded connected components of a semi-algebraic
set $S \subset \mathbbm{R}^k$ defined by a quantifier-free formula involving
$s$ polynomials in $\mathbbm{Z}[X_1, ..., X_k]$ having degrees at most $d$, and
whose coefficients have bitsizes at most $\tau$. Our bound is an explicit
function of $s, d, k$ and $\tau$, and does not contain any undetermined
constants. We also prove a similar bound on the radius of a ball guaranteed to
intersect every connected component of $S$ (including the unbounded
components). While asymptotic bounds of the form $2^{\tau d^{O (k)}}$ on these
quantities were known before, some applications require bounds which are
explicit and which hold for all values of $s, d, k$ and $\tau$. The bounds
proved in this paper are of this nature."
"We describe a new algorithm for computing exp(f) where f is a power series in
C[[x]]. If M(n) denotes the cost of multiplying polynomials of degree n, the
new algorithm costs (2.1666... + o(1)) M(n) to compute exp(f) to order n. This
improves on the previous best result, namely (2.333... + o(1)) M(n)."
"In this paper, we give new explicit representations of the Hilbert scheme of
$\mu$ points in $\PP^{r}$ as a projective subvariety of a Grassmanniann
variety. This new explicit description of the Hilbert scheme is simpler than
the previous ones and global. It involves equations of degree $2$. We show how
these equations are deduced from the commutation relations characterizing
border bases. Next, we consider infinitesimal perturbations of an input system
of equations on this Hilbert scheme and describe its tangent space. We propose
an effective criterion to test if it is a flat deformation, that is if the
perturbed system remains on the Hilbert scheme of the initial equations. This
criterion involves in particular formal reduction with respect to border bases."
"In this paper, we study the weighted difference substitutions from
geometrical views. First, we give the geometric meanings of the weighted
difference substitutions, and introduce the concept of convergence of the
sequence of substitution sets. Then it is proven that the sequence of the
successive weighted difference substitution sets is convergent. Based on the
convergence of the sequence of the successive weighted difference sets, a new,
simpler method to prove that if the form F is positive definite on T_n, then
the sequence of sets {SDS^m(F)} is positively terminating is presented, which
is different from the one given in [11]. That is, we can decide the
nonnegativity of a positive definite form by successively running the weighted
difference substitutions finite times. Finally, an algorithm for deciding an
indefinite form with a counter-example is obtained, and some examples are
listed by using the obtained algorithm."
"We present a lattice algorithm specifically designed for some classical
applications of lattice reduction. The applications are for lattice bases with
a generalized knapsack-type structure, where the target vectors are boundably
short. For such applications, the complexity of the algorithm improves
traditional lattice reduction by replacing some dependence on the bit-length of
the input vectors by some dependence on the bound for the output vectors. If
the bit-length of the target vectors is unrelated to the bit-length of the
input, then our algorithm is only linear in the bit-length of the input
entries, which is an improvement over the quadratic complexity floating-point
LLL algorithms. To illustrate the usefulness of this algorithm we show that a
direct application to factoring univariate polynomials over the integers leads
to the first complexity bound improvement since 1984. A second application is
algebraic number reconstruction, where a new complexity bound is obtained as
well."
"An Artin-Schreier tower over the finite field F_p is a tower of field
extensions generated by polynomials of the form X^p - X - a. Following Cantor
and Couveignes, we give algorithms with quasi-linear time complexity for
arithmetic operations in such towers. As an application, we present an
implementation of Couveignes' algorithm for computing isogenies between
elliptic curves using the p-torsion."
"In this paper, we mainly study the robust stability of linear continuous
systems with parameter uncertainties, a more general kind of uncertainties for
system matrices is considered, i.e., entries of system matrices are rational
functions of uncertain parameters which are varying in intervals. we present a
method which can check the robust Hurwitz stability of such uncertain systems
in finite steps. Examples show the efficiency of our approach."
"We propose a generic design for Chinese remainder algorithms. A Chinese
remainder computation consists in reconstructing an integer value from its
residues modulo non coprime integers. We also propose an efficient linear data
structure, a radix ladder, for the intermediate storage and computations. Our
design is structured into three main modules: a black box residue computation
in charge of computing each residue; a Chinese remaindering controller in
charge of launching the computation and of the termination decision; an integer
builder in charge of the reconstruction computation. We then show that this
design enables many different forms of Chinese remaindering (e.g.
deterministic, early terminated, distributed, etc.), easy comparisons between
these forms and e.g. user-transparent parallelism at different parallel grains."
"We present an efficient solution to the following problem, of relevance in a
numerical optimization scheme: calculation of integrals of the type \[\iint_{T
\cap \{f\ge0\}} \phi_1\phi_2 \, dx\,dy\] for quadratic polynomials
$f,\phi_1,\phi_2$ on a plane triangle $T$. The naive approach would involve
consideration of the many possible shapes of $T\cap\{f\geq0\}$ (possibly after
a convenient transformation) and parameterizing its border, in order to
integrate the variables separately. Our solution involves partitioning the
triangle into smaller triangles on which integration is much simpler."
"In order to appreciate how well off we mathematicians and scientists are
today, with extremely fast hardware and lots and lots of memory, as well as
with powerful software, both for numeric and symbolic computation, it may be a
good idea to go back to the early days of electronic computers and compare how
things went then. We have chosen, as a case study, a problem that was
considered a huge challenge at the time. Namely, we looked at C.L. Pekeris's
seminal 1958 work on the ground state energies of two-electron atoms. We went
through all the computations ab initio with today's software and hardware, with
a special emphasis on the symbolic computations which in 1958 had to be made by
hand, and which nowadays can be automated and generalized."
"We provide a ""shared axiomatization"" of natural numbers and hereditarily
finite sets built around a polymorphic abstraction of bijective base-2
arithmetics.
  The ""axiomatization"" is described as a progressive refinement of Haskell type
classes with examples of instances converging to an efficient implementation in
terms of arbitrary length integers and bit operations. As an instance, we
derive algorithms to perform arithmetic operations efficiently directly with
hereditarily finite sets.
  The self-contained source code of the paper is available at
http://logic.cse.unt.edu/tarau/research/2010/unified.hs ."
"The context of this work is the design of a software, called MEMSALab,
dedicated to the automatic derivation of multiscale models of arrays of micro-
and nanosystems. In this domain a model is a partial differential equation.
Multiscale methods approximate it by another partial differential equation
which can be numerically simulated in a reasonable time. The challenge consists
in taking into account a wide range of geometries combining thin and periodic
structures with the possibility of multiple nested scales.
  In this paper we present a transformation language that will make the
development of MEMSALab more feasible. It is proposed as a Maple package for
rule-based programming, rewriting strategies and their combination with
standard Maple code. We illustrate the practical interest of this language by
using it to encode two examples of multiscale derivations, namely the two-scale
limit of the derivative operator and the two-scale model of the stationary heat
equation."
"We propose new algorithms for computing triangular decompositions of
polynomial systems incrementally. With respect to previous works, our
improvements are based on a {\em weakened} notion of a polynomial GCD modulo a
regular chain, which permits to greatly simplify and optimize the
sub-algorithms. Extracting common work from similar expensive computations is
also a key feature of our algorithms. In our experimental results the
implementation of our new algorithms, realized with the {\RegularChains}
library in {\Maple}, outperforms solvers with similar specifications by several
orders of magnitude on sufficiently difficult problems."
"We give an algebraic quantifier elimination algorithm for the first-order
theory over any given finite field using Gr\""obner basis methods. The algorithm
relies on the strong Nullstellensatz and properties of elimination ideals over
finite fields. We analyze the theoretical complexity of the algorithm and show
its application in the formal analysis of a biological controller model."
"We consider algebras over a field K, generated by two variables x and y
subject to the single relation yx = qxy + ax + by + c for q in K^* and a, b, c
in K. We prove, that among such algebras there are precisely five isomorphism
classes. The representatives of these classes, which are ubiquitous operator
algebras, are called model algebras. We derive explicit multiplication formulas
for y^m*x^n in terms of standard monomials x^i*y^j for many algebras of the
considered type. Such formulas are used in establishing formulas of binomial
type and in implementing non-commutative multiplication in a computer algebra
system. By using the formulas we also study centers and ring-theoretic
properties of the non-commutative model algebras."
"Ritt-Wu's algorithm of characteristic sets is the most representative for
triangularizing sets of multivariate polynomials. Pseudo-division is the main
operation used in this algorithm. In this paper we present a new algorithmic
scheme for computing generalized characteristic sets by introducing other
admissible reductions than pseudo-division. A concrete subalgorithm is designed
to triangularize polynomial sets using selected admissible reductions and
several effective elimination strategies and to replace the algorithm of basic
sets (used in Ritt-Wu's algorithm). The proposed algorithm has been implemented
and experimental results show that it performs better than Ritt-Wu's algorithm
in terms of computing time and simplicity of output for a number of non-trivial
test examples."
"The purpose of this paper is twofold. An immediate practical use of the
presented algorithm is its applicability to the parametric solution of
underdetermined linear ordinary differential equations (ODEs) with coefficients
that are arbitrary analytic functions in the independent variable. A second
conceptual aim is to present an algorithm that is in some sense dual to the
fundamental Euclids algorithm, and thus an alternative to the special case of a
Groebner basis algorithm as it is used for solving linear ODE-systems. In the
paper Euclids algorithm and the new `dual version' are compared and their
complementary strengths are analysed on the task of solving underdetermined
ODEs. An implementation of the described algorithm is interactively accessible
under http://lie.math.brocku.ca/crack/demo."
"We present an algorithm for computing a holonomic system for a definite
integral of a holonomic function over a domain defined by polynomial
inequalities. If the integrand satisfies a holonomic difference-differential
system including parameters, then a holonomic difference-differential system
for the integral can also be computed. In the algorithm, holonomic
distributions (generalized functions in the sense of L. Schwartz) are
inevitably involved even if the integrand is a usual function."
"We design an algorithm to compute the Newton polytope of the resultant, known
as resultant polytope, or its orthogonal projection along a given direction.
The resultant is fundamental in algebraic elimination, optimization, and
geometric modeling. Our algorithm exactly computes vertex- and
halfspace-representations of the polytope using an oracle producing resultant
vertices in a given direction, thus avoiding walking on the polytope whose
dimension is alpha-n-1, where the input consists of alpha points in Z^n. Our
approach is output-sensitive as it makes one oracle call per vertex and facet.
It extends to any polytope whose oracle-based definition is advantageous, such
as the secondary and discriminant polytopes. Our publicly available
implementation uses the experimental CGAL package triangulation. Our method
computes 5-, 6- and 7-dimensional polytopes with 35K, 23K and 500 vertices,
respectively, within 2hrs, and the Newton polytopes of many important surface
equations encountered in geometric modeling in <1sec, whereas the corresponding
secondary polytopes are intractable. It is faster than tropical geometry
software up to dimension 5 or 6. Hashing determinantal predicates accelerates
execution up to 100 times. One variant computes inner and outer approximations
with, respectively, 90% and 105% of the true volume, up to 25 times faster."
"We introduce a majorization order on monomials. With the help of this order,
we derive a necessary condition on the positive termination of a general
successive difference substitution algorithm (KSDS) for an input form $f$."
"The paper reports on a computer algebra program LSSS (Linear Selective
Systems Solver) for solving linear algebraic systems with rational
coefficients. The program is especially efficient for very large sparse systems
that have a solution in which many variables take the value zero. The program
is applied to the symmetry investigation of a non-abelian Laurent ODE
introduced recently by M. Kontsevich. The computed symmetries confirmed that a
Lax pair found for this system earlier generates all first integrals of degree
at least up to 14."
"We introduce a new algorithm denoted DSC2 to isolate the real roots of a
univariate square-free polynomial f with integer coefficients. The algorithm
iteratively subdivides an initial interval which is known to contain all real
roots of f. The main novelty of our approach is that we combine Descartes' Rule
of Signs and Newton iteration. More precisely, instead of using a fixed
subdivision strategy such as bisection in each iteration, a Newton step based
on the number of sign variations for an actual interval is considered, and,
only if the Newton step fails, we fall back to bisection. Following this
approach, our analysis shows that, for most iterations, we can achieve
quadratic convergence towards the real roots. In terms of complexity, our
method induces a recursion tree of almost optimal size O(nlog(n tau)), where n
denotes the degree of the polynomial and tau the bitsize of its coefficients.
The latter bound constitutes an improvement by a factor of tau upon all
existing subdivision methods for the task of isolating the real roots. In
addition, we provide a bit complexity analysis showing that DSC2 needs only
\tilde{O}(n^3tau) bit operations to isolate all real roots of f. This matches
the best bound known for this fundamental problem. However, in comparison to
the much more involved algorithms by Pan and Sch\""onhage (for the task of
isolating all complex roots) which achieve the same bit complexity, DSC2
focuses on real root isolation, is very easy to access and easy to implement."
"The foundational theory of differentiation was developed as part of the
original release of ACL2(r). In work reported at the last ACL2 Workshop, we
presented theorems justifying the usual differentiation rules, including the
chain rule and the derivative of inverse functions. However, the process of
applying these theorems to formalize the derivative of a particular function is
completely manual. More recently, we developed a macro and supporting functions
that can automate this process. This macro uses the ACL2 table facility to keep
track of functions and their derivatives, and it also interacts with the macro
that introduces inverse functions in ACL2(r), so that their derivatives can
also be automated. In this paper, we present the implementation of this macro
and related functions."
"A new concept, decomposition-unstable (DU) variety of a parametric polynomial
system, is introduced in this paper and the stabilities of several triangular
decomposition methods, such as characteristic set decomposition, relatively
simplicial decomposition and regular chain decomposition, for parametric
polynomial systems are discussed in detail. The concept leads to a definition
of weakly comprehensive triangular decomposition (WCTD) and a new algorithm for
computing comprehensive triangular decomposition (CTD) which was first
introduced in [4] for computing an analogue of comprehensive Groebner systems
for parametric polynomial systems. Our algorithm takes advantage of a
hierarchical solving strategy and a self-adaptive order of parameters. The
algorithm has been implemented with Maple 15 and experimented with a number of
benchmarks from the literature. Comparison with the Maple package
RegularChains, which contains an implementation of the algorithm in [4], is
provided and the results illustrate that the time costs by our program for
computing CTDs of most examples are no more than those by RegularChains."
"We prove three conjectures concerning the evaluation of determinants, which
are related to the counting of plane partitions and rhombus tilings. One of
them was posed by George Andrews in 1980, the other two were by Guoce Xin and
Christian Krattenthaler. Our proofs employ computer algebra methods, namely,
the holonomic ansatz proposed by Doron Zeilberger and variations thereof. These
variations make Zeilberger's original approach even more powerful and allow for
addressing a wider variety of determinants. Finally, we present, as a challenge
problem, a conjecture about a closed-form evaluation of Andrews's determinant."
"A fundamental problem in computer science is to find all the common zeroes of
$m$ quadratic polynomials in $n$ unknowns over $\mathbb{F}_2$. The
cryptanalysis of several modern ciphers reduces to this problem. Up to now, the
best complexity bound was reached by an exhaustive search in $4\log_2 n\,2^n$
operations. We give an algorithm that reduces the problem to a combination of
exhaustive search and sparse linear algebra. This algorithm has several
variants depending on the method used for the linear algebra step. Under
precise algebraic assumptions on the input system, we show that the
deterministic variant of our algorithm has complexity bounded by
$O(2^{0.841n})$ when $m=n$, while a probabilistic variant of the Las Vegas type
has expected complexity $O(2^{0.792n})$. Experiments on random systems show
that the algebraic assumptions are satisfied with probability very close to~1.
We also give a rough estimate for the actual threshold between our method and
exhaustive search, which is as low as~200, and thus very relevant for
cryptographic applications."
"We present an algorithm to compute the annihilator of (i.e., the linear
differential equations for) the logarithm of a polynomial in the ring of
differential operators with polynomial coefficients. The algorithm consists of
differentiation with respect to the parameter s of the annihilator of f^s for a
polynomial f and quotient computation. More generally, the annihilator of
f^s(log f)^m for a complex number s and a positive integer m can be computed,
which constitutes what is called a holonomic system in D-module theory. This
enables us to compute a holonomic system for the integral of a function
involving the logarithm of a polynomial by using integration algorithm for
D-modules."
"We present algorithms to compute the Smith Normal Form of matrices over two
families of local rings.
  The algorithms use the \emph{black-box} model which is suitable for sparse
and structured matrices. The algorithms depend on a number of tools, such as
matrix rank computation over finite fields, for which the best-known time- and
memory-efficient algorithms are probabilistic.
  For an $\nxn$ matrix $A$ over the ring $\Fzfe$, where $f^e$ is a power of an
irreducible polynomial $f \in \Fz$ of degree $d$, our algorithm requires
$\bigO(\eta de^2n)$ operations in $\F$, where our black-box is assumed to
require $\bigO(\eta)$ operations in $\F$ to compute a matrix-vector product by
a vector over $\Fzfe$ (and $\eta$ is assumed greater than $\Pden$). The
algorithm only requires additional storage for $\bigO(\Pden)$ elements of $\F$.
In particular, if $\eta=\softO(\Pden)$, then our algorithm requires only
$\softO(n^2d^2e^3)$ operations in $\F$, which is an improvement on known dense
methods for small $d$ and $e$.
  For the ring $\ZZ/p^e\ZZ$, where $p$ is a prime, we give an algorithm which
is time- and memory-efficient when the number of nontrivial invariant factors
is small. We describe a method for dimension reduction while preserving the
invariant factors. The time complexity is essentially linear in $\mu n r e \log
p,$ where $\mu$ is the number of operations in $\ZZ/p\ZZ$ to evaluate the
black-box (assumed greater than $n$) and $r$ is the total number of non-zero
invariant factors.
  To avoid the practical cost of conditioning, we give a Monte Carlo
certificate, which at low cost, provides either a high probability of success
or a proof of failure. The quest for a time- and memory-efficient solution
without restrictions on the number of nontrivial invariant factors remains
open. We offer a conjecture which may contribute toward that end."
"In this paper we consider polynomial representability of functions defined
over $Z_{p^n}$, where $p$ is a prime and $n$ is a positive integer. Our aim is
to provide an algorithmic characterization that (i) answers the decision
problem: to determine whether a given function over $Z_{p^n}$ is polynomially
representable or not, and (ii) finds the polynomial if it is polynomially
representable. The previous characterizations given by Kempner (1921) and
Carlitz (1964) are existential in nature and only lead to an exhaustive search
method, i.e., algorithm with complexity exponential in size of the input. Our
characterization leads to an algorithm whose running time is linear in size of
input. We also extend our result to the multivariate case."
"Total degree reverse lexicographic order is currently generally regarded as
most often fastest for computing Groebner bases. This article describes an
alternate less mysterious algorithm for computing this order using exponent
subtotals and describes why it should be very nearly the same speed the
traditional algorithm, all other things being equal. However, experimental
evidence suggests that subtotal order is actually slightly faster for the
Mathematica Groebner basis implementation more often than not. This is probably
because the weight vectors associated with the natural subtotal weight matrix
and with the usual total degree reverse lexicographic weight matrix are
different, and Mathematica also uses those the corresponding weight vectors to
help select successive S polynomials and divisor polynomials: Those selection
heuristics appear to work slightly better more often with subtotal weight
vectors.
  However, the most important advantage of exponent subtotals is pedagogical.
It is easier to understand than the total degree reverse lexicographic
algorithm, and it is more evident why the resulting order is often the fastest
known order for computing Groebner bases.
  Keywords: Term order, Total degree reverse lexicographic, tdeg, grevlex,
Groebner basis"
"Most computer algebra systems incorrectly simplify (z - z)/(sqrt(w^2)/w^3 -
1/(w*sqrt(w^2))) to 0 rather than to 0/0. The reasons for this are:
  1. The default simplification doesn't succeed in simplifying the denominator
to 0.
  2. There is a rule that 0 is the result of 0 divided by anything that doesn't
simplify to either 0 or 0/0.
  Try it on your computer algebra systems!
  This article describes how to simplify products of the form w^a*(w^b1)^g1 ...
(w^bn)^gn correctly and well, where w is any real or complex expression and the
exponents are rational numbers.
  It might seem that correct good simplification of such a restrictive
expression class must already be published and/or built into at least one
widely used computer-algebra system, but apparently this issue has been
overlooked. Default and relevant optional simplification was tested with 86
examples for n=1 on Derive, Maple, Mathematica, Maxima and TI-CAS. Totaled over
all five systems, 11% of the results were not equivalent to the input
everywhere, 50% of the results did not simplify to 0 a result that was
equivalent to 0, and at least 16% of the results exhibited one or more of four
additional flaw types. There was substantial room for improvement in all five
systems, including the two for which I was a co-author.
  The good news is: These flaws are easy to fix."
"Puiseux series are power series in which the exponents can be fractional
and/or negative rational numbers. Several computer algebra systems have one or
more built-in or loadable functions for computing truncated Puiseux series --
perhaps generalized to allow coefficients containing functions of the series
variable that are dominated by any power of that variable, such as logarithms
and nested logarithms of the series variable. Some computer-algebra systems
also offer functions that can compute more-general truncated recursive
hierarchical series. However, for all of these kinds of truncated series there
are important implementation details that haven't been addressed before in the
published literature and in current implementations.
  For implementers this article contains ideas for designing more convenient,
correct, and efficient implementations or improving existing ones. For users,
this article is a warning about some of these limitations. Many of the ideas in
this article have been implemented in the computer-algebra within the TI-Nspire
calculator, Windows and Macintosh products."
"Puiseux series are power series in which the exponents can be fractional
and/or negative rational numbers. Several computer algebra systems have one or
more built-in or loadable functions for computing truncated Puiseux series.
Some are generalized to allow coefficients containing functions of the series
variable that are dominated by any power of that variable, such as logarithms
and nested logarithms of the series variable. Some computer algebra systems
also have built-in or loadable functions that compute infinite Puiseux series.
Unfortunately, there are some little-known pitfalls in computing Puiseux
series. The most serious of these is expansions within branch cuts or at branch
points that are incorrect for some directions in the complex plane. For example
with each series implementation accessible to you:
  Compare the value of (z^2 + z^3)^(3/2) with that of its truncated series
expansion about z = 0, approximated at z = -0.01. Does the series converge to a
value that is the negative of the correct value?
  Compare the value of ln(z^2 + z^3) with its truncated series expansion about
z = 0, approximated at z = -0.01 + 0.1i. Does the series converge to a value
that is incorrect by 2pi i?
  Compare arctanh(-2 + ln(z)z) with its truncated series expansion about z = 0,
approximated at z = -0.01. Does the series converge to a value that is
incorrect by about pi i?
  At the time of this writing, most implementations that accommodate such
series exhibit such errors. This article describes how to avoid these errors
both for manual derivation of series and when implementing series packages."
"We present version 4.0 of the symbolic manipulation system FORM. The most
important new features are manipulation of rational polynomials and the
factorization of expressions. Many other new functions and commands are also
added; some of them are very general, while others are designed for building
specific high level packages, such as one for Groebner bases. New is also the
checkpoint facility, that allows for periodic backups during long calculations.
Lastly, FORM 4.0 has become available as open source under the GNU General
Public License version 3."
"We introduce the notion of radical parametrization of a surface, and we
provide algorithms to compute such type of parametrizations for families of
surfaces, like: Fermat surfaces, surfaces with a high multiplicity (at least
the degree minus 4) singularity, all irreducible surfaces of degree at most 5,
all irreducible singular surfaces of degree 6, and surfaces containing a pencil
of low-genus curves. In addition, we prove that radical parametrizations are
preserved under certain type of geometric constructions that include offset and
conchoids."
"To compute difference Groebner bases of ideals generated by linear
polynomials we adopt to difference polynomial rings the involutive algorithm
based on Janet-like division. The algorithm has been implemented in Maple in
the form of the package LDA (Linear Difference Algebra) and we describe the
main features of the package. Its applications are illustrated by generation of
finite difference approximations to linear partial differential equations and
by reduction of Feynman integrals. We also present the algorithm for an ideal
generated by a finite set of nonlinear difference polynomials. If the algorithm
terminates, then it constructs a Groebner basis of the ideal."
"We present a criterion for the existence of telescopers for mixed
hypergeometric terms, which is based on multiplicative and additive
decompositions. The criterion enables us to determine the termination of
Zeilberger's algorithms for mixed hypergeometric inputs."
"We present an algorithm for efficient computation of the constant term of a
power of a multivariate Laurent polynomial. The algorithm is based on
univariate interpolation, does not require the storage of intermediate data and
can be easily parallelized. As an application we compute the power series
expansion of the principal period of some toric Calabi-Yau varieties and find
previously unknown differential operators of Calabi-Yau type."
"Theano is a linear algebra compiler that optimizes a user's
symbolically-specified mathematical computations to produce efficient low-level
implementations. In this paper, we present new features and efficiency
improvements to Theano, and benchmarks demonstrating Theano's performance
relative to Torch7, a recently introduced machine learning library, and to
RNNLM, a C++ library targeted at recurrent neural networks."
"The aim of this work is to certify lower bounds for real-valued multivariate
functions, defined by semialgebraic or transcendental expressions. The
certificate must be, eventually, formally provable in a proof system such as
Coq. The application range for such a tool is widespread; for instance Hales'
proof of Kepler's conjecture yields thousands of inequalities. We introduce an
approximation algorithm, which combines ideas of the max-plus basis method (in
optimal control) and of the linear templates method developed by Manna et al.
(in static analysis). This algorithm consists in bounding some of the
constituents of the function by suprema of quadratic forms with a well chosen
curvature. This leads to semialgebraic optimization problems, solved by
sum-of-squares relaxations. Templates limit the blow up of these relaxations at
the price of coarsening the approximation. We illustrate the efficiency of our
framework with various examples from the literature and discuss the interfacing
with Coq."
"Let $\mathbf{f}=(f\_1,\ldots,f\_m)$ and $\mathbf{g}=(g\_1,\ldots,g\_m)$ be
two sets of $m\geq 1$ nonlinear polynomials over $\mathbb{K}[x\_1,\ldots,x\_n]$
($\mathbb{K}$ being a field). We consider the computational problem of finding
-- if any -- an invertible transformation on the variables mapping $\mathbf{f}$
to $\mathbf{g}$. The corresponding equivalence problem is known as {\tt
Isomorphism of Polynomials with one Secret} ({\tt IP1S}) and is a fundamental
problem in multivariate cryptography. The main result is a randomized
polynomial-time algorithm for solving {\tt IP1S} for quadratic instances, a
particular case of importance in cryptography and somewhat justifying {\it a
posteriori} the fact that {\it Graph Isomorphism} reduces to only cubic
instances of {\tt IP1S} (Agrawal and Saxena). To this end, we show that {\tt
IP1S} for quadratic polynomials can be reduced to a variant of the classical
module isomorphism problem in representation theory, which involves to test the
orthogonal simultaneous conjugacy of symmetric matrices. We show that we can
essentially {\it linearize} the problem by reducing quadratic-{\tt IP1S} to
test the orthogonal simultaneous similarity of symmetric matrices; this latter
problem was shown by Chistov, Ivanyos and Karpinski to be equivalent to finding
an invertible matrix in the linear space $\mathbb{K}^{n \times n}$ of $n \times
n$ matrices over $\mathbb{K}$ and to compute the square root in a matrix
algebra. While computing square roots of matrices can be done efficiently using
numerical methods, it seems difficult to control the bit complexity of such
methods. However, we present exact and polynomial-time algorithms for computing
the square root in $\mathbb{K}^{n \times n}$ for various fields (including
finite fields). We then consider \\#{\tt IP1S}, the counting version of {\tt
IP1S} for quadratic instances. In particular, we provide a (complete)
characterization of the automorphism group of homogeneous quadratic
polynomials. Finally, we also consider the more general {\it Isomorphism of
Polynomials} ({\tt IP}) problem where we allow an invertible linear
transformation on the variables \emph{and} on the set of polynomials. A
randomized polynomial-time algorithm for solving {\tt IP} when
\(\mathbf{f}=(x\_1^d,\ldots,x\_n^d)\) is presented. From an algorithmic point
of view, the problem boils down to factoring the determinant of a linear matrix
(\emph{i.e.}\ a matrix whose components are linear polynomials). This extends
to {\tt IP} a result of Kayal obtained for {\tt PolyProj}."
"Symbolic computation is the science of computing with symbolic objects
(terms, formulae, programs, algebraic objects, geometrical objects, etc).
Powerful symbolic algorithms have been developed during the past decades and
have played an influential role in theorem proving, automated reasoning,
software verification, model checking, rewriting, formalisation of mathematics,
network security, Groebner bases, characteristic sets, etc.
  The international Symposium on ""Symbolic Computation in Software Science"" is
the fourth in the SCSS workshop series. SCSS 2008 and 2010 took place at the
Research Institute for Symbolic Computation (RISC), Hagenberg, Austria, and,
SCSS 2009 took place in Gammarth, Tunisia. These symposium grew out of internal
workshops that bring together researchers from: a) SCORE (Symbolic Computation
Research Group) at the University of Tsukuba, Japan, b) Theorema Group at the
Research Institute for Symbolic Computation, Johannes Kepler University Linz,
Austria, c) SSFG (Software Science Foundation Group) at Kyoto University,
Japan, and d) Sup'Com (Higher School of Communication of Tunis) at the
University of Carthage, Tunisia."
"Let $f, f_1, \ldots, f_\nV$ be polynomials with rational coefficients in the
indeterminates $\bfX=X_1, \ldots, X_n$ of maximum degree $D$ and $V$ be the set
of common complex solutions of $\F=(f_1,\ldots, f_\nV)$. We give an algorithm
which, up to some regularity assumptions on $\F$, computes an exact
representation of the global infimum $f^\star=\inf_{x\in V\cap\R^n} f\Par{x}$,
i.e. a univariate polynomial vanishing at $f^\star$ and an isolating interval
for $f^\star$. Furthermore, this algorithm decides whether $f^\star$ is reached
and if so, it returns $x^\star\in V\cap\R^n$ such that
$f\Par{x^\star}=f^\star$. This algorithm is probabilistic. It makes use of the
notion of polar varieties. Its complexity is essentially cubic in $\Par{\nV
D}^n$ and linear in the complexity of evaluating the input. This fits within
the best known deterministic complexity class $D^{O(n)}$. We report on some
practical experiments of a first implementation that is available as a Maple
package. It appears that it can tackle global optimization problems that were
unreachable by previous exact algorithms and can manage instances that are hard
to solve with purely numeric techniques. As far as we know, even under the
extra genericity assumptions on the input, it is the first probabilistic
algorithm that combines practical efficiency with good control of complexity
for this problem."
"We study the use of the Euler-Maclaurin formula to numerically evaluate the
Hurwitz zeta function $\zeta(s,a)$ for $s, a \in \mathbb{C}$, along with an
arbitrary number of derivatives with respect to $s$, to arbitrary precision
with rigorous error bounds. Techniques that lead to a fast implementation are
discussed. We present new record computations of Stieltjes constants, Keiper-Li
coefficients and the first nontrivial zero of the Riemann zeta function,
obtained using an open source implementation of the algorithms described in
this paper."
"Cylindrical algebraic decompositions (CADs) are a key tool in real algebraic
geometry, used primarily for eliminating quantifiers over the reals and
studying semi-algebraic sets. In this paper we introduce cylindrical algebraic
sub-decompositions (sub-CADs), which are subsets of CADs containing all the
information needed to specify a solution for a given problem.
  We define two new types of sub-CAD: variety sub-CADs which are those cells in
a CAD lying on a designated variety; and layered sub-CADs which have only those
cells of dimension higher than a specified value. We present algorithms to
produce these and describe how the two approaches may be combined with each
other and the recent theory of truth-table invariant CAD.
  We give a complexity analysis showing that these techniques can offer
substantial theoretical savings, which is supported by experimentation using an
implementation in Maple."
"Parallel telescoping is a natural generalization of differential
creative-telescoping for single integrals to line integrals. It computes a
linear ordinary differential operator $L$, called a parallel telescoper, for
several multivariate functions, such that the applications of $L$ to the
functions yield antiderivatives of a single function. We present a necessary
and sufficient condition guaranteeing the existence of parallel telescopers for
differentially finite functions, and develop an algorithm to compute minimal
ones for compatible hyperexponential functions. Besides computing annihilators
of parametric line integrals, we use the parallel telescoping for determining
Galois groups of parameterized partial differential systems of first order."
"We propose a generalization of the Weierstrass iteration for over-constrained
systems of equations and we prove that the proposed method is the Gauss-Newton
iteration to find the nearest system which has at least $k$ common roots and
which is obtained via a perturbation of prescribed structure. In the univariate
case we show the connection of our method to the optimization problem
formulated by Karmarkar and Lakshman for the nearest GCD. In the multivariate
case we generalize the expressions of Karmarkar and Lakshman, and give
explicitly several iteration functions to compute the optimum.
  The arithmetic complexity of the iterations is detailed."
"A new algorithm to compute cylindrical algebraic decompositions (CADs) is
presented, building on two recent advances. Firstly, the output is truth table
invariant (a TTICAD) meaning given formulae have constant truth value on each
cell of the decomposition. Secondly, the computation uses regular chains theory
to first build a cylindrical decomposition of complex space (CCD) incrementally
by polynomial. Significant modification of the regular chains technology was
used to achieve the more sophisticated invariance criteria. Experimental
results on an implementation in the RegularChains Library for Maple verify that
combining these advances gives an algorithm superior to its individual
components and competitive with the state of the art."
"Kaltofen has proposed a new approach in [Kaltofen 1992] for computing matrix
determinants. The algorithm is based on a baby steps/giant steps construction
of Krylov subspaces, and computes the determinant as the constant term of a
characteristic polynomial. For matrices over an abstract field and by the
results of Baur and Strassen 1983, the determinant algorithm, actually a
straight-line program, leads to an algorithm with the same complexity for
computing the adjoint of a matrix [Kaltofen 1992]. However, the latter is
obtained by the reverse mode of automatic differentiation and somehow is not
``explicit''. We study this adjoint algorithm, show how it can be implemented
(without resorting to an automatic transformation), and demonstrate its use on
polynomial matrices."
"In this paper we present algorithmic considerations and theoretical results
about the relation between the orders of certain groups associated to the
components of a polynomial and the order of the group that corresponds to the
polynomial, proving it for arbitrary tame polynomials, and considering the case
of rational functions."
"One of the main contributions which Volker Weispfenning made to mathematics
is related to Groebner bases theory. In this paper we present an algorithm for
computing all algebraic intermediate subfields in a separably generated
unirational field extension (which in particular includes the zero
characteristic case). One of the main tools is Groebner bases theory. Our
algorithm also requires computing primitive elements and factoring over
algebraic extensions. Moreover, the method can be extended to finitely
generated K-algebras."
"In this paper we present an algorithm for computing all algebraic
intermediate subfields in a separably generated unirational field extension
(which in particular includes the zero characteristic case). One of the main
tools is Groebner bases theory. Our algorithm also requires computing computing
primitive elements and factoring over algebraic extensions. Moreover, the
method can be extended to finitely generated K-algebras."
"In this paper we describe by a number of examples how to deduce one single
characterizing higher order differential equation for output quantities of an
analog circuit.
  In the linear case, we apply basic ""symbolic"" methods from linear algebra to
the system of differential equations which is used to model the analog circuit.
For nonlinear circuits and their corresponding nonlinear differential
equations, we show how to employ computer algebra tools implemented in Maple,
which are based on differential algebra."
"Let G=Aut_K (K(x)) be the Galois group of the transcendental degree one pure
field extension K(x)/K. In this paper we describe polynomial time algorithms
for computing the field Fix(H) fixed by a subgroup H < G and for computing the
fixing group G_f of a rational function f in K(x)."
"In this paper we present an algorithm to compute all unirational fields of
transcendence degree one containing a given finite set of multivariate rational
functions. In particular, we provide an algorithm to decompose a multivariate
rational function f of the form f=g(h), where g is a univariate rational
function and h a multivariate one."
"Constructive methods for matrices of multihomogeneous (or multigraded)
resultants for unmixed systems have been studied by Weyman, Zelevinsky,
Sturmfels, Dickenstein and Emiris. We generalize these constructions to mixed
systems, whose Newton polytopes are scaled copies of one polytope, thus taking
a step towards systems with arbitrary supports. First, we specify matrices
whose determinant equals the resultant and characterize the systems that admit
such formulae. Bezout-type determinantal formulae do not exist, but we describe
all possible Sylvester-type and hybrid formulae. We establish tight bounds for
all corresponding degree vectors, and specify domains that will surely contain
such vectors; the latter are new even for the unmixed case. Second, we make use
of multiplication tables and strong duality theory to specify resultant
matrices explicitly, for a general scaled system, thus including unmixed
systems. The encountered matrices are classified; these include a new type of
Sylvester-type matrix as well as Bezout-type matrices, known as partial
Bezoutians. Our public-domain Maple implementation includes efficient storage
of complexes in memory, and construction of resultant matrices."
"Existing algorithms for isolating real solutions of zero-dimensional
polynomial systems do not compute the multiplicities of the solutions. In this
paper, we define in a natural way the multiplicity of solutions of
zero-dimensional triangular polynomial systems and prove that our definition is
equivalent to the classical definition of local (intersection) multiplicity.
Then we present an effective and complete algorithm for isolating real
solutions with multiplicities of zero-dimensional triangular polynomial systems
using our definition. The algorithm is based on interval arithmetic and
square-free factorization of polynomials with real algebraic coefficients. The
computational results on some examples from the literature are presented."
"Given an n x n matrix over the ring of differential polynomials
F(t)[\D;\delta], we show how to compute the Hermite form H of A, and a
unimodular matrix U such that UA=H. The algorithm requires a polynomial number
of operations in terms of n, deg_D(A), and deg_t(A). When F is the field of
rational numbers, it also requires time polynomial in the bit-length of the
coefficients."
"Let $\mathbf{x_1}, ..., \mathbf{x_t} \in \mathbb{R}^{n}$. A simultaneous
integer relation (SIR) for $\mathbf{x_1}, ..., \mathbf{x_t}$ is a vector
$\mathbf{m} \in \mathbb{Z}^{n}\setminus\{\textbf{0}\}$ such that
$\mathbf{x_i}^T\mathbf{m} = 0$ for $i = 1, ..., t$. In this paper, we propose
an algorithm SIRD to detect an SIR for real vectors, which constructs an SIR
within $\mathcal {O}(n^4 + n^3 \log \lambda(X))$ arithmetic operations, where
$\lambda(X)$ is the least Euclidean norm of SIRs for $\mathbf{x_1}, >...,
\mathbf{x_t}$. One can easily generalize SIRD to complex number field.
Experimental results show that SIRD is practical and better than another
detecting algorithm in the literature. In its application, we present a new
algorithm for finding the minimal polynomial of an arbitrary complex algebraic
number from its an approximation, which is not based on LLL. We also provide a
sufficient condition on the precision of the approximate value, which depends
only on the height and the degree of the algebraic number."
"We describe how we connected three programs that compute Groebner bases to
Coq, to do automated proofs on algebraic, geometrical and arithmetical
expressions. The result is a set of Coq tactics and a certificate mechanism
(downloadable at http://www-sop.inria.fr/marelle/Loic.Pottier/gb-keappa.tgz).
The programs are: F4, GB \, and gbcoq. F4 and GB are the fastest (up to our
knowledge) available programs that compute Groebner bases. Gbcoq is slow in
general but is proved to be correct (in Coq), and we adapted it to our specific
problem to be efficient. The automated proofs concern equalities and
non-equalities on polynomials with coefficients and indeterminates in R or Z,
and are done by reducing to Groebner computation, via Hilbert's
Nullstellensatz. We adapted also the results of Harrison, to allow to prove
some theorems about modular arithmetics. The connection between Coq and the
programs that compute Groebner bases is done using the ""external"" tactic of Coq
that allows to call arbitrary programs accepting xml inputs and outputs. We
also produce certificates in order to make the proof scripts independant from
the external programs."
"A nonhomogeneous system of linear recurrence equations can be recognized by
an automaton $\mathcal{A}$ over a one-letter alphabet $A = \{z\}$. Conversely,
the automaton $\mathcal{A}$ generates precisely this nonhomogeneous system of
linear recurrence equations. We present the solutions of these systems and
apply the $z$-transform to these solutions to obtain their series
representation. Finally, we show some results that simplify the series
representation of the $z$-transform of these solutions. We consider single
systems as well as the composition of two systems."
"In the current work, the author present a symbolic algorithm for finding the
determinant of any general nonsingular cyclic heptadiagonal matrices and
inverse of anti-cyclic heptadiagonal matrices. The algorithms are mainly based
on the work presented in [A. A. KARAWIA, A New Algorithm for Inverting General
Cyclic Heptadiagonal Matrices Recursively, arXiv:1011.2306v1 [cs.SC]]. The
symbolic algorithms are suited for implementation using Computer Algebra
Systems (CAS) such as MATLAB, MAPLE and MATHEMATICA. An illustrative example is
given."
"Efficient characteristic set methods for computing solutions of polynomial
equation systems in a finite field are proposed. The concept of proper
triangular sets is introduced and an explicit formula for the number of
solutions of a proper and monic (or regular) triangular set is given. An
improved zero decomposition algorithm which can be used to reduce the zero set
of an equation system in general form to the union of zero sets of monic proper
triangular sets is proposed. As a consequence, we can give an explicit formula
for the number of solutions of an equation system. Bitsize complexity for the
algorithm is given in the case of Boolean polynomials. We also give a
multiplication free characteristic set method for Boolean polynomials, where
the sizes of the polynomials are effectively controlled. The algorithms are
implemented in the case of Boolean polynomials and extensive experiments show
that they are quite efficient for solving certain classes of Boolean equations."
"In this paper, we consider an extended concept of invariant for polynomial
dynamical system (PDS) with domain and initial condition, and establish a sound
and complete criterion for checking semi-algebraic invariants (SAI) for such
PDSs. The main idea is encoding relevant dynamical properties as conditions on
the high order Lie derivatives of polynomials occurring in the SAI. A direct
consequence of this criterion is a relatively complete method of SAI generation
based on template assumption and semi-algebraic constraint solving. Relative
completeness means if there is an SAI in the form of a predefined template,
then our method can indeed find one using this template."
"Let $\xx_n=(x_1,\ldots,x_n)$ and $f\in \R[\xx_n,k]$. The problem of finding
all $k_0$ such that $f(\xx_n,k_0)\ge 0$ on $\mathbb{R}^n$ is considered in this
paper, which obviously takes as a special case the problem of computing the
global infimum or proving the semi-definiteness of a polynomial.
  For solving the problems, we propose a simplified Brown's CAD projection
operator, \Nproj, of which the projection scale is always no larger than that
of Brown's. For many problems, the scale is much smaller than that of Brown's.
As a result, the lifting phase is also simplified. Some new algorithms based on
\Nproj\ for solving those problems are designed and proved to be correct.
Comparison to some existing tools on some examples is reported to illustrate
the effectiveness of our new algorithms."
"We show how to improve the efficiency of the computation of fast Fourier
transforms over F_p where p is a word-sized prime. Our main technique is
optimisation of the basic arithmetic, in effect decreasing the total number of
reductions modulo p, by making use of a redundant representation for integers
modulo p. We give performance results showing a significant improvement over
Shoup's NTL library."
"Gr\""obner Bases and Cylindrical Algebraic Decomposition are generally thought
of as two, rather different, methods of looking at systems of equations and, in
the case of Cylindrical Algebraic Decomposition, inequalities. However, even
for a mixed system of equalities and inequalities, it is possible to apply
Gr\""obner bases to the (conjoined) equalities before invoking CAD. We see that
this is, quite often but not always, a beneficial preconditioning of the CAD
problem.
  It is also possible to precondition the (conjoined) inequalities with respect
to the equalities, and this can also be useful in many cases."
"We describe the implementation of output code optimization in the open source
computer algebra system FORM. This implementation is based on recently
discovered techniques of Monte Carlo tree search to find efficient multivariate
Horner schemes, in combination with other optimization algorithms, such as
common subexpression elimination. For systems for which no specific knowledge
is provided it performs significantly better than other methods we could
compare with. Because the method has a number of free parameters, we also show
some methods by which to tune them to different types of problems."
"A period of a rational integral is the result of integrating, with respect to
one or several variables, a rational function over a closed path. This work
focuses particularly on periods depending on a parameter: in this case the
period under consideration satisfies a linear differential equation, the
Picard-Fuchs equation. I give a reduction algorithm that extends the
Griffiths-Dwork reduction and apply it to the computation of Picard-Fuchs
equations. The resulting algorithm is elementary and has been successfully
applied to problems that were previously out of reach."
"Cylindrical algebraic decomposition(CAD) is a key tool in computational
algebraic geometry, particularly for quantifier elimination over real-closed
fields. When using CAD, there is often a choice for the ordering placed on the
variables. This can be important, with some problems infeasible with one
variable ordering but easy with another. Machine learning is the process of
fitting a computer model to a complex function based on properties learned from
measured data. In this paper we use machine learning (specifically a support
vector machine) to select between heuristics for choosing a variable ordering,
outperforming each of the separate heuristics."
"We give several new algorithms for dense polynomial multiplication based on
the Kronecker substitution method. For moderately sized input polynomials, the
new algorithms improve on the performance of the standard Kronecker
substitution by a sizeable constant, both in theory and in empirical tests."
"In this note we prove a generalization of the flat extension theorem of Curto
and Fialkow for truncated moment matrices. It applies to moment matrices
indexed by an arbitrary set of monomials and its border, assuming that this set
is connected to 1. When formulated in a basis-free setting, this gives an
equivalent result for truncated Hankel operators."
"A constructive approach to get the reduced row echelon form of a given matrix
$A$ is presented. It has been shown that after the $k$th step of the
Gauss-Jordan procedure, each entry $a^k_{ij}(i<>j; j > k)$ in the new matrix
$A^k$ can always be expressed as a ratio of two determinants whose entries are
from the original matrix $A$. The new method also gives a more general
generalization of Cramer's rule than existing methods."
"We give new algorithms for the computation of square roots and reciprocals of
power series in C[[x]]. If M(n) denotes the cost of multiplying polynomials of
degree n, the square root to order n costs (1.333... + o(1)) M(n) and the
reciprocal costs (1.444... + o(1)) M(n). These improve on the previous best
results, respectively (1.8333... + o(1)) M(n) and (1.5 + o(1)) M(n)."
"We present a new algorithm for solving the real roots of a bivariate
polynomial system $\Sigma=\{f(x,y),g(x,y)\}$ with a finite number of solutions
by using a zero-matching method. The method is based on a lower bound for
bivariate polynomial system when the system is non-zero. Moreover, the
multiplicities of the roots of $\Sigma=0$ can be obtained by a given
neighborhood. From this approach, the parallelization of the method arises
naturally. By using a multidimensional matching method this principle can be
generalized to the multivariate equation systems."
"We propose a generic design for Chinese remainder algorithms. A Chinese
remainder computation consists in reconstructing an integer value from its
residues modulo non coprime integers. We also propose an efficient linear data
structure, a radix ladder, for the intermediate storage and computations. Our
design is structured into three main modules: a black box residue computation
in charge of computing each residue; a Chinese remaindering controller in
charge of launching the computation and of the termination decision; an integer
builder in charge of the reconstruction computation. We then show that this
design enables many different forms of Chinese remaindering (e.g.
deterministic, early terminated, distributed, etc.), easy comparisons between
these forms and e.g. user-transparent parallelism at different parallel grains."
"In this note we reinvestigate the task of computing creative telescoping
relations in differential-difference operator algebras. Our approach is based
on an ansatz that explicitly includes the denominators of the delta parts. We
contribute several ideas of how to make an implementation of this approach
reasonably fast and provide such an implementation. A selection of examples
shows that it can be superior to existing methods by a large factor."
"Finding the product of two polynomials is an essential and basic problem in
computer algebra. While most previous results have focused on the worst-case
complexity, we instead employ the technique of adaptive analysis to give an
improvement in many ""easy"" cases. We present two adaptive measures and methods
for polynomial multiplication, and also show how to effectively combine them to
gain both advantages. One useful feature of these algorithms is that they
essentially provide a gradient between existing ""sparse"" and ""dense"" methods.
We prove that these approaches provide significant improvements in many cases
but in the worst case are still comparable to the fastest existing algorithms."
"The extended L\""uroth's Theorem says that if the transcendence degree of
$\KK(\mathsf{f}_1,\dots,\mathsf{f}_m)/\KK$ is 1 then there exists $f \in
\KK(\underline{X})$ such that $\KK(\mathsf{f}_1,\dots,\mathsf{f}_m)$ is equal
to $\KK(f)$. In this paper we show how to compute $f$ with a probabilistic
algorithm. We also describe a probabilistic and a deterministic algorithm for
the decomposition of multivariate rational functions. The probabilistic
algorithms proposed in this paper are softly optimal when $n$ is fixed and $d$
tends to infinity. We also give an indecomposability test based on gcd
computations and Newton's polytope. In the last section, we show that we get a
polynomial time algorithm, with a minor modification in the exponential time
decomposition algorithm proposed by Gutierez-Rubio-Sevilla in 2001."
"Decomposing the domain of a function into parts has many uses in mathematics.
A domain may naturally be a union of pieces, a function may be defined by
cases, or different boundary conditions may hold on different regions. For any
particular problem the domain can be given explicitly, but when dealing with a
family of problems given in terms of symbolic parameters, matters become more
difficult. This article shows how hybrid sets, that is multisets allowing
negative multiplicity, may be used to express symbolic domain decompositions in
an efficient, elegant and uniform way, simplifying both computation and
reasoning. We apply this theory to the arithmetic of piecewise functions and
symbolic matrices and show how certain operations may be reduced from
exponential to linear complexity."
"Let $\mathbb{Q}(\alpha)$ and $\mathbb{Q}(\beta)$ be algebraic number fields.
We describe a new method to find (if they exist) all isomorphisms,
$\mathbb{Q}(\beta) \rightarrow \mathbb{Q}(\alpha)$. The algorithm is
particularly efficient if the number of isomorphisms is one."
"Modular exponentiation is a common mathematical operation in modern
cryptography. This, along with modular multiplication at the base and exponent
levels (to different moduli) plays an important role in a large number of key
agreement protocols. In our earlier work, we gave many decidability as well as
undecidability results for multiple equational theories, involving various
properties of modular exponentiation. Here, we consider a partial subtheory
focussing only on exponentiation and multiplication operators. Two main results
are proved. The first result is positive, namely, that the unification problem
for the above theory (in which no additional property is assumed of the
multiplication operators) is decidable. The second result is negative: if we
assume that the two multiplication operators belong to two different abelian
groups, then the unification problem becomes undecidable."
"We prove that the Tiden and Arnborg algorithm for equational unification
modulo one-sided distributivity is not polynomial time bounded as previously
thought. A set of counterexamples is developed that demonstrates that the
algorithm goes through exponentially many steps."
"This paper presents a conception for computing gr\""{o}bner basis. We convert
some of gr\""{o}bner-computing algorithms, e.g., F5, extended F5 and GWV
algorithms into a special type of algorithm. The new algorithm's finite
termination problem can be described by equivalent conditions, so all the above
algorithms can be determined when they terminate finitely. At last, a new
criterion is presented. It is an improvement for the Rewritten and Signature
Criterion."
"The problem of the determination of the Hilbert-space metric which renders a
given Hamiltonian $H$ self-adjoint is addressed from the point of view of
applicability of computer-assisted algebraic manipulations. An exactly solvable
example of the so called Gegenbauerian quantum-lattice oscillator is recalled
for the purpose. Both the construction of suitable metric (basically, the
solution of the Dieudonne's operator equation) and the determination of its
domain of positivity are shown facilitated by the symbolic algebraic
manipulations and by MAPLE-supported numerics and graphics."
"In this paper, we first introduce the concept of Laurent differentially
essential systems and give a criterion for Laurent differentially essential
systems in terms of their supports. Then the sparse differential resultant for
a Laurent differentially essential system is defined and its basic properties
are proved. In particular, order and degree bounds for the sparse differential
resultant are given. Based on these bounds, an algorithm to compute the sparse
differential resultant is proposed, which is single exponential in terms of the
number of indeterminates, the Jacobi number of the system, and the size of the
system."
"In 1977, Adleman, Manders and Miller had briefly described how to extend
their square root extraction method to the general $r$th root extraction over
finite fields, but not shown enough details. Actually, there is a dramatic
difference between the square root extraction and the general $r$th root
extraction because one has to solve discrete logarithms for $r$th root
extraction. In this paper, we clarify their method and analyze its complexity.
Our heuristic presentation is helpful to grasp the method entirely and deeply."
"Given the equations of the first and the second order surfaces in
multidimensional space, our goal is to construct a univariate polynomial one of
the zeros of which coincides with the square of the distance between these
surfaces. To achieve this goal we employ Elimination Theory methods. The
proposed approach is also extended for the case of parameter dependent
surfaces."
"This paper revisits an algorithm for isolating real roots of univariate
polynomials based on continued fractions. It follows the work of Vincent,
Uspen- sky, Collins and Akritas, Johnson and Krandick. We use some tricks,
especially a new algorithm for computing an upper bound of positive roots. In
this way, the algorithm of isolating real roots is improved. The complexity of
our method for computing an upper bound of positive roots is O(n log(u+1))
where u is the optimal upper bound satisfying Theorem 3 and n is the degree of
the polynomial. Our method has been implemented as a software package logcf
using C++ language. For many benchmarks logcf is two or three times faster than
the function RootIntervals of Mathematica. And it is much faster than another
continued fractions based software CF, which seems to be one of the fastest
available open software for exact real root isolation. For those benchmarks
which have only real roots, logcf is much faster than Sleeve and eigensolve
which are based on numerical computation."
"We study algorithms for the fast computation of modular inverses.
Newton-Raphson iteration over $p$-adic numbers gives a recurrence relation
computing modular inverse modulo $p^m$, that is logarithmic in $m$. We solve
the recurrence to obtain an explicit formula for the inverse. Then we study
different implementation variants of this iteration and show that our explicit
formula is interesting for small exponent values but slower or large exponent,
say of more than 700 bits. Overall we thus propose a hybrid combination of our
explicit formula and the best asymptotic variants. This hybrid combination
yields then a constant factor improvement, also for large exponents."
"We describe a new Maple package for treating boundary problems for linear
ordinary differential equations, allowing two-/multipoint as well as Stieltjes
boundary conditions. For expressing differential operators, boundary
conditions, and Green's operators, we employ the algebra of
integro-differential operators. The operations implemented for regular boundary
problems include computing Green's operators as well as composing and factoring
boundary problems. Our symbolic approach to singular boundary problems is new;
it provides algorithms for computing compatibility conditions and generalized
Green's operators."
"In this paper, the author present a reliable symbolic computational algorithm
for inverting a general comrade matrix by using parallel computing along with
recursion. The computational cost of our algorithm is O(n^2). The algorithm is
implementable to the Computer Algebra System (CAS) such as MAPLE, MATLAB and
MATHEMATICA. Three examples are presented for the sake of illustration."
"In this paper, the concept of sparse difference resultant for a Laurent
transformally essential system of difference polynomials is introduced and a
simple criterion for the existence of sparse difference resultant is given. The
concept of transformally homogenous polynomial is introduced and the sparse
difference resultant is shown to be transformally homogenous. It is shown that
the vanishing of the sparse difference resultant gives a necessary condition
for the corresponding difference polynomial system to have non-zero solutions.
The order and degree bounds for sparse difference resultant are given. Based on
these bounds, an algorithm to compute the sparse difference resultant is
proposed, which is single exponential in terms of the number of variables, the
Jacobi number, and the size of the Laurent transformally essential system.
Furthermore, the precise order and degree, a determinant representation, and a
Poisson-type product formula for the difference resultant are given."
"We present a reduction algorithm that simultaneously extends Hermite's
reduction for rational functions and the Hermite-like reduction for
hyperexponential functions. It yields a unique additive decomposition and
allows to decide hyperexponential integrability. Based on this reduction
algorithm, we design a new method to compute minimal telescopers for bivariate
hyperexponential functions. One of its main features is that it can avoid the
costly computation of certificates. Its implementation outperforms Maple's
function DEtools[Zeilberger]. Moreover, we derive an order bound on minimal
telescopers, which is more general and tighter than the known one."
"A finite number of rational functions are compatible if they satisfy the
compatibility conditions of a first-order linear functional system involving
differential, shift and q-shift operators. We present a theorem that describes
the structure of compatible rational functions. The theorem enables us to
decompose a solution of such a system as a product of a rational function,
several symbolic powers, a hyperexponential function, a hypergeometric term,
and a q-hypergeometric term. We outline an algorithm for computing this
product, and present an application."
"We present a new superfast algorithm for solving Toeplitz systems. This
algorithm is based on a relation between the solution of such problems and
syzygies of polynomials or moving lines. We show an explicit connection between
the generators of a Toeplitz matrix and the generators of the corresponding
module of syzygies. We show that this module is generated by two elements and
the solution of a Toeplitz system T u=g can be reinterpreted as the remainder
of a vector depending on g, by these two generators. We obtain these generators
and this remainder with computational complexity O(n log^2 n) for a Toeplitz
matrix of size nxn."
"Inspired by previous work of Shoup, Lenstra-De Smit and Couveignes-Lercier,
we give fast algorithms to compute in (the first levels of) the ell-adic
closure of a finite field. In many cases, our algorithms have quasi-linear
complexity."
"We present algorithms to factorize weighted homogeneous elements in the first
polynomial Weyl algebra and $q$-Weyl algebra, which are both viewed as a
$\mathbb{Z}$-graded rings. We show, that factorization of homogeneous
polynomials can be almost completely reduced to commutative univariate
factorization over the same base field with some additional uncomplicated
combinatorial steps. This allows to deduce the complexity of our algorithms in
detail. Furthermore, we will show for homogeneous polynomials that
irreducibility in the polynomial first Weyl algebra also implies irreducibility
in the rational one, which is of interest for practical reasons. We report on
our implementation in the computer algebra system \textsc{Singular}. It
outperforms for homogeneous polynomials currently available implementations
dealing with factorization in the first Weyl algebra both in speed and elegancy
of the results."
"In this paper, we address the problem of safety verification of interval
hybrid systems in which the coefficients are intervals instead of explicit
numbers. A hybrid symbolic-numeric method, based on SOS relaxation and interval
arithmetic certification, is proposed to generate exact inequality invariants
for safety verification of interval hybrid systems. As an application, an
approach is provided to verify safety properties of non-polynomial hybrid
systems. Experiments on the benchmark hybrid systems are given to illustrate
the efficiency of our method."
"This volume contains the proceedings of the Seventh International Workshop on
Computing with Terms and Graphs (TERMGRAPH 2013). The workshop took place in
Rome, Italy, on March 23rd, 2013, as part of the sixteenth edition of the
European Joint Conferences on Theory and Practice of Software (ETAPS 2013).
  Research in term and graph rewriting ranges from theoretical questions to
practical issues. Computing with graphs handles the sharing of common
subexpressions in a natural and seamless way, and improves the efficiency of
computations in space and time. Sharing is ubiquitous in several research
areas, as witnessed by the modelling of first- and higher-order term rewriting
by (acyclic or cyclic) graph rewriting, the modelling of biological or chemical
abstract machines, and the implementation techniques of programming languages:
many implementations of functional, logic, object-oriented, concurrent and
mobile calculi are based on term graphs. Term graphs are also used in automated
theorem proving and symbolic computation systems working on shared structures.
The aim of this workshop is to bring together researchers working in different
domains on term and graph transformation and to foster their interaction, to
provide a forum for presenting new ideas and work in progress, and to enable
newcomers to learn about current activities in term graph rewriting.
  These proceedings contain six accepted papers and the abstracts of two
invited talks. All submissions were subject to careful refereeing. The topics
of accepted papers range over a wide spectrum, including theoretical aspects of
term graph rewriting, concurrency, semantics as well as application issues of
term graph transformation."
"In classical invariant theory, the Gr\""obner base of the ideal of syzygies
and the normal forms of polynomials of invariants are two core contents. To
improve the performance of invariant theory in symbolic computing of classical
geometry, advanced invariants are introduced via Clifford product. This paper
addresses and solves the two key problems in advanced invariant theory: the
Gr\""obner base of the ideal of syzygies among advanced invariants, and the
normal forms of polynomials of advanced invariants. These results beautifully
extend the straightening of Young tableaux to advanced invariants."
"In this paper, we consider the problem of computing estimates of the
domain-of-attraction for non-polynomial systems. A polynomial approximation
technique, based on multivariate polynomial interpolation and error analysis
for remaining functions, is applied to compute an uncertain polynomial system,
whose set of trajectories contains that of the original non-polynomial system.
Experiments on the benchmark non-polynomial systems show that our approach
gives better estimates of the domain-of-attraction."
"We present an algorithm for computing a separating linear form of a system of
bivariate polynomials with integer coefficients, that is a linear combination
of the variables that takes different values when evaluated at distinct
(complex) solutions of the system. In other words, a separating linear form
defines a shear of the coordinate system that sends the algebraic system in
generic position, in the sense that no two distinct solutions are vertically
aligned. The computation of such linear forms is at the core of most algorithms
that solve algebraic systems by computing rational parameterizations of the
solutions and, moreover, the computation a separating linear form is the
bottleneck of these algorithms, in terms of worst-case bit complexity. Given
two bivariate polynomials of total degree at most $d$ with integer coefficients
of bitsize at most~$\tau$, our algorithm computes a separating linear form in
$\sOB(d^{8}+d^7\tau)$ bit operations in the worst case, where the previously
known best bit complexity for this problem was $\sOB(d^{10}+d^9\tau)$ (where
$\sO$ refers to the complexity where polylogarithmic factors are omitted and
$O_B$ refers to the bit complexity)."
"We address the problem of solving systems of two bivariate polynomials of
total degree at most $d$ with integer coefficients of maximum bitsize $\tau$.
It is known that a linear separating form, that is a linear combination of the
variables that takes different values at distinct solutions of the system, can
be computed in $\sOB(d^{8}+d^7\tau)$ bit operations (where $O_B$ refers to bit
complexities and $\sO$ to complexities where polylogarithmic factors are
omitted) and we focus here on the computation of a Rational Univariate
Representation (RUR) given a linear separating form. We present an algorithm
for computing a RUR with worst-case bit complexity in $\sOB(d^7+d^6\tau)$ and
bound the bitsize of its coefficients by $\sO(d^2+d\tau)$. We show in addition
that isolating boxes of the solutions of the system can be computed from the
RUR with $\sOB(d^{8}+d^7\tau)$ bit operations. Finally, we show how a RUR can
be used to evaluate the sign of a bivariate polynomial (of degree at most $d$
and bitsize at most $\tau$) at one real solution of the system in
$\sOB(d^{8}+d^7\tau)$ bit operations and at all the $\Theta(d^2)$ {real}
solutions in only $O(d)$ times that for one solution."
"In this paper, we extend the characterization of $\mathbb{Z}[x]/\ < f \ >$,
where $f \in \mathbb{Z}[x]$ to be a free $\mathbb{Z}$-module to multivariate
polynomial rings over any commutative Noetherian ring, $A$. The
characterization allows us to extend the Gr\""obner basis method of computing a
$\Bbbk$-vector space basis of residue class polynomial rings over a field
$\Bbbk$ (Macaulay-Buchberger Basis Theorem) to rings, i.e.
$A[x_1,\ldots,x_n]/\mathfrak{a}$, where $\mathfrak{a} \subseteq
A[x_1,\ldots,x_n]$ is an ideal. We give some insights into the characterization
for two special cases, when $A = \mathbb{Z}$ and $A =
\Bbbk[\theta_1,\ldots,\theta_m]$. As an application of this characterization,
we show that the concept of border bases can be extended to rings when the
corresponding residue class ring is a finitely generated, free $A$-module."
"Many computer algebra systems have more than 1000 built-in functions, making
expertise difficult. Using mock dialog boxes, this article describes a proposed
interactive general-purpose wizard for organizing optional transformations and
allowing easy fine grain control over the form of the result even by amateurs.
This wizard integrates ideas including:
  * flexible subexpression selection;
  * complete control over the ordering of variables and commutative operands,
with well-chosen defaults;
  * interleaving the choice of successively less main variables with applicable
function choices to provide detailed control without incurring a combinatorial
number of applicable alternatives at any one level;
  * quick applicability tests to reduce the listing of inapplicable
transformations;
  * using an organizing principle to order the alternatives in a helpful
manner;
  * labeling quickly-computed alternatives in dialog boxes with a preview of
their results,
  * using ellipsis elisions if necessary or helpful;
  * allowing the user to retreat from a sequence of choices to explore other
branches of the tree of alternatives or to return quickly to branches already
visited;
  * allowing the user to accumulate more than one of the alternative forms;
  * integrating direct manipulation into the wizard; and
  * supporting not only the usual input-result pair mode, but also the useful
alternative derivational and in situ replacement modes in a unified window."
"We present a Sage implementation of Ore algebras. The main features for the
most common instances include basic arithmetic and actions; gcrd and lclm;
D-finite closure properties; natural transformations between related algebras;
guessing; desingularization; solvers for polynomials, rational functions and
(generalized) power series. This paper is a tutorial on how to use the package."
"Accurate and comprehensible knowledge about the position of branch cuts is
essential for correctly working with multi-valued functions, such as the square
root and logarithm. We discuss the new tools in Maple 17 for calculating and
visualising the branch cuts of such functions, and others built up from them.
The cuts are described in an intuitive and accurate form, offering substantial
improvement on the descriptions previously available."
"We present a deterministic polynomial-time algorithm which computes the
multilinear factors of multivariate lacunary polynomials over number fields. It
is based on a new Gap theorem which allows to test whether $P(X)=\sum_{j=1}^k
a_j X^{\alpha_j}(vX+t)^{\beta_j}(uX+w)^{\gamma_j}$ is identically zero in
polynomial time. Previous algorithms for this task were based on Gap Theorems
expressed in terms of the height of the coefficients. Our Gap Theorem is based
on the valuation of the polynomial and is valid for any field of characteristic
zero. As a consequence we obtain a faster and more elementary algorithm.
Furthermore, we can partially extend the algorithm to other situations, such as
absolute and approximate factorizations.
  We also give a version of our Gap Theorem valid for fields of large
characteristic, and deduce a randomized polynomial-time algorithm to compute
multilinear factors with at least three monomials of multivariate lacunary
polynomials of finite fields of large characteristic. We provide
$\mathsf{NP}$-hardness results to explain our inability to compute binomial
factors."
"Computer algebra systems are a great help for mathematical research but
sometimes unexpected errors in the software can also badly affect it. As an
example, we show how we have detected an error of Mathematica computing
determinants of matrices of integer numbers: not only it computes the
determinants wrongly, but also it produces different results if one evaluates
the same determinant twice."
"We present a divide-and-conquer version of the Cylindrical Algebraic
Decomposition (CAD) algorithm. The algorithm represents the input as a Boolean
combination of subformulas, computes cylindrical algebraic decompositions of
solution sets of the subformulas, and combines the results. We propose a
graph-based heuristic to find a suitable partitioning of the input and present
empirical comparison with direct CAD computation."
"We propose efficient parallel algorithms and implementations on shared memory
architectures of LU factorization over a finite field. Compared to the
corresponding numerical routines, we have identified three main difficulties
specific to linear algebra over finite fields. First, the arithmetic complexity
could be dominated by modular reductions. Therefore, it is mandatory to delay
as much as possible these reductions while mixing fine-grain parallelizations
of tiled iterative and recursive algorithms. Second, fast linear algebra
variants, e.g., using Strassen-Winograd algorithm, never suffer from
instability and can thus be widely used in cascade with the classical
algorithms. There, trade-offs are to be made between size of blocks well suited
to those fast variants or to load and communication balancing. Third, many
applications over finite fields require the rank profile of the matrix (quite
often rank deficient) rather than the solution to a linear system. It is thus
important to design parallel algorithms that preserve and compute this rank
profile. Moreover, as the rank profile is only discovered during the algorithm,
block size has then to be dynamic. We propose and compare several block
decomposition: tile iterative with left-looking, right-looking and Crout
variants, slab and tile recursive. Experiments demonstrate that the tile
recursive variant performs better and matches the performance of reference
numerical software when no rank deficiency occur. Furthermore, even in the most
heterogeneous case, namely when all pivot blocks are rank deficient, we show
that it is possbile to maintain a high efficiency."
"Let $K$ be a field equipped with a valuation. Tropical varieties over $K$ can
be defined with a theory of Gr\""obner bases taking into account the valuation
of $K$. Because of the use of the valuation, this theory is promising for
stable computations over polynomial rings over a $p$-adic fields.We design a
strategy to compute such tropical Gr\""obner bases by adapting the Matrix-F5
algorithm. Two variants of the Matrix-F5 algorithm, depending on how the
Macaulay matrices are built, are available to tropical computation with
respective modifications. The former is more numerically stable while the
latter is faster.Our study is performed both over any exact field with
valuation and some inexact fields like $\mathbb{Q}\_p$ or $\mathbb{F}\_q
\llbracket t \rrbracket.$ In the latter case, we track the loss in precision,
and show that the numerical stability can compare very favorably to the case of
classical Gr\""obner bases when the valuation is non-trivial. Numerical examples
are provided."
"Let $(f\_1,\dots, f\_s) \in \mathbb{Q}\_p [X\_1,\dots, X\_n]^s$ be a sequence
of homogeneous polynomials with $p$-adic coefficients. Such system may happen,
for example, in arithmetic geometry. Yet, since $\mathbb{Q}\_p$ is not an
effective field, classical algorithm does not apply.We provide a definition for
an approximate Gr{\""o}bner basis with respect to a monomial order $w.$ We
design a strategy to compute such a basis, when precision is enough and under
the assumption that the input sequence is regular and the ideals $\langle
f\_1,\dots,f\_i \rangle$ are weakly-$w$-ideals. The conjecture of Moreno-Socias
states that for the grevlex ordering, such sequences are generic.Two variants
of that strategy are available, depending on whether one lean more on precision
or time-complexity. For the analysis of these algorithms, we study the loss of
precision of the Gauss row-echelon algorithm, and apply it to an adapted
Matrix-F5 algorithm. Numerical examples are provided.Moreover, the fact that
under such hypotheses, Gr{\""o}bner bases can be computed stably has many
applications. Firstly, the mapping sending $(f\_1,\dots,f\_s)$ to the reduced
Gr{\""o}bner basis of the ideal they span is differentiable, and its
differential can be given explicitly. Secondly, these hypotheses allows to
perform lifting on the Grobner bases, from $\mathbb{Z}/p^k \mathbb{Z}$ to
$\mathbb{Z}/p^{k+k'} \mathbb{Z}$ or $\mathbb{Z}.$ Finally, asking for the same
hypotheses on the highest-degree homogeneous components of the entry
polynomials allows to extend our strategy to the affine case."
"The theory of border bases for zero-dimensional ideals has attracted several
researchers in symbolic computation due to their numerical stability and
mathematical elegance. As shown in (Francis & Dukkipati, J. Symb. Comp., 2014),
one can extend the concept of border bases over Noetherian rings whenever the
corresponding residue class ring is finitely generated and free. In this paper
we address the following problem: Can the concept of border basis over
Noetherian rings exists for ideals when the corresponding residue class rings
are finitely generated but need not necessarily be free modules? We present a
border division algorithm and prove the termination of the algorithm for a
special class of border bases. We show the existence of such border bases over
Noetherian rings and present some characterizations in this regard. We also
show that certain reduced Gr\""{o}bner bases over Noetherian rings are contained
in this class of border bases."
"It is our intention here only to discuss the nature, complexity and tools
concerning the design of Smart Help, an expert help facility for aiding users
of Computer Algebra Systems. Although the expert help system presented here has
been particularly oriented to REDUCE (as a consequence of our former experience
with this system), we point out that the concept of Smart Help can be extended
to other Computer Algebra Systems. Technically, Smart Help is a Production
System on the top of a particular implementation of MANTRA, a hybrid knowledge
representation system, which has REDUCE integrated as an additional knowledge
representation module. Since the heuristic level of MANTRA has not yet been
implemented, being presently represented by the Lisp language itself, Smart
Help is coded in Lisp and resides in the same Lisp session of MANTRA. A
prototype of Smart Help is now running on a SUN work-station on an experimental
basis."
"Differential (Ore) type polynomials with approximate polynomial coefficients
are introduced. These provide a useful representation of approximate
differential operators with a strong algebraic structure, which has been used
successfully in the exact, symbolic, setting. We then present an algorithm for
the approximate Greatest Common Right Divisor (GCRD) of two approximate
differential polynomials, which intuitively is the differential operator whose
solutions are those common to the two inputs operators. More formally, given
approximate differential polynomials $f$ and $g$, we show how to find ""nearby""
polynomials $\widetilde f$ and $\widetilde g$ which have a non-trivial GCRD.
Here ""nearby"" is under a suitably defined norm. The algorithm is a
generalization of the SVD-based method of Corless et al. (1995) for the
approximate GCD of regular polynomials. We work on an appropriately
""linearized"" differential Sylvester matrix, to which we apply a block SVD. The
algorithm has been implemented in Maple and a demonstration of its robustness
is presented."
"We study the Isomorphism of Polynomial (IP2S) problem with m=2 homogeneous
quadratic polynomials of n variables over a finite field of odd characteristic:
given two quadratic polynomials (a, b) on n variables, we find two bijective
linear maps (s,t) such that b=t . a . s. We give an algorithm computing s and t
in time complexity O~(n^4) for all instances, and O~(n^3) in a dominant set of
instances.
  The IP2S problem was introduced in cryptography by Patarin back in 1996. The
special case of this problem when t is the identity is called the isomorphism
with one secret (IP1S) problem. Generic algebraic equation solvers (for example
using Gr\""obner bases) solve quite well random instances of the IP1S problem.
For the particular cyclic instances of IP1S, a cubic-time algorithm was later
given and explained in terms of pencils of quadratic forms over all finite
fields; in particular, the cyclic IP1S problem in odd characteristic reduces to
the computation of the square root of a matrix.
  We give here an algorithm solving all cases of the IP1S problem in odd
characteristic using two new tools, the Kronecker form for a singular quadratic
pencil, and the reduction of bilinear forms over a non-commutative algebra.
Finally, we show that the second secret in the IP2S problem may be recovered in
cubic time."
"A wide range of numerical methods exists for computing polynomial
approximations of solutions of ordinary differential equations based on
Chebyshev series expansions or Chebyshev interpolation polynomials. We consider
the application of such methods in the context of rigorous computing (where we
need guarantees on the accuracy of the result), and from the complexity point
of view. It is well-known that the order-n truncation of the Chebyshev
expansion of a function over a given interval is a near-best uniform polynomial
approximation of the function on that interval. In the case of solutions of
linear differential equations with polynomial coefficients, the coefficients of
the expansions obey linear recurrence relations with polynomial coefficients.
Unfortunately, these recurrences do not lend themselves to a direct recursive
computation of the coefficients, owing among other things to a lack of initial
conditions. We show how they can nevertheless be used, as part of a validated
process, to compute good uniform approximations of D-finite functions together
with rigorous error bounds, and we study the complexity of the resulting
algorithms. Our approach is based on a new view of a classical numerical method
going back to Clenshaw, combined with a functional enclosure method."
"This paper summarizes the essential functionality of the computer algebra
package HarmonicSums. On the one hand HarmonicSums can work with nested sums
such as harmonic sums and their generalizations and on the other hand it can
treat iterated integrals of the Poincare and Chen-type, such as harmonic
polylogarithms and their generalizations. The interplay of these
representations and the analytic aspects are illustrated by concrete examples."
"Let $\Delta_x f(x,y)=f(x+1,y)-f(x,y)$ and $\Delta_y f(x,y)=f(x,y+1)-f(x,y)$
be the difference operators with respect to $x$ and $y$. A rational function
$f(x,y)$ is called summable if there exist rational functions $g(x,y)$ and
$h(x,y)$ such that $f(x,y)=\Delta_x g(x,y) + \Delta_y h(x,y)$. Recently, Chen
and Singer presented a method for deciding whether a rational function is
summable. To implement their method in the sense of algorithms, we need to
solve two problems. The first is to determine the shift equivalence of two
bivariate polynomials. We solve this problem by presenting an algorithm for
computing the dispersion sets of any two bivariate polynomials. The second is
to solve a univariate difference equation in an algebraically closed field. By
considering the irreducible factorization of the denominator of $f(x,y)$ in a
general field, we present a new criterion which requires only finding a
rational solution of a bivariate difference equation. This goal can be achieved
by deriving a universal denominator of the rational solutions and a degree
bound on the numerator. Combining these two algorithms, we can decide the
summability of a bivariate rational function."
"Multiplicative order of an element $a$ of group $G$ is the least positive
integer $n$ such that $a^n=e$, where $e$ is the identity element of $G$. If the
order of an element is equal to $|G|$, it is called generator or primitive
root. This paper describes the algorithms for computing multiplicative order
and primitive root in $\mathbb{Z}^*_{p}$, we also present a logarithmic
improvement over classical algorithms."
"We show that Ore operators can be desingularized by calculating a least
common left multiple with a random operator of appropriate order. Our result
generalizes a classical result about apparent singularities of linear
differential equations, and it gives rise to a surprisingly simple
desingularization algorithm."
"We provide bounds on the size of operators obtained by algorithms for
executing D-finite closure properties. For operators of small order, we give
bounds on the degree and on the height (bit-size). For higher order operators,
we give degree bounds that are parameterized with respect to the order and
reflect the phenomenon that higher order operators may have lower degrees
(order-degree curves)."
"In this paper, we draw connections between ideal lattices and multivariate
polynomial rings over integers using Gr\""obner bases. Ideal lattices are ideals
in the residue class ring, $\mathbb{Z}[x]/\langle f \rangle$ (here $f$ is a
monic polynomial), and cryptographic primitives have been built based on these
objects. As ideal lattices in the univariate case are generalizations of cyclic
lattices, we introduce the notion of multivariate cyclic lattices and show that
multivariate ideal lattices are indeed a generalization of them. Based on
multivariate ideal lattices, we establish the existence of collision resistant
hash functions using Gr\""obner basis techniques. For the construction of hash
functions, we define a worst case problem, shortest substitution problem w.r.t.
an ideal in $\mathbb{Z}[x_1,\ldots, x_n]$, and establish hardness results using
functional fields."
"It is still a challenging task of today to recognize the type of a given
algebraic surface which is described only by its implicit representation.
In~this paper we will investigate in more detail the case of canal surfaces
that are often used in geometric modelling, Computer-Aided Design and technical
practice (e.g. as blending surfaces smoothly joining two parts with circular
ends). It is known that if the squared medial axis transform is a rational
curve then so is also the corresponding surface. However, starting from a
polynomial it is not known how to decide if the corresponding algebraic surface
is rational canal surface or not. Our goal is to formulate a simple and
efficient algorithm whose input is a~polynomial with the coefficients from some
subfield of real numbers and the output is the answer whether the surface is a
rational canal surface. In the affirmative case we also compute a rational
parameterization of the squared medial axis transform which can be then used
for finding a rational parameterization of the implicitly given canal surface."
"Given a straight-line program whose output is a polynomial function of the
inputs, we present a new algorithm to compute a concise representation of that
unknown function. Our algorithm can handle any case where the unknown function
is a multivariate polynomial, with coefficients in an arbitrary finite field,
and with a reasonable number of nonzero terms but possibly very large degree.
It is competitive with previously known sparse interpolation algorithms that
work over an arbitrary finite field, and provides an improvement when there are
a large number of variables."
"Let $\A_0, \A_1, \ldots, \A_n$ be given square matrices of size $m$ with
rational coefficients. The paper focuses on the exact computation of one point
in each connected component of the real determinantal variety $\{\X \in\RR^n \:
:\: \det(\A_0+x_1\A_1+\cdots+x_n\A_n)=0\}$. Such a problem finds applications
in many areas such as control theory, computational geometry, optimization,
etc. Using standard complexity results this problem can be solved using
$m^{O(n)}$ arithmetic operations. Under some genericity assumptions on the
coefficients of the matrices, we provide an algorithm solving this problem
whose runtime is essentially quadratic in ${{n+m}\choose{n}}^{3}$. We also
report on experiments with a computer implementation of this algorithm. Its
practical performance illustrates the complexity estimates. In particular, we
emphasize that for subfamilies of this problem where $m$ is fixed, the
complexity is polynomial in $n$."
"We propose a differential analog of the notion of integral closure of
algebraic function fields. We present an algorithm for computing the integral
closure of the algebra defined by a linear differential operator. Our algorithm
is a direct analog of van Hoeij's algorithm for computing integral bases of
algebraic function fields."
"The Abramov-Petkovsek reduction computes an additive decomposition of a
hypergeometric term, which extends the functionality of the Gosper algorithm
for indefinite hypergeometric summation. We modify the Abramov-Petkovsek
reduction so as to decompose a hypergeometric term as the sum of a summable
term and a non-summable one. The outputs of the Abramov-Petkovsek reduction and
our modified version share the same required properties. The modified reduction
does not solve any auxiliary linear difference equation explicitly. It is also
more efficient than the original reduction according to computational
experiments. Based on this reduction, we design a new algorithm to compute
minimal telescopers for bivariate hypergeometric terms. The new algorithm can
avoid the costly computation of certificates."
"We consider existential problems over the reals. Extended quantifier
elimination generalizes the concept of regular quantifier elimination by
providing in addition answers, which are descriptions of possible assignments
for the quantified variables. Implementations of extended quantifier
elimination via virtual substitution have been successfully applied to various
problems in science and engineering. So far, the answers produced by these
implementations included infinitesimal and infinite numbers, which are hard to
interpret in practice. We introduce here a post-processing procedure to
convert, for fixed parameters, all answers into standard real numbers. The
relevance of our procedure is demonstrated by application of our implementation
to various examples from the literature, where it significantly improves the
quality of the results."
"We generalize the framework of virtual substitution for real quantifier
elimination to arbitrary but bounded degrees. We make explicit the
representation of test points in elimination sets using roots of parametric
univariate polynomials described by Thom codes. Our approach follows an early
suggestion by Weispfenning, which has never been carried out explicitly.
Inspired by virtual substitution for linear formulas, we show how to
systematically construct elimination sets containing only test points
representing lower bounds."
"When studying local properties of a polynomial ideal, one usually needs a
theoretic technique called localization. For most cases, in spite of its
importance, the computation in a localized ring cannot be algorithmically
preformed. On the other hand, the standard basis method is very effective for
the computation in a special kind of localized rings, but for a general
semigroup order the geometry of the localization of a positive-dimensional
ideal is difficult to interpret.
  In this paper, we introduce a new ideal operation called extraction. For an
ideal $I$ in a polynomial ring $K[x_1,\ldots,x_n]$ over a field $K$, we use
another ideal $J$ to control the primary components of $I$ and the result
$\beta(I,J)$ is called the extraction of $I$ by $J$. It is still a polynomial
ideal and has a concrete geometric meaning in $\bar{K}^n$, i.e., we keep the
branches of $\textbf{V}(I) \subset \bar{K}^n$ that intersect with
$\textbf{V}(J) \subset \bar{K}^n$ and delete others, where $\bar{K}$ is the
algebraic closure of $K$. This is what we mean by visible. On the other hand,
we can use the standard basis method to compute a localized ideal corresponding
to $\beta(I,J)$ without a complete primary decomposition, and can do further
computation in the localized ring such as determining the membership problem of
$\beta(I,J)$. Moreover, we prove that extractions are as powerful as
localizations in the sense that for any multiplicatively closed subset $S$ of
$K[x_1,\ldots,x_n]$ and any polynomial ideal $I$, there always exists a
polynomial ideal $J$ such that $\beta(I,J)=(S^{-1}I)^c$."
"Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in
machine learning. Automatic differentiation (AD) is a technique for calculating
derivatives of numeric functions expressed as computer programs efficiently and
accurately, used in fields such as computational fluid dynamics, nuclear
engineering, and atmospheric sciences. Despite its advantages and use in other
fields, machine learning practitioners have been little influenced by AD and
make scant use of available tools. We survey the intersection of AD and machine
learning, cover applications where AD has the potential to make a big impact,
and report on some recent developments in the adoption of this technique. We
aim to dispel some misconceptions that we contend have impeded the use of AD
within the machine learning community."
"In this paper, we consider the existence of a factorization of a monic,
bounded motion polynomial. We prove existence of factorizations, possibly after
multiplication with a real polynomial and provide algorithms for computing
polynomial factor and factorizations. The first algorithm is conceptually
simpler but may require a high degree of the polynomial factor. The second
algorithm gives an optimal degree."
"Tropical differential equations are introduced and an algorithm is designed
which tests solvability of a system of tropical linear differential equations
within the complexity polynomial in the size of the system and in its
coefficients. Moreover, we show that there exists a minimal solution, and the
algorithm constructs it (in case of solvability). This extends a similar
complexity bound established for tropical linear systems. In case of tropical
linear differential systems in one variable a polynomial complexity algorithm
for testing its solvability is designed.
  We prove also that the problem of solvability of a system of tropical
non-linear differential equations in one variable is $NP$-hard, and this
problem for arbitrary number of variables belongs to $NP$. Similar to tropical
algebraic equations, a tropical differential equation expresses the (necessary)
condition on the dominant term in the issue of solvability of a differential
equation in power series."
"A polynomial complexity algorithm is designed which tests whether a point
belongs to a given tropical linear variety."
"In the 1930s Tarski showed that real quantifier elimination was possible, and
in 1975 Collins gave a remotely practicable method, albeit with
doubly-exponential complexity, which was later shown to be inherent. We discuss
some of the recent major advances in Collins method: such as an alternative
approach based on passing via the complexes, and advances which come closer to
""solving the question asked"" rather than ""solving all problems to do with these
polynomials""."
"We give two efficient methods to derive Pfaffian systems for A-hypergeometric
systems for the application to the holonomic gradient method for statistics. We
utilize the Hilbert driven Buchberger algorithm and Macaulay type matrices in
the two methods."
"Systems of differential-algebraic equations (DAEs) are generated routinely by
simulation and modeling environments such as Modelica and MapleSim. Before a
simulation starts and a numerical solution method is applied, some kind of
structural analysis is performed to determine the structure and the index of a
DAE. Structural analysis methods serve as a necessary preprocessing stage, and
among them, Pantelides's algorithm is widely used.
  Recently Pryce's $\Sigma$-method is becoming increasingly popular, owing to
its straightforward approach and capability of analyzing high-order systems.
Both methods are equivalent in the sense that when one succeeds, producing a
nonsingular system Jacobian, the other also succeeds, and the two give the same
structural index.
  Although provably successful on fairly many problems of interest, the
structural analysis methods can fail on some simple, solvable DAEs and give
incorrect structural information including the index. In this report, we focus
on the $\Sigma$-method. We investigate its failures, and develop two
symbolic-numeric conversion methods for converting a DAE, on which the
$\Sigma$-method fails, to an equivalent problem on which this method succeeds.
Aimed at making structural analysis methods more reliable, our conversion
methods exploit structural information of a DAE, and require a symbolic tool
for their implementation."
"To interpolate a supersparse polynomial with integer coefficients, two
alternative approaches are the Prony-based ""big prime"" technique, which acts
over a single large finite field, or the more recently-proposed ""small primes""
technique, which reduces the unknown sparse polynomial to many low-degree dense
polynomials. While the latter technique has not yet reached the same
theoretical efficiency as Prony-based methods, it has an obvious potential for
parallelization. We present a heuristic ""small primes"" interpolation algorithm
and report on a low-level C implementation using FLINT and MPI."
"The problem of finding low rank $m\times m$ matrices in a real affine
subspace of dimension n has many applications in information and systems
theory, where low rank is synonymous of structure and parcimony. We design a
symbolic computation algorithm to solve this problem efficiently, exactly and
rigorously: the input are the rational coefficients of the matrices spanning
the affine subspace as well as the expected maximum rank, and the output is a
rational parametrization encoding a finite set of points that intersects each
connected component of the low rank real algebraic set. The complexity of our
algorithm is studied thoroughly. It is essentially polynomial in
binomial(n+m(m--r),n) where r is the expected maximum rank; it improves on the
state-of-the-art in the field. Moreover, computer experiments show the
practical efficiency of our approach."
"Multiple binomial sums form a large class of multi-indexed sequences, closed
under partial summation, which contains most of the sequences obtained by
multiple summation of products of binomial coefficients and also all the
sequences with algebraic generating function. We study the representation of
the generating functions of binomial sums by integrals of rational functions.
The outcome is twofold. Firstly, we show that a univariate sequence is a
multiple binomial sum if and only if its generating function is the diagonal of
a rational function. Secondly, we propose algorithms that decide the equality
of multiple binomial sums and that compute recurrence relations for them. In
conjunction with geometric simplifications of the integral representations,
this approach behaves well in practice. The process avoids the computation of
certificates and the problem of the appearance of spurious singularities that
afflicts discrete creative telescoping, both in theory and in practice."
"In this paper, a polynomial-time algorithm is given to compute the
generalized Hermite normal form for a matrix F over Z[x], or equivalently, the
reduced Groebner basis of the Z[x]-module generated by the column vectors of F.
The algorithm is also shown to be practically more efficient than existing
algorithms. The algorithm is based on three key ingredients. First, an F4 style
algorithm to compute the Groebner basis is adopted, where a novel prolongation
is designed such that the coefficient matrices under consideration have
polynomial sizes. Second, fast algorithms to compute Hermite normal forms of
matrices over Z are used. Third, the complexity of the algorithm are guaranteed
by a nice estimation for the degree and height bounds of the polynomials in the
generalized Hermite normal form."
"In this paper, we solve the existence problem of telescopers for rational
functions in three discrete variables. We reduce the problem to that of
deciding the summability of bivariate rational functions, which has been solved
recently. The existence criteria we present is needed for detecting the
termination of Zeilberger's algorithm to the function classes studied in this
paper."
"With the exception of q-hypergeometric summation, the use of computer algebra
packages implementing Zeilberger's ""holonomic systems approach"" in a broader
mathematical sense is less common in the field of q-series and basic
hypergeometric functions. A major objective of this article is to popularize
the usage of such tools also in these domains. Concrete case studies showing
software in action introduce to the basic techniques. An application highlight
is a new computer-assisted proof of the celebrated Ismail-Zhang formula, an
important q-analog of a classical expansion formula of plane waves in terms of
Gegenbauer polynomials."
"We address the question of computing one selected term of an algebraic power
series. In characteristic zero, the best algorithm currently known for
computing the $N$th coefficient of an algebraic series uses differential
equations and has arithmetic complexity quasi-linear in $\sqrt{N}$. We show
that over a prime field of positive characteristic $p$, the complexity can be
lowered to $O(\log N)$. The mathematical basis for this dramatic improvement is
a classical theorem stating that a formal power series with coefficients in a
finite field is algebraic if and only if the sequence of its coefficients can
be generated by an automaton. We revisit and enhance two constructive proofs of
this result for finite prime fields. The first proof uses Mahler equations,
whose sizes appear to be prohibitively large. The second proof relies on
diagonals of rational functions; we turn it into an efficient algorithm, of
complexity linear in $\log N$ and quasi-linear in $p$."
"Nowadays, many strategies to solve polynomial systems use the computation of
a Gr{\""o}bner basis for the graded reverse lexicographical ordering, followed
by a change of ordering algorithm to obtain a Gr{\""o}bner basis for the
lexicographical ordering. The change of ordering algorithm is crucial for these
strategies. We study the p-adic stability of the main change of ordering
algorithm, FGLM. We show that FGLM is stable and give explicit upper bound on
the loss of precision occuring in its execution. The variant of FGLM designed
to pass from the grevlex ordering to a Gr{\""o}bner basis in shape position is
also stable. Our study relies on the application of Smith Normal Form
computations for linear algebra."
"In the convergence analysis of numerical methods for solving partial
differential equations (such as finite element methods) one arrives at certain
generalized eigenvalue problems, whose maximal eigenvalues need to be estimated
as accurately as possible. We apply symbolic computation methods to the
situation of square elements and are able to improve the previously known upper
bound, given in ""p- and hp-finite element methods"" (Schwab, 1998), by a factor
of 8. More precisely, we try to evaluate the corresponding determinant using
the holonomic ansatz, which is a powerful tool for dealing with determinants,
proposed by Zeilberger in 2007. However, it turns out that this method does not
succeed on the problem at hand. As a solution we present a variation of the
original holonomic ansatz that is applicable to a larger class of determinants,
including the one we are dealing with here. We obtain an explicit closed form
for the determinant, whose special form enables us to derive new and tight
upper resp. lower bounds on the maximal eigenvalue, as well as its asymptotic
behaviour."
"In this paper, binomial difference ideals are studied. Three canonical
representations for Laurent binomial difference ideals are given in terms of
the reduced Groebner basis of Z[x]-lattices, regular and coherent difference
ascending chains, and partial characters over Z[x]-lattices, respectively.
Criteria for a Laurent binomial difference ideal to be reflexive, prime,
well-mixed, and perfect are given in terms of their support lattices. The
reflexive, well-mixed, and perfect closures of a Laurent binomial difference
ideal are shown to be binomial. Most of the properties of Laurent binomial
difference ideals are extended to the case of difference binomial ideals.
Finally, algorithms are given to check whether a given Laurent binomial
difference ideal I is reflexive, prime, well-mixed, or perfect, and in the
negative case, to compute the reflexive, well-mixed, and perfect closures of I.
An algorithm is given to decompose a finitely generated perfect binomial
difference ideal as the intersection of reflexive prime binomial difference
ideals."
"In this paper, the concept of toric difference varieties is defined and four
equivalent descriptions for toric difference varieties are presented in terms
of difference rational parametrization, difference coordinate rings, toric
difference ideals, and group actions by difference tori. Connections between
toric difference varieties and affine N[x]-semimodules are established by
proving the correspondence between the irreducible invariant difference
subvarieties and the faces of the N[x]-submodules and the orbit-face
correspondence. Finally, an algorithm is given to decide whether a binomial
difference ideal represented by a Z[x]-lattice defines a toric difference
variety."
"Based on a modified version of Abramov-Petkov\v{s}ek reduction, a new
algorithm to compute minimal telescopers for bivariate hypergeometric terms was
developed last year. We investigate further in this paper and present a new
argument for the termination of this algorithm, which provides an independent
proof of the existence of telescopers and even enables us to derive lower as
well as upper bounds for the order of telescopers for hypergeometric terms.
Compared to the known bounds in the literature, our bounds are sometimes
better, and never worse than the known ones."
"Given a zero-dimensional polynomial system consisting of n integer
polynomials in n variables, we propose a certified and complete method to
compute all complex solutions of the system as well as a corresponding
separating linear form l with coefficients of small bit size. For computing l,
we need to project the solutions into one dimension along O(n) distinct
directions but no further algebraic manipulations. The solutions are then
directly reconstructed from the considered projections. The first step is
deterministic, whereas the second step uses randomization, thus being
Las-Vegas.
  The theoretical analysis of our approach shows that the overall cost for the
two problems considered above is dominated by the cost of carrying out the
projections. We also give bounds on the bit complexity of our algorithms that
are exclusively stated in terms of the number of variables, the total degree
and the bitsize of the input polynomials."
"Analytic combinatorics studies the asymptotic behaviour of sequences through
the analytic properties of their generating functions. This article provides
effective algorithms required for the study of analytic combinatorics in
several variables, together with their complexity analyses. Given a
multivariate rational function we show how to compute its smooth isolated
critical points, with respect to a polynomial map encoding asymptotic
behaviour, in complexity singly exponential in the degree of its denominator.
We introduce a numerical Kronecker representation for solutions of polynomial
systems with rational coefficients and show that it can be used to decide
several properties (0 coordinate, equal coordinates, sign conditions for real
solutions, and vanishing of a polynomial) in good bit complexity. Among the
critical points, those that are minimal---a property governed by inequalities
on the moduli of the coordinates---typically determine the dominant asymptotics
of the diagonal coefficient sequence. When the Taylor expansion at the origin
has all non-negative coefficients (known as the `combinatorial case') and under
regularity conditions, we utilize this Kronecker representation to determine
probabilistically the minimal critical points in complexity singly exponential
in the degree of the denominator, with good control over the exponent in the
bit complexity estimate. Generically in the combinatorial case, this allows one
to automatically and rigorously determine asymptotics for the diagonal
coefficient sequence. Examples obtained with a preliminary implementation show
the wide applicability of this approach."
"We present a method for solving polynomial equations over idempotent
omega-continuous semirings. The idea is to iterate over the semiring of
functions rather than the semiring of interest, and only evaluate when needed.
The key operation is substitution. In the initial step, we compute a linear
completion of the system of equations that exhaustively inserts the equations
into one another. With functions as approximants, the following steps insert
the current approximant into itself. Since the iteration improves its precision
by substitution rather than computation we named it Munchausen, after the
fictional baron that pulled himself out of a swamp by his own hair. The first
result shows that an evaluation of the n-th Munchausen approximant coincides
with the 2^n-th Newton approximant. Second, we show how to compute linear
completions with standard techniques from automata theory. In particular, we
are not bound to (but can use) the notion of differentials prominent in Newton
iteration."
"For a finite separable field extension K/k, all subfields can be obtained by
intersecting so-called principal subfields of K/k. In this work we present a
way to quickly compute these intersections. If the number of subfields is high,
then this leads to faster run times and an improved complexity."
"We present two algorithms for computing hypergeometric solutions of second
order linear differential operators with rational function coefficients. Our
first algorithm searches for solutions of the form \[ \exp(\int r \,
dx)\cdot{_{2}F_1}(a_1,a_2;b_1;f) \] where $r,f \in \overline{\mathbb{Q}(x)}$,
and $a_1,a_2,b_1 \in \mathbb{Q}$. It uses modular reduction and Hensel lifting.
Our second algorithm tries to find solutions in the form \[ \exp(\int r \,
dx)\cdot \left( r_0 \cdot{_{2}F_1}(a_1,a_2;b_1;f) + r_1
\cdot{_{2}F_1}'(a_1,a_2;b_1;f) \right) \] where $r_0, r_1 \in
\overline{\mathbb{Q}(x)}$, as follows: It tries to transform the input equation
to another equation with solutions of the first type, and then uses the first
algorithm."
"Symbolic Computation and Satisfiability Checking are viewed as individual
research areas, but they share common interests in the development,
implementation and application of decision procedures for arithmetic theories.
Despite these commonalities, the two communities are currently only weakly
connected. We introduce a new project SC-square to build a joint community in
this area, supported by a newly accepted EU (H2020-FETOPEN-CSA) project of the
same name. We aim to strengthen the connection between these communities by
creating common platforms, initiating interaction and exchange, identifying
common challenges, and developing a common roadmap. This abstract and
accompanying poster describes the motivation and aims for the project, and
reports on the first activities."
"Symbolic Computation and Satisfiability Checking are two research areas, both
having their individual scientific focus but sharing also common interests in
the development, implementation and application of decision procedures for
arithmetic theories. Despite their commonalities, the two communities are
rather weakly connected. The aim of our newly accepted SC-square project
(H2020-FETOPEN-CSA) is to strengthen the connection between these communities
by creating common platforms, initiating interaction and exchange, identifying
common challenges, and developing a common roadmap from theory along the way to
tools and (industrial) applications. In this paper we report on the aims and on
the first activities of this project, and formalise some relevant challenges
for the unified SC-square community."
"In this paper we report on an application of computer algebra in which
mathematical puzzles are generated of a type that had been widely used in
mathematics contests by a large number of participants worldwide.
  The algorithmic aspect of our work provides a method to compute rational
solutions of single polynomial equations that are typically large with $10^2
\ldots 10^5$ terms and that are heavily underdetermined. This functionality was
obtained by adding modules for a new type of splitting of equations to the
existing package CRACK that is normally used to solve polynomial algebraic and
differential systems."
"Cylindrical Algebraic Decomposition (CAD) is a key tool in computational
algebraic geometry, particularly for quantifier elimination over real-closed
fields. However, it can be expensive, with worst case complexity doubly
exponential in the size of the input. Hence it is important to formulate the
problem in the best manner for the CAD algorithm. One possibility is to
precondition the input polynomials using Groebner Basis (GB) theory. Previous
experiments have shown that while this can often be very beneficial to the CAD
algorithm, for some problems it can significantly worsen the CAD performance.
  In the present paper we investigate whether machine learning, specifically a
support vector machine (SVM), may be used to identify those CAD problems which
benefit from GB preconditioning. We run experiments with over 1000 problems
(many times larger than previous studies) and find that the machine learned
choice does better than the human-made heuristic."
"Differential-algebraic equation systems (DAEs) are generated routinely by
simulation and modeling environments. Before a simulation starts and a
numerical method is applied, some kind of structural analysis (SA) is used to
determine which equations to be differentiated, and how many times. Both
Pantelides's algorithm and Pryce's $\Sigma$-method are equivalent: if one of
them finds correct structural information, the other does also. Nonsingularity
of the Jacobian produced by SA indicates a success, which occurs on many
problems of interest. However, these methods can fail on simple, solvable DAEs
and give incorrect structural information including the index. This article
investigates $\Sigma$-method's failures and presents two conversion methods for
fixing them. Both methods convert a DAE on which the $\Sigma$-method fails to
an equivalent problem on which this SA is more likely to succeed."
"In a previous article, the authors developed two conversion methods to
improve the $\Sigma$-method for structural analysis (SA) of
differential-algebraic equations (DAEs). These methods reformulate a DAE on
which the $\Sigma$-method fails into an equivalent problem on which this SA is
more likely to succeed with a generically nonsingular Jacobian. The basic
version of these methods processes the DAE as a whole. This article presents
the block version that exploits block triangularization of a DAE. Using a block
triangular form of a Jacobian sparsity pattern, we identify which diagonal
blocks of the Jacobian are identically singular and then perform a conversion
on each such block. This approach improves the efficiency of finding a suitable
conversion for fixing SA's failures. All of our conversion methods can be
implemented in a computer algebra system so that every conversion can be
automated."
"In the paper which inspired the SC-Square project, [E. Abraham, Building
Bridges between Symbolic Computation and Satisfiability Checking, Proc. ISSAC
'15, pp. 1-6, ACM, 2015] the author identified the use of sophisticated
heuristics as a technique that the Satisfiability Checking community excels in
and from which it is likely the Symbolic Computation community could learn and
prosper. To start this learning process we summarise our experience with
heuristic development for the computer algebra algorithm Cylindrical Algebraic
Decomposition. We also propose and discuss standards and benchmarks as another
area where Symbolic Computation could prosper from Satisfiability Checking
expertise, noting that these have been identified as initial actions for the
new SC-Square community in the CSA project, as described in [E.~Abraham et al.,
SC$^2$: Satisfiability Checking meets Symbolic Computation (Project Paper)},
Intelligent Computer Mathematics (LNCS 9761), pp. 28--43, Springer, 2015]."
"We provide an illustrative implementation of an analytic,
infinitely-differentiable virtual machine, implementing
infinitely-differentiable programming spaces and operators acting upon them, as
constructed in the paper Operational calculus on programming spaces.
Implementation closely follows theorems and derivations of the paper, intended
as an educational guide for those transitioning from automatic differentiation
to this general theory."
"We propose a new algorithm for multiplying dense polynomials with integer
coefficients in a parallel fashion, targeting multi-core processor
architectures. Complexity estimates and experimental comparisons demonstrate
the advantages of this new approach."
"Differential (Ore) type polynomials with ""approximate"" polynomial
coefficients are introduced. These provide an effective notion of approximate
differential operators, with a strong algebraic structure. We introduce the
approximate Greatest Common Right Divisor Problem (GCRD) of differential
polynomials, as a non-commutative generalization of the well-studied
approximate GCD problem.
  Given two differential polynomials, we present an algorithm to find nearby
differential polynomials with a non-trivial GCRD, where nearby is defined with
respect to a suitable coefficient norm. Intuitively, given two linear
differential polynomials as input, the (approximate) GCRD problem corresponds
to finding the (approximate) differential polynomial whose solution space is
the intersection of the solution spaces of the two inputs.
  The approximate GCRD problem is proven to be locally well-posed. A method
based on the singular value decomposition of a differential Sylvester matrix is
developed to produce an initial approximation of the GCRD. With a sufficiently
good initial approximation, Newton iteration is shown to converge quadratically
to an optimal solution. Finally, sufficient conditions for existence of a
solution to the global problem are presented along with examples demonstrating
that no solution exists when these conditions are not satisfied."
"This paper proposes a totally constructive approach for the proof of
Hilbert's theorem on ternary quartic forms. The main contribution is the ladder
technique, with which the Hilbert's theorem is proved vividly."
"We describe an algorithm for fast multiplication of skew polynomials. It is
based on fast modular multiplication of such skew polynomials, for which we
give an algorithm relying on evaluation and interpolation on normal bases. Our
algorithms improve the best known complexity for these problems, and reach the
optimal asymptotic complexity bound for large degree. We also give an
adaptation of our algorithm for polynomials of small degree. Finally, we use
our methods to improve on the best known complexities for various arithmetics
problems."
"The Butler-Portugal algorithm for obtaining the canonical form of a tensor
expression with respect to slot symmetries and dummy-index renaming suffers, in
certain cases with a high degree of symmetry, from $O(n!)$ explosion in both
computation time and memory. We present a modified algorithm which alleviates
this problem in the most common cases---tensor expressions with subsets of
indices which are totally symmetric or totally antisymmetric---in polynomial
time. We also present an implementation of the label-renaming mechanism which
improves upon that of the original Butler-Portugal algorithm, thus providing a
significant speed increase for the average case as well as the highly-symmetric
special case. The worst-case behavior remains $O(n!)$, although it occurs in
more limited situations unlikely to appear in actual computations. We comment
on possible strategies to take if the nature of a computation should make these
situations more likely."
"We present new algorithms for computing the low n bits or the high n bits of
the product of two n-bit integers. We show that these problems may be solved in
asymptotically 75% of the time required to compute the full 2n-bit product,
assuming that the underlying integer multiplication algorithm relies on
computing cyclic convolutions of real sequences."
"For matrices with displacement structure, basic operations like
multiplication, inversion, and linear system solving can all be expressed in
terms of the following task: evaluate the product $\mathsf{A}\mathsf{B}$, where
$\mathsf{A}$ is a structured $n \times n$ matrix of displacement rank $\alpha$,
and $\mathsf{B}$ is an arbitrary $n\times\alpha$ matrix. Given $\mathsf{B}$ and
a so-called ""generator"" of $\mathsf{A}$, this product is classically computed
with a cost ranging from $O(\alpha^2 \mathscr{M}(n))$ to $O(\alpha^2
\mathscr{M}(n)\log(n))$ arithmetic operations, depending on the type of
structure of $\mathsf{A}$; here, $\mathscr{M}$ is a cost function for
polynomial multiplication. In this paper, we first generalize classical
displacement operators, based on block diagonal matrices with companion
diagonal blocks, and then design fast algorithms to perform the task above for
this extended class of structured matrices. The cost of these algorithms ranges
from $O(\alpha^{\omega-1} \mathscr{M}(n))$ to $O(\alpha^{\omega-1}
\mathscr{M}(n)\log(n))$, with $\omega$ such that two $n \times n$ matrices over
a field can be multiplied using $O(n^\omega)$ field operations. By combining
this result with classical randomized regularization techniques, we obtain
faster Las Vegas algorithms for structured inversion and linear system solving."
"The problem to decide whether a given multivariate (quasi-)rational function
has only positive coefficients in its power series expansion has a long
history. It dates back to Szego in 1933 who showed certain quasi-rational
function to be positive, in the sense that all the series coefficients are
positive, using an involved theory of special functions. In contrast to the
simplicity of the statement, the method was surprisingly difficult. This
dependency motivated further research for positivity of (quasi-)rational
functions. More and more (quasi-)rational functions have been proven to be
positive, and some of the proofs are even quite simple. However, there are also
others whose positivity are still open conjectures. In this talk, we focus on a
less difficult but also interesting question to decide whether the diagonal of
a given quasi-rational function is ultimately positive, especially for the one
conjectured to be positive by Kauers in 2007. To solve this question, it
suffices to compute the asymptotics of the diagonal coefficients, which can be
done by the multivariate singularity analysis developed by Baryshnikov,
Pemantle and Wilson. Note that the ultimate positivity is a necessary condition
for the positivity, and therefore can be used to either exclude the nonpositive
cases or further support the conjectural positivity."
"We present an algorithm for computation of cell adjacencies for well-based
cylindrical algebraic decomposition. Cell adjacency information can be used to
compute topological operations e.g. closure, boundary, connected components,
and topological properties e.g. homology groups. Other applications include
visualization and path planning. Our algorithm determines cell adjacency
information using validated numerical methods similar to those used in CAD
construction, thus computing CAD with adjacency information in time comparable
to that of computing CAD without adjacency information. We report on
implementation of the algorithm and present empirical data."
"Let K be a field equipped with a valuation. Tropical varieties over K can be
defined with a theory of Gr{\""o}bner bases taking into account the valuation of
K. While generalizing the classical theory of Gr{\""o}bner bases, it is not
clear how modern algorithms for computing Gr{\""o}bner bases can be adapted to
the tropical case. Among them, one of the most efficient is the celebrated F5
Algorithm of Faug{\`e}re. In this article, we prove that, for homogeneous
ideals, it can be adapted to the tropical case. We prove termination and
correctness. Because of the use of the valuation, the theory of tropical
Gr{\""o}b-ner bases is promising for stable computations over polynomial rings
over a p-adic field. We provide numerical examples to illustrate
time-complexity and p-adic stability of this tropical F5 algorithm."
"An improved multi-summation approach is introduced and discussed that enables
one to simultaneously handle indefinite nested sums and products in the setting
of difference rings and holonomic sequences. Relevant mathematics is reviewed
and the underlying advanced difference ring machinery is elaborated upon. The
flexibility of this new toolbox contributed substantially to evaluating
complicated multi-sums coming from particle physics. Illustrative examples of
the functionality of the new software package RhoSum are given."
"This document explains how to create or modify an existing LATEX document
with commands enabling computations in the HTML5 output: when the reader opens
the HTML5 output, he can run a computation in his browser, or modify the
command to be executed and run it. This is done by combining different
softwares: hevea for compilation to HTML5, giac.js for the CAS computing kernel
(itself compiled from the C++ Giac library with emscripten), and a modified
version of itex2MML for fast and nice rendering in MathML in browsers that
support MathML."
"The celebrated integer relation finding algorithm PSLQ has been successfully
used in many applications. However, the PSLQ was only analyzed theoretically
for exact input. When the input data are irrational numbers, they must be
approximate ones due to finite precision in computer. That is, when the
algorithm takes empirical data (inexact data with error bounded) instead of
exact real numbers as its input, how do we ensure theoretically the output of
the algorithm to be an exact integer relation? In this paper, we investigate
the PSLQ algorithm for empirical data as its input. First, we give a
termination condition for this case. Secondly we analyze a perturbation on the
hyperplane matrix constructed from the input data and hence disclose a
relationship between the accuracy of the input data and the output quality (an
upper bound on the absolute value of the inner product of the exact data and
the computed integer relation). Further, we also analyze the computational
complexity for PSLQ with empirical data. Examples on transcendental numbers and
algebraic numbers show the meaningfulness of our error control strategies."
"Using a new definition of generalized divisors we prove that the lattice of
such divisors for a given linear partial differential operator is modular and
obtain analogues of the well-known theorems of the Loewy-Ore theory of
factorization of linear ordinary differential operators. Possible applications
to factorized Groebner bases computations in the commutative and
non-commutative cases are discussed, an application to finding criterions of
Darboux integrability of nonlinear PDEs is given."
"The traditional split-up into a low level language and a high level language
in the design of computer algebra systems may become obsolete with the advent
of more versatile computer languages. We describe GiNaC, a special-purpose
system that deliberately denies the need for such a distinction. It is entirely
written in C++ and the user can interact with it directly in that language. It
was designed to provide efficient handling of multivariate polynomials,
algebras and special functions that are needed for loop calculations in
theoretical quantum field theory. It also bears some potential to become a more
general purpose symbolic package."
"This thesis is devoted to the study of a calculus that describes the
application of conditional rewriting rules and the obtained results at the same
level of representation. We introduce the rewriting calculus, also called the
rho-calculus, which generalizes the first order term rewriting and
lambda-calculus, and makes possible the representation of the non-determinism.
In our approach the abstraction operator as well as the application operator
are objects of calculus. The result of a reduction in the rewriting calculus is
either an empty set representing the application failure, or a singleton
representing a deterministic result, or a set having several elements
representing a not-deterministic choice of results.
  In this thesis we concentrate on the properties of the rewriting calculus
where a syntactic matching is used in order to bind the variables to their
current values. We define evaluation strategies ensuring the confluence of the
calculus and we show that these strategies become trivial for restrictions of
the general rewriting calculus to simpler calculi like the lambda-calculus. The
rewriting calculus is not terminating in the untyped case but the strong
normalization is obtained for the simply typed calculus.
  In the rewriting calculus extended with an operator allowing to test the
application failure we define terms representing innermost and outermost
normalizations with respect to a set of rewriting rules. By using these terms,
we obtain a natural and concise description of the conditional rewriting.
Finally, starting from the representation of the conditional rewriting rules,
we show how the rewriting calculus can be used to give a semantics to ELAN, a
language based on the application of rewriting rules controlled by strategies."
"These lectures give a brief introduction to the Computer Algebra systems
Reduce and Maple. The aim is to provide a systematic survey of most important
commands and concepts. In particular, this includes a discussion of
simplification schemes and the handling of simplification and substitution
rules (e.g., a Lie Algebra is implemented in Reduce by means of simplification
rules).
  Another emphasis is on the different implementations of tensor calculi and
the exterior calculus by Reduce and Maple and their application in Gravitation
theory and Differential Geometry.
  I held the lectures at the Universidad Autonoma Metropolitana-Iztapalapa,
Departamento de Fisica, Mexico, in November 1999."
"GNU TeXmacs is a free wysiwyg word processor providing an excellent
typesetting quality of texts and formulae. It can also be used as an interface
to Computer Algebra Systems (CASs). In the present work, interfaces to three
general-purpose CASs have been implemented."
"The paper compares computational aspects of four approaches to compute
conservation laws of single differential equations (DEs) or systems of them,
ODEs and PDEs. The only restriction, required by two of the four corresponding
computer algebra programs, is that each DE has to be solvable for a leading
derivative. Extra constraints for the conservation laws can be specified.
Examples include new conservation laws that are non-polynomial in the
functions, that have an explicit variable dependence and families of
conservation laws involving arbitrary functions. The following equations are
investigated in examples: Ito, Liouville, Burgers, Kadomtsev-Petviashvili,
Karney-Sen-Chu-Verheest, Boussinesq, Tzetzeica, Benney."
"Given a quadratic map Q : K^n -> K^k defined over a computable subring D of a
real closed field K, and a polynomial p(Y_1,...,Y_k) of degree d, we consider
the zero set Z=Z(p(Q(X)),K^n) of the polynomial p(Q(X_1,...,X_n)). We present a
procedure that computes, in (dn)^O(k) arithmetic operations in D, a set S of
(real univariate representations of) sampling points in K^n that intersects
nontrivially each connected component of Z. As soon as k=o(n), this is faster
than the standard methods that all have exponential dependence on n in the
complexity. In particular, our procedure is polynomial-time for constant k. In
contrast, the best previously known procedure (due to A.Barvinok) is only
capable of deciding in n^O(k^2) operations the nonemptiness (rather than
constructing sampling points) of the set Z in the case of p(Y)=sum_i Y_i^2 and
homogeneous Q.
  A by-product of our procedure is a bound (dn)^O(k) on the number of connected
components of Z.
  The procedure consists of exact symbolic computations in D and outputs
vectors of algebraic numbers. It involves extending K by infinitesimals and
subsequent limit computation by a novel procedure that utilizes knowledge of an
explicit isomorphism between real algebraic sets."
"After an introduction to the sequential version of FORM and the mechanisms
behind, we report on the status of our project of parallelization. We have now
a parallel version of FORM running on Cluster- and SMP-architectures. This
version can be used to run arbitrary FORM programs in parallel."
"We examine two associative products over the ring of symmetric functions
related to the intransitive and Cartesian products of permutation groups. As an
application, we give an enumeration of some Feynman type diagrams arising in
Bender's QFT of partitions. We end by exploring possibilities to construct
noncommutative analogues."
"We give a new procedure for generalized factorization and construction of the
complete solution of strictly hyperbolic linear partial differential equations
or strictly hyperbolic systems of such equations in the plane. This procedure
generalizes the classical theory of Laplace transformations of second-order
equations in the plane."
"In this paper we propose a new symbolic-numeric algorithm to find positive
equilibria of a n-dimensional dynamical system. This algorithm implies a
symbolic manipulation of ODE in order to give a local approximation of
differential equations with power-law dynamics (S-systems). A numerical
calculus is then needed to converge towards an equilibrium, giving at the same
time a S-system approximating the initial system around this equilibrium. This
algorithm is applied to a real biological example in 14 dimensions which is a
subsystem of a metabolic pathway in Arabidopsis Thaliana."
"This tutorial presents features of the new and improved TeXmacs-maxima
interface. It is designed for running maxima-5.9.2 from TeXmacs-1.0.5 (or
later)."
"We present algorithmic, complexity and implementation results concerning real
root isolation of integer univariate polynomials using the continued fraction
expansion of real algebraic numbers. One motivation is to explain the method's
good performance in practice. We improve the previously known bound by a factor
of $d \tau$, where $d$ is the polynomial degree and $\tau$ bounds the
coefficient bitsize, thus matching the current record complexity for real root
isolation by exact methods. Namely, the complexity bound is $\sOB(d^4 \tau^2)$
using the standard bound on the expected bitsize of the integers in the
continued fraction expansion. We show how to compute the multiplicities within
the same complexity and extend the algorithm to non square-free polynomials.
Finally, we present an efficient open-source \texttt{C++} implementation in the
algebraic library \synaps, and illustrate its efficiency as compared to other
available software. We use polynomials with coefficient bitsize up to 8000 and
degree up to 1000."
"Field theory is an area in physics with a deceptively compact notation.
Although general purpose computer algebra systems, built around generic
list-based data structures, can be used to represent and manipulate
field-theory expressions, this often leads to cumbersome input formats,
unexpected side-effects, or the need for a lot of special-purpose code. This
makes a direct translation of problems from paper to computer and back
needlessly time-consuming and error-prone. A prototype computer algebra system
is presented which features TeX-like input, graph data structures, lists with
Young-tableaux symmetries and a multiple-inheritance property system. The
usefulness of this approach is illustrated with a number of explicit
field-theory problems."
"In this paper, the result of applying iterative univariate resultant
constructions to multivariate polynomials is analyzed. We consider the input
polynomials as generic polynomials of a given degree and exhibit explicit
decompositions into irreducible factors of several constructions involving two
times iterated univariate resultants and discriminants over the integer
universal ring of coefficients of the entry polynomials. Cases involving from
two to four generic polynomials and resultants or discriminants in one of their
variables are treated. The decompositions into irreducible factors we get are
obtained by exploiting fundamental properties of the univariate resultants and
discriminants and induction on the degree of the polynomials. As a consequence,
each irreducible factor can be separately and explicitly computed in terms of a
certain multivariate resultant. With this approach, we also obtain as direct
corollaries some results conjectured by Collins and McCallum which correspond
to the case of polynomials whose coefficients are themselves generic
polynomials in other variables. Finally, a geometric interpretation of the
algebraic factorization of the iterated discriminant of a single polynomial is
detailled."
"The Invar package is introduced, a fast manipulator of generic scalar
polynomial expressions formed from the Riemann tensor of a four-dimensional
metric-compatible connection. The package can maximally simplify any polynomial
containing tensor products of up to seven Riemann tensors within seconds. It
has been implemented both in Mathematica and Maple algebraic systems."
"The software TrIm offers implementations of tropical implicitization and
tropical elimination, as developed by Tevelev and the authors. Given a
polynomial map with generic coefficients, TrIm computes the tropical variety of
the image. When the image is a hypersurface, the output is the Newton polytope
of the defining polynomial. TrIm can thus be used to compute mixed fiber
polytopes, including secondary polytopes."
"Here, we present a family of time series with a simple growth constraint.
This family can be the basis of a model to apply to emerging computation in
business and micro-economy where global functions can be expressed from local
rules. We explicit a double statistics on these series which allows to
establish a one-to-one correspondence between three other ballot-like
strunctures."
"The long standing problem of the relations among the scalar invariants of the
Riemann tensor is computationally solved for all 6x10^23 objects with up to 12
derivatives of the metric. This covers cases ranging from products of up to 6
undifferentiated Riemann tensors to cases with up to 10 covariant derivatives
of a single Riemann. We extend our computer algebra system Invar to produce
within seconds a canonical form for any of those objects in terms of a basis.
The process is as follows: (1) an invariant is converted in real time into a
canonical form with respect to the permutation symmetries of the Riemann
tensor; (2) Invar reads a database of more than 6x10^5 relations and applies
those coming from the cyclic symmetry of the Riemann tensor; (3) then applies
the relations coming from the Bianchi identity, (4) the relations coming from
commutations of covariant derivatives, (5) the dimensionally-dependent
identities for dimension 4, and finally (6) simplifies invariants that can be
expressed as product of dual invariants. Invar runs on top of the tensor
computer algebra systems xTensor (for Mathematica) and Canon (for Maple)."
"We present a very fast implementation of the Butler-Portugal algorithm for
index canonicalization with respect to permutation symmetries. It is called
xPerm, and has been written as a combination of a Mathematica package and a C
subroutine. The latter performs the most demanding parts of the computations
and can be linked from any other program or computer algebra system. We
demonstrate with tests and timings the effectively polynomial performance of
the Butler-Portugal algorithm with respect to the number of indices, though we
also show a case in which it is exponential. Our implementation handles generic
tensorial expressions with several dozen indices in hundredths of a second, or
one hundred indices in a few seconds, clearly outperforming all other current
canonicalizers. The code has been already under intensive testing for several
years and has been essential in recent investigations in large-scale tensor
computer algebra."
"We use the remark that, through Bargmann-Fock representation, diagonal
operators of the Heisenberg-Weyl algebra are scalars for the Hadamard product
to give some properties (like the stability of periodic fonctions) of the
Hadamard product by a rational fraction. In particular, we provide through this
way explicit formulas for the multiplication table of the Hadamard product in
the algebra of rational functions in $\C[[z]]$."
"Given a ""black box"" function to evaluate an unknown rational polynomial f in
Q[x] at points modulo a prime p, we exhibit algorithms to compute the
representation of the polynomial in the sparsest shifted power basis. That is,
we determine the sparsity t, the shift s (a rational), the exponents 0 <= e1 <
e2 < ... < et, and the coefficients c1,...,ct in Q\{0} such that f(x) =
c1(x-s)^e1+c2(x-s)^e2+...+ct(x-s)^et. The computed sparsity t is absolutely
minimal over any shifted power basis. The novelty of our algorithm is that the
complexity is polynomial in the (sparse) representation size, and in particular
is logarithmic in deg(f). Our method combines previous celebrated results on
sparse interpolation and computing sparsest shifts, and provides a way to
handle polynomials with extremely high degree which are, in some sense, sparse
in information."
"We point out four problems which have arisen during the recent research in
the domain of Combinatorial Physics."
"In this paper the acoustic field propagating in the early hot ($p=\epsilon/$)
universe of arbitrary space curvature ($K=0, \pm 1$) is considered. The field
equations are reduced to the d'Alembert equation in an auxiliary static
Roberson-Walker space-time. Symbolic computation in {\em Mathematica} is
applied."
"We present an implementation of the algorithm for computing Groebner bases
for operads due to the first author and A. Khoroshkin. We discuss the actual
algorithms, the choices made for the implementation platform and the data
representation, and strengths and weaknesses of our approach."
"Finite-state tree automata are a well studied formalism for representing term
languages. This paper studies the problem of determining the regularity of the
set of instances of a finite set of terms with variables, where each variable
is restricted to instantiations of a regular set given by a tree automaton. The
problem was recently proved decidable, but with an unknown complexity. Here,
the exact complexity of the problem is determined by proving
EXPTIME-completeness. The main contribution is a new, exponential time
algorithm that performs various exponential transformations on the involved
terms and tree automata, and decides regularity by analyzing formulas over
inequality and height predicates."
"We observe that the vocabulary used to construct the ""answer"" to problems in
computer algebra can have a dramatic effect on the computational complexity of
solving that problem. We recall a formalization of this observation and explain
the classic example of sparse polynomial arithmetic. For this case, we show
that it is possible to extend the vocabulary so as reap the benefits of
conciseness whilst avoiding the obvious pitfall of repeating the problem
statement as the ""solution"".
  It is possible to extend the vocabulary either by irreducible cyclotomics or
by $x^n-1$: we look at the options and suggest that the pragmatist might opt
for both."
"Regular chains and triangular decompositions are fundamental and
well-developed tools for describing the complex solutions of polynomial
systems. This paper proposes adaptations of these tools focusing on solutions
of the real analogue: semi-algebraic systems. We show that any such system can
be decomposed into finitely many {\em regular semi-algebraic systems}. We
propose two specifications of such a decomposition and present corresponding
algorithms. Under some assumptions, one type of decomposition can be computed
in singly exponential time w.r.t.\ the number of variables. We implement our
algorithms and the experimental results illustrate their effectiveness."
"We present in this paper an evolution of a tool from a user interface for a
concrete Computer Algebra system for Algebraic Topology (the Kenzo system), to
a front-end allowing the interoperability among different sources for
computation and deduction. The architecture allows the system not only to
interface several systems, but also to make them cooperate in shared
calculations."
"We give a new algorithm to find local maximum and minimum of a holonomic
function and apply it for the Fisher-Bingham integral on the sphere $S^n$,
which is used in the directional statistics. The method utilizes the theory and
algorithms of holonomic systems."
"We consider the problem of finding a sparse multiple of a polynomial. Given f
in F[x] of degree d over a field F, and a desired sparsity t, our goal is to
determine if there exists a multiple h in F[x] of f such that h has at most t
non-zero terms, and if so, to find such an h. When F=Q and t is constant, we
give a polynomial-time algorithm in d and the size of coefficients in h. When F
is a finite field, we show that the problem is at least as hard as determining
the multiplicative order of elements in an extension field of F (a problem
thought to have complexity similar to that of factoring integers), and this
lower bound is tight when t=2."
"An algorithm which either finds an nonzero integer vector ${\mathbf m}$ for
given $t$ real $n$-dimensional vectors ${\mathbf x}_1,...,{\mathbf x}_t$ such
that ${\mathbf x}_i^T{\mathbf m}=0$ or proves that no such integer vector with
norm less than a given bound exists is presented in this paper. The cost of the
algorithm is at most ${\mathcal O}(n^4 + n^3 \log \lambda(X))$ exact arithmetic
operations in dimension $n$ and the least Euclidean norm $\lambda(X)$ of such
integer vectors. It matches the best complexity upper bound known for this
problem. Experimental data show that the algorithm is better than an already
existing algorithm in the literature. In application, the algorithm is used to
get a complete method for finding the minimal polynomial of an unknown complex
algebraic number from its approximation, which runs even faster than the
corresponding \emph{Maple} built-in function."
"In this paper, by way of three examples - a fourth order low pass active RC
filter, a rudimentary BJT amplifier, and an LC ladder - we show, how the
algebraic capabilities of modern computer algebra systems can, or in the last
example, might be brought to use in the task of designing analog circuits."
"We consider the problem of interpolating an unknown multivariate polynomial
with coefficients taken from a finite field or as numerical approximations of
complex numbers. Building on the recent work of Garg and Schost, we improve on
the best-known algorithm for interpolation over large finite fields by
presenting a Las Vegas randomized algorithm that uses fewer black box
evaluations. Using related techniques, we also address numerical interpolation
of sparse polynomials with complex coefficients, and provide the first provably
stable algorithm (in the sense of relative error) for this problem, at the cost
of modestly more evaluations. A key new technique is a randomization which
makes all coefficients of the unknown polynomial distinguishable, producing
what we call a diverse polynomial. Another departure from most previous
approaches is that our algorithms do not rely on root finding as a subroutine.
We show how these improvements affect the practical performance with trial
implementations."
"We introduce a method and an algorithm for computing the weighted
Moore-Penrose inverse of multiple-variable polynomial matrix and the related
algorithm which is appropriated for sparse polynomial matrices. These methods
and algorithms are generalizations of algorithms developed in [M.B. Tasic, P.S.
Stanimirovic, M.D. Petkovic, Symbolic computation of weighted Moore-Penrose
inverse using partitioning method, Appl. Math. Comput. 189 (2007) 615-640] to
multiple-variable rational and polynomial matrices and improvements of these
algorithms on sparse matrices. Also, these methods are generalizations of the
partitioning method for computing the Moore-Penrose inverse of rational and
polynomial matrices introduced in [P.S. Stanimirovic, M.B. Tasic, Partitioning
method for rational and polynomial matrices, Appl. Math. Comput. 155 (2004)
137-163; M.D. Petkovic, P.S. Stanimirovic, Symbolic computation of the
Moore-Penrose inverse using partitioning method, Internat. J. Comput. Math. 82
(2005) 355-367] to the case of weighted Moore-Penrose inverse. Algorithms are
implemented in the symbolic computational package MATHEMATICA."
"We propose a method and algorithm for computing the weighted Moore-Penrose
inverse of one-variable rational matrices. Continuing this idea, we develop an
algorithm for computing the weighted Moore-Penrose inverse of one-variable
polynomial matrix. These methods and algorithms are generalizations of the
method for computing the weighted Moore-Penrose inverse for constant matrices,
originated in Wang and Chen [G.R. Wang, Y.L. Chen, A recursive algorithm for
computing the weighted Moore-Penrose inverse AMN, J. Comput. Math. 4 (1986)
74-85], and the partitioning method for computing the Moore-Penrose inverse of
rational and polynomial matrices introduced in Stanimirovic and Tasic [P.S.
Stanimirovic, M.B. Tasic, Partitioning method for rational and polynomial
matrices, Appl. Math. Comput. 155 (2004) 137-163]. Algorithms are implemented
in the symbolic computational package MATHEMATICA."
"An algorithm for computing {2, 3}, {2, 4}, {1, 2, 3}, {1, 2, 4} -inverses and
the Moore-Penrose inverse of a given rational matrix A is established. Classes
A(2, 3)s and A(2, 4)s are characterized in terms of matrix products (R*A)+R*
and T*(AT*)+, where R and T are rational matrices with appropriate dimensions
and corresponding rank. The proposed algorithm is based on these general
representations and the Cholesky factorization of symmetric positive matrices.
The algorithm is implemented in programming languages MATHEMATICA and DELPHI,
and illustrated via examples. Numerical results of the algorithm, corresponding
to the Moore-Penrose inverse, are compared with corresponding results obtained
by several known methods for computing the Moore-Penrose inverse."
"An overview of the solution methods for ordinary differential equations in
the Mathematica function DSolve is presented."
"An algorithm for the symbolic computation of recursion operators for systems
of nonlinear differential-difference equations (DDEs) is presented. Recursion
operators allow one to generate an infinite sequence of generalized symmetries.
The existence of a recursion operator therefore guarantees the complete
integrability of the DDE. The algo-rithm is based in part on the concept of
dilation invariance and uses our earlier algorithms for the symbolic
computation of conservation laws and generalized symmetries.
  The algorithm has been applied to a number of well-known DDEs, including the
Kac-van Moerbeke (Volterra), Toda, and Ablowitz-Ladik lattices, for which
recursion opera-tors are shown. The algorithm has been implemented in
Mathematica, a leading com-puter algebra system. The package
DDERecursionOperator.m is briefly discussed."
"We study the complexity of computing the real solutions of a bivariate
polynomial system using the recently proposed algorithm BISOLVE. BISOLVE is a
classical elimination method which first projects the solutions of a system
onto the $x$- and $y$-axes and, then, selects the actual solutions from the so
induced candidate set. However, unlike similar algorithms, BISOLVE requires no
genericity assumption on the input nor it needs any change of the coordinate
system. Furthermore, extensive benchmarks from \cite{bes-bisolve-2011} confirm
that the algorithm outperforms state of the art approaches by a large factor.
In this work, we show that, for two polynomials $f,g\in\mathbb{Z}[x,y]$ of
total degree at most $n$ with integer coefficients bounded by $2^\tau$, BISOLVE
computes isolating boxes for all real solutions of the system $f=g=0$ using
$\Otilde(n^8\tau^{2})$ bit operations, thereby improving the previous record
bound by a factor of at least $n^{2}$."
"One of the most famous conjectures in computer algebra is that matrix
multiplication might be feasible in not much more than quadratic time. The best
known exponent is 2.376, due to Coppersmith and Winograd. Many attempts to
solve this problems in the literature work by solving, fixed-size problems and
then apply the solution recursively. This leads to pure combinatorial
optimisation problems with fixed size. These problems are unlikely to be
solvable in polynomial time.
  In 1976 Laderman published a method to multiply two 3x3 matrices using only
23 multiplications. This result is non-commutative, and therefore can be
applied recursively to smaller sub-matrices. In 35 years nobody was able to do
better and it remains an open problem if this can be done with 22
multiplications. We proceed by solving the so called Brent equations [7]. We
have implemented a method to converting this very hard problem to a SAT
problem, and we have attempted to solve it, with our portfolio of some 500 SAT
solvers. With this new method we were able to produce new solutions to the
Laderman's problem. We present a new fully general non-commutative solution
with 23 multiplications and show that this solution is new and is NOT an
equivalent variant of the Laderman's original solution. This result
demonstrates that the space of solutions to Laderman's problem is larger than
expected, and therefore it becomes now more plausible that a solution with 22
multiplications exists. If it exists, we might be able to find it soon just by
running our algorithms longer, or due to further improvements in the SAT solver
algorithms."
"We develop symbolic methods of asymptotic approximations for solutions of
linear ordinary differential equations and use to them stabilize numerical
calculations. Our method follows classical analysis for first-order systems and
higher-order scalar equations where growth behavior is expressed in terms of
elementary functions. We then recast our equations in mollified form - thereby
obtaining stability."
"Root extraction is a classical problem in computers algebra. It plays an
essential role in cryptosystems based on elliptic curves. In 2006, Barreto and
Voloch proposed an algorithm to compute $r$th roots in ${F}_{q^m} $ for certain
choices of $m$ and $q$. If $r\,||\,q-1$ and $ (m, r)=1, $ they proved that the
complexity of their method is $\widetilde{\mathcal {O}}(r(\log m+\log\log
q)m\log q) $. In this paper, we extend the Barreto-Voloch algorithm to the
general case that $r\,||\,q^m-1$, without the restrictions $r\,||\,q-1$ and
$(m, r)=1 $. We also specify the conditions that the Barreto-Voloch algorithm
can be preferably applied."
"We show that the problem of constructing telescopers for functions of m
variables is equivalent to the problem of constructing telescopers for
algebraic functions of m -1 variables and present a new algorithm to construct
telescopers for algebraic functions of two variables. These considerations are
based on analyzing the residues of the input. According to experiments, the
resulting algorithm for rational functions of three variables is faster than
known algorithms, at least in some examples of combinatorial interest. The
algorithm for algebraic functions implies a new bound on the order of the
telescopers."
"In this paper we outline an algorithmic approach to compute Puiseux series
expansions for algebraic surfaces. The series expansions originate at the
intersection of the surface with as many coordinate planes as the dimension of
the surface. Our approach starts with a polyhedral method to compute cones of
normal vectors to the Newton polytopes of the given polynomial system that
defines the surface. If as many vectors in the cone as the dimension of the
surface define an initial form system that has isolated solutions, then those
vectors are potential tropisms for the initial term of the Puiseux series
expansion. Our preliminary methods produce exact representations for solution
sets of the cyclic $n$-roots problem, for $n = m^2$, corresponding to a result
of Backelin."
"In this paper we provide a computational approach to the shape of curves
which are rational in polar coordinates, i.e. which are defined by means of a
parametrization (r(t),\theta(t)) where both r(t),\theta(t) are rational
functions. Our study includes theoretical aspects on the shape of these curves,
and algorithmic results which eventually lead to an algorithm for plotting the
""interesting parts"" of the curve, i.e. the parts showing the main geometrical
features of it. On the theoretical side, we prove that these curves, with the
exceptions of lines and circles, cannot be algebraic (in cartesian
coordinates), we characterize the existence of infinitely many
self-intersections, and we connect this with certain phenomena which are not
possible in the algebraic world, namely the existence of limit circles, limit
points, or spiral branches. On the practical side, we provide an algorithm
which has been implemented in the computer algebra system Maple to visualize
this kind of curves. Our implementation makes use (and improves some aspects
of) the command polarplot currently available in Maple for plotting curves in
polar form."
"Sparse (or toric) elimination exploits the structure of polynomials by
measuring their complexity in terms of Newton polytopes instead of total
degree. The sparse, or Newton, resultant generalizes the classical homogeneous
resultant and its degree is a function of the mixed volumes of the Newton
polytopes. We sketch the sparse resultant constructions of Canny and Emiris and
show how they reduce the problem of root-finding to an eigenproblem. A novel
method for achieving this reduction is presented which does not increase the
dimension of the problem. Together with an implementation of the sparse
resultant construction, it provides a general solver for polynomial systems. We
discuss the overall implementation and illustrate its use by applying it to
concrete problems from vision, robotics and structural biology. The high
efficiency and accuracy of the solutions suggest that sparse elimination may be
the method of choice for systems of moderate size."
"We present a variation of the modular algorithm for computing the Hermite
Normal Form of an $\OK$-module presented by Cohen, where $\OK$ is the ring of
integers of a number field K. The modular strategy was conjectured to run in
polynomial time by Cohen, but so far, no such proof was available in the
literature. In this paper, we provide a new method to prevent the coefficient
explosion and we rigorously assess its complexity with respect to the size of
the input and the invariants of the field K."
"We present abstraction techniques that transform a given non-linear dynamical
system into a linear system or an algebraic system described by polynomials of
bounded degree, such that, invariant properties of the resulting abstraction
can be used to infer invariants for the original system. The abstraction
techniques rely on a change-of-basis transformation that associates each state
variable of the abstract system with a function involving the state variables
of the original system. We present conditions under which a given change of
basis transformation for a non-linear system can define an abstraction.
Furthermore, the techniques developed here apply to continuous systems defined
by Ordinary Differential Equations (ODEs), discrete systems defined by
transition systems and hybrid systems that combine continuous as well as
discrete subsystems. The techniques presented here allow us to discover, given
a non-linear system, if a change of bases transformation involving
degree-bounded polynomials yielding an algebraic abstraction exists. If so, our
technique yields the resulting abstract system, as well. This approach is
further extended to search for a change of bases transformation that abstracts
a given non-linear system into a system of linear differential inclusions. Our
techniques enable the use of analysis techniques for linear systems to infer
invariants for non-linear systems. We present preliminary evidence of the
practical feasibility of our ideas using a prototype implementation."
"In this paper, we consider parametric ideals and introduce a notion of
comprehensive involutive system. This notion plays the same role in theory of
involutive bases as the notion of comprehensive Groebner system in theory of
Groebner bases. Given a parametric ideal, the space of parameters is decomposed
into a finite set of cells. Each cell yields the corresponding involutive basis
of the ideal for the values of parameters in that cell. Using the Gerdt-Blinkov
algorithm for computing involutive bases and also the Montes algorithm for
computing comprehensive Groebner systems, we present an algorithm for
construction of comprehensive involutive systems. The proposed algorithm has
been implemented in Maple, and we provide an illustrative example showing the
step-by-step construction of comprehensive involutive system by our algorithm."
"We report on our experiences exploring state of the art Groebner basis
computation. We investigate signature based algorithms in detail. We also
introduce new practical data structures and computational techniques for use in
both signature based Groebner basis algorithms and more traditional variations
of the classic Buchberger algorithm. Our conclusions are based on experiments
using our new freely available open source standalone C++ library."
"Forward Automatic Differentiation (AD) is a technique for augmenting programs
to both perform their original calculation and also compute its directional
derivative. The essence of Forward AD is to attach a derivative value to each
number, and propagate these through the computation. When derivatives are
nested, the distinct derivative calculations, and their associated attached
values, must be distinguished. In dynamic languages this is typically
accomplished by creating a unique tag for each application of the derivative
operator, tagging the attached values, and overloading the arithmetic
operators. We exhibit a subtle bug, present in fielded implementations, in
which perturbations are confused despite the tagging machinery."
"We present a new algorithm for the computation of the irreducible factors of
degree at most $d$, with multiplicity, of multivariate lacunary polynomials
over fields of characteristic zero. The algorithm reduces this computation to
the computation of irreducible factors of degree at most $d$ of univariate
lacunary polynomials and to the factorization of low-degree multivariate
polynomials. The reduction runs in time polynomial in the size of the input
polynomial and in $d$. As a result, we obtain a new polynomial-time algorithm
for the computation of low-degree factors, with multiplicity, of multivariate
lacunary polynomials over number fields, but our method also gives partial
results for other fields, such as the fields of $p$-adic numbers or for
absolute or approximate factorization for instance.
  The core of our reduction uses the Newton polygon of the input polynomial,
and its validity is based on the Newton-Puiseux expansion of roots of bivariate
polynomials. In particular, we bound the valuation of $f(X,\phi)$ where $f$ is
a lacunary polynomial and $\phi$ a Puiseux series whose vanishing polynomial
has low degree."
"We present new techniques for reducing a multivariate sparse polynomial to a
univariate polynomial. The reduction works similarly to the classical and
widely-used Kronecker substitution, except that we choose the degrees randomly
based on the number of nonzero terms in the multivariate polynomial, that is,
its sparsity. The resulting univariate polynomial often has a significantly
lower degree than the Kronecker substitution polynomial, at the expense of a
small number of term collisions. As an application, we give a new algorithm for
multivariate interpolation which uses these new techniques along with any
existing univariate interpolation algorithm."
"We consider the following problem: Given a nested sum expression, find a sum
representation such that the nested depth is minimal. We obtain a symbolic
summation framework that solves this problem for sums defined, e.g., over
hypergeometric, $q$-hypergeometric or mixed hypergeometric expressions.
Recently, our methods have found applications in quantum field theory."
"We present a new proof of Stembridge's theorem about the enumeration of
totally symmetric plane partitions using the methodology suggested in the
recent Koutschan-Kauers-Zeilberger semi-rigorous proof of the Andrews-Robbins
q-TSPP conjecture. Our proof makes heavy use of computer algebra and is
completely automatic. We describe new methods that make the computations
feasible in the first place. The tantalizing aspect of this work is that the
same methods can be applied to prove the q-TSPP conjecture (that is a
q-analogue of Stembridge's theorem and open for more than 25 years); the only
hurdle here is still the computational complexity."
"We propose a technique for pattern classification in symbolic streams via
selective erasure of observed symbols, in cases where the patterns of interest
are represented as Probabilistic Finite State Automata (PFSA). We define an
additive abelian group for a slightly restricted subset of probabilistic finite
state automata (PFSA), and the group sum is used to formulate pattern-specific
semantic annihilators. The annihilators attempt to identify pre-specified
patterns via removal of essentially all inter-symbol correlations from observed
sequences, thereby turning them into symbolic white noise. Thus a perfect
annihilation corresponds to a perfect pattern match. This approach of
classification via information annihilation is shown to be strictly
advantageous, with theoretical guarantees, for a large class of PFSA models.
The results are supported by simulation experiments."
"In this paper we show how we can compute in a deterministic way the
decomposition of a multivariate rational function with a recombination
strategy. The key point of our recombination strategy is the used of Darboux
polynomials. We study the complexity of this strategy and we show that this
method improves the previous ones. In appendix, we explain how the strategy
proposed recently by J. Berthomieu and G. Lecerf for the sparse factorization
can be used in the decomposition setting. Then we deduce a decomposition
algorithm in the sparse bivariate case and we give its complexity"
"We present a zero decomposition theorem and an algorithm based on Wu's
method, which computes a zero decomposition with multiplicity for a given
zero-dimensional polynomial system. If the system satisfies some condition, the
zero decomposition is of triangular form."
"In this paper, we describe a reliable symbolic computational algorithm for
inverting general cyclic heptadiagonal matrices by using parallel computing
along with recursion. The algorithm is implementable to the Computer Algebra
System(CAS) such as MAPLE, Matlab and Mathematica . An example is presented for
the sake of illustration."
"Borel-fixed ideals play a key role in the study of Hilbert schemes. Indeed
each component and each intersection of components of a Hilbert scheme contains
at least one Borel-fixed point, i.e. a point corresponding to a subscheme
defined by a Borel-fixed ideal. Moreover Borel-fixed ideals have good
combinatorial properties, which make them very interesting in an algorithmic
perspective. In this paper, we propose an implementation of the algorithm
computing all the saturated Borel-fixed ideals with number of variables and
Hilbert polynomial assigned, introduced from a theoretical point of view in the
paper ""Segment ideals and Hilbert schemes of points"", Discrete Mathematics 311
(2011)."
"Quasi-stable ideals appear as leading ideals in the theory of Pommaret bases.
We show that quasi-stable leading ideals share many of the properties of the
generic initial ideal. In contrast to genericity, quasi-stability is a
characteristic independent property that can be effectively verified. We also
relate Pommaret bases to some invariants associated with local cohomology,
exhibit the existence of linear quotients in Pommaret bases and prove some
results on componentwise linear ideals."
"A large class of Feynman integrals, like e.g., two-point parameter integrals
with at most one mass and containing local operator insertions, can be
transformed to multi-sums over hypergeometric expressions. In this survey
article we present a difference field approach for symbolic summation that
enables one to simplify such definite nested sums to indefinite nested sums. In
particular, the simplification is given -if possible- in terms of harmonic
sums, generalized harmonic sums, cyclotomic harmonic sums or binomial sums.
Special emphasis is put on the developed packages Sigma, EvaluateMultiSums and
SumProduction that assist in the task to perform these simplifications
completely automatically for huge input expressions."
"Algebraic cryptanalysis usually requires to recover the secret key by solving
polynomial equations. Faugere's F4 is a well-known Grobner bases algorithm to
solve this problem. However, a serious drawback exists in the Grobner bases
based algebraic attacks, namely, any information won't be got if we couldn't
work out the Grobner bases of the polynomial equations system. In this paper,
we in-depth research the F4 algorithm over GF(2). By using S-polynomials to
replace critical pairs and computing the normal form of the productions with
respect to the field equations in certain steps, many ""redundant"" reductors are
avoided during the computation process of the F4 algorithm. By slightly
modifying the logic of F4 algorithm, we solve the univariate polynomials
appeared in the algorithm and then back-substitute the values of the solved
variables at each iteration of the algorithm. We call our improvements
Middle-Solving F4. The heuristic strategy of Middle-Solving overcomes the
drawback of algebraic attacks and well suits algebraic attacks. It has never
been applied to the Grobner bases algorithm before. Experiments to some Hidden
Field Equation instances and some classical benchmarks (Cyclic 6, Gonnet83)
show that Middle-Solving F4 is faster and uses less memory than Faugere's F4."
"We present fast algorithms for computing rational first integrals with
bounded degree of a planar polynomial vector field. Our approach is inspired by
an idea of Ferragut and Giacomini. We improve upon their work by proving that
rational first integrals can be computed via systems of linear equations
instead of systems of quadratic equations. This leads to a probabilistic
algorithm with arithmetic complexity $\bigOsoft(N^{2 \omega})$ and to a
deterministic algorithm solving the problem in $\bigOsoft(d^2N^{2 \omega+1})$
arithmetic operations, where $N$ denotes the given bound for the degree of the
rational first integral, and where $d \leq N$ is the degree of the vector
field, and $\omega$ the exponent of linear algebra. We also provide a fast
heuristic variant which computes a rational first integral, or fails, in
$\bigOsoft(N^{\omega+2})$ arithmetic operations. By comparison, the best
previous algorithm uses at least $d^{\omega+1}\, N^{4\omega +4}$ arithmetic
operations. We then show how to apply a similar method to the computation of
Darboux polynomials. The algorithms are implemented in a Maple package which is
available to interested readers with examples showing its efficiency."
"To compute solutions of sparse polynomial systems efficiently we have to
exploit the structure of their Newton polytopes. While the application of
polyhedral methods naturally excludes solutions with zero components, an
irreducible decomposition of a variety is typically understood in affine space,
including also those components with zero coordinates. We present a polyhedral
method to compute all affine solution sets of a polynomial system. The method
enumerates all factors contributing to a generalized permanent. Toric solution
sets are recovered as a special case of this enumeration. For sparse systems as
adjacent 2-by-2 minors our methods scale much better than the techniques from
numerical algebraic geometry."
"In this paper we will present SDeval, a software project that contains tools
for creating and running benchmarks with a focus on problems in computer
algebra. It is built on top of the Symbolic Data project, able to translate
problems in the database into executable code for various computer algebra
systems. The included tools are designed to be very flexible to use and to
extend, such that they can be utilized even in contexts of other communities.
With the presentation of SDEval, we will also address particularities of
benchmarking in the field of computer algebra. Furthermore, with SDEval, we
provide a feasible and automatizable way of reproducing benchmarks published in
current research works, which appears to be a difficult task in general due to
the customizability of the available programs. We will simultaneously present
the current developments in the Symbolic Data project."
"We consider solution operators of linear ordinary boundary problems with ""too
many"" boundary conditions, which are not always solvable. These generalized
Green's operators are a certain kind of generalized inverses of differential
operators. We answer the question when the product of two generalized Green's
operators is again a generalized Green's operator for the product of the
corresponding differential operators and which boundary problem it solves.
Moreover, we show that---provided a factorization of the underlying
differential operator---a generalized boundary problem can be factored into
lower order problems corresponding to a factorization of the respective Green's
operators. We illustrate our results by examples using the Maple package
IntDiffOp, where the presented algorithms are implemented."
"In this paper, we present a new algorithm and an experimental implementation
for factoring elements in the polynomial n'th Weyl algebra, the polynomial n'th
shift algebra, and ZZ^n-graded polynomials in the n'th q-Weyl algebra.
  The most unexpected result is that this noncommutative problem of factoring
partial differential operators can be approached effectively by reducing it to
the problem of solving systems of polynomial equations over a commutative ring.
In the case where a given polynomial is ZZ^n-graded, we can reduce the problem
completely to factoring an element in a commutative multivariate polynomial
ring.
  The implementation in Singular is effective on a broad range of polynomials
and increases the ability of computer algebra systems to address this important
problem. We compare the performance and output of our algorithm with other
implementations in commodity computer algebra systems on nontrivial examples."
"Systems biology focuses on the study of entire biological systems rather than
on their individual components. With the emergence of high-throughput data
generation technologies for molecular biology and the development of advanced
mathematical modeling techniques, this field promises to provide important new
insights. At the same time, with the availability of increasingly powerful
computers, computer algebra has developed into a useful tool for many
applications. This article illustrates the use of computer algebra in systems
biology by way of a well-known gene regulatory network, the Lac Operon in the
bacterium E. coli."
"Using specializations of unfold and fold on a generic tree data type we
derive unranking and ranking functions providing natural number encodings for
various Hereditarily Finite datatypes.
  In this context, we interpret unranking operations as instances of a generic
anamorphism and ranking operations as instances of the corresponding
catamorphism.
  Starting with Ackerman's Encoding from Hereditarily Finite Sets to Natural
Numbers we define pairings and tuple encodings that provide building blocks for
a theory of Hereditarily Finite Functions.
  The more difficult problem of ranking and unranking Hereditarily Finite
Permutations is then tackled using Lehmer codes and factoradics.
  The self-contained source code of the paper, as generated from a literate
Haskell program, is available at
\url{http://logic.csci.unt.edu/tarau/research/2008/fFUN.zip}.
  Keywords: ranking/unranking, pairing/tupling functions, Ackermann encoding,
hereditarily finite sets, hereditarily finite functions, permutations and
factoradics, computational mathematics, Haskell data representations"
"In this article we present a refined summation theory based on Karr's
difference field approach. The resulting algorithms find sum representations
with optimal nested depth. For instance, the algorithms have been applied
successively to evaluate Feynman integrals from Perturbative Quantum Field
Theory."
"Usually creative telescoping is used to derive recurrences for sums. In this
article we show that the non-existence of a creative telescoping solution, and
more generally, of a parameterized telescoping solution, proves algebraic
independence of certain types of sums. Combining this fact with
summation-theory shows transcendence of whole classes of sums. Moreover, this
result throws new light on the question why, e.g., Zeilberger's algorithm fails
to find a recurrence with minimal order."
"The framework used to prove the multiplicative law deformation of the algebra
of Feynman-Bender diagrams is a \textit{twisted shifted dual law} (in fact,
twice). We give here a clear interpretation of its two parameters. The crossing
parameter is a deformation of the tensor structure whereas the superposition
parameters is a perturbation of the shuffle coproduct of Hoffman type which, in
turn, can be interpreted as the diagonal restriction of a superproduct. Here,
we systematically detail these constructions."
"In this paper we show how to extend the known algorithm of nodal analysis in
such a way that, in the case of circuits without nullors and controlled sources
(but allowing for both, independent current and voltage sources), the system of
nodal equations describing the circuit is partitioned into one part, where the
nodal variables are explicitly given as linear combinations of the voltage
sources and the voltages of certain reference nodes, and another, which
contains the node variables of these reference nodes only and which moreover
can be read off directly from the given circuit. Neither do we need
preparational graph transformations, nor do we need to introduce additional
current variables (as in MNA). Thus this algorithm is more accessible to
students, and consequently more suitable for classroom presentations."