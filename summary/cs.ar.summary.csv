summary
"As the one-chip integration of HW-modules designed by different companies
becomes more and more popular reliability of a HW-design and evaluation of the
timing behavior during the prototype stage are absolutely necessary. One way to
guarantee reliability is the use of robust design styles, e.g.,
delay-insensitivity. For early timing evaluation two aspects must be
considered: a) The timing needs to be proportional to technology variations and
b) the implemented architecture should be identical for prototype and target.
The first can be met also by delay-insensitive implementation. The latter one
is the key point. A unified architecture is needed for prototyping as well as
implementation. Our new approach to rapid prototyping of signal processing
tasks is based on a configurable, delay-insensitive implemented processor
called Flysig. In essence, the Flysig processor can be understood as a complex
FPGA where the CLBs are substituted by bit-serial operators. In this paper the
general concept is detailed and first experimental results are given for
demonstration of the main advantages: delay-insensitive design style, direct
correspondence between prototyping and target architecture, high performance
and reasonable shortening of the design cycle."
"Field-Programmable Gate Arrays (FPGAs) have provided Thomas Jefferson
National Accelerator Facility (Jefferson Lab) with versatile VME-based data
acquisition and control interfaces with minimal development times. FPGA designs
have been used to interface to VME and provide control logic for numerous
systems. The building blocks of these logic designs can be tailored to the
individual needs of each system and provide system operators with read-backs
and controls via a VME interface to an EPICS based computer. This versatility
allows the system developer to choose components and define operating
parameters and options that are not readily available commercially. Jefferson
Lab has begun developing standard FPGA libraries that result in quick turn
around times and inexpensive designs."
"A Dual Digital Signal Processing VME Board was developed for the Continuous
Electron Beam Accelerator Facility (CEBAF) Beam Current Monitor (BCM) system at
Jefferson Lab. It is a versatile general-purpose digital signal processing
board using an open architecture, which allows for adaptation to various
applications. The base design uses two independent Texas Instrument (TI)
TMS320C6711, which are 900 MFLOPS floating-point digital signal processors
(DSP). Applications that require a fixed point DSP can be implemented by
replacing the baseline DSP with the pin-for-pin compatible TMS320C6211. The
design can be manufactured with a reduced chip set without redesigning the
printed circuit board. For example it can be implemented as a single-channel
DSP with no analog I/O."
"This poster describes the timing system being designed for Spallation Neutron
Source being built at Oak Ridge National lab."
"This paper introduces a novel method for synthesizing digital circuits
derived from Binary Decision Diagrams (BDDs) that can yield to reduction in
power dissipation. The power reduction is achieved by decreasing the switching
activity in a circuit while paying close attention to information measures as
an optimization criterion. We first present the technique of efficient
BDD-based computation of information measures which are used to guide the power
optimization procedures. Using this technique, we have developed an algorithm
of BDD reordering which leads to reducing the power consumption of the circuits
derived from BDDs. Results produced by the synthesis on the ISCAS benchmark
circuits are very encouraging."
"This paper addresses a new approach to find a spectrum of information
measures for the process of digital circuit synthesis. We consider the problem
from the information engine point of view. The circuit synthesis as a whole and
different steps of the design process (an example of decision diagram is given)
are presented via such measurements as entropy, logical work and information
vitality. We also introduce new information measures to provide better
estimates of synthesis criteria. We show that the basic properties of
information engine, such as the conservation law of information flow and the
equilibrium law of information can be formulated."
"Reconfigurable computing refers to the use of processors, such as Field
Programmable Gate Arrays (FPGAs), that can be modified at the hardware level to
take on different processing tasks. A reconfigurable computing platform
describes the hardware and software base on top of which modular extensions can
be created, depending on the desired application. Such reconfigurable computing
platforms can take on varied designs and implementations, according to the
constraints imposed and features desired by the scope of applications. This
paper introduces a PC-based reconfigurable computing platform software
frameworks that is flexible and extensible enough to abstract the different
hardware types and functionality that different PCs may have. The requirements
of the software platform, architectural issues addressed, rationale behind the
decisions made, and frameworks design implemented are discussed."
"A standard approach to building a fuzzy controller based on stochastic logic
uses binary random signals with an average (expected value of a random
variable) in the range [0, 1]. A different approach is presented, founded on a
representation of the membership functions with the probability density
functions."
"Many reconfigurable platforms require that applications be written
specifically to take advantage of the reconfigurable hardware. In a PC-based
environment, this presents an undesirable constraint in that the many already
available applications cannot leverage on such hardware. Greatest benefit can
only be derived from reconfigurable devices if even native OS applications can
transparently utilize reconfigurable devices as they would normal full-fledged
hardware devices. This paper presents how Proteus Virtual Devices are used to
expose reconfigurable hardware in a transparent manner for use by typical
native OS applications."
"In the paper we define and characterize the asynchronous systems from the
point of view of their autonomy, determinism, order, non-anticipation, time
invariance, symmetry, stability and other important properties. The study is
inspired by the models of the asynchronous circuits."
"This paper presents a data stationary architecture in which each word has an
attached address field. Address fields massively update in parallel to record
data interchanges. Words do not move until memory is read for post processing.
A sea of such cells can test large-scale quantum algorithms, although other
programming is possible."
"Management of communication by on-line routing in new FPGAs with a large
amount of logic resources and partial reconfigurability is a new challenging
problem. A Network-on-Chip
  (NoC) typically uses packet routing mechanism, which has often unsafe data
transfers, and network interface overhead. In this paper, circuit routing for
such dynamic NoCs is investigated, and a practical 1-dimensional network with
an efficient routing algorithm is proposed and implemented. Also, this concept
has been extended to the 2-dimensional case. The implementation results show
the low area overhead and high performance of this network."
"Wiring diagrams are given for a quantum algorithm processor in CMOS to
compute, in parallel, all divisors of an n-bit integer. Lines required in a
wiring diagram are proportional to n. Execution time is proportional to the
square of n."
"A new paradigm to support the communication among modules dynamically placed
on a reconfigurable device at run-time is presented. Based on the network on
chip (NoC) infrastructure, we developed a dynamic communication infrastructure
as well as routing methodologies capable to handle routing in a NoC with
obstacles created by dynamically placed components. We prove the unrestricted
reachability of components and pins, the deadlock-freeness and we finally show
the feasibility of our approach by means on real life example applications."
"This paper reviews various engineering hurdles facing the field of quantum
computing. Specifically, problems related to decoherence, state preparation,
error correction, and implementability of gates are considered."
"IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and
a major enhancement to the standard is the addition of decimal format. This
paper proposes two novel BCD adders called carry skip and carry look-ahead BCD
adders respectively. Furthermore, in the recent years, reversible logic has
emerged as a promising technology having its applications in low power CMOS,
quantum computing, nanotechnology, and optical computing. It is not possible to
realize quantum computing without reversible logic. Thus, this paper also paper
provides the reversible logic implementation of the conventional BCD adder as
the well as the proposed Carry Skip BCD adder using a recently proposed TSG
gate. Furthermore, a new reversible gate called TS-3 is also being proposed and
it has been shown that the proposed reversible logic implementation of the BCD
Adders is much better compared to recently proposed one, in terms of number of
reversible gates used and garbage outputs produced. The reversible BCD circuits
designed and proposed here form the basis of the decimal ALU of a primitive
quantum CPU."
"In the recent years, reversible logic has emerged as a promising technology
having its applications in low power CMOS, quantum computing, nanotechnology,
and optical computing. The classical set of gates such as AND, OR, and EXOR are
not reversible. This paper proposes a new 4 * 4 reversible gate called TSG
gate. The proposed gate is used to design efficient adder units. The most
significant aspect of the proposed gate is that it can work singly as a
reversible full adder i.e reversible full adder can now be implemented with a
single gate only. The proposed gate is then used to design reversible ripple
carry and carry skip adders. It is demonstrated that the adder architectures
designed using the proposed gate are much better and optimized, compared to
their existing counterparts in literature; in terms of number of reversible
gates and garbage outputs. Thus, this paper provides the initial threshold to
building of more complex system which can execute more complicated operations
using reversible logic."
"In recent years, reversible logic has emerged as a promising computing
paradigm having its applications in low power computing, quantum computing,
nanotechnology, optical computing and DNA computing. The classical set of gates
such as AND, OR, and EXOR are not reversible. Recently, it has been shown how
to encode information in DNA and use DNA amplification to implement Fredkin
gates. Furthermore, in the past Fredkin gates have been constructed using DNA,
whose outputs are used as inputs for other Fredkin gates. Thus, it can be
concluded that arbitrary circuits of Fredkin gates can be constructed using
DNA. This paper provides the initial threshold to building of more complex
system having reversible sequential circuits and which can execute more
complicated operations. The novelty of the paper is the reversible designs of
sequential circuits using Fredkin gate. Since, Fredkin gate has already been
realized using DNA, it is expected that this work will initiate the building of
complex systems using DNA. The reversible circuits designed here are highly
optimized in terms of number of gates and garbage outputs. The modularization
approach that is synthesizing small circuits and thereafter using them to
construct bigger circuits is used for designing the optimal reversible
sequential circuits."
"In the recent years, reversible logic has emerged as a promising technology
having its applications in low power CMOS, quantum computing, nanotechnology,
and optical computing. The classical set of gates such as AND, OR, and EXOR are
not reversible. Recently a 4 * 4 reversible gate called TSG is proposed. The
most significant aspect of the proposed gate is that it can work singly as a
reversible full adder, that is reversible full adder can now be implemented
with a single gate only. This paper proposes a NXN reversible multiplier using
TSG gate. It is based on two concepts. The partial products can be generated in
parallel with a delay of d using Fredkin gates and thereafter the addition can
be reduced to log2N steps by using reversible parallel adder designed from TSG
gates. Similar multiplier architecture in conventional arithmetic (using
conventional logic) has been reported in existing literature, but the proposed
one in this paper is totally based on reversible logic and reversible cells as
its building block. A 4x4 architecture of the proposed reversible multiplier is
also designed. It is demonstrated that the proposed multiplier architecture
using the TSG gate is much better and optimized, compared to its existing
counterparts in literature; in terms of number of reversible gates and garbage
outputs. Thus, this paper provides the initial threshold to building of more
complex system which can execute more complicated operations using reversible
logic."
"We detail a procedure for the computation of the polynomial form of an
electronic combinational circuit from the design equations in a truth table.
The method uses the Buchberger algorithm rather than current traditional
methods based on search algorithms. We restrict the analysis to a single
output, but the procedure can be generalized to multiple outputs. The procedure
is illustrated with the design of a simple arithmetic and logic unit with two
3-bit operands and two control bits."
"The systems supporting signal and image applications process large amount of
data. That involves an intensive use of the memory which becomes the bottleneck
of systems. Memory limits performances and represents a significant proportion
of total consumption. In the development high level synthesis tool called GAUT
Low Power, we are interested in the synthesis of the memory unit. In this work,
we integrate the data storage and data transfert to constraint the high level
synthesis of the datapath's execution unit."
"The design of complex Systems-on-Chips implies to take into account
communication and memory access constraints for the integration of dedicated
hardware accelerator. In this paper, we present a methodology and a tool that
allow the High-Level Synthesis of DSP algorithm, under both I/O timing and
memory constraints. Based on formal models and a generic architecture, this
tool helps the designer to find a reasonable trade-off between both the
required I/O timing behavior and the internal memory access parallelism of the
circuit. The interest of our approach is demonstrated on the case study of a
FFT algorithm."
"We introduce a new approach to take into account the memory architecture and
the memory mapping in High- Level Synthesis for data intensive applications. We
formalize the memory mapping as a set of constraints for the synthesis, and
defined a Memory Constraint Graph and an accessibility criterion to be used in
the scheduling step. We use a memory mapping file to include those memory
constraints in our HLS tool GAUT. It is possible, with the help of GAUT, to
explore a wide range of solutions, and to reach a good tradeoff between time,
power-consumption, and area."
"We introduce a new approach to take into account the memory architecture and
the memory mapping in the High- Level Synthesis of Real-Time embedded systems.
We formalize the memory mapping as a set of constraints used in the scheduling
step. We use a memory mapping file to include those memory constraints in our
HLS tool GAUT. Our scheduling algorithm exhibits a relatively low complexity
that permits to tackle complex designs in a reasonable time. Finally, we show
how to explore, with the help of GAUT, a wide range of solutions, and to reach
a good tradeoff between time, power-consumption, and area."
"The design of complex Digital Signal Processing systems implies to minimize
architectural cost and to maximize timing performances while taking into
account communication and memory accesses constraints for the integration of
dedicated hardware accelerator. Unfortunately, the traditional Matlab/ Simulink
design flows gather not very flexible hardware blocs. In this paper, we present
a methodology and a tool that permit the High-Level Synthesis of DSP
applications, under both I/O timing and memory constraints. Based on formal
models and a generic architecture, our tool GAUT helps the designer in finding
a reasonable trade-off between the circuit's performance and its architectural
complexity. The efficiency of our approach is demonstrated on the case study of
a FFT algorithm."
"Media-processing applications, such as signal processing, 2D and 3D graphics
rendering, and image compression, are the dominant workloads in many embedded
systems today. The real-time constraints of those media applications have
taxing demands on today's processor performances with low cost, low power and
reduced design delay. To satisfy those challenges, a fast and efficient
strategy consists in upgrading a low cost general purpose processor core. This
approach is based on the personalization of a general RISC processor core
according the target multimedia application requirements. Thus, if the extra
cost is justified, the general purpose processor GPP core can be enforced with
instruction level coprocessors, coarse grain dedicated hardware, ad hoc
memories or new GPP cores. In this way the final design solution is tailored to
the application requirements. The proposed approach is based on three main
steps: the first one is the analysis of the targeted application using
efficient metrics. The second step is the selection of the appropriate
architecture template according to the first step results and recommendations.
The third step is the architecture generation. This approach is experimented
using various image and video algorithms showing its feasibility."
"In recent years, reversible logic has emerged as a promising computing
paradigm having application in low power CMOS, quantum computing,
nanotechnology, and optical computing. The classical set of gates such as AND,
OR, and EXOR are not reversible. This paper utilizes a new 4 * 4 reversible
gate called TSG gate to build the components of a primitive reversible/quantum
ALU. The most significant aspect of the TSG gate is that it can work singly as
a reversible full adder, that is reversible full adder can now be implemented
with a single gate only. A Novel reversible 4:2 compressor is also designed
from the TSG gate which is later used to design a novel 8x8 reversible Wallace
tree multiplier. It is proved that the adder, 4:2 compressor and multiplier
architectures designed using the TSG gate are better than their counterparts
available in literature, in terms of number of reversible gates and garbage
outputs. This is perhaps, the first attempt to design a reversible 4:2
compressor and a reversible Wallace tree multiplier as far as existing
literature and our knowledge is concerned. Thus, this paper provides an initial
threshold to build more complex systems which can execute complicated
operations using reversible logic."
"This paper proposes the hardware implementation of RSA encryption/decryption
algorithm using the algorithms of Ancient Indian Vedic Mathematics that have
been modified to improve performance. The recently proposed hierarchical
overlay multiplier architecture is used in the RSA circuitry for multiplication
operation. The most significant aspect of the paper is the development of a
division architecture based on Straight Division algorithm of Ancient Indian
Vedic Mathematics and embedding it in RSA encryption/decryption circuitry for
improved efficiency. The coding is done in Verilog HDL and the FPGA synthesis
is done using Xilinx Spartan library. The results show that RSA circuitry
implemented using Vedic division and multiplication is efficient in terms of
area/speed compared to its implementation using conventional multiplication and
division architectures"
"In recent years, reversible logic has emerged as a promising computing
paradigm having application in low power CMOS, quantum computing,
nanotechnology, and optical computing. The classical set of gates such as AND,
OR, and EXOR are not reversible. In this paper, the authors have proposed
reversible programmable logic array (RPLA) architecture using reversible
Fredkin and Feynman gates. The proposed RPLA has n inputs and m outputs and can
realize m functions of n variables. In order to demonstrate the design of RPLA,
a 3 input RPLA is designed which can perform any 28 functions using the
combination of 8 min terms (23). Furthermore, the application of the designed 3
input RPLA is shown by implementing the full adder and full subtractor
functions through it."
"IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and
a major enhancement to the standard is the addition of decimal format. Firstly,
this paper proposes novel two transistor AND and OR gates. The proposed AND
gate has no power supply, thus it can be referred as the Powerless AND gate.
Similarly, the proposed two transistor OR gate has no ground and can be
referred as Groundless OR. Secondly for IEEE 754r format, two novel BCD adders
called carry skip and carry look-ahead BCD adders are also proposed in this
paper. In order to design the carry look-ahead BCD adder, a novel 4 bit carry
look-ahead adder called NCLA is proposed which forms the basic building block
of the proposed carry look-ahead BCD adder. Finally, the proposed two
transistors AND and OR gates are used to provide the optimized small area low
power high throughput circuitries of the proposed BCD adders."
"In this paper, the authors propose the idea of a combined integer and
floating point multiplier(CIFM) for FPGAs. The authors propose the replacement
of existing 18x18 dedicated multipliers in FPGAs with dedicated 24x24
multipliers designed with small 4x4 bit multipliers. It is also proposed that
for every dedicated 24x24 bit multiplier block designed with 4x4 bit
multipliers, four redundant 4x4 multiplier should be provided to enforce the
feature of self repairability (to recover from the faults). In the proposed
CIFM reconfigurability at run time is also provided resulting in low power. The
major source of motivation for providing the dedicated 24x24 bit multiplier
stems from the fact that single precision floating point multiplier requires
24x24 bit integer multiplier for mantissa multiplication. A reconfigurable,
self-repairable 24x24 bit multiplier (implemented with 4x4 bit multiply
modules) will ideally suit this purpose, making FPGAs more suitable for integer
as well floating point operations. A dedicated 4x4 bit multiplier is also
proposed in this paper. Moreover, in the recent years, reversible logic has
emerged as a promising technology having its applications in low power CMOS,
quantum computing, nanotechnology, and optical computing. It is not possible to
realize quantum computing without reversible logic. Thus, this paper also paper
provides the reversible logic implementation of the proposed CIFM. The
reversible CIFM designed and proposed here will form the basis of the
completely reversible FPGAs."
"This paper presents a solution to efficiently explore the design space of
communication adapters. In most digital signal processing (DSP) applications,
the overall architecture of the system is significantly affected by
communication architecture, so the designers need specifically optimized
adapters. By explicitly modeling these communications within an effective
graph-theoretic model and analysis framework, we automatically generate an
optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs
a C description of Input/Output data scheduling, and user requirements
(throughput, latency, parallelism...), and formalizes communication constraints
through a Resource Constraints Graph (RCG). The RCG properties enable an
efficient architecture space exploration in order to synthesize a STAR
component. The proposed approach has been tested to design an industrial data
mixing block example: an Ultra-Wideband interleaver."
"This paper presents a solution to efficiently explore the design space of
communication adapters. In most digital signal processing (DSP) applications,
the overall architecture of the system is significantly affected by
communication architecture, so the designers need specifically optimized
adapters. By explicitly modeling these communications within an effective
graph-theoretic model and analysis framework, we automatically generate an
optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs
a C description of Input/Output data scheduling, and user requirements
(throughput, latency, parallelism...), and formalizes communication constraints
through a Resource Constraints Graph (RCG). The RCG properties enable an
efficient architecture space exploration in order to synthesize a STAR
component. The proposed approach has been tested to design an industrial data
mixing block example: an Ultra-Wideband interleaver."
"The re-use of pre-designed blocks is a well-known concept of the software
development. This technique has been applied to System-on-Chip (SoC) design
whose complexity and heterogeneity are growing. The re-use is made thanks to
high level components, called virtual components (IP), available in more or
less flexible forms. These components are dedicated blocks: digital signal
processing (DCT, FFT), telecommunications (Viterbi, TurboCodes),... These
blocks rest on a model of fixed architecture with very few degrees of
personalization. This rigidity is particularly true for the communication
interface whose orders of acquisition and production of data, the temporal
behavior and protocols of exchanges are fixed. The successful integration of
such an IP requires that the designer (1) synchronizes the components (2)
converts the protocols between ""incompatible"" blocks (3) temporizes the data to
guarantee the temporal constraints and the order of the data. This phase
remains however very manual and source of errors. Our approach proposes a
formal modeling, based on an original Ressource Compatibility Graph. The
synthesis flow is based on a set of transformations of the initial graph to
lead to an interface architecture allowing the space-time adaptation of the
data exchanges between several components."
"This paper encompasses a super helical memory system's design, 'Boolean logic
& image-logic' as a theoretical concept of an invention-model to 'store
time-data' in terms of anticipating the best memory location ever for
data/time. A waterfall effect is deemed to assist the process of
potential-difference output-switch into diverse logic states in quantum dot
computational methods via utilizing coiled carbon nanotubes (CCNTs) and carbon
nanotube field effect transistors (CNFETs). A 'quantum confinement' is thus
derived for a flow of particles in a categorized quantum well substrate with a
normalized capacitance rectifying high B-field flux into electromagnetic
induction. Multi-access of coherent sequences of 'qubit addressing' is gained
in any magnitude as pre-defined for the orientation of array displacement.
Briefly, Gaussian curvature of k<0 is debated in aim of specifying the 2D
electron gas characteristics in scenarios where data is stored in short
intervals versus long ones e.g. when k'>(k<0) for greater CCNT diameters,
space-time continuum is folded by chance for the particle. This benefits from
Maxwell-Lorentz theory in Minkowski's space-time viewpoint alike to crystal
oscillators for precise data timing purposes and radar systems e.g., time
varying self-clocking devices in diverse geographic locations. This application
could also be optional for data depository versus extraction, in the best
supercomputer system's locations, autonomously. For best performance in
minimizing current limiting mechanisms including electromigration, a multilevel
metallization and implant process forming elevated sources/drains for the
circuit's staircase pyramidal construction, is discussed accordingly."
"The purpose of this paper is to formally specify a flow devoted to the design
of Differential Power Analysis (DPA) resistant QDI asynchronous circuits. The
paper first proposes a formal modeling of the electrical signature of QDI
asynchronous circuits. The DPA is then applied to the formal model in order to
identify the source of leakage of this type of circuits. Finally, a complete
design flow is specified to minimize the information leakage. The relevancy and
efficiency of the approach is demonstrated using the design of an AES
crypto-processor."
"This paper describes JANUS, a modular massively parallel and reconfigurable
FPGA-based computing system. Each JANUS module has a computational core and a
host. The computational core is a 4x4 array of FPGA-based processing elements
with nearest-neighbor data links. Processors are also directly connected to an
I/O node attached to the JANUS host, a conventional PC. JANUS is tailored for,
but not limited to, the requirements of a class of hard scientific applications
characterized by regular code structure, unconventional data manipulation
instructions and not too large data-base size. We discuss the architecture of
this configurable machine, and focus on its use on Monte Carlo simulations of
statistical mechanics. On this class of application JANUS achieves impressive
performances: in some cases one JANUS processing element outperfoms high-end
PCs by a factor ~ 1000. We also discuss the role of JANUS on other classes of
scientific applications."
"Reduction in power consumption has become a major criterion of design in
modern ICs. One such scheme to reduce power consumption by an IC is the use of
multiple power supplies for critical and non-critical paths. To maintain the
impedance of a power distribution system below a specified level, multiple
decoupling capacitors are placed at different levels of power grid hierarchy.
This paper describes about three-voltage supply power distribution systems. The
noise at one power supply can propagate to the other power supply, causing
power and signal integrity problems in the overall system. Effects such as
anti-resonance and remedies for these effects are studied. Impedance of the
three-voltage supply power distribution system is calculated in terms of
RLC-model of decoupling capacitors. Further the obtained impedance depends on
the frequency; hence brief frequency analysis of impedance is done."
"This paper presents a method to automatically generate compact symbolic
performance models of analog circuits with no prior specification of an
equation template. The approach takes SPICE simulation data as input, which
enables modeling of any nonlinear circuits and circuit characteristics. Genetic
programming is applied as a means of traversing the space of possible symbolic
expressions. A grammar is specially designed to constrain the search to a
canonical form for functions. Novel evolutionary search operators are designed
to exploit the structure of the grammar. The approach generates a set of
symbolic models which collectively provide a tradeoff between error and model
complexity. Experimental results show that the symbolic models generated are
compact and easy to understand, making this an effective method for aiding
understanding in analog design. The models also demonstrate better prediction
quality than posynomials."
"In this paper, the program control unit of an embedded RISC processor is
enhanced with a novel zero-overhead loop controller (ZOLC) supporting arbitrary
loop structures with multiple-entry/exit nodes. The ZOLC has been incorporated
to an open RISC processor core to evaluate the performance of the proposed unit
for alternative configurations of the selected processor. It is proven that
speed improvements of 8.4% to 48.2% are feasible for the used benchmarks."
"Since the advent of new nanotechnologies, the variability of gate delay due
to process variations has become a major concern. This paper proposes a new
gate delay model that includes impact from both process variations and multiple
input switching. The proposed model uses orthogonal polynomial based
probabilistic collocation method to construct a delay analytical equation from
circuit timing performance. From the experimental results, our approach has
less that 0.2% error on the mean delay of gates and less than 3% error on the
standard deviation."
"Let's be clear from the outset: SoC can most certainly make use of UML; SoC
just doesn't need more UML, or even all of it. The advent of model mappings,
coupled with marks that indicate which mapping rule to apply, enable a major
simplification of the use of UML in SoC."
"We have presented an optimal buffer sizing and buffer insertion methodology
which uses stochastic models of the architecture and Continuous Time Markov
Decision Processes CTMDPs. Such a methodology is useful in managing the scarce
buffer resources available on chip as compared to network based data
communication which can have large buffer space. The modeling of this problem
in terms of a CT-MDP framework lead to a nonlinear formulation due to usage of
bridges in the bus architecture. We present a methodology to split the problem
into several smaller though linear systems and we then solve these subsystems."
"In signal integrity analysis, the joint effect of propagated noise through
library cells, and of the noise injected on a quiet net by neighboring
switching nets through coupling capacitances, must be considered in order to
accurately estimate the overall noise impact on design functionality and
performances. In this work the impact of the cell non-linearity on the noise
glitch waveform is analyzed in detail, and a new macromodel that allows to
accurately and efficiently modeling the non-linear effects of the victim driver
in noise analysis is presented. Experimental results demonstrate the
effectiveness of our method, and confirm that existing noise analysis
approaches based on linear superposition of the propagated and
crosstalk-injected noise can be highly inaccurate, thus impairing the sign-off
functional verification phase."
"In this paper, the application of a cycle accurate binary translator for
rapid prototyping of SoCs will be presented. This translator generates code to
run on a rapid prototyping system consisting of a VLIW processor and FPGAs. The
generated code is annotated with information that triggers cycle generation for
the hardware in parallel to the execution of the translated program. The VLIW
processor executes the translated program whereas the FPGAs contain the
hardware for the parallel cycle generation and the bus interface that adapts
the bus of the VLIW processor to the SoC bus of the emulated processor core."
"This paper describes a flexible logic BIST scheme that features high fault
coverage achieved by fault-simulation guided test point insertion, real
at-speed test capability for multi-clock designs without clock frequency
manipulation, and easy physical implementation due to the use of a low-speed SE
signal. Application results of this scheme to two widely used IP cores are also
reported."
"In this paper is proposed a technique to integrate and simulate a dynamic
memory in a multiprocessor framework based on C/C++/SystemC. Using host
machine's memory management capabilities, dynamic data processing is supported
without compromising speed and accuracy of the simulation. A first prototype in
a shared memory context is presented."
"In this paper, we investigate the impact of interconnect and device process
variations on voltage fluctuations in power grids. We consider random
variations in the power grid's electrical parameters as spatial stochastic
processes and propose a new and efficient method to compute the stochastic
voltage response of the power grid. Our approach provides an explicit
analytical representation of the stochastic voltage response using orthogonal
polynomials in a Hilbert space. The approach has been implemented in a
prototype software called OPERA (Orthogonal Polynomial Expansions for Response
Analysis). Use of OPERA on industrial power grids demonstrated speed-ups of up
to two orders of magnitude. The results also show a significant variation of
about $\pm$ 35% in the nominal voltage drops at various nodes of the power
grids and demonstrate the need for variation-aware power grid analysis."
"Utilizing on-chip caches in embedded multiprocessor-system-on-a-chip (MPSoC)
based systems is critical from both performance and power perspectives. While
most of the prior work that targets at optimizing cache behavior are performed
at hardware and compilation levels, operating system (OS) can also play major
role as it sees the global access pattern information across applications. This
paper proposes a cache-conscious OS process scheduling strategy based on data
reuse. The proposed scheduler implements two complementary approaches. First,
the processes that do not share any data between them are scheduled at
different cores if it is possible to do so. Second, the processes that could
not be executed at the same time (due to dependences) but share data among each
other are mapped to the same processor core so that they share the cache
contents. Our experimental results using this new data locality aware OS
scheduling strategy are promising, and show significant improvements in task
completion times."
"Power dissipation during test is a major challenge in testing integrated
circuits. Dynamic power has been the dominant part of power dissipation in CMOS
circuits, however, in future technologies the static portion of power
dissipation will outreach the dynamic portion. This paper proposes an efficient
technique to reduce both dynamic and static power dissipation in scan
structures. Scan cell outputs which are not on the critical path(s) are
multiplexed to fixed values during scan mode. These constant values and primary
inputs are selected such that the transitions occurred on non-multiplexed scan
cells are suppressed and the leakage current during scan mode is decreased. A
method for finding these vectors is also proposed. Effectiveness of this
technique is proved by experiments performed on ISCAS89 benchmark circuits."
"Assessing IC manufacturing process fluctuations and their impacts on IC
interconnect performance has become unavoidable for modern DSM designs.
However, the construction of parametric interconnect models is often hampered
by the rapid increase in computational cost and model complexity. In this paper
we present an efficient yet accurate parametric model order reduction algorithm
for addressing the variability of IC interconnect performance. The efficiency
of the approach lies in a novel combination of low-rank matrix approximation
and multi-parameter moment matching. The complexity of the proposed parametric
model order reduction is as low as that of a standard Krylov subspace method
when applied to a nominal system. Under the projection-based framework, our
algorithm also preserves the passivity of the resulting parametric models."
"This paper proposes a diagnosis scheme aimed at reducing diagnosis time of
distributed small embedded SRAMs (e-SRAMs). This scheme improves the one
proposed in [A parallel built-in self-diagnostic method for embedded memory
buffers, A parallel built-in self-diagnostic method for embedded memory
arrays]. The improvements are mainly two-fold. On one hand, the diagnosis of
time-consuming Data Retention Faults (DRFs), which is neglected by the
diagnosis architecture in [A parallel built-in self-diagnostic method for
embedded memory buffers, A parallel built-in self-diagnostic method for
embedded memory arrays], is now considered and performed via a DFT technique
referred to as the ""No Write Recovery Test Mode (NWRTM)"". On the other hand, a
pair comprising a Serial to Parallel Converter (SPC) and a Parallel to Serial
Converter (PSC) is utilized to replace the bi-directional serial interface, to
avoid the problems of serial fault masking and defect rate dependent diagnosis.
Results from our evaluations show that the proposed diagnosis scheme achieves
an increased diagnosis coverage and reduces diagnosis time compared to those
obtained in [A parallel built-in self-diagnostic method for embedded memory
buffers, A parallel built-in self-diagnostic method for embedded memory
arrays], with neglectable extra area cost."
"The memory subsystem has always been a bottleneck in performance as well as
significant power contributor in memory intensive applications. Many
researchers have presented multi-layered memory hierarchies as a means to
design energy and performance efficient systems. However, most of the previous
work do not explore trade-offs systematically. We fill this gap by proposing a
formalized technique that takes into consideration data reuse, limited lifetime
of the arrays of an application and application specific prefetching
opportunities, and performs a thorough trade-off exploration for different
memory layer sizes. This technique has been implemented on a prototype tool,
which was tested successfully using nine real-life applications of industrial
relevance. Following this approach we have able to reduce execution time up to
60%, and energy consumption up to 70%."
"This paper gives an overview of a new technique, named pseudo-ring testing
(PRT). PRT can be applied for testing wide type of random access memories
(RAM): bit- or word-oriented and single- or dual-port RAM's. An essential
particularity of the proposed methodology is the emulation of a linear
automaton over Galois field by memory own components."
"In this paper we present our contribution in terms of synchronization
processor for a SoC design methodology based on the theory of the latency
insensitive systems (LIS) of Carloni et al. Our contribution consists in IP
encapsulation into a new wrapper model which speed and area are optimized and
synthetizability guarantied. The main benefit of our approach is to preserve
the local IP performances when encapsulating them and reduce SoC silicon area."
"Temperature affects not only the reliability but also the performance, power,
and cost of the embedded system. This paper proposes a thermal-aware task
allocation and scheduling algorithm for embedded systems. The algorithm is used
as a sub-routine for hardware/software co-synthesis to reduce the peak
temperature and achieve a thermally even distribution while meeting real time
constraints. The paper investigates both power-aware and thermal-aware
approaches to task allocation and scheduling. The experimental results show
that the thermal-aware approach outperforms the power-aware schemes in terms of
maximal and average temperature reductions. To the best of our knowledge, this
is the first task allocation and scheduling algorithm that takes temperature
into consideration."
"As feature sizes shrink, it will be necessary to use AAPSM
(Alternating-Aperture Phase Shift Masking) to image critical features,
especially on the polysilicon layer. This imposes additional constraints on the
layouts beyond traditional design rules. Of particular note is the requirement
that all critical features be flanked by opposite-phase shifters, while the
shifters obey minimum width and spacing requirements. A layout is called
phase-assignable if it satisfies this requirement. If a layout is not
phase-assignable, the phase conflicts have to removed to enable the use of
AAPSM for the layout. Previous work has sought to detect a suitable set of
phase Conflicts to be removed, as well as correct them. The contribution of
this paper are the following: (1) a new approach to detect a minimal set of
phase conflicts (also referred to as AAPSM conflicts), which when corrected
will produce a phase-assignable layout; (2) a novel layout modification scheme
for correcting these AAPSM conflicts. The proposed approach for conflict
detection shows significant improvements in the quality of results and runtime
for real industrial circuits, when compared to previous methods. To the best of
our knowledge, this is the first time layout modification results are presented
for bright-field AAPSM. Our experiments show that the percentage area increase
for making a layout phase-assignable ranges from 0.7-11.8%."
"Operating frequency of a pipelined circuit is determined by the delay of the
slowest pipeline stage. However, under statistical delay variation in sub-100nm
technology regime, the slowest stage is not readily identifiable and the
estimation of the pipeline yield with respect to a target delay is a
challenging problem. We have proposed analytical models to estimate yield for a
pipelined design based on delay distributions of individual pipe stages. Using
the proposed models, we have shown that change in logic depth and imbalance
between the stage delays can improve the yield of a pipeline. A statistical
methodology has been developed to optimally design a pipeline circuit for
enhancing yield. Optimization results show that, proper imbalance among the
stage delays in a pipeline improves design yield by 9% for the same area and
performance (and area reduction by about 8.4% under a yield constraint) over a
balanced design."
"Application of Microelectronic to bioanalysis is an emerging field which
holds great promise. From the standpoint of electronic and system design,
biochips imply a radical change of perspective, since new, completely different
constraints emerge while other usual constraints can be relaxed. While
electronic parts of the system can rely on the usual established design-flow,
fluidic and packaging design, calls for a new approach which relies
significantly on experiments. We hereby make some general considerations based
on our experience in the development of biochips for cell analysis."
"On a commercial digital still camera (DSC) controller chip we practice a
novel SOC test integration platform, solving real problems in test scheduling,
test IO reduction, timing of functional test, scan IO sharing, embedded memory
built-in self-test (BIST), etc. The chip has been fabricated and tested
successfully by our approach. Test results justify that short test integration
cost, short test time, and small area overhead can be achieved. To support SOC
testing, a memory BIST compiler and an SOC testing integration system have been
developed."
"We provide a general formulation for the code-based test compression problem
with fixed-length input blocks and propose a solution approach based on
Evolutionary Algorithms. In contrast to existing code-based methods, we allow
unspecified values in matching vectors, which allows encoding of arbitrary test
sets using a relatively small number of code-words. Experimental results for
both stuck-at and path delay fault test sets for ISCAS circuits demonstrate an
improvement compared to existing techniques."
"As the communication requirements of current and future Multiprocessor
Systems on Chips (MPSoCs) continue to increase, scalable communication
architectures are needed to support the heavy communication demands of the
system. This is reflected in the recent trend that many of the standard bus
products such as STbus, have now introduced the capability of designing a
crossbar with multiple buses operating in parallel. The crossbar configuration
should be designed to closely match the application traffic characteristics and
performance requirements. In this work we address this issue of
application-specific design of optimal crossbar (using STbus crossbar
architecture), satisfying the performance requirements of the application and
optimal binding of cores onto the crossbar resources. We present a simulation
based design approach that is based on analysis of actual traffic trace of the
application, considering local variations in traffic rates, temporal overlap
among traffic streams and criticality of traffic streams. Our methodology is
applied to several MPSoC designs and the resulting crossbar platforms are
validated for performance by cycle-accurate SystemC simulation of the designs.
The experimental case studies show large reduction in packet latencies (up to
7x) and large crossbar component savings (up to 3.5x) compared to traditional
design approaches."
"As microfluidics-based biochips become more complex, manufacturing yield will
have significant influence on production volume and product cost. We propose an
interstitial redundancy approach to enhance the yield of biochips that are
based on droplet-based microfluidics. In this design method, spare cells are
placed in the interstitial sites within the microfluidic array, and they
replace neighboring faulty cells via local reconfiguration. The proposed design
method is evaluated using a set of concurrent real-life bioassays."
"Microfluidics-based biochips are soon expected to revolutionize clinical
diagnosis, DNA sequencing, and other laboratory procedures involving molecular
biology. Most microfluidic biochips are based on the principle of continuous
fluid flow and they rely on permanently-etched microchannels, micropumps, and
microvalves. We focus here on the automated design of ""digital"" droplet-based
microfluidic biochips. In contrast to continuous-flow systems, digital
microfluidics offers dynamic reconfigurability; groups of cells in a
microfluidics array can be reconfigured to change their functionality during
the concurrent execution of a set of bioassays. We present a simulated
annealing-based technique for module placement in such biochips. The placement
procedure not only addresses chip area, but it also considers fault tolerance,
which allows a microfluidic module to be relocated elsewhere in the system when
a single cell is detected to be faulty. Simulation results are presented for a
case study involving the polymerase chain reaction."
"CMOS-based sensor array chips provide new and attractive features as compared
to today's standard tools for medical, diagnostic, and biotechnical
applications. Examples for molecule- and cell-based approaches and related
circuit design issues are discussed."
"On-chip buses are typically designed to meet performance constraints at
worst-case conditions, including process corner, temperature, IR-drop, and
neighboring net switching pattern. This can result in significant performance
slack at more typical operating conditions. In this paper, we propose a dynamic
voltage scaling (DVS) technique for buses, based on a double sampling latch
which can detect and correct for delay errors without the need for
retransmission. The proposed approach recovers the available slack at
non-worst-case operating points through more aggressive voltage scaling and
tracks changing conditions by monitoring the error recovery rate. Voltage
margins needed in traditional designs to accommodate worst-case performance
conditions are therefore eliminated, resulting in a significant improvement in
energy efficiency. The approach was implemented for a 6mm memory read bus
operating at 1.5GHz (0.13 $\mu$m technology node) and was simulated for a
number of benchmark programs. Even at the worst-case process and environment
conditions, energy gains of up to 17% are achieved, with error recovery rates
under 2.3%. At more typical process and environment conditions, energy gains
range from 35% to 45%, with a performance degradation under 2%. An analysis of
optimum interconnect architectures for maximizing energy gains with this
approach shows that the proposed approach performs well with technology
scaling."
"As Moore's Law continues to fuel the ability to build ever increasingly
complex system-on-chips (SoCs), achieving performance goals is rising as a
critical challenge to completing designs. In particular, the system
interconnect must efficiently service a diverse set of data flows with widely
ranging quality-of-service (QoS) requirements. However, the known solutions for
off-chip interconnects such as large-scale networks are not necessarily
applicable to the on-chip environment. Latency and memory constraints for
on-chip interconnects are quite different from larger-scale interconnects. This
paper introduces a novel on-chip interconnect arbitration scheme. We show how
this scheme can be distributed across a chip for high-speed implementation. We
compare the performance of the arbitration scheme with other known interconnect
arbitration schemes. Existing schemes typically focus heavily on either low
latency of service for some initiators, or alternatively on guaranteed
bandwidth delivery for other initiators. Our scheme allows service latency on
some initiators to be traded off smoothly against jitter bounds on other
initiators, while still delivering bandwidth guarantees. This scheme is a
subset of the QoS controls that are available in the SonicsMX? (SMX) product."
"Importance of addressing soft errors in both safety critical applications and
commercial consumer products is increasing, mainly due to ever shrinking
geometries, higher-density circuits, and employment of power-saving techniques
such as voltage scaling and component shut-down. As a result, it is becoming
necessary to treat reliability as a first-class citizen in system design. In
particular, reliability decisions taken early in system design can have
significant benefits in terms of design quality. Motivated by this observation,
this paper presents a reliability-centric high-level synthesis approach that
addresses the soft error problem. The proposed approach tries to maximize
reliability of the design while observing the bounds on area and performance,
and makes use of our reliability characterization of hardware components such
as adders and multipliers. We implemented the proposed approach, performed
experiments with several designs, and compared the results with those obtained
by a prior proposal."
"The design of reliable circuits has received a lot of attention in the past,
leading to the definition of several design techniques introducing fault
detection and fault tolerance properties in systems for critical
applications/environments. Such design methodologies tackled the problem at
different abstraction levels, from switch-level to logic, RT level, and more
recently to system level. Aim of this paper is to introduce a novel
system-level technique based on the redefinition of the operators functionality
in the system specification. This technique provides reliability properties to
the system data path, transparently with respect to the designer. Feasibility,
fault coverage, performance degradation and overheads are investigated on a FIR
circuit."
"Many SOCs today contain both digital and analog embedded cores. Even though
the test cost for such mixed-signal SOCs is significantly higher than that for
digital SOCs, most prior research in this area has focused exclusively on
digital cores. We propose a low-cost test development methodology for
mixed-signal SOCs that allows the analog and digital cores to be tested in a
unified manner, thereby minimizing the overall test cost. The analog cores in
the SOC are wrapped such that they can be accessed using a digital test access
mechanism (TAM). We evaluate the impact of the use of analog test wrappers on
area overhead and test time. To reduce area overhead, we present an analog test
wrapper optimization technique, which is then combined with TAM optimization in
a cost-oriented heuristic approach for test scheduling. We also demonstrate the
feasibility of using analog wrappers by presenting transistor-level simulations
for an analog wrapper and a representative core. We present experimental
results on test scheduling for an ITC'02 benchmark SOC that has been augmented
with five analog cores."
"Multi-site testing is a popular and effective way to increase test throughput
and reduce test costs. We present a test throughput model, in which we focus on
wafer testing, and consider parameters like test time, index time,
abort-on-fail, and contact yield. Conventional multi-site testing requires
sufficient ATE resources, such as ATE channels, to allow to test multiple SOCs
in parallel. In this paper, we design and optimize on-chip DfT, in order to
maximize the test throughput for a given SOC and ATE. The on-chip DfT consists
of an E-RPCT wrapper, and, for modular SOCs, module wrappers and TAMs. We
present experimental results for a Philips SOC and several ITC'02 SOC Test
Benchmarks."
"Triple Modular Redundancy (TMR) is a suitable fault tolerant technique for
SRAM-based FPGA. However, one of the main challenges in achieving 100%
robustness in designs protected by TMR running on programmable platforms is to
prevent upsets in the routing from provoking undesirable connections between
signals from distinct redundant logic parts, which can generate an error in the
output. This paper investigates the optimal design of the TMR logic (e.g., by
cleverly inserting voters) to ensure robustness. Four different versions of a
TMR digital filter were analyzed by fault injection. Faults were randomly
inserted straight into the bitstream of the FPGA. The experimental results
presented in this paper demonstrate that the number and placement of voters in
the TMR design can directly affect the fault tolerance, ranging from 4.03% to
0.98% the number of upsets in the routing able to cause an error in the TMR
circuit."
"Buffer insertion is a popular technique to reduce the interconnect delay. The
classic buffer insertion algorithm of van Ginneken has time complexity O(n^2),
where n is the number of buffer positions. Lillis, Cheng and Lin extended van
Ginneken's algorithm to allow b buffer types in time O (b^2 n^2). For modern
design libraries that contain hundreds of buffers, it is a serious challenge to
balance the speed and performance of the buffer insertion algorithm. In this
paper, we present a new algorithm that computes the optimal buffer insertion in
O (bn^2) time. The reduction is achieved by the observation that the (Q, C)
pairs of the candidates that generate the new candidates must form a convex
hull. On industrial test cases, the new algorithm is faster than the previous
best buffer insertion algorithms by orders of magnitude."
"Single-chip CMOS-based biosensors that feature microcantilevers as transducer
elements are presented. The cantilevers are functionalized for the capturing of
specific analytes, e.g., proteins or DNA. The binding of the analyte changes
the mechanical properties of the cantilevers such as surface stress and
resonant frequency, which can be detected by an integrated Wheatstone bridge.
The monolithic integrated readout allows for a high signal-to-noise ratio,
lowers the sensitivity to external interference and enables autonomous device
operation."
"This paper presents the effectiveness of various stress conditions (mainly
voltage and frequency) on detecting the resistive shorts and open defects in
deep sub-micron embedded memories in an industrial environment. Simulation
studies on very-low voltage, high voltage and at-speed testing show the need of
the stress conditions for high quality products; i.e., low defect-per-million
(DPM) level, which is driving the semiconductor market today. The above test
conditions have been validated to screen out bad devices on real silicon (a
test-chip) built on CMOS 0.18 um technology. IFA (inductive fault analysis)
based simulation technique leads to an efficient fault coverage and DPM
estimator, which helps the customers upfront to make decisions on test
algorithm implementations under different stress conditions in order to reduce
the number of test escapes."
"The increased dominance of intra-die process variations has motivated the
field of Statistical Static Timing Analysis (SSTA) and has raised the need for
SSTA-based circuit optimization. In this paper, we propose a new sensitivity
based, statistical gate sizing method. Since brute-force computation of the
change in circuit delay distribution to gate size change is computationally
expensive, we propose an efficient and exact pruning algorithm. The pruning
algorithm is based on a novel theory of perturbation bounds which are shown to
decrease as they propagate through the circuit. This allows pruning of gate
sensitivities without complete propagation of their perturbations. We apply our
proposed optimization algorithm to ISCAS benchmark circuits and demonstrate the
accuracy and efficiency of the proposed method. Our results show an improvement
of up to 10.5% in the 99-percentile circuit delay for the same circuit area,
using the proposed statistical optimizer and a run time improvement of up to
56x compared to the brute-force approach."
"This paper presents a technique for eliminating redundant cache-tag and
cache-way accesses to reduce power consumption. The basic idea is to keep a
small number of Most Recently Used (MRU) addresses in a Memory Address Buffer
(MAB) and to omit redundant tag and way accesses when there is a MAB-hit. Since
the approach keeps only tag and set-index values in the MAB, the energy and
area overheads are relatively small even for a MAB with a large number of
entries. Furthermore, the approach does not sacrifice the performance. In other
words, neither the cycle time nor the number of executed cycles increases. The
proposed technique has been applied to Fujitsu VLIW processor (FR-V) and its
power saving has been estimated using NanoSim. Experiments for 32kB 2-way set
associative caches show the power consumption of I-cache and D-cache can be
reduced by 40% and 50%, respectively."
"Coarse-grained reconfigurable architectures aim to achieve both goals of high
performance and flexibility. However, existing reconfigurable array
architectures require many resources without considering the specific
application domain. Functional resources that take long latency and/or large
area can be pipelined and/or shared among the processing elements. Therefore
the hardware cost and the delay can be effectively reduced without any
performance degradation for some application domains. We suggest such
reconfigurable array architecture template and design space exploration flow
for domain-specific optimization. Experimental results show that our approach
is much more efficient both in performance and area compared to existing
reconfigurable architectures."
"Field programmable gate arrays (FPGAs) provide designers with the ability to
quickly create hardware circuits. Increases in FPGA configurable logic capacity
and decreasing FPGA costs have enabled designers to more readily incorporate
FPGAs in their designs. FPGA vendors have begun providing configurable soft
processor cores that can be synthesized onto their FPGA products. While FPGAs
with soft processor cores provide designers with increased flexibility, such
processors typically have degraded performance and energy consumption compared
to hard-core processors. Previously, we proposed warp processing, a technique
capable of optimizing a software application by dynamically and transparently
re-implementing critical software kernels as custom circuits in on-chip
configurable logic. In this paper, we study the potential of a MicroBlaze
soft-core based warp processing system to eliminate the performance and energy
overhead of a soft-core processor compared to a hard-core processor. We
demonstrate that the soft-core based warp processor achieves average speedups
of 5.8 and energy reductions of 57% compared to the soft core alone. Our data
shows that a soft-core based warp processor yields performance and energy
consumption competitive with existing hard-core processors, thus expanding the
usefulness of soft processor cores on FPGAs to a broader range of applications."
"This paper presents an infrastructure to test the functionality of the
specific architectures output by a high-level compiler targeting dynamically
reconfigurable hardware. It results in a suitable scheme to verify the
architectures generated by the compiler, each time new optimization techniques
are included or changes in the compiler are performed. We believe this kind of
infrastructure is important to verify, by functional simulation, further
research techniques, as far as compilation to Field-Programmable Gate Array
(FPGA) platforms is concerned."
"In this paper, we present a methodology for customized communication
architecture synthesis that matches the communication requirements of the
target application. This is an important problem, particularly for
network-based implementations of complex applications. Our approach is based on
using frequently encountered generic communication primitives as an alphabet
capable of characterizing any given communication pattern. The proposed
algorithm searches through the entire design space for a solution that
minimizes the system total energy consumption, while satisfying the other
design constraints. Compared to the standard mesh architecture, the customized
architecture generated by the newly proposed approach shows about 36%
throughput increase and 51% reduction in the energy required to encrypt 128
bits of data with a standard encryption algorithm."
"This special session adresses the problems that designers face when
implementing analog and digital circuits in nanometer technologies. An
introductory embedded tutorial will give an overview of the design problems at
hand : the leakage power and process variability and their implications for
digital circuits and memories, and the reducing supply voltages, the design
productivity and signal integrity problems for embedded analog blocks. Next, a
panel of experts from both industrial semiconductor houses and design
companies, EDA vendors and research institutes will present and discuss with
the audience their opinions on whether the design road ends at marker ""65nm"" or
not."
"This paper presents a novel FPGA architecture for implementing various styles
of asynchronous logic. The main objective is to break the dependency between
the FPGA architecture dedicated to asynchronous logic and the logic style. The
innovative aspects of the architecture are described. Moreover the structure is
well suited to be rebuilt and adapted to fit with further asynchronous logic
evolutions thanks to the architecture genericity. A full-adder was implemented
in different styles of logic to show the architecture flexibility."
"In this paper, we present an accurate but very fast soft error rate (SER)
estimation technique for digital circuits based on error propagation
probability (EPP) computation. Experiments results and comparison of the
results with the random simulation technique show that our proposed method is
on average within 6% of the random simulation method and four to five orders of
magnitude faster."
"A new approach for enhancing the process-variation tolerance of digital
circuits is described. We extend recent advances in statistical timing analysis
into an optimization framework. Our objective is to reduce the performance
variance of a technology-mapped circuit where delays across elements are
represented by random variables which capture the manufacturing variations. We
introduce the notion of statistical critical paths, which account for both
means and variances of performance variation. An optimization engine is used to
size gates with a goal of reducing the timing variance along the statistical
critical paths. We apply a pair of nested statistical analysis methods
deploying a slower more accurate approach for tracking statistical critical
paths and a fast engine for evaluation of gate size assignments. We derive a
new approximation for the max operation on random variables which is deployed
for the faster inner engine. Circuit optimization is carried out using a
gain-based algorithm that terminates when constraints are satisfied or no
further improvements can be made. We show optimization results that demonstrate
an average of 72% reduction in performance variation at the expense of average
20% increase in design area."
"With the scaling of technology and higher requirements on performance and
functionality, power dissipation is becoming one of the major design
considerations in the development of network processors. In this paper, we use
an assertion-based methodology for system-level power/performance analysis to
study two dynamic voltage scaling (DVS) techniques, traffic-based DVS and
execution-based DVS, in a network processor model. Using the automatically
generated distribution analyzers, we analyze the power and performance
distributions and study their trade-offs for the two DVS policies with
different parameter settings such as threshold values and window sizes. We
discuss the optimal configurations of the two DVS policies under different
design requirements. By a set of experiments, we show that the assertion-based
trace analysis methodology is an efficient tool that can help a designer easily
compare and study optimal architectural configurations in a large design space."
"As device sizes shrink and current densities increase, the probability of
device failures due to gate oxide breakdown (OBD) also increases. To provide
designs that are tolerant to such failures, we must investigate and understand
the manifestations of this physical phenomenon at the circuit and system level.
In this paper, we develop a model for operational OBD defects, and we explore
how to test for faults due to OBD. For a NAND gate, we derive the necessary
input conditions that excite and detect errors due to OBD defects at the gate
level. We show that traditional pattern generators fail to exercise all of
these defects. Finally, we show that these test patterns can be propagated and
justified for a combinational circuit in a manner similar to traditional ATPG."
"FPGAs, as computing devices, offer significant speedup over microprocessors.
Furthermore, their configurability offers an advantage over traditional ASICs.
However, they do not yet enjoy high-level language programmability, as
microprocessors do. This has become the main obstacle for their wider
acceptance by application designers. ROCCC is a compiler designed to generate
circuits from C source code to execute on FPGAs, more specifically on CSoCs. It
generates RTL level HDLs from frequently executing kernels in an application.
In this paper, we describe ROCCC's system overview and focus on its data path
generation. We compare the performance of ROCCC-generated VHDL code with that
of Xilinx IPs. The synthesis result shows that ROCCC-generated circuit takes
around 2x ~ 3x area and runs at comparable clock rate."
"This paper presents the novel idea of multi-placement structures, for a fast
and optimized placement instantiation in analog circuit synthesis. These
structures need to be generated only once for a specific circuit topology. When
used in synthesis, these pre-generated structures instantiate various layout
floorplans for various sizes and parameters of a circuit. Unlike procedural
layout generators, they enable fast placement of circuits while keeping the
quality of the placements at a high level during a synthesis process. The fast
placement is a result of high speed instantiation resulting from the efficiency
of the multi-placement structure. The good quality of placements derive from
the extensive and intelligent search process that is used to build the
multi-placement structure. The target benchmarks of these structures are analog
circuits in the vicinity of 25 modules. An algorithm for the generation of such
multi-placement structures is presented. Experimental results show placement
execution times with an average of a few milliseconds making them usable during
layout-aware synthesis for optimized placements."
"Testing a non-digital integrated system against all of its specifications can
be quite expensive due to the elaborate test application and measurement setup
required. We propose to eliminate redundant tests by employing e-SVM based
statistical learning. Application of the proposed methodology to an operational
amplifier and a MEMS accelerometer reveal that redundant tests can be
statistically identified from a complete set of specification-based tests with
negligible error. Specifically, after eliminating five of eleven
specification-based tests for an operational amplifier, the defect escape and
yield loss is small at 0.6% and 0.9%, respectively. For the accelerometer,
defect escape of 0.2% and yield loss of 0.1% occurs when the hot and colt tests
are eliminated. For the accelerometer, this level of Compaction would reduce
test cost by more than half."
"Nanometer circuits are becoming increasingly susceptible to soft-errors due
to alpha-particle and atmospheric neutron strikes as device scaling reduces
node capacitances and supply/threshold voltage scaling reduces noise margins.
It is becoming crucial to add soft-error tolerance estimation and optimization
to the design flow to handle the increasing susceptibility. The first part of
this paper presents a tool for accurate soft-error tolerance analysis of
nanometer circuits (ASERTA) that can be used to estimate the soft-error
tolerance of nanometer circuits consisting of millions of gates. The tolerance
estimates generated by the tool match SPICE generated estimates closely while
taking orders of magnitude less computation time. The second part of the paper
presents a tool for soft-error tolerance optimization of nanometer circuits
(SERTOPT) using the tolerance estimates generated by ASERTA. The tool finds
optimal sizes, channel lengths, supply voltages and threshold voltages to be
assigned to gates in a combinational circuit such that the soft-error tolerance
is increased while meeting the timing constraint. Experiments on ISCAS'85
benchmark circuits showed that soft-error rate of the optimized circuit
decreased by as much as 47% with marginal increase in circuit delay."
"An analogue testing standard IEEE 1149.4 is mainly targeted for low-frequency
testing. The problem studied in this paper is extending the standard also for
radio frequency testing. IEEE 1149.4 compatible measurement structures (ABMs)
developed in this study extract the information one is measuring from the radio
frequency signal and represent the result as a DC voltage level. The ABMs
presented in this paper are targeted for power and frequency measurements
operating in frequencies from 1 GHz to 2 GHz. The power measurement error
caused by temperature, supply voltage and process variations is roughly 2 dB
and the frequency measurement error is 0.1 GHz, respectively."
"This paper suggests a practical ""hybrid"" synthesis methodology which
integrates designer-derived analytical models for system-level description with
simulation-based models at the circuit level. We show how to optimize
stage-resolution to minimize the power in a pipelined ADC. Exploration (via
detailed synthesis) of several ADC configurations is used to show that a
4-3-2... resolution distribution uses the least power for a 13-bit 40 MSPS
converter in a 0.25 $\mu$m CMOS process."
"The emerging concept of SoC-AMS leads to research new top-down methodologies
to aid systems designers in sizing analog and mixed devices. This work applies
this idea to the high-level optimization of pipeline ADC. Considering a given
technology, it consists in comparing different configurations according to
their imperfections and their architectures without FFT computation or
time-consuming simulations. The final selection is based on a figure of merit."
"We present a complete top-down design of a low-power multi-channel clock
recovery circuit based on gated current-controlled oscillators. The flow
includes several tools and methods used to specify block constraints, to design
and verify the topology down to the transistor level, as well as to achieve a
power consumption as low as 5mW/Gbit/s. Statistical simulation is used to
estimate the achievable bit error rate in presence of phase and frequency
errors and to prove the feasibility of the concept. VHDL modeling provides
extensive verification of the topology. Thermal noise modeling based on
well-known concepts delivers design parameters for the device sizing and
biasing. We present two practical examples of possible design improvements
analyzed and implemented with this methodology."
"As the scale of electronic devices shrinks, ""electronic textiles""
(e-textiles) will make possible a wide variety of novel applications which are
currently unfeasible. Due to the wearability concerns, low-power techniques are
critical for e-textile applications. In this paper, we address the issue of the
energy-aware routing for e-textile platforms and propose an efficient algorithm
to solve it. The platform we consider consists of dedicated components for
e-textiles, including computational modules, dedicated transmission lines and
thin-film batteries on fiber substrates. Furthermore, we derive an analytical
upper bound for the achievable number of jobs completed over all possible
routing strategies. From a practical standpoint, for the Advanced Encryption
Standard (AES) cipher, the routing technique we propose achieves about fifty
percent of this analytical upper bound. Moreover, compared to the
non-energy-aware counterpart, our routing technique increases the number of
encryption jobs completed by one order of magnitude."
"In nanometer scaled CMOS devices significant increase in the subthreshold,
the gate and the reverse biased junction band-to-band-tunneling (BTBT) leakage,
results in the large increase of total leakage power in a logic circuit.
Leakage components interact with each other in device level (through device
geometry, doping profile) and also in the circuit level (through node
voltages). Due to the circuit level interaction of the different leakage
components, the leakage of a logic gate strongly depends on the circuit
topology i.e. number and nature of the other logic gates connected to its input
and output. In this paper, for the first time, we have analyzed loading effect
on leakage and proposed a method to accurately estimate the total leakage in a
logic circuit, from its logic level description considering the impact of
loading and transistor stacking."
"On-chip networks have been proposed as the interconnect fabric for future
systems-on-chip and multi-processors on chip. Power is one of the main
constraints of these systems and interconnect consumes a significant portion of
the power budget. In this paper, we propose four leakage-aware interconnect
schemes. Our schemes achieve 10.13%~63.57% active leakage savings and
12.35%~95.96% standby leakage savings across schemes while the delay penalty
ranges from 0% to 4.69%."
"In this paper we present a simple and efficient built-in temperature sensor
for thermal monitoring of standard-cell based VLSI circuits. The proposed smart
temperature sensor uses a ring-oscillator composed of complex gates instead of
inverters to optimize their linearity. Simulation results from a 0.18$\mu$m
CMOS technology show that the non-linearity error of the sensor can be reduced
when an adequate set of standard logic gates is selected."
"Test sets that detect each target fault n times (n-detection test sets) are
typically generated for restricted values of n due to the increase in test set
size with n. We perform both a worst-case analysis and an average-case analysis
to check the effect of restricting n on the unmodeled fault coverage of an
(arbitrary) n-detection test set. Our analysis is independent of any particular
test set or test generation approach. It is based on a specific set of target
faults and a specific set of untargeted faults. It shows that, depending on the
circuit, very large values of n may be needed to guarantee the detection of all
the untargeted faults. We discuss the implications of these results."
"The embedded DRAM (eDRAM) is more and more used in System On Chip (SOC). The
integration of the DRAM capacitor process into a logic process is challenging
to get satisfactory yields. The specific process of DRAM capacitor and the low
capacitance value (~30F) of this device induce problems of process monitoring
and failure analysis. We propose a new test structure to measure the
capacitance value of each DRAM cell capacitor in a DRAM array. This concept has
been validated by simulation on a 0.18$\mu$m eDRAM technology."
"Complex applications implemented as Systems on Chip (SoCs) demand extensive
use of system level modeling and validation. Their implementation gathers a
large number of complex IP cores and advanced interconnection schemes, such as
hierarchical bus architectures or networks on chip (NoCs). Modeling
applications involves capturing its computation and communication
characteristics. Previously proposed communication weighted models (CWM)
consider only the application communication aspects. This work proposes a
communication dependence and computation model (CDCM) that can simultaneously
consider both aspects of an application. It presents a solution to the problem
of mapping applications on regular NoCs while considering execution time and
energy consumption. The use of CDCM is shown to provide estimated average
reductions of 40% in execution time, and 20% in energy consumption, for current
technologies."
"In this paper, we present power emulation, a novel design paradigm that
utilizes hardware acceleration for the purpose of fast power estimation. Power
emulation is based on the observation that the functions necessary for power
estimation (power model evaluation, aggregation, etc.) can be implemented as
hardware circuits. Therefore, we can enhance any given design with ""power
estimation hardware"", map it to a prototyping platform, and exercise it with
any given test stimuli to obtain power consumption estimates. Our empirical
studies with industrial designs reveal that power emulation can achieve
significant speedups (10X to 500X) over state-of-the-art commercial
register-transfer level (RTL) power estimation tools."
"Memory cores are usually the densest portion with the smallest feature size
in system-on-chip (SOC) designs. The reliability of memory cores thus has heavy
impact on the reliability of SOCs. Transparent test is one of useful technique
for improving the reliability of memories during life time. This paper presents
a systematic algorithm used for transforming a bit-oriented march test into a
transparent word-oriented march test. The transformed transparent march test
has shorter test complexity compared with that proposed in the previous works
[Theory of transparent BIST for RAMs, A transparent online memory test for
simultaneous detection of functional faults and soft errors in memories]. For
example, if a memory with 32-bit words is tested with March C-, time complexity
of the transparent word-oriented test transformed by the proposed scheme is
only about 56% or 19% time complexity of the transparent word-oriented test
converted by the scheme reported in [Theory of transparent BIST for RAMs] or [A
transparent online memory test for simultaneous detection of functional faults
and soft errors in memories], respectively."
"This paper gives an overview of a transaction level modeling (TLM) design
flow for straightforward embedded system design with SystemC. The goal is to
systematically develop both application-specific HW and SW components of an
embedded system using the TLM approach, thus allowing for fast communication
architecture exploration, rapid prototyping and early embedded SW development.
To this end, we specify the lightweight transaction-based communication
protocol SHIP and present a methodology for automatic mapping of the
communication part of a system to a given architecture, including HW/SW
interfaces."
"Safety-critical embedded systems having to meet real-time constraints are
expected to be highly predictable in order to guarantee at design time that
certain timing deadlines will always be met. This requirement usually prevents
designers from utilizing caches due to their highly dynamic, thus hardly
predictable behavior. The integration of scratchpad memories represents an
alternative approach which allows the system to benefit from a performance gain
comparable to that of caches while at the same time maintaining predictability.
In this work, we compare the impact of scratchpad memories and caches on worst
case execution time (WCET) analysis results. We show that caches, despite
requiring complex techniques, can have a negative impact on the predicted WCET,
while the estimated WCET for scratchpad memories scales with the achieved
Performance gain at no extra analysis cost."
"Research studies have demonstrated the feasibility and advantages of
Network-on-Chip (NoC) over traditional bus-based architectures but have not
focused on compatibility communication standards. This paper describes a number
of issues faced when designing a VC-neutral NoC, i.e. compatible with standards
such as AHB 2.0, AXI, VCI, OCP, and various other proprietary protocols, and
how a layered approach to communication helps solve these issues."
"Very deep submicron and nanometer technologies have increased notably
integrated circuit (IC) sensitiveness to radiation. Soft errors are currently
appearing into ICs working at earth surface. Hardened circuits are currently
required in many applications where Fault Tolerance (FT) was not a requirement
in the very near past. The use of platform FPGAs for the emulation of
single-event upset effects (SEU) is gaining attention in order to speed up the
FT evaluation. In this work, a new emulation system for FT evaluation with
respect to SEU effects is proposed, providing shorter evaluation times by
performing all the evaluation process in the FPGA and avoiding emulator-host
communication bottlenecks."
"As technology scales down, the static power is expected to become a
significant fraction of the total power. The exponential dependence of static
power with the operating temperature makes the thermal profile estimation of
high-performance ICs a key issue to compute the total power dissipated in
next-generations. In this paper we present accurate and compact analytical
models to estimate the static power dissipation and the temperature of
operation of CMOS gates. The models are the fundamentals of a performance
estimation tool in which numerical procedures are avoided for any computation
to set a faster estimation and optimization. The models developed are compared
to measurements and SPICE simulations for a 0.12mm technology showing excellent
results."
"Low power oriented circuit optimization consists in selecting the best
alternative between gate sizing, buffer insertion and logic structure
transformation, for satisfying a delay constraint at minimum area cost. In this
paper we used a closed form model of delay in CMOS structures to define metrics
for a deterministic selection of the optimization alternative. The target is
delay constraint satisfaction with minimum area cost. We validate the design
space exploration method, defining maximum and minimum delay bounds on logical
paths. Then we adapt this method to a ""constant sensitivity method"" allowing to
size a circuit at minimum area under a delay constraint. An optimisation
protocol is finally defined to manage the trade-off performance constraint -
circuit structure. These methods are implemented in an optimization tool (POPS)
and validated by comparing on a 0.25$\mu$m process, the optimization efficiency
obtained on various benchmarks (ISCAS?85) to that resulting from an industrial
tool."
"This paper describes two research projects that develop new low-cost
techniques for testing devices with multiple high-speed (2 to 5 Gbps) signals.
Each project uses commercially available components to keep costs low, yet
achieves performance characteristics comparable to (and in some ways exceeding)
more expensive ATE. A common CMOS FPGA-based logic core provides flexibility,
adaptability, and communication with controlling computers while customized
positive emitter-coupled logic (PECL) achieves multi-gigahertz data rates with
about $\pm$25ps timing accuracy."
"This paper presents a design flow for an improved selective
multi-threshold(Selective-MT) circuit. The Selective-MT circuit is improved so
that plural MT-cells can share one switch transistor. We propose the design
methodology from RTL(Register Transfer Level) to final layout with optimizing
switch transistor structure."
"This paper addresses delay test for SOC devices with high frequency clock
domains. A logic design for on-chip high-speed clock generation, implemented to
avoid expensive test equipment, is described in detail. Techniques for on-chip
clock generation, meant to reduce test vector count and to increase test
quality, are discussed. ATPG results for the proposed techniques are given."
"Many existing thermal management techniques focus on reducing the overall
power consumption of the chip, and do not address location-specific temperature
problems referred to as hotspots. We propose the use of dynamic runtime
reconfiguration to shift the hotspot-inducing computation periodically and make
the thermal profile more uniform. Our analysis shows that dynamic
reconfiguration is an effective technique in reducing hotspots for NoCs."
"In this paper, we investigate the impact of T_{ox} and Vth on power
performance trade-offs for on-chip caches. We start by examining the
optimization of the various components of a single level cache and then extend
this to two level cache systems. In addition to leakage, our studies also
account for the dynamic power expanded as a result of cache misses. Our results
show that one can often reduce overall power by increasing the size of the L2
cache if we only allow one pair of Vth/T_{ox} in L2. However, if we allow the
memory cells and the peripherals to have their own Vth's and T_{ox}'s, we show
that a two-level cache system with smaller L2's will yield less total leakage.
We further show that two Vth's and two T_{ox}'s are sufficient to get close to
an optimal solution, and that Vth is generally a better design knob than T_{ox}
for leakage optimization, thus it is better to restrict the number of T_{ox}'s
rather than Vth's if cost is a concern."
"The increasing complexity and the short life cycles of embedded systems are
pushing the current system-on-chip designs towards a rapid increasing on the
number of programmable processing units, while decreasing the gate count for
custom logic. Considering this trend, this work proposes a test planning method
capable of reusing available processors as test sources and sinks, and the
on-chip network as the test access mechanism. Experimental results are based on
ITC'02 benchmarks and on two open core processors compliant with MIPS and SPARC
instruction set. The results show that the cooperative use of both the on-chip
network and the embedded processors can increase the test parallelism and
reduce the test time without additional cost in area and pins."
"Due to the emergence of highly dynamic multimedia applications there is a
need for flexible platforms and run-time scheduling support for embedded
systems. Dynamic Reconfigurable Hardware (DRHW) is a promising candidate to
provide this flexibility but, currently, not sufficient run-time scheduling
support to deal with the run-time reconfigurations exists. Moreover, executing
at run-time a complex scheduling heuristic to provide this support may generate
an excessive run-time penalty. Hence, we have developed a hybrid
design/run-time prefetch heuristic that schedules the reconfigurations at
run-time, but carries out the scheduling computations at design-time by
carefully identifying a set of near-optimal schedules that can be selected at
run-time. This approach provides run-time flexibility with a negligible
penalty."
"Early scheduling algorithms usually adjusted the clock cycle duration to the
execution time of the slowest operation. This resulted in large slack times
wasted in those cycles executing faster operations. To reduce the wasted times
multi-cycle and chaining techniques have been employed. While these techniques
have produced successful designs, its effectiveness is often limited due to the
area increment that may derive from chaining, and the extra latencies that may
derive from multicycling. In this paper we present an optimization method that
solves the time-constrained scheduling problem by transforming behavioural
specifications into new ones whose subsequent synthesis substantially improves
circuit performance. Our proposal breaks up some of the specification
operations, allowing their execution during several possibly unconsecutive
cycles, and also the calculation of several data-dependent operation fragments
in the same cycle. To do so, it takes into account the circuit latency and the
execution time of every specification operation. The experimental results
carried out show that circuits obtained from the optimized specification are on
average 60% faster than those synthesized from the original specification, with
only slight increments in the circuit area."
"The idea of design domain specific Mother Model of IP block family as a base
of modeling of system integration is presented here. A common reconfigurable
Mother Model for ten different standardized digital OFDM transmitters has been
developed. By means of a set of parameters, the mother model can be
reconfigured to any of the ten selected standards. So far the applicability of
the proposed reconfiguration and analog-digital co-modeling methods have been
proved by modeling the function of the digital parts of three, 802.11a, ADSL
and DRM, transmitters in an RF system simulator. The model is intended to be
used as signal source template in RF system simulations. The concept is not
restricted to signal sources, it can be applied to any IP block development.
The idea of the Mother Model will be applied in other design domains to prove
that in certain application areas, OFDM transceivers in this case, the design
process can progress simultaneously in different design domains - mixed signal,
system and RTL-architectural - without the need of high-level synthesis. Only
the Mother Models of three design domains are needed to be formally proved to
function as specified."
"This paper presents a digital VLSI design flow to create secure, side-channel
attack (SCA) resistant integrated circuits. The design flow starts from a
normal design in a hardware description language such as VHDL or Verilog and
provides a direct path to a SCA resistant layout. Instead of a full custom
layout or an iterative design process with extensive simulations, a few key
modifications are incorporated in a regular synchronous CMOS standard cell
design flow. We discuss the basis for side-channel attack resistance and adjust
the library databases and constraints files of the synthesis and place & route
procedures accordingly. Experimental results show that a DPA attack on a
regular single ended CMOS standard cell implementation of a module of the DES
algorithm discloses the secret key after 200 measurements. The same attack on a
secure version still does not disclose the secret key after more than 2000
measurements."
"Transaction Level Modeling (TLM) approach is used to meet the simulation
speed as well as cycle accuracy for large scale SoC performance analysis. We
implemented a transaction-level model of a proprietary bus called AHB+ which
supports an extended AMBA2.0 protocol. The AHB+ transaction-level model shows
353 times faster than pin-accurate RTL model while maintaining 97% of accuracy
on average. We also present the development procedure of TLM of a bus
architecture."
"The algorithms used in wireless applications are increasingly more
sophisticated and consequently more challenging to implement in hardware.
Traditional design flows require developing the micro architecture, coding the
RTL, and verifying the generated RTL against the original functional C or
MATLAB specification. This paper describes a C-based design flow that is well
suited for the hardware implementation of DSP algorithms commonly found in
wireless applications. The C design flow relies on guided synthesis to generate
the RTL directly from the untimed C algorithm. The specifics of the C-based
design flow are described using a simple DSP filtering algorithm consisting of
a forward adaptive equalizer, a 64-QAM slicer and an adaptive decision feedback
equalizer. The example illustrates some of the capabilities and advantages
offered by this flow."
"The JPEG2000 standard defines the discrete wavelet transform (DWT) as a
linear space-to-frequency transform of the image domain in an irreversible
compression. This irreversible discrete wavelet transform is implemented by FIR
filter using 9/7 Daubechies coefficients or a lifting scheme of factorizated
coefficients from 9/7 Daubechies coefficients. This work investigates the
tradeoffs between area, power and data throughput (or operating frequency) of
several implementations of the Discrete Wavelet Transform using the lifting
scheme in various pipeline designs. This paper shows the results of five
different architectures synthesized and simulated in FPGAs. It concludes that
the descriptions with pipelined operators provide the best area-power-operating
frequency trade-off over non-pipelined operators descriptions. Those
descriptions require around 40% more hardware to increase the maximum operating
frequency up to 100% and reduce power consumption to less than 50%. Starting
from behavioral HDL descriptions provide the best area-power-operating
frequency trade-off, improving hardware cost and maximum operating frequency
around 30% in comparison to structural descriptions for the same power
requirement."
"One of the main bottlenecks when designing a network processing system is
very often its memory subsystem. This is mainly due to the state-of-the-art
network links operating at very high speeds and to the fact that in order to
support advanced Quality of Service (QoS), a large number of independent queues
is desirable. In this paper we analyze the performance bottlenecks of various
data memory managers integrated in typical Network Processing Units (NPUs). We
expose the performance limitations of software implementations utilizing the
RISC processing cores typically found in most NPU architectures and we identify
the requirements for hardware assisted memory management in order to achieve
wire-speed operation at gigabit per second rates. Furthermore, we describe the
architecture and performance of a hardware memory manager that fulfills those
requirements. This memory manager, although it is implemented in a
reconfigurable technology, it can provide up to 6.2Gbps of aggregate
throughput, while handling 32K independent queues."
"This paper briefly describes the picoArray? architecture, and in particular
the deterministic internal communication fabric. The methods that have been
developed for debugging and verifying systems using devices from the picoArray
family are explained. In order to maximize the computational ability of these
devices, hardware debugging support has been kept to a minimum and the methods
and tools developed to take this into account."
"Customization of processor architectures through Instruction Set Extensions
(ISEs) is an effective way to meet the growing performance demands of embedded
applications. A high-quality ISE generation approach needs to obtain results
close to those achieved by experienced designers, particularly for complex
applications that exhibit regularity: expert designers are able to exploit
manually such regularity in the data flow graphs to generate high-quality ISEs.
In this paper, we present ISEGEN, an approach that identifies high-quality ISEs
by iterative improvement following the basic principles of the well-known
Kernighan-Lin (K-L) min-cut heuristic. Experimental results on a number of
MediaBench, EEMBC and cryptographic applications show that our approach matches
the quality of the optimal solution obtained by exhaustive search. We also show
that our ISEGEN technique is on average 20x faster than a genetic formulation
that generates equivalent solutions. Furthermore, the ISEs identified by our
technique exhibit 35% more speedup than the genetic solution on a large
cryptographic application (AES) by effectively exploiting its regular
structure."
"With growing computational needs of many real-world applications, frequently
changing specifications of standards, and the high design and NRE costs of
ASICs, an algorithm-agile FPGA based co-processor has become a viable
alternative. In this article, we report about the general design of an
algorith-agile co-processor and the proof-of-concept implementation."
"The importance of embedded systems in driving innovation in automotive
applications continues to grow. Understanding the specific needs of developers
targeting this market is also helping to drive innovation in RISC core design.
This paper describes how a RISC instruction set architecture has evolved to
better meet those needs, and the key implementation features in two very
different RISC cores are used to demonstrate the challenges of designing for
real-time automotive systems."
"This paper presents an innovative application of EDAA - European design and
Automation Association 1149.4 and the Integrated Diagnostic Reconfiguration
(IDR) as tools for the implementation of an embedded test solution for an
Automotive Electronic Control Unit implemented as a fully integrated mixed
signal system. The paper described how the test architecture can be used for
fault avoidance with results from a hardware prototype presented. The paper
concludes that fault avoidance can be integrated into mixed signal electronic
systems to handle key failure modes."
"The introduction of complex SoCs with multiple processor cores presents new
development challenges, such that development support is now a decisive factor
when choosing a System-on-Chip (SoC). The presented developments support
strategy addresses the challenges using both architecture and technology
approaches. The Multi-Core Debug Support (MCDS) architecture provides flexible
triggering using cross triggers and a multiple core break and suspend switch.
Temporal trace ordering is guaranteed down to cycle level by on-chip time
stamping. The Package Sized-ICE (PSI) approach is a novel method of including
trace buffers, overlay memories, processing resources and communication
interfaces without changing device behavior. PSI requires no external emulation
box, as the debug host interfaces directly with the SoC using a standard
interface."
"This paper presents a new dynamic power management architecture of a System
on Chip. The Power State Machine describing the status of the core follows the
recommendations of the ACPI standard. The algorithm controls the power states
of each block on the basis of battery status, chip temperature and a user
defined task priority."
"We present a system for the boresighting of sensors using inertial
measurement devices as the basis for developing a range of dynamic real-time
sensor fusion applications. The proof of concept utilizes a COTS FPGA platform
for sensor fusion and real-time correction of a misaligned video sensor. We
exploit a custom-designed 32-bit soft processor core and C-based design &
synthesis for rapid, platform-neutral development. Kalman filter and sensor
fusion techniques established in advanced aviation systems are applied to
automotive vehicles with results exceeding typical industry requirements for
sensor alignment. Results of the static and the dynamic tests demonstrate that
using inexpensive accelerometers mounted on (or during assembly of) a sensor
and an Inertial Measurement Unit (IMU) fixed to a vehicle can be used to
compute the misalignment of the sensor to the IMU and thus vehicle. In some
cases the model predications and test results exceeded the requirements by an
order of magnitude with a 3-sigma or 99% confidence."
"In this paper a general architecture suitable to interface several kinds of
sensors for automotive applications is presented. A platform based design
approach is pursued to improve system performance while minimizing
time-to-market.. The platform is composed by an analog front-end and a digital
section. The latter is based on a microcontroller core (8051 IP by Oregano)
plus a set of dedicated hardware dedicated to the complex signal processing
required for sensor conditioning. The microcontroller handles also the
communication with external devices (as a PC) for data output and fast
prototyping. A case study is presented concerning the conditioning of a Gyro
yaw rate sensor for automotive applications. Measured performance results
outperform current state-of-the-art commercial devices."
"A 6bit flash-ADC with 1.2GSps, wide analog bandwidth and low power, realized
in a standard digital 0.13 $\mu$m CMOS copper technology is presented.
Employing capacitive interpolation gives various advantages when designing for
low power: no need for a reference resistor ladder, implicit sample-and-hold
operation, no edge effects in the interpolation network (as compared to
resistive interpolation), and a very low input capacitance of only 400fF, which
leads to an easily drivable analog converter interface. Operating at 1.2GSps
the ADC achieves an effective resolution bandwidth (ERBW) of 700MHz, while
consuming 160mW of power. At 600MSps we achieve an ERBW of 600MHz with only
90mW power consumption, both from a 1.5V supply. This corresponds to
outstanding Figure-of-Merit numbers (FoM) of 2.2 and 1.5pJ/convstep,
respectively. The module area is 0.12mm^2."
"A 12 bit Pipeline ADC fabricated in a 0.18 $\mu$m pure digital CMOS
technology is presented. Its nominal conversion rate is 110MS/s and the nominal
supply voltage is 1.8V. The effective number of bits is 10.4 when a 10MHz input
signal with 2V_{P-P} signal swing is applied. The occupied silicon area is
0.86mm^2 and the power consumption equals 97mW. A switched capacitor bias
current circuit scale the bias current automatically with the conversion rate,
which gives scaleable power consumption and full performance of the ADC from 20
to 140MS/s."
"In this paper we describe how we applied a BIST-based approach to the test of
a logic core to be included in System-on-a-chip (SoC) environments. The
approach advantages are the ability to protect the core IP, the simple test
interface (thanks also to the adoption of the P1500 standard), the possibility
to run the test at-speed, the reduced test time, and the good diagnostic
capabilities. The paper reports figures about the achieved fault coverage, the
required area overhead, and the performance slowdown, and compares the figures
with those for alternative approaches, such as those based on full scan and
sequential ATPG."
"At 130 nm and 90 nm, power consumption (both dynamic and static) has become a
barrier in the roadmap for SoC designs targeting battery powered, mobile
applications. This paper presents the results of dynamic and static power
reduction achieved implementing Tensilica's 32-bit Xtensa microprocessor core,
using Virtual Silicon's Power Management IP. Independent voltage islands are
created using Virtual Silicon's VIP PowerSaver standard cells by using voltage
level shifting cells and voltage isolation cells to implement power islands.
The VIP PowerSaver standard cells are characterized at 1.2V, 1.0V and 0.8V, to
accommodate voltage scaling. Power islands can also be turned off completely.
Designers can significantly lower both the dynamic power and the quiescent or
leakage power of their SoC designs, with very little impact on speed or area
using Virtual Silicon's VIP Gate Bias standard cells."
"The MultiNoC system implements a programmable on-chip multiprocessing
platform built on top of an efficient, low area overhead intra-chip
interconnection scheme. The employed interconnection structure is a Network on
Chip, or NoC. NoCs are emerging as a viable alternative to increasing demands
on interconnection architectures, due to the following characteristics: (i)
energy efficiency and reliability; (ii) scalability of bandwidth, when compared
to traditional bus architectures; (iii) reusability; (iv) distributed routing
decisions. An external host computer feeds MultiNoC with application
instructions and data. After this initialization procedure, MultiNoC executes
some algorithm. After finishing execution of the algorithm, output data can be
read back by the host. Sequential or parallel algorithms conveniently adapted
to the MultiNoC structure can be executed. The main motivation to propose this
design is to enable the investigation of current trends to increase the number
of embedded processors in SoCs, leading to the concept of ""sea of processors""
systems."
"In this paper, we propose a methodology for partitioning and mapping
computational intensive applications in reconfigurable hardware blocks of
different granularity. A generic hybrid reconfigurable architecture is
considered so as the methodology can be applicable to a large number of
heterogeneous reconfigurable platforms. The methodology mainly consists of two
stages, the analysis and the mapping of the application onto fine and
coarse-grain hardware resources. A prototype framework consisting of analysis,
partitioning and mapping tools has been also developed. For the coarse-grain
reconfigurable hardware, we use our previous-developed high-performance
coarse-grain data-path. In this work, the methodology is validated using two
real-world applications, an OFDM transmitter and a JPEG encoder. In the case of
the OFDM transmitter, a maximum clock cycles decrease of 82% relative to the
ones in an all fine-grain mapping solution is achieved. The corresponding
performance improvement for the JPEG is 43%."
"This paper evaluates the use of pin and cycle accurate SystemC models for
embedded system design exploration and early software development. The target
system is MicroBlaze VanillaNet Platform running MicroBlaze uClinux operating
system. The paper compares Register Transfer Level (RTL) Hardware Description
Language (HDL) simulation speed to the simulation speed of several different
SystemC models. It is shown that simulation speed of pin and cycle accurate
models can go up to 150 kHz, compared to 100 Hz range of HDL simulation.
Furthermore, utilising techniques that temporarily compromise cycle accuracy,
effective simulation speed of up to 500 kHz can be obtained."
"This contribution presents a new approach for allocating suitable
function-implementation variants depending on given quality-of-service
function-requirements for run-time reconfigurable multi-device systems. Our
approach adapts methodologies from the domain of knowledge-based systems which
can be used for doing run-time hardware/software resource usage optimizations."
"We introduce a strict version of the concept of a load/store instruction set
architecture in the setting of Maurer machines. We take the view that
transformations on the states of a Maurer machine are achieved by applying
threads as considered in thread algebra to the Maurer machine. We study how the
transformations on the states of the main memory of a strict load/store
instruction set architecture that can be achieved by applying threads depend on
the operating unit size, the cardinality of the instruction set, and the
maximal number of states of the threads."
"The recently proposed Golden code is an optimal space-time block code for 2 X
2 multiple-input multiple-output (MIMO) systems. The aim of this work is the
design of a VLSI decoder for a MIMO system coded with the Golden code. The
architecture is based on a rearrangement of the sphere decoding algorithm that
achieves maximum-likelihood (ML) decoding performance. Compared to other
approaces, the proposed solution exhibits an inherent flexibility in terms of
modulation schemes QAM modulation size and this makes our architecture
particularly suitable for adaptive modulation schemes."
"In this paper, we propose an architecture/methodology for making FPGAs
suitable for integer as well as variable precision floating point
multiplication. The proposed work will of great importance in applications
which requires variable precision floating point multiplication such as
multi-media processing applications. In the proposed architecture/methodology,
we propose the replacement of existing 18x18 bit and 25x18 bit dedicated
multipliers in FPGAs with dedicated 24x24 bit and 24x9 bit multipliers,
respectively. We have proved that our approach of providing the dedicated 24x24
bit and 24x9 bit multipliers in FPGAs will make them efficient for performing
integer as well as single precision, double precision, and Quadruple precision
floating point multiplications."
"IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and
a major enhancement to the standard is the addition of decimal format.
Furthermore, in the recent years reversible logic has emerged as a promising
computing paradigm having its applications in low power CMOS, quantum
computing, nanotechnology, and optical computing. The major goal in reversible
logic is to minimize the number of reversible gates and garbage outputs. Thus,
this paper proposes the novel concept of partial reversible gates that will
satisfy the reversibility criteria for specific cases in BCD arithmetic. The
partial reversible gate is proposed to minimize the number of reversible gates
and garbage outputs, while designing the reversible BCD arithmetic circuits."
"The new vision presented is aimed to overcome the logic overhead issues that
previous works exhibit when applying GALS techniques to programmable logic
devices. The proposed new view relies in a 2-phase, bundled data parity based
protocol for data transfer and clock generation tasks. The ability of the
introduced methodology for smart real-time delay selection allows the
implementation of a variety of new methodologies for electromagnetic
interference mitigation and device environment changes adaptation."
"This paper introduces Archer, a community-based computing resource for
computer architecture research and education. The Archer infrastructure
integrates virtualization and batch scheduling middleware to deliver
high-throughput computing resources aggregated from resources distributed
across wide-area networks and owned by different participating entities in a
seamless manner. The paper discusses the motivations leading to the design of
Archer, describes its core middleware components, and presents an analysis of
the functionality and performance of a prototype wide-area deployment running a
representative computer architecture simulation workload."
"Particle Image Velocimetry (PIV) is a method of im-aging and analysing fields
of flows. The PIV tech-niques compute and display all the motion vectors of the
field in a resulting image. Speeds more than thou-sand vectors per second can
be required, each speed being environment-dependent. Essence of this work is to
propose an adaptive FPGA-based system for real-time PIV algorithms. The
proposed structure is ge-neric so that this unique structure can be re-used for
any PIV applications that uses the cross-correlation technique. The major
structure remains unchanged, adaptations only concern the number of processing
operations. The required speed (corresponding to the number of vector per
second) is obtained thanks to a parallel processing strategy. The image
processing designer duplicates the processing modules to distrib-ute the
operations. The result is a FPGA-based archi-tecture, which is easily adapted
to algorithm specifica-tions without any hardware requirement. The design flow
is fast and reliable."
"An adaptive FPGA architecture based on the NoC (Network-on-Chip) approach is
used for the multispectral image correlation. This architecture must contain
several distance algorithms depending on the characteristics of spectral images
and the precision of the authentication. The analysis of distance algorithms is
required which bases on the algorithmic complexity, result precision, execution
time and the adaptability of the implementation. This paper presents the
comparison of these distance computation algorithms on one spectral database.
The result of a RGB algorithm implementation was discussed."
"At present, the mostly used and developed mechanism is hardware
virtualization which provides a common platform to run multiple operating
systems and applications in independent partitions. More precisely, it is all
about resource virtualization as the term hardware virtualization is
emphasized. In this paper, the aim is to find out the advantages and
limitations of current virtualization techniques, analyze their cost and
performance and also depict which forthcoming hardware virtualization
techniques will able to provide efficient solutions for multiprocessor
operating systems. This is done by making a methodical literature survey and
statistical analysis of the benchmark reports provided by SPEC (Standard
Performance Evaluation Corporation) and TPC (Transaction processing Performance
Council). Finally, this paper presents the current aspects of hardware
virtualization which will help the IT managers of the large organizations to
take effective decision while choosing server with virtualization support.
Again, the future works described in section 4 of this paper focuses on some
real world challenges such as abstraction of multiple servers, language level
virtualization, pre-virtualization etc. which may be point of great interest
for the researchers."
"This work proposes a general framework for the design and simulation of
network on chip based turbo decoder architectures. Several parameters in the
design space are investigated, namely the network topology, the parallelism
degree, the rate at which messages are sent by processing nodes over the
network and the routing strategy. The main results of this analysis are: i) the
most suited topologies to achieve high throughput with a limited complexity
overhead are generalized de-Bruijn and generalized Kautz topologies; ii)
depending on the throughput requirements different parallelism degrees, message
injection rates and routing algorithms can be used to minimize the network area
overhead."
"The aim of this paper is to present an adaptable Fat Tree NoC architecture
for Field Programmable Gate Array (FPGA) designed for image analysis
applications. Traditional NoCs (Network on Chip) are not optimal for dataflow
applications with large amount of data. On the opposite, point to point
communications are designed from the algorithm requirements but they are
expensives in terms of resource and wire. We propose a dedicated communication
architecture for image analysis algorithms. This communication mechanism is a
generic NoC infrastructure dedicated to dataflow image processing applications,
mixing circuit-switching and packet-switching communications. The complete
architecture integrates two dedicated communication architectures and reusable
IP blocks. Communications are based on the NoC concept to support the high
bandwidth required for a large number and type of data."
"Today every circuit has to face the power consumption issue for both portable
device aiming at large battery life and high end circuits avoiding cooling
packages and reliability issues that are too complex. It is generally accepted
that during logic synthesis power tracks well with area. This means that a
larger design will generally consume more power. The multiplier is an important
kernel of digital signal processors. Because of the circuit complexity, the
power consumption and area are the two important design considerations of the
multiplier. In this paper a low power low area architecture for the shift and
add multiplier is proposed. For getting the low power low area architecture,
the modifications made to the conventional architecture consist of the
reduction in switching activities of the major blocks of the multiplier, which
includes the reduction in switching activity of the adder and counter. This
architecture avoids the shifting of the multiplier register. The simulation
result for 8 bit multipliers shows that the proposed low power architecture
lowers the total power consumption by 35.25% and area by 52.72 % when compared
to the conventional architecture. Also the reduction in power consumption
increases with the increase in bit width."
"Now a days, power has become a primary consideration in hardware design, and
is critical in computer systems especially for portable devices with high
performance and more functionality. Clock-gating is the most common technique
used for reducing processor's power. In this work clock gating technique is
applied to optimize the power of fully programmable Embedded Controller (PEC)
employing RISC architecture. The CPU designed supports i) smart instruction
set, ii) I/O port, UART iii) on-chip clocking to provide a range of frequencies
, iv) RISC as well as controller concepts. The whole design is captured using
VHDL and is implemented on FPGA chip using Xilinx .The architecture and clock
gating technique together is found to reduce the power consumption by 33.33% of
total power consumed by this chip."
"Optimization techniques for decreasing the time and area of adder circuits
have been extensively studied for years mostly in binary logic system. In this
paper, we provide the necessary equations required to design a full adder in
quaternary logic system. We develop the equations for single-stage parallel
adder which works as a carry look-ahead adder. We also provide the design of a
logarithmic stage parallel adder which can compute the carries within log2(n)
time delay for n qudits. At last, we compare the designs and finally propose a
hybrid adder which combines the advantages of serial and parallel adder."
"Reversible logic design has become one of the promising research directions
in low power dissipating circuit design in the past few years and has found its
application in low power CMOS design, digital signal processing and
nanotechnology. This paper presents the efficient design approaches of fault
tolerant carry skip adders (FTCSAs) and compares those designs with the
existing ones. Variable block carry skip logic (VBCSL) using the fault tolerant
full adders (FTFAs) has also been developed. The designs are minimized in terms
of hardware complexity, gate count, constant inputs and garbage outputs.
Besides of it, technology independent evaluation of the proposed designs
clearly demonstrates its superiority with the existing counterparts."
"Segmented display is widely used for efficient display of alphanumeric
characters. English numerals are displayed by 7 segment and 16 segment display.
The segment size is uniform in this two display architecture. Display
architecture using 8, 10, 11, 18 segments have been proposed for Bengali
numerals 0...9 yet no display architecture is designed using segments of
uniform size and uniform power consumption. In this paper we have proposed a
uniform 10 segment architecture for Bengali numerals. This segment architecture
uses segments of uniform size and no bent segment is used."
"Segmentation display plays a vital role to display numerals. But in today's
world matrix display is also used in displaying numerals. Because numerals has
lots of curve edges which is better supported by matrix display. But as matrix
display is costly and complex to implement and also needs more memory, segment
display is generally used to display numerals. But as there is yet no proposed
compact display architecture to display multiple language numerals at a time,
this paper proposes uniform display architecture to display multiple language
digits and general mathematical expressions with higher accuracy and simplicity
by using a 18-segment display, which is an improvement over the 16 segment
display."
"Considerable research has taken place in recent times in the area of
parameterization of software defined radio (SDR) architecture. Parameterization
decreases the size of the software to be downloaded and also limits the
hardware reconfiguration time. The present paper is based on the design and
development of a programmable baseband modulator that perform the QPSK
modulation schemes and as well as its other three commonly used variants to
satisfy the requirement of several established 2G and 3G wireless communication
standards. The proposed design has been shown to be capable of operating at a
maximum data rate of 77 Mbps on Xilinx Virtex 2-Pro University field
programmable gate array (FPGA) board. The pulse shaping root raised cosine
(RRC) filter has been implemented using distributed arithmetic (DA) technique
in the present work in order to reduce the computational complexity, and to
achieve appropriate power reduction and enhanced throughput. The designed
multiplier-less programmable 32-tap FIR-based RRC filter has been found to
withstand a peak inter-symbol interference (ISI) distortion of -41 dBs"
"Multiple-input, multiple-output (MIMO) technology provides high data rate and
enhanced QoS for wireless com- munications. Since the benefits from MIMO result
in a heavy computational load in detectors, the design of low-complexity
sub-optimum receivers is currently an active area of research.
Lattice-reduction-aided detection (LRAD) has been shown to be an effective
low-complexity method with near-ML performance. In this paper we advocate the
use of systolic array architectures for MIMO receivers, and in particular we
exhibit one of them based on LRAD. The ""LLL lattice reduction algorithm"" and
the ensuing linear detections or successive spatial-interference cancellations
can be located in the same array, which is con- siderably hardware-efficient.
Since the conventional form of the LLL algorithm is not immediately suitable
for parallel processing, two modified LLL algorithms are considered here for
the systolic array. LLL algorithm with full-size reduction (FSR-LLL) is one of
the versions more suitable for parallel processing. Another variant is the
all-swap lattice-reduction (ASLR) algorithm for complex-valued lattices, which
processes all lattice basis vectors simultaneously within one iteration. Our
novel systolic array can operate both algorithms with different external logic
controls. In order to simplify the systolic array design, we replace the
Lov\'asz condition in the definition of LLL-reduced lattice with the looser
Siegel condition. Simulation results show that for LR- aided linear detections,
the bit-error-rate performance is still maintained with this relaxation.
Comparisons between the two algorithms in terms of bit-error-rate performance,
and average FPGA processing time in the systolic array are made, which shows
that ASLR is a better choice for a systolic architecture, especially for
systems with a large number of antennas."
"Comparison of RISC & CISC in details, encompassing the addressing modes,
evolution, definitions and characteristics. Pre - RISC design is also
elaborated. Both the architectures are explained with the help of example.
Analysis is made based on performance."
"High speed Full-Adder (FA) module is a critical element in designing high
performance arithmetic circuits. In this paper, we propose a new high speed
multiple-valued logic FA module. The proposed FA is constructed by 14
transistors and 3 capacitors, using carbon nano-tube field effect transistor
(CNFET) technology. Furthermore, our proposed technique has been examined in
different voltages (i.e., 0.65v and 0.9v). The observed results reveal power
consumption and power delay product (PDP) improvements compared to existing FA
counterparts"
"The traditional approach to fault tolerant computing involves replicating
computation units and applying a majority vote operation on individual result
bits. This approach, however, has several limitations; the most severe is the
resource requirement. This paper presents a new method for fault tolerant
computing where for a given error rate, the hamming distance between correct
inputs and faulty inputs as well as the hamming distance between a correct
result and a faulty result is preserved throughout processing thereby enabling
correction of up to transient faults per computation cycle. The new method is
compared and contrasted with current protection methods and its cost /
performance is analyzed."
"Scan and ring schemes of the pseudo-ring memory selftesting are investigated.
Both schemes are based on emulation of the linear or nonlinear feedback shift
register by memory itself. Peculiarities of the pseudo-ring schemes
implementation for multi-port and embedded memories, and for register file are
described. It is shown that only small additional logic is required and allows
microcontrollers at-speed testing. Also, in this article,are given the a
posteriori values of some type of memories faults coverage when pseudo-ring
testing schemes are applied."
"A novel approach to evaluation of hardware and software testability,
represented in the form of register transfer graph, is proposed. Instances of
making of software graph models for their subsequent testing and diagnosis are
shown."
"Semi-parallel, or folded, VLSI architectures are used whenever hardware
resources need to be saved at design time. Most recent applications that are
based on Projective Geometry (PG) based balanced bipartite graph also fall in
this category. In this paper, we provide a high-level, top-down design
methodology to design optimal semi-parallel architectures for applications,
whose Data Flow Graph (DFG) is based on PG bipartite graph. Such applications
have been found e.g. in error-control coding and matrix computations. Unlike
many other folding schemes, the topology of connections between physical
elements does not change in this methodology. Another advantage is the ease of
implementation. To lessen the throughput loss due to folding, we also
incorporate a multi-tier pipelining strategy in the design methodology. The
design methodology has been verified by implementing a synthesis tool in C++,
which has been verified as well. The tool is publicly available. Further, a
complete decoder was manually protototyped before the synthesis tool design, to
verify all the algorithms evolved in this paper, towards various steps of
refinement. Another specific high-performance design of an LDPC decoder based
on this methodology was worked out in past, and has been patented as well."
"In this work, a novel quaternary algebra has been proposed that can be used
to implement any quaternary logic function. Unlike other variants of quaternary
algebra, this algebra is closely related to Boolean algebra and can be used to
convert any binary function into quaternary without any significant
modification. For this purpose, we have defined a set of quaternary operators
and developed two ways to express any quaternary function mathematically.
Finally, we have presented the design of several combinational logic circuits
and compared these designs with several other variants of quaternary logic.
Since a quaternary digit can contain as much information as a pair of binary
digits, this new logic may be quite useful in the fields of communication and
computing."
"The semiconductor industry is reaching a fascinating confluence in several
evolutionary trends that will likely lead to a number of revolutionary changes
in the design, implementation, scaling, and the use of computer systems.
However, recently Moore's law has come to a stand-still since device scaling
beyond 65 nm is not practical. 2D integration has problems like memory latency,
power dissipation, and large foot-print. 3D technology comes as a solution to
the problems posed by 2D integration. The utilization of 3D is limited by the
problem of temperature crisis. It is important to develop an accurate power
profile extraction methodology to design 3D structure. In this paper, design of
3D integration of memory is considered and hence the static power dissipation
of the memory cell is analysed in transistor level and is used to accurately
model the inter-layer thermal effects for 3D memory stack. Subsequently,
packaging of the chip is considered and modelled using an architecture level
simulator. This modelling is intended to analyse the thermal effects of 3D
memory, its reliability and lifetime of the chip, with greater accuracy."
"Integrated optical interconnect is believed to be one of the main
technologies to replace electrical wires. Optical Network-on-Chip (ONoC) has
attracted more attentions nowadays. Benes topology is a good choice for ONoC
for its rearrangeable non-blocking character, multistage feature and easy
scalability. Routing algorithm plays an important role in determining the
performance of ONoC. But traditional routing algorithms for Benes network are
not suitable for ONoC communication, we developed a new distributed routing
algorithm for Benes ONoC in this paper. Our algorithm selected the routing path
dynamically according to network condition and enables more path choices for
the message traveling in the network. We used OPNET to evaluate the performance
of our routing algorithm and also compared it with a well-known bit-controlled
routing algorithm. ETE delay and throughput were showed under different packet
length and network sizes. Simulation results show that our routing algorithm
can provide better performance for ONoC."
"Networks-on-Chip (NoCs) for future many-core processor platforms integrate
more and more heterogeneous components of different types and many real-time
and latency-sensitive applications can run on a single chip concurrently. The
reconfigurable FPGA and reconfigurable NoCs have emerged for the purpose of
reusability. Those types' traffics within NoCs exhibit diverse, burst, and
unpredictable communication patterns. QoS guaranteed mechanisms are necessary
to provide guaranteed throughput (GT) or guaranteed bandwidth (GB) performance
for NoCs. In this paper, we propose a QoS routing algorithm inspired by bees'
foraging behaviors to provide guaranteed bandwidth performance. Virtual
circuits and Spatial Division Multiplexing are employed to maintain available
paths for different type's traffics."
"Full adders are important components in applications such as digital signal
processors (DSP) architectures and microprocessors. Apart from the basic
addition adders also used in performing useful operations such as subtraction,
multiplication, division, address calculation, etc. In most of these systems
the adder lies in the critical path that determines the overall performance of
the system. In this paper conventional complementary metal oxide semiconductor
(CMOS) and adiabatic adder circuits are analyzed in terms of power and
transistor count using 0.18UM technology."
"In this work faster Baugh-Wooley multiplication has been achieved by using a
combination of two design techniques: partition of the partial products into
two parts for independent parallel column compression and acceleration of the
final addition using a hybrid adder proposed in this work. Based on the
proposed techniques 8, 16, 32 and 64-bit Dadda based Baugh-Wooley multipliers
has been developed and compared with the regular Baugh-Wooley multiplier. The
performance of the proposed multiplier is analyzed by evaluating the delay,
area and power, with 180 nm process technologies on interconnect and layout
using industry standard design and layout tools. The result analysis shows that
the 64-bit proposed multiplier is as much as 26.9% faster than the regular
Baugh-Wooley multiplier and requires only 2.21% more power. Also the
power-delay product of the proposed design is significantly lower than that of
the regular Baugh-Wooley multiplier."
"In this work faster unsigned multiplication has been achieved by using a
combination of High Performance Multiplication [HPM] column reduction technique
and implementing a N-bit multiplier using 4 N/2-bit multipliers (recursive
multiplication) and acceleration of the final addition using a hybrid adder.
Low power has been achieved by using clock gating technique. Based on the
proposed technique 16 and 32-bit multipliers are developed. The performance of
the proposed multiplier is analyzed by evaluating the delay, area and power,
with TCBNPHP 90 nm process technology on interconnect and layout using Cadence
NC launch, RTL compiler and ENCOUNTER tools. The results show that the 32-bit
proposed multiplier is as much as 22% faster, occupies only 3% more area and
consumes 30% lesser power with respect to the recently reported twin precision
multiplier."
"Microprocessors have revolutionized the world we live in and continuous
efforts are being made to manufacture not only faster chips but also smarter
ones. A number of techniques such as data level parallelism, instruction level
parallelism and hyper threading (Intel's HT) already exists which have
dramatically improved the performance of microprocessor cores. This paper
briefs on evolution of multi-core processors followed by introducing the
technology and its advantages in today's world. The paper concludes by
detailing on the challenges currently faced by multi-core processors and how
the industry is trying to address these issues."
"Based on the ASIC layout level simulation of 7 types of adder structures each
of four different sizes, i.e. a total of 28 adders, we propose expressions for
the width of each of the three regions of the final Carry Propagate Adder (CPA)
to be used in parallel multipliers. We also propose the types of adders to be
used in each region that would lead to the optimal performance of the hybrid
final adders in parallel multipliers. This work evaluates the complete
performance of the analyzed designs in terms of delay, area, power through
custom design and layout in 0.18 um CMOS process technology."
"In this paper, the acceleration of algorithms using a design of a field
programmable gate array (FPGA) as a prototype of a static dataflow architecture
is discussed. The static dataflow architecture using operators interconnected
by parallel buses was implemented. Accelerating algorithms using a dataflow
graph in a reconfigurable system shows the potential for high computation
rates. The results of benchmarks implemented using the static dataflow
architecture are reported at the end of this paper."
"This paper presents an efficient approach for multiplierless implementation
for eight-point DCT approximation, which based on coordinate rotation digital
computer (CORDIC) algorithm. The main design objective is to make critical path
of corresponding circuits shorter and reduce the combinational delay of
proposed scheme."
"Reversible logic allows low power dissipating circuit design and founds its
application in cryptography, digital signal processing, quantum and optical
information processing. This paper presents a novel quantum cost efficient
reversible BCD adder for nanotechnology based systems using PFAG gate. It has
been demonstrated that the proposed design offers less hardware complexity and
requires minimum number of garbage outputs than the existing counterparts. The
remarkable property of the proposed designs is that its quantum realization is
given in NMR technology."
"A high-speed multiprocessor architecture for brain-like analyzing information
represented in analytic, graph- and table forms of associative relations to
search, recognize and make a decision in n-dimensional vector discrete space is
offered. Vector-logical process models of actual applications, where the
quality of solution is estimated by the proposed integral non-arithmetical
metric of the interaction between binary vectors, are described. The
theoretical proof of the metric for a vector logical space and the quality
criteria for estimating solutions is created."
"Modern communication and computer systems require rapid (Gbps), efficient and
large bandwidth data transfers. Agressive scaling of digital integrated systems
allow buses and communication controller circuits to be integrated with the
microprocessor on the same chip. The Peripheral Component Interconnect Express
(PCIe) protocol handles all communcation between the central processing unit
(CPU) and hardware devices. PCIe buses require efficient clock data recovery
circuits (CDR) to recover clock signals embedded in data during transmission.
This paper describes the theoretical modeling and simulation of a phase-locked
loop (PLL) used in a CDR circuit. A simple PLL architecture for a 5 GHz CDR
circuit is proposed and elaborated in this work. Simulations were carried out
using a Hardware Description Language, Verilog- AMS. The effect of jitter on
the proposed design is also simulated and evaluated in this work. It was found
that the proposed design is robust against both input and VCO jitter."
"Communication systems use the concept of transmitting information using the
electrical distribution network as a communication channel. To enable the
transmission data signal modulated on a carrier signal is superimposed on the
electrical wires. Typical power lines are designed to handle 50/60 Hz of AC
power signal; however they can carry the signals up to 500 KHz frequency. This
work aims to aid transmission/reception of an audio signal in the spectrum from
300 Hz to 4000 Hz using PLCC on a tunable carrier frequency in the spectrum
from 200 KHz to 500 KHz. For digital amplitude modulation the sampling rate of
the carrier and the audio signal has to be matched. Tunable carrier generation
can be achieved with Direct Digital Synthesizers at a desired sampling rate.
DSP Sample rate conversion techniques are very useful to make the sampling
circuits to work on their own sampling rates which are fine for the
data/modulated-carrier signal's bandwidth. This also simplifies the complexity
of the sampling circuits. Digital Up Conversion (DUC) and Digital Down
Conversion (DDC) are DSP sample rate conversion techniques which refer to
increasing and decreasing the sampling rate of a signal respectively."
"It is intended in this document to introduce a handy systematic method for
enumerating all possible data dependency cases that could occur between any two
instructions that might happen to be processed at the same time at different
stages of the pipeline. Given instructions of the instruction set, specific
information about operands of each instruction and when an instruction reads or
writes data, the method could be used to enumerate all possible data hazard
cases and to determine whether forwarding or stalling is suitable for resolving
each case."
"The Scaling of microchip technologies, from micron to submicron and now to
deep sub-micron (DSM) range, has enabled large scale systems-on-chip (SoC). In
future deep submicron (DSM) designs, the interconnect effect will definitely
dominate performance. Network-on-Chip (NoC) has become a promising solution to
bus-based communication infrastructure limitations. NoC designs usually targets
Application Specific Integrated Circuits (ASICs), however, the fabrication
process costs a lot. Implementing a NoC on an FPGA does not only reduce the
cost but also decreases programming and verification cycles. In this paper, an
Asynchronous NoC has been implemented on a SPARTAN-3E\textregistered device.
The NoC supports basic transactions of both widely used on-chip interconnection
standards, the Open Core Protocol (OCP) and the WISHBONE Protocol. Although,
FPGA devices are synchronous in nature, it has been shown that they can be used
to prototype a Global Asynchronous Local Synchronous (GALS) systems, comprising
an Asynchronous NoC connecting IP cores operating in different clock domains."
"This document shows the detailed specification of LOCKE coherence protocol
for each cache controller, using a table-based technique. This representation
provides clear, concise visual information yet includes sufficient detail
(e.g., transient states) arguably lacking in the traditional, graphical form of
state diagrams."
"The demand for high performance embedded processors, for consumer
electronics, is rapidly increasing for the past few years. Many of these
embedded processors depend upon custom built Instruction Ser Architecture (ISA)
such as game processor (GPU), multimedia processors, DSP processors etc.
Primary requirement for consumer electronic industry is low cost with high
performance and low power consumption. A lot of research has been evolved to
enhance the performance of embedded processors through parallel computing. But
some of them focus superscalar processors i.e. single processors with more
resources like Instruction Level Parallelism (ILP) which includes Very Long
Instruction Word (VLIW) architecture, custom instruction set extensible
processor architecture and others require more number of processing units on a
single chip like Thread Level Parallelism (TLP) that includes Simultaneous
Multithreading (SMT), Chip Multithreading (CMT) and Chip Multiprocessing (CMP).
In this paper, we present a new technique, named C-slow, to enhance performance
for embedded processors for consumer electronics by exploiting multithreading
technique in single core processors. Without resulting into the complexity of
micro controlling with Real Time Operating system (RTOS), C-slowed processor
can execute multiple threads in parallel using single datapath of Instruction
Set processing element. This technique takes low area & approach complexity of
general purpose processor running RTOS."
"Embedded applications are widely used in portable devices such as wireless
phones, personal digital assistants, laptops, etc. High throughput and real
time requirements are especially important in such data-intensive tasks.
Therefore, architectures that provide the required performance are the most
desirable. On the other hand, processor performance is severely related to the
average memory access delay, number of processor registers and also size of the
instruction window and superscalar parameters. Therefore, cache, register file
and superscalar parameters are the major architectural concerns in designing a
superscalar architecture for embedded processors. Although increasing cache and
register file size leads to performance improvements in high performance
embedded processors, the increased area, power consumption and memory delay are
the overheads of these techniques. This paper explores the effect of cache,
register file and superscalar parameters on the processor performance to
specify the optimum size of these parameters for embedded applications.
Experimental results show that although having bigger size of these parameters
is one of the performance improvement approaches in embedded processors,
however, by increasing the size of some parameters over a threshold value,
performance improvement is saturated and especially in cache size, increments
over this threshold value decrease the performance."
"This paper present the research work directed towards the design of
reversible programmable logic array using very high speed integrated circuit
hardware description language (VHDL). Reversible logic circuits have
significant importance in bioinformatics, optical information processing, CMOS
design etc. In this paper the authors propose the design of new RPLA using
Feynman & MUX gate.VHDL based codes of reversible gates with simulating results
are shown .This proposed RPLA may be further used to design any reversible
logic function or Boolean function (Adder, subtractor etc.) which dissipate
very low or ideally no heat."
"In engineering applications sorting is an important and widely studied
problem where execution speed and resources used for computation are of extreme
importance, especially if we think about real time data processing. Most of the
traditional sorting techniques compute the process after receiving all of the
data and hence the process needs large amount of resources for data storage.
So, suitable design strategy needs to be adopted if we wish to sort a large
amount of data in real time, which essential means higher speed of process
execution and utilization of fewer resources in most of the cases. This paper
proposes a single chip scalable architecture based on Field Programmable Gate
Array(FPGA), for a modified counting sort algorithm where data acquisition and
sorting is being done in real time scenario. Our design promises to work
efficiently, where data can be accepted in the run time scenario without any
need of prior storage of data and also the execution speed of our algorithm is
invariant to the length of the data stream. The proposed design is implemented
and verified on Spartan 3E(XC3S500E-FG320) FPGA system. The results prove that
our design is better in terms of some of the design parameters compared to the
existing research works."
"The advances in IC process make future chip multiprocessors (CMPs) more and
more vulnerable to transient faults. To detect transient faults, previous
core-level schemes provide redundancy for each core separately. As a result,
they may leave transient faults in the uncore parts, which consume over 50%
area of a modern CMP, escaped from detection. This paper proposes RepTFD, the
first core-level transient fault detection scheme with 100% coverage. Instead
of providing redundancy for each core separately, RepTFD provides redundancy
for a group of cores as a whole. To be specific, it replays the execution of
the checked group of cores on a redundant group of cores. Through comparing the
execution results between the two groups of cores, all malignant transient
faults can be caught. Moreover, RepTFD adopts a novel pending period based
record-replay approach, which can greatly reduce the number of execution orders
that need to be enforced in the replay-run. Hence, RepTFD brings only 4.76%
performance overhead in comparison to the normal execution without
fault-tolerance according to our experiments on the RTL design of an industrial
CMP named Godson-3. In addition, RepTFD only consumes about 0.83% area of
Godson-3, while needing only trivial modifications to existing components of
Godson-3."
"Directory-based protocols have been the de facto solution for maintaining
cache coherence in shared-memory parallel systems comprising multi/many cores,
where each store instruction is eagerly made globally visible by invalidating
the private cache (PC) backups of other cores. Consequently, the directory not
only consumes large chip area, but also incurs considerable energy consumption
and performance degradation, due to the large number of Invalidation/Ack
messages transferred in the interconnection network and resulting network
congestion. In this paper, we reveal the interesting fact that the directory is
actually an unnecessary luxury for practical parallel systems. Because of
widely deployed software/hardware techniques involving instruction reordering,
most (if not all) parallel systems work under the weak consistency model, where
a remote store instruction is allowed to be invisible to a core before the next
synchronization of the core, instead of being made visible eagerly by
invalidating PC backups of other cores. Based on this key observation, we
propose a lightweight novel scheme called {\em DLS (DirectoryLess Shared
last-level cache)}, which completely removes the directory and Invalidation/Ack
messages, and efficiently maintains cache coherence using a novel {\em
self-suspicion + speculative execution} mechanism. Experimental results over
SPLASH-2 benchmarks show that on a 16-core processor, DLS not only completely
removes the chip area cost of the directory, but also improves processor
performance by 11.08%, reduces overall network traffic by 28.83%, and reduces
energy consumption of the network by 15.65% on average (compared with
traditional MESI protocol with full directory). Moreover, DLS does not involve
any modification to programming languages and compilers, and hence is
seamlessly compatible with legacy codes."
"In this paper, a digital clock is designed where the microcontroller is used
for timing controller and the font of the Bangla digits are designed, and
programmed within the microcontroller. The design is cost effective, simple and
easy for maintenance."
"Modern GPUs synchronize threads grouped in a warp at every instruction. These
results in improving SIMD efficiency and makes sharing fetch and decode
resources possible. The number of threads included in each warp (or warp size)
affects divergence, synchronization overhead and the efficiency of memory
access coalescing. Small warps reduce the performance penalty associated with
branch and memory divergence at the expense of a reduction in memory
coalescing. Large warps enhance memory coalescing significantly but also
increase branch and memory divergence. Dynamic workload behavior, including
branch/memory divergence and coalescing, is an important factor in determining
the warp size returning best performance. Optimal warp size can vary from one
workload to another or from one program phase to the next. Based on this
observation, we propose Dynamic Warp Resizing (DWR). DWR takes innovative
microarchitectural steps to adjust warp size during runtime and according to
program characteristics. DWR outperforms static warp size decisions, up to 1.7X
to 2.28X, while imposing less than 1% area overhead. We investigate various
alternative configurations and show that DWR performs better for narrower SIMD
and larger caches."
"Residue Number System (RNS), which originates from the Chinese Remainder
Theorem, offers a promising future in VLSI because of its carry-free operations
in addition, subtraction and multiplication. This property of RNS is very
helpful to reduce the complexity of calculation in many applications. A residue
number system represents a large integer using a set of smaller integers,
called residues. But the area overhead, cost and speed not only depend on this
word length, but also the selection of moduli, which is a very crucial step for
residue system. This parameter determines bit efficiency, area, frequency etc.
In this paper a new moduli set selection technique is proposed to improve bit
efficiency which can be used to construct a residue system for digital signal
processing environment. Subsequently, it is theoretically proved and
illustrated using examples, that the proposed solution gives better results
than the schemes reported in the literature. The novelty of the architecture is
shown by comparison the different schemes reported in the literature. Using the
novel moduli set, a guideline for a Reconfigurable Processor is presented here
that can process some predefined functions. As RNS minimizes the carry
propagation, the scheme can be implemented in Real Time Signal Processing &
other fields where high speed computations are required."
"Here we describe the design details and performance of proposed Carry
Propagate Adder based on GDI technique. GDI technique is power efficient
technique for designing digital circuit that consumes less power as compare to
most commonly used CMOS technique. GDI also has an advantage of minimum
propagation delay, minimum area required and less complexity for designing any
digital circuit. We designed Carry Propagate Adder using GDI technique and
compared its performance with CMOS technique in terms of area, delay and power
dissipation. Circuit designed using CADENCE EDA tool and simulated using
SPECTRE VIRTUOSO tool at 0.18m technology. Comparative performance result shows
that Carry Propagate Adder using GDI technique dissipated 55.6% less power as
compare to Carry Propagate Adder using CMOS technique."
"Thermal density and hot spots limit three-dimensional (3D) implementation of
massively-parallel SIMD processors and prohibit stacking DRAM dies above them.
This study proposes replacing SIMD by an Associative Processor (AP). AP
exhibits close to uniform thermal distribution with reduced hot spots.
Additionally, AP may outperform SIMD processor when the data set size is
sufficiently large, while dissipating less power. Comparative performance and
thermal analysis supported by simulation confirm that AP might be preferable
over SIMD for 3D implementation of large scale massively parallel processing
engines combined with 3D DRAM integration."
"Current day processors employ multi-level cache hierarchy with one or two
levels of private caches and a shared last-level cache (LLC). An efficient
cache replacement policy at LLC is essential for reducing the off-chip memory
transfer as well as conflict for memory bandwidth. Cache replacement techniques
for inclusive LLCs may not be efficient for multilevel cache as it can be
shared by enormous applications with varying access behavior, running
simultaneously. One application may dominate another by flooding of cache
requests and evicting the useful data of the other application. From the
performance point of view, an exclusive LLC make the replacement policies more
demanding, as compared to an inclusive LLC. This paper analyzes some of the
existing replacement techniques on the LLC with their performance assessment."
"Minimization of computational errors in the fixed-point data path is often
difficult task. Many signal processing algorithms use chains of consecutive
additions. The analyzing technique that can be applied to fixed-point data path
synthesis has been proposed. This technique takes advantage of allocating the
chains of consecutive additions in order to predict growing width of the data
path and minimize the design complexity and computational errors."
"Digital mobile systems must function with low power, small size and weight,
and low cost. High-performance desktop microprocessors, with built-in floating
point hardware, are not suitable in these cases. For embedded systems, it can
be advantageous to implement these calculations with fixed point arithmetic
instead. We present an automated fixed-point data path synthesis tool FpSynt
for designing embedded applications in fixed-point domain with sufficient
accuracy for most applications. FpSynt is available under the GNU General
Public License from the following GitHub repository:
http://github.com/izhbannikov/FPSYNT"
"Power consumption has emerged as a primary design constraint for integrated
circuits (ICs). In the Nano meter technology regime, leakage power has become a
major component of total power. Full adder is the basic functional unit of an
ALU. The power consumption of a processor is lowered by lowering the power
consumption of an ALU, and the power consumption of an ALU can be lowered by
lowering the power consumption of Full adder. So the full adder designs with
low power characteristics are becoming more popular these days. This proposed
work illustrates the design of the low-power less transistor full adder designs
using cadence tool and virtuoso platform, the entire simulations have been done
on 180nm single n-well CMOS bulk technology, in virtuoso platform of cadence
tool with the supply voltage 1.8V and frequency of 100MHz. These circuits
consume less power with maximum (6T design)of 93.1% power saving compare to
conventional 28T design and 80.2% power saving compare to SERF design without
much delay degradation. The proposed circuit exploits the advantage of GDI
technique and pass transistor logic"
"Fluid Stochastic Petri Nets are used to capture the dynamic behavior of an
ILP processor, and discrete-event simulation is applied to assess the
performance potential of predictions and speculative execution in boosting the
performance of ILP processors that fetch, issue, execute and commit a large
number of instructions per cycle."
"In recent years, the energy consumption of computing systems has increased
and a large fraction of this energy is consumed in main memory. Towards this,
researchers have proposed use of non-volatile memory, such as phase change
memory (PCM), which has low read latency and power; and nearly zero leakage
power. However, the write latency and power of PCM are very high and this,
along with limited write endurance of PCM present significant challenges in
enabling wide-spread adoption of PCM. To address this, several
architecture-level techniques have been proposed. In this report, we review
several techniques to manage power consumption of PCM. We also classify these
techniques based on their characteristics to provide insights into them. The
aim of this work is encourage researchers to propose even better techniques for
improving energy efficiency of PCM based main memory."
"In the past, efforts were taken to improve the performance of a processor via
frequency scaling. However, industry has reached the limits of increasing the
frequency and therefore concurrent execution of instructions on multiple cores
seems the only possible option. It is not enough to provide concurrent
execution by the hardware, software also have to introduce concurrency in order
to exploit the parallelism."
"Traditional processors use the von Neumann execution model, some other
processors in the past have used the dataflow execution model. A combination of
von Neuman model and dataflow model is also tried in the past and the resultant
model is referred as hybrid dataflow execution model. We describe a hybrid
dataflow model known as the microthreading. It provides constructs for
creation, synchronization and communication between threads in an intermediate
language. The microthreading model is an abstract programming and machine model
for many-core architecture. A particular instance of this model is named as the
microthreaded architecture or the Microgrid. This architecture implements all
the concurrency constructs of the microthreading model in the hardware with the
management of these constructs in the hardware."
"Design space exploration is commonly performed in embedded system, where the
architecture is a complicated piece of engineering. With the current trend of
many-core systems, design space exploration in general-purpose computers can no
longer be avoided. Microgrid is a complicated architecture, and therefor we
need to perform design space exploration. Generally, simulators are used for
the design space exploration of an architecture. Different simulators with
different levels of complexity, simulation time and accuracy are used.
Simulators with little complexity, low simulation time and reasonable accuracy
are desirable for the design space exploration of an architecture. These
simulators are referred as high-level simulators and are commonly used in the
design of embedded systems. However, the use of high-level simulation for
design space exploration in general-purpose computers is a relatively new area
of research."
"There has been a significant increase in leakage energy dissipation of CMOS
circuits with each technology generation. Further, due to their large size,
last level caches (LLCs) spend a large fraction of their energy in the form of
leakage energy and hence, addressing this has become extremely important to
meet the challenges of chip power budget. For addressing this, several
techniques have been proposed. However, most of these techniques require
offline profiling and hence cannot be used for real-life systems which usually
run multitasking programs, with possible pre-emptions. In this paper, we
propose a dynamic profiling based technique for saving cache leakage energy in
multitasking systems. Our technique uses a small coloring-based profiling
cache, to estimate performance and energy consumption of multiple cache
configurations and then selects the best (least-energy) configuration among
them. Our technique uses non-intrusive profiling and saves energy despite
intra-task and inter-task variations; thus, it is suitable for multitasking
systems. Simulations performed using workloads from SPEC2006 suite show the
superiority of our technique over an existing cache energy saving technique.
With a 2MB baseline cache, the average saving in memory sub-system energy is
22.8%."
"In recent years, the size and leakage energy consumption of large last level
caches (LLCs) has increased. To address this, embedded DRAM (eDRAM) caches have
been considered which have lower leakage energy consumption; however eDRAM
caches consume a significant amount of energy in the form of refresh energy. In
this paper, we present a technique for saving both leakage and refresh energy
in eDRAM caches. We use dynamic cache reconfiguration approach to intelligently
turn-off part of the cache to save leakage energy and refresh only valid data
of the active (i.e. not turned-off) cache to save refresh energy. We evaluate
our technique using an x86-64 simulator and SPEC2006 benchmarks and compare it
with a recently proposed technique for saving refresh energy, named Refrint.
The experiments have shown that our technique provides better performance and
energy efficiency than Refrint. Using our technique, for a 2MB LLC and 40
micro-seconds eDRAM refresh period, the average saving in energy over eDRAM
baseline (which periodically refreshes all cache lines) is 22.8%."
"This paper proposes a Low-Power, Energy Efficient 4-bit Binary Coded Decimal
(BCD) adder design where the conventional 4-bit BCD adder has been modified
with the Clock Gated Power Gating Technique. Moreover, the concept of DVT
(Dual-vth) scheme has been introduced while designing the full adder blocks to
reduce the Leakage Power, as well as, to maintain the overall performance of
the entire circuit. The reported architecture of 4-bit BCD adder is designed
using 45 nm technology and it consumes 1.384 {\mu}Watt of Average Power while
operating with a frequency of 200 MHz, and a Supply Voltage (Vdd) of 1 Volt.
The results obtained from different simulation runs on SPICE, indicate the
superiority of the proposed design compared to the conventional 4-bit BCD
adder. Considering the product of Average Power and Delay, for the operating
frequency of 200 MHz, a fair 47.41 % reduction compared to the conventional
design has been achieved with this proposed scheme."
"In this work, we provide energy-efficient architectural support for floating
point accuracy. Our goal is to provide accuracy that is far greater than that
provided by the processor's hardware floating point unit (FPU). Specifically,
for each floating point addition performed, we ""recycle"" that operation's
error: the difference between the finite-precision result produced by the
hardware and the result that would have been produced by an infinite-precision
FPU. We make this error architecturally visible such that it can be used, if
desired, by software. Experimental results on physical hardware show that
software that exploits architecturally recycled error bits can achieve accuracy
comparable to a 2B-bit FPU with performance and energy that are comparable to a
B-bit FPU."
"Polar codes are a new family of error correction codes for which efficient
hardware architectures have to be defined for the encoder and the decoder.
Polar codes are decoded using the successive cancellation decoding algorithm
that includes partial sums computations. We take advantage of the recursive
structure of polar codes to introduce an efficient partial sums computation
unit that can also implements the encoder. The proposed architecture is
synthesized for several codelengths in 65nm ASIC technology. The area of the
resulting design is reduced up to 26% and the maximum working frequency is
improved by ~25%."
"Recent technological advances have greatly improved the performance and
features of embedded systems. With the number of just mobile devices now
reaching nearly equal to the population of earth, embedded systems have truly
become ubiquitous. These trends, however, have also made the task of managing
their power consumption extremely challenging. In recent years, several
techniques have been proposed to address this issue. In this paper, we survey
the techniques for managing power consumption of embedded systems. We discuss
the need of power management and provide a classification of the techniques on
several important parameters to highlight their similarities and differences.
This paper is intended to help the researchers and application-developers in
gaining insights into the working of power management techniques and designing
even more efficient high-performance embedded systems of tomorrow."
"Soft errors have a significant impact on the circuit reliability at nanoscale
technologies. At the architectural level, soft errors are commonly modeled by a
probabilistic bit-flip model. In developing such abstract fault models, an
important issue to consider is the likelihood of multiple bit errors caused by
particle strikes. This likelihood has been studied to a great extent in
memories, but has not been understood to the same extent in logic circuits. In
this paper, we attempt to quantify the likelihood that a single transient event
can cause multiple bit errors in logic circuits consisting of combinational
gates and flip-flops. In particular, we calculate the conditional probability
of multiple bit-flips given that a single bit flips as a result of the
transient. To calculate this conditional probability, we use a Monte Carlo
technique in which samples are generated using detailed post-layout circuit
simulations. Our experiments on the ISCAS'85 benchmarks and a few other
circuits indicate that, this conditional probability is quite significant and
can be as high as 0.31. Thus we conclude that multiple bit-flips must
necessarily be considered in order to obtain a realistic architectural fault
model for soft errors."
"In the field of cryptography till date the 2-byte in 1-clock is the best
known RC4 hardware design [1], while 1-byte in 1-clock [2], and the 1-byte in 3
clocks [3][4] are the best known implementation. The design algorithm in[2]
considers two consecutive bytes together and processes them in 2 clocks. The
design [1] is a pipelining architecture of [2]. The design of 1-byte in
3-clocks is too much modular and clock hungry. In this paper considering the
RC4 algorithm, as it is, a simpler RC4 hardware design providing higher
throughput is proposed in which 6 different architecture has been proposed. In
design 1, 1-byte is processed in 1-clock, design 2 is a dynamic KSA-PRGA
architecture of Design 1. Design 3 can process 2 byte in a single clock, where
as Design 4 is Dynamic KSA-PRGA architecture of Design 3. Design 5 and Design 6
are parallelization architecture design 2 and design 4 which can compute 4 byte
in a single clock. The maturity in terms of throughput, power consumption and
resource usage, has been achieved from design 1 to design 6. The RC4 encryption
and decryption designs are respectively embedded on two FPGA boards as
co-processor hardware, the communication between the two boards performed using
Ethernet."
"In hardware implementation of a cryptographic algorithm, one may achieve
leakage of secret information by creating scopes to introduce controlled faulty
bit(s) even though the algorithm is mathematically a secured one. The technique
is very effective in respect of crypto processors embedded in smart cards. In
this paper few fault detecting architectures for RC4 algorithm are designed and
implemented on Virtex5(ML505, LX110t) FPGA board. The results indicate that the
proposed architectures can handle most of the faults without loss of throughput
consuming marginally additional hardware and power."
"A new design and novel architecture suitable for FPGA/ASIC implementation of
a 2D Gaussian surround function for image processing application is presented
in this paper. The proposed scheme results in enormous savings of memory
normally required for 2D Gaussian function implementation. In the present work,
the Gaussian symmetric characteristics which quickly falls off toward
plus/minus infinity has been used in order to save the memory. The 2D Gaussian
function implementation is presented for use in applications such as image
enhancement, smoothing, edge detection and filtering etc. The FPGA
implementation of the proposed 2D Gaussian function is capable of processing
(blurring, smoothing, and convolution) high resolution color pictures of size
up to $1600\times1200$ pixels at the real time video rate of 30 frames/sec. The
Gaussian design exploited here has been used in the core part of retinex based
color image enhancement. Therefore, the design presented produces Gaussian
output with three different scales, namely, 16, 64 and 128. The design was
coded in Verilog, a popular hardware design language used in industries,
conforming to RTL coding guidelines and fits onto a single chip with a gate
count utilization of 89,213 gates. Experimental results presented confirms that
the proposed method offers a new approach for development of large sized
Gaussian pyramid while reducing the on-chip memory utilization."
"Optical interconnection networks, as enabled by recent advances in silicon
photonic device and fabrication technology, have the potential to address
on-chip and off-chip communication bottlenecks in many-core systems. Although
several designs have shown superior power efficiency and performance compared
to electrical alternatives, these networks will not scale to the thousands of
cores required in the future.
  In this paper, we introduce Hermes, a hybrid network composed of an optimized
broadcast for power-efficient low-latency global-scale coordination and
circuit-switch sub-networks for high-throughput data delivery. This network
will scale for use in thousand core chip systems. At the physical level,
SoI-based adiabatic coupler has been designed to provide low-loss and compact
optical power splitting. Based on the adiabatic coupler, a topology based on
2-ary folded butterfly is designed to provide linear power division in a
thousand core layout with minimal cross-overs. To address the network agility
and provide for efficient use of optical bandwidth, a flow control and routing
mechanism is introduced to dynamically allocate bandwidth and provide fairness
usage of network resources. At the system level, bloom filter-based filtering
for localization of communication are designed for reducing global traffic. In
addition, a novel greedy-based data and workload migration are leveraged to
increase the locality of communication in a NUCA (non-uniform cache access)
architecture. First order analytic evaluation results have indicated that
Hermes is scalable to at least 1024 cores and offers significant performance
improvement and power savings over prior silicon photonic designs."
"When the Network-On-Chip (NoC) paradigm was introduced, many researchers have
proposed many novelistic NoC architectures, tools and design strategies. In
this paper we introduce a new approach in the field of designing
Network-On-Chip (NoC). Our inspiration came from an avionic protocol which is
the AFDX protocol. The proposed NoC architecture is a switch centric
architecture, with exclusive shortcuts between hosts and utilizes the
flexibility, the reliability and the performances offered by AFDX."
"By using the dynamic reconfigurable transceiver in high speed interface
design, designer can solve critical technology problems such as ensuring signal
integrity conveniently, with lower error binary rate. In this paper, we
designed a high speed XAUI (10Gbps Ethernet Attachment Unit Interface) to
transparently extend the physical reach of the XGMII. The following points are
focused: (1) IP (Intellectual Property) core usage. Altera Co. offers two
transceiver IP cores in Quartus II MegaWizard Plug-In Manager for XAUI design
which is featured of dynamic reconfiguration performance, that is,
ALTGX_RECO?FIG instance and ALTGX instance, we can get various groups by
changing settings of the devices without power off. These two blocks can
accomplish function of PCS (Physical Coding Sub-layer) and PMA (Physical Medium
Attachment), however, with higher efficiency and reliability. (2) 1+1
protection. In our design, two ALTGX IP cores are used to work in parallel,
which named XAUI0 and XAUI1. The former works as the main channel while the
latter redundant channel. When XAUI0 is out of service for some reasons, XAUI1
will start to work to keep the business. (3) RTL (Register Transfer Level)
coding with Verilog HDL and simulation. Create the ALTGX_RECO?FIG instance and
ALTGX instance, enable dynamic reconfiguration in the ALTGXB Megafunction, then
connect the ALTGX_RECO?FIG with the ALTGX instances. After RTL coding, the
design was simulated on VCS simulator. The validated result indicates that the
packets are transferred efficiently. FPGA makes high-speed optical
communication system design simplified."
"This paper discusses how hot carrier injection (HCI) can be exploited to
create a trojan that will cause hardware failures. The trojan is produced not
via additional logic circuitry but by controlled scenarios that maximize and
accelerate the HCI effect in transistors. These scenarios range from
manipulating the manufacturing process to varying the internal voltage
distribution. This new type of trojan is difficult to test due to its gradual
hardware degradation mechanism. This paper describes the HCI effect, detection
techniques and discusses the possibility for maliciously induced HCI trojans."
"This paper discusses the possible introduction of hidden reliability defects
during CMOS foundry fabrication processes that may lead to accelerated wearout
of the devices. These hidden defects or hardware Trojans can be created by
deviation from foundry design rules and processing parameters. The Trojans are
produced by exploiting time-based wearing mechanisms (HCI, NBTI, TDDB and EM)
and/or condition-based triggers (ESD, Latchup and Softerror). This class of
latent damage is difficult to test due to its gradual degradation nature. The
paper describes life-time expectancy results for various Trojan induced
scenarios. Semiconductor properties, processing and design parameters critical
for device reliability and Trojan creation are discussed."
"Field Programmable Gate Array technology (FPGA) is a highly configurable
option for implementing many sophisticated signal processing tasks in Software
Defined Radios (SDRs). Those types of radios are realized using highly
configurable hardware platforms. Convolutional codes are used in every robust
digital communication system and Viterbi algorithm is employed in wireless
communications to decode the convolutional codes. Such decoders are complex and
dissipate large amount of power. In this paper, a low power-reconfigurable
Viterbi decoder for WiMAX receiver is described using a VHDL code for FPGA
implementation. The proposed design is implemented on Xilinx Virtex-II Pro,
XC2vpx30 FPGA using the FPGA Advantage Pro package provided by Mentor Graphics
and ISE 10.1 by Xilinx."
"This article exhibits a particular encoding of logic circuits into a sheaf
formalism. The central result of this article is that there exists strictly
more information available to a circuit designer in this setting than exists in
static truth tables, but less than exists in event-level simulation. This
information is related to the timing behavior of the logic circuits, and
thereby provides a ``bridge'' between static logic analysis and detailed
simulation."
"Reversible logic is emerging as an important research area having its
application in diverse fields such as low power CMOS design, digital signal
processing, cryptography, quantum computing and optical information processing.
This paper presents a new 4*4 parity preserving reversible logic gate, IG. The
proposed parity preserving reversible gate can be used to synthesize any
arbitrary Boolean function. It allows any fault that affects no more than a
single signal readily detectable at the circuit's primary outputs. It is shown
that a fault tolerant reversible full adder circuit can be realized using only
two IGs. The proposed fault tolerant full adder (FTFA) is used to design other
arithmetic logic circuits for which it is used as the fundamental building
block. It has also been demonstrated that the proposed design offers less
hardware complexity and is efficient in terms of gate count, garbage outputs
and constant inputs than the existing counterparts."
"Irreversible logic circuits dissipate heat for every bit of information that
is lost. Information is lost when the input vector cannot be recovered from its
corresponding output vector. Reversible logic circuit naturally takes care of
heating because it implements only the functions that have one-to-one mapping
between its input and output vectors. Therefore reversible logic design becomes
one of the promising research directions in low power dissipating circuit
design in the past few years and has found its application in low power CMOS
design, digital signal processing and nanotechnology. This paper presents the
efficient approaches for designing reversible fast adders that implement carry
look-ahead and carry-skip logic. The proposed 16-bit high speed reversible
adder will include IG gates for the realization of its basic building block.
The IG gate is universal in the sense that it can be used to synthesize any
arbitrary Boolean-functions. The IG gate is parity preserving, that is, the
parity of the inputs matches the parity of the outputs. It allows any fault
that affects no more than a single signal readily detectable at the circuit's
primary outputs. Therefore, the proposed high speed adders will have the
inherent opportunity of detecting errors in its output side. It has also been
demonstrated that the proposed design offers less hardware complexity and is
efficient in terms of gate count, garbage outputs and constant inputs than the
existing counterparts."
"Reversible logic is emerging as an important research area having its
application in diverse fields such as low power CMOS design, digital signal
processing, cryptography, quantum computing and optical information processing.
This paper presents a new 4*4 universal reversible logic gate, IG. It is a
parity preserving reversible logic gate, that is, the parity of the inputs
matches the parity of the outputs. The proposed parity preserving reversible
gate can be used to synthesize any arbitrary Boolean function. It allows any
fault that affects no more than a single signal readily detectable at the
circuit's primary outputs. Finally, it is shown how a fault tolerant reversible
full adder circuit can be realized using only two IGs. It has also been
demonstrated that the proposed design offers less hardware complexity and is
efficient in terms of gate count, garbage outputs and constant inputs than the
existing counterparts."
"Combinational or Classical logic circuits dissipate heat for every bit of
information that is lost. Information is lost when the input vector cannot be
recovered from its corresponding output vector. Reversible logic circuit
implements only the functions having one-to-one mapping between its input and
output vectors and therefore naturally takes care of heating. Reversible logic
design becomes one of the promising research directions in low power
dissipating circuit design in the past few years and has found its application
in low power CMOS design, digital signal processing and nanotechnology. This
paper presents the efficient approaches for designing fault tolerant reversible
fast adders that implement carry look-ahead and carry-skip logic. The proposed
high speed reversible adders include MIG gates for the realization of its basic
building block. The MIG gate is universal and parity preserving. It allows any
fault that affects no more than a single signal readily detectable at the
circuit's primary outputs. It has also been demonstrated that the proposed
design offers less hardware complexity and is efficient in terms of gate count,
garbage outputs and constant inputs than the existing counterparts."
"Reversible circuits have applications in digital signal processing, computer
graphics, quantum computation and cryptography. In this paper, a generalized
k*k reversible gate family is proposed and a 3*3 gate of the family is
discussed. Inverter, AND, OR, NAND, NOR, and EXOR gates can be realized by this
gate. Implementation of a full-adder circuit using two such 3*3 gates is given.
This full-adder circuit contains only two reversible gates and produces no
extra garbage outputs. The proposed full-adder circuit is efficient in terms of
gate count, garbage outputs and quantum cost. A 4-bit carry skip adder is
designed using this full-adder circuit and a variable block carry skip adder is
discussed. Necessary equations required to evaluate these adder are presented."
"In this paper, we have implemented and designed a sorting network for
reversible logic circuits synthesis in terms of n*n Toffoli gates. The
algorithm presented in this paper constructs a Toffoli Network based on
swapping bit strings. Reduction rules are then applied by simple template
matching and removing useless gates from the network. Random selection of bit
strings and reduction of control inputs are used to minimize both the number of
gates and gate width. The method produces near optimal results for up to
3-input 3-output circuits."
"In almost all of the currently working circuits, especially in analog
circuits implementing signal processing applications, basic arithmetic
operations such as multiplication, addition, subtraction and division are
performed on values which are represented by voltages or currents. However, in
this paper, we propose a new and simple method for performing analog arithmetic
operations which in this scheme, signals are represented and stored through a
memristance of the newly found circuit element, i.e. memristor, instead of
voltage or current. Some of these operators such as divider and multiplier are
much simpler and faster than their equivalent voltage-based circuits and they
require less chip area. In addition, a new circuit is designed for programming
the memristance of the memristor with predetermined analog value. Presented
simulation results demonstrate the effectiveness and the accuracy of the
proposed circuits."
"Reversible logic has become one of the promising research directions in low
power dissipating circuit design in the past few years and has found its
applications in low power CMOS design, cryptography, optical information
processing and nanotechnology. This paper presents a novel and quantum cost
efficient reversible full adder gate in nanotechnology. This gate can work
singly as a reversible full adder unit and requires only one clock cycle. The
proposed gate is a universal gate in the sense that it can be used to
synthesize any arbitrary Boolean functions. It has been demonstrated that the
hardware complexity offered by the proposed gate is less than the existing
counterparts. The proposed reversible full adder gate also adheres to the
theoretical minimum established by the researchers."
"In this paper, we have introduced an algorithm to implement a sorting network
for reversible logic synthesis based on swapping bit strings. The algorithm
first constructs a network in terms of n*n Toffoli gates read from left to
right. The number of gates in the circuit produced by our algorithm is then
reduced by template matching and removing useless gates from the network. We
have also compared the efficiency of the proposed method with the existing
ones."
"In this paper, we have introduced the notion of UselessGate and
ReverseOperation. We have also given an algorithm to implement a sorting
network for reversible logic synthesis based on swapping bit strings. The
network is constructed in terms of n*n Toffoli Gates read from left to right
and it has shown that there will be no more gates than the number of swappings
the algorithm requires. The gate complexity of the network is O(n2). The number
of gates in the network can be further reduced by template reduction technique
and removing UselessGate from the network."
"The purpose of this project was to design and implement a pipeline
Analog-to-Digital Converter using 0.35um CMOS technology. Initial requirements
of a 25-MHz conversion rate and 8-bits of resolution where the only given ones.
Although additional secondary goals such as low power consumption and small
area were stated. The architecture is based on a 1.5 bit per stage structure
utilizing digital correction for each stage [12]. A differential switched
capacitor circuit consisting of a cascade gm-C op-amp with 200MHz ft is used
for sampling and amplification in each stage [12]. Differential dynamic
comparators are used to implement the decision levels required for the 1.5-b
per stage structure. Correction of the pipeline is accomplished by using
digital correction circuit consist of D-latches and full-adders. Area and Power
consumption of whole design was 0.24mm2 and 35mW respectively. The maximum
sample rate at which the converter gave an adequate output was 33MHz."
"This paper investigates the impact of the changes of the characteristic
polynomials and initial loadings, on behaviour of aliasing errors of parallel
signature analyzer (Multi-Input Shift Register), used in an LFSR based digital
circuit testing technique. The investigation is carried-out through an
extensive simulation study of the effectiveness of the LFSR based digital
circuit testing technique. The results of the study show that when the
identical characteristic polynomials of order n are used in both pseudo-random
test-pattern generator, as well as in Multi-Input Shift Register (MISR)
signature analyzer (parallel type) then the probability of aliasing errors
remains unchanged due to the changes in the initial loadings of the
pseudo-random test-pattern generator."
"In the field of cryptography till date the 1-byte in 1-clock is the best
known RC4 hardware design [1], while the 1-byte in 3clocks is the best known
implementation [2,3]. The design algorithm in [1] considers two consecutive
bytes together and processes them in 2 clocks. The design of 1-byte in 3-clocks
is too much modular and clock hungry. In this paper considering the RC4
algorithm, as it is, a simpler RC4 hardware design providing higher throughput
is proposed in which 1-byte is processed in 1-clock. In the design two
sequential tasks are executed as two independent events during rising and
falling edges of the same clock and the swapping is directly executed using a
MUX-DEMUX combination. The power consumed in behavioral and structural designs
of RC4 are estimated and a power optimization technique is proposed. The NIST
statistical test suite is run on RC4 key streams in order to know its
randomness property. The encryption and decryption designs are respectively
embedded on two FPGA boards with RC4 in a custom coprocessor followed by
Ethernet communication."
"The performance of an on-chip interconnection architecture used for
communication between IP cores depends on the efficiency of its bus
architecture. Any bus architecture having advantages of faster bus clock speed,
extra data transfer cycle, improved bus width and throughput is highly
desirable for a low cost, reduced time-to-market and efficient System-on-Chip
(SoC). This paper presents a survey of WISHBONE bus architecture and its
comparison with three other on-chip bus architectures viz. Advanced Micro
controller Bus Architecture (AMBA) by ARM, CoreConnect by IBM and Avalon by
Altera. The WISHBONE Bus Architecture by Silicore Corporation appears to be
gaining an upper edge over the other three bus architecture types because of
its special performance parameters like the use of flexible arbitration scheme
and additional data transfer cycle (Read-Modify-Write cycle). Moreover, its IP
Cores are available free for use requiring neither any registration nor any
agreement or license."
"Testing core based System on Chip is a challenge for the test engineers. To
test the complete SOC at one time with maximum fault coverage, test engineers
prefer to test each IP-core separately. At speed testing using external testers
is more expensive because of gigahertz processor. The purpose of this paper is
to develop cost efficient and flexible test methodology for testing digital
IP-cores . The prominent feature of the approach is to use microcontroller to
test IP-core. The novel feature is that there is no need of test pattern
generator and output response analyzer as microcontroller performs the function
of both. This approach has various advantages such as at speed testing, low
cost, less area overhead and greater flexibility since most of the testing
process is based on software."
"In the future, embedded processors must process more computation-intensive
network applications and internet traffic and packet-processing tasks become
heavier and sophisticated. Since the processor performance is severely related
to the average memory access delay and also the number of processor registers
affects the performance, cache and register file are two major parts in
designing embedded processor architecture. Although increasing cache and
register file size leads to performance improvement in embedded applications
and packet-processing tasks in high traffic networks with too much packets, the
increased area, power consumption and memory hierarchy delay are the overheads
of these techniques. Therefore, implementing these components in the optimum
size is of significant interest in the design of embedded processors. This
paper explores the effect of cache and register file size on the processor
performance to calculate the optimum size of these components for embedded
applications. Experimental results show that although having bigger cache and
register file is one of the performance improvement approaches in embedded
processors, however, by increasing the size of these parameters over a
threshold level, performance improvement is saturated and then, decreased."
"Security is the most important part in data communication system, where more
randomization in secret keys increases the security as well as complexity of
the cryptography algorithms. As a result in recent dates these algorithms are
compensating with enormous memory spaces and large execution time on hardware
platform. Field programmable gate arrays (FPGAs), provide one of the major
alternative in hardware platform scenario due to its reconfiguration nature,
low price and marketing speed. In FPGA based embedded system we can use
embedded processor to execute particular algorithm with the inclusion of a real
time operating System (RTOS), where threads may reduce resource utilization and
time consumption. A process in the runtime is separated in different smaller
tasks which are executed by the scheduler to meet the real time dead line using
RTOS. In this paper we demonstrate the design and implementation of a 128-bit
Advanced Encryption Standard (AES) both symmetric key encryption and decryption
algorithm by developing suitable hardware and software design on Xilinx
Spartan- 3E (XC3S500E-FG320) device using an Xilkernel RTOS, the implementation
has been tested successfully The system is optimized in terms of execution
speed and hardware utilization."
"Low-density parity-check codes are attractive for high throughput
applications because of their low decoding complexity per bit, but also because
all the codeword bits can be decoded in parallel. However, achieving this in a
circuit implementation is complicated by the number of wires required to
exchange messages between processing nodes. Decoding algorithms that exchange
binary messages are interesting for fully-parallel implementations because they
can reduce the number and the length of the wires, and increase logic density.
This paper introduces the Relaxed Half-Stochastic (RHS) decoding algorithm, a
binary message belief propagation (BP) algorithm that achieves a coding gain
comparable to the best known BP algorithms that use real-valued messages. We
derive the RHS algorithm by starting from the well-known Sum-Product algorithm,
and then derive a low-complexity version suitable for circuit implementation.
We present extensive simulation results on two standardized codes having
different rates and constructions, including low bit error rate results. These
simulations show that RHS can be an advantageous replacement for the existing
state-of-the-art decoding algorithms when targeting fully-parallel
implementations."
"There are a number of design decisions that impact a GPU's performance. Among
such decisions deciding the right warp size can deeply influence the rest of
the design. Small warps reduce the performance penalty associated with branch
divergence at the expense of a reduction in memory coalescing. Large warps
enhance memory coalescing significantly but also increase branch divergence.
This leaves designers with two choices: use a small warps and invest in finding
new solutions to enhance coalescing or use large warps and address branch
divergence employing effective control-flow solutions. In this work our goal is
to investigate the answer to this question. We analyze warp size impact on
memory coalescing and branch divergence. We use our findings to study two
machines: a GPU using small warps but equipped with excellent memory coalescing
(SW+) and a GPU using large warps but employing an MIMD engine immune from
control-flow costs (LW+). Our evaluations show that building
coalescing-enhanced small warp GPUs is a better approach compared to pursuing a
control-flow enhanced large warp GPU."
"We proposed in ""Functional Constraint Extraction From Register Transfer Level
for ATPG"" that is currently submitted to TVLSI, an automatic functional
constraint extractor that can be applied on the RT level. These functional
constraints are used to generate pseudo functional test patterns with ATPG
tools. The patterns are then used to improve the verification process. This
technical report complements the work proposed as it contains the
implementation details of the proposed methodology and shows the detailed
intermediate and final results of the application of this methodology on a
concrete example."
"In this paper, a novel reconfigurable architecture is proposed for
multifunctional image signal processing systems. A circuit-switched NoC is used
to provide interconnection because the non-TMD links ensure fixed throughput,
which is a desirable behavior for computational intensive image processing
algorithms compared with packet-switched NoC. Image processing algorithms are
modeled as synchronous dataflow graphs which provide a unified model for
general computing procedure. An image processing system is considered as
several temporally mutually exclusive algorithms. Thus, their dataflow graph
representations could be considered as a group and a merging algorithm could be
applied to generate a union graph while eliminating spatial redundancy for area
consumption optimization. After the union graph have been mapped and routed on
the NoC, the reconfigurable system could be configured to any of its target
image processing algorithms by properly setting the NoC topology. Experiments
show the demo reconfigurable system with two image processing applications cost
26.4% less hardware resource, compared with the non-reconfigurable
implementations."
"Modern multicore processors are employing large last-level caches, for
example Intel's E7-8800 processor uses 24MB L3 cache. Further, with each CMOS
technology generation, leakage energy has been dramatically increasing and
hence, leakage energy is expected to become a major source of energy
dissipation, especially in last-level caches (LLCs). The conventional schemes
of cache energy saving either aim at saving dynamic energy or are based on
properties specific to first-level caches, and thus these schemes have limited
utility for last-level caches. Further, several other techniques require
offline profiling or per-application tuning and hence are not suitable for
product systems. In this research, we propose novel cache leakage energy saving
schemes for single-core and multicore systems; desktop, QoS, real-time and
server systems. We propose software-controlled, hardware-assisted techniques
which use dynamic cache reconfiguration to configure the cache to the most
energy efficient configuration while keeping the performance loss bounded. To
profile and test a large number of potential configurations, we utilize
low-overhead, micro-architecture components, which can be easily integrated
into modern processor chips. We adopt a system-wide approach to save energy to
ensure that cache reconfiguration does not increase energy consumption of other
components of the processor. We have compared our techniques with the
state-of-art techniques and have found that our techniques outperform them in
their energy efficiency. This research has important applications in improving
energy-efficiency of higher-end embedded, desktop, server processors and
multitasking systems. We have also proposed performance estimation approach for
efficient design space exploration and have implemented time-sampling based
simulation acceleration approach for full-system architectural simulators."
"The trend in industry is towards heterogeneous multicore processors (HMCs),
including chips with CPUs and massively-threaded throughput-oriented processors
(MTTOPs) such as GPUs. Although current homogeneous chips tightly couple the
cores with cache-coherent shared virtual memory (CCSVM), this is not the
communication paradigm used by any current HMC. In this paper, we present a
CCSVM design for a CPU/MTTOP chip, as well as an extension of the pthreads
programming model, called xthreads, for programming this HMC. Our goal is to
evaluate the potential performance benefits of tightly coupling heterogeneous
cores with CCSVM."
"In recent years, researchers have explored use of non-volatile devices such
as STT-RAM (spin torque transfer RAM) for designing on-chip caches, since they
provide high density and consume low leakage power. A common limitation of all
non-volatile devices is their limited write endurance. Further, since existing
cache management policies are write-variation unaware, excessive writes to a
few blocks may lead to a quick failure of the whole cache. We propose an
architectural technique for wear-leveling of non-volatile last level caches
(LLCs). Our technique uses cache-coloring approach which adds a
software-controlled mapping layer between groups of physical pages and cache
sets. Periodically the mapping is altered to ensure that write-traffic can be
spread uniformly to different sets of the cache to achieve wear-leveling.
Simulations performed with an x86-64 simulator and SPEC2006 benchmarks show
that our technique reduces the worst-case writes to cache blocks and thus
improves the cache lifetime by 4.07X."
"The alignment of code in the flash memory of deeply embedded SoCs can have a
large impact on the total energy consumption of a computation. We investigate
the effect of code alignment in six SoCs and find that a large proportion of
this energy (up to 15% of total SoC energy consumption) can be saved by changes
to the alignment.
  A flexible model is created to predict the read-access energy consumption of
flash memory on deeply embedded SoCs, where code is executed in place. This
model uses the instruction level memory accesses performed by the processor to
calculate the flash energy consumption of a sequence of instructions. We derive
the model parameters for five SoCs and validate them. The error is as low as
5%, with a 11% average normalized RMS deviation overall.
  The scope for using this model to optimize code alignment is explored across
a range of benchmarks and SoCs. Analysis shows that over 30% of loops can be
better aligned. This can significantly reduce energy while increasing code size
by less than 4%. We conclude that this effect has potential as an effective
optimization, saving significant energy in deeply embedded SoCs."
"In this paper, we present a novel signal processing unit built upon the
theory of factor graphs, which is able to address a wide range of signal
processing algorithms. More specifically, the demonstrated factor graph
processor (FGP) is tailored to Gaussian message passing algorithms. We show how
to use a highly configurable systolic array to solve the message update
equations of nodes in a factor graph efficiently. A proper instruction set and
compilation procedure is presented. In a recursive least squares channel
estimation example we show that the FGP can compute a message update faster
than a state-ofthe- art DSP. The results demonstrate the usabilty of the FGP
architecture as a flexible HW accelerator for signal-processing and
communication systems."
"We introduce and experimentally validate a new macro-level model of the CPU
temperature/power relationship within nanometer-scale application processors or
system-on-chips. By adopting a holistic view, this model is able to take into
account many of the physical effects that occur within such systems. Together
with two algorithms described in the paper, our results can be used, for
instance by engineers designing power or thermal management units, to cancel
the temperature-induced bias on power measurements. This will help them gather
temperature-neutral power data while running multiple instance of their
benchmarks. Also power requirements and system failure rates can be decreased
by controlling the CPU's thermal behavior.
  Even though it is usually assumed that the temperature/power relationship is
exponentially related, there is however a lack of publicly available physical
temperature/power measurements to back up this assumption, something our paper
corrects. Via measurements on two pertinent platforms sporting nanometer-scale
application processors, we show that the power/temperature relationship is
indeed very likely exponential over a 20{\deg}C to 85{\deg}C temperature range.
Our data suggest that, for application processors operating between 20{\deg}C
and 50{\deg}C, a quadratic model is still accurate and a linear approximation
is acceptable."
"Due to the emergence of embedded applications in image and video processing,
communication and cryptography, improvement of pictorial information for better
human perception like deblurring, denoising in several fields such as satellite
imaging, medical imaging, mobile applications etc. are gaining importance for
renewed research. Behind such developments, the primary responsibility lies
with the advancement of semiconductor technology leading to FPGA based
programmable logic devices, which combines the advantages of both custom
hardware and dedicated DSP resources. In addition, FPGA provides powerful
reconfiguration feature and hence is an ideal target for rapid prototyping. We
have endeavoured to exploit exceptional features of FPGA technology in respect
to hardware parallelism leading to higher computational density and throughput,
and have observed better performances than those one can get just merely
porting the image processing software algorithms to hardware. In this paper, we
intend to present an elaborate review, based on our expertise and experiences,
on undertaking necessary transformation to an image processing software
algorithm including the optimization techniques that makes its operation in
hardware comparatively faster."
"Recent years have witnessed a phenomenal growth in the computational
capabilities and applications of GPUs. However, this trend has also led to
dramatic increase in their power consumption. This paper surveys research works
on analyzing and improving energy efficiency of GPUs. It also provides a
classification of these techniques on the basis of their main research idea.
Further, it attempts to synthesize research works which compare energy
efficiency of GPUs with other computing systems, e.g. FPGAs and CPUs. The aim
of this survey is to provide researchers with knowledge of state-of-the-art in
GPU power management and motivate them to architect highly energy-efficient
GPUs of tomorrow."
"The variation in the prescribed modulation schemes and code rates for WiMAX
interleaver design, as defined by IEEE 802.16 standard, demands a plethora of
hardware if all the modulation schemes and code rates have to be unified into a
single electronic device. Add to this the complexities involved with the
algorithms and permutations of the WiMAX standard, invariably dependent on
floor function which is extremely hardware inefficient. This paper is an
attempt towards removing the complexities and excess hardware involvement in
the implementation of the permutations involved in Deinterleaver designs as
defined by IEEE 802.16"
"This paper presents the FPGA hardware design of a turbo decoder for the
cdma2000 standard. The work includes a study and mathematical analysis of the
turbo decoding process, based on the MAX-Log-MAP algorithm. Results of decoding
for a packet size of two hundred fifty bits are presented, as well as an
analysis of area versus performance, and the key variables for hardware design
in turbo decoding."
"This paper presents a low-power ECG recording system-on-chip (SoC) with
on-chip low-complexity lossless ECG compression for data reduction in
wireless/ambulatory ECG sensor devices. The chip uses a linear slope predictor
for data compression, and incorporates a novel low-complexity dynamic
coding-packaging scheme to frame the prediction error into fixed-length 16-bit
format. The proposed technique achieves an average compression ratio of 2.25x
on MIT/BIH ECG database. Implemented in a standard 0.35 um process, the
compressor uses 0.565K gates/channel occupying 0.4 mm2 for four channels, and
consumes 535 nW/channel at 2.4 V for ECG sampled at 512 Hz. Small size and
ultra-low power consumption makes the proposed technique suitable for wearable
ECG sensor applications."
"This paper describes a highly integrated, low power chip solution for ECG
signal processing in wearable devices. The chip contains an instrumentation
amplifier with programmable gain, a band-pass filter, a 12-bit SAR ADC, a novel
QRS detector, 8K on-chip SRAM, and relevant control circuitry and CPU
interfaces. The analog front end circuits accurately senses and digitizes the
raw ECG signal, which is then filtered to extract the QRS. The sampling
frequency used is 256 Hz. ECG samples are buffered locally on an asynchronous
FIFO and is read out using a faster clock, as and when it is required by the
host CPU via an SPI interface. The chip was designed and implemented in 0.35um
standard CMOS process. The analog core operates at 1V while the digital
circuits and SRAM operate at 3.3V. The chip total core area is 5.74 mm^2 and
consumes 9.6uW. Small size and low power consumption make this design suitable
for usage in wearable heart monitoring devices."
"In this paper, we study how certain conditions can affect the transformations
on the states of the memory of a strict load-store Maurer ISA, when half of the
data memory serves as the part of the operating unit."
"The article describes an attempt to solve at once three basic problems
arising at testing a complex digital equipment for defects: 1) the problem of
an exponential increasing of the complexity of testing the equipment with the
complexity of the equipment; 2) the problem of testing of the tester; 3) the
problem of a mutual masking of defects. The proposed solution is nothing more
than using certain limitations for connections between usual logical gates.
Arbitrary multiple stuck-at-faults are supposed as defects."
"Reversible logic is experience renewed interest as we are approach the limits
of CMOS technologies. While physical implementations of reversible gates have
yet to materialize, it is safe to assume that they will rely on faulty
individual components. In this work we present a present a method to provide
fault tolerance to a reversible circuit based on invariant relationships."
"Multiple-input multiple-output (MIMO) wireless transmission imposes huge
challenges on the design of efficient hardware architectures for iterative
receivers. A major challenge is soft-input soft-output (SISO) MIMO demapping,
often approached by sphere decoding (SD). In this paper, we introduce the - to
our best knowledge - first VLSI architecture for SISO SD applying a single
tree-search approach. Compared with a soft-output-only base architecture
similar to the one proposed by Studer et al. in IEEE J-SAC 2008, the
architectural modifications for soft input still allow a one-node-per-cycle
execution. For a 4x4 16-QAM system, the area increases by 57% and the operating
frequency degrades by 34% only."
"To cope with the soft errors and make full use of the multi-core system, this
paper gives an efficient fault-tolerant hardware and software co-designed
architecture for multi-core systems. And with a not large number of test
patterns, it will use less than 33% hardware resources compared with the
traditional hardware redundancy (TMR) and it will take less than 50% time
compared with the traditional software redundancy (time redundant).Therefore,
it will be a good choice for the fault-tolerant architecture for the future
high-reliable multi-core systems."
"In this paper we propose an Intelligent Management System which is capable of
managing the automobile functions using the rigorous real-time principles and a
multicore processor in order to realize higher efficiency and safety for the
vehicle. It depicts how various automobile functionalities can be fine grained
and treated to fit in real time concepts. It also shows how the modern
multicore processors can be of good use in organizing vast amounts of
correlated functions to be executed in real-time with excellent time
commitments. The modeling of the automobile tasks with real time commitments,
organizing appropriate scheduling for various real time tasks and the usage of
a multicore processor enables the system to realize higher efficiency and offer
better safety levels to the vehicle. The industry available real time operating
system is used for scheduling various tasks and jobs on the multicore
processor."
"The main goal of this research is to develop the concepts of a revolutionary
processor system called Functional Processor System. The fairly novel work
carried out in this proposal concentrates on decoding of function pipelines and
distributing it in FPUs as a part of scheduling approach. As the functional
programs are super-level programs that entails requirements only at functional
level, decoding of functions and distribution of functions in the heterogeneous
functional processor units are a challenge. We explored the possibilities of
segregation of the functions from the application program and distributing the
functions on the relevant FPUs by using address mapping techniques. Here we
pursue the perception of feeding the functions into the processor farm rather
than the processor fetching the instructions or functions and executing it.
This work is carried out at theoretical levels and it requires a long way to go
in the realization of this work in hardware perhaps with a large industrial
team with a pragmatic time frame."
"This chapter describes the main architectures proposed in the literature to
implement the channel decoders required by the WiMax standard, namely
convolutional codes, turbo codes (both block and convolutional) and LDPC. Then
it shows a complete design of a convolutional turbo code encoder/decoder system
for WiMax."
"In this work novel results concerning Network-on-Chip-based turbo decoder
architectures are presented. Stemming from previous publications, this work
concentrates first on improving the throughput by exploiting adaptive-bandwidth
reduction techniques. This technique shows in the best case an improvement of
more than 60 Mb/s. Moreover, it is known that double-binary turbo decoders
require higher area than binary ones. This characteristic has the negative
effect of increasing the data width of the network nodes. Thus, the second
contribution of this work is to reduce the network complexity to support
doublebinary codes, by exploiting bit-level and pseudo-floating-point
representation of the extrinsic information. These two techniques allow for an
area reduction of up to more than the 40% with a performance degradation of
about 0.2 dB."
"An algebra-logical repair method for FPGA functional logic blocks on the
basis of solving the coverage problem is proposed. It is focused on
implementation into Infrastructure IP for system-on-a chip and
system-in-package. A method is designed for providing the operability of FPGA
blocks and digital system as a whole. It enables to obtain exact and optimal
solution associated with the minimum number of spares needed to repair the FPGA
logic components with multiple faults."
"This article describes high-speed multiprocessor architecture for the
concurrent analyzing information represented in analytic, graph- and table
forms of associative relations to search, recognize and make a decision in
n-dimensional vector discrete space. Vector-logical process models of actual
applications,for which the quality of solution is estimated by the proposed
integral non-arithmetical metric of the interaction between Boolean vectors,
are described."
"LDPC (Low Density Parity Check) codes are among the most powerful and widely
adopted modern error correcting codes. The iterative decoding algorithms
required for these codes involve high computational complexity and high
processing throughput is achieved by allocating a sufficient number of
processing elements (PEs). Supporting multiple heterogeneous LDPC codes on a
parallel decoder poses serious problems in the design of the interconnect
structure for such PEs. The aim of this work is to explore the feasibility of
NoC (Network on Chip) based decoders, where full flexibility in terms of
supported LDPC codes is obtained resorting to an NoC to connect PEs. NoC based
LDPC decoders have been previously considered unfeasible because of the cost
overhead associated to packet management and routing. On the contrary, the
designed NoC adopts a low complexity routing, which introduces a very limited
cost overhead with respect to architectures dedicated to specific classes of
codes. Moreover the paper proposes an efficient configuration technique, which
allows for fast on--the--fly switching among different codes. The decoder
architecture is scalable and VLSI synthesis results are presented for several
cases of study, including the whole set of WiMAX LDPC codes, WiFi codes and
DVB-S2 standard."
"This paper presents Multi-Amdahl, a resource allocation analytical tool for
heterogeneous systems. Our model includes multiple program execution segments,
where each one is accelerated by a specific hardware unit. The acceleration
speedup of the specific hardware unit is a function of a limited resource, such
as the unit area, power, or energy. Using the Lagrange theorem we discover the
optimal resource distribution between all specific units. We then illustrate
this general Multi-Amdahl technique using several examples of area and power
allocation among several cores and accelerators."
"Quantum computer requires quantum arithmetic. The sophisticated design of a
reversible arithmetic logic unit (reversible ALU) for quantum arithmetic has
been investigated in this letter. We provide explicit construction of
reversible ALU effecting basic arithmetic operations. By provided the
corresponding control unit, the proposed reversible ALU can combine the
classical arithmetic and logic operation in a reversible integrated system.
This letter provides actual evidence to prove the possibility of the
realization of reversible Programmable Logic Device (RPLD) using reversible
ALU."
"This paper presents approaches to develop efficient network for non-binary
quasi-cyclic LDPC (QC-LDPC) decoders. By exploiting the intrinsic shifting and
symmetry properties of the check matrices, significant reduction of memory size
and routing complexity can be achieved. Two different efficient network
architectures for Class-I and Class-II non-binary QC-LDPC decoders have been
proposed, respectively. Comparison results have shown that for the code of the
64-ary (1260, 630) rate-0.5 Class-I code, the proposed scheme can save more
than 70.6% hardware required by shuffle network than the state-of-the-art
designs. The proposed decoder example for the 32-ary (992, 496) rate-0.5
Class-II code can achieve a 93.8% shuffle network reduction compared with the
conventional ones. Meanwhile, based on the similarity of Class-I and Class-II
codes, similar shuffle network is further developed to incorporate both classes
of codes at a very low cost."
"Polar codes have become one of the most favorable capacity achieving error
correction codes (ECC) along with their simple encoding method. However, among
the very few prior successive cancellation (SC) polar decoder designs, the
required long code length makes the decoding latency high. In this paper,
conventional decoding algorithm is transformed with look-ahead techniques. This
reduces the decoding latency by 50%. With pipelining and parallel processing
schemes, a parallel SC polar decoder is proposed. Sub-structure sharing
approach is employed to design the merged processing element (PE). Moreover,
inspired by the real FFT architecture, this paper presents a novel input
generating circuit (ICG) block that can generate additional input signals for
merged PEs on-the-fly. Gate-level analysis has demonstrated that the proposed
design shows advantages of 50% decoding latency and twice throughput over the
conventional one with similar hardware cost."
"Nowadays polar codes are becoming one of the most favorable capacity
achieving error correction codes for their low encoding and decoding
complexity. However, due to the large code length required by practical
applications, the few existing successive cancellation (SC) decoder
implementations still suffer from not only the high hardware cost but also the
long decoding latency. This paper presents novel several approaches to design
low-latency decoders for polar codes based on look-ahead techniques. Look-ahead
techniques can be employed to reschedule the decoding process of polar decoder
in numerous approaches. However, among those approaches, only well-arranged
ones can achieve good performance in terms of both latency and hardware
complexity. By revealing the recurrence property of SC decoding chart, the
authors succeed in reducing the decoding latency by 50% with look-ahead
techniques. With the help of VLSI-DSP design techniques such as pipelining,
folding, unfolding, and parallel processing, methodologies for four different
polar decoder architectures have been proposed to meet various application
demands. Sub-structure sharing scheme has been adopted to design the merged
processing element (PE) for further hardware reduction. In addition, systematic
methods for construction refined pipelining decoder (2nd design) and the input
generating circuits (ICG) block have been given. Detailed gate-level analysis
has demonstrated that the proposed designs show latency advantages over
conventional ones with similar hardware cost."
"Advancements in multi-core have created interest among many research groups
in finding out ways to harness the true power of processor cores. Recent
research suggests that on-board component such as cache memory plays a crucial
role in deciding the performance of multi-core systems. In this paper,
performance of cache memory is evaluated through the parameters such as cache
access time, miss rate and miss penalty. The influence of cache parameters over
execution time is also discussed. Results obtained from simulated studies of
multi-core environments with different instruction set architectures (ISA) like
ALPHA and X86 are produced."
"Power dissipation and energy consumption have become one of the most
important problems in the design of processors today. This is especially true
in power-constrained environments, such as embedded and mobile computing. While
lowering the operational voltage can reduce power consumption, there are limits
imposed at design time, beyond which hardware components experience faulty
operation. Moreover, the decrease in feature size has led to higher
susceptibility to process variations, leading to reliability issues and
lowering yield. However, not all computations and all data in a workload need
to maintain 100% fidelity. In this paper, we explore the idea of employing
functional or storage units that let go the conservative guardbands imposed on
the design to guarantee reliable execution. Rather, these units exhibit Elastic
Fidelity, by judiciously lowering the voltage to trade-off reliable execution
for power consumption based on the error guarantees required by the executing
code. By estimating the accuracy required by each computational segment of a
workload, and steering each computation to different functional and storage
units, Elastic Fidelity Computing obtains power and energy savings while
reaching the reliability targets required by each computational segment. Our
preliminary results indicate that even with conservative estimates, Elastic
Fidelity can reduce the power and energy consumption of a processor by 11-13%
when executing applications involving human perception that are typically
included in modern mobile platforms, such as audio, image, and video decoding."
"In this paper a low power and low area array multiplier with carry save adder
is proposed. The proposed adder eliminates the final addition stage of the
multiplier than the conventional parallel array multiplier. The conventional
and proposed multiplier both are synthesized with 16-T full adder. Among
Transmission Gate, Transmission Function Adder, 14-T, 16-T full adder shows
energy efficiency. In the proposed 4x4 multiplier to add carry bits with out
using Ripple Carry Adder (RCA) in the final stage, the carries given to the
input of the next left column input. Due to this the proposed multiplier shows
56 less transistor count, then cause trade off in power and area. The proposed
multiplier has shown 13.91% less power, 34.09% more speed and 59.91% less
energy consumption for TSMC 0.18nm technology at a supply voltage 2.0V than the
conventional multiplier."
"Now days, manufacturers are focusing on increasing the concurrency in
multiprocessor system-on-a-chip (MPSoC) architecture instead of increasing
clock speed, for embedded systems. Traditionally lock-based synchronization is
provided to support concurrency; as managing locks can be very difficult and
error prone. Transactional memories and lock based systems have been
extensively used to provide synchronization between multiple processors [1] in
general-purpose systems. It has been shown that locks have numerous
shortcomings over transactional memory in terms of power consumption, ease of
programming and performance. In this paper, we propose a new semaphore scheme
for synchronization in shared cache memory in an MPSoC. Moreover, we have
evaluated and compared our scheme with locks and transactions in terms of
energy consumption and cache miss rate using SimpleScalar functional simulator."
"This report introduces a shared resource arbitration scheme ""DPQ - Dynamic
Priority Queue"" which provides bandwidth guarantees and low worst case latency
to each master in an MPSoC. Being a non-trivial candidate for timing analysis,
SDRAM has been chosen as a showcase, but the approach is valid for any shared
resource arbitration.
  Due to its significant cost, data rate and physical size advantages, SDRAM is
a potential candidate for cost sensitive, safety critical and space conserving
systems. The variable access latency is a major drawback of SDRAM that induces
largely over estimated Worst Case Execution Time (WCET) bounds of applications.
In this report we present the DPQ together with an algorithm to predict the
shared SDRAM's worst case latencies. We use the approach to calculate WCET
bounds of six hardware tasks executing on an Altera Cyclone III FPGA with
shared DDR2 memory. The results show that the DPQ is a fair arbitration scheme
and produces low WCET bounds."
"This paper describes the design and development of low cost USB Data
Acquisition System (DAS) for the measurement of physical parameters. Physical
parameters such as temperature, humidity, light intensity etc., which are
generally slowly varying signals are sensed by respective sensors or integrated
sensors and converted into voltages. The DAS is designed using PIC18F4550
microcontroller, communicating with Personal Computer (PC) through USB
(Universal Serial Bus). The designed DAS has been tested with the application
program developed in Visual Basic, which allows online monitoring in graphical
as well as numerical display."
"The present paper describes the design of a cost effective, better resolution
data acquisition system (DAS) which is compatible to most of the PC and
laptops. A low cost DAS has been designed using PIC12F675 having 4-channel
analog input with 10-bit resolution for the monitoring of slowly varying
signals. The DAS so designed is interfaced to the serial port of the PC.
Firmware is written in Basic using Oshonsoft PIC IDE and burn to the
microcontroller by using PICkit2 programmer. An application program is also
developed using Visual Basic 6 which allows to display the waveform of the
signal(s) and simultaneously the data also can be saved into the hard disk of
the computer for future use and analysis."
"A low cost PC based real time data logging system can be used in the
laboratories for the measurement, monitoring and storage of the data for slowly
varying signals in science and engineering stream. This can be designed and
interfaced to the PCs Parallel Port, which is common to all desktop computers
or Personal Computers (PCs). By the use of this data logging system one can
monitor, measure and store data for slowly varying signals, which is hard to
visualise the signal waveforms by ordinary CRO (Cathode Ray Oscilloscope) and
DSO (Digital Storage Oscilloscope). The data so stored can be used for further
study and analysis. It can be used for a wide range of applications to monitor
and store data of temperature, humidity, light intensity, ECG signals etc. with
proper signal conditioning circuitry."
"Adder cells using Gate Diffusion Technique (GDI) & PTL-GDI technique are
described in this paper. GDI technique allows reducing power consumption,
propagation delay and low PDP (power delay product) whereas Pass Transistor
Logic (PTL) reduces the count of transistors used to make different logic
gates, by eliminating redundant transistors. Performance comparison with
various Hybrid Adder is been presented. In this paper, we propose two new
designs based on GDI & PTL techniques, which is found to be much more power
efficient in comparison with existing design technique. Only 10 transistors are
used to implement the SUM & CARRY function for both the designs. The SUM and
CARRY cell are implemented in a cascaded way i.e. firstly the XOR cell is
implemented and then using XOR as input SUM as well as CARRY cell is
implemented. For Proposed GDI adder the SUM as well as CARRY cell is designed
using GDI technique. On the other hand in Proposed PTL-GDI adder the SUM cell
is constructed using PTL technique and the CARRY cell is designed using GDI
technique. The advantages of both the designs are discussed. The significance
of these designs is substantiated by the simulation results obtained from
Cadence Virtuoso 180nm environment."
"Prices of NAND flash memories are falling drastically due to market growth
and fabrication process mastering while research efforts from a technological
point of view in terms of endurance and density are very active. NAND flash
memories are becoming the most important storage media in mobile computing and
tend to be less confined to this area. The major constraint of such a
technology is the limited number of possible erase operations per block which
tend to quickly provoke memory wear out. To cope with this issue,
state-of-the-art solutions implement wear leveling policies to level the wear
out of the memory and so increase its lifetime. These policies are integrated
into the Flash Translation Layer (FTL) and greatly contribute in decreasing the
write performance. In this paper, we propose to reduce the flash memory wear
out problem and improve its performance by absorbing the erase operations
throughout a dual cache system replacing FTL wear leveling and garbage
collection services. We justify this idea by proposing a first performance
evaluation of an exclusively cache based system for embedded flash memories.
Unlike wear leveling schemes, the proposed cache solution reduces the total
number of erase operations reported on the media by absorbing them in the cache
for workloads expressing a minimal global sequential rate."
"Increase in the speed of processors has led to crucial role of communication
in the performance of systems. As a result, routing is taken into consideration
as one of the most important subjects of the Network on Chip architecture.
Routing algorithms to deadlock avoidance prevent packets route completely based
on network traffic condition by means of restricting the route of packets. This
action leads to less performance especially in non-uniform traffic patterns. On
the other hand True Fully Adoptive Routing algorithm provides routing of
packets completely based on traffic condition. However, deadlock detection and
recovery mechanisms are needed to handle deadlocks. Use of global bus beside
NoC as a parallel supportive environment, provide platform to offer advantages
of both features of bus and NoC. This bus is useful for broadcast and multicast
operations, sending delay sensitive signals, system management and other
services. In this research, we use this bus as an escaping path for deadlock
recovery technique. According to simulation results, this bus is suitable
platform for deadlock recovery technique."
"A prototype of a very high dynamic range 32-bits Digital to Analog Converter
(DAC) was designed and built for the purpose of direct auditory stimulus
generation. It provides signals from less than 100 nV up to 50 Watts peak power
output, driving a 32-Ohms earphone or speaker. The use of ternary cells makes
possible a 170 dB dynamic range that is basically limited by thermal noise
only."
"Memory system is often the main bottleneck in chipmultiprocessor (CMP)
systems in terms of latency, bandwidth and efficiency, and recently
additionally facing capacity and power problems in an era of big data. A lot of
research works have been done to address part of these problems, such as
photonics technology for bandwidth, 3D stacking for capacity, and NVM for power
as well as many micro-architecture level innovations. Many of them need a
modification of current memory architecture, since the decades-old synchronous
memory architecture (SDRAM) has become an obstacle to adopt those advances.
However, to the best of our knowledge, none of them is able to provide a
universal memory interface that is scalable enough to cover all these problems.
  In this paper, we argue that a message-based interface should be adopted to
replace the traditional bus-based interface in memory system. A novel message
interface based memory system (MIMS) is proposed. The key innovation of MIMS is
that processor and memory system communicate through a universal and flexible
message interface. Each message packet could contain multiple memory requests
or commands along with various semantic information. The memory system is more
intelligent and active by equipping with a local buffer scheduler, which is
responsible to process packet, schedule memory requests, and execute specific
commands with the help of semantic information. The experimental results by
simulator show that, with accurate granularity message, the MIMS would improve
performance by 53.21%, while reducing energy delay product (EDP) by 55.90%, the
effective bandwidth utilization is improving by 62.42%. Furthermore, combining
multiple requests in a packet would reduce link overhead and provide
opportunity for address compression."
"NoCs have become a widespread paradigm in the system-on-chip design world,
not only for multi-purpose SoCs, but also for application-specific ICs. The
common approach in the NoC design world is to separate the design of the
interconnection from the design of the processing elements: this is well suited
for a large number of developments, but the need for joint application and NoC
design is not uncommon, especially in the application specific case. The
correlation between processing and communication tasks can be strong, and
separate or trace-based simulations fall often short of the desired precision.
In this work, the OMNET++ based JANoCS simulator is presented: concurrent
simulation of processing and communication allow cycle-accurate evaluation of
the system. Two cases of study are presented, showing both the need for joint
simulations and the effectiveness of JANoCS."
"An intensive use of reconfigurable hardware is expected in future embedded
systems. This means that the system has to decide which tasks are more suitable
for hardware execution. In order to make an efficient use of the FPGA it is
convenient to choose one that allows hardware multitasking, which is
implemented by using partial dynamic reconfiguration. One of the challenges for
hardware multitasking in embedded systems is the online management of the only
reconfiguration port of present FPGA devices. This paper presents different
online reconfiguration scheduling strategies which assign the reconfiguration
interface resource using different criteria: workload distribution or task
deadline. The online scheduling strategies presented take efficient and fast
decisions based on the information available at each moment. Experiments have
been made in order to analyze the performance and convenience of these
reconfiguration strategies."
"Performance evaluation of the routing node in terms of latency is the
characteristics of an efficient design of Buffer in input module. It is
intended to study and quantify the behavior of the single packet array design
in relation to the multiple packet array design. The utilization efficiency of
the packet buffer array improves when a common buffer is used instead of
individual buffers in each input port. First Poissons Queuing model was
prepared to manifest the differences in packet delays. The queuing model can be
classified as (M/M/1), (32/FIFO). Arrival rate has been assumed to be Poisson
distributed with a mean arrival rate of 10 x 1000000. The service rate is
assumed to be exponentially distributed with a mean service rate of 10.05 x
1000000. It has been observed that latency in Common Buffer improved by 46
percent over its distributed buffer. A Simulink model later simulated on MATLAB
to calculate the improvement in packet delay. It has been observed that the
delay improved by approximately 40 percent through the use of a common buffer.
A verilog RTL for both common and shared buffer has been prepared and later
synthesized using Design Compiler of SYNOPSYS. In distributed buffer, arrival
of data packet could be delayed by 2 or 4 clock cycles which lead to latency
improvement either by 17 percent or 34 percent in a common buffer"
"A low-power Content-Addressable-Memory (CAM) is introduced employing a new
mechanism for associativity between the input tags and the corresponding
address of the output data. The proposed architecture is based on a recently
developed clustered-sparse-network using binary-weighted connections that
on-average will eliminate most of the parallel comparisons performed during a
search. Therefore, the dynamic energy consumption of the proposed design is
significantly lower compared to that of a conventional low-power CAM design.
Given an input tag, the proposed architecture computes a few possibilities for
the location of the matched tag and performs the comparisons on them to locate
a single valid match. A 0.13 um CMOS technology was used for simulation
purposes. The energy consumption and the search delay of the proposed design
are 9.5%, and 30.4% of that of the conventional NAND architecture respectively
with a 3.4% higher number of transistors."
"This paper describes a new memristor crossbar architecture that is proposed
for use in a high density cache design. This design has less than 10% of the
write energy consumption than a simple memristor crossbar. Also, it has up to 4
times the bit density of an STT-MRAM system and up to 11 times the bit density
of an SRAM architecture. The proposed architecture is analyzed using a detailed
SPICE analysis that accounts for the resistance of the wires in the memristor
structure. Additionally, the memristor model used in this work has been matched
to specific device characterization data to provide accurate results in terms
of energy, area, and timing."
"Complementary metal oxide semiconductor technology (CMOS) has been faced
critical challenges in nano-scale regime. CNTFET (Carbon Nanotube Field effect
transistor) technology is a promising alternative for CMOS technology. In this
paper, we proposed a novel 7-input minority gate in CNTFET technology that has
only 9 CNTFETs. Minority function is utilized in the voting systems for
decision making and also it is used in data mining. This proposed 7-input
minority gate is utilized less fewer transistors than the conventional CMOS
method which utilizes many transistors for implementing sum of products. By
means of this proposed 7-input minority gate, a 4-input NAND gate can be
implemented, which gets better the conventional design in terms of delay and
energy efficiency and has much more deriving power at its output."
"This report lays flat my personal views on D-RISC and Microgrids as of March
2013. It reflects the opinions and insights that I have gained from working on
this project during the period 2008-2013. This report is structed in two parts:
deconstruction and reconstruction. In the deconstruction phase, I review what I
believe are the fundamental motivation and goals of the D-RISC/Microgrids
enterprise, and identify what I judge are shortcomings: that the project did
not deliver on its expectations, that fundamental questions are left
unanswered, and that its original motivation may not even be relevant in
scientific research any more in this day and age. In the reconstruction phase,
I start by identifying the merits of the current D-RISC/Microgrids technology
and know-how taken at face value, re-motivate its existence from a different
angle, and suggest new, relevant research questions that could justify
continued scientific investment."
"Due to continuous evolution of Systems-on-Chip (SoC), the complexity of their
design and development has augmented exponentially. To deal with the
ever-growing complexity of such embedded systems, we introduce, in this paper,
an object-oriented approach to rapid SoC design using auto-generation of
hardware custom instructions to simplify and accelerate the SoC design process.
In our approach, a Data Flow Graph (DFG) is adopted as a representation of the
arithmetic operation to convert it to a custom instruction. Then VHDL code will
be automatically generated. The input C code is automatically updated for
calling the new hardware components. To prove the effectiveness of the proposed
approach, a Java source code framework named Automatic Custom Architecture
generator (ACAgen) is developed. Experimental results on 3D sample application
validate our approach and demonstrate how the proposed framework facilitates
and accelerates the SoC design process at low costs."
"As the process technologies scale into deep submicron region, crosstalk delay
is becoming increasingly severe, especially for global on-chip buses. To cope
with this problem, accurate delay models of coupled interconnects are needed.
In particular, delay models based on analytical approaches are desirable,
because they not only are largely transparent to technology, but also
explicitly establish the connections between delays of coupled interconnects
and transition patterns, thereby enabling crosstalk alleviating techniques such
as crosstalk avoidance codes (CACs). Unfortunately, existing analytical delay
models, such as the widely cited model in [1], have limited accuracy and do not
account for loading capacitance. In this paper, we propose analytical delay
models for coupled interconnects that address these disadvantages. By
accounting for more wires and eschewing the Elmore delay, our delay models
achieve better accuracy than the model in [1]."
"Manycore System-on-Chip include an increasing amount of processing elements
and have become an important research topic for improvements of both hardware
and software. While research can be conducted using system simulators,
prototyping requires a variety of components and is very time consuming. With
the Open Tiled Manycore System-on-Chip (OpTiMSoC) we aim at building such an
environment for use in our and other research projects as prototyping platform.
  This paper describes the project goals and aspects of OpTiMSoC and summarizes
the current status and ideas."
"As the number of cores in a single chip increases, a typical implementation
of coherence protocol adds significant hardware and complexity overhead.
Besides, the performance of CMP system depends on the data access latency,
which is highly affected by coherence protocol and on-chip interconnect. In
this paper, we propose PPB (Phase-Priority Based) cache coherence protocol, an
optimization of modern directory coherence protocol. We take advantage of the
observation that transient states occur in directory coherence protocol,
resulting in some unnecessary transient states and stalling. PPB cache
coherence protocol decouples a coherence transaction and introduces the idea of
phase message. This phase is considered as the priority of the message.
Additionally, we also add new priority-based arbitrators in on-chip network to
support PPB cache coherence protocol. This mechanism in on-chip network can
support effective cache access, which makes the on-chip network more efficient.
Our analysis on an execution-driven full system simulator using SPLASH-2
benchmark shows that PPB cache coherence outperforms a MESI based directory,
and the number of unnecessary transient states and stalling reduces up to 24%.
Also it reported the speedup of 7.4%. Other advantages of this strategy are
reduced delay of flits and significantly less energy consumption in on-chip
network."
"Development of modern integrated circuit technologies makes it feasible to
develop cheaper, faster and smaller special purpose signal processing function
circuits. Digital Signal processing functions are generally implemented either
on ASICs with inflexibility, or on FPGAs with bottlenecks of relatively smaller
utilization factor or lower speed compared to ASIC. Field Programmable DSP
Array (FPDA) is the proposed DSP dedicated device, redolent to FPGA, but with
basic fixed common modules (CMs) (like adders, subtractors, multipliers,
scaling units, shifters) instead of CLBs. This paper introduces the development
of reconfigurable system architecture with a focus on FPDA that integrates
different DSP functions like DFT, FFT, DCT, FIR, IIR, and DWT etc. The
switching between DSP functions is occurred by reconfiguring the
interconnection between CMs. Validation of the proposed architecture has been
achieved on Virtex5 FPGA. The architecture provides sufficient amount of
flexibility, parallelism and scalability."
"In today's world everyday a new technology which is faster, smaller and more
complex than its predecessor is being developed. The increased number of
transistors packed onto a chip of a conventional system results in increased
power consumption that is why Reversible logic has drawn attention of
Researchers due to its less heat dissipating characteristics. Reversible logic
can be imposed over applications such as quantum computing, optical computing,
quantum dot cellular automata, low power VLSI circuits, DNA computing. This
paper presents the reversible combinational circuit of adder, subtractor and
parity preserving subtractor. The suggested circuit in this paper are designed
using Feynman, Double Feynman and MUX gates which are better than the existing
one in literature in terms of Quantum cost, Garbage output and Total logical
calculations."
"This study presents a novel computer architecture where a last level cache
and a SIMD accelerator are replaced by an Associative Processor. Associative
Processor combines data storage and data processing and provides parallel
computational capabilities and data memory at the same time. An analytic
performance model of the new computer architecture is introduced. Comparative
analysis supported by simulation shows that this novel architecture may
outperform a conventional architecture comprising a SIMD coprocessor and a
shared last level cache while consuming less power."
"This paper proposes a PCI Express (PCIE) Wrapper core named PWrapper with
FIFO interfaces. Compared with other PCIE solutions, PWrapper has several
advantages such as flexibility, isolation of clock domain, etc. PWrapper is
implemented and verified on Vertex -5-FX70T which is a development board
provided by Xilinx Inc. Architecture of PWrapper and design of two key modules
are illustrated, which timing optimization methods have been adopted. Then we
explained the advantages and challenges of on-chip interfaces technology based
on FIFOs. The verification results show that PWrapper can achieve the speed of
1.8Gbps (Giga bits per second)."
"Associative memories are structures that can retrieve previously stored
information given a partial input pattern instead of an explicit address as in
indexed memories. A few hardware approaches have recently been introduced for a
new family of associative memories based on Sparse-Clustered Networks (SCN)
that show attractive features. These architectures are suitable for
implementations with low retrieval latency, but are limited to small networks
that store a few hundred data entries. In this paper, a new hardware
architecture of SCNs is proposed that features a new data-storage technique as
well as a method we refer to as Selective Decoding (SD-SCN). The SD-SCN has
been implemented using a similar FPGA used in the previous efforts and achieves
two orders of magnitude higher capacity, with no error-performance penalty but
with the cost of few extra clock cycles per data access."
"Due to increasing cache sizes and large leakage consumption of SRAM device,
conventional SRAM caches contribute significantly to the processor power
consumption. Recently researchers have used non-volatile memory devices to
design caches, since they provide high density, comparable read latency and low
leakage power dissipation. However, their high write latency may increase the
execution time and hence, leakage energy consumption. Also, since their write
endurance is small, a conventional energy saving technique may further
aggravate the problem of write-variations, thus reducing their lifetime. In
this paper, we present a cache energy saving technique for non-volatile caches,
which also attempts to improve their lifetime by making writes equally
distributed to the cache. Our technique uses dynamic cache reconfiguration to
adjust the cache size to meet program requirement and turns off the remaining
cache to save energy. Microarchitectural simulations performed using an x86-64
simulator, SPEC2006 benchmarks and a resistive-RAM LLC (last level cache) show
that over an 8MB baseline cache, our technique saves 17.55% memory subsystem
(last level cache + main memory) energy and improves the lifetime by 1.33X.
Over the same resistive-RAM baseline, an SRAM of similar area with no cache
reconfiguration leads to an energy loss of 186.13%."
"In this paper, we present a SRAM-PCM hybrid cache design, along with a cache
replacement policy, named dead fast block (DFB) to manage the hybrid cache.
This design aims to leverage the best features of both SRAM and PCM devices.
Compared to a PCM-only cache, the hybrid cache with DFB policy provides
superior results on all relevant evaluation metrics, viz. cache lifetime,
performance and energy efficiency. Also, use of DFB policy for managing the
hybrid cache provides better results compared to LRU replacement policy on all
the evaluation metrics."
"Effects of radiation on electronic circuits used in extra-terrestrial
applications and radiation prone environments need to be corrected. Since FPGAs
offer flexibility, the effects of radiation on them need to be studied and
robust methods of fault tolerance need to be devised. In this paper a new
fault-tolerant design strategy has been presented. This strategy exploits the
relation between changes in inputs and the expected change in output.
Essentially, it predicts whether or not a change in the output is expected and
thereby calculates the error. As a result this strategy reduces hardware and
time redundancy required by existing strategies like Duplication with
Comparison (DWC) and Triple Modular Redundancy (TMR). The design arising from
this strategy has been simulated and its robustness to fault-injection has been
verified. Simulations for a 16 bit multiplier show that the new design strategy
performs better than the state-of-the-art on critical factors such as hardware
redundancy, time redundancy and power consumption."
"3D integration has the potential to improve the scalability and performance
of Chip Multiprocessors (CMP). A closed form analytical solution for optimizing
3D CMP cache hierarchy is developed. It allows optimal partitioning of the
cache hierarchy levels into 3D silicon layers and optimal allocation of area
among cache hierarchy levels under constrained area and power budgets. The
optimization framework is extended by incorporating the impact of multithreaded
data sharing on the private cache miss rate. An analytical model for cache
access time as a function of cache size and a number of 3D partitions is
proposed and verified using CACTI simulation."
"Subthreshold circuit designs are very much popular for some of the ultra low
power applications, where the minimum energy consumption is the primary
concern. But, due to the weak driving current, these circuits generally suffer
from huge performance degradation. Therefore, in this paper, we primarily
targeted to analyze the performance of a Near-Threshold Circuit (NTC), which
retains the excellent energy efficiency of the subthreshold design, while
improving the performance to a certain extent. A modified row-based dual Vdd
4-operand CSA (Carry Save Adder) design has been reported in the present work
using 45 nm technology. Moreover, to find out the effectiveness of the
near-threshold operation of the 4-operand CSA design; it has been compared with
the other design styles. From the simulation results, obtained for the
frequency of 20 MHz, we found that the proposed scheme of CSA design consumes
3.009*10-7 Watt of Average Power (Pavg), which is almost 90.9 % lesser than
that of the conventional CSA design. Whereas, looking at the perspective of
maximum delay at output, the proposed scheme of CSA design provides a fair
44.37 % improvement, compared to that of the subthreshold CSA design."
"Last level caches (LLCs) occupy a large chip-area and there size is expected
to grow further to offset the limitations of memory bandwidth and speed. Due to
high leakage consumption of SRAM device, caches designed with SRAM consume
large amount of energy. To address this, use of emerging technologies such as
spin torque transfer RAM (STT-RAM) has been investigated which have lower
leakage power dissipation. However, the high write latency and power of it may
lead to large energy consumption which present challenges in its use. In this
report, we propose a cache reconfiguration based technique for improving the
energy efficiency of STT-RAM based LLCs. Our technique dynamically adjusts the
active cache size to reduce the cache leakage energy consumption with minimum
performance loss. We choose a suitable value of STT-RAM retention time for
avoiding refresh overhead and gaining performance. Single-core simulations have
been performed using SPEC2006 benchmarks and Sniper x86-64 simulator. The
results show that while, compared to an STT-RAM LLC of similar area, an SRAM
LLC incurs nearly 100% loss in energy and 7.3% loss in performance; our
technique using STT-RAM cache saves 21.8% energy and incurs only 1.7% loss in
performance."
"Nowadays System-On-Chips (SoCs) have evolved considerably in term of
performances, reliability and integration capacity. The last advantage has
induced the growth of the number of cores or Intellectual Properties (IPs) in a
same chip. Unfortunately, this important number of IPs has caused a new issue
which is the intra-communication between the elements of a same chip. To
resolve this problem, a new paradigm has been introduced which is the
Network-On-Chip (NoC). Since the introduction of the NoC paradigm in the last
decade, new methodologies and approaches have been presented by research
community and many of them have been adopted by industrials. The literature
contains many relevant studies and surveys discussing NoC proposals and
contributions. However, few of them have discussed or proposed a comparative
study of NoC tools. The objective of this work is to establish a reliable
survey about available design, simulation or implementation NoC tools. We
collected an important amount of information and characteristics about NoC
dedicated tools that we will present throughout this survey. This study is
built around a respectable amount of references and we hope it will help
scientists."
"With the high demand of low power digital systems, energy dissipation in the
digital system is one of the limiting factors. Reversible logic is one of the
alternate to reduce heat/energy dissipation in the digital circuits and have a
very significant importance in bioinformatics, optical information processing,
CMOS design etc. In this paper the authors propose the design of new 2- bit
binary Squaring circuit used in most of the digital signal processing hardware
using Feynman & MUX gate. The proposed squaring circuit having less garbage
outputs, constant inputs, Quantum cost and Total logical calculation i.e. less
delay as compared to the traditional method of squaring operation by reversible
multiplier. The simulating results and quantized results are also shown in the
paper which shows the greatest improvement in the design against the previous
methodology."
"Layout fracturing is a fundamental step in mask data preparation and e-beam
lithography (EBL) writing. To increase EBL throughput, recently a new L-shape
writing strategy is proposed, which calls for new L-shape fracturing, versus
the conventional rectangular fracturing. Meanwhile, during layout fracturing,
one must minimize very small/narrow features, also called slivers, due to
manufacturability concern. This paper addresses this new research problem of
how to perform L-shaped fracturing with sliver minimization. We propose two
novel algorithms. The first one, rectangular merging (RM), starts from a set of
rectangular fractures and merges them optimally to form L-shape fracturing. The
second algorithm, direct L-shape fracturing (DLF), directly and effectively
fractures the input layouts into L-shapes with sliver minimization. The
experimental results show that our algorithms are very effective."
"Triple patterning lithography (TPL) is one of the most promising techniques
in the 14nm logic node and beyond. However, traditional LELELE type TPL
technology suffers from native conflict and overlapping problems. Recently
LELEEC process was proposed to overcome the limitations, where the third mask
is used to generate the end-cuts. In this paper we propose the first study for
LELEEC layout decomposition. Conflict graphs and end-cut graphs are constructed
to extract all the geometrical relationships of input layout and end-cut
candidates. Based on these graphs, integer linear programming (ILP) is
formulated to minimize the conflict number and the stitch number."
"Electron beam lithography (EBL) is a promising maskless solution for the
technology beyond 14nm logic node. To overcome its throughput limitation,
recently the traditional EBL system is extended into MCC system. %to further
improve the throughput. In this paper, we present E-BLOW, a tool to solve the
overlapping aware stencil planning (OSP) problems in MCC system. E-BLOW is
integrated with several novel speedup techniques, i.e., successive relaxation,
dynamic programming and KD-Tree based clustering, to achieve a good performance
in terms of runtime and solution quality. Experimental results show that,
compared with previous works, E-BLOW demonstrates better performance for both
conventional EBL system and MCC system."
"Self-aligned double patterning (SADP) has become a promising technique to
push pattern resolution limit to sub-22nm technology node. Although SADP
provides good overlay controllability, it encounters many challenges in
physical design stages to obtain conflict-free layout decomposition. In this
paper, we study the impact on placement by different standard cell layout
decomposition strategies. We propose a SADP friendly standard cell
configuration which provides pre-coloring results for standard cells. These
configurations are brought into the placement stage to help ensure layout
decomposability and save the extra effort for solving conflicts in later
stages."
"As minimum feature size and pitch spacing further decrease, triple patterning
lithography (TPL) is a possible 193nm extension along the paradigm of double
patterning lithography (DPL). However, there is very little study on TPL layout
decomposition. In this paper, we show that TPL layout decomposition is a more
difficult problem than that for DPL. We then propose a general integer linear
programming formulation for TPL layout decomposition which can simultaneously
minimize conflict and stitch numbers. Since ILP has very poor scalability, we
propose three acceleration techniques without sacrificing solution quality:
independent component computation, layout graph simplification, and bridge
computation. For very dense layouts, even with these speedup techniques, ILP
formulation may still be too slow. Therefore, we propose a novel vector
programming formulation for TPL decomposition, and solve it through effective
semidefinite programming (SDP) approximation. Experimental results show that
the ILP with acceleration techniques can reduce 82% runtime compared to the
baseline ILP. Using SDP based algorithm, the runtime can be further reduced by
42% with some tradeoff in the stitch number (reduced by 7%) and the conflict
(9% more). However, for very dense layouts, SDP based algorithm can achieve
140x speed-up even compared with accelerated ILP."
"Low power design has become one of the most significant requirements when
CMOS technology entered the nanometer era. Therefore, timing budget is often
performed to slow down as many components as possible so that timing slacks can
be applied to reduce the power consumption while maintaining the performance of
the whole design. Retiming is a procedure that involves the relocation of
flip-flops (FFs) across logic gates to achieve faster clocking speed. In this
paper we show that the retiming and slack budgeting problem can be formulated
to a convex cost dual network flow problem. Both the theoretical analysis and
experimental results show the efficiency of our approach which can not only
reduce power consumption by 8.9%, but also speedup previous work by 500 times."
"Network-on-chip (NoC) architectures have been proposed as a promising
alternative to classical bus-based communication architectures. In this paper,
we propose a two phases framework to solve application-specific NoCs topology
generation problem. At floorplanning phase, we carry out partition driven
floorplanning. At post-floorplanning phase, a heuristic method and a min-cost
max-flow algorithm is used to insert switches and network interfaces. Finally,
we allocate paths to minimize power consumption. The experimental results show
our algorithm is effective for power saving."
"In today VLSI system design, power consumption is gaining more attention as
compared to performance and area. This is due to battery life in portable
devices and operating frequency of the design. Power consumption mainly
consists of static power, dynamic power, leakage power and short circuit power.
Dynamic power is dominant among all which depends on many factors viz. power
supply, load capacitance and frequency. Switching activity also affects dynamic
power consumption of bus which is determined by calculating the number of bit
transitions on bus. The purpose of this paper is to design a bit transition
counter which can be used to calculate the switching activity of the circuit
nodes. The novel feature is that it can be inserted at any node of the circuit,
thus helpful for calculating power consumption of bus."
"As the feature size of semiconductor process further scales to sub-16nm
technology node, triple patterning lithography (TPL) has been regarded one of
the most promising lithography candidates. M1 and contact layers, which are
usually deployed within standard cells, are most critical and complex parts for
modern digital designs. Traditional design flow that ignores TPL in early
stages may limit the potential to resolve all the TPL conflicts. In this paper,
we propose a coherent framework, including standard cell compliance and
detailed placement to enable TPL friendly design. Considering TPL constraints
during early design stages, such as standard cell compliance, improves the
layout decomposability. With the pre-coloring solutions of standard cells, we
present a TPL aware detailed placement, where the layout decomposition and
placement can be resolved simultaneously. Our experimental results show that,
with negligible impact on critical path delay, our framework can resolve the
conflicts much more easily, compared with the traditional physical design flow
and followed layout decomposition."
"Triple patterning lithography (TPL) has received more and more attentions
from industry as one of the leading candidate for 14nm/11nm nodes. In this
paper, we propose a high performance layout decomposer for TPL. Density
balancing is seamlessly integrated into all key steps in our TPL layout
decomposition, including density-balanced semi-definite programming (SDP),
density-based mapping, and density-balanced graph simplification. Our new TPL
decomposer can obtain high performance even compared to previous
state-of-the-art layout decomposers which are not balanced-density aware, e.g.,
by Yu et al. (ICCAD'11), Fang et al. (DAC'12), and Kuang et al. (DAC'13).
Furthermore, the balanced-density version of our decomposer can provide more
balanced density which leads to less edge placement error (EPE), while the
conflict and stitch numbers are still very comparable to our
non-balanced-density baseline."
"As technology scales, low power design has become a significant requirement
for SOC designers. Among the existing techniques, Multiple-Supply Voltage (MSV)
is a popular and effective method to reduce both dynamic and static power.
Besides, level shifters consume area and delay, and should be considered during
floorplanning. In this paper, we present a new floorplanning system, called
MVLSAF, to solve multi-voltage and level shifter assignment problem. We use a
convex cost network flow algorithm to assign arbitrary number of legal working
voltages and a minimum cost flow algorithm to handle level-shifter assignment.
The experimental results show MVLSAF is effective."
"In this paper, we examine the integration potential and explore the design
space of low power thermal reliable on-chip interconnect synthesis featuring
nanophotonics Wavelength Division Multiplexing (WDM). With the recent
advancements, it is foreseen that nanophotonics holds the promise to be
employed for future on-chip data signalling due to its unique power efficiency,
signal delay and huge multiplexing potential. However, there are major
challenges to address before feasible on-chip integration could be reached. In
this paper, we present GLOW, a hybrid global router to provide low power
opto-electronic interconnect synthesis under the considerations of thermal
reliability and various physical design constraints such as optical power,
delay and signal quality. GLOW is evaluated with testing cases derived from
ISPD07-08 global routing benchmarks. Compared with a greedy approach, GLOW
demonstrates around 23%-50% of total optical power reduction, revealing great
potential of on-chip WDM interconnect synthesis."
"In this paper we present EPIC, an efficient and effective predictor for IC
manufacturing hotspots in deep sub-wavelength lithography. EPIC proposes a
unified framework to combine different hotspot detection methods together, such
as machine learning and pattern matching, using mathematical
programming/optimization. EPIC algorithm has been tested on a number of
industry benchmarks under advanced manufacturing conditions. It demonstrates so
far the best capability in selectively combining the desirable features of
various hotspot detection methods (3.5-8.2% accuracy improvement) as well as
significant suppression of the detection noise (e.g., 80% false-alarm
reduction). These characteristics make EPIC very suitable for conducting high
performance physical verification and guiding efficient manufacturability
friendly physical design."
"TPL-friendly detailed routers require a systematic approach to detect TPL
conflicts. However, the complexity of conflict graph (CG) impedes directly
detecting TPL conflicts in CG. This work proposes a token graph-embedded
conflict graph (TECG) to facilitate the TPL conflict detection while
maintaining high coloring-flexibility. We then develop a TPL aware detailed
router (TRIAD) by applying TECG to a gridless router with the TPL stitch
generation. Compared to a greedy coloring approach, experimental results
indicate that TRIAD generates no conflicts and few stitches with shorter
wirelength at the cost of 2.41x of runtime."
"Low Power Design has become a significant requirement when the CMOS
technology entered the nanometer era. Multiple-Supply Voltage (MSV) is a
popular and effective method for both dynamic and static power reduction while
maintaining performance. Level shifters may cause area and Interconnect Length
Overhead (ILO), and should be considered at both floorplanning and
post-floorplanning stages. In this paper, we propose a two phases algorithm
framework, called VLSAF, to solve voltage and level shifter assignment problem.
At floorplanning phase, we use a convex cost network flow algorithm to assign
voltage and a minimum cost flow algorithm to handle level-shifter assignment.
At post-floorplanning phase, a heuristic method is adopted to redistribute
white spaces and calculate the positions and shapes of level shifters. The
experimental results show VLSAF is effective."
"With continued feature size scaling, even state of the art semiconductor
manufacturing processes will often run into layouts with poor printability and
yield. Identifying lithography hotspots is important at both physical
verification and early physical design stages. While detailed lithography
simulations can be very accurate, they may be too computationally expensive for
full-chip scale and physical design inner loops. Meanwhile, pattern matching
and machine learning based hotspot detection methods can provide acceptable
quality and yet fast turn-around-time for full-chip scale physical verification
and design. In this paper, we discuss some key issues and recent results on
lithography hotspot detection and mitigation in nanometer VLSI."
"This paper presents the design and implementation of three System on Chip
(SoC) cores, which implement the Digital Signal Processing (DSP) functions:
Finite Impulse Response (FIR) filter, Infinite Impulse Response (IIR) filter
and Fast Fourier Transform (FFT). The FIR filter core is based on the
symmetrical realization form, the IIR filter core is based on the Second Order
Sections (SOS) architecture and the FFT core is based on the Radix $2^2$ Single
Delay Feedback (R$2^2$SDF) architecture. The three cores are compatible with
the Wishbone SoC bus and they were described using generic and structural VHDL.
In system hardware verification was performed by using an OpenRisc-based SoC
synthesized on an Altera FPGA, the tests showed that the designed DSP cores are
suitable for building SoC based on the OpenRisc processor and the Wishbone bus."
"Hazard radiation can lead the system fault therefore Fault Tolerance is
required. Fault Tolerant is a system, which is designed to keep operations
running, despite the degradation in the specific module is happening. Many
fault tolerances have been developed to handle the problem, to find the most
robust and efficient in the possible technology. This paper will present the
Five Modular Redundancy (FMR) with Mitigation Technique to Recover the Error
Module. With Dynamic Partial Reconfiguration technology that have already
available today, such fault tolerance technique can be implemented
successfully. The project showed the robustness of the system is increased and
module which is error can be recovered immediately."
"Reversible computing is gaining high interest from researchers due to its
various promises. One of the prominent advantages perceived from reversible
logic is that of reduced power dissipation with many reversible gates at hand,
designing a reversible circuit (combinational) has received due attention and
achievement. A proposed language for description of reversible circuit, namely
SyReC, is also in place. What remain are the software tools which would help in
reversible circuit synthesis through simulation. Beginning with the smallest
reversible circuit realizations the SyReC statements and expressions, we employ
a hierarchal approach to develop a complete reversible circuit, entirely from
its SyReC code. We implement this as a software tool. The tool allows a user to
expand a reversible circuit of choice in terms of bit width of its inputs. The
background approach of expansion of a reversible circuit has also been proposed
as a part of this dissertation. Also, a user can use the tool to observe the
effect of expansion on incurred costs, in terms of increase in number of lines,
number of gates and quantum cost. The importance of observing the change in
costs with respect to scale of expansion is important not only from analysis
point of view, but also because the cost depends on the approach used for
expansion. This dissertation also proposes a reversible circuit design for
elevator controller (combinational) and the related costs. The aim is to
emphasize use of the proposed approach is designing customized circuits."
"This paper presents a novel statistical state-dependent timing model for
voltage over scaled (VoS) logic circuits that accurately and rapidly finds the
timing distribution of output bits. Using this model erroneous VoS circuits can
be represented as error-free circuits combined with an error-injector. A case
study of a two point DFT unit employing the proposed model is presented and
compared to HSPICE circuit simulation. Results show an accurate match, with
significant speedup gains."
"FIR filters are used in many performance/power critical applications such as
mobile communication devices, analogue to digital converters and digital signal
processing applications. Design of appropriate FIR filters usually causes the
order of filter to be increased. Synthesis and tape-out of high-order FIR
filters with reasonable delay, area and power has become an important challenge
for hardware designers. In many cases the complexity of high-order filters
causes the constraints of the total design could not be satisfied. In this
paper, efficient hardware architecture is proposed for distributed arithmetic
(DA) based FIR filters. The architecture is based on optimized combination of
Look-up Tables (LUTs) and compressors. The optimized system level solution is
obtained from a set of dynamic programming optimization algorithms. The
experiments show the proposed design educed the delay cost between 16%-62.5% in
comparison of previous optimized structures for DA-based architectures."
"In this paper, the ByoRISC (Build your own RISC) configurable
application-specific instruction-set processor (ASIP) family is presented.
ByoRISCs, as vendor-independent cores, provide extensive architectural
parameters over a baseline processor, which can be customized by
application-specific hardware extensions (ASHEs). Such extensions realize
multi-input multi-output (MIMO) custom instructions with local state and
load/store accesses to the data memory. ByoRISCs incorporate a true multi-port
register file, zero-overhead custom instruction decoding, and scalable data
forwarding mechanisms. Given these design decisions, ByoRISCs provide a unique
combination of features that allow their use as architectural testbeds and the
seamless and rapid development of new high-performance ASIPs.
  The performance characteristics of ByoRISCs, implemented as
vendor-independent cores, have been evaluated for both ASIC and FPGA
implementations, and it is proved that they provide a viable solution in
FPGA-based system-on-a-chip design. A case study of an image processing
pipeline is also presented to highlight the process of utilizing a ByoRISC
custom processor. A peak performance speedup of up to 8.5$\times$ can be
observed, whereas an average performance speedup of 4.4$\times$ on Xilinx
Virtex-4 targets is achieved. In addition, ByoRISC outperforms an experimental
VLIW architecture named VEX even in its 16-wide configuration for a number of
data-intensive application kernels."
"Efficiency in embedded systems is paramount to achieve high performance while
consuming less area and power. Processors in embedded systems have to be
designed carefully to achieve such design constraints. Application Specific
Instruction set Processors (ASIPs) exploit the nature of applications to design
an optimal instruction set. Despite being not general to execute any
application, ASIPs are highly preferred in the embedded systems industry where
the devices are produced to satisfy a certain type of application domain/s
(either intra-domain or inter-domain). Typically, ASIPs are designed from a
base-processor and functionalities are added for applications. This paper
studies the multi-application ASIPs and their instruction sets, extensively
analysing the instructions for inter-domain and intra-domain designs. Metrics
analysed are the reusable instructions and the extra cost to add a certain
application. A wide range of applications from various application benchmarks
(MiBench, MediaBench and SPEC2006) and domains are analysed for two different
architectures (ARM-Thumb and PISA). Our study shows that the intra-domain
applications contain larger number of common instructions, whereas the
inter-domain applications have very less common instructions, regardless of the
architecture (and therefore the ISA)."
"Processing data received as a stream is a task commonly performed by modern
embedded devices, in a wide range of applications such as multimedia
(encoding/decoding/ playing media), networking (switching and routing), digital
security, scientific data processing, etc. Such processing normally tends to be
calculation intensive and therefore requiring significant processing power.
Therefore, hardware acceleration methods to increase the performance of such
applications constitute an important area of study. In this paper, we present
an evaluation of one such method to process streaming data, namely
multi-processor pipeline architecture. The hardware is based on a
Multiple-Processor System on Chip (MPSoC), using a data encryption algorithm as
a case study. The algorithm is partitioned on a coarse grained level and mapped
on to an MPSoC with five processor cores in a pipeline, using specifically
configured Xtensa LX3 cores. The system is then selectively optimized by
strengthening and pruning the resources of each processor core. The optimized
system is evaluated and compared against an optimal single-processor System on
Chip (SoC) for the same application. The multiple-processor pipeline system for
data encryption algorithms used was observed to provide significant speed ups,
up to 4.45 times that of the single-processor system, which is close to the
ideal speed up from a five-stage pipeline."
"Modern platform-based design involves the application-specific extension of
embedded processors to fit customer requirements. To accomplish this task, the
possibilities offered by recent custom/extensible processors for tuning their
instruction set and microarchitecture to the applications of interest have to
be exploited. A significant factor often determining the success of this
process is the utomation available in application analysis and custom
instruction generation.
  In this paper we present YARDstick, a design automation tool for custom
processor development flows that focuses on generating and evaluating
application-specific hardware extensions. YARDstick is a building block for
ASIP development, integrating application analysis, custom instruction
generation and selection with user-defined compiler intermediate
representations. In a YARDstick-enabled environment, practical issues in
traditional ASIP design are confronted efficiently; the exploration
infrastructure is liberated from compiler and simulator idiosyncrasies, since
the ASIP designer is empowered with the freedom of specifying the target
architectures of choice and adding new implementations of analyses and custom
instruction generation/selection methods. To illustrate the capabilities of the
YARDstick approach, we present interesting exploration scenarios: quantifying
the effect of machine-dependent compiler optimizations and the selection of the
target architecture in terms of operation set and memory model on custom
instruction generation/selection under different input/output constraints."
"In this contribution the emulation of an ASIC temperature and power
monitoring system (TPMon) for FPGA prototyping is presented and tested to
control processor temperatures under different control targets and operating
strategies. The approach for emulating the power monitor is based on an
instruction-level energy model. For emulating the temperature monitor, a
thermal RC model is used. The monitoring system supplies an invasive MPSoC
computing architecture with hardware status information (power and temperature
data of the processors within the system). These data are required for
resource-aware load distribution. As a proof of concept different operating
strategies and control targets were evaluated for a 2-tile invasive MPSoC
computing system."
"Standard memory modules to store (and access) data are designed for use with
a single system accessing it. More complicated memory modules would be accessed
through a memory controller, which are also designed for one system. For
multiple systems to access a single memory module there must be some
facilitation that allows them to access the memory without overriding or
corrupting the access from the others. This was done with the use of a memory
arbiter, which controls the flow of traffic into the memory controller. The
arbiter has a set of rules to abide to in order to choose which system gets
through to the memory controller. In this project, a regular RAM module is
designed for use with one system. Furthermore, a memory arbiter is also
designed in Verilog that allows for more than one system to use a single RAM
module in a controlled and synchronized manner. The arbiter uses a fixed
priority scheme to avoid starvation of the system. In addition one of the major
problems associated with such systems i.e. The Address Clash Problem has been
nicely tackled and solved. The design is verified in simulation and validated
on a Xilinx ML605 evaluation board with a Virtex 6 FPGA."
"This paper provides the first comprehensive description of the Z1, the
mechanical computer built by the German inventor Konrad Zuse in Berlin from
1936 to 1938. The paper describes the main structural elements of the machine,
the high-level architecture, and the dataflow between components. The computer
could perform the four basic arithmetic operations using floating-point
numbers. Instructions were read from punched tape. A program consisted of a
sequence of arithmetical operations, intermixed with memory store and load
instructions, interrupted possibly by input and output operations. Numbers were
stored in a mechanical memory. The machine did not include conditional
branching in the instruction set. While the architecture of the Z1 is similar
to the relay computer Zuse finished in 1941 (the Z3) there are some significant
differences. The Z1 implements operations as sequences of microinstructions, as
in the Z3, but does not use rotary switches as micro-steppers. The Z1 uses a
digital incrementer and a set of conditions which are translated into
microinstructions for the exponent and mantissa units, as well as for the
memory blocks. Microinstructions select one out of 12 layers in a machine with
a 3D mechanical structure of binary mechanical elements. The exception circuits
for mantissa zero, necessary for normalized floating-point, were lacking; they
were first implemented in the Z3. The information for this article was
extracted from careful study of the blueprints drawn by Zuse for the
reconstruction of the Z1 for the German Technology Museum in Berlin, from some
letters, and from sketches in notebooks. Although the machine has been in
exhibition since 1989 (non-operational), no detailed high-level description of
the machine's architecture had been available. This paper fills that gap."
"Custom memory organization are challenging task in the area of VLSI design.
This study aims to design high speed and low power consumption memory for
embedded system. Synchronous SRAM has been proposed and analyzed using various
simulators. Xilinx simulator simulates the Synchronous SRAM memories which can
perform efficient read/write capability for embedded systems. Xinix tool also
provide the access time that required selecting a word and reading it.
Synchronous Static RAM which has easily read /writes capability and performs
scheduled read /writes operation in efficient manner."
"An Efficient Simulation of application specific instruction-set processors
(ASIP) is a challenging onus in the area of VLSI design. This paper
reconnoiters the possibility of use of ASIP simulators for ASIP Simulation.
This proposed study allow as the simulation of the cache memory design with
various ASIP simulators like Simple scalar and VEX. In this paper we have
implemented the memory configuration according to desire application. These
simulators performs the cache related results such as cache name, sets, cache
associativity, cache block size, cache replacement policy according to specific
application."
"A Content Addressable Memory (CAM) is a memory primarily designed for high
speed search operation. Parallel search scheme forms the basis of CAM, thus
power reduction is the challenge associated with a large amount of parallel
active circuits. We are presenting a novel algorithm and architecture described
as Selective Match-Line Energizer Content Addressable Memory (SMLE-CAM) which
energizes only those MLs (Match-Line) whose first three bits are conditionally
matched with corresponding first three search bit using special architecture
which comprises of novel XNOR-CAM cell and novel XOR-CAM cell. The rest of the
CAM chain is followed by NOR-CAM cell. The 256 X 144 bit SMLE-CAM is
implemented in TSMC 90 nm technology and its robustness across PVT variation is
verified. The post-layout simulation result shows, it has energy metric of
0.115 fJ/bit/search with search time 361.6 ps, the best reported so far. The
maximum operating frequency is 1GHz."
"Application specific simulation is challenging task in various real time high
performance embedded devices. In this study specific application is implemented
with the help of Xilinx. Xilinx provides SDK and XPS tools, XPS tools used for
develop complete hardware platform and SDK provides software platform for
application creation and verification. Xilinx XUP-5 board have been used and
implemented various specific Applications with hardware platform. In this study
the base instruction set with customized instructions, supported with specific
hardware resources are analyzed."
"The Digital Image processing applications like medical imaging, satellite
imaging, Biometric trait images etc., rely on multipliers to improve the
quality of image. However, existing multiplication techniques introduce errors
in the output with consumption of more time, hence error free high speed
multipliers has to be designed. In this paper we propose FPGA based Recursive
Error Free Mitchell Log Multiplier (REFMLM) for image Filters. The 2x2 error
free Mitchell log multiplier is designed with zero error by introducing error
correction term is used in higher order Karastuba-Ofman Multiplier (KOM)
Architectures. The higher order KOM multipliers is decomposed into number of
lower order multipliers using radix 2 till basic multiplier block of order 2x2
which is designed by error free Mitchell log multiplier. The 8x8 REFMLM is
tested for Gaussian filter to remove noise in fingerprint image. The Multiplier
is synthesized using Spartan 3 FPGA family device XC3S1500-5fg320. It is
observed that the performance parameters such as area utilization, speed, error
and PSNR are better in the case of proposed architecture compared to existing
architectures"
"This paper presents a low power ECG recording Sys-tem-on-Chip (SoC) with
on-chip low complexity lossless ECG compression for data reduction in
wireless/ambulatory ECG sensor devices. The proposed algorithm uses a linear
slope predictor to estimate the ECG samples, and uses a novel low complexity
dynamic coding-packaging scheme to frame the resulting estimation error into
fixed-length 16-bit format. The proposed technique achieves an average
compression ratio of 2.25x on MIT/BIH ECG database. Implemented in 0.35 {\mu}m
process, the compressor uses 0.565 K gates/channel occupying 0.4 mm2 for
4-channel, and consumes 535 nW/channel at 2.4V for ECG sampled at 512 Hz. Small
size and ultra-low power consumption makes the proposed technique suitable for
wearable ECG sensor application."
"The current manufacturing technology allows the integration of a complex
multiprocessor system on one piece of silicon (MPSoC for Multiprocessor
System-on- Chip). One way to manage the growing complexity of these systems is
to increase the level of abstraction and to address the system-level design. In
this paper, we focus on the implementation in SystemC language with TLM
(Transaction Level Model) to model an MPSOC platform. Our main contribution is
to define a comprehensive, fast and accurate method for designing and
evaluating performance for MPSoC systems. The studied MPSoC is composed of
MicroBlaze microprocessors, memory, a timer, a VGA and an interrupt handler
with two examples of software. This paper has two novel contributions: the
first is to develop this MPSOC at CABA and TLM for ISS (Instruction Set
Simulator), Native simulations and timed Programmer s View (PV+T); the second
is to show that with PV+T simulations we can achieve timing fidelity with
higher speeds than CABA simulations and have almost the same precision."
"In this work, we propose a configurable many-core overlay for
high-performance embedded computing. The size of internal memory, supported
operations and number of ports can be configured independently for each core of
the overlay. The overlay was evaluated with matrix multiplication, LU
decomposition and Fast-Fourier Transform (FFT) on a ZYNQ-7020 FPGA platform.
The results show that using a system-level many-core overlay avoids complex
hardware design and still provides good performance results."
"In this paper a pipelined architecture of a high speed network security
processor (NSP) for SSL,TLS protocol is implemented on a system on chip (SOC)
where hardware information of all encryption, hashing and key exchange
algorithms are stored in flash memory in terms of bit files, in contrary to
related works where all are actually implemented in hardware. The NSP finds
applications in e-commerce, virtual private network (VPN) and in other fields
that require data confidentiality. The motivation of the present work is to
dynamically execute applications with stipulated throughput within budgeted
hardware resource and power. A preferential algorithm choosing an appropriate
cipher suite is proposed, which is based on Efficient System Index (ESI) budget
comprising of power, throughput and resource given by the user. The bit files
of the chosen security algorithms are downloaded from the flash memory to the
partial region of field programmable gate array (FPGA). The proposed SOC
controls data communication between an application running in a system through
a PCI and the Ethernet interface of a network. Partial configuration feature is
used in ISE14.4 suite with ZYNQ 7z020-clg484 FPGA platform. The performances"
"In the context of mapping high-level algorithms to hardware, we consider the
basic problem of generating an efficient hardware implementation of a single
threaded program, in particular, that of an inner loop. We describe a
control-flow mechanism which provides dynamic loop-pipelining capability in
hardware, so that multiple iterations of an arbitrary inner loop can be made
simultaneously active in the generated hardware, We study the impact of this
loop-pipelining scheme in conjunction with source-level loop-unrolling. In
particular, we apply this technique to some common loop kernels: regular
kernels such as the fast-fourier transform and matrix multiplication, as well
as an example of an inner loop whose body has branching. The resulting
resulting hardware descriptions are synthesized to an FPGA target, and then
characterized for performance and resource utilization. We observe that the use
of dynamic loop-pipelining mechanism alone typically results in a significant
improvements in the performance of the hardware. If the loop is statically
unrolled and if loop-pipelining is applied to the unrolled program, then the
performance improvement is still substantial. When dynamic loop pipelining is
used in conjunction with static loop unrolling, the improvement in performance
ranges from 6X to 20X (in terms of number of clock cycles needed for the
computation) across the loop kernels that we have studied. These optimizations
do have a hardware overhead, but, in spite of this, we observe that the joint
use of these loop optimizations not only improves performance, but also the
performance/cost ratio of the resulting hardware."
"Full Adder is one of the critical parts of logical and arithmetic units. So,
presenting a low power full adder cell reduces the power consumption of the
entire circuit. Also, using Nano-scale transistors, because of their unique
characteristics will save energy consumption and decrease the chip area. In
this paper we presented a low power full adder cell by using carbon nanotube
field effect transistors (CNTFETs). Simulation results were carried out using
HSPICE based on the CNTFET model in 32 nanometer technology in Different values
of temperature and VDD."
"This article presents novel high speed and low power full adder cells based
on carbon nanotube field effect transistor (CNFET). Four full adder cells are
proposed in this article. First one (named CN9P4G) and second one (CN9P8GBUFF)
utilizes 13 and 17 CNFETs respectively. Third design that we named CN10PFS uses
only 10 transistors and is full swing. Finally, CN8P10G uses 18 transistors and
divided into two modules, causing Sum and Cout signals are produced in a
parallel manner. All inputs have been used straight, without inverting. These
designs also used the special feature of CNFET that is controlling the
threshold voltage by adjusting the diameters of CNFETs to achieve the best
performance and right voltage levels. All simulation performed using Synopsys
HSPICE software and the proposed designs are compared to other classical and
modern CMOS and CNFET-based full adder cells in terms of delay, power
consumption and power delay product."
"We consider the problem of constructing fast and small parallel prefix adders
for non-uniform input arrival times. This problem arises whenever the adder is
embedded into a more complex circuit, e. g. a multiplier.
  Most previous results are based on representing binary carry-propagate adders
as so-called parallel prefix graphs, in which pairs of generate and propagate
signals are combined using complex gates known as prefix gates. Adders
constructed in this model usually minimize the delay in terms of these prefix
gates. However, the delay in terms of logic gates can be worse by a factor of
two.
  In contrast, we aim to minimize the delay of the underlying logic circuit
directly. We prove a lower bound on the delay of a carry bit computation
achievable by any prefix carry bit circuit and develop an algorithm that
computes a prefix carry bit circuit with optimum delay up to a small additive
constant. Furthermore, we use this algorithm to construct a small parallel
prefix adder.
  Compared to existing algorithms we simultaneously improve the delay and size
guarantee, as well as the running time for constructing prefix carry bit and
adder circuits."
"We present the design and evaluation of a predictable Network-on-Chip (NoC)
to interconnect processing units running multimedia applications with
variable-bit-rate. The design is based on a connectionless strategy in which
flits from different communication flows are interleaved in the same
communication channel between routers. Each flit carries routing information
used by routers to perform arbitration and scheduling of the corresponding
output communication channel. Analytic comparisons show that our approach keeps
average latency lower than a network based on resource reservation, when both
networks are working over 80% of offered load. We also evaluate the proposed
NoC on FPGA and ASIC technologies to understand the trade-off due to our
approach, in terms of silicon consumption."
"Imaging and Image sensors is a field that is continuously evolving. There are
new products coming into the market every day. Some of these have very severe
Size, Weight and Power constraints whereas other devices have to handle very
high computational loads. Some require both these conditions to be met
simultaneously. Current imaging architectures and digital image processing
solutions will not be able to meet these ever increasing demands. There is a
need to develop novel imaging architectures and image processing solutions to
address these requirements. In this work we propose analog signal processing as
a solution to this problem. The analog processor is not suggested as a
replacement to a digital processor but it will be used as an augmentation
device which works in parallel with the digital processor, making the system
faster and more efficient. In order to show the merits of analog processing the
highly computational Normalized Cross Correlation algorithm is implemented. We
propose two novel modifications to the algorithm and a new imaging architecture
which, significantly reduces the computation time."
"It has been pointed out by counterexamples in a 2013 paper in the IEEE
Transactions on Computers [1], that there is an error in the previously ibid.\
in 2005 published paper [2] on the construction of valid digit selection tables
for SRT type division and square root algorithms. The error has been corrected,
and new results found on selection constants for maximally redundant digit
sets."
"The Sphynx project was an exploratory study to discover what might be done to
improve the heavy replication of in- structions in independent instruction
caches for a massively parallel machine where a single program is executing
across all of the cores. While a machine with only many cores (fewer than 50)
might not have any issues replicating the instructions for each core, as we
approach the era where thousands of cores can be placed on one chip, the
overhead of instruction replication may become unacceptably large. We believe
that a large amount of sharing should be possible when the ma- chine is
configured for all of the threads to issue from the same set of instructions.
We propose a technique that allows sharing an instruction cache among a number
of independent processor cores to allow for inter-thread sharing and reuse of
instruction memory. While we do not have test cases to demonstrate the
potential magnitude of performance gains that could be achieved, the potential
for sharing reduces the die area required for instruction storage on chip."
"This study proposes a new router architecture to improve the performance of
dynamic allocation of virtual channels. The proposed router is designed to
reduce the hardware complexity and to improve power and area consumption,
simultaneously. In the new structure of the proposed router, all of the
controlling components have been implemented sequentially inside the allocator
router modules. This optimizes communications between the controlling
components and eliminates the most of hardware overloads of modular
communications. Eliminating additional communications also reduces the hardware
complexity. In order to show the validity of the proposed design in real
hardware resources, the proposed router has been implemented onto a
Field-Programmable Gate Array (FPGA). Since the implementation of a
Network-on-Chip (NoC) requires certain amount of area on the chip, the
suggested approach is also able to reduce the demand of hardware resources. In
this method, the internal memory of the FPGA is used for implementing control
units. This memory is faster and can be used with specific patterns. The use of
the FPGA memory saves the hardware resources and allows the implementation of
NoC based FPGA."
"Speculative multi-threading (SpMT) has been proposed as a perspective method
to exploit Chip Multiprocessors (CMP) hardware potential. It is a thread level
speculation (TLS) model mainly depending on software and hardware co-design.
This paper researches speculative thread-level parallelism of general purpose
programs and a speculative multi-threading execution model called Prophet is
presented. The architectural support for Prophet execution model is designed
based on CMP. In Prophet the inter-thread data dependency are predicted by
pre-computation slice (p-slice) to reduce RAW violation. Prophet
multi-versioning Cache system along with thread state control mechanism in
architectural support are utilized for buffering the speculative data, and a
snooping bus based cache coherence protocol is used to detect data dependence
violation. The simulation-based evaluation shows that the Prophet system could
achieve significant speedup for general-purpose programs."
"This paper proposes a high-throughput energy-efficient Successive
Cancellation (SC) decoder architecture for polar codes based on combinational
logic. The proposed combinational architecture operates at relatively low clock
frequencies compared to sequential circuits, but takes advantage of the high
degree of parallelism inherent in such architectures to provide a favorable
tradeoff between throughput and energy efficiency at short to medium block
lengths. At longer block lengths, the paper proposes a hybrid-logic SC decoder
that combines the advantageous aspects of the combinational decoder with the
low-complexity nature of sequential-logic decoders. Performance characteristics
on ASIC and FPGA are presented with a detailed power consumption analysis for
combinational decoders. Finally, the paper presents an analysis of the
complexity and delay of combinational decoders, and of the throughput gains
obtained by hybrid-logic decoders with respect to purely synchronous
architectures."
"In this letter we present a new architecture for a polar decoder using a
reduced complexity successive cancellation decoding algorithm. This novel
fully-unrolled, deeply-pipelined architecture is capable of achieving a coded
throughput of over 237 Gbps for a (1024,512) polar code implemented using an
FPGA. This decoder is two orders of magnitude faster than state-of-the-art
polar decoders."
"ASIPs are designed in order to execute instructions of a particular domain of
applications. The designing of ASIPs addresses the major challenges faced by a
system on chip such as size, cost, performance and energy consumption. The
higher the number of similar instructions within the domain to be mapped the
lesser the energy consumption, the smaller the size and the higher the
performance of the ASIP. Thus, designing processors for domains with more
similar programs would overcome these issues. This paper describes the
investigation of whether the domains of programmer specific programs have any
significance like application specific program domains and thus, whether the
approach of designing processors known as Programmer Specific Instruction Set
Processors is worthwhile. We performed the evaluation at the instruction level
by using four different measures to obtain the similarity of programs: (1) by
the existence of each instruction, (2) by the frequency of each instruction,
(3) by two consecutive instruction patterns and (4) by three consecutive
instruction patterns of application specific and programmer specific programs.
We found that although programmer specific instructions show some impact on the
similarity measures, they are much smaller and therefore insignificant compared
to the impact from application specific programs."
"With an increasing number of power-states, finer- grained power management
and larger dynamic ranges of digital circuits, the integration of compact,
scalable linear-regulators embedded deep within logic blocks has become
important. While analog linear-regulators have traditionally been used in
digital ICs, the need for digitally implementable designs that can be
synthesized and embedded in digital functional units for ultra fine- grained
power management has emerged. This paper presents the circuit design and
control models of an all-digital, discrete-time linear regulator and explores
the parametric design space for transient response time and loop stability."
"In order to meet the requirement of high data rates for the next generation
wireless systems, the efficient implementation of receiver algorithms is
essential. On the other hand, the rapid development of technology motivates the
investigation of programmable implementations. This paper summarizes the design
of a programmable turbo decoder as an applicationspecific instruction-set
processor (ASIP) using Transport Triggered Architecture (TTA). The processor
architecture is designed in such manner that it can be programmed to support
other receiver algorithms, for example, decoding based on the Viterbi
algorithm. Different suboptimal maximum a posteriori (MAP) algorithms are used
and compared to one another for the softinput soft-output (SISO) component
decoders in a single TTA processor. The max-log-MAP algorithm outperforms the
other suboptimal algorithms in terms of latency. The design enables the
designer to change the suboptimal algorithms according to the bit error rate
(BER) performance requirement. Unlike many other programmable turbo decoder
implementations, quadratic polynomial permutation (QPP) interleaver is used in
this work for contention-free memory access and to make the processor 3GPP LTE
compliant. Several optimization techniques to enable real time processing on
programmable platforms are introduced. Using our method, with a single
iteration 31.32 Mbps throughput is achieved for the max-log-MAP algorithm for a
clock frequency of 200 MHz."
"In this report we show results that validate the Tejas architectural
simulator against native hardware. We report mean error rates of 11.45% and
18.77% for the SPEC2006 and Splash2 benchmark suites respectively. These error
rates are competitive and in most cases better than the numbers reported by
other contemporary simulators."
"This paper shows the usage of C-Slow Retiming (CSR) in safety critical and
low power applications. CSR generates C copies of a design by reusing the given
logic resources in a time sliced fashion. When all C design copies are
stimulated with the same input values, then all C design copies should behave
the same way and will therefore create a redundant system. The paper shows that
this special method of using CSR offers great benefits when used in safety
critical and low power applications. Additional optimization techniques towards
reducing register count are shown and an on-the-fly recovery mechanism is
discussed."
"We propose a novel solid-state disk (SSD) architecture that utilizes a
double-data-rate synchronous NAND flash interface for improving read and write
performance. Unlike the conventional design, the data transfer rate in the
proposed design is doubled in harmony with synchronous signaling. The new
architecture does not require any extra pins with respect to the conventional
architecture, thereby guaranteeing backward compatibility. For performance
evaluation, we simulated various SSD designs that adopt the proposed
architecture and measured their performance in terms of read/write bandwidths
and energy consumption. Both NAND flash cell types, namely single-level cells
(SLCs) and multi-level cells (MLCs), were considered. In the experiments using
SLC-type NAND flash chips, the read and write speeds of the proposed
architecture were 1.65-2.76 times and 1.09-2.45 times faster than those of the
conventional architecture, respectively. Similar improvements were observed for
the MLC-based architectures tested. It was particularly effective to combine
the proposed architecture with the way-interleaving technique that multiplexes
the data channel between the controller and each flash chip. For a reasonably
high degree of way interleaving, the read/write performance and the energy
consumption of our approach were notably better than those of the conventional
design."
"Fast Fourier transform (FFT) of large number of samples requires huge
hardware resources of field programmable gate arrays (FPGA), which needs more
area and power. In this paper, we present an area efficient architecture of FFT
processor that reuses the butterfly elements several times. The FFT processor
is simulated using VHDL and the results are validated on a Virtex-6 FPGA. The
proposed architecture outperforms the conventional architecture of a $N$-point
FFT processor in terms of area which is reduced by a factor of $log_N 2$ with
negligible increase in processing time."
"Every CPU carries one or more arithmetical and logical units. One popular
operation that is performed by these units is multiplication. Automatic
generation of custom VHDL models for performing this operation, allows the
designer to achieve a time efficient design space exploration. Although these
units are heavily utilized in modern digital circuits and DSP, there is no
tool, accessible from the web, to generate the HDL description of such designs
for arbitrary and different input bitwidths. In this paper, we present our web
accessible tool to construct completely custom optimized multiplication units
together with random generated test vectors for their verification. Our novel
tool is one of the firsts web based EDA tools to automate the design of such
units and simultaneously provide custom testbenches to verify their
correctness. Our synthesized circuits on Xilinx Virtex 6 FPGA, operate up to
589 Mhz."
"In this paper, a general circuit scheme for noise-tolerant logic design based
on Markov Random Field theory and differential Cascade Voltage Switch technique
has been proposed, which is an extension of the work in [1-3], [4]. A block
with only four transistors has been successfully inserted to the original
circuit scheme from [3] and extensive simulation results show that our proposed
design can operate correctly with the input signal of 1 dB signal-noise-ratio.
When using the evaluation parameter from [5], the output value of our design
decreases by 76.5% on average than [3] which means that superior noise-immunity
could be obtained through our work."
"The paper describes the design of high performance MIPS Cryptography
processor based on triple data encryption standard. The organization of
pipeline stages in such a way that pipeline can be clocked at high frequency.
Encryption and Decryption blocks of triple data encryption standard (T-DES)
crypto system and dependency among themselves are explained in detail with the
help of block diagram. In order to increase the processor functionality and
performance, especially for security applications we include three new 32-bit
instructions LKLW, LKUW and CRYPT. The design has been synthesized at 40nm
process technology targeting using Xilinx Virtex-6 device. The overall MIPS
Crypto processor works at 209MHz."
"Cloud computing relies on secure and efficient virtualization. Software level
security solutions compromise the performance of virtual machines (VMs), as a
large amount of computational power would be utilized for running the security
modules. Moreover, software solutions are only as secure as the level that they
work on. For example a security module on a hypervisor cannot provide security
in the presence of an infected hypervisor. It is a challenge for virtualization
technology architects to enhance the security of VMs without degrading their
performance. Currently available server machines are not fully equipped to
support a secure VM environment without compromising on performance. A few
hardware modifications have been introduced by manufactures like Intel and AMD
to provide a secure VM environment with low performance degradation. In this
paper we propose a novel memory architecture model named \textit{ Architectural
Support for Memory Isolation(ASMI)}, that can achieve a true isolated physical
memory region to each VM without degrading performance. Along with true memory
isolation, ASMI is designed to provide lower memory access times, better
utilization of available memory, support for DMA isolation and support for
platform independence for users of VMs."
"Many believe that in-field hardware faults are too rare in practice to
justify the need for Logic Built-In Self-Test (LBIST) in a design. Until now,
LBIST was primarily used in safety-critical applications. However, this may
change soon. First, even if costly methods like burn-in are applied, it is no
longer possible to get rid of all latent defects in devices at leading-edge
technology. Second, demands for high reliability spread to consumer electronics
as smartphones replace our wallets and IDs. However, today many ASIC vendors
are reluctant to use LBIST. In this paper, we describe the needs for successful
deployment of LBIST in the industrial practice and discuss how these needs can
be addressed. Our work is hoped to attract a wider attention to this important
research topic."
"Graphics Processing Units (GPUs) consisting of Streaming Multiprocessors
(SMs) achieve high throughput by running a large number of threads and context
switching among them to hide execution latencies. The number of thread blocks,
and hence the number of threads that can be launched on an SM, depends on the
resource usage--e.g. number of registers, amount of shared memory--of the
thread blocks. Since the allocation of threads to an SM is at the thread block
granularity, some of the resources may not be used up completely and hence will
be wasted.
  We propose an approach that shares the resources of SM to utilize the wasted
resources by launching more thread blocks. We show the effectiveness of our
approach for two resources: register sharing, and scratchpad (shared memory)
sharing. We further propose optimizations to hide long execution latencies,
thus reducing the number of stall cycles. We implemented our approach in
GPGPU-Sim simulator and experimentally validated it on several applications
from 4 different benchmark suites: GPGPU-Sim, Rodinia, CUDA-SDK, and Parboil.
We observed that with register sharing, applications show maximum improvement
of 24%, and average improvement of 11%. With scratchpad sharing, we observed a
maximum improvement of 30% and an average improvement of 12.5%."
"For polar codes with short-to-medium code length, list successive
cancellation decoding is used to achieve a good error-correcting performance.
However, list pruning in the current list decoding is based on the sorting
strategy and its timing complexity is high. This results in a long decoding
latency for large list size. In this work, aiming at a low-latency list
decoding implementation, a double thresholding algorithm is proposed for a fast
list pruning. As a result, with a negligible performance degradation, the list
pruning delay is greatly reduced. Based on the double thresholding, a
low-latency list decoding architecture is proposed and implemented using a UMC
90nm CMOS technology. Synthesis results show that, even for a large list size
of 16, the proposed low-latency architecture achieves a decoding throughput of
220 Mbps at a frequency of 641 MHz."
"With the imminent slowing down of DRAM scaling, Phase Change Memory (PCM) is
emerging as a lead alternative for main memory technology. While PCM achieves
low energy due to various technology-specific advantages, PCM is significantly
slower than DRAM (especially for writes) and can endure far fewer writes before
wearing out. Previous work has proposed to use a large, DRAM-based hardware
cache to absorb writes and provide faster access. However, due to ineffectual
caching where blocks are evicted before sufficient number of accesses, hardware
caches incur significant overheads in energy and bandwidth, two key but scarce
resources in modern multicores. Because using hardware for detecting and
removing such ineffectual caching would incur additional hardware cost and
complexity, we leverage the OS virtual memory support for this purpose. We
propose a DRAM-PCM hybrid memory architecture where the OS migrates pages on
demand from the PCM to DRAM. We call the DRAM part of our memory as
MigrantStore which includes two ideas. First, to reduce the energy, bandwidth,
and wear overhead of ineffectual migrations, we propose migration hysteresis.
Second, to reduce the software overhead of good replacement policies, we
propose recently- accessed-page-id (RAPid) buffer, a hardware buffer to track
the addresses of recently-accessed MigrantStore pages."
"High Performance Computing (HPC) platforms allow scientists to model
computationally intensive algorithms. HPC clusters increasingly use
General-Purpose Graphics Processing Units (GPGPUs) as accelerators; FPGAs
provide an attractive alternative to GPGPUs for use as co-processors, but they
are still far from being mainstream due to a number of challenges faced when
using FPGA-based platforms. Our research aims to make FPGA-based high
performance computing more accessible to the scientific community. In this work
we present the results of investigating the acceleration of a particular
atmospheric model, Flexpart, on FPGAs. We focus on accelerating the most
computationally intensive kernel from this model. The key contribution of our
work is the architectural exploration we undertook to arrive at a solution that
best exploits the parallelism available in the legacy code, and is also
convenient to program, so that eventually the compilation of high-level legacy
code to our architecture can be fully automated. We present the three different
types of architecture, comparing their resource utilization and performance,
and propose that an architecture where there are a number of computational
cores, each built along the lines of a vector instruction processor, works best
in this particular scenario, and is a promising candidate for a generic
FPGA-based platform for scientific computation. We also present the results of
experiments done with various configuration parameters of the proposed
architecture, to show its utility in adapting to a range of scientific
applications."
"In this work, we present a family of architectures for polar decoders using a
reduced-complexity successive-cancellation decoding algorithm that employs
unrolling to achieve extremely high throughput values while retaining moderate
implementation complexity. The resulting fully-unrolled, deeply-pipelined
architecture is capable of achieving a coded throughput in excess of 1 Tbps on
a 65 nm ASIC at 500 MHz---three orders of magnitude greater than current
state-of-the-art polar decoders. However, unrolled decoders are built for a
specific, fixed code. Therefore we also present a new method to enable the use
of multiple code lengths and rates in a fully-unrolled polar decoder
architecture. This method leads to a length- and rate-flexible decoder while
retaining the very high speed typical to unrolled decoders. The resulting
decoders can decode a master polar code of a given rate and length, and several
shorter codes of different rates and lengths. We present results for two
versions of a multi-mode decoder supporting eight and ten different polar
codes, respectively. Both are capable of a peak throughput of 25.6 Gbps. For
each decoder, the energy efficiency for the longest supported polar code is
shown to be of 14.8 pJ/bit at 250 MHz and of 8.8 pJ/bit at 500 MHz."
"Commodity memory interfaces have difficulty in scaling memory capacity to
meet the needs of modern multicore and big data systems. DRAM device density
and maximum device count are constrained by technology, package, and signal in-
tegrity issues that limit total memory capacity. Synchronous DRAM protocols
require data to be returned within a fixed latency, and thus memory extension
methods over commodity DDRx interfaces fail to support scalable topologies.
Current extension approaches either use slow PCIe interfaces, or require
expensive changes to the memory interface, which limits commercial
adoptability. Here we propose twin-load, a lightweight asynchronous memory
access mechanism over the synchronous DDRx interface. Twin-load uses two
special loads to accomplish one access request to extended memory, the first
serves as a prefetch command to the DRAM system, and the second asynchronously
gets the required data. Twin-load requires no hardware changes on the processor
side and only slight soft- ware modifications. We emulate this system on a
prototype to demonstrate the feasibility of our approach. Twin-load has
comparable performance to NUMA extended memory and outperforms a page-swapping
PCIe-based system by several orders of magnitude. Twin-load thus enables
instant capacity increases on commodity platforms, but more importantly, our
architecture opens opportunities for the design of novel, efficient, scalable,
cost-effective memory subsystems."
"We propose an approach to data memory prefetching which augments the standard
prefetch buffer with selection criteria based on performance and usage pattern
of a given instruction. This approach is built on top of a pattern matching
based prefetcher, specifically one which can choose between a stream, a stride,
or a stream followed by a stride. We track the most recently called
instructions to make a decision on the quantity of data to prefetch next. The
decision is based on the frequency with which these instructions are called and
the hit/miss rate of the prefetcher. In our approach, we separate the amount of
data to prefetch into three categories: a high degree, a standard degree and a
low degree. We ran tests on different values for the high prefetch degree,
standard prefetch degree and low prefetch degree to determine that the most
optimal combination was 1, 4, 8 lines respectively. The 2 dimensional selection
criteria improved the performance of the prefetcher by up to 9.5% over the
first data prefetching championship winner. Unfortunately performance also fell
by as much as 14%, but remained similar on average across all of the benchmarks
we tested."
"The increasing data rates expected to be of the order of Gb/s for future
wireless systems directly impact the throughput requirements of the modulation
and coding subsystems of the physical layer. In an effort to design a suitable
channel coding solution for 5G wireless systems, in this brief we present a
massively-parallel 2.48Gb/s Quasi-Cyclic Low-Density Parity-Check (QC-LDPC)
decoder implementation operating at 200MHz on the NI USRP-2953R, on a single
FPGA. The high-level description of the entire massively-parallel decoder was
translated to a Hardware Description Language (HDL), namely VHDL, using the
algorithmic compiler in the National Instruments LabVIEW Communication System
Design Suite (CSDS) in approximately 2 minutes. This implementation not only
demonstrates the scalability of our decoder architecture but also, the rapid
prototyping capability of the LabVIEW CSDS tools. As per our knowledge, at the
time of writing this paper, this is the fastest implementation of a standard
compliant QC-LDPC decoder on a USRP using an algorithmic compiler."
"The Compressed Baryonic Matter (CBM) experiment is a part of the Facility for
Antiproton and Ion Research (FAIR) in Darmstadt at the GSI. The CBM experiment
will investigate the highly compressed nuclear matter using nucleus-nucleus
collisions. This experiment will examine heavy-ion collisions in fixed target
geometry and will be able to measure hadrons, electrons and muons. CBM requires
precise time synchronization, compact hardware, radiation tolerance,
self-triggered front-end electronics, efficient data aggregation schemes and
capability to handle high data rate (up to several TB/s). As a part of the
implementation of read out chain of MUCH in India, we have tried to implement
FPGA based emulator of GBTx in India. GBTx is a radiation tolerant ASIC that
can be used to implement multipurpose high speed bidirectional optical links
for high-energy physics (HEP) experiments and is developed by CERN. GBTx will
be used in highly irradiated area and more prone to be affected by multi bit
error. To mitigate this effect instead of single bit error correcting RS code
we have used two bit error correcting (15, 7) BCH code. It will increase the
redundancy which in turn increases the reliability of the coded data. So the
coded data will be less prone to be affected by noise due to radiation. Data
will go from detector to PC through multiple nodes through the communication
channel. In order to make the data communication secure, advanced encryption
standard (AES - a symmetric key cryptography) and RSA (asymmetric key
cryptography) are used after the channel coding."
"Modern SoCs integrate multiple CPU cores and Hardware Accelerators (HWAs)
that share the same main memory system, causing interference among memory
requests from different agents. The result of this interference, if not
controlled well, is missed deadlines for HWAs and low CPU performance.
State-of-the-art mechanisms designed for CPU-GPU systems strive to meet a
target frame rate for GPUs by prioritizing the GPU close to the time when it
has to complete a frame. We observe two major problems when such an approach is
adapted to a heterogeneous CPU-HWA system. First, HWAs miss deadlines because
they are prioritized only close to their deadlines. Second, such an approach
does not consider the diverse memory access characteristics of different
applications running on CPUs and HWAs, leading to low performance for
latency-sensitive CPU applications and deadline misses for some HWAs, including
GPUs.
  In this paper, we propose a Simple Quality of service Aware memory Scheduler
for Heterogeneous systems (SQUASH), that overcomes these problems using three
key ideas, with the goal of meeting deadlines of HWAs while providing high CPU
performance. First, SQUASH prioritizes a HWA when it is not on track to meet
its deadline any time during a deadline period. Second, SQUASH prioritizes HWAs
over memory-intensive CPU applications based on the observation that the
performance of memory-intensive applications is not sensitive to memory
latency. Third, SQUASH treats short-deadline HWAs differently as they are more
likely to miss their deadlines and schedules their requests based on worst-case
memory access time estimates.
  Extensive evaluations across a wide variety of different workloads and
systems show that SQUASH achieves significantly better CPU performance than the
best previous scheduler while always meeting the deadlines for all HWAs,
including GPUs, thereby largely improving frame rates."
"Limited memory bandwidth is a critical bottleneck in modern systems.
3D-stacked DRAM enables higher bandwidth by leveraging wider
Through-Silicon-Via (TSV) channels, but today's systems cannot fully exploit
them due to the limited internal bandwidth of DRAM. DRAM reads a whole row
simultaneously from the cell array to a row buffer, but can transfer only a
fraction of the data from the row buffer to peripheral IO circuit, through a
limited and expensive set of wires referred to as global bitlines. In presence
of wider memory channels, the major bottleneck becomes the limited data
transfer capacity through these global bitlines. Our goal in this work is to
enable higher bandwidth in 3D-stacked DRAM without the increased cost of adding
more global bitlines. We instead exploit otherwise-idle resources, such as
global bitlines, already existing within the multiple DRAM layers by accessing
the layers simultaneously. Our architecture, Simultaneous Multi Layer Access
(SMLA), provides higher bandwidth by aggregating the internal bandwidth of
multiple layers and transferring the available data at a higher IO frequency.
  To implement SMLA, simultaneous data transfer from multiple layers through
the same IO TSVs requires coordination between layers to avoid channel
conflict. We first study coordination by static partitioning, which we call
Dedicated-IO, that assigns groups of TSVs to each layer. We then provide a
simple, yet sophisticated mechanism, called Cascaded-IO, which enables
simultaneous access to each layer by time-multiplexing the IOs. By operating at
a frequency proportional to the number of layers, SMLA provides a higher
bandwidth (4X for a four-layer stacked DRAM). Our evaluations show that SMLA
provides significant performance improvement and energy reduction (55%/18% on
average for multi-programmed workloads, respectively) over a baseline
3D-stacked DRAM with very low area overhead."
"Increasing the speed of cache simulation to obtain hit/miss rates en- ables
performance estimation, cache exploration for embedded sys- tems and energy
estimation. Previously, such simulations, particu- larly exact approaches, have
been exclusively for caches which uti- lize the least recently used (LRU)
replacement policy. In this paper, we propose a new, fast and exact cache
simulation method for the First In First Out(FIFO) replacement policy. This
method, called DEW, is able to simulate multiple level 1 cache configurations
(dif- ferent set sizes, associativities, and block sizes) with FIFO replace-
ment policy. DEW utilizes a binomial tree based representation of cache
configurations and a novel searching method to speed up sim- ulation over
single cache simulators like Dinero IV. Depending on different cache block
sizes and benchmark applications, DEW oper- ates around 8 to 40 times faster
than Dinero IV. Dinero IV compares 2.17 to 19.42 times more cache ways than DEW
to determine accu- rate miss rates."
"Hitherto discovered approaches analyze the execution time of a real time
application on all the possible cache hierarchy setups to find the application
specific optimal two level inclusive data cache hierarchy to reduce cost, space
and energy consumption while satisfying the time deadline in real time
Multiprocessor Systems on Chip. These brute force like approaches can take
years to complete. Alternatively, memory access trace driven crude estimation
methods can find a cache hierarchy quickly by compromising the accuracy of
results. In this article, for the first time, we propose a fast and accurate
trace driven approach to find the optimal real time application specific two
level inclusive data cache hierarchy. Our proposed approach TRISHUL predicts
the optimal cache hierarchy performance first and then utilizes that
information to find the optimal cache hierarchy quickly. TRISHUL can suggest a
cache hierarchy, which has up to 128 times smaller size, up to 7 times faster
compared to the suggestion of the state of the art crude trace driven two level
inclusive cache hierarchy selection approach for the application traces
analyzed."
"In this paper, for the first time, we introduce a cache property called the
Intersection Property that helps to reduce singlepass simulation time in a
manner similar to inclusion property. An intersection property defines
conditions that if met, prove a particular element exists in larger caches,
thus avoiding further search time. We have discussed three such intersection
properties for caches using the FIFO replacement policy in this paper. A rapid
singlepass FIFO cache simulator CIPARSim has also been proposed. CIPARSim is
the first singlepass simulator dependent on the FIFO cache properties to reduce
simulation time significantly. CIPARSim simulation time was up to 5 times
faster compared to the state of the art singlepass FIFO cache simulator for the
cache configurations tested. CIPARSim produces the cache hit and miss rates of
an application accurately on various cache configurations. During simulation,
CIPARSim intersection properties alone predict up to 90% of the total hits,
reducing simulationtime immensely"
"In this article, we propose a technique to accelerate nonvolatile or hybrid
of volatile and nonvolatile processor cache design space exploration for
application specific embedded systems. Utilizing a novel cache behavior
modeling equation and a new accurate cache miss prediction mechanism, our
proposed technique can accelerate NVM or hybrid FIFO processor cache design
space exploration for SPEC CPU 2000 applications up to 249 times compared to
the conventional approach."
"Present state of the art applications in the area of high energy physics
experiments (HEP), radar communication, satellite communication and bio medical
instrumentation require fault resilient data acquisition (DAQ) system with the
data rate in the order of Gbps. In order to keep the high speed DAQ system
functional in such radiation environment where direct intervention of human is
not possible, a robust and error free communication system is necessary. In
this work we present an efficient DAQ design and its implementation on field
programmable gate array (FPGA). The proposed DAQ system supports high speed
data communication (~4.8 Gbps) and achieves multi-bit error correction
capabilities. BCH code (named after Raj Bose and D. K. RayChaudhuri) has been
used for multi-bit error correction. The design has been implemented on Xilinx
Kintex-7 board and is tested for board to board communication as well as for
board to PC using PCIe (Peripheral Component Interconnect express) interface.
To the best of our knowledge, the proposed FPGA based high speed DAQ system
utilizing optical link and multi-bit error resiliency can be considered first
of its kind. Performance estimation of the implemented DAQ system is done based
on resource utilization, critical path delay, efficiency and bit error rate
(BER)."
"Hybrid memory systems comprised of dynamic random access memory (DRAM) and
non-volatile memory (NVM) have been proposed to exploit both the capacity
advantage of NVM and the latency and dynamic energy advantages of DRAM. An
important problem for such systems is how to place data between DRAM and NVM to
improve system performance.
  In this paper, we devise the first mechanism, called UBM (page Utility Based
hybrid Memory management), that systematically estimates the system performance
benefit of placing a page in DRAM versus NVM and uses this estimate to guide
data placement. UBM's estimation method consists of two major components.
First, it estimates how much an application's stall time can be reduced if the
accessed page is placed in DRAM. To do this, UBM comprehensively considers
access frequency, row buffer locality, and memory level parallelism (MLP) to
estimate the application's stall time reduction. Second, UBM estimates how much
each application's stall time reduction contributes to overall system
performance. Based on this estimation method, UBM can determine and place the
most critical data in DRAM to directly optimize system performance.
Experimental results show that UBM improves system performance by 14% on
average (and up to 39%) compared to the best of three state-of-the-art
mechanisms for a large number of data-intensive workloads from the SPEC CPU2006
and Yahoo Cloud Serving Benchmark (YCSB) suites."
"We present a tool flow and results for a model-based hardware design for
FPGAs from Simulink descriptions which nicely integrates into existing
environments. While current commercial tools do not exploit some high-level
optimizations, we investigate the promising approach of using reusable
subcircuits for folding transformations to control embedded multiplier usage
and to optimize logic block usage. We show that resource improvements of up to
70% compared to the original model are possible, but it is also shown that
subcircuit selection is a critical task. While our tool flow provides good
results already, the investigation and optimization of subcircuit selection is
clearly identified as an additional keypoint to extend high-level control on
low-level FPGA mapping properties."
"In this work, we propose an architecture and methodology to design
hardware/software systems for high-performance embedded computing on FPGA. The
hardware side is based on a many-core architecture whose design is generated
automatically given a set of architectural parameters. Both the architecture
and the methodology were evaluated running dense matrix multiplication and
sparse matrix-vector multiplication on a ZYNQ-7020 FPGA platform. The results
show that using a system-level design of the system avoids complex hardware
design and still provides good performance results."
"The well known method C-Slow Retiming (CSR) can be used to automatically
convert a given CPU into a multithreaded CPU with independent threads. These
CPUs are then called streaming or barrel processors. System Hyper Pipelining
(SHP) adds a new flexibility on top of CSR by allowing a dynamic number of
threads to be executed and by enabling the threads to be stalled, bypassed and
reordered. SHP is now applied on the programming elements (PE) of a
coarse-grained reconfigurable architecture (CGRA). By using SHP, more
performance can be achieved per PE. Fork-Join operations can be implemented on
a PE using the flexibility provided by SHP to dynamically adjust the number of
threads per PE. Multiple threads can share the same data locally, which greatly
reduces the data traffic load on the CGRA's routing structure. The paper shows
the results of a CGRA using SHP-ed RISC-V cores as PEs implemented on a FPGA."
"The Baugh-Wooley algorithm is a well-known iterative algorithm for performing
multiplication in digital signal processing applications. Decomposition logic
is used with Baugh-Wooley algorithm to enhance the speed and to reduce the
critical path delay. In this paper a high speed multiplier is designed and
implemented using decomposition logic and Baugh-Wooley algorithm. The result is
compared with booth multiplier. FPGA based architecture is presented and design
has been implemented using Xilinx 12.3 device."
"The initial location of data in DRAMs is determined and controlled by the
'address-mapping' and even modern memory controllers use a fixed and
run-time-agnostic address mapping. On the other hand, the memory access pattern
seen at the memory interface level will dynamically change at run-time. This
dynamic nature of memory access pattern and the fixed behavior of address
mapping process in DRAM controllers, implied by using a fixed address mapping
scheme, means that DRAM performance cannot be exploited efficiently. DReAM is a
novel hardware technique that can detect a workload-specific address mapping at
run-time based on the application access pattern which improves the performance
of DRAMs. The experimental results show that DReAM outperforms the best
evaluated address mapping on average by 9%, for mapping-sensitive workloads, by
2% for mapping-insensitive workloads, and up to 28% across all the workloads.
DReAM can be seen as an insurance policy capable of detecting which scenarios
are not well served by the predefined address mapping."
"Memory controllers have used static page closure policies to decide whether a
row should be left open, open-page policy, or closed immediately, close-page
policy, after the row has been accessed. The appropriate choice for a
particular access can reduce the average memory latency. However, since
application access patterns change at run time, static page policies cannot
guarantee to deliver optimum execution time. Hybrid page policies have been
investigated as a means of covering these dynamic scenarios and are now
implemented in state-of-the-art processors. Hybrid page policies switch between
open-page and close-page policies while the application is running, by
monitoring the access pattern of row hits/conflicts and predicting future
behavior. Unfortunately, as the size of DRAM memory increases, fine-grain
tracking and analysis of memory access patterns does not remain practical. We
propose a compact memory address-based encoding technique which can improve or
maintain the performance of DRAMs page closure predictors while reducing the
hardware overhead in comparison with state-of-the-art techniques. As a case
study, we integrate our technique, HAPPY, with a state-of-the-art monitor, the
Intel-adaptive open-page policy predictor employed by the Intel Xeon X5650, and
a traditional Hybrid page policy. We evaluate them across 70 memory intensive
workload mixes consisting of single-thread and multi-thread applications. The
experimental results show that using the HAPPY encoding applied to the
Intel-adaptive page closure policy can reduce the hardware overhead by 5X for
the evaluated 64 GB memory (up to 40X for a 512 GB memory) while maintaining
the prediction accuracy."
"This paper presents a memory efficient, high throughput parallel lifting
based running three dimensional discrete wavelet transform (3-D DWT)
architecture. 3-D DWT is constructed by combining the two spatial and four
temporal processors. Spatial processor (SP) apply the two dimensional DWT on a
frame, using lifting based 9/7 filter bank through the row rocessor (RP) in row
direction and then apply in the colum direction through column processor (CP).
To reduce the temporal memory and the latency, the temporal processor (TP) has
been designed with lifting based 1-D Haar wavelet filter. The proposed
architecture replaced the multiplications by pipeline shift-add operations to
reduce the CPD. Two spatial processors works simultaneously on two adjacent
frames and provide 2-D DWT coefficients as inputs to the temporal processors.
TPs apply the one dimensional DWT in temporal direction and provide eight 3-D
DWT coefficients per clock (throughput). Higher throughput reduces the
computing cycles per frame and enable the lower power consumption.
Implementation results shows that the proposed architecture has the advantage
in reduced memory, low power consumption, low latency, and high throughput over
the existing designs. The RTL of the proposed architecture is described using
verilog and synthesized using 90-nm technology CMOS standard cell library and
results show that it consumes 43.42 mW power and occupies an area equivalent to
231.45 K equivalent gate at frequency of 200 MHz. The proposed architecture has
also been synthesised for the Xilinx zynq 7020 series field programmable gate
array (FPGA)."
"A large amount of research is currently going on in the field of reversible
logic, which have low heat dissipation, low power consumption, which is the
main factor to apply reversible in digital VLSI circuit design. This paper
introduces reversible gate named as Inventive0 gate. The novel gate is
synthesis the efficient adder modules with minimum garbage output and gate
count. The Inventive0 gate capable of implementing a 4-bit ripple carry adder
and carry skip adders.It is presented that Inventive0 gate is much more
efficient and optimized approach as compared to their existing design, in terms
of gate count, garbage outputs and constant inputs. In addition, some popular
available reversible gates are implemented in the MOS transistor design the
implementation kept in mind for minimum MOS transistor count and are completely
reversible in behavior more precise forward and backward computation. Lesser
architectural complexity show that the novel designs are compact, fast as well
as low power."
"Field Programmable Gate Arrays (FPGAs) are more prone to be affected by
transient faults in presence of radiation and other environmental hazards
compared to Application Specific Integrated Circuits (ASICs). Hence, error
mitigation and recovery techniques are absolutely necessary to protect the FPGA
hardware from soft errors arising due to such transient faults. In this paper,
a new efficient multi-bit error correcting method for FPGAs is proposed using
adaptive cross parity check (ACPC) code. ACPC is easy to implement and the
needed decoding circuit is also simple. In the proposed scheme total
configuration memory is partitioned into two parts. One part will contain ACPC
hardware, which is static and assumed to be unaffected by any kind of errors.
Other portion will store the binary file for logic, which is to be protected
from transient error and is assumed to be dynamically reconfigurable (Partial
reconfigurable area). Binary file from the secondary memory passes through ACPC
hardware and the bits for forward error correction (FEC) field are calculated
before entering into the reconfigurable portion. In the runtime scenario, the
data from the dynamically reconfigurable portion of the configuration memory
will be read back and passed through the ACPC hardware. The ACPC hardware will
correct the errors before the data enters into the dynamic configuration
memory. We propose a first of its kind methodology for novel transient fault
correction using ACPC code for FPGAs. To validate the design we have tested the
proposed methodology with Kintex FPGA. We have also measured different
parameters like critical path, power consumption, overhead resource and error
correction efficiency to estimate the performance of our proposed method."
"Balancing (equalization) of latency in parallel paths in the pipelined data
processing system is an important problem. Without that the data from different
paths arrive at the processing blocks in different clock cycles, and incorrect
results are produced. Manual correction of latencies is a tedious and
error-prone work. This paper presents an automatic method of latency
equalization in systems described in VHDL. The method is based on simulation
and is portable between different simulation and synthesis tools. The method
does not increase the complexity of the synthesized design comparing to the
solution based on manual latency adjustment. The example implementation of the
proposed methodology together with a simple design demonstrating its use is
available as an open source project under BSD license."
"Ultra Deep-Sub-Micron CMOS chips have to function correctly and reliably, not
only during their early post-fabrication life, but also for their entire life
span. In this paper, we present an architectural-level in-field repair
technique. The key idea is to trade area for reliability by adding repair
features to the system while keeping the power and the performance overheads as
low as possible. In the case of permanent faults, spare blocks will replace the
faulty blocks on the fly. Meanwhile by shutting down the main logic blocks,
partial threshold voltage recovery can be achieved which will alleviate the
ageing-related delays and timing issues. The technique can avoid fatal
shut-downs in the system and will decrease the down-time, hence the
availability of such a system will be preserved. We have implemented the
proposed idea on a pipelined processor core using a conventional ASIC design
flow. The simulation results show that by tolerating about 70% area overhead
and less than 18% power overhead we can dramatically increase the reliability
and decrease the downtime of the processor."
"While long polar codes can achieve the capacity of arbitrary binary-input
discrete memoryless channels when decoded by a low complexity successive
cancelation (SC) algorithm, the error performance of the SC algorithm is
inferior for polar codes with finite block lengths. The cyclic redundancy check
(CRC) aided successive cancelation list (SCL) decoding algorithm has better
error performance than the SC algorithm. However, current CRC aided SCL
(CA-SCL) decoders still suffer from long decoding latency and limited
throughput. In this paper, a reduced latency list decoding (RLLD) algorithm for
polar codes is proposed. Our RLLD algorithm performs the list decoding on a
binary tree, whose leaves correspond to the bits of a polar code. In existing
SCL decoding algorithms, all the nodes in the tree are traversed and all
possibilities of the information bits are considered. Instead, our RLLD
algorithm visits much fewer nodes in the tree and considers fewer possibilities
of the information bits. When configured properly, our RLLD algorithm
significantly reduces the decoding latency and hence improves throughput, while
introducing little performance degradation. Based on our RLLD algorithm, we
also propose a high throughput list decoder architecture, which is suitable for
larger block lengths due to its scalable partial sum computation unit. Our
decoder architecture has been implemented for different block lengths and list
sizes using the TSMC 90nm CMOS technology. The implementation results
demonstrate that our decoders achieve significant latency reduction and area
efficiency improvement compared with other list polar decoders in the
literature."
"A clock synchronizing circuit for repeaterless low swing interconnects is
presented in this paper. The circuit uses a delay locked loop (DLL) to generate
multiple phases of the clock, of which the one closest to the center of the eye
is picked by a phase detector loop. The picked phase is then further fine tuned
by an analog voltage controlled delay to position the sampling clock at the
center of the eye. A clock domain transfer circuit then transfers the sampled
data to the receiver clock domain with a maximum latency of three clock cycles.
The proposed synchronizer has been designed and fabricated in 130 nm UMC MM
CMOS technology. The circuit consumes 1.4 mW from a 1.2 V supply at a data rate
of 1.3 Gbps. Further, the proposed synchronizer has been designed and simulated
in TSMC 65 nm CMOS technology. Post layout simulations show that the
synchronizer consumes 1.5 mW from a 1 V supply, at a data rate of 4 Gbps in
this technology."
"This paper presents the evaluation of a Network-on-Chip (NoC) that offers
load balancing for Systems-on-Chip (SoCs) dedicated for multimedia applications
that require high traffic of variable bitrate communication. The NoC is based
on a technique that allows the interleaving of flits from diferente flows in
the same communication channel, and keep the load balancing without a
centralized control in the network. For this purpose, all flits in the network
received extra bits, such that every flit carries routing information. The
routers use this extra information to perform arbitration and schedule the
flits to the corresponding output ports. Analytic comparisons and experimental
data show that the approach adopted in the network keeps average latency lower
for variable bitrate flows than a network based on resource reservation when
both networks are working over 80% of offered load."
"Security of embedded computing systems is becoming of paramount concern as
these devices become more ubiquitous, contain personal information and are
increasingly used for financial transactions. Security attacks targeting
embedded systems illegally gain access to the information in these devices or
destroy information. The two most common types of attacks embedded systems
encounter are code-injection and power analysis attacks. In the past, a number
of countermeasures, both hardware- and software-based, were proposed
individually against these two types of attacks. However, no single system
exists to counter both of these two prominent attacks in a processor based
embedded system. Therefore, this paper, for the first time, proposes a
hardware/software based countermeasure against both code-injection attacks and
power analysis based side-channel attacks in a dual core embedded system. The
proposed processor, named SecureD, has an area overhead of just 3.80% and an
average runtime increase of 20.0% when compared to a standard dual processing
system. The overhead were measured using a set of industry standard application
benchmarks, with two encryption and five other programs."
"Repeaterless low swing interconnects use mixed signal circuits to achieve
high performance at low power. When these interconnects are used in large scale
and high volume digital systems their testability becomes very important. This
paper discusses the testability of low swing repeaterless on-chip interconnects
with equalization and clock synchronization. A capacitively coupled transmitter
with a weak driver is used as the transmitter. The receiver samples the low
swing input data at the center of the data eye and converts it to rail to rail
levels and also synchronizes the data to the receiver's clock domain. The
system is a mixed signal circuit and the digital components are all scan
testable. For the analog section, just a DC test has a fault coverage of 50% of
the structural faults. Simple techniques allow integration of the analog
components into the digital scan chain increasing the coverage to 74%. Finally,
a BIST with low overhead enhances the coverage to 95% of the structural faults.
The design and simulations have been done in UMC 130 nm CMOS technology."
"Cache coherence scalability is a big challenge in shared memory systems.
Traditional protocols do not scale due to the storage and traffic overhead of
cache invalidation. Tardis, a recently proposed coherence protocol, removes
cache invalidation using logical timestamps and achieves excellent scalability.
The original Tardis protocol, however, only supports the Sequential Consistency
(SC) memory model, limiting its applicability. Tardis also incurs extra network
traffic on some benchmarks due to renew messages, and has suboptimal
performance when the program uses spinning to communicate between threads.
  In this paper, we address these downsides of Tardis protocol and make it
significantly more practical. Specifically, we discuss the architectural,
memory system and protocol changes required in order to implement the TSO
consistency model on Tardis, and prove that the modified protocol satisfies
TSO. We also describe modifications for Partial Store Order (PSO) and Release
Consistency (RC). Finally, we propose optimizations for better leasing policies
and to handle program spinning. On a set of benchmarks, optimized Tardis
improves on a full-map directory protocol in the metrics of performance,
storage and network traffic, while being simpler to implement."
"On-chip voltage regulation using distributed Digital Low Drop Out (LDO)
voltage regulators has been identified as a promising technique for efficient
power-management for emerging multi-core processors. Digital LDOs (DLDO) can
offer low voltage operation, faster transient response, and higher current
efficiency. Response time as well as output voltage ripple can be reduced by
increasing the speed of the dynamic comparators. However, the comparator offset
steeply increases for high clock frequencies, thereby leading to enhanced
variations in output voltage. In this work we explore the design of digital
LDOs with multiple dynamic comparators that can overcome this bottleneck. In
the proposed topology, we apply time-interleaved comparators with the same
voltage threshold and uniform current step in order to accomplish the
aforementioned features. Simulation based analysis shows that the DLDO with
time-interleaved comparators can achieve better overall performance in terms of
current efficiency, ripple and settling time. For a load step of 50mA, a DLDO
with 8 time-interleaved comparators could achieve an output ripple of less than
5mV, while achieving a settling time of less than 0.5us. Load current dependant
dynamic adjustment of clock frequency is proposed to maintain high current
efficiency of ~97%."
"This paper presents a design of low power data channel for application in
High Definition Multimedia Interface (HDMI) Transmitter circuit. The input is
10 bit parallel data and output is serial data at 1.65 Gbps. This circuit uses
only a single frequency of serial clock input. All other timing signals are
derived within the circuit from the serial clock. This design has dedicated
lines to disable and enable all its channels within two pixel-clock periods
only. A pair of disable and enable functions performed immediately after
power-on of the circuit serves as the reset function. The presented design is
immune to data-dependent switching spikes in supply current and pushes them in
the range of serial frequency and its multiples. Thus filtering requirements
are relaxed. The output stage uses a bias voltage of 2.8 volts for a receiver
pull-up voltage of 3.3 volts. The reported data channel is designed using UMC
180 nm CMOS Technology. The design is modifiable for other inter-board serial
interfaces like USB and LAN with different number of bits at the parallel
input."
"Modern DRAM cells are periodically refreshed to prevent data loss due to
leakage. Commodity DDR DRAM refreshes cells at the rank level. This degrades
performance significantly because it prevents an entire rank from serving
memory requests while being refreshed. DRAM designed for mobile platforms,
LPDDR DRAM, supports an enhanced mode, called per-bank refresh, that refreshes
cells at the bank level. This enables a bank to be accessed while another in
the same rank is being refreshed, alleviating part of the negative performance
impact of refreshes. However, there are two shortcomings of per-bank refresh.
First, the per-bank refresh scheduling scheme does not exploit the full
potential of overlapping refreshes with accesses across banks because it
restricts the banks to be refreshed in a sequential round-robin order. Second,
accesses to a bank that is being refreshed have to wait.
  To mitigate the negative performance impact of DRAM refresh, we propose two
complementary mechanisms, DARP (Dynamic Access Refresh Parallelization) and
SARP (Subarray Access Refresh Parallelization). The goal is to address the
drawbacks of per-bank refresh by building more efficient techniques to
parallelize refreshes and accesses within DRAM. First, instead of issuing
per-bank refreshes in a round-robin order, DARP issues per-bank refreshes to
idle banks in an out-of-order manner. Furthermore, DARP schedules refreshes
during intervals when a batch of writes are draining to DRAM. Second, SARP
exploits the existence of mostly-independent subarrays within a bank. With
minor modifications to DRAM organization, it allows a bank to serve memory
accesses to an idle subarray while another subarray is being refreshed.
Extensive evaluations show that our mechanisms improve system performance and
energy efficiency compared to state-of-the-art refresh policies and the benefit
increases as DRAM density increases."
"This paper summarizes the idea of Tiered-Latency DRAM, which was published in
HPCA 2013. The key goal of TL-DRAM is to provide low DRAM latency at low cost,
a critical problem in modern memory systems. To this end, TL-DRAM introduces
heterogeneity into the design of a DRAM subarray by segmenting the bitlines,
thereby creating a low-latency, low-energy, low-capacity portion in the
subarray (called the near segment), which is close to the sense amplifiers, and
a high-latency, high-energy, high-capacity portion, which is farther away from
the sense amplifiers. Thus, DRAM becomes heterogeneous with a small portion
having lower latency and a large portion having higher latency. Various
techniques can be employed to take advantage of the low-latency near segment
and this new heterogeneous DRAM substrate, including hardware-based caching and
software based caching and memory allocation of frequently used data in the
near segment. Evaluations with simple such techniques show significant
performance and energy-efficiency benefits."
"Die-stacked DRAM has been proposed for use as a large, high-bandwidth,
last-level cache with hundreds or thousands of megabytes of capacity. Not all
workloads (or phases) can productively utilize this much cache space, however.
Unfortunately, the unused (or under-used) cache continues to consume power due
to leakage in the peripheral circuitry and periodic DRAM refresh. Dynamically
adjusting the available DRAM cache capacity could largely eliminate this energy
overhead. However, the current proposed DRAM cache organization introduces new
challenges for dynamic cache resizing. The organization differs from a
conventional SRAM cache organization because it places entire cache sets and
their tags within a single bank to reduce on-chip area and power overhead.
Hence, resizing a DRAM cache requires remapping sets from the powered-down
banks to active banks.
  In this paper, we propose CRUNCH (Cache Resizing Using Native Consistent
Hashing), a hardware data remapping scheme inspired by consistent hashing, an
algorithm originally proposed to uniformly and dynamically distribute Internet
traffic across a changing population of web servers. CRUNCH provides a
load-balanced remapping of data from the powered-down banks alone to the active
banks, without requiring sets from all banks to be remapped, unlike naive
schemes to achieve load balancing. CRUNCH remaps only sets from the
powered-down banks, so it achieves this load balancing with low bank
power-up/down transition latencies. CRUNCH's combination of good load balancing
and low transition latencies provides a substrate to enable efficient DRAM
cache resizing."
"In multithreaded applications with high degree of data sharing, the miss rate
of private cache is shown to exhibit a compulsory miss component. It manifests
because at least some of the shared data originates from other cores and can
only be accessed in a shared cache. The compulsory component does not change
with the private cache size, causing its miss rate to diminish slower as the
cache size grows. As a result, the peak performance of a Chip Multiprocessor
(CMP) for workloads with high degree of data sharing is achieved with a smaller
private cache, compared to workloads with no data sharing. The CMP performance
can be improved by reassigning some of the constrained area or power resource
from private cache to core. Alternatively, the area or power budget of a CMP
can be reduced without a performance hit."
"Modern Graphics Processing Units (GPUs) are well provisioned to support the
concurrent execution of thousands of threads. Unfortunately, different
bottlenecks during execution and heterogeneous application requirements create
imbalances in utilization of resources in the cores. For example, when a GPU is
bottlenecked by the available off-chip memory bandwidth, its computational
resources are often overwhelmingly idle, waiting for data from memory to
arrive.
  This work describes the Core-Assisted Bottleneck Acceleration (CABA)
framework that employs idle on-chip resources to alleviate different
bottlenecks in GPU execution. CABA provides flexible mechanisms to
automatically generate ""assist warps"" that execute on GPU cores to perform
specific tasks that can improve GPU performance and efficiency.
  CABA enables the use of idle computational units and pipelines to alleviate
the memory bandwidth bottleneck, e.g., by using assist warps to perform data
compression to transfer less data from memory. Conversely, the same framework
can be employed to handle cases where the GPU is bottlenecked by the available
computational units, in which case the memory pipelines are idle and can be
used by CABA to speed up computation, e.g., by performing memoization using
assist warps.
  We provide a comprehensive design and evaluation of CABA to perform effective
and flexible data compression in the GPU memory hierarchy to alleviate the
memory bandwidth bottleneck. Our extensive evaluations show that CABA, when
used to implement data compression, provides an average performance improvement
of 41.7% (as high as 2.6X) across a variety of memory-bandwidth-sensitive GPGPU
applications."
"Recently the hardware emulation technique has emerged as a promising approach
to accelerating hardware verification/debugging process. To fully evaluate the
powerfulness of the emulation approach and demonstrate its potential impact, we
propose to emulate a system-on-chip (SoC) design using Mentor Graphics Veloce
emulation platform. This article presents our project setup and the results we
have achieved. The results are encouraging. ORPSoC emulation with Veloce has
more than ten times faster than hardware simulation. Our experimental results
demonstrate that Mentor Graphics Veloce has major advantages in emulation,
verification, and debugging of complicated real hardware designs, especially in
the context of SoC complexity. Through our three major tasks, we will
demonstrate that (1) Veloce can successfully emulate large-scale SoC designs;
(2) it has much better performance comparing to the state-of-the-art simulation
tools; (3) it can significantly accelerate the process of hardware verification
and debugging while maintaining full signal visibility."
"Due to embedded systems` stringent design constraints, much prior work
focused on optimizing energy consumption and/or performance. Since embedded
systems typically have fewer cooling options, rising temperature, and thus
temperature optimization, is an emergent concern. Most embedded systems only
dissipate heat by passive convection, due to the absence of dedicated thermal
management hardware mechanisms. The embedded system`s temperature not only
affects the system`s reliability, but could also affect the performance, power,
and cost. Thus, embedded systems require efficient thermal management
techniques. However, thermal management can conflict with other optimization
objectives, such as execution time and energy consumption. In this paper, we
focus on managing the temperature using a synergy of cache optimization and
dynamic frequency scaling, while also optimizing the execution time and energy
consumption. This paper provides new insights on the impact of cache parameters
on efficient temperature-aware cache tuning heuristics. In addition, we present
temperature-aware phase-based tuning, TaPT, which determines Pareto optimal
clock frequency and cache configurations for fine-grained execution time,
energy, and temperature tradeoffs. TaPT enables autonomous system optimization
and also allows designers to specify temperature constraints and optimization
priorities. Experiments show that TaPT can effectively reduce execution time,
energy, and temperature, while imposing minimal hardware overhead."
"Networked embedded systems typically leverage a collection of low-power
embedded systems (nodes) to collaboratively execute applications spanning
diverse application domains (e.g., video, image processing, communication,
etc.) with diverse application requirements. The individual networked nodes
must operate under stringent constraints (e.g., energy, memory, etc.) and
should be specialized to meet varying application requirements in order to
adhere to these constraints. Phase-based tuning specializes system tunable
parameters to the varying runtime requirements of different execution phases to
meet optimization goals. Since the design space for tunable systems can be very
large, one of the major challenges in phase-based tuning is determining the
best configuration for each phase without incurring significant tuning overhead
(e.g., energy and/or performance) during design space exploration. In this
paper, we propose phase distance mapping, which directly determines the best
configuration for a phase, thereby eliminating design space exploration. Phase
distance mapping applies the correlation between the characteristics and best
configuration of a known phase to determine the best configuration of a new
phase. Experimental results verify that our phase distance mapping approach,
when applied to cache tuning, determines cache configurations within 1 % of the
optimal configurations on average and yields an energy delay product savings of
27 % on average."
"Barriers that prevent programmers from using FPGAs include the need to work
within vendor specific CAD tools, knowledge of hardware programming models, and
the requirement to pass each design through synthesis, place and route. In this
work, a dynamic overlay is designed to support Just- In-Time assembly by
composing hardware operators to construct full accelerators. The hardware
operators are pre-synthesized bit- streams and can be downloaded to Partially
Reconfigurable(PR) regions at runtime."
"A complex digital circuit comprises of adder as a basic unit. The performance
of the circuit depends on the design of this basic adder unit. The speed of
operation of a circuit is one of the important performance criteria of many
digital circuits which ultimately depends on the delay of the basic adder unit.
Many research works have been devoted in improving the delay of the adder
circuit. In this paper we have proposed an improved carry increment adder (CIA)
that improves the delay performance of the circuit. The improvement is achieved
by incorporating carry look adder (CLA) in the design of CIA contrary to the
previous design of CIA that employs ripple carry adder (RCA). A simulation
study is carried out for comparative analysis. The coding is done in Verilog
hardware description language (HDL) and the simulation is carried out in Xilinx
ISE 13.1 environment."
"The use of asynchronous design approaches to construct digital signal
processing (DSP) systems is a rapidly growing research area driven by a wide
range of emerging energy constrained applications such as wireless sensor
network, portable medical devices and brain implants. The asynchronous design
techniques allow the construction of systems which are samples driven, which
means they only dissipate dynamic energy when there processing data and idle
otherwise. This inherent advantage of asynchronous design over conventional
synchronous circuits allows them to be energy efficient. However the
implementation flow of asynchronous systems is still difficult due to its lack
of compatibility with industry-standard synchronous design tools and modelling
languages. This paper devises a novel asynchronous design for a finite impulse
response (FIR) filter, an essential building block of DSP systems, which is
synthesizable and suitable for implementation using conventional synchronous
systems design flow and tools. The proposed design is based on a modified
version of the micropipline architecture and it is constructed using four phase
bundled data protocol. A hardware prototype of the proposed filter has been
developed on an FPGA, and systematically verified. The results prove correct
functionality of the novel design and a superior performance compared to a
synchronous FIR implementation. The findings of this work will allow a wider
adoption of asynchronous circuits by DSP designers to harness their energy and
performance benefits."
"The section-carry based carry lookahead adder (SCBCLA) topology was proposed
as an improved high-speed alternative to the conventional carry lookahead adder
(CCLA) topology in previous works. Self-timed and FPGA-based implementations of
SCBCLAs and CCLAs were considered earlier, and it was found that SCBCLAs could
help in delay reduction i.e. pave the way for improved speed compared to CCLAs
at the expense of some increase in area and/or power parameters. In this work,
we consider semi-custom ASIC-based implementations of different variants of
SCBCLAs and CCLAs to perform 32-bit dual-operand addition. Based on the
simulation results for 32-bit dual-operand addition obtained by targeting a
high-end 32/28nm CMOS process, it is found that an optimized SCBCLA
architecture reports a 9.8% improvement in figure-of-merit (FOM) compared to an
optimized CCLA architecture, where the FOM is defined as the inverse of the
product of power, delay, and area. It is generally inferred from the
simulations that the SCBCLA architecture could be more beneficial compared to
the CCLA architecture in terms of the design metrics whilst benefitting a
variety of computer arithmetic operations involving dual-operand and/or
multi-operand additions. Also, it is observed that heterogeneous CLA
architectures tend to fare well compared to homogeneous CLA architectures, as
substantiated by the simulation results."
"This paper analyzes the merits and demerits of global weak-indication
self-timed function blocks versus local weak-indication self-timed function
blocks, implemented using a delay-insensitive data code and adhering to 4-phase
return-to-zero handshaking. A self-timed ripple carry adder is considered as an
example function block for the analysis. The analysis shows that while global
weak-indication could help in optimizing the power, latency and area
parameters, local weak-indication facilitates the optimum performance in terms
of realizing the data-dependent cycle time that is characteristic of a
weak-indication self-timed design."
"N-modular redundancy (NMR) is commonly used to enhance the fault tolerance of
a circuit/system, when subject to a fault-inducing environment such as in space
or military systems, where upsets due to radiation phenomena, temperature
and/or other environmental conditions are anticipated. Triple Modular
Redundancy (TMR), which is a 3-tuple version of NMR, is widely preferred for
mission-control space, military, and aerospace, and safety-critical nuclear,
power, medical, and industrial control and automation systems. The TMR scheme
involves the two-times duplication of a simplex system hardware, with a
majority voter ensuring correctness provided at least two out of three copies
of the hardware remain operational. Thus the majority voter plays a pivotal
role in ensuring the correct operation of the TMR scheme. In this paper, a
number of standard-cell based majority voter designs relevant to TMR
architectures are presented, and their power, delay and area parameters are
estimated based on physical realization using a 32/28nm CMOS process."
"This paper summarizes the idea of Adaptive-Latency DRAM (AL-DRAM), which was
published in HPCA 2015. The key goal of AL-DRAM is to exploit the extra margin
that is built into the DRAM timing parameters to reduce DRAM latency. The key
observation is that the timing parameters are dictated by the worst-case
temperatures and worst-case DRAM cells, both of which lead to small amount of
charge storage and hence high access latency. One can therefore reduce latency
by adapting the timing parameters to the current operating temperature and the
current DIMM that is being accessed. Using an FPGA-based testing platform, our
work first characterizes the extra margin for 115 DRAM modules from three major
manufacturers. The experimental results demonstrate that it is possible to
reduce four of the most critical timing parameters by a minimum/maximum of
17.3%/54.8% at 55C while maintaining reliable operation. AL-DRAM adaptively
selects between multiple different timing parameters for each DRAM module based
on its current operating condition. AL-DRAM does not require any changes to the
DRAM chip or its interface; it only requires multiple different timing
parameters to be specified and supported by the memory controller. Real system
evaluations show that AL-DRAM improves the performance of memory-intensive
workloads by an average of 14% without introducing any errors."
"We present a first of its kind framework which overcomes a major challenge in
the design of digital systems that are resilient to reliability failures:
achieve desired resilience targets at minimal costs (energy, power, execution
time, area) by combining resilience techniques across various layers of the
system stack (circuit, logic, architecture, software, algorithm). This is also
referred to as cross-layer resilience. In this paper, we focus on
radiation-induced soft errors in processor cores. We address both single-event
upsets (SEUs) and single-event multiple upsets (SEMUs) in terrestrial
environments. Our framework automatically and systematically explores the large
space of comprehensive resilience techniques and their combinations across
various layers of the system stack (586 cross-layer combinations in this
paper), derives cost-effective solutions that achieve resilience targets at
minimal costs, and provides guidelines for the design of new resilience
techniques. We demonstrate the practicality and effectiveness of our framework
using two diverse designs: a simple, in-order processor core and a complex,
out-of-order processor core. Our results demonstrate that a carefully optimized
combination of circuit-level hardening, logic-level parity checking, and
micro-architectural recovery provides a highly cost-effective soft error
resilience solution for general-purpose processor cores. For example, a 50x
improvement in silent data corruption rate is achieved at only 2.1% energy cost
for an out-of-order core (6.1% for an in-order core) with no speed impact.
However, selective circuit-level hardening alone, guided by a thorough analysis
of the effects of soft errors on application benchmarks, provides a
cost-effective soft error resilience solution as well (with ~1% additional
energy cost for a 50x improvement in silent data corruption rate)."
"This article presents two area/latency optimized gate level asynchronous full
adder designs which correspond to early output logic. The proposed full adders
are constructed using the delay-insensitive dual-rail code and adhere to the
four-phase return-to-zero handshaking. For an asynchronous ripple carry adder
(RCA) constructed using the proposed early output full adders, the
relative-timing assumption becomes necessary and the inherent advantages of the
relative-timed RCA are: (1) computation with valid inputs, i.e., forward
latency is data-dependent, and (2) computation with spacer inputs involves a
bare minimum constant reverse latency of just one full adder delay, thus
resulting in the optimal cycle time. With respect to different 32-bit RCA
implementations, and in comparison with the optimized strong-indication,
weak-indication, and early output full adder designs, one of the proposed early
output full adders achieves respective reductions in latency by 67.8, 12.3 and
6.1 %, while the other proposed early output full adder achieves corresponding
reductions in area by 32.6, 24.6 and 6.9 %, with practically no power penalty.
Further, the proposed early output full adders based asynchronous RCAs enable
minimum reductions in cycle time by 83.4, 15, and 8.8 % when considering
carry-propagation over the entire RCA width of 32-bits, and maximum reductions
in cycle time by 97.5, 27.4, and 22.4 % for the consideration of a typical
carry chain length of 4 full adder stages, when compared to the least of the
cycle time estimates of various strong-indication, weak-indication, and early
output asynchronous RCAs of similar size. All the asynchronous full adders and
RCAs were realized using standard cells in a semi-custom design fashion based
on a 32/28 nm CMOS process technology."
"In modern systems, DRAM-based main memory is significantly slower than the
processor. Consequently, processors spend a long time waiting to access data
from main memory, making the long main memory access latency one of the most
critical bottlenecks to achieving high system performance. Unfortunately, the
latency of DRAM has remained almost constant in the past decade. This is mainly
because DRAM has been optimized for cost-per-bit, rather than access latency.
As a result, DRAM latency is not reducing with technology scaling, and
continues to be an important performance bottleneck in modern and future
systems.
  This dissertation seeks to achieve low latency DRAM-based memory systems at
low cost in three major directions. First, based on the observation that long
bitlines in DRAM are one of the dominant sources of DRAM latency, we propose a
new DRAM architecture, Tiered-Latency DRAM (TL-DRAM), which divides the long
bitline into two shorter segments using an isolation transistor, allowing one
segment to be accessed with reduced latency. Second, we propose a fine-grained
DRAM latency reduction mechanism, Adaptive-Latency DRAM, which optimizes DRAM
latency for the common operating conditions for individual DRAM module. Third,
we propose a new technique, Architectural-Variation-Aware DRAM (AVA-DRAM),
which reduces DRAM latency at low cost, by profiling and identifying only the
inherently slower regions in DRAM to dynamically determine the lowest latency
DRAM can operate at without causing failures.
  This dissertation provides a detailed analysis of DRAM latency by using both
circuit-level simulation with a detailed DRAM model and FPGA-based profiling of
real DRAM modules. Our latency analysis shows that our low latency DRAM
mechanisms enable significant latency reductions, leading to large improvement
in both system performance and energy efficiency."
"We present a fixed point architecture (source VHDL code is provided) for
powering computation. The fully customized architecture, based on the expanded
hyperbolic CORDIC algorithm, allows for design space exploration to establish
trade-offs among design parameters (numerical format, number of iterations),
execution time, resource usage and accuracy. We also generate Pareto-optimal
realizations in the resource-accuracy space: this approach can produce optimal
hardware realizations that simultaneously satisfy resource and accuracy
requirements."
"This article presents the design of a new asynchronous early output full
adder which when cascaded leads to a relative-timed ripple carry adder (RCA).
The relative-timed RCA requires imposing a very small relative-timing
assumption to overcome the problem of gate orphans associated with internal
carry propagation. The relative-timing assumption is however independent of the
RCA size. The primary benefits of the relative-timed RCA are processing of
valid data incurs data-dependent forward latency, while the processing of
spacer involves a very fast constant time reverse latency of just 1 full adder
delay which represents the ultimate in the design of an asynchronous RCA with
the fastest reset. The secondary benefits of the relative-timed RCA are it
achieves good optimization of power and area metrics simultaneously. A 32-bit
relative-timed RCA constructed using the proposed early output full adder
achieves respective reductions in forward latency by 67%, 10% and 3.5% compared
to the optimized strong-indication, weak-indication, and early output 32-bit
asynchronous RCAs existing in the literature. Based on a similar comparison,
the proposed 32-bit relative-timed RCA achieves corresponding reductions in
cycle time by 83%, 12.7% and 6.4%. In terms of area, the proposed 32-bit
relative-timed RCA occupies 27% less Silicon than its optimized
strong-indication counterpart and 17% less Silicon than its optimized
weak-indication counterpart, and features increased area occupancy by a meager
1% compared to the optimized early output 32-bit asynchronous RCA. The average
power dissipation of all the asynchronous 32-bit RCAs are found to be
comparable since they all satisfy the monotonic cover constraint. The
simulation results obtained correspond to a 32/28nm CMOS process."
"For digital system designs, triple modular redundancy (TMR), which is a
3-tuple version of N-modular redundancy is widely preferred for many
mission-control and safety-critical applications. The TMR scheme involves
two-times duplication of the simplex system hardware, with a majority voter
ensuring correctness provided at least two out of three copies of the system
remain operational. Thus the majority voter plays a pivotal role in ensuring
the correct operation of the system. The fundamental assumption implicit in the
TMR scheme is that the majority voter does not become faulty, which may not
hold well for implementations based on latest technology nodes with dimensions
of the order of just tens of nanometers. To overcome the drawbacks of the
classical majority voter some new voter designs were put forward in the
literature with the aim of enhancing the fault tolerance. However, these voter
designs generally ensure the correct system operation in the presence of either
a faulty function module or the faulty voter, considered only in isolation.
Since multiple faults may no longer be excluded in the nanoelectronics regime,
simultaneous fault occurrences on both the function module and the voter should
be considered, and the fault tolerance of the voters have to be analyzed under
such a scenario. In this context, this article proposes a new fault-tolerant
majority voter which is found to be more robust to faults than the existing
voters in the presence of faults occurring internally and/or externally to the
voter. Moreover, the proposed voter features less power dissipation, delay, and
area metrics based on the simulation results obtained by using a 32/28nm CMOS
process."
"Heterogeneous computing can potentially offer significant performance and
performance per watt improvements over homogeneous computing, but the question
""what is the ideal mapping of algorithms to architectures?"" remains an open
one. In the past couple of years new types of computing devices such as FPGAs
have come into general computing use. In this work we attempt to add to the
body of scientific knowledge by comparing Kernel performance and performance
per watt of seven key algorithms according to Berkley's dwarf taxonomy. We do
so using the Rodinia benchmark suite on three different high-end hardware
architecture representatives from the CPU, GPU and FPGA families. We find
results that support some distinct mappings between the architecture and
performance per watt. Perhaps the most interesting finding is that, for our
specific hardware representatives, FPGAs should be considered as alternatives
to GPUs and CPUs in several key algorithms: N-body simulations, dense linear
algebra and structured grid."
"In most modern systems, the memory subsystem is managed and accessed at
multiple different granularities at various resources. We observe that such
multi-granularity management results in significant inefficiency in the memory
subsystem. Specifically, we observe that 1) page-granularity virtual memory
unnecessarily triggers large memory operations, and 2) existing cache-line
granularity memory interface is inefficient for performing bulk data operations
and operations that exhibit poor spatial locality. To address these problems,
we present a series of techniques in this thesis.
  First, we propose page overlays, a framework augments the existing virtual
memory framework with the ability to track a new version of a subset of cache
lines within each virtual page. We show that this extension is powerful by
demonstrating its benefits on a number of applications.
  Second, we show that DRAM can be used to perform more complex operations than
just store data. We propose RowClone, a mechanism to perform bulk data copy and
initialization completely inside DRAM, and Buddy RAM, a mechanism to perform
bulk bitwise operations using DRAM. Both these techniques achieve an
order-of-magnitude improvement in the efficiency of the respective operations.
  Third, we propose Gather-Scatter DRAM, a technique that exploits DRAM
organization to effectively gather/scatter values with a power-of-2 strided
access patterns. For these access patterns, GS-DRAM achieves near-ideal
bandwidth and cache utilization, without increasing the latency of fetching
data from memory.
  Finally, we propose the Dirty-Block Index, a new way of tracking dirty
blocks. In addition to improving the efficiency of bulk data coherence, DBI has
several applications including high-performance memory scheduling, efficient
cache lookup bypassing, and enabling heterogeneous ECC."
"The 2nd International Workshop on Overlay Architectures for FPGAs (OLAF 2016)
was held on 21 Mar, 2016 as a co-located workshop at the 24th ACM/SIGDA
International Symposium on Field-Programmable Gate Arrays (FPGA 2016). This
year, the program committee selected 6 papers and 3 extended abstracts to be
presented at the workshop, which are subsequently collected in this online
volume."
"GRVI is an FPGA-efficient RISC-V RV32I soft processor. Phalanx is a parallel
processor and accelerator array framework. Groups of processors and
accelerators form shared memory clusters. Clusters are interconnected with each
other and with extreme bandwidth I/O and memory devices by a 300- bit-wide
Hoplite NOC. An example Kintex UltraScale KU040 system has 400 RISC-V cores,
peak throughput of 100,000 MIPS, peak shared memory bandwidth of 600 GB/s, NOC
bisection bandwidth of 700 Gbps, and uses 13 W."
"In this paper, we describe a high-level synthesis (HLS) tool that
automatically allows area/throughput trade-offs for implementing streaming task
graphs (STG). Our tool targets a massively parallel processor array (MPPA)
architecture, very similar to the Ambric MPPA chip architecture, which is to be
implemented as an FPGA overlay. Similar to Ambric tools, our HLS tool accepts a
STG as input written in a subset of Java and a structural language in the style
of a Kahn Processing Network (KPN). Unlike the Ambric tools, our HLS tool
analyzes the parallelism internal to each Java ""node"" and evaluates the
throughput and area of several possible implementations. It then analyzes the
full graph for bottlenecks or excess compute capacity, selects an
implementation for each node, and even considers replicating or splitting nodes
while either minimizing area (for a fixed throughput target), or maximizing
throughput (for a fixed area target). In addition to traditional node selection
and replication methods used in prior work, we have uniquely implemented node
combining and splitting to find a better area/throughput trade-off. We present
two optimization approaches, a formal ILP formulation and a heuristic solution.
Results show that the heuristic is more flexible and can find design points not
available to the ILP, thereby achieving superior results."
"With power consumption becoming a critical processor design issue,
specialized architectures for low power processing are becoming popular.
Several studies have shown that neural networks can be used for signal
processing and pattern recognition applications. This study examines the design
of memristor based multicore neural processors that would be used primarily to
process data directly from sensors. Additionally, we have examined the design
of SRAM based neural processors for the same task. Full system evaluation of
the multicore processors based on these specialized cores were performed taking
I/O and routing circuits into consideration. The area and power benefits were
compared with traditional multicore RISC processors. Our results show that the
memristor based architectures can provide an energy efficiency between three
and five orders of magnitude greater than that of RISC processors for the
benchmarks examined."
"A low-power precision-scalable processor for ConvNets or convolutional neural
networks (CNN) is implemented in a 40nm technology. Its 256 parallel processing
units achieve a peak 102GOPS running at 204MHz. To minimize energy consumption
while maintaining throughput, this works is the first to both exploit the
sparsity of convolutions and to implement dynamic precision-scalability
enabling supply- and energy scaling. The processor is fully C-programmable,
consumes 25-288mW at 204 MHz and scales efficiency from 0.3-2.6 real TOPS/W.
This system hereby outperforms the state-of-the-art up to 3.9x in energy
efficiency."
"The section-carry based carry lookahead adder (SCBCLA) architecture was
proposed as an efficient alternative to the conventional carry lookahead adder
(CCLA) architecture for the physical implementation of computer arithmetic. In
previous related works, self-timed SCBCLA architectures and synchronous SCBCLA
architectures were realized using standard cells and FPGAs. In this work, we
deal with improved realizations of synchronous SCBCLA architectures designed in
a semi-custom fashion using standard cells. The improvement is quantified in
terms of a figure of merit (FOM), where the FOM is defined as the inverse
product of power, delay and area. Since power, delay and area of digital
designs are desirable to be minimized, the FOM is desirable to be maximized.
Starting from an efficient conventional carry lookahead generator, we show how
an optimized section-carry based carry lookahead generator is realized. In
comparison with our recent work dealing with standard cells based
implementation of SCBCLAs to perform 32-bit addition of two binary operands, we
show in this work that with improved section-carry based carry lookahead
generators, the resulting SCBCLAs exhibit significant improvements in FOM.
Compared to the earlier optimized hybrid SCBCLA, the proposed optimized hybrid
SCBCLA improves the FOM by 88.3%. Even the optimized hybrid CCLA features
improvement in FOM by 77.3% over the earlier optimized hybrid CCLA. However,
the proposed optimized hybrid SCBCLA is still the winner and has a better FOM
than the currently optimized hybrid CCLA by 15.3%. All the CCLAs and SCBCLAs
are implemented to realize 32-bit dual-operand binary addition using a 32/28nm
CMOS process."
"Typically, a memory request from a processor may need to go through many
intermediate interconnect routers, directory node, owner node, etc before it is
finally serviced. Current multiprocessors do not give preference to any
particular memory request. But certain memory requests are more critical to
multiprocessor's performance than other requests. Example: memory requests from
critical sections, load request feeding into multiple dependent instructions,
etc. This knowledge can be used to improve the performance of current
multiprocessors by letting the ordering point and the interconnect routers
prioritize critical requests over non-critical ones. In this paper, we evaluate
using SIMICS/GEMS infrastructure. For lock-intensive microbenchmarks,
criticality-aware multiprocessors showed 5-15% performance improvement over
baseline multiprocessor. Criticality aware multiprocessor provides a new
direction for tapping performance in a shared memory multiprocessor and can
provide substantial speedup in lock intensive benchmarks."
"In this work, we present a new approach to high level synthesis (HLS), where
high level functions are first mapped to an architectural template, before
hardware synthesis is performed. As FPGA platforms are especially suitable for
implementing streaming processing pipelines, we perform transformations on
conventional high level programs where they are turned into multi-stage
dataflow engines [1]. This target template naturally overlaps slow memory data
accesses with computations and therefore has much better tolerance towards
memory subsystem latency. Using a state-of-the-art HLS tool for the actual
circuit generation, we observe up to 9x improvement in overall performance when
the dataflow architectural template is used as an intermediate compilation
target."
"The FPGA overlay architectures have been mainly proposed to improve design
productivity, circuit portability and system debugging. In this paper, we
address the use of overlay architectures for building fault tolerant SRAM-based
FPGA systems and discuss the main features and design challenges of a
reliability-aware overlay architecture."
"We present a customizable soft architecture which allows for the execution of
GPGPU code on an FPGA without the need to recompile the design. Issues related
to scaling the overlay architecture to multiple GPGPU multiprocessors are
considered along with application-class architectural optimizations. The
overlay architecture is optimized for FPGA implementation to support efficient
use of embedded block memories and DSP blocks. This architecture supports
direct CUDA compilation of integer computations to a binary which is executable
on the FPGA-based GPGPU. The benefits of our architecture are evaluated for a
collection of five standard CUDA benchmarks which are compiled using standard
GPGPU compilation tools. Speedups of 44x, on average, versus a MicroBlaze
microprocessor are achieved. We show dynamic energy savings versus a soft-core
processor of 80% on average. Application-customized versions of the soft GPGPU
can be used to further reduce dynamic energy consumption by an average of 14%."
"FPGAs are going mainstream. Major companies that were not traditionally
FPGA-focused are now seeking ways to exploit the benefits of reconfigurable
technology and provide it to their customers. In order to do so, a debug
ecosystem that provides for effective visibility into a working design and
quick debug turn-around times is essential. Overlays have the opportunity to
play a key role in this ecosystem. In this overview paper, we discuss how an
overlay fabric that allows the user to rapidly add debug instrumentation to a
design can be created and exploited. We discuss the requirements of such an
overlay and some of the research challenges and opportunities that need to be
addressed. To make our exposition concrete, we use two previously-published
examples of overlays that have been developed to implement debug
instrumentation."
"Coarse grained overlay architectures improve FPGA design productivity by
providing fast compilation and software-like programmability. Throughput
oriented spatially configurable overlays typically suffer from area overheads
due to the requirement of one functional unit for each compute kernel
operation. Hence, these overlays have often been of limited size, supporting
only relatively small compute kernels while consuming considerable FPGA
resources. This paper examines the possibility of sharing the functional units
among kernel operations for reducing area overheads. We propose a linear
interconnected array of time-multiplexed FUs as an overlay architecture with
reduced instruction storage and interconnect resource requirements, which uses
a fully-pipelined, architecture-aware FU design supporting a fast context
switching time. The results presented show a reduction of up to 85% in FPGA
resource requirements compared to existing throughput oriented overlay
architectures, with an operating frequency which approaches the theoretical
limit for the FPGA device."
"FPGA overlays are commonly implemented as coarse-grained reconfigurable
architectures with a goal to improve designers' productivity through balancing
flexibility and ease of configuration of the underlying fabric. To truly
facilitate full application acceleration, it is often necessary to also include
a highly efficient processor that integrates and collaborates with the
accelerators while maintaining the benefits of being implemented within the
same overlay framework. This paper presents an open-source soft processor that
is designed to tightly-couple with FPGA accelerators as part of an overlay
framework. RISC-V is chosen as the instruction set for its openness and
portability, and the soft processor is designed as a 4-stage pipeline to
balance resource consumption and performance when implemented on FPGAs. The
processor is generically implemented so as to promote design portability and
compatibility across different FPGA platforms. Experimental results show that
integrated software-hardware applications using the proposed tightly-coupled
architecture achieve comparable performance as hardware-only accelerators while
the proposed architecture provides additional run-time flexibility. The
processor has been synthesized to both low-end and high-performance FPGA
families from different vendors, achieving the highest frequency of 268.67MHz
and resource consumption comparable to existing RISC-V designs."
"FPMax implements four FPUs optimized for latency or throughput workloads in
two precisions, fabricated in 28nm UTBB FDSOI. Each unit's parameters, e.g
pipeline stages, booth encoding etc., were optimized to yield 1.42ns latency at
110GLOPS/W (SP) and 1.39ns latency at 36GFLOPS/W (DP). At 100% activity,
body-bias control improves the energy efficiency by about 20%; at 10% activity
this saving is almost 2x.
  Keywords: FPU, energy efficiency, hardware generator, SOI"
"Multi-core, Mixed Criticality Embedded (MCE) real-time systems require high
timing precision and predictability to guarantee there will be no interference
between tasks. These guarantees are necessary in application areas such as
avionics and automotive, where task interference or missed deadlines could be
catastrophic, and safety requirements are strict. In modern multi-core systems,
the interconnect becomes a potential point of uncertainty, introducing major
challenges in proving behaviour is always within specified constraints,
limiting the means of growing system performance to add more tasks, or provide
more computational resources to existing tasks.
  We present MCENoC, a Network-on-Chip (NoC) switching architecture that
provides innovations to overcome this with predictable, formally verifiable
timing behaviour that is consistent across the whole NoC. We show how the
fundamental properties of Benes networks benefit MCE applications and meet our
architecture requirements. Using SystemVerilog Assertions (SVA), formal
properties are defined that aid the refinement of the specification of the
design as well as enabling the implementation to be exhaustively formally
verified. We demonstrate the performance of the design in terms of size,
throughput and predictability, and discuss the application level considerations
needed to exploit this architecture."
"Convolutional neural networks (CNNs) are revolutionizing a variety of machine
learning tasks, but they present significant computational challenges.
Recently, FPGA-based accelerators have been proposed to improve the speed and
efficiency of CNNs. Current approaches construct a single processor that
computes the CNN layers one at a time; this single processor is optimized to
maximize the overall throughput at which the collection of layers are computed.
However, this approach leads to inefficient designs because the same processor
structure is used to compute CNN layers of radically varying dimensions.
  We present a new CNN accelerator paradigm and an accompanying automated
design methodology that partitions the available FPGA resources into multiple
processors, each of which is tailored for a different subset of the CNN
convolutional layers. Using the same FPGA resources as a single large
processor, multiple smaller specialized processors result in increased
computational efficiency and lead to a higher overall throughput. Our design
methodology achieves 1.51x higher throughput than the state of the art approach
on evaluating the popular AlexNet CNN on a Xilinx Virtex-7 FPGA. Our
projections indicate that the benefit of our approach increases with the amount
of available FPGA resources, already growing to over 3x over the state of the
art within the next generation of FPGAs."
"This report makes the case that a well-designed Reduced Instruction Set
Computer (RISC) can match, and even exceed, the performance and code density of
existing commercial Complex Instruction Set Computers (CISC) while maintaining
the simplicity and cost-effectiveness that underpins the original RISC goals.
  We begin by comparing the dynamic instruction counts and dynamic instruction
bytes fetched for the popular proprietary ARMv7, ARMv8, IA-32, and x86-64
Instruction Set Architectures (ISAs) against the free and open RISC-V RV64G and
RV64GC ISAs when running the SPEC CINT2006 benchmark suite. RISC-V was designed
as a very small ISA to support a wide range of implementations, and has a less
mature compiler toolchain. However, we observe that on SPEC CINT2006 RV64G
executes on average 16% more instructions than x86-64, 3% more instructions
than IA-32, 9% more instructions than ARMv8, but 4% fewer instructions than
ARMv7.
  CISC x86 implementations break up complex instructions into smaller internal
RISC-like micro-ops, and the RV64G instruction count is within 2% of the x86-64
retired micro-op count. RV64GC, the compressed variant of RV64G, is the densest
ISA studied, fetching 8% fewer dynamic instruction bytes than x86-64. We
observed that much of the increased RISC-V instruction count is due to a small
set of common multi-instruction idioms.
  Exploiting this fact, the RV64G and RV64GC effective instruction count can be
reduced by 5.4% on average by leveraging macro-op fusion. Combining the
compressed RISC-V ISA extension with macro-op fusion provides both the densest
ISA and the fewest dynamic operations retired per program, reducing the
motivation to add more instructions to the ISA. This approach retains a single
simple ISA suitable for both low-end and high-end implementations, where
high-end implementations can boost performance through microarchitectural
techniques."
"GPGPU applications exploit on-chip scratchpad memory available in the
Graphics Processing Units (GPUs) to improve performance. The amount of thread
level parallelism present in the GPU is limited by the number of resident
threads, which in turn depends on the availability of scratchpad memory in its
streaming multiprocessor (SM). Since the scratchpad memory is allocated at
thread block granularity, part of the memory may remain unutilized. In this
paper, we propose architectural and compiler optimizations to improve the
scratchpad utilization. Our approach, Scratchpad Sharing, addresses scratchpad
under-utilization by launching additional thread blocks in each SM. These
thread blocks use unutilized scratchpad and also share scratchpad with other
resident blocks. To improve the performance of scratchpad sharing, we propose
Owner Warp First (OWF) scheduling that schedules warps from the additional
thread blocks effectively. The performance of this approach, however, is
limited by the availability of the shared part of scratchpad.
  We propose compiler optimizations to improve the availability of shared
scratchpad. We describe a scratchpad allocation scheme that helps in allocating
scratchpad variables such that shared scratchpad is accessed for short
duration. We introduce a new instruction, relssp, that when executed, releases
the shared scratchpad. Finally, we describe an analysis for optimal placement
of relssp instructions such that shared scratchpad is released as early as
possible.
  We implemented the hardware changes using the GPGPU-Sim simulator and
implemented the compiler optimizations in Ocelot framework. We evaluated the
effectiveness of our approach on 19 kernels from 3 benchmarks suites: CUDA-SDK,
GPGPU-Sim, and Rodinia. The kernels that underutilize scratchpad memory show an
average improvement of 19% and maximum improvement of 92.17% compared to the
baseline approach."
"Approaching ideal wire latency using a network-on-chip (NoC) is an important
practical problem for many-core systems, particularly hundreds-cores. Although
other researchers have focused on optimizing large meshes, bypassing or
speculating router pipelines, or creating more intricate logarithmic
topologies, this paper proposes a balanced combination that trades queue
buffers for simplicity. Preliminary analysis of nine benchmarks from PARSEC and
SPLASH using execution-driven simulation shows that utilization rises from 2%
when connecting a single core per mesh port to at least 50%, as slack for delay
in concentrator and router queues is around 6x higher compared to the ideal
latency of just 20 cycles. That is, a 16-port mesh suffices because queueing is
the uncommon case for system performance. In this way, the mesh hop count is
bounded to three, as load becomes uniform via extended concentration, and ideal
latency is approached using conventional four-stage pipelines for the mesh
routers together with minor logarithmic edges. A realistic Uber is also
detailed, featuring the same performance as a 64-port mesh that employs
optimized router pipelines, improving the baseline by 12%. Ongoing work
develops techniques to better balance load by tuning the placement of cache
blocks, and compares Uber with bufferless routing."
"This paper presents a new early output hybrid input encoded asynchronous full
adder designed using dual-rail and 1-of-4 delay-insensitive data codes. The
proposed full adder when cascaded to form a ripple carry adder (RCA)
necessitates the use of a small relative-timing assumption with respect to the
internal carries, which is independent of the RCA size. The forward latency of
the proposed hybrid input encoded full adder based RCA is data-dependent while
its reverse latency is the least equaling the propagation delay of just one
full adder. Compared to the best of the existing hybrid input encoded full
adders based 32-bit RCAs, the proposed early output hybrid input encoded full
adder based 32-bit RCA enables respective reductions in forward latency and
area by 7.9% and 5.6% whilst dissipating the same average power; in terms of
the theoretically computed cycle time, the latter reports a 10.9% reduction
compared to the former."
"Memory consistency models (MCMs) which govern inter-module interactions in a
shared memory system, are a significant, yet often under-appreciated, aspect of
system design. MCMs are defined at the various layers of the hardware-software
stack, requiring thoroughly verified specifications, compilers, and
implementations at the interfaces between layers. Current verification
techniques evaluate segments of the system stack in isolation, such as proving
compiler mappings from a high-level language (HLL) to an ISA or proving
validity of a microarchitectural implementation of an ISA.
  This paper makes a case for full-stack MCM verification and provides a
toolflow, TriCheck, capable of verifying that the HLL, compiler, ISA, and
implementation collectively uphold MCM requirements. The work showcases
TriCheck's ability to evaluate a proposed ISA MCM in order to ensure that each
layer and each mapping is correct and complete. Specifically, we apply TriCheck
to the open source RISC-V ISA, seeking to verify accurate, efficient, and legal
compilations from C11. We uncover under-specifications and potential
inefficiencies in the current RISC-V ISA documentation and identify possible
solutions for each. As an example, we find that a RISC-V-compliant
microarchitecture allows 144 outcomes forbidden by C11 to be observed out of
1,701 litmus tests examined. Overall, this paper demonstrates the necessity of
full-stack verification for detecting MCM-related bugs in the hardware-software
stack."
"Endpoint devices for Internet-of-Things not only need to work under extremely
tight power envelope of a few milliwatts, but also need to be flexible in their
computing capabilities, from a few kOPS to GOPS. Near-threshold(NT) operation
can achieve higher energy efficiency, and the performance scalability can be
gained through parallelism. In this paper we describe the design of an
open-source RISC-V processor core specifically designed for NT operation in
tightly coupled multi-core clusters. We introduce instruction-extensions and
microarchitectural optimizations to increase the computational density and to
minimize the pressure towards the shared memory hierarchy. For typical
data-intensive sensor processing workloads the proposed core is on average 3.5x
faster and 3.2x more energy-efficient, thanks to a smart L0 buffer to reduce
cache access contentions and support for compressed instructions. SIMD
extensions, such as dot-products, and a built-in L0 storage further reduce the
shared memory accesses by 8x reducing contentions by 3.2x. With four
NT-optimized cores, the cluster is operational from 0.6V to 1.2V achieving a
peak efficiency of 67MOPS/mW in a low-cost 65nm bulk CMOS technology. In a low
power 28nm FDSOI process a peak efficiency of 193MOPS/mW(40MHz, 1mW) can be
achieved."
"This dissertation develops hardware that automatically reduces the effective
latency of accessing memory in both single-core and multi-core systems. To
accomplish this, the dissertation shows that all last level cache misses can be
separated into two categories: dependent cache misses and independent cache
misses. Independent cache misses have all of the source data that is required
to generate the address of the memory access available on-chip, while dependent
cache misses depend on data that is located off-chip. This dissertation
proposes that dependent cache misses are accelerated by migrating the
dependence chain that generates the address of the memory access to the memory
controller for execution. Independent cache misses are accelerated using a new
mode for runahead execution that only executes filtered dependence chains. With
these mechanisms, this dissertation demonstrates a 62% increase in performance
and a 19% decrease in effective memory access latency for a quad-core processor
on a set of high memory intensity workloads."
"In this paper, we have proposed a novel VLSI-oriented approach to computing
the rotation matrix entries from the quaternion coefficients. The advantage of
this approach is the complete elimination of multiplications and replacing them
by less costly squarings. Our approach uses Logan's identity, which proposes to
replace the calculation of the product of two numbers on summing the squares
via the Binomial Theorem. Replacing multiplications by squarings implies
reducing power consumption as well as decreases hardware circuit complexity."
"Development of large computerized systems requires both combinational and
sequential circuits. Registers and counters are two important examples of
sequential circuits, which are widely used in practical applications like CPUs.
The basic element of sequential logic is Flip-Flop, which stores an input value
and returns two outputs (Q and Q_bar). This paper presents an innovative
ternary D Flip-Flap-Flop, which offers circuit designers to customize their
design by eliminating one of the outputs if it is not required. This unique
feature of the new design leads to considerable power reduction in comparison
with the previously presented structures. The proposed design is simulated and
tested by HSPICE and 45 nm CMOS technology."
"In this paper I demonstrate a novel design for an optoelectronic State
Machine which replaces input/output forming logic found in conventional state
machines with BDD based optical logic while still using solid state memory in
the form of flip-flops in order to store states. This type of logic makes use
of waveguides and ring resonators to create binary switches. These switches in
turn can be used to create combinational logic which can be used as
input/output forming logic for a state machine. Replacing conventional
combinational logic with BDD based optical logic allows for a faster range of
state machines that can certainly outperform conventional state machines as
propagation delays within the logic described are in the order of picoseconds
as opposed to nanoseconds in digital logic."
"DRAM-based memory is a critical factor that creates a bottleneck on the
system performance since the processor speed largely outperforms the DRAM
latency. In this thesis, we develop a low-cost mechanism, called ChargeCache,
which enables faster access to recently-accessed rows in DRAM, with no
modifications to DRAM chips. Our mechanism is based on the key observation that
a recently-accessed row has more charge and thus the following access to the
same row can be performed faster. To exploit this observation, we propose to
track the addresses of recently-accessed rows in a table in the memory
controller. If a later DRAM request hits in that table, the memory controller
uses lower timing parameters, leading to reduced DRAM latency. Row addresses
are removed from the table after a specified duration to ensure rows that have
leaked too much charge are not accessed with lower latency. We evaluate
ChargeCache on a wide variety of workloads and show that it provides
significant performance and energy benefits for both single-core and multi-core
systems."
"Modern System-on-Chip (SoC) platforms typically consist of multiple
processors and a communication interconnect between them. Network-on-Chip (NoC)
arises as a solution to interconnect these systems, which provides a scalable,
reusable, and an efficient interconnect. For these SoC platforms, multicast
communication is significantly used for parallel applications. Cache coherency
in distributed sharedmemory,clock synchronization, replication, or barrier
synchronization are examples of these requests. This paper presents an overview
of research on NoC with support for multicast communication and delineates the
major issues addressed so far by the scientific community in this investigation
area."
"This paper describes the design of a 1024-core processor chip in 16nm FinFet
technology. The chip (""Epiphany-V"") contains an array of 1024 64-bit RISC
processors, 64MB of on-chip SRAM, three 136-bit wide mesh Networks-On-Chip, and
1024 programmable IO pins. The chip has taped out and is being manufactured by
TSMC.
  This research was developed with funding from the Defense Advanced Research
Projects Agency (DARPA). The views, opinions and/or findings expressed are
those of the author and should not be interpreted as representing the official
views or policies of the Department of Defense or the U.S. Government."
"Cycle-accurate software simulation of multicores with complex
microarchitectures is often excruciatingly slow. People use simplified core
models to gain simulation speed. However, a persistent question is to what
extent the results derived from a simplified core model can be used to
characterize the behavior of a real machine.
  We propose a new methodology of validating simplified simulation models,
which focuses on the trends of metric values across benchmarks and
architectures, instead of errors of absolute metric values. To illustrate this
methodology, we conduct a case study using an FPGA-accelerated cycle-accurate
full system simulator. We evaluated three cache replacement polices on a
10-stage in-order core model, and then re-conducted all the experiments by
substituting a 1-IPC core model for the 10-stage core model. We found that the
1-IPC core model generally produces qualitatively the same results as the
accurate core model except for a few mismatches. We argue that most observed
mismatches were either indistinguishable from experimental noise or
corresponded to the cases where the policy differences even in the accurate
model showed inconclusive results. We think it is fair to use simplified core
models to study a feature once the influence of the simplification is
understood. Additional studies on branch predictors and scaling properties of
multithread benchmarks reinforce our argument. However, the validation of a
simplified model requires a detailed cycle-accurate model!"
"This paper describes a multi-functional deep in-memory processor for
inference applications. Deep in-memory processing is achieved by embedding
pitch-matched low-SNR analog processing into a standard 6T 16KB SRAM array in
65 nm CMOS. Four applications are demonstrated. The prototype achieves up to
5.6X (9.7X estimated for multi-bank scenario) energy savings with negligible
(<1%) accuracy degradation in all four applications as compared to the
conventional architecture."
"Basic Linear Algebra Subprograms (BLAS) and Linear Algebra Package (LAPACK)
form basic building blocks for several High Performance Computing (HPC)
applications and hence dictate performance of the HPC applications. Performance
in such tuned packages is attained through tuning of several algorithmic and
architectural parameters such as number of parallel operations in the Directed
Acyclic Graph of the BLAS/LAPACK routines, sizes of the memories in the memory
hierarchy of the underlying platform, bandwidth of the memory, and structure of
the compute resources in the underlying platform. In this paper, we closely
investigate the impact of the Floating Point Unit (FPU) micro-architecture for
performance tuning of BLAS and LAPACK. We present theoretical analysis for
pipeline depth of different floating point operations like multiplier, adder,
square root, and divider followed by characterization of BLAS and LAPACK to
determine several parameters required in the theoretical framework for deciding
optimum pipeline depth of the floating operations. A simple design of a
Processing Element (PE) is presented and shown that the PE outperforms the most
recent custom realizations of BLAS and LAPACK by 1.1X to 1.5X in Gflops/W, and
1.9X to 2.1X in Gflops/mm^2."
"In existing systems, the off-chip memory interface allows the memory
controller to perform only read or write operations. Therefore, to perform any
operation, the processor must first read the source data and then write the
result back to memory after performing the operation. This approach consumes
high latency, bandwidth, and energy for operations that work on a large amount
of data. Several works have proposed techniques to process data near memory by
adding a small amount of compute logic closer to the main memory chips. In this
article, we describe two techniques proposed by recent works that take this
approach of processing in memory further by exploiting the underlying operation
of the main memory technology to perform more complex tasks. First, we describe
RowClone, a mechanism that exploits DRAM technology to perform bulk copy and
initialization operations completely inside main memory. We then describe a
complementary work that uses DRAM to perform bulk bitwise AND and OR operations
inside main memory. These two techniques significantly improve the performance
and energy efficiency of the respective operations."
"Variation has been shown to exist across the cells within a modern DRAM chip.
We empirically demonstrate a new form of variation that exists within a real
DRAM chip, induced by the design and placement of different components in the
DRAM chip. Our goals are to understand design-induced variation that exists in
real, state-of-the-art DRAM chips, exploit it to develop low-cost mechanisms
that can dynamically find and use the lowest latency at which to operate a DRAM
chip reliably, and, thus, improve overall system performance while ensuring
reliable system operation.
  To this end, we first experimentally demonstrate and analyze designed-induced
variation in modern DRAM devices by testing and characterizing 96 DIMMs (768
DRAM chips). Our characterization identifies DRAM regions that are vulnerable
to errors, if operated at lower latency, and finds consistency in their
locations across a given DRAM chip generation, due to design-induced variation.
Based on our extensive experimental analysis, we develop two mechanisms that
reliably reduce DRAM latency. First, DIVA Profiling uses runtime profiling to
dynamically identify the lowest DRAM latency that does not introduce failures.
DIVA Profiling exploits design-induced variation and periodically profiles only
the vulnerable regions to determine the lowest DRAM latency at low cost. Our
second mechanism, DIVA Shuffling, shuffles data such that values stored in
vulnerable regions are mapped to multiple error-correcting code (ECC)
codewords. Combined together, our two mechanisms reduce read/write latency by
40.0%/60.5%, which translates to an overall system performance improvement of
14.7%/13.7%/13.8% (in 2-/4-/8-core systems) across a variety of workloads,
while ensuring reliable operation."
"Compared to conventional general-purpose processors, accelerator-rich
architectures (ARAs) can provide orders-of-magnitude performance and energy
gains and are emerging as one of the most promising solutions in the age of
dark silicon. However, many design issues related to the complex interaction
between general-purpose cores, accelerators, customized on-chip interconnects,
and memory systems remain unclear and difficult to evaluate.
  In this paper we design and implement the ARAPrototyper to enable rapid
design space explorations for ARAs in real silicons and reduce the tedious
prototyping efforts far down to manageable efforts. First, ARAPrototyper
provides a reusable baseline prototype with a highly customizable memory
system, including interconnect between accelerators and buffers, interconnect
between buffers and last-level cache (LLC) or DRAM, coherency choice at LLC or
DRAM, and address translation support. Second, ARAPrototyper provides a clean
interface to quickly integrate users' own accelerators written in high-level
synthesis (HLS) code. The whole design flow is highly automated to generate a
prototype of ARA on an FPGA system-on-chip (SoC). Third, to quickly develop
applications that run seamlessly on the ARA prototype, ARAPrototyper provides a
system software stack, abstracts the accelerators as software libraries, and
provides APIs for software developers. Our experimental results demonstrate
that ARAPrototyper enables a wide range of design space explorations for ARAs
at manageable prototyping efforts, which has 4,000X to 10,000X faster
evaluation time than full-system simulations. We believe that ARAPrototyper can
be an attractive alternative for ARA design and evaluation."
"Convolutional neural networks (CNNs) have been widely employed in many
applications such as image classification, video analysis and speech
recognition. Being compute-intensive, CNN computations are mainly accelerated
by GPUs with high power dissipations. Recently, studies were carried out
exploiting FPGA as CNN accelerator because of its reconfigurability and energy
efficiency advantage over GPU, especially when OpenCL-based high-level
synthesis tools are now available providing fast verification and
implementation flows. Previous OpenCL-based design only focused on creating a
generic framework to identify performance-related hardware parameters, without
utilizing FPGA's special capability of pipelining kernel functions to minimize
memory bandwidth requirement. In this work, we propose an FPGA accelerator with
a new architecture of deeply pipelined OpenCL kernels. Data reuse and task
mapping techniques are also presented to improve design efficiency. The
proposed schemes are verified by implementing two representative large-scale
CNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar
peak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks
compared to previous work. Our design is openly accessible and thus can be
reused to explore new architectures for neural network accelerators."
"Hierarchical Temporal Memory (HTM) is a biomimetic machine learning algorithm
imbibing the structural and algorithmic properties of the neocortex. Two main
functional components of HTM that enable spatio-temporal processing are the
spatial pooler and temporal memory. In this research, we explore a scalable
hardware realization of the spatial pooler closely coupled with the
mathematical formulation of spatial pooler. This class of neuromorphic
algorithms are advantageous in solving a subset of the future engineering
problems by extracting nonintuitive patterns in complex data. The proposed
architecture, Non-volatile HTM (NVHTM), leverages large-scale solid state flash
memory to realize a optimal memory organization, area and power envelope. A
behavioral model of NVHTM is evaluated against the MNIST dataset, yielding
91.98% classification accuracy. A full custom layout is developed to validate
the design in a TSMC 180nm process. The area and power profile of the spatial
pooler are 30.538mm2 and 64.394mW, respectively. This design is a
proof-of-concept that storage processing is a viable platform for large scale
HTM network models."
"Throughout the world, the numbers of researchers or hardware designer
struggle for the reducing of power dissipation in low power VLSI systems. This
paper presented an idea of using the power gating structure for reducing the
sub threshold leakage in the reversible system. This concept presented in the
paper is entirely new and presented in the literature of reversible logics. By
using the reversible logics for the digital systems, the energy can be saved up
to the gate level implementation. But at the physical level designing of the
reversible logics by the modern CMOS technology the heat or energy is
dissipated due the sub-threshold leakage at the time of inactivity or standby
mode. The Reversible Programming logic array (RPLA) is one of the important
parts of the low power industrial applications and in this paper the physical
design of the RPLA is presented by using the sleep transistor and the results
is shown with the help of TINA- PRO software. The results for the proposed
design is also compare with the CMOS design and shown that of 40.8% of energy
saving. The Transient response is also produces in the paper for the switching
activity and showing that the proposed design is much better that the modern
CMOS design of the RPLA."
"The technique for hardware multiplication based upon Fourier transformation
has been introduced. The technique has the highest efficiency on multiplication
units with up to 8 bit range. Each multiplication unit is realized on base of
the minimized Boolean functions. Experimental data showed that this technique
the multiplication process speed up to 20% higher for 2-8 bit range of input
operands and up to 3% higher for 8-32 bit range of input operands than
analogues designed by Synopsys technique."
"Electronic circuits and systems used in mission and safety-critical
applications usually employ redundancy in the design to overcome arbitrary
fault(s) or failure(s) and guarantee the correct operation. In this context,
the distributed minority and majority voting based redundancy (DMMR) scheme
forms an efficient alternative to the conventional N-modular redundancy (NMR)
scheme for implementing mission and safety-critical circuits and systems by
significantly minimizing their weight and design cost and also their design
metrics whilst providing a similar degree of fault tolerance. This article
presents the first FPGAs based implementation of example DMMR circuits and
compares it with counterpart NMR circuits on the basis of area occupancy and
critical path delay viz. area-delay product (ADP). The example DMMR circuits
and counterpart NMR circuits are able to accommodate the faulty or failure
states of 2, 3 and 4 function modules. For physical synthesis, two commercial
Xilinx FPGAs viz. Spartan 3E and Virtex 5 corresponding to 90nm and 65nm CMOS
processes, and two radiation-tolerant and military grade Xilinx FPGAs viz. QPro
Virtex 2 and QPro Virtex E corresponding to 150nm and 180nm CMOS processes were
considered for the NMR and DMMR circuit realizations which employ the 4-by-4
array multiplier as a representative function module. To achieve a fault
tolerance of 2 function modules, both the DMMR and the NMR schemes provide near
similar mean ADPs across all the four FPGAs. But while achieving a fault
tolerance of 3 function modules the DMMR features reduced ADP by 44.5% on
average compared to the NMR, and in achieving a fault tolerance of 4 function
modules the DMMR reports reduced ADP by 56.5% on average compared to the NMR
with respect to all the four FPGAs considered."
"This paper proposes the architecture of partial sum generator for constituent
codes based polar code decoder. Constituent codes based polar code decoder has
the advantage of low latency. However, no purposefully designed partial sum
generator design exists that can yield desired timing for the decoder. We first
derive the mathematical presentation with the partial sums set $\bm{\beta^c}$
which is corresponding to each constituent codes. From this, we concoct a
shift-register based partial sum generator. Next, the overall architecture and
design details are described, and the overhead compared with conventional
partial sum generator is evaluated. Finally, the implementation results with
both ASIC and FPGA technology and relevant discussions are presented."
"Bitwise operations are an important component of modern day programming. Many
widely-used data structures (e.g., bitmap indices in databases) rely on fast
bitwise operations on large bit vectors to achieve high performance.
Unfortunately, in existing systems, regardless of the underlying architecture
(e.g., CPU, GPU, FPGA), the throughput of such bulk bitwise operations is
limited by the available memory bandwidth.
  We propose Buddy, a new mechanism that exploits the analog operation of DRAM
to perform bulk bitwise operations completely inside the DRAM chip. Buddy
consists of two components. First, simultaneous activation of three DRAM rows
that are connected to the same set of sense amplifiers enables us to perform
bitwise AND and OR operations. Second, the inverters present in each sense
amplifier enables us to perform bitwise NOT operations, with modest changes to
the DRAM array. These two components make Buddy functionally complete. Our
implementation of Buddy largely exploits the existing DRAM structure and
interface, and incurs low overhead (1% of DRAM chip area).
  Our evaluations based on SPICE simulations show that, across seven
commonly-used bitwise operations, Buddy provides between 10.9X---25.6X
improvement in raw throughput and 25.1X---59.5X reduction in energy
consumption. We evaluate three real-world data-intensive applications that
exploit bitwise operations: 1) bitmap indices, 2) BitWeaving, and 3)
bitvector-based implementation of sets. Our evaluations show that Buddy
significantly outperforms the state-of-the-art."
"This work studies the behavior of state-of-the-art memory controller designs
when executing scale-out workloads. It considers memory scheduling techniques,
memory page management policies, the number of memory channels, and the address
mapping scheme used. Experimental measurements demonstrate: 1)~Several recently
proposed memory scheduling policies are not a good match for these scale-out
workloads. 2)~The relatively simple First-Ready-First-Come-First-Served
(FR-FCFS) policy performs consistently better, and 3)~for most of the studied
workloads, the even simpler First-Come-First-Served scheduling policy is within
1\% of FR-FCFS. 4)~Increasing the number of memory channels offers negligible
performance benefits, e.g., performance improves by 1.7\% on average for
4-channels vs. 1-channel. 5)~77\%-90\% of DRAM rows activations are accessed
only once before closure. These observation can guide future development and
optimization of memory controllers for scale-out workloads."
"In this paper, we describe the design and implementation of a high precision
real time NAND simulator called Copycat that runs on a commodity multi-core
desktop environment. This NAND simulator facilitates the development of
embedded flash memory management software such as the flash translation layer
(FTL). The simulator also allows a comprehensive fault injection for testing
the reliability of the FTL. Compared against a real FPGA implementation, the
simulator's response time deviation is under 0.28% on average, with a maximum
of 10.12%."
"We present the design of a low-power 4-bit 1GS/s folding-flash ADC with a
folding factor of two. The design of a new unbalanced double-tail dynamic
comparator affords an ultra-low power operation and a high dynamic range.
Unlike the conventional approaches, this design uses a fully matched input
stage, an unbalanced latch stage, and a two-clock operation scheme. A
combination of these features yields significant reduction of the kick-back
noise, while allowing the design flexibility for adjusting the trip points of
the comparators. As a result, the ADC achieves SNDR of 22.3 dB at 100MHz and
21.8 dB at 500MHz (i.e. the Nyquist frequency). The maximum INL and DNL are
about 0.2 LSB. The converter consumes about 700uW from a 1-V supply yielding a
figure of merit of 65fJ/conversion step. These attributes make the proposed
folding-flash ADC attractive for the next-generation wireless applications."
"HADES is a fully automated verification tool for pipeline-based
microprocessors that aims at flaws caused by improperly handled data hazards.
It focuses on single-pipeline microprocessors designed at the register transfer
level (RTL) and deals with read-after-write, write-after-write, and
write-after-read hazards. HADES combines several techniques, including
data-flow analysis, error pattern matching, SMT solving, and abstract regular
model checking. It has been successfully tested on several microprocessors for
embedded applications."
"This paper starts with a comprehensive survey on RTL ATPG. It then proposes a
novel RTL ATPG model based on ""Gate Inherent Faults"" (GIF). These GIF are
extracted from each complex gate (adder, case-statement, etc.) of the RTL
source code individually. They are related to the internal logic paths of a
complex gate. They are not related to any net/signal in the RTL design. It is
observed, that when all GIF on RTL are covered (100%) and the same stimulus is
applied, then all gate level stuck-at faults of the netlist are covered (100%)
as well. The proposed RTL ATPG model is therefore synthesis independent. This
is shown on ITC'99 testcases. The applied semi-automatic test pattern
generation process is based on functional simulation."
"In the last decade we have witnessed a rapid growth in data center systems,
requiring new and highly complex networking devices. The need to refresh
networking infrastructure whenever new protocols or functions are introduced,
and the increasing costs that this entails, are of a concern to all data center
providers. New generations of Systems on Chip (SoC), integrating
microprocessors and higher bandwidth interfaces, are an emerging solution to
this problem. These devices permit entirely new systems and architectures that
can obviate the replacement of existing networking devices while enabling
seamless functionality change. In this work, we explore open source, RISC
based, SoC architectures with high performance networking capabilities. The
prototype architectures are implemented on the NetFPGA-SUME platform. Beyond
details of the architecture, we also describe the hardware implementation and
the porting of operating systems to the platform. The platform can be exploited
for the development of practical networking appliances, and we provide use case
examples."
"High energy particles from cosmic rays or packaging materials can generate a
glitch or a current transient (single event transient or SET) in a logic
circuit. This SET can eventually get captured in a register resulting in a flip
of the register content, which is known as soft error or single-event upset
(SEU). A soft error is typically modeled as a probabilistic single bit-flip
model. In developing such abstract fault models, an important issue to consider
is the likelihood of multiple bit errors caused by particle strikes. The fact
that an SET causes multiple flips is noted in the literature. We perform a
characterization study of the impact of an SET on a logic circuit to quantify
the extent to which an SET can cause multiple bit flips. We use post-layout
circuit simulations and Monte Carlo sampling scheme to get accurate bit-flip
statistics. We perform our simulations on ISCAS'85, ISCAS'89 and ITC'99
benchmarks in 180nm and 65nm technologies. We find that a substantial fraction
of SEU outcomes had multiple register flips. We futher analyse the individual
contributions of the strike on a register and the strike on a logic gate, to
multiple flips. We find that, amongst the erroneous outcomes, the probability
of multiple bit-flips for 'gate-strike' cases was substantial and went up to
50%, where as those for 'register-strike' cases was just about 2%. This implies
that, in principle, we can eliminate the flips due to register strikes using
hardened flip-flop designs. However, in such designs, out of the remaining
flips which will be due to gate strikes, a large fraction is likely to be
multiple flips."
"The increasing number of threads inside the cores of a multicore processor,
and competitive access to the shared cache memory, become the main reasons for
an increased number of competitive cache misses and performance decline.
Inevitably, the development of modern processor architectures leads to an
increased number of cache misses. In this paper, we make an attempt to
implement a technique for decreasing the number of competitive cache misses in
the first level of cache memory. This technique enables competitive access to
the entire cache memory when there is a hit - but, if there are cache misses,
memory data (by using replacement techniques) is put in a virtual part given to
threads, so that competitive cache misses are avoided. By using a simulator
tool, the results show a decrease in the number of cache misses and performance
increase for up to 15%. The conclusion that comes out of this research is that
cache misses are a real challenge for future processor designers, in order to
hide memory latency."
"L1 caches are critical to the performance of modern computer systems. Their
design involves a delicate balance between fast lookups, high hit rates, low
access energy, and simplicity of implementation. Unfortunately, constraints
imposed by virtual memory make it difficult to satisfy all these attributes
today. Specifically, the modern staple of supporting virtual-indexing and
physical-tagging (VIPT) for parallel TLB-L1 lookups means that L1 caches are
usually grown with greater associativity rather than sets. This compromises
performance -- by degrading access times without significantly boosting hit
rates -- and increases access energy. We propose VIPT Enhancements for
SuperPage Accesses or VESPA in response. VESPA side-steps the traditional
problems of VIPT by leveraging the increasing ubiquity of superpages; since
superpages have more page offset bits, they can accommodate L1 cache
organizations with more sets than baseline pages can. VESPA dynamically adapts
to any OS distribution of page sizes to operate L1 caches with good access
times, hit rates, and energy, for both single- and multi-threaded workloads.
Since the hardware changes are modest, and there are no OS or application
changes, VESPA is readily-implementable.
  By superpages (also called huge or large pages) we refer to any page sizes
supported by the architecture bigger than baseline page size."
"This paper describes HoLiSwap a method to reduce L1 cache wire energy, a
significant fraction of total cache energy, by swapping hot lines to the cache
way nearest to the processor. We observe that (i) a small fraction (<3%) of
cache lines (hot lines) serve over 60% of the L1 cache accesses and (ii) the
difference in wire energy between the nearest and farthest cache subarray can
be over 6$\times$. Our method exploits this difference in wire energy to
dynamically identify hot lines and swap them to the nearest physical way in a
set-associative L1 cache. This provides up to 44% improvement in the wire
energy (1.82% saving in overall system energy) with no impact on the cache miss
rate and 0.13% performance drop. We also show that HoLiSwap can simplify
way-prediction."
"This paper describes the design and implementation of an audio interface for
the Patmos processor, which runs on an Altera DE2-115 FPGA board. This board
has an audio codec included, the WM8731. The interface described in this work
allows to receive and send audio from and to the WM8731, and to synthesize,
store or manipulate audio signals writing C programs for Patmos. The audio
interface described in this paper is intended to be used with the Patmos
processor. Patmos is an open source RISC ISAs with a load-store architecture,
that is optimized for Real-Time Systems. Patmos is part of a project founded by
the European Union called T-CREST (Time-predictable Multi-Core Architecture for
Embedded Systems).[5] The structure of this project is integrated with the
Patmos project: new hardware modules have been added as IOs, which allow the
communication between the processor and the audio codec. These modules include
a clock generator for the audio chip, ADC and DAC modules for the audio
conversion from analog to digital and vice versa, and an I2C module which
allows setting configuration parameters on the audio codec. Moreover, a top
module has been created, which connects all the modules previously mentioned
between them, to Patmos and to the WM8731, using the external pins of the FPGA."
"Portable computing devices, which include tablets, smart phones and various
types of wearable sensors, experienced a rapid development in recent years. One
of the most critical limitations for these devices is the power consumption as
they use batteries as the power supply. However, the bottleneck of the power
saving schemes in both hardware design and software algorithm is the huge
variability in power consumption. The variability is caused by a myriad of
factors, including the manufacturing process, the ambient environment
(temperature, humidity), the aging effects and etc. As the technology node
scaled down to 28nm and even lower, the variability becomes more severe. As a
result, a platform for variability characterization seems to be very necessary
and helpful."
"To improve system performance, modern operating systems (OSes) often
undertake activities that require modification of virtual-to-physical page
translation mappings. For example, the OS may migrate data between physical
frames to defragment memory and enable superpages. The OS may migrate pages of
data between heterogeneous memory devices. We refer to all such activities as
page remappings. Unfortunately, page remappings are expensive. We show that
translation coherence is a major culprit and that systems employing
virtualization are especially badly affected by their overheads. In response,
we propose hardware translation invalidation and coherence or HATRIC, a readily
implementable hardware mechanism to piggyback translation coherence atop
existing cache coherence protocols. We perform detailed studies using KVM-based
virtualization, showing that HATRIC achieves up to 30% performance and 10%
energy benefits, for per-CPU area overheads of 2%. We also quantify HATRIC's
benefits on systems running Xen and find up to 33% performance improvements."
"In the context of embedded systems design, two important challenges are still
under investigation. First, improve real-time data processing,
reconfigurability, scalability, and self-adjusting capabilities of hardware
components. Second, reduce power consumption through low-power design
techniques as clock gating, logic gating, and dynamic partial reconfiguration
(DPR) capabilities. Today, several application, e.g., cryptography,
Software-defined radio or aerospace missions exploit the benefits of DPR of
programmable logic devices. The DPR allows well defined reconfigurable FPGA
region to be modified during runtime. However, it introduces an overhead in
term of power consumption and time during the reconfiguration phase. In this
paper, we present an investigation of power consumption overhead of the DPR
process using a high-speed digital oscilloscope and the shunt resistor method.
Results in terms of reconfiguration time and power consumption overhead for
Virtex 5 FPGAs are shown."
"Neuromorphic vision processor is an electronic implementation of vision
algorithm processor on semiconductor. To image the world, a low-power CMOS
image sensor array is required in the vision processor. The image sensor array
is typically formed through photo diodes and analog to digital converter (ADC).
To achieve low power acquisition, a low-power mid-resolution ADC is necessary.
In this paper, a 1.8V, 8-bit, 166MS/s pipelined ADC was proposed in a 0.18 um
CMOS technology. The ADC used operational amplifier sharing architecture to
reduce power consumption and achieved maximum DNL of 0.24 LSB, maximum INL of
0.35 LSB, at a power consumption of 38.9mW. When input frequency is 10.4MHz, it
achieved an SNDR 45.9dB, SFDR 50dB, and an ENOB of 7.33 bit."
"In this paper, a real-time 105-channel data acquisition platform based on
FPGA for imaging will be implemented for mm-wave imaging systems. PC platform
is also realized for imaging results monitoring purpose. Mm-wave imaging
expands our vision by letting us see things under poor visibility conditions.
With this extended vision ability, a wide range of military imaging missions
would benefit, such as surveillance, precision targeting, navigation, and
rescue. Based on the previously designed imager modules, this project would go
on finishing the PCB design (both schematic and layout) of the following signal
processing systems consisting of Programmable Gain Amplifier(PGA) (4 PGA for
each ADC) and 16-channel Analog to Digital Converter (ADC) (7 ADC in total).
Then the system verification would be performed on the Artix-7 35T Arty FPGA
with the developing of proper controlling code to configure the ADC and realize
the communication between the FPGA and the PC (through both UART and Ethernet).
For the verification part, a simple test on a breadboard with a simple analog
input (generated from a resistor divider) would first be performed. After the
PCB design is finished, the whole system would be tested again with a precise
reference and analog input."
"Polar codes are a new class of block codes with an explicit construction that
provably achieve the capacity of various communications channels, even with the
low-complexity successive-cancellation (SC) decoding algorithm. Yet, the more
complex successive-cancellation list (SCL) decoding algorithm is gathering more
attention lately as it significantly improves the error-correction performance
of short- to moderate-length polar codes, especially when they are concatenated
with a cyclic redundancy check code. However, as SCL decoding explores several
decoding paths, existing hardware implementations tend to be significantly
slower than SC-based decoders. In this paper, we show how the unrolling
technique, which has already been used in the context of SC decoding, can be
adapted to SCL decoding yielding a multi-Gbps SCL-based polar decoder with an
error-correction performance that is competitive when compared to an LDPC code
of similar length and rate. Post-place-and-route ASIC results for 28 nm CMOS
are provided showing that this decoder can sustain a throughput greater than 10
Gbps at 468 MHz with an energy efficiency of 7.25 pJ/bit."
"A decision feedback circuit with integrated offset compensation is presented
in this paper. The circuit is built around the sense amplifier comparator. The
feedback loop is closed around the first stage of the comparator resulting in
minimum loop latency. The feedback loop is implemented using a switched
capacitor network that picks from one of pre-computed voltages to be fed back.
The comparator's offset that is to be compensated for, is added in the same
path. Hence, an extra offset correction input is not required. The circuit is
used as a receiver for a 10 mm low swing interconnect implemented in UMC 130 nm
CMOS technology. The circuit is tested at a frequency of 1 GHz and it consumes
145 $\mu$A from a 1.2V supply at this frequency."
"In recent years, we have observed a clear trend in the rapid rise of
autonomous vehicles, robotics, virtual reality, and augmented reality. The core
technology enabling these applications, Simultaneous Localization And Mapping
(SLAM), imposes two main challenges: first, these workloads are computationally
intensive and they often have real-time requirements; second, these workloads
run on battery-powered mobile devices with limited energy budget. In short, the
essence of these challenges is that performance should be improved while
simultaneously reducing energy consumption, two rather contradicting goals by
conventional wisdom. In this paper, we take a close look at state-of-the-art
Simultaneous Localization And Mapping (SLAM) workloads, especially how these
workloads behave on mobile devices. Based on the results, we propose a mobile
architecture to improve SLAM performance on mobile devices."
"To avoid packet loss and deadlock scenarios that arise due to faults or power
gating in multicore and many-core systems, the network-on-chip needs to possess
resilient communication and load-balancing properties. In this work, we
introduce the Fashion router, a self-monitoring and self-reconfiguring design
that allows for the on-chip network to dynamically adapt to component failures.
First, we introduce a distributed intelligence unit, called Self-Awareness
Module (SAM), which allows the router to detect permanent component failures
and build a network connectivity map. Using local information, SAM adapts to
faults, guarantees connectivity and deadlock-free routing inside the maximal
connected subgraph and keeps routing tables up-to-date. Next, to reconfigure
network links or virtual channels around faulty/power-gated components, we add
bidirectional link and unified virtual channel structure features to the
Fashion router. This version of the router, named Ex-Fashion, further mitigates
the negative system performance impacts, leads to larger maximal connected
subgraph and sustains a relatively high degree of fault-tolerance. To support
the router, we develop a fault diagnosis and recovery algorithm executed by the
Built-In Self-Test, self-monitoring, and self-reconfiguration units at runtime
to provide fault-tolerant system functionalities. The Fashion router places no
restriction on topology, position or number of faults. It drops 54.3-55.4%
fewer nodes for same number of faults (between 30 and 60 faults) in an 8x8
2D-mesh over other state-of-the-art solutions. It is scalable and efficient.
The area overheads are 2.311% and 2.659% when implemented in 8x8 and 16x16
2D-meshes using the TSMC 65nm library at 1.38GHz clock frequency."
"The Physical Unclonable Function (PUF) is a promising hardware security
primitive because of its inherent uniqueness and low cost. To extract the
device-specific variation from delay-based strong PUFs, complex routing
constraints are imposed to achieve symmetric path delays; and systematic
variations can severely compromise the uniqueness of the PUF. In addition, the
metastability of the arbiter circuit of an Arbiter PUF can also degrade the
quality of the PUF due to the induced instability. In this paper we propose a
novel strong UNBIAS PUF that can be implemented purely by Register Transfer
Language (RTL), such as verilog, without imposing any physical design
constraints or delay characterization effort to solve the aforementioned
issues. Efficient inspection bit prediction models for unbiased response
extraction are proposed and validated. Our experimental results of the strong
UNBIAS PUF show 5.9% intra-Fractional Hamming Distance (FHD) and 45.1%
inter-FHD on 7 Field Programmable Gate Array (FPGA) boards without applying any
physical layout constraints or additional XOR gates. The UNBIAS PUF is also
scalable because no characterization cost is required for each challenge to
compensate the implementation bias. The averaged intra-FHD measured at worst
temperature and voltage variation conditions is 12%, which is still below the
margin of practical Error Correction Code (ECC) with error reduction techniques
for PUFs."
"Putting the DRAM on the same package with a processor enables several times
higher memory bandwidth than conventional off-package DRAM. Yet, the latency of
in-package DRAM is not appreciably lower than that of off-package DRAM. A
promising use of in-package DRAM is as a large cache. Unfortunately, most
previous DRAM cache designs mainly optimize for hit latency and do not consider
off-chip bandwidth efficiency as a first-class design constraint. Hence, as we
show in this paper, these designs are suboptimal for use with in-package DRAM.
  We propose a new DRAM cache design, Banshee, that optimizes for both in- and
off-package DRAM bandwidth efficiency without degrading access latency. The key
ideas are to eliminate the in-package DRAM bandwidth overheads due to costly
tag accesses through virtual memory mechanism and to incorporate a
bandwidth-aware frequency-based replacement policy that is biased to reduce
unnecessary traffic to off-package DRAM. Our extensive evaluation shows that
Banshee provides significant performance improvement and traffic reduction over
state-of-the-art latency-optimized DRAM cache designs."
"NAND flash-based Solid State Drives (SSDs), which are widely used from
embedded systems to enterprise servers, are enhancing performance by exploiting
the parallelism of NAND flash memories. To cope with the performance
improvement of SSDs, storage systems have rapidly adopted the host interface
for SSDs from Serial-ATA, which is used for existing hard disk drives, to
high-speed PCI express. Since NAND flash memory does not allow in-place
updates, it requires special software called Flash Translation Layer (FTL), and
SSDs are equipped with embedded processors to run FTL. Existing SSDs increase
the clock frequency of embedded processors or increase the number of embedded
processors in order to prevent FTL from acting as bottleneck of SSD
performance, but these approaches are not scalable. This paper proposes a
hardware-automated Flash Map Management Unit, called FMMU, that handles the
address translation process dominating the execution time of the FTL by
hardware automation. FMMU provides methods for exploiting the parallelism of
flash memory by processing outstanding requests in a non-blocking manner while
reducing the number of flash operations. The experimental results show that the
FMMU reduces the FTL execution time in the map cache hit case and the miss case
by 44% and 37%, respectively, compared with the existing software-based
approach operating in 4-core. FMMU also prevents FTL from acting as a
performance bottleneck for up to 32-channel, 8-way SSD using PCIe 3.0 x32 host
interface."
"High capacity and scalable memory systems play a vital role in enabling our
desktops, smartphones, and pervasive technologies like Internet of Things
(IoT). Unfortunately, memory systems are becoming increasingly prone to faults.
This is because we rely on technology scaling to improve memory density, and at
small feature sizes, memory cells tend to break easily. Today, memory
reliability is seen as the key impediment towards using high-density devices,
adopting new technologies, and even building the next Exascale supercomputer.
To ensure even a bare-minimum level of reliability, present-day solutions tend
to have high performance, power and area overheads. Ideally, we would like
memory systems to remain robust, scalable, and implementable while keeping the
overheads to a minimum. This dissertation describes how simple cross-layer
architectural techniques can provide orders of magnitude higher reliability and
enable seamless scalability for memory systems while incurring negligible
overheads."
"In this paper, we present a novel cache design based on Multi-Level Cell
Spin-Transfer Torque RAM (MLC STTRAM) that can dynamically adapt the set
capacity and associativity to use efficiently the full potential of MLC STTRAM.
We exploit the asymmetric nature of the MLC storage scheme to build cache lines
featuring heterogeneous performances, that is, half of the cache lines are
read-friendly, while the other is write-friendly. Furthermore, we propose to
opportunistically deactivate ways in underutilized sets to convert MLC to
Single-Level Cell (SLC) mode, which features overall better performance and
lifetime. Our ultimate goal is to build a cache architecture that combines the
capacity advantages of MLC and performance/energy advantages of SLC. Our
experiments show an improvement of 43% in total numbers of conflict misses, 27%
in memory access latency, 12% in system performance, and 26% in LLC access
energy, with a slight degradation in cache lifetime (about 7%) compared to an
SLC cache."
"Storage-class memory (SCM) combines the benefits of a solid-state memory,
such as high-performance and robustness, with the archival capabilities and low
cost of conventional hard-disk magnetic storage. Among candidate solid-state
nonvolatile memory technologies that could potentially be used to construct
SCM, flash memory is a well-established technology and have been widely used in
commercially available SCM incarnations. Flash-based SCM enables much better
tradeoffs between performance, space and power than disk-based systems.
However, write endurance is a significant challenge for a flash-based SCM (each
act of writing a bit may slightly damage a cell, so one flash cell can be
written 10^4--10^5 times, depending on the flash technology, before it becomes
unusable). This is a well-documented problem and has received a lot of
attention by manufactures that are using some combination of write reduction
and wear-leveling techniques for achieving longer lifetime. In an effort to
improve flash lifetime, first, by quantifying data longevity in an SCM, we show
that a majority of the data stored in a solid-state SCM do not require long
retention times provided by flash memory (i.e., up to 10 years in modern
devices); second, by exploiting retention time relaxation, we propose a novel
mechanism, called Dense-SLC (D-SLC), which enables us perform multiple writes
into a cell during each erase cycle for lifetime extension; and finally, we
discuss the required changes in the flash management software (FTL) in order to
use this characteristic for extending the lifetime of the solid-state part of
an SCM. Using an extensive simulation-based analysis of a flash-based SCM, we
demonstrate that D-SLC is able to significantly improve device lifetime
(between 5.1X and 8.6X) with no performance overhead and also very small
changes at the FTL software."
"This paper presents the designs of asynchronous early output dual-bit full
adders without and with redundant logic (implicit) corresponding to homogeneous
and heterogeneous delay-insensitive data encoding. For homogeneous
delay-insensitive data encoding only dual-rail i.e. 1-of-2 code is used, and
for heterogeneous delay-insensitive data encoding 1-of-2 and 1-of-4 codes are
used. The 4-phase return-to-zero protocol is used for handshaking. To
demonstrate the merits of the proposed dual-bit full adder designs, 32-bit
ripple carry adders (RCAs) are constructed comprising dual-bit full adders. The
proposed dual-bit full adders based 32-bit RCAs incorporating redundant logic
feature reduced latency and area compared to their non-redundant counterparts
with no accompanying power penalty. In comparison with the weakly indicating
32-bit RCA constructed using homogeneously encoded dual-bit full adders
containing redundant logic, the early output 32-bit RCA comprising the proposed
homogeneously encoded dual-bit full adders with redundant logic reports
corresponding reductions in latency and area by 22.2% and 15.1% with no
associated power penalty. On the other hand, the early output 32-bit RCA
constructed using the proposed heterogeneously encoded dual-bit full adder
which incorporates redundant logic reports respective decreases in latency and
area than the weakly indicating 32-bit RCA that consists of heterogeneously
encoded dual-bit full adders with redundant logic by 21.5% and 21.3% with nil
power overhead. The simulation results obtained are based on a 32/28nm CMOS
process technology."
"This paper provides modified Distributed Arithmetic based technique to
compute sum of products saving appreciable number of Multiply And accumulation
blocks and this consecutively reduces circuit size. In this technique
multiplexer based structure is used to reuse the blocks so as to reduce the
required memory locations. In this technique a Carry Look Ahead based adder
tree is used to have better area-delay product. Designing of FIR filter is done
using VHDL and synthesized using Xilinx 12.2 synthesis tool and ISIM simulator.
The power analysis is done using Xilinx Xpower analyzer. The proposed structure
requires nearly 42% less cells, 40% less LUT flip-flop pairs used, and also 2%
less power compared with existing structure."
"The 3rd International Workshop on Overlay Architectures for FPGAs (OLAF 2017)
was held on 22 Feb, 2017 as a co-located workshop at the 25th ACM/SIGDA
International Symposium on Field-Programmable Gate Arrays (FPGA 2017). This
year, the program committee selected 3 papers and 3 extended abstracts to be
presented at the workshop, which are subsequently collected in this online
volume."
"Floating point division, even though being an infrequent operation in the
traditional sense, is indis- pensable when it comes to a range of
non-traditional applications such as K-Means Clustering and QR Decomposition
just to name a few. In such applications, hardware support for floating point
division would boost the performance of the entire system. In this paper, we
present a novel architecture for a floating point division unit based on the
Taylor-series expansion algorithm. We show that the Iterative Logarithmic
Multiplier is very well suited to be used as a part of this architecture. We
propose an implementation of the powering unit that can calculate an odd power
and an even power of a number simultaneously, meanwhile having little hardware
overhead when compared to the Iterative Logarithmic Multiplier."
"Coarse-Grained Reconfigurable Arrays (CGRAs) enable ease of programmability
and result in low development costs. They enable the ease of use specifically
in reconfigurable computing applications. The smaller cost of compilation and
reduced reconfiguration overhead enables them to become attractive platforms
for accelerating high-performance computing applications such as image
processing. The CGRAs are ASICs and therefore, expensive to produce. However,
Field Programmable Gate Arrays (FPGAs) are relatively cheaper for low volume
products but they are not so easily programmable. We combine best of both
worlds by implementing a Virtual Coarse-Grained Reconfigurable Array (VCGRA) on
FPGA. VCGRAs are a trade off between FPGA with large routing overheads and
ASICs. In this perspective we present a novel heterogeneous Virtual
Coarse-Grained Reconfigurable Array (VCGRA) called ""Pixie"" which is suitable
for implementing high performance image processing applications. The proposed
VCGRA contains generic processing elements and virtual channels that are
described using the Hardware Description Language VHDL. Both elements have been
optimized by using the parameterized configuration tool flow and result in a
resource reduction of 24% for each processing elements and 82% for each virtual
channels respectively."
"For large circuits, static timing analysis (STA) needs to be performed in a
hierarchical manner to achieve higher performance in arrival time propagation.
In hierarchical STA, efficient and accurate timing models of sub-modules need
to be created. We propose a timing model extraction method that significantly
reduces the size of timing models without losing any accuracy by removing
redundant timing information. Circuit components which do not contribute to the
delay of any input to output pair are removed. The proposed method is
deterministic. Compared to the original models, the numbers of edges and
vertices of the resulting timing models are reduced by 84% and 85% on average,
respectively, which are significantly more than the results achieved by other
methods."
"FPGA vendors have recently started focusing on OpenCL for FPGAs because of
its ability to leverage the parallelism inherent to heterogeneous computing
platforms. OpenCL allows programs running on a host computer to launch
accelerator kernels which can be compiled at run-time for a specific
architecture, thus enabling portability. However, the prohibitive compilation
times (specifically the FPGA place and route times) are a major stumbling block
when using OpenCL tools from FPGA vendors. The long compilation times mean that
the tools cannot effectively use just-in-time (JIT) compilation or runtime
performance scaling. Coarse-grained overlays represent a possible solution by
virtue of their coarse granularity and fast compilation. In this paper, we
present a methodology for run-time compilation of OpenCL kernels to a DSP block
based coarse-grained overlay, rather than directly to the fine-grained FPGA
fabric. The proposed methodology allows JIT compilation and on-demand
resource-aware kernel replication to better utilize available overlay
resources, raising the abstraction level while reducing compile times
significantly. We further demonstrate that this approach can even be used for
run-time compilation of OpenCL kernels on the ARM processor of the embedded
heterogeneous Zynq device."
"Productivity issues such as lengthy compilation and limited code reuse have
restricted usage of field-programmable gate arrays (FPGAs), despite significant
technical advantages. Recent work into overlays -- virtual coarse-grained
architectures implemented atop FPGAs -- has aimed to address these concerns
through abstraction, but have mostly focused on pipelined applications with
minimal control requirements. Although research has introduced overlays for
finite-state machines, those architectures suffer from limited scalability and
flexibility, which we address with a new overlay architecture using memory
decomposition on transitional logic. Although our overlay provides modest
average improvements of 15% to 29% fewer lookup tables for individual
finite-state machines, for the more common usage of an overlay supporting
different finite-state machines, our overlay achieves a 77% to 99% reduction in
lookup tables. In addition, our overlay reduces compilation time to tenths of a
second to enable rapid iterative-development methodologies."
"We exploit floating-point DSPs in the Arria10 FPGA and multi-pumping feature
of the M20K RAMs to build a dataflow-driven soft processor fabric for large
graph workloads. In this paper, we introduce the idea of out-of-order node
scheduling across a large number of local nodes (thousands) per processor by
combining an efficient node tagging scheme along with leading-one detector
circuits. We use a static one-time node labeling algorithm to sort nodes based
on criticality to organize local memory inside each soft processor. This
translates to a small ~6% memory overhead. When compared to a memory-expensive
FIFO-based first-come-first-serve approach used in previous studies, we deliver
up to 50% performance improvement while eliminating the cost of the FIFOs. On
the Arria10 10AX115S board, we can create an overlay design of up to 300
processors connected by high bandwidth Hoplite NoC at frequencies up to 250MHz."
"Resource utilization is one of the emerging problems in many-chip SSDs. In
this paper, we propose Sprinkler, a novel device-level SSD controller, which
targets maximizing resource utilization and achieving high performance without
additional NAND flash chips. Specifically, Sprinkler relaxes parallelism
dependency by scheduling I/O requests based on internal resource layout rather
than the order imposed by the device-level queue. In addition, Sprinkler
improves flash-level parallelism and reduces the number of transactions (i.e.,
improves transactional-locality) by over-committing flash memory requests to
specific resources. Our extensive experimental evaluation using a
cycle-accurate large-scale SSD simulation framework shows that a many-chip SSD
equipped with our Sprinkler provides at least 56.6% shorter latency and 1.8 ~
2.2 times better throughput than the state-of-the-art SSD controllers. Further,
it improves overall resource utilization by 68.8% under different I/O request
patterns and provides, on average, 80.2% more flash-level parallelism by
reducing half of the flash memory requests at runtime."
"Statistical static timing analysis deals with the increasing variations in
manufacturing processes to reduce the pessimism in the worst case timing
analysis. Because of the correlation between delays of circuit components,
timing model generation and hierarchical timing analysis face more challenges
than in static timing analysis. In this paper, a novel method to generate
timing models for combinational circuits considering variations is proposed.
The resulting timing models have accurate input-output delays and are about 80%
smaller than the original circuits. Additionally, an accurate hierarchical
timing analysis method at design level using pre-characterized timing models is
proposed. This method incorporates the correlation between modules by replacing
independent random variables to improve timing accuracy. Experimental results
show that the correlation between modules strongly affects the delay
distribution of the hierarchical design and the proposed method has good
accuracy compared with Monte Carlo simulation, but is faster by three orders of
magnitude."
"As semiconductor devices continue to scale down, process vari- ations become
more relevant for circuit design. Facing such variations, statistical static
timing analysis is introduced to model variations more accurately so that the
pessimism in tra- ditional worst case timing analysis is reduced. Because all
de- lays are modeled using correlated random variables, most statis- tical
timing methods are much slower than corner based timing analysis. To speed up
statistical timing analysis, we propose a method to extract timing models for
flip-flop and latch based sequential circuits respectively. When such a circuit
is used as a module in a hierarchical design, the timing model instead of the
original circuit is used for timing analysis. The extracted timing models are
much smaller than the original circuits. Ex- periments show that using
extracted timing models accelerates timing verification by orders of magnitude
compared to previ- ous approaches using flat netlists directly. Accuracy is
main- tained, however, with the mean and standard deviation of the clock period
both showing usually less than 1% error compared to Monte Carlo simulation on a
number of benchmark circuits."
"Post-Silicon Tunable (PST) clock buffers are widely used in high performance
designs to counter process variations. By allowing delay compensation between
consecutive register stages, PST buffers can effectively improve the yield of
digital circuits. To date, the evaluation of manufacturing yield in the
presence of PST buffers is only possible using Monte Carlo simulation. In this
paper, we propose an alternative method based on graph transformations, which
is much faster, more than 1000 times, and computes a parametric minimum clock
period. It also identifies the gates which are most critical to the circuit
performance, therefore enabling a fast analysis-optimization flow."
"In this paper, we investigate the challenges to apply Statistical Static
Timing Analysis (SSTA) in hierarchical design flow, where modules supplied by
IP vendors are used to hide design details for IP protection and to reduce the
complexity of design and verification. For the three basic circuit types,
combinational, flip-flop-based and latch-controlled, we propose methods to
extract timing models which contain interfacing as well as compressed internal
constraints. Using these compact timing models the runtime of full-chip timing
analysis can be reduced, while circuit details from IP vendors are not exposed.
We also propose a method to reconstruct the correlation between modules during
full-chip timing analysis. This correlation can not be incorporated into timing
models because it depends on the layout of the corresponding modules in the
chip. In addition, we investigate how to apply the extracted timing models with
the reconstructed correlation to evaluate the performance of the complete
design. Experiments demonstrate that using the extracted timing models and
reconstructed correlation full-chip timing analysis can be several times faster
than applying the flattened circuit directly, while the accuracy of statistical
timing analysis is still well maintained."
"In this paper, we propose a post-processing framework which iteratively
refines the routing results from an existing PCB router by removing dense
meander segments. By swapping and detouring dense meander segments the proposed
method can effectively alleviate accumulating crosstalk noise, while respecting
pre-defined area constraints. Experimental results show more than 85% reduction
of the meander segments and hence the noise cost."
"Length-matching is an important technique to balance delays of bus signals in
high-performance PCB routing. Existing routers, however, may generate dense
meander segments with small distance. Signals propagating across these meander
segments exhibit a speedup effect due to crosstalks between the segments of the
same wire, thus leading to mismatch of arrival times even with the same
physical wire length. In this paper, we propose a post-processing method to
enlarge the width and the distance of meander segments and distribute them more
evenly on the board so that the crosstalks can be reduced. In the proposed
framework, we model the sharing combinations of available routing areas after
removing dense meander segments from the initial routing, as well as the
generation of relaxed meander segments and their groups in subareas.
Thereafter, this model is transformed into an ILP problem and solved
efficiently. Experimental results show that the proposed method can extend the
width and the distance of meander segments about two times even under very
tight area constraints, so that the crosstalks and thus the speedup effect can
be alleviated effectively in high-performance PCB designs."
"Length-matching is an important technique to bal- ance delays of bus signals
in high-performance PCB routing. Existing routers, however, may generate very
dense meander segments. Signals propagating along these meander segments
exhibit a speedup effect due to crosstalk between the segments of the same
wire, thus leading to mismatch of arrival times even under the same physical
wire length. In this paper, we present a post-processing method to enlarge the
width and the distance of meander segments and hence distribute them more
evenly on the board so that crosstalk can be reduced. In the proposed
framework, we model the sharing of available routing areas after removing dense
meander segments from the initial routing, as well as the generation of relaxed
meander segments and their groups for wire length compensation. This model is
transformed into an ILP problem and solved for a balanced distribution of wire
patterns. In addition, we adjust the locations of long wire segments according
to wire priorities to swap free spaces toward critical wires that need much
length compensation. To reduce the problem space of the ILP model, we also
introduce a progressive fixing technique so that wire patterns are grown
gradually from the edge of the routing toward the center area. Experimental
results show that the proposed method can expand meander segments significantly
even under very tight area constraints, so that the speedup effect can be
alleviated effectively in high- performance PCB designs."
"Post-silicon clock tuning elements are widely used in high-performance
designs to mitigate the effects of process variations and aging. Located on
clock paths to flip-flops, these tuning elements can be configured through the
scan chain so that clock skews to these flip-flops can be adjusted after man-
ufacturing. Owing to the delay compensation across consecutive register stages
enabled by the clock tuning elements, higher yield and enhanced robustness can
be achieved. These benefits are, nonetheless, attained by increasing die area
due to the inserted clock tuning elements. For balancing performance
improvement and area cost, an efficient timing analysis algorithm is needed to
evaluate the performance of such a circuit. So far this evaluation is only
possible by Monte Carlo simulation which is very timing- consuming. In this
paper, we propose an alternative method using graph transformation, which
computes a parametric minimum clock period and is more than 10 4 times faster
than Monte Carlo simulation while maintaining a good accuracy. This method also
identifies the gates that are critical to circuit performance, so that a fast
analysis-optimization flow becomes possible."
"At submicron manufacturing technology nodes process variations affect circuit
performance significantly. This trend leads to a large timing margin and thus
overdesign to maintain yield. To combat this pessimism, post-silicon clock
tuning buffers can be inserted into circuits to balance timing budgets of
critical paths with their neighbors. After manufacturing, these clock buffers
can be configured for each chip individually so that chips with timing failures
may be rescued to improve yield. In this paper, we propose a sampling-based
method to determine the proper locations of these buffers. The goal of this
buffer insertion is to reduce the number of buffers and their ranges, while
still maintaining a good yield improvement. Experimental results demonstrate
that our algorithm can achieve a significant yield improvement (up to 35%) with
only a small number of buffers."
"At nanometer manufacturing technology nodes, process variations significantly
affect circuit performance. To combat them, post- silicon clock tuning buffers
can be deployed to balance timing bud- gets of critical paths for each
individual chip after manufacturing. The challenge of this method is that path
delays should be mea- sured for each chip to configure the tuning buffers
properly. Current methods for this delay measurement rely on path-wise
frequency stepping. This strategy, however, requires too much time from ex-
pensive testers. In this paper, we propose an efficient delay test framework
(EffiTest) to solve the post-silicon testing problem by aligning path delays
using the already-existing tuning buffers in the circuit. In addition, we only
test representative paths and the delays of other paths are estimated by
statistical delay prediction. Exper- imental results demonstrate that the
proposed method can reduce the number of frequency stepping iterations by more
than 94% with only a slight yield loss."
"In static timing analysis, clock-to-q delays of flip-flops are considered as
constants. Setup times and hold times are characterized separately and also
used as constants. The characterized delays, setup times and hold times, are
ap- plied in timing analysis independently to verify the perfor- mance of
circuits. In reality, however, clock-to-q delays of flip-flops depend on both
setup and hold times. Instead of being constants, these delays change with
respect to different setup/hold time combinations. Consequently, the simple ab-
straction of setup/hold times and constant clock-to-q delays introduces
inaccuracy in timing analysis. In this paper, we propose a holistic method to
consider the relation between clock-to-q delays and setup/hold time
combinations with a piecewise linear model. The result is more accurate than
that of traditional timing analysis, and the incorporation of the
interdependency between clock-to-q delays, setup times and hold times may also
improve circuit performance."
"At submicron manufacturing technology nodes, pro- cess variations affect
circuit performance significantly. To counter these variations, engineers are
reserving more timing margin to maintain yield, leading to an unaffordable
overdesign. Most of these margins, however, are wasted after manufacturing,
because process variations cause only some chips to be really slow, while other
chips can easily meet given timing specifications. To reduce this pessimism, we
can reserve less timing margin and tune failed chips after manufacturing with
clock buffers to make them meet timing specifications. With this post-silicon
clock tuning, critical paths can be balanced with neighboring paths in each
chip specifically to counter the effect of process variations. Consequently,
chips with timing failures can be rescued and the yield can thus be improved.
This is specially useful in high- performance designs, e.g., high-end CPUs,
where clock binning makes chips with higher performance much more profitable.
In this paper, we propose a method to determine where to insert post-silicon
tuning buffers during the design phase to improve the overall profit with clock
binning. This method learns the buffer locations with a Sobol sequence
iteratively and reduces the buffer ranges afterwards with tuning concentration
and buffer grouping. Experimental results demonstrate that the proposed method
can achieve a profit improvement of about 14% on average and up to 26%, with
only a small number of tuning buffers inserted into the circuit."
"Existing solid state drive (SSD) simulators unfortunately lack hardware
and/or software architecture models. Consequently, they are far from capturing
the critical features of contemporary SSD devices. More importantly, while the
performance of modern systems that adopt SSDs can vary based on their numerous
internal design parameters and storage-level configurations, a full system
simulation with traditional SSD models often requires unreasonably long
runtimes and excessive computational resources. In this work, we propose
SimpleSSD, a highfidelity simulator that models all detailed characteristics of
hardware and software, while simplifying the nondescript features of storage
internals. In contrast to existing SSD simulators, SimpleSSD can easily be
integrated into publicly-available full system simulators. In addition, it can
accommodate a complete storage stack and evaluate the performance of SSDs along
with diverse memory technologies and microarchitectures. Thus, it facilitates
simulations that explore the full design space at different levels of system
abstraction."
"Future multiprocessor chips will integrate many different units, each
tailored to a specific computation. When designing such a system, the chip
architect must decide how to distribute limited system resources such as area,
power, and energy among the computational units. We extend MultiAmdahl, an
analytical optimization technique for resource allocation in heterogeneous
architectures, for energy optimality under a variety of constant system power
scenarios. We conclude that reduction in constant system power should be met by
reallocating resources from general-purpose computing to heterogeneous
accelerator-dominated computing, to keep the overall energy consumption at a
minimum. We extend this conclusion to offer an intuition regarding
energy-optimal resource allocation in data center computing."
"This work studies the influence of temperature on performance and scalability
of 3D Chip Multiprocessors (CMP) from Amdahl law perspective. We find that 3D
CMP may reach its thermal limit before reaching its maximum power. We show that
a high level of parallelism may lead to high peak temperatures even in small
scale 3D CMPs, thus limiting 3D CMP scalability and calling for different,
in-memory computing architectures."
"Power consumption, off-chip memory bandwidth, chip area and Network on Chip
(NoC) capacity are among main chip resources limiting the scalability of Chip
Multiprocessors (CMP). A closed form analytical solution for optimizing the CMP
cache hierarchy and optimally allocating area among hierarchy levels under such
constrained resources is developed. The optimization framework is extended by
incorporating the impact of data sharing on cache miss rate. An analytical
model for cache access time as a function of cache size is proposed and
verified using CACTI simulation."
"In this paper, new schemes for a squarer, multiplier and divider of complex
numbers are proposed. Traditional structural solutions for each of these
operations require the presence some number of general-purpose binary
multipliers. The advantage of our solutions is a removing of multiplications
through replacing them by less costly squarers. We use Logan's trick and
quarter square technique, which propose to replace the calculation of the
product of two real numbers by summing the squares. Replacing usual multipliers
on digital squares implies reducing power consumption as well as decreases
hardware circuit complexity. The squarer requiring less area and power as
compared to general-purpose multiplier, it is interesting to assess the use of
squarers to implementation of complex arithmetic."
"It remains a challenge to run Deep Learning in devices with stringent power
budget in the Internet-of-Things. This paper presents a low-power accelerator
for processing Deep Neural Networks in the embedded devices. The power
reduction is realized by avoiding multiplications of near-zero valued data. The
near-zero approximation and a dedicated Near-Zero Approximation Unit (NZAU) are
proposed to predict and skip the near-zero multiplications under certain
thresholds. Compared with skipping zero-valued computations, our design
achieves 1.92X and 1.51X further reduction of the total multiplications in
LeNet-5 and Alexnet respectively, with negligible lose of accuracy. In the
proposed accelerator, 256 multipliers are grouped into 16 independent
Processing Lanes (PL) to support up to 16 neuron activations simultaneously.
With the help of data pre-processing and buffering in each PL, multipliers can
be clock-gated in most of the time even the data is excessively streaming in.
Designed and simulated in UMC 65 nm process, the accelerator operating at 500
MHz is $>$ 4X faster than the mobile GPU Tegra K1 in processing the
fully-connected layer FC8 of Alexnet, while consuming 717X less energy."
"Sparse matrix multiplication is an important component of linear algebra
computations. In this paper, an architecture based on Content Addressable
Memory (CAM) and Resistive Content Addressable Memory (ReCAM) is proposed for
accelerating sparse matrix by sparse vector and matrix multiplication in CSR
format. Using functional simulation, we show that the proposed ReCAM-based
accelerator exhibits two orders of magnitude higher power efficiency as
compared to existing sparse matrix-vector multiplication implementations."
"The energy consumption of DRAM is a critical concern in modern computing
systems. Improvements in manufacturing process technology have allowed DRAM
vendors to lower the DRAM supply voltage conservatively, which reduces some of
the DRAM energy consumption. We would like to reduce the DRAM supply voltage
more aggressively, to further reduce energy. Aggressive supply voltage
reduction requires a thorough understanding of the effect voltage scaling has
on DRAM access latency and DRAM reliability.
  In this paper, we take a comprehensive approach to understanding and
exploiting the latency and reliability characteristics of modern DRAM when the
supply voltage is lowered below the nominal voltage level specified by DRAM
standards. Using an FPGA-based testing platform, we perform an experimental
study of 124 real DDR3L (low-voltage) DRAM chips manufactured recently by three
major DRAM vendors. We find that reducing the supply voltage below a certain
point introduces bit errors in the data, and we comprehensively characterize
the behavior of these errors. We discover that these errors can be avoided by
increasing the latency of three major DRAM operations (activation, restoration,
and precharge). We perform detailed DRAM circuit simulations to validate and
explain our experimental findings. We also characterize the various
relationships between reduced supply voltage and error locations, stored data
patterns, DRAM temperature, and data retention.
  Based on our observations, we propose a new DRAM energy reduction mechanism,
called Voltron. The key idea of Voltron is to use a performance model to
determine by how much we can reduce the supply voltage without introducing
errors and without exceeding a user-specified threshold for performance loss.
Voltron reduces the average system energy by 7.3% while limiting the average
system performance loss to only 1.8%, for a variety of workloads."
"Recent advances in neural networks (NNs) exhibit unprecedented success at
transforming large, unstructured data streams into compact higher-level
semantic information for tasks such as handwriting recognition, image
classification, and speech recognition. Ideally, systems would employ
near-sensor computation to execute these tasks at sensor endpoints to maximize
data reduction and minimize data movement. However, near- sensor computing
presents its own set of challenges such as operating power constraints, energy
budgets, and communication bandwidth capacities. In this paper, we propose a
stochastic- binary hybrid design which splits the computation between the
stochastic and binary domains for near-sensor NN applications. In addition, our
design uses a new stochastic adder and multiplier that are significantly more
accurate than existing adders and multipliers. We also show that retraining the
binary portion of the NN computation can compensate for precision losses
introduced by shorter stochastic bit-streams, allowing faster run times at
minimal accuracy losses. Our evaluation shows that our hybrid stochastic-binary
design can achieve 9.8x energy efficiency savings, and application-level
accuracies within 0.05% compared to conventional all-binary designs."
"Processing-in-memory (PIM) architectures have seen an increase in popularity
recently, as the high internal bandwidth available within 3D-stacked memory
provides greater incentive to move some computation into the logic layer of the
memory. To maintain program correctness, the portions of a program that are
executed in memory must remain coherent with the portions of the program that
continue to execute within the processor. Unfortunately, PIM architectures
cannot use traditional approaches to cache coherence due to the high off-chip
traffic consumed by coherence messages, which, as we illustrate in this work,
can undo the benefits of PIM execution for many data-intensive applications. We
propose LazyPIM, a new hardware cache coherence mechanism designed specifically
for PIM. Prior approaches for coherence in PIM are ill-suited to applications
that share a large amount of data between the processor and the PIM logic.
LazyPIM uses a combination of speculative cache coherence and compressed
coherence signatures to greatly reduce the overhead of keeping PIM coherent
with the processor, even when a large amount of sharing exists.We find that
LazyPIM improves average performance across a range of data-intensive PIM
applications by 19.6%, reduces off-chip traffic by 30.9%, and reduces energy
consumption by 18.0%, over the best prior approaches to PIM coherence."
"This whitepaper proposes the design and adoption of a new generation of
Tensor Processing Unit which has the performance of Google's TPU, yet performs
operations on wide precision data. The new generation TPU is made possible by
implementing arithmetic circuits which compute using a new general purpose,
fractional arithmetic based on the residue number system."
"Cameras are the defacto sensor. The growing demand for real-time and
low-power computer vision, coupled with trends towards high-efficiency
heterogeneous systems, has given rise to a wide range of image processing
acceleration techniques at the camera node and in the cloud. In this paper, we
characterize two novel camera systems that employ acceleration techniques to
push the extremes of energy and performance scaling, and explore the
computation-communication tradeoffs in their designs. The first case study is a
camera system designed to detect and authenticate individual faces, running
solely on energy harvested from RFID readers. We design a multi-accelerator SoC
design operating in the sub-mW range, and evaluate it with real-world workloads
to show performance and energy efficiency improvements over a general purpose
microprocessor. The second camera system is a 16-camera rig processing over 32
Gb/s of data to produce real-time 3D-360 degree virtual reality video. We
design a multi-FPGA processing pipeline that outperforms CPU and GPU
configurations by up to 10$\times$ in the computation time, producing panoramic
stereo video directly from the camera rig at 30 frames per second. We find that
an early data reduction step, either before complex processing or offloading,
is the most critical optimization for in-camera systems."
"Asynchronous circuits employing delay-insensitive codes for data
representation i.e. encoding and following a 4-phase return-to-zero protocol
for handshaking are generally robust. Depending upon whether a single
delay-insensitive code or multiple delay-insensitive code(s) are used for data
encoding, the encoding scheme is called homogeneous or heterogeneous
delay-insensitive data encoding. This article proposes a new latency optimized
early output asynchronous ripple carry adder (RCA) that utilizes single-bit
asynchronous full adders (SAFAs) and dual-bit asynchronous full adders (DAFAs)
which incorporate redundant logic and are based on the delay-insensitive
dual-rail code i.e. homogeneous data encoding, and follow a 4-phase
return-to-zero handshaking. Amongst various RCA, carry lookahead adder (CLA),
and carry select adder (CSLA) designs, which are based on homogeneous or
heterogeneous delay-insensitive data encodings which correspond to the
weak-indication or the early output timing model, the proposed early output
asynchronous RCA that incorporates SAFAs and DAFAs with redundant logic is
found to result in reduced latency for a dual-operand addition operation. In
particular, for a 32-bit asynchronous RCA, utilizing 15 stages of DAFAs and 2
stages of SAFAs leads to reduced latency. The theoretical worst-case latencies
of the different asynchronous adders were calculated by taking into account the
typical gate delays of a 32/28nm CMOS digital cell library, and a comparison is
made with their practical worst-case latencies estimated. The theoretical and
practical worst-case latencies show a close correlation...."
"This work addresses the challenge of allowing simultaneous and predictable
accesses to shared data on multi-core systems. We accomplish this by proposing
a predictable cache coherence protocol, which mandates the use of certain
invariants to ensure predictability. In particular, we enforce these invariants
by augmenting the classic modify-share-invalid (MSI) protocol with transient
coherence states, and minimal architectural changes. This allows us to derive
worst-case latency bounds on predictable MSI (PMSI) protocol. Our analysis
shows that while the arbitration latency scales linearly, the coherence latency
scales quadratically with the number of cores. We implement PMSI in gem5, and
execute SPLASH-2 and synthetic multi-threaded workloads. Our empirical results
show that our approach is always within the analytical worst-case latency
bounds, and that PMSI improves average-case performance by up to 4x over the
next best predictable alternative. PMSI has average slowdowns of 1.45x and
1.46x compared to conventional MSI and MESI protocols, respectively."
"NAND flash memory is ubiquitous in everyday life today because its capacity
has continuously increased and cost has continuously decreased over decades.
This positive growth is a result of two key trends: (1) effective process
technology scaling, and (2) multi-level (e.g., MLC, TLC) cell data coding.
Unfortunately, the reliability of raw data stored in flash memory has also
continued to become more difficult to ensure, because these two trends lead to
(1) fewer electrons in the flash memory cell (floating gate) to represent the
data and (2) larger cell-to-cell interference and disturbance effects. Without
mitigation, worsening reliability can reduce the lifetime of NAND flash memory.
As a result, flash memory controllers in solid-state drives (SSDs) have become
much more sophisticated: they incorporate many effective techniques to ensure
the correct interpretation of noisy data stored in flash memory cells.
  In this article, we review recent advances in SSD error characterization,
mitigation, and data recovery techniques for reliability and lifetime
improvement. We provide rigorous experimental data from state-of-the-art MLC
and TLC NAND flash devices on various types of flash memory errors, to motivate
the need for such techniques. Based on the understanding developed by the
experimental characterization, we describe several mitigation and recovery
techniques, including (1) cell-to-cell interference mitigation, (2) optimal
multi-level cell sensing, (3) error correction using state-of-the-art
algorithms and methods, and (4) data recovery when error correction fails. We
quantify the reliability improvement provided by each of these techniques.
Looking forward, we briefly discuss how flash memory and these techniques could
evolve into the future."
"Modern DRAM modules are often equipped with hardware error correction
capabilities, especially for DRAM deployed in large-scale data centers, as
process technology scaling has increased the susceptibility of these devices to
errors. To provide fast error detection and correction, error-correcting codes
(ECC) are placed on an additional DRAM chip in a DRAM module. This additional
chip expands the raw capacity of a DRAM module by 12.5%, but the applications
are unable to use any of this extra capacity, as it is used exclusively to
provide reliability for all data. In reality, there are a number of
applications that do not need such strong reliability for all their data
regions (e.g., some user batch jobs executing on a public cloud), and can
instead benefit from using additional DRAM capacity to store extra data. Our
goal in this work is to provide the additional capacity within an ECC DRAM
module to applications when they do not need the high reliability of error
correction.
  In this paper, we propose Capacity- and Reliability-Adaptive Memory (CREAM),
a hardware mechanism that adapts error correcting DRAM modules to offer
multiple levels of error protection, and provides the capacity saved from using
weaker protection to applications. For regions of memory that do not require
strong error correction, we either provide no ECC protection or provide error
detection using multibit parity. We evaluate several layouts for arranging the
data within ECC DRAM in these reduced-protection modes, taking into account the
various trade-offs exposed from exploiting the extra chip. Our experiments show
that the increased capacity provided by CREAM improves performance by 23.0% for
a memory caching workload, and by 37.3% for a commercial web search workload
executing production query traces. In addition, CREAM can increase bank-level
parallelism within DRAM, offering further performance improvements."
"Graph algorithms and techniques are increasingly being used in scientific and
commercial applications to express relations and explore large data sets.
Although conventional or commodity computer architectures, like CPU or GPU, can
compute fairly well dense graph algorithms, they are often inadequate in
processing large sparse graph applications. Memory access patterns, memory
bandwidth requirements and on-chip network communications in these applications
do not fit in the conventional program execution flow. In this work, we propose
and design a new architecture for fast processing of large graph applications.
To leverage the lack of the spatial and temporal localities in these
applications and to support scalable computational models, we design the
architecture around two key concepts. (1) The architecture is a multicore
processor of independently clocked processing elements. These elements
communicate in a self-timed manner and use handshaking to perform
synchronization, communication, and sequencing of operations. By being
asynchronous, the operating speed at each processing element is determined by
actual local latencies rather than global worst-case latencies. We create a
specialized ISA to support these operations. (2) The application compilation
and mapping process uses a graph clustering algorithm to optimize parallel
computing of graph operations and load balancing. Through the clustering
process, we make scalability an inherent property of the architecture where
task-to-element mapping can be done at the graph node level or at node cluster
level. A prototyped version of the architecture outperforms a comparable CPU by
10~20x across all benchmarks and provides 2~5x better power efficiency when
compared to a GPU."
"This paper presents a novel method to identify and insert redundant logic
into a combinational circuit to improve its fault tolerance without having to
replicate the entire circuit as is the case with conventional redundancy
techniques. In this context, it is discussed how to estimate the fault masking
capability of a combinational circuit using the truth-cum-fault enumeration
table, and then it is shown how to identify the logic that can introduced to
add redundancy into the original circuit without affecting its native
functionality and with the aim of improving its fault tolerance though this
would involve some trade-off in the design metrics. However, care should be
taken while introducing redundant logic since redundant logic insertion may
give rise to new internal nodes and faults on those may impact the fault
tolerance of the resulting circuit. The combinational circuit that is
considered and its redundant counterparts are all implemented in semi-custom
design style using a 32/28nm CMOS digital cell library and their respective
design metrics and fault tolerances are compared."
"In nanoelectronic circuit synthesis, the majority gate and the inverter form
the basic combinational logic primitives. This paper deduces the mathematical
formulae to estimate the logical masking capability of majority gates, which
are used extensively in nanoelectronic digital circuit synthesis. The
mathematical formulae derived to evaluate the logical masking capability of
majority gates holds well for minority gates, and a comparison with the logical
masking capability of conventional gates such as NOT, AND/NAND, OR/NOR, and
XOR/XNOR is provided. It is inferred from this research work that the logical
masking capability of majority/minority gates is similar to that of XOR/XNOR
gates, and with an increase of fan-in the logical masking capability of
majority/minority gates also increases."
"Existing scrubbing techniques for SEU mitigation on FPGAs do not guarantee an
error-free operation after SEU recovering if the affected configuration bits do
belong to feedback loops of the implemented circuits. In this paper, we a)
provide a netlist-based circuit analysis technique to distinguish so-called
critical configuration bits from essential bits in order to identify
configuration bits which will need also state-restoring actions after a
recovered SEU and which not. Furthermore, b) an alternative classification
approach using fault injection is developed in order to compare both
classification techniques. Moreover, c) we will propose a floorplanning
approach for reducing the effective number of scrubbed frames and d),
experimental results will give evidence that our optimization methodology not
only allows to detect errors earlier but also to minimize the
Mean-Time-To-Repair (MTTR) of a circuit considerably. In particular, we show
that by using our approach, the MTTR for datapath-intensive circuits can be
reduced by up to 48.5% in comparison to standard approaches."
"NCs are the natural evolution of PCs, ubiquitous computers everywhere. The
current vision of NCs requires two improbable developments: (1) inexpensive
high-bandwidth WAN links to the Internet, and (2) inexpensive centralized
servers. The large NC bandwidth requirements will force each home or office to
have a local server LAN attached to the NCs. These servers will be much less
expensive to purchase and manage than a centralized solution. Centralized staff
are expensive and unresponsive."
"Defines a vocabulary for scaleable systems: Geoplexes, Farms, Clones, RACS,
RAPS, clones, partitions, and packs and dicusses the design tradeoffs of using
clones, partitons, and packs."
"We describe a model that enables us to analyze the running time of an
algorithm in a computer with a memory hierarchy with limited associativity, in
terms of various cache parameters. Our model, an extension of Aggarwal and
Vitter's I/O model, enables us to establish useful relationships between the
cache complexity and the I/O complexity of computations. As a corollary, we
obtain cache-optimal algorithms for some fundamental problems like sorting,
FFT, and an important subclass of permutations in the single-level cache model.
We also show that ignoring associativity concerns could lead to inferior
performance, by analyzing the average-case cache behavior of mergesort. We
further extend our model to multiple levels of cache with limited associativity
and present optimal algorithms for matrix transpose and sorting. Our techniques
may be used for systematic exploitation of the memory hierarchy starting from
the algorithm design stage, and dealing with the hitherto unresolved problem of
limited associativity."
"""Grid"" computing has emerged as an important new field, distinguished from
conventional distributed computing by its focus on large-scale resource
sharing, innovative applications, and, in some cases, high-performance
orientation. In this article, we define this new field. First, we review the
""Grid problem,"" which we define as flexible, secure, coordinated resource
sharing among dynamic collections of individuals, institutions, and
resources-what we refer to as virtual organizations. In such settings, we
encounter unique authentication, authorization, resource access, resource
discovery, and other challenges. It is this class of problem that is addressed
by Grid technologies. Next, we present an extensible and open Grid
architecture, in which protocols, services, application programming interfaces,
and software development kits are categorized according to their roles in
enabling resource sharing. We describe requirements that we believe any such
mechanisms must satisfy, and we discuss the central role played by the
intergrid protocols that enable interoperability among different Grid systems.
Finally, we discuss how Grid technologies relate to other contemporary
technologies, including enterprise integration, application service provider,
storage service provider, and peer-to-peer computing. We maintain that Grid
concepts and technologies complement and have much to contribute to these other
approaches."
"Self-managing software has emerged as modern systems have become more
complex. Some of the distributed object systems may contain thousands of
objects deployed on tens or even hundreds hosts. Development and support of
such systems often costs a lot. To solve this issue the systems, which are
capable supporting multiple self-managing attributes, should be created. In the
paper, the Adaptive domain concept is introduced as an extension to the basic
domain concept to support a generic adaptation environment for building
distributed object systems with multiple self-managing attributes."
"For several years, MPI has been the de facto standard for writing parallel
applications. One of the most popular MPI implementations is MPICH. Its
successor, MPICH2, features a completely new design that provides more
performance and flexibility. To ensure portability, it has a hierarchical
structure based on which porting can be done at different levels. In this
paper, we present our experiences designing and implementing MPICH2 over
InfiniBand. Because of its high performance and open standard, InfiniBand is
gaining popularity in the area of high-performance computing. Our study focuses
on optimizing the performance of MPI-1 functions in MPICH2. One of our
objectives is to exploit Remote Direct Memory Access (RDMA) in Infiniband to
achieve high performance. We have based our design on the RDMA Channel
interface provided by MPICH2, which encapsulates architecture-dependent
communication functionalities into a very small set of functions. Starting with
a basic design, we apply different optimizations and also propose a
zero-copy-based design. We characterize the impact of our optimizations and
designs using microbenchmarks. We have also performed an application-level
evaluation using the NAS Parallel Benchmarks. Our optimized MPICH2
implementation achieves 7.6 $\mu$s latency and 857 MB/s bandwidth, which are
close to the raw performance of the underlying InfiniBand layer. Our study
shows that the RDMA Channel interface in MPICH2 provides a simple, yet
powerful, abstraction that enables implementations with high performance by
exploiting RDMA operations in InfiniBand. To the best of our knowledge, this is
the first high-performance design and implementation of MPICH2 on InfiniBand
using RDMA support."
"Field Programmable Gate Arrays (FPGAs) have recently been increasingly used
for highly-parallel processing of compute intensive tasks. This paper
introduces an FPGA hardware platform architecture that is PC-based, allows for
fast reconfiguration over the PCI bus, and retains a simple physical hardware
design. The design considerations are first discussed, then the resulting
system architecture designed is illustrated. Finally, experimental results on
the FPGA resources utilized for this design are presented."
"ScotGrid is a prototype regional computing centre formed as a collaboration
between the universities of Durham, Edinburgh and Glasgow as part of the UK's
national particle physics grid, GridPP. We outline the resources available at
the three core sites and our optimisation efforts for our user communities. We
discuss the work which has been conducted in extending the centre to embrace
new projects both from particle physics and new user communities and explain
our methodology for doing this."
"Modern generations of field-programmable gate arrays (FPGAs) allow for
partial reconfiguration. In an online context, where the sequence of modules to
be loaded on the FPGA is unknown beforehand, repeated insertion and deletion of
modules leads to progressive fragmentation of the available space, making
defragmentation an important issue. We address this problem by propose an
online and an offline component for the defragmentation of the available space.
We consider defragmenting the module layout on a reconfigurable device. This
corresponds to solving a two-dimensional strip packing problem. Problems of
this type are NP-hard in the strong sense, and previous algorithmic results are
rather limited. Based on a graph-theoretic characterization of feasible
packings, we develop a method that can solve two-dimensional defragmentation
instances of practical size to optimality. Our approach is validated for a set
of benchmark instances."
"Quantum computer versus quantum algorithm processor in CMOS are compared to
find (in parallel) all Hamiltonian cycles in a graph with m edges and n
vertices, each represented by k bits. A quantum computer uses quantum states
analogous to CMOS registers. With efficient initialization, number of CMOS
registers is proportional to (n-1)! Number of qubits in a quantum computer is
approximately proportional to kn+2mn in the approach below. Using CMOS, the
bits per register is about proportional to kn, which is less since bits can be
irreversibly reset. In either concept, number of gates, or operations to
identify Hamiltonian cycles is proportional to kmn. However, a quantum computer
needs an additional exponentially large number of operations to accomplish a
probabilistic readout. In contrast, CMOS is deterministic and readout is
comparable to ordinary memory."
"Proposed below is a reversible digital computer modeled after the natural
behavior of a quantum system. Using approaches usually reserved for idealized
quantum computers, the Reversible CAM, or State Vector Parallel (RSVP)
processor can easily find keywords in an unstructured database (that is, it can
solve a needle in a haystack problem). The RSVP processor efficiently solves a
SAT (Satisfiability of Boolean Formulae) problem; also it can aid in the
solution of a GP (Global Properties of Truth Table) problem. The power delay
product of the RSVP processor is exponentially lower than that of a standard
CAM programmed to perform similar operations."
"Presented below is an interesting type of associative memory called toggle
memory based on the concept of T flip flops, as opposed to D flip flops. Toggle
memory supports both reversible programming and charge recovery. Circuits
designed using the principles delineated below permit matchlines to charge and
discharge with near zero energy dissipation. The resulting lethargy is
compensated by the massive parallelism of associative memory. Simulation
indicates over 33x reduction in energy dissipation using a sinusoidal power
supply at 2 MHz, assuming realistic 50 nm MOSFET models."
"The Graphic Processing Unit (GPU) has evolved into a powerful and flexible
processor. The latest graphic processors provide fully programmable vertex and
pixel processing units that support vector operations up to single
floating-point precision. This computational power is now being used for
general-purpose computations. However, some applications require higher
precision than single precision. This paper describes the emulation of a 44-bit
floating-point number format and its corresponding operations. An
implementation is presented along with performance and accuracy results."
"In modern transistor based logic gates, the impact of noise on computation
has become increasingly relevant since the voltage scaling strategy, aimed at
decreasing the dissipated power, has increased the probability of error due to
the reduced switching threshold voltages. In this paper we discuss the role of
noise in a two state model that mimic the dynamics of standard logic gates and
show that the presence of the noise sets a fundamental limit to the computing
speed. An optimal idle time interval that minimizes the error probability, is
derived."
"We implement Kolmogorov-Uspensky machine on a plasmodium of true slime mold
{\em Physarum polycephalum}. We provide experimental findings on realization of
the machine instructions, illustrate basic operations, and elements of
programming."
"In this paper we suggest the use of light for performing useful computations.
Namely, we propose a special device which uses light rays for solving the
Hamiltonian path problem on a directed graph. The device has a graph-like
representation and the light is traversing it following the routes given by the
connections between nodes. In each node the rays are uniquely marked so that
they can be easily identified. At the destination node we will search only for
particular rays that have passed only once through each node. We show that the
proposed device can solve small and medium instances of the problem in
reasonable time."
"In this paper we propose a special computational device which uses light rays
for solving the Hamiltonian path problem on a directed graph. The device has a
graph-like representation and the light is traversing it by following the
routes given by the connections between nodes. In each node the rays are
uniquely marked so that they can be easily identified. At the destination node
we will search only for particular rays that have passed only once through each
node. We show that the proposed device can solve small and medium instances of
the problem in reasonable time."
"We suggest a new optical solution for solving the YES/NO version of the Exact
Cover problem by using the massive parallelism of light. The idea is to build
an optical device which can generate all possible solutions of the problem and
then to pick the correct one. In our case the device has a graph-like
representation and the light is traversing it by following the routes given by
the connections between nodes. The nodes are connected by arcs in a special way
which lets us to generate all possible covers (exact or not) of the given set.
For selecting the correct solution we assign to each item, from the set to be
covered, a special integer number. These numbers will actually represent delays
induced to light when it passes through arcs. The solution is represented as a
subray arriving at a certain moment in the destination node. This will tell us
if an exact cover does exist or not."
"Detailed modeling of processors and high performance cycle-accurate
simulators are essential for today's hardware and software design. These
problems are challenging enough by themselves and have seen many previous
research efforts. Addressing both simultaneously is even more challenging, with
many existing approaches focusing on one over another. In this paper, we
propose the Reduced Colored Petri Net (RCPN) model that has two advantages:
first, it offers a very simple and intuitive way of modeling pipelined
processors; second, it can generate high performance cycle-accurate simulators.
RCPN benefits from all the useful features of Colored Petri Nets without
suffering from their exponential growth in complexity. RCPN processor models
are very intuitive since they are a mirror image of the processor pipeline
block diagram. Furthermore, in our experiments on the generated cycle-accurate
simulators for XScale and StrongArm processor models, we achieved an order of
magnitude (~15 times) speedup over the popular SimpleScalar ARM simulator."
"Conventional cache models are not suited for real-time parallel processing
because tasks may flush each other's data out of the cache in an unpredictable
manner. In this way the system is not compositional so the overall performance
is difficult to predict and the integration of new tasks expensive. This paper
proposes a new method that imposes compositionality to the system?s performance
and makes different memory hierarchy optimizations possible for multimedia
communicating tasks when running on embedded multiprocessor architectures. The
method is based on a cache allocation strategy that assigns sets of the unified
cache exclusively to tasks and to the communication buffers. We also
analytically formulate the problem and describe a method to compute the cache
partitioning ratio for optimizing the throughput and the consumed power. When
applied to a multiprocessor with memory hierarchy our technique delivers also
performance gain. Compared to the shared cache case, for an application
consisting of two jpeg decoders and one edge detection algorithm 5 times less
misses are experienced and for an mpeg2 decoder 6.5 times less misses are
experienced."
"We present our experience of designing a single-chip controller for advanced
digital still camera from specification all the way to mass production. The
process involves collaboration with camera system designer, IP vendors, EDA
vendors, silicon wafer foundry, package and testing houses, and camera maker.
We also co-work with academic research groups to develop a JPEG codec IP and
memory BIST and SOC testing methodology. In this presentation, we cover the
problems encountered, our solutions, and lessons learned."
"Pipelining is a well understood and often used implementation technique for
increasing the performance of a hardware system. We develop several SystemC/C++
modeling techniques that allow us to quickly model, simulate, and evaluate
pipelines. We employ a small domain specific language (DSL) based on resource
usage patterns that automates the drudgery of boilerplate code needed to
configure connectivity in simulation models. The DSL is embedded directly in
the host modeling language SystemC/C++. Additionally we develop several
techniques for parameterizing a pipeline's behavior based on policies of
function, communication, and timing (performance modeling)."
"-Residue Number System (RNS) is a valuable tool for fast and parallel
arithmetic. It has a wide application in digital signal processing, fault
tolerant systems, etc. In this work, we introduce the 3-moduli set {2^n,
2^{2n}-1, 2^{2n}+1} and propose its"
"The growing amount of XML encoded data exchanged over the Internet increases
the importance of XML based publish-subscribe (pub-sub) and content based
routing systems. The input in such systems typically consists of a stream of
XML documents and a set of user subscriptions expressed as XML queries. The
pub-sub system then filters the published documents and passes them to the
subscribers. Pub-sub systems are characterized by very high input ratios,
therefore the processing time is critical. In this paper we propose a ""pure
hardware"" based solution, which utilizes XPath query blocks on FPGA to solve
the filtering problem. By utilizing the high throughput that an FPGA provides
for parallel processing, our approach achieves drastically better throughput
than the existing software or mixed (hardware/software) architectures. The
XPath queries (subscriptions) are translated to regular expressions which are
then mapped to FPGA devices. By introducing stacks within the FPGA we are able
to express and process a wide range of path queries very efficiently, on a
scalable environment. Moreover, the fact that the parser and the filter
processing are performed on the same FPGA chip, eliminates expensive
communication costs (that a multi-core system would need) thus enabling very
fast and efficient pipelining. Our experimental evaluation reveals more than
one order of magnitude improvement compared to traditional pub/sub systems."
"DDR SDRAM is similar in function to the regular SDRAM but doubles the
bandwidth of the memory by transferring data on both edges of the clock cycles.
DDR SDRAM most commonly used in various embedded application like networking,
image or video processing, Laptops ete. Now a days many applications needs more
and more cheap and fast memory. Especially in the field of signal processing,
requires significant amount of memory. The most used type of dynamic memory for
that purpose is DDR SDRAM. For FPGA design the IC manufacturers are providing
commercial memory controller IP cores working only on their products. Main
disadvantage is the lack of memory access optimization for random memory access
patterns. The data path part of those controllers can be used free of charge.
This work propose an architecture of a DDR SDRAM controller, which takes
advantage of those available and well tested data paths and can be used for any
FPGA device or ASIC design.(5). In most of the SOC design, DDR SDRAM is
commonly used. ARM processor is widely used in SOCs; so that we focused to
implement AHB compatible DDR SDRAM controller suitable for ARM based SOC
design."
"This article is about the architecture of a lossless wavelet filter bank with
reprogrammable logic. It is based on second generation of wavelets with a
reduced of number of operations. A new basic structure for parallel
architecture and modules to forward and backward integer discrete wavelet
transform is proposed."
"Reversible logic has promising applications in emerging nanotechnologies,
such as quantum computing, quantum dot cellular automata and optical computing,
etc. Faults in reversible logic circuits that result in multi-bit error at the
outputs are very tough to detect, and thus in literature, researchers have only
addressed the problem of online testing of faults that result single-bit error
at the outputs based on parity preserving logic. In this work, we propose a
methodology for the concurrent error detection in reversible logic circuits to
detect faults that can result in multi-bit error at the outputs. The
methodology is based on the inverse property of reversible logic and is termed
as 'inverse and compare' method. By using the inverse property of reversible
logic, all the inputs can be regenerated at the outputs. Thus, by comparing the
original inputs with the regenerated inputs, the faults in reversible circuits
can be detected. Minimizing the garbage outputs is one of the main goals in
reversible logic design and synthesis. We show that the proposed methodology
results in 'garbageless' reversible circuits. A design of reversible full adder
that can be concurrently tested for multi-bit error at the outputs is
illustrated as the application of the proposed scheme. Finally, we showed the
application of the proposed scheme of concurrent error detection towards fault
detection in quantum dot cellular automata (QCA) emerging nanotechnology."
"Memory trace analysis is an important technology for architecture research,
system software (i.e., OS, compiler) optimization, and application performance
improvements. Hardware-snooping is an effective and efficient approach to
monitor and collect memory traces. Compared with software-based approaches,
memory traces collected by hardware-based approaches are usually lack of
semantic information, such as process/function/loop identifiers, virtual
address and I/O access. In this paper we propose a hybrid hardware/software
mechanism which is able to collect memory reference trace as well as semantic
information. Based on this mechanism, we designed and implemented a prototype
system called HMTT (Hybrid Memory Trace Tool) which adopts a DIMMsnooping
mechanism to snoop on memory bus and a software-controlled tracing mechanism to
inject semantic information into normal memory trace. To the best of our
knowledge, the HMTT system is the first hardware tracing system capable of
correlating memory trace with high-level events. Comprehensive validations and
evaluations show that the HMTT system has both hardware's (e.g., no distortion
or pollution) and software's advantages (e.g., flexibility and more
information)."
"This paper presents an efficient architecture for various image filtering
algorithms and tumor characterization using Xilinx System Generator (XSG). This
architecture offers an alternative through a graphical user interface that
combines MATLAB, Simulink and XSG and explores important aspects concerned to
hardware implementation. Performance of this architecture implemented in
SPARTAN-3E Starter kit (XC3S500E-FG320) exceeds those of similar or greater
resources architectures. The proposed architecture reduces the resources
available on target device by 50%."
"One of the most demanding challenges for the designers of parallel computing
architectures is to deliver an efficient network infrastructure providing low
latency, high bandwidth communications while preserving scalability. Besides
off-chip communications between processors, recent multi-tile (i.e. multi-core)
architectures face the challenge for an efficient on-chip interconnection
network between processor's tiles. In this paper, we present a configurable and
scalable architecture, based on our Distributed Network Processor (DNP) IP
Library, targeting systems ranging from single MPSoCs to massive HPC platforms.
The DNP provides inter-tile services for both on-chip and off-chip
communications with a uniform RDMA style API, over a multi-dimensional direct
network with a (possibly) hybrid topology."
"According to the increasing complexity of network application and internet
traffic, network processor as a subset of embedded processors have to process
more computation intensive tasks. By scaling down the feature size and emersion
of chip multiprocessors (CMP) that are usually multi-thread processors, the
performance requirements are somehow guaranteed. As multithread processors are
the heir of uni-thread processors and there isn't any general design flow to
design a multithread embedded processor, in this paper we perform a
comprehensive design space exploration for an optimum uni-thread embedded
processor based on the limited area and power budgets. Finally we run multiple
threads on this architecture to find out the maximum thread level parallelism
(TLP) based on performance per power and area optimum uni-thread architecture."
"Chip multiprocessors (CMPs) are ubiquitous in most of today's computing
fields. Although they provide noticeable benefits in terms of performance, cost
and power efficiency, they also introduce some new issues. In this paper we
analyze how the interference from Virtual Private Servers running in other
cores is a significant component of performance unpredictability and can
threaten the attainment of cloud computing. Even if virtualization is used, the
sharing of the on-chip section of the memory hierarchy by different cores makes
performance isolation strongly dependent on what is running elsewhere in the
system. We will show in three actual computing systems, based on Sun UltraSparc
T1, Sun UltraSparc T2 and Intel Xeon processors, how state-of-the-art
virtualization techniques are unable to guarantee performance isolation in a
representative workload such as SPECweb2005. In an especially conceived near
worst-case scenario, it is possible to reduce the performance achieved by a
Solaris Zones consolidated server for this suite of benchmarks in a Sun Fire
T1000 and a Sun Enterprise T5120 by up to 80%. The performance drop observed by
a Xen consolidated server running in a HP Proliant DL160 G5 is almost 45%. For
all systems under study, off-chip bandwidth is shown to be the most critical
resource."
"Reversible Logic is gaining significant consideration as the potential logic
design style for implementation in modern nanotechnology and quantum computing
with minimal impact on physical entropy .Fault Tolerant reversible logic is one
class of reversible logic that maintain the parity of the input and the
outputs. Significant contributions have been made in the literature towards the
design of fault tolerant reversible logic gate structures and arithmetic units,
however, there are not many efforts directed towards the design of fault
tolerant reversible ALUs. Arithmetic Logic Unit (ALU) is the prime performing
unit in any computing device and it has to be made fault tolerant. In this
paper we aim to design one such fault tolerant reversible ALU that is
constructed using parity preserving reversible logic gates. The designed ALU
can generate up to seven Arithmetic operations and four logical operations."
"Video decoding is considered as one of the most compute and energy intensive
application in energy constrained mobile devices. Some specific processing
units, such as DSPs, are added to those devices in order to optimize the
performance and the energy consumption. However, in DSP video decoding, the
inter-processor communication overhead may have a considerable impact on the
performance and the energy consumption. In this paper, we propose to evaluate
this overhead and analyse its impact on the performance and the energy
consumption as compared to the GPP decoding. Our work revealed that the GPP can
be the best choice in many cases due to the a significant overhead in DSP
decoding which may represents 30% of the total decoding energy."
"Cryptographic algorithms are computationally costly and the challenge is more
if we need to execute them in resource constrained embedded systems. Field
Programmable Gate Arrays (FPGAs) having programmable logic de- vices and
processing cores, have proven to be highly feasible implementation platforms
for embedded systems providing lesser design time and reconfig- urability.
Design parameters like throughput, resource utilization and power requirements
are the key issues. The popular Elliptic Curve Cryptography (ECC), which is
superior over other public-key crypto-systems like RSA in many ways, such as
providing greater security for a smaller key size, is cho- sen in this work and
the possibilities of its implementation in FPGA based embedded systems for both
single and dual processor core architectures in- volving task parallelization
have been explored. This exploration, which is first of its kind considering
the other existing works, is a needed activity for evaluating the best possible
architectural environment for ECC implementa- tion on FPGA (Virtex4 XC4VFX12,
FF668, -10) based embedded platform."
"In order to protect the security of network data, a high speed chip module
for encrypting and decrypting of network data packet is designed. The chip
module is oriented for internet information security SOC (System on Chip)
design. During the design process, AES (Advanced Encryption Standard) and 3DES
(Data Encryption Standard) encryption algorithm are adopted to protect the
security of network data. The following points are focused: (1) The SOC (System
on Chip) design methodology based on IP (Intellectual Property) core is used.
AES (Advanced Encryption Standard) and 3DES (Data Encryption Standard) IP
(Intellectual Property) cores are embedded in the chip module, peripheral
control sub-modules are designed to control the encryption-decryption module,
which is capable of shortening the design period of the chip module. (2) The
implementation of encryption-decryption with hardware was presented, which
improves the safety of data through the encryption-decryption chip and reduce
the load of CPU. (3) In our hardware solution, two AES (Advanced Encryption
Standard) cores are used to work in parallel, which improves the speed of the
encryption module. Moreover, the key length of AES (Advanced Encryption
Standard) encryption algorithm is designed with three optional configurations
at 128 bits, 256 bits and 192 bits respectively and six optional encryption
algorithm modes: CBC (Cipher Block Chaining) mode, ECB (Electronic Code Book)
mode, GCM (Galois/Counter Mode) mode, XTS(cipherteXT Stealing) mode, CTR
(CounTeR) mode and 3DES respectively, which adds the flexibility to its
applications."
"The approach of applying associative processor for decision making problem
was proposed. It focuses on hardware implementations of fuzzy processing
systems, associativity as effective management basis of fuzzy processor. The
structural approach is being developed resulting in a quite simple and compact
parallel associative memory unit (PAMU). The memory cost and speed comparison
of processors with rigid and soft-variable structure is given. Also the example
PAMU flashing is considered."
"Invention of Transistors in 1948 started a new era in technology, called
Solid State Electronics. Since then, sustaining development and advancement in
electronics and fabrication techniques has caused the devices to shrink in size
and become smaller, paving the quest for increasing density and clock speed.
That quest has suddenly come to a halt due to fundamental bounds applied by
physical laws. But, demand for more and more computational power is still
prevalent in the computing world. As a result, the microprocessor industry has
started exploring the technology along a different dimension. Speed of a single
work unit (CPU) is no longer the concern, rather increasing the number of
independent processor cores packed in a single package has become the new
concern. Such processors are commonly known as multi-core processors. Scaling
the performance by using multiple cores has gained so much attention from the
academia and the industry, that not only desktops, but also laptops, PDAs, cell
phones and even embedded devices today contain these processors. In this paper,
we explore state of the art technologies for multi-core processors and existing
software tools to support parallelism. We also discuss present and future trend
of research in this field. From our survey, we conclude that next few decades
are going to be marked by the success of this ""Ubiquitous parallel processing""."
"In the above letter, Chin and Chen proposed an IEEE 1588 clock
synchronization method based on dual slave clocks, where they claim that
multiple unknown parameters --- i.e., clock offset, clock skew, and
master-to-slave delay --- can be estimated with only one-way time transfers
using more equations than usual. This comment investigates Chin and Chen's dual
clock scheme with detailed models for a master and dual slave clocks and shows
that the formulation of multi-parameter estimation is invalid, which affirms
that it is impossible to distinguish the effect of delay from that of clock
offset at a slave even with dual slave clocks."
"This paper presents the development of a new algorithm for Gaussian based
color image enhancement system. The algorithm has been designed into
architecture suitable for FPGA/ASIC implementation. The color image enhancement
is achieved by first convolving an original image with a Gaussian kernel since
Gaussian distribution is a point spread function which smoothen the image.
Further, logarithm-domain processing and gain/offset corrections are employed
in order to enhance and translate pixels into the display range of 0 to 255.
The proposed algorithm not only provides better dynamic range compression and
color rendition effect but also achieves color constancy in an image. The
design exploits high degrees of pipelining and parallel processing to achieve
real time performance. The design has been realized by RTL compliant Verilog
coding and fits into a single FPGA with a gate count utilization of 321,804.
The proposed method is implemented using Xilinx Virtex-II Pro XC2VP40-7FF1148
FPGA device and is capable of processing high resolution color motion pictures
of sizes of up to 1600x1200 pixels at the real time video rate of 116 frames
per second. This shows that the proposed design would work for not only still
images but also for high resolution video sequences."
"The WaveScalar is the first DataFlow Architecture that can efficiently
provide the sequential memory semantics required by imperative languages. This
work presents an alternative memory ordering mechanism for this architecture,
the Transaction WaveCache. Our mechanism maintains the execution order of
memory operations within blocks of code, called Waves, but adds the ability to
speculatively execute, out-of-order, operations from different waves. This
ordering mechanism is inspired by progress in supporting Transactional
Memories. Waves are considered as atomic regions and executed as nested
transactions. If a wave has finished the execution of all its memory
operations, as soon as the previous waves are committed, it can be committed.
If a hazard is detected in a speculative Wave, all the following Waves
(children) are aborted and re-executed. We evaluate the WaveCache on a set
artificial benchmarks. If the benchmark does not access memory often, we could
achieve speedups of around 90%. Speedups of 33.1% and 24% were observed on more
memory intensive applications, and slowdowns up to 16% arise if memory
bandwidth is a bottleneck. For an application full of WAW, WAR and RAW hazards,
a speedup of 139.7% was verified."
"Graphics processing units (GPUs) are gaining widespread use in computational
chemistry and other scientific simulation contexts because of their huge
performance advantages relative to conventional CPUs. However, the reliability
of GPUs in error-intolerant applications is largely unproven. In particular, a
lack of error checking and correcting (ECC) capability in the memory subsystems
of graphics cards has been cited as a hindrance to the acceptance of GPUs as
high-performance coprocessors, but the impact of this design has not been
previously quantified.
  In this article we present MemtestG80, our software for assessing memory
error rates on NVIDIA G80 and GT200-architecture-based graphics cards.
Furthermore, we present the results of a large-scale assessment of GPU error
rate, conducted by running MemtestG80 on over 20,000 hosts on the Folding@home
distributed computing network. Our control experiments on consumer-grade and
dedicated-GPGPU hardware in a controlled environment found no errors. However,
our survey over cards on Folding@home finds that, in their installed
environments, two-thirds of tested GPUs exhibit a detectable, pattern-sensitive
rate of memory soft errors. We demonstrate that these errors persist after
controlling for overclocking and environmental proxies for temperature, but
depend strongly on board architecture."
"The paper describes the new computers architecture, the main features of
which has been claimed in the Russian Federation patent 2312388 and in the US
patent application 11/991331. This architecture is intended to effective
support of the General Purpose Parallel Computing (GPPC), the essence of which
is extremely frequent switching of threads between states of activity and
states of viewed in the paper the algorithmic latency. To emphasize the same
impact of the architectural latency and the algorithmic latency upon GPPC, is
introduced the new notion of the generalized latency and is defined its
quantitative measure - the Generalized Latency Tolerance (GLT). It is shown
that a well suited for GPPC implementation architecture should have high level
of GLT and is described such architecture, which is called the Virtual-Threaded
Machine. This architecture originates a processor virtualization in the
direction of activities virtualization, which is orthogonal to the well-known
direction of memory virtualization. The key elements of the architecture are 1)
the distributed fine grain representation of the architectural register file,
which elements are hardware swapped through levels of a microarchitectural
memory, 2) the prioritized fine grain direct hardware multiprogramming, 3) the
access controlled virtual addressing and 4) the hardware driven semaphores. The
composition of these features lets to introduce new styles of operating system
(OS) programming, which is free of interruptions, and of applied programming
with a very rare using the OS services."
"Every year, the computing resources available on dynamically partially
reconfigurable devices increase enormously. In the near future, we expect many
applications to run on a single reconfigurable device. In this paper, we
present a concept for multitasking on dynamically partially reconfigurable
systems called virtual area management. We explain its advantages, show its
challenges, and discuss possible solutions. Furthermore, we investigate one
problem in more detail: Packing modules with time-varying resource requests.
This problem from the reconfigurable computing field results in a completely
new optimization problem not tackled before. ILP-based and heuristic approaches
are compared in an experimental study and the drawbacks and benefits discussed."
"This article presents an asynchronous FPGA architecture for implementing
cryptographic algorithms secured against physical cryptanalysis. We discuss the
suitability of asynchronous reconfigurable architectures for such applications
before proceeding to model the side channel and defining our objectives. The
logic block architecture is presented in detail. We discuss several solutions
for the interconnect architecture, and how these solutions can be ported to
other flavours of interconnect (i.e. single driver). Next We discuss in detail
a high speed asynchronous configuration chain architecture used to configure
our asynchronous FPGA with simulation results, and we present a 3 X 3 prototype
FPGA fabricated in 65 nm CMOS. Lastly we present experiments to test the high
speed asynchronous configuration chain and evaluate how far our objectives have
been achieved with proposed solutions, and we conclude with emphasis on
complementary FPGA CAD algorithms, and the effect of CMOS variation on
Side-Channel Vulnerability."
"As the demand for Internet expands significantly in numbers of users,
servers, IP addresses, switches and routers, the IP based network architecture
must evolve and change. The design of domain specific processors that require
high performance, low power and high degree of programmability is the
bottleneck in many processor based applications. This paper describes the
design of ethernet packet processor for system-on-chip (SoC) which performs all
core packet processing functions, including segmentation and reassembly,
packetization classification, route and queue management which will speedup
switching/routing performance. Our design has been configured for use with
multiple projects ttargeted to a commercial configurable logic device the
system is designed to support 10/100/1000 links with a speed advantage. VHDL
has been used to implement and simulated the required functions in FPGA."
"Sequential computation is well understood but does not scale well with
current technology. Within the next decade, systems will contain large numbers
of processors with potentially thousands of processors per chip. Despite this,
many computational problems exhibit little or no parallelism and many existing
formulations are sequential. It is therefore essential that highly-parallel
architectures can support sequential computation by emulating large memories
with collections of smaller ones, thus supporting efficient execution of
sequential programs or sequential components of parallel programs.
  This paper demonstrates that a realistic parallel architecture with scalable
low-latency communications can execute large-memory sequential programs with a
factor of only 2 to 3 slowdown, when compared to a conventional sequential
architecture. This overhead seems an acceptable price to pay to be able to
switch between executing highly-parallel programs and sequential programs with
large memory requirements. Efficient emulation of large memories could
therefore facilitate a transition from sequential machines by allowing existing
programs to be compiled directly to a highly-parallel architecture and then for
their performance to be improved by exploiting parallelism in memory accesses
and computation."
"In this paper the focus is on a family of Interconnection Networks (INs)
known as Multistage Interconnection Networks (MINs). When it is exploited in
Network-on-Chip (NoC) architecture designs, smaller circuit area, lower power
consumption, less junctions and broader bandwidth can be achieved. Each MIN can
be considered as an alternative for an NoC architecture design for its simple
topology and easy scalability with low degree. This paper includes two major
contributions. First, it compares the performance of seven prominent MINs (i.e.
Omega, Butterfly, Flattened Butterfly, Flattened Baseline, Generalized Cube,
Bene\v{s} and Clos networks) based on 45nm-CMOS technology and under different
types of Synthetic and Trace-driven workloads. Second, a network called
Meta-Flattened Network (MFN), was introduced that can decrease the blocking
probability by means of reduction the number of hops and increase the
intermediate paths between stages. This is also led into significant decrease
in power consumption."
"MGSim is an open source discrete event simulator for on-chip hardware
components, developed at the University of Amsterdam. It is intended to be a
research and teaching vehicle to study the fine-grained hardware/software
interactions on many-core and hardware multithreaded processors. It includes
support for core models with different instruction sets, a configurable
multi-core interconnect, multiple configurable cache and memory models, a
dedicated I/O subsystem, and comprehensive monitoring and interaction
facilities. The default model configuration shipped with MGSim implements
Microgrids, a many-core architecture with hardware concurrency management.
MGSim is furthermore written mostly in C++ and uses object classes to represent
chip components. It is optimized for architecture models that can be described
as process networks."
"This paper addresses a novel five-transistor (5T) CMOS SRAM design with high
performance and reliability in 65nm CMOS, and illustrates how it reduces the
dynamic power consumption in comparison with the conventional and low-power 6T
SRAM counterparts. This design can be used as cache memory in processors and
low-power portable devices. The proposed SRAM cell features ~13% area reduction
compared to a conventional 6T cell, and features a unique bit-line and negative
supply voltage biasing methodology and ground control architecture to enhance
performance, and suppress standby leakage power."
"Communication fabrics play a key role in the correctness and performance of
modern multi-core processors and systems-on-chip. To enable formal
verification, a recent trend is to use high-level micro-architectural models to
capture designers' intent about the communication and processing of messages.
Intel proposed the xMAS language to support the formal definition of executable
specifications of micro-architectures. We formalise the semantics of xMAS in
ACL2. Our formalisation represents the computation of the values of all wires
of a design. Our main function computes a set of possible routing targets for
each message and whether a message can make progress according to the current
network state. We prove several properties on the semantics, including
termination, non-emptiness of routing, and correctness of progress conditions.
Our current effort focuses on a basic subset of the entire xMAS language, which
includes queues, functions, and switches."
"This work analyses the effects of sequential-to-parallel synchronization and
inter-core communication on multicore performance, speedup and scaling. A
modification of Amdahl law is formulated, to reflect the finding that parallel
speedup is lower than originally predicted, due to these effects. In
applications with high inter-core communication requirements, the workload
should be executed on a small number of cores, and applications of high
sequential-to-parallel synchronization requirements may better be executed by
the sequential core, even when f, the Amdahl fraction of parallelization, is
very close to 1. To improve the scalability and performance speedup of a
multicore, it is as important to address the synchronization and connectivity
intensities of parallel algorithms as their parallelization factor."
"Making a reversible circuit fault-tolerant is much more difficult than
classical circuit and there have been only a few works in the area of
parity-preserving reversible logic design. Moreover, all of these designs are
ad hoc, based on some pre-defined parity preserving reversible gates as
building blocks. In this paper, we for the first time propose a novel and
systematic approach towards parity preserving reversible circuits design. We
provide some related theoretical results and give two algorithms, one from
reversible specification to parity preserving reversible specification and
another from irreversible specification to parity preserving reversible
specification. We also evaluate the effectiveness of our approach by extensive
experimental results."
"We present a class of massively parallel processor architectures called
invasive tightly coupled processor arrays (TCPAs). The presented processor
class is a highly parameterizable template, which can be tailored before
runtime to fulfill costumers' requirements such as performance, area cost, and
energy efficiency. These programmable accelerators are well suited for
domain-specific computing from the areas of signal, image, and video processing
as well as other streaming processing applications. To overcome future scaling
issues (e.g., power consumption, reliability, resource management, as well as
application parallelization and mapping), TCPAs are inherently designed in a
way to support self-adaptivity and resource awareness at hardware level. Here,
we follow a recently introduced resource-aware parallel computing paradigm
called invasive computing where an application can dynamically claim, execute,
and release resources. Furthermore, we show how invasive computing can be used
as an enabler for power management. Finally, we will introduce ideas on how to
realize fault-tolerant loop execution on such massively parallel architectures
through employing on-demand spatial redundancies at the processor array level."
"We describe the formal language MASC, based on a subset of SystemC and
intended for modeling algorithms to be implemented in hardware. By means of a
special-purpose parser, an algorithm coded in SystemC is converted to a MASC
model for the purpose of documentation, which in turn is translated to ACL2 for
formal verification. The parser also generates a SystemC variant that is
suitable as input to a high-level synthesis tool. As an illustration of this
methodology, we describe a proof of correctness of a simple 32-bit radix-4
multiplier."
"Recent NVIDIA Graphics Processing Units (GPUs) can execute multiple kernels
concurrently. On these GPUs, the thread block scheduler (TBS) uses the FIFO
policy to schedule their thread blocks. We show that FIFO leaves performance to
chance, resulting in significant loss of performance and fairness. To improve
performance and fairness, we propose use of the preemptive Shortest Remaining
Time First (SRTF) policy instead. Although SRTF requires an estimate of runtime
of GPU kernels, we show that such an estimate of the runtime can be easily
obtained using online profiling and exploiting a simple observation on GPU
kernels' grid structure. Specifically, we propose a novel Structural Runtime
Predictor. Using a simple Staircase model of GPU kernel execution, we show that
the runtime of a kernel can be predicted by profiling only the first few thread
blocks. We evaluate an online predictor based on this model on benchmarks from
ERCBench, and find that it can estimate the actual runtime reasonably well
after the execution of only a single thread block. Next, we design a thread
block scheduler that is both concurrent kernel-aware and uses this predictor.
We implement the SRTF policy and evaluate it on two-program workloads from
ERCBench. SRTF improves STP by 1.18x and ANTT by 2.25x over FIFO. When compared
to MPMax, a state-of-the-art resource allocation policy for concurrent kernels,
SRTF improves STP by 1.16x and ANTT by 1.3x. To improve fairness, we also
propose SRTF/Adaptive which controls resource usage of concurrently executing
kernels to maximize fairness. SRTF/Adaptive improves STP by 1.12x, ANTT by
2.23x and Fairness by 2.95x compared to FIFO. Overall, our implementation of
SRTF achieves system throughput to within 12.64% of Shortest Job First (SJF, an
oracle optimal scheduling policy), bridging 49% of the gap between FIFO and
SJF."
"In this paper we introduce Epiphany as a high-performance energy-efficient
manycore architecture suitable for real-time embedded systems. This scalable
architecture supports floating point operations in hardware and achieves 50
GFLOPS/W in 28 nm technology, making it suitable for high performance streaming
applications like radio base stations and radar signal processing. Through an
efficient 2D mesh Network-on-Chip and a distributed shared memory model, the
architecture is scalable to thousands of cores on a single chip. An
Epiphany-based open source computer named Parallella was launched in 2012
through Kickstarter crowd funding and has now shipped to thousands of customers
around the world."
"There are increasing concerns about possible malicious modifications of
integrated circuits (ICs) used in critical applications. Such attacks are often
referred to as hardware Trojans. While many techniques focus on hardware Trojan
detection during IC testing, it is still possible for attacks to go undetected.
Using a combination of new design techniques and new memory technologies, we
present a new approach that detects a wide variety of hardware Trojans during
IC testing and also during system operation in the field. Our approach can also
prevent a wide variety of attacks during synthesis, place-and-route, and
fabrication of ICs. It can be applied to any digital system, and can be tuned
for both traditional and split-manufacturing methods. We demonstrate its
applicability for both ASICs and FPGAs. Using fabricated test chips with Trojan
emulation capabilities and also using simulations, we demonstrate: 1. The area
and power costs of our approach can range between 7.4-165% and 0.07-60%,
respectively, depending on the design and the attacks targeted; 2. The speed
impact can be minimal (close to 0%); 3. Our approach can detect 99.998% of
Trojans (emulated using test chips) that do not require detailed knowledge of
the design being attacked; 4. Our approach can prevent 99.98% of specific
attacks (simulated) that utilize detailed knowledge of the design being
attacked (e.g., through reverse-engineering). 5. Our approach never produces
any false positives, i.e., it does not report attacks when the IC operates
correctly."
"When designing modern embedded computing systems, most software programmers
choose to use multicore processors, possibly in combination with
general-purpose graphics processing units (GPGPUs) and/or hardware
accelerators. They also often use an embedded Linux O/S and run
multi-application workloads that may even be multi-threaded. Modern FPGAs are
large enough to combine multicore hard/soft processors with multiple hardware
accelerators as custom compute units, enabling entire embedded compute systems
to be implemented on a single FPGA. Furthermore, the large FPGA vendors also
support embedded Linux kernels for both their soft and embedded processors.
When combined with high-level synthesis to generate hardware accelerators using
a C-to-gates flows, the necessary primitives for a framework that can enable
software designers to use FPGAs as their custom compute platform now exist.
However, in order to ensure that computing resources are integrated and shared
effectively, software developers need to be able to monitor and debug the
runtime performance of the applications in their workload. This paper describes
ABACUS, a performance-monitoring framework that can be used to debug the
execution behaviours and interactions of multi-application workloads on
multicore systems. We also discuss how this framework is extensible for use
with hardware accelerators in heterogeneous systems."
"We propose a virtualization architecture for NoC-based reconfigurable
systems. The motivation of this work is to develop a service-oriented
architecture that includes Partial Reconfigurable Region as a Service (PRRaaS)
and Processing Element as a Service (PEaaS) for software applications.
According to the requirements of software applications, new PEs can be created
on-demand by (re)configuring the logic resource of the PRRs in the FPGA, while
the configured PEs can also be virtualized to support multiple application
tasks at the same time. As a result, such a two-level virtualization mechanism,
including the gate-level virtualization and the PE-level virtualization,
enables an SoC to be dynamically adapted to changing application requirements.
Therefore, more software applications can be performed, and system performance
can be further enhanced."
"Offloading compute intensive nested loops to execute on FPGA accelerators
have been demonstrated by numerous researchers as an effective performance
enhancement technique across numerous application domains. To construct such
accelerators with high design productivity, researchers have increasingly
turned to the use of overlay architectures as an intermediate generation target
built on top of off-the-shelf FPGAs. However, achieving the desired
performance-overhead trade-off remains a major productivity challenge as
complex application-specific customizations over a large design space covering
multiple architectural parameters are needed.
  In this work, an automatic nested loop acceleration framework utilizing a
regular soft coarse-grained reconfigurable array (SCGRA) overlay is presented.
Given high-level resource constraints, the framework automatically customizes
the overlay architectural design parameters, high-level compilation options as
well as communication between the accelerator and the host processor for
optimized performance specifically to the given application. In our
experiments, at a cost of 10 to 20 minutes additional tools run time, the
proposed customization process resulted in up to 5 times additional speedup
over a baseline accelerator generated by the same framework without
customization. Overall, when compared to the equivalent software running on the
host ARM processor alone on the Zedboard, the resulting accelerators achieved
up to 10 times speedup."
"Memory access efficiency is a key factor in fully utilizing the computational
power of graphics processing units (GPUs). However, many details of the GPU
memory hierarchy are not released by GPU vendors. In this paper, we propose a
novel fine-grained microbenchmarking approach and apply it to three generations
of NVIDIA GPUs, namely Fermi, Kepler and Maxwell, to expose the previously
unknown characteristics of their memory hierarchies. Specifically, we
investigate the structures of different GPU cache systems, such as the data
cache, the texture cache and the translation look-aside buffer (TLB). We also
investigate the throughput and access latency of GPU global memory and shared
memory. Our microbenchmark results offer a better understanding of the
mysterious GPU memory hierarchy, which will facilitate the software
optimization and modelling of GPU architectures. To the best of our knowledge,
this is the first study to reveal the cache properties of Kepler and Maxwell
GPUs, and the superiority of Maxwell in shared memory performance under bank
conflict."
"Now a day reversible logic is an attractive research area due to its low
power consumption in the area of VLSI circuit design. The reversible logic gate
is utilized to optimize power consumption by a feature of retrieving input
logic from an output logic because of bijective mapping between input and
output. In this manuscript, we design 4 2 and 5 2 reversible compressor
circuits using a new type of reversible gate. In addition, we propose new gate,
named as inventive0 gate for optimizing a compressor circuit. The utility of
the inventive0 gate is that it can be used as full adder and full subtraction
with low value of garbage outputs and quantum cost. An algorithm is shown for
designing a compressor structure. The comparative study shows that the proposed
compressor structure outperforms the existing ones in terms of garbage outputs,
number of gates and quantum cost. The compressor can reduce the effect of carry
(Produce from full adder) of the arithmetic frame design. In addition, we
implement a basic reversible gate of MOS transistor with less number of MOS
transistor count."
"Improving the efficiency of edge detection in embedded applications, such as
UAV control, is critical for reducing system cost and power dissipation. Field
programmable gate arrays (FPGA) are a good platform for making improvements
because of their specialised internal structure. However, current FPGA edge
detectors do not exploit this structure well. A new edge detection architecture
is proposed that is better optimised for FPGAs. The basis of the architecture
is the Sobel edge kernels that are shown to be the most suitable because of
their separability and absence of multiplications. Edge intensities are
calculated with a new 4:2 compressor that consists of two custom-designed 3:2
compressors. Addition speed is increased by breaking carry propagation chains
with look-ahead logic. Testing of the design showed it gives a 28% increase in
speed and 4.4% reduction in area over previous equivalent designs, which
demonstrated that it will lower the cost of edge detection systems, dissipate
less power and still maintain high-speed control."
"The memory system of a modern embedded processor consumes a large fraction of
total system energy. We explore a range of different configuration options and
show that a reconfigurable design can make better use of the resources
available to it than any fixed implementation, and provide large improvements
in both performance and energy consumption. Reconfigurability becomes
increasingly useful as resources become more constrained, so is particularly
relevant in the embedded space.
  For an optimised architectural configuration, we show that a configurable
cache system performs an average of 20% (maximum 70%) better than the best
fixed implementation when two programs are competing for the same resources,
and reduces cache miss rate by an average of 70% (maximum 90%). We then present
a case study of AES encryption and decryption, and find that a custom memory
configuration can almost double performance, with further benefits being
achieved by specialising the task of each core when parallelising the program."
"As energy efficiency became a critical factor in the embedded systems domain,
dynamic voltage and frequency scaling (DVFS) techniques have emerged as means
to control the system's power and energy efficiency. Additionally, due to the
compact design, thermal issues become prominent. State of the art work promotes
software decoupled access-execution (DAE) that statically generates code
amenable to DVFS techniques. The compiler builds memory-bound access phases,
designed to prefetch data in the cache at low frequency, and compute-bound
phases, that consume the data and perform computations at high frequency. This
work investigates techniques to find the optimal balance between lightweight
and efficient access phases. A profiling step guides the selection of loads to
be prefetched in the access phase. For applications whose behavior vary
significantly with respect to the input data, the profiling can be performed
online, accompanied by just-in-time compilation. We evaluated the benefits in
energy efficiency and performance for both static and dynamic code generation
and showed that precise prefetching of critical loads can result in 20% energy
improvements, on average. DAE is particularly beneficial for embedded systems
as by alternating access phases (executed at low frequency) and execute phases
(at high frequency) DAE proactively reduces the temperature and therefore
prevents thermal emergencies."
"Deep neural networks (DNNs) demand a very large amount of computation and
weight storage, and thus efficient implementation using special purpose
hardware is highly desired. In this work, we have developed an FPGA based
fixed-point DNN system using only on-chip memory not to access external DRAM.
The execution time and energy consumption of the developed system is compared
with a GPU based implementation. Since the capacity of memory in FPGA is
limited, only 3-bit weights are used for this implementation, and training
based fixed-point weight optimization is employed. The implementation using
Xilinx XC7Z045 is tested for the MNIST handwritten digit recognition benchmark
and a phoneme recognition task on TIMIT corpus. The obtained speed is about one
quarter of a GPU based implementation and much better than that of a PC based
one. The power consumption is less than 5 Watt at the full speed operation
resulting in much higher efficiency compared to GPU based systems."
"This paper presents a complete video fusion system with hardware acceleration
and investigates the energy trade-offs between computing in the CPU or the FPGA
device. The video fusion application is based on the Dual-Tree Complex Wavelet
Transforms (DT-CWT). In this work the transforms are mapped to a hardware
accelerator using high-level synthesis tools for the FPGA and also vectorized
code for the single instruction multiple data (SIMD) engine available in the
CPU. The accelerated system reduces computation time and energy by a factor of
2. Moreover, the results show a key finding that the FPGA is not always the
best choice for acceleration, and the SIMD engine should be selected when the
wavelet decomposition reduces the frame size below a certain threshold. This
dependency on workload size means that an adaptive system that intelligently
selects between the SIMD engine and the FPGA achieves the most energy and
performance efficiency point."
"A two-dimensional Ising model with nearest-neighbors ferromagnetic
interactions is implemented in a Field Programmable Gate Array (FPGA)
board.Extensive Monte Carlo simulations were carried out using an efficient
hardware representation of individual spins and a combined global-local LFSR
random number generator. Consistent results regarding the descriptive
properties of magnetic systems, like energy, magnetization and susceptibility
are obtained while a speed-up factor of approximately 6 times is achieved in
comparison to previous FPGA-based published works and almost $10^4$ times in
comparison to a standard CPU simulation. A detailed description of the logic
design used is given together with a careful analysis of the quality of the
random number generator used. The obtained results confirm the potential of
FPGAs for analyzing the statistical mechanics of magnetic systems."
"The key challenge to improving performance in the age of Dark Silicon is how
to leverage transistors when they cannot all be used at the same time. In
modern SOCs, these transistors are often used to create specialized
accelerators which improve energy efficiency for some applications by 10-1000X.
While this might seem like the magic bullet we need, for most CPU applications
more energy is dissipated in the memory system than in the processor: these
large gains in efficiency are only possible if the DRAM and memory hierarchy
are mostly idle. We refer to this desirable state as Dark Memory, and it only
occurs for applications with an extreme form of locality.
  To show our findings, we introduce Pareto curves in the energy/op and
mm$^2$/(ops/s) metric space for compute units, accelerators, and on-chip
memory/interconnect. These Pareto curves allow us to solve the power,
performance, area constrained optimization problem to determine which
accelerators should be used, and how to set their design parameters to optimize
the system. This analysis shows that memory accesses create a floor to the
achievable energy-per-op. Thus high performance requires Dark Memory, which in
turn requires co-design of the algorithm for parallelism and locality, with the
hardware."
"We introduce the Coarse-Grain Out-of-Order (CG- OoO) general purpose
processor designed to achieve close to In-Order processor energy while
maintaining Out-of-Order (OoO) performance. CG-OoO is an energy-performance
proportional general purpose architecture that scales according to the program
load. Block-level code processing is at the heart of the this architecture;
CG-OoO speculates, fetches, schedules, and commits code at block-level
granularity. It eliminates unnecessary accesses to energy consuming tables, and
turns large tables into smaller and distributed tables that are cheaper to
access. CG-OoO leverages compiler-level code optimizations to deliver efficient
static code, and exploits dynamic instruction-level parallelism and block-level
parallelism. CG-OoO introduces Skipahead issue, a complexity effective, limited
out-of-order instruction scheduling model. Through the energy efficiency
techniques applied to the compiler and processor pipeline stages, CG-OoO closes
64% of the average energy gap between the In-Order and Out-of-Order baseline
processors at the performance of the OoO baseline. This makes CG-OoO 1.9x more
efficient than the OoO on the energy-delay product inverse metric."
"Innovation in hardware is slowing due to rising costs of chip design and
diminishing benefits from Moore's law and Dennard scaling. Software innovation,
on the other hand, is flourishing, helped in good measure by a thriving
open-source ecosystem. We believe that open source can similarly help hardware
innovation, but has not yet due to several reasons. We identify these reasons
and how the industry, academia, and the hardware community at large can come
together to address them."
"The rapid development of multi-core system and increase of data-intensive
application in recent years call for larger main memory. Traditional DRAM
memory can increase its capacity by reducing the feature size of storage cell.
Now further scaling of DRAM faces great challenge, and the frequent refresh
operations of DRAM can bring a lot of energy consumption. As an emerging
technology, Phase Change Memory (PCM) is promising to be used as main memory.
It draws wide attention due to the advantages of low power consumption, high
density and nonvolatility, while it incurs finite endurance and relatively long
write latency. To handle the problem of write, optimizing the cache replacement
policy to protect dirty cache block is an efficient way. In this paper, we
construct a systematically multilevel structure, and based on it propose a
novel cache replacement policy called MAC. MAC can effectively reduce write
traffic to PCM memory with low hardware overhead. We conduct simulation
experiments on GEM5 to evaluate the performances of MAC and other related
works. The results show that MAC performs best in reducing the amount of writes
(averagely 25.12%) without increasing the program execution time."
"There is much interest in incorporating inference capabilities into
sensor-rich embedded platforms such as autonomous vehicles, wearables, and
others. A central problem in the design of such systems is the need to extract
information locally from sensed data on a severely limited energy budget. This
necessitates the design of energy-efficient sensory embedded system. A typical
sensory embedded system enforces a physical separation between sensing and
computational subsystems - a separation mandated by the differing requirements
of the sensing and computational functions. As a consequence, the energy
consumption in such systems tends to be dominated by the energy consumed in
transferring data over the sensor-processor interface (communication energy)
and the energy consumed in processing the data in digital processor
(computational energy). In this article, we propose an in-sensor computing
architecture which (mostly) eliminates the sensor-processor interface by
embedding inference computations in the noisy sensor fabric in analog and
retraining the hyperparameters in order to compensate for non-ideal
computations. The resulting architecture referred to as the Compute Sensor - a
sensor that computes in addition to sensing - represents a radical departure
from the conventional. We show that a Compute Sensor for image data can be
designed by embedding both feature extraction and classification functions in
the analog domain in close proximity to the CMOS active pixel sensor (APS)
array. Significant gains in energy-efficiency are demonstrated using behavioral
and energy models in a commercial semiconductor process technology. In the
process, the Compute Sensor creates a unique opportunity to develop machine
learning algorithms for information extraction from data on a noisy underlying
computational fabric."
"As capacity and complexity of on-chip cache memory hierarchy increases, the
service cost to the critical loads from Last Level Cache (LLC), which are
frequently repeated, has become a major concern. The processor may stall for a
considerable interval while waiting to access the data stored in the cache
blocks in LLC, if there are no independent instructions to execute. To provide
accelerated service to the critical loads requests from LLC, this work
concentrates on leveraging the additional capacity offered by replacing
SRAM-based L2 with Spin-Transfer Torque Random Access Memory (STT-RAM) to
accommodate frequently accessed cache blocks in exclusive read mode in favor of
reducing the overall read service time. Our proposed technique partitions L2
cache into two STT-RAM arrangements with different write performance and data
retention time. The retention-relaxed STT-RAM arrays are utilized to
effectively deal with the regular L2 cache requests while the high retention
STT-RAM arrays in L2 are selected for maintaining repeatedly read accessed
cache blocks from LLC by incurring negligible energy consumption for data
retention. Our experimental results show that the proposed technique can reduce
the mean L2 read miss ratio by 51.4% and increase the IPC by 11.7% on average
across PARSEC benchmark suite while significantly decreasing the total L2
energy consumption compared to conventional SRAM-based L2 design."
"Whereas contemporary Error Correcting Codes (ECC) designs occupy a
significant fraction of total die area in chip-multiprocessors (CMPs),
approaches to deal with the vulnerability increase of CMP architecture against
Single Event Upsets (SEUs) and Multi-Bit Upsets (MBUs) are sought. In this
paper, we focus on reliability assessment of multithreaded applications running
on CMPs to propose an adaptive application-relevant architecture design to
accommodate the impact of both SEUs and MBUs in the entire CMP architecture.
This work concentrates on leveraging the intrinsic soft-error-immunity feature
of Spin-Transfer Torque RAM (STT-RAM) as an alternative for SRAM-based storage
and operation components. We target a specific portion of working set for
reallocation to improve the reliability level of the CMP architecture design. A
selected portion of instructions in multithreaded program which experience high
rate of referencing with the lowest memory modification are ideal candidate to
be stored and executed in STT-RAM based components. We argue about why we
cannot use STT-RAM for the global storage and operation counterparts and
describe the obtained resiliency compared to the baseline setup. In addition, a
detail study of the impact of SEUs and MBUs on multithreaded programs will be
presented in the Appendix."
"The distributed minority and majority voting based redundancy (DMMR) scheme
was recently proposed as an efficient alternative to the conventional N-modular
redundancy (NMR) scheme for the physical design of mission/safety-critical
circuits and systems. The DMMR scheme enables significant improvements in fault
tolerance and design metrics compared to the NMR scheme albeit at the expense
of a slight decrease in the system reliability. In this context, this paper
studies the system reliability, fault tolerance and design metrics tradeoffs in
the DMMR scheme compared to the NMR scheme when the majority logic group of the
DMMR scheme is increased in size relative to the minority logic group. Some
example DMMR and NMR systems were realized using a 32/28nm CMOS process and
compared. The results show that 5-of-M DMMR systems have a similar or better
fault tolerance whilst requiring similar or fewer function modules than their
counterpart NMR systems and simultaneously achieve optimizations in design
metrics. Nevertheless, 3-of-M DMMR systems have the upper hand with respect to
fault tolerance and design metrics optimizations than the comparable NMR and
5-of-M DMMR systems. With regard to system reliability, NMR systems are closely
followed by 5-of-M DMMR systems which are closely followed by 3-of-M DMMR
systems. The verdict is 3-of-M DMMR systems are preferable to implement higher
levels of redundancy from a combined system reliability, fault tolerance and
design metrics perspective to realize mission/safety-critical circuits and
systems."
"Response time requirements for big data processing systems are shrinking. To
meet this strict response time requirement, many big data systems store all or
most of their data in main memory to reduce the access latency. Main memory
capacities have grown, and systems with 2 TB of main memory capacity available
today. However, the rate at which processors can access this data--the memory
bandwidth--has not grown at the same rate. In fact, some of these big-memory
systems can access less than 10% of their main memory capacity in one second
(billions of processor cycles).
  3D die-stacking is one promising solution to this bandwidth problem, and
industry is investing significantly in 3D die-stacking. We use a simple
back-of-the-envelope-style model to characterize if and when the 3D die-stacked
architecture is more cost-effective than current architectures for in-memory
big data workloads. We find that die-stacking has much higher performance than
current systems (up to 256x lower response times), and it does not require
expensive memory over provisioning to meet real-time (10 ms) response time
service-level agreements. However, the power requirements of the die-stacked
systems are significantly higher (up to 50x) than current systems, and its
memory capacity is lower in many cases. Even in this limited case study, we
find 3D die-stacking is not a panacea. Today, die-stacking is the most
cost-effective solution for strict SLAs and by reducing the power of the
compute chip and increasing memory densities die-stacking can be cost-effective
under other constraints in the future."
"In this thesis, we describe a new, practical approach to integrating
hardware-based data compression within the memory hierarchy, including on-chip
caches, main memory, and both on-chip and off-chip interconnects. This new
approach is fast, simple, and effective in saving storage space. A key insight
in our approach is that access time (including decompression latency) is
critical in modern memory hierarchies. By combining inexpensive hardware
support with modest OS support, our holistic approach to compression achieves
substantial improvements in performance and energy efficiency across the memory
hierarchy. Using this new approach, we make several major contributions in this
thesis. First, we propose a new compression algorithm, Base-Delta-Immediate
Compression (BDI), that achieves high compression ratio with very low
compression/decompression latency. BDI exploits the existing low dynamic range
of values present in many cache lines to compress them to smaller sizes using
Base+Delta encoding. Second, we observe that the compressed size of a cache
block can be indicative of its reuse. We use this observation to develop a new
cache insertion policy for compressed caches, the Size-based Insertion Policy
(SIP), which uses the size of a compressed block as one of the metrics to
predict its potential future reuse. Third, we propose a new main memory
compression framework, Linearly Compressed Pages (LCP), that significantly
reduces the complexity and power cost of supporting main memory compression. We
demonstrate that any compression algorithm can be adapted to fit the
requirements of LCP, and that LCP can be efficiently integrated with the
existing cache compression designs, avoiding extra compression/decompression."
"As the complexity of current data flow systems and according infrastructure
networks increases, the security of data transition through such platforms
becomes more important. Thus, different areas of steganography turn to one of
the most challengeable topics of current researches. In this paper a novel
method is presented to hide an image into the host image and Hardware/Software
design is proposed to implement our stagenography system on FPGA- DE2 70 Altera
board. The size of the secret image is quadrant of the host image. Host image
works as a cipher key to completely distort and encrypt the secret image using
XOR operand. Each pixel of the secret image is composed of 8 bits (4 bit-pair)
in which each bit-pair is distorted by XORing it with two LSB bits of the host
image and putting the results in the location of two LSB bits of host image.
The experimental results show the effectiveness of the proposed method compared
to the most recently proposed algorithms by considering that the obtained
information entropy for encrypt image is approximately equal to 8."
"In this paper we present quaternary and ternary routing tracks for FPGAs, and
their implementation in 28nm FDSOI technology. We discuss the transistor level
design of multi-valued repeaters, multiplexers and translators, and specific
features of FDSOI technology which make it possible. Next we compare the
multi-valued routing architectures with equivalent single driver two-valued
routing architectures. We show that for long tracks, it is possible to achieve
upto 3x reduction in dynamic switching energy, upto 2x reduction in routing
wire area and 10% reduction in area dedicated to routing resources. The
multi-valued tracks are slightly more susceptible to process variation. We
present a layout method for multivalued standard cells and determine the layout
overhead.We conclude with various usage scenarios of these tracks."
"Basic Linear Algebra Subprograms (BLAS) play key role in high performance and
scientific computing applications. Experimentally, yesteryear multicore and
General Purpose Graphics Processing Units (GPGPUs) are capable of achieving up
to 15 to 57% of the theoretical peak performance at 65W to 240W respectively
for compute bound operations like Double/Single Precision General Matrix
Multiplication (XGEMM). For bandwidth bound operations like Single/Double
precision Matrix-vector Multiplication (XGEMV) the performance is merely 5 to
7% of the theoretical peak performance in multicores and GPGPUs respectively.
Achieving performance in BLAS requires moving away from conventional wisdom and
evolving towards customized accelerator tailored for BLAS through
algorithm-architecture co-design. In this paper, we present acceleration of
Level-1 (vector operations), Level-2 (matrix-vector operations), and Level-3
(matrix-matrix operations) BLAS through algorithm architecture co-design on a
Coarse-grained Reconfigurable Architecture (CGRA). We choose REDEFINE CGRA as a
platform for our experiments since REDEFINE can be adapted to support domain of
interest through tailor-made Custom Function Units (CFUs). For efficient
sequential realization of BLAS, we present design of a Processing Element (PE)
and perform micro-architectural enhancements in the PE to achieve up-to 74% of
the theoretical peak performance of PE in DGEMM, 40% in DGEMV and 20% in double
precision inner product (DDOT). We attach this PE to REDEFINE CGRA as a CFU and
show the scalability of our solution. Finally, we show performance improvement
of 3-140x in PE over commercially available Intel micro-architectures,
ClearSpeed CSX700, FPGA, and Nvidia GPGPUs."
"Oblivious RAM (ORAM) is a cryptographic primitive which obfuscates the access
patterns to a storage thereby preventing privacy leakage. So far in the current
literature, only `fully functional' ORAMs are widely studied which can protect,
at a cost of considerable performance penalty, against the strong adversaries
who can monitor all read and write operations. However, recent research has
shown that information can still be leaked even if only the write access
pattern (not reads) is visible to the adversary. For such weaker adversaries, a
fully functional ORAM turns out to be an overkill causing unnecessary
overheads. Instead, a simple `write-only' ORAM is sufficient, and, more
interestingly, is preferred as it can offer far more performance and energy
efficiency than a fully functional ORAM.
  In this work, we present Flat ORAM: an efficient write-only ORAM scheme which
outperforms the closest existing write-only ORAM called HIVE. HIVE suffers from
performance bottlenecks while managing the memory occupancy information vital
for correctness of the protocol. Flat ORAM resolves these bottlenecks by
introducing a simple idea of Occupancy Map (OccMap) which efficiently manages
the memory occupancy information resulting in far better performance. Our
simulation results show that, on average, Flat ORAM only incurs a moderate
slowdown of $3\times$ over the insecure DRAM for memory intensive benchmarks
among Splash2 and $1.6\times$ for SPEC06. Compared to HIVE, Flat ORAM offers
$50\%$ performance gain on average and up to $80\%$ energy savings."
"We present a novel architecture for sparse pattern processing, using flash
storage with embedded accelerators. Sparse pattern processing on large data
sets is the essence of applications such as document search, natural language
processing, bioinformatics, subgraph matching, machine learning, and graph
processing. One slice of our prototype accelerator is capable of handling up to
1TB of data, and experiments show that it can outperform C/C++ software
solutions on a 16-core system at a fraction of the power and cost; an optimized
version of the accelerator can match the performance of a 48-core server."
"In data communication via internet, security is becoming one of the most
influential aspects. One way to support it is by classifying and filtering
ethernet packets within network devices. Packet classification is a fundamental
task for network devices such as routers, firewalls, and intrusion detection
systems. In this paper we present architecture of fast and reconfigurable
Packet Classification Engine (PCE). This engine is used in FPGA-based firewall.
Our PCE inspects multi-dimensional field of packet header sequentially based on
tree-based algorithm. This algorithm simplifies overall system to a lower scale
and leads to a more secure system. The PCE works with an adaptation of single
cycle processor architecture in the system. Ethernet packet is examined with
PCE based on Source IP Address, Destination IP Address, Source Port,
Destination Port, and Protocol fields of the packet header. These are basic
fields to know whether it is a dangerous or normal packet before inspecting the
content. Using implementation of tree-based algorithm in the architecture,
firewall rules are rebuilt into 24-bit sub-rules which are read as processor
instruction in the inspection process. The inspection process is comparing one
sub-rule with input field of header every clock cycle. The proposed PCE shows
91 MHz clock frequency in Cyclone II EP2C70F896C6 with 13 clocks throughput
average from input to output generation. The use of tree-based algorithm
simplifies the multidimensional packet inspection and gives us reconfigurable
as well as scalable system. The architecture is fast, reliable, and adaptable
and also can maximize the advantages of the algorithm very well. Although the
PCE has high frequency and little amount of clock, filtering speed of a
firewall also depends on the other components, such as packet FIFO buffer. Fast
and reliable FIFO buffer is needed to support the PCE. This PCE also is not
completed with rule update mechanism yet. This proposed PCE is tested as a
component of FPGA-based firewall to filter Ethernet packet with FPGA DE2 Board
using NIOS II platform."
"Memory and logic integration on the same chip is becoming increasingly cost
effective, creating the opportunity to offload data-intensive functionality to
processing units placed inside memory chips. The introduction of memory-side
processing units (MPUs) into conventional systems faces virtual memory as the
first big showstopper: without efficient hardware support for address
translation MPUs have highly limited applicability. Unfortunately, conventional
translation hardware (i.e., TLBs, MMU caches, and page table walkers) incurs
dramatic overheads due to the limited reach, increasingly high miss penalty,
and high translation coherence cost incurred with rapidly growing aggregate
memory size.
  In this paper, we are the first to show that the historically important
flexibility to map any virtual page to any page frame is unnecessary in today's
servers. We find that while limiting the associativity of the
virtual-to-physical mapping incurs no penalty, it can break the
translate-then-fetch serialization if combined with careful data placement in
the MPU's memory, allowing for translation and data fetch to proceed
independently and in parallel. We propose the Distributed Inverted Page Table
(DIPTA), a near-memory structure in which the smallest memory partition keeps
the translation information for its data share, ensuring that the translation
completes together with the data fetch. DIPTA completely eliminates the
conventional translation hardware, as well as the performance overhead of
translation, achieving speedups of up to 4.95x and 2.44x over conventional
translation using 4KB and 1GB pages respectively, and obviating the need for
translation-related broadcasts."
"Application trends, device technologies and the architecture of systems drive
progress in information technologies. However, the former engines of such
progress - Moore's Law and Dennard Scaling - are rapidly reaching the point of
diminishing returns. The time has come for the computing community to boldly
confront a new challenge: how to secure a foundational future for information
technology's continued progress. The computer architecture community engaged in
several visioning exercises over the years. Five years ago, we released a white
paper, 21st Century Computer Architecture, which influenced funding programs in
both academia and industry. More recently, the IEEE Rebooting Computing
Initiative explored the future of computing systems in the architecture,
device, and circuit domains. This report stems from an effort to continue this
dialogue, reach out to the applications and devices/circuits communities, and
understand their trends and vision. We aim to identify opportunities where
architecture research can bridge the gap between the application and device
domains."
"Near-sensor data analytics is a promising direction for IoT endpoints, as it
minimizes energy spent on communication and reduces network load - but it also
poses security concerns, as valuable data is stored or sent over the network at
various stages of the analytics pipeline. Using encryption to protect sensitive
data at the boundary of the on-chip analytics engine is a way to address data
security issues. To cope with the combined workload of analytics and encryption
in a tight power envelope, we propose Fulmine, a System-on-Chip based on a
tightly-coupled multi-core cluster augmented with specialized blocks for
compute-intensive data processing and encryption functions, supporting software
programmability for regular computing tasks. The Fulmine SoC, fabricated in
65nm technology, consumes less than 20mW on average at 0.8V achieving an
efficiency of up to 70pJ/B in encryption, 50pJ/px in convolution, or up to
25MIPS/mW in software. As a strong argument for real-life flexible application
of our platform, we show experimental results for three secure analytics use
cases: secure autonomous aerial surveillance with a state-of-the-art deep CNN
consuming 3.16pJ per equivalent RISC op; local CNN-based face detection with
secured remote recognition in 5.74pJ/op; and seizure detection with encrypted
data collection from EEG within 12.7pJ/op."
"High-performance computing systems are moving toward 2.5D memory hierarchies,
based on High-bandwidth memory (HBM) and Hybrid Memory Cube (HMC) to mitigate
the main memory bottlenecks. This trend is also creating new opportunities to
revisit near-memory computation. In this paper, we propose a flexible
processor-in-memory (PIM) solution for scalable and energy-efficient execution
of deep convolutional networks (ConvNets), one of the fastest-growing workloads
for servers and high-end embedded systems. Our co-design approach consists of a
network of Smart Memory Cubes (modular extensions to the standard HMC) each
augmented with a manycore PIM platform called NeuroCluster. NeuroClusters have
a modular design based on NeuroStream co-processors (for Convolution-intensive
computations) and general-purpose RISCV cores. In addition, a DRAM-friendly
tiling mechanism and a scalable computation paradigm are presented to
efficiently harness this computational capability with a very low programming
effort. NeuroCluster occupies only 8% of the total logic-base (LoB) die area in
a standard HMC and achieves an average performance of 240 GFLOPS for complete
execution of full-featured state-of-the-art (SoA) ConvNets within a power
budget of 2.5 W. Overall 11 W is consumed in a single SMC device, with 22.5
GFLOPS/W energy-efficiency which is 3.5X better than the best GPU
implementations in similar technologies. The minor increase in system-level
power and the negligible area increase make our PIM system a cost-effective and
energy efficient solution, easily scalable to 955 GFLOPS with a small network
of just four SMCs."
"We describe the computing tasks involved in autonomous driving, examine
existing autonomous driving computing platform implementations. To enable
autonomous driving, the computing stack needs to simultaneously provide high
performance, low power consumption, and low thermal dissipation, at low cost.
We discuss possible approaches to design computing platforms that will meet
these needs."
"Deep convolutional neural networks (CNN) have shown their good performances
in many computer vision tasks. However, the high computational complexity of
CNN involves a huge amount of data movements between the computational
processor core and memory hierarchy which occupies the major of the power
consumption. This paper presents Chain-NN, a novel energy-efficient 1D chain
architecture for accelerating deep CNNs. Chain-NN consists of the dedicated
dual-channel process engines (PE). In Chain-NN, convolutions are done by the 1D
systolic primitives composed of a group of adjacent PEs. These systolic
primitives, together with the proposed column-wise scan input pattern, can
fully reuse input operand to reduce the memory bandwidth requirement for energy
saving. Moreover, the 1D chain architecture allows the systolic primitives to
be easily reconfigured according to specific CNN parameters with fewer design
complexity. The synthesis and layout of Chain-NN is under TSMC 28nm process. It
costs 3751k logic gates and 352KB on-chip memory. The results show a 576-PE
Chain-NN can be scaled up to 700MHz. This achieves a peak throughput of
806.4GOPS with 567.5mW and is able to accelerate the five convolutional layers
in AlexNet at a frame rate of 326.2fps. 1421.0GOPS/W power efficiency is at
least 2.5 to 4.1x times better than the state-of-the-art works."
"Persistent memory provides high-performance data persistence at main memory.
Memory writes need to be performed in strict order to satisfy storage
consistency requirements and enable correct recovery from system crashes.
Unfortunately, adhering to such a strict order significantly degrades system
performance and persistent memory endurance. This paper introduces a new
mechanism, Loose-Ordering Consistency (LOC), that satisfies the ordering
requirements at significantly lower performance and endurance loss. LOC
consists of two key techniques. First, Eager Commit eliminates the need to
perform a persistent commit record write within a transaction. We do so by
ensuring that we can determine the status of all committed transactions during
recovery by storing necessary metadata information statically with blocks of
data written to memory. Second, Speculative Persistence relaxes the write
ordering between transactions by allowing writes to be speculatively written to
persistent memory. A speculative write is made visible to software only after
its associated transaction commits. To enable this, our mechanism supports the
tracking of committed transaction ID and multi-versioning in the CPU cache. Our
evaluations show that LOC reduces the average performance overhead of memory
persistence from 66.9% to 34.9% and the memory write traffic overhead from
17.1% to 3.4% on a variety of workloads."
"Three-dimensional (3D)-stacking technology, which enables the integration of
DRAM and logic dies, offers high bandwidth and low energy consumption. It also
empowers new memory designs for executing tasks not traditionally associated
with memories. A practical 3D-stacked memory is Hybrid Memory Cube (HMC), which
provides significant access bandwidth and low power consumption in a small
area. Although several studies have taken advantage of the novel architecture
of HMC, they have not fully explored either its characteristics in terms of
latency and bandwidth or their correlation with temperature and power
consumption. This paper is the first to characterize the thermal behavior of
HMC in a real environment using the AC-510 accelerator and to identify
temperature as a new limitation for this state-of-the-art design space.
Moreover, we deconstruct factors that contribute to access latency and reveal
their sources for high- and low-load accesses. The results and observations of
this work should provide constructive information for
high-performance-computing (HPC) research."
"In this paper, an optimized efficient VLSI architecture of a pipeline Fast
Fourier transform (FFT) processor capable of producing the reverse output order
sequence is presented. Paper presents Radix-2 multipath delay architecture for
FFT calculation. The implementation of FFT in hardware is very critical because
for calculation of FFT number of butterfly operations i.e. number of
multipliers requires due to which hardware gets increased means indirectly cost
of hardware is automatically gets increased. Also multiplier operations are
slow that's why it limits the speed of operation of architecture. The optimized
VLSI implementation of FFT algorithm is presented in this paper. Here
architecture is pipelined to optimize it and to increase the speed of
operation. Also to increase the speed of operation 2 levels parallel processing
is used."
"In order to overcome the branch execution penalties of hard-to-predict
instruction branches, two new instruction fetch micro-architectural methods are
proposed in this paper. In addition, to compare performance of the two proposed
methods, different instruction fetch policy schemes of existing multi-branch
path architectures are evaluated. An improvement in Instructions Per Cycle
(IPC) of 29.4% on average over single-thread execution with gshare branch
predictor on SPEC 2000/2006 benchmark is shown. In this paper, wide pipeline
machines are simulated for evaluation purposes. The methods discussed in this
paper can be extended to High Performance Scientific Computing needs, if the
demands of IPC improvement are far more critical than $cost."
"Memories that exploit three-dimensional (3D)-stacking technology, which
integrate memory and logic dies in a single stack, are becoming popular. These
memories, such as Hybrid Memory Cube (HMC), utilize a network-on-chip (NoC)
design for connecting their internal structural organizations. This novel usage
of NoC, in addition to aiding processing-in-memory capabilities, enables
numerous benefits such as high bandwidth and memory-level parallelism. However,
the implications of NoCs on the characteristics of 3D-stacked memories in terms
of memory access latency and bandwidth have not been fully explored. This paper
addresses this knowledge gap by (i) characterizing an HMC prototype on the
AC-510 accelerator board and revealing its access latency behaviors, and (ii)
by investigating the implications of such behaviors on system and software
designs."
"In the recent years image processing techniques are used as a tool to improve
detection and diagnostic capabilities in the medical applications. Medical
applications have been so much affected by these techniques which some of them
are embedded in medical instruments such as MRI, CT and other medical devices.
Among these techniques, medical image enhancement algorithms play an essential
role in removal of the noise which can be produced by medical instruments and
during image transfer. It has been proved that impulse noise is a major type of
noise, which is produced during medical operations, such as MRI, CT, and
angiography, by their image capturing devices. An embeddable hardware module
which is able to denoise medical images before and during surgical operations
could be very helpful. In this paper an accurate algorithm is proposed for
real-time removal of impulse noise in medical images. All image blocks are
divided into three categories of edge, smooth, and disordered areas. A
different reconstruction method is applied to each category of blocks for the
purpose of noise removal. The proposed method is tested on MR images.
Simulation results show acceptable denoising accuracy for various levels of
noise. Also an FPAG implementation of our denoising algorithm shows acceptable
hardware resource utilization. Hence, the algorithm is suitable for embedding
in medical hardware instruments such as radiosurgery devices."
"This paper presents a broad, pathfinding design space exploration of memory
management units (MMUs) for heterogeneous systems. We consider a variety of
designs, ranging from accelerators tightly coupled with CPUs (and using their
MMUs) to fully independent accelerators that have their own MMUs. We find that
regardless of the CPU-accelerator communication, accelerators should not rely
on the CPU MMU for any aspect of address translation, and instead must have its
own, local, fully-fledged MMU. That MMU, however, can and should be as
application-specific as the accelerator itself, as our data indicates that even
a 100% hit rate in a small, standard L1 Translation Lookaside Buffer (TLB)
presents a substantial accelerator performance overhead. Furthermore, we
isolate the benefits of individual MMU components (e.g., TLBs versus page table
walkers) and discover that their relative performance, area, and energy are
workload dependent, with their interplay resulting in different area-optimal
and energy-optimal configurations."
"Neural networks are an increasingly attractive algorithm for natural language
processing and pattern recognition applications. Deep networks with >50M
parameters made possible by modern GPU clusters operating at <50 pJ per op and
more recently, production accelerators capable of <5pJ per operation at the
board level. However, with the slowing of CMOS scaling, new paradigms will be
required to achieve the next several orders of magnitude in performance per
watt gains. Using an analog resistive memory (ReRAM) crossbar to perform key
matrix operations in an accelerator is an attractive option that is gaining
significant interest. This work presents a detailed design using a state of the
art 14/16 nm PDK for of an analog crossbar circuit block designed to process
three key kernels required in training and inference of neural networks. A
detailed circuit and device-level analysis of energy, latency, area, and
accuracy are given and compared to relevant designs using standard digital
ReRAM and SRAM operations. It is shown that the analog accelerator has a 310x
energy and 270x latency advantage over a similar block utilizing only digital
ReRAM and takes only 11 fJ per multiply and accumulate (MAC). Although training
accuracy is degraded in the analog accelerator, several options to improve this
are presented. The possible gains over a similar digital-only version of this
accelerator block suggest that continued optimization of analog resistive
memories is valuable. This detailed circuit and device analysis of a training
accelerator may serve as a foundation for further architecture-level studies."
"The problem of verifying multi-threaded execution against the memory
consistency model of a processor is known to be an NP hard problem. However
polynomial time algorithms exist that detect almost all failures in such
execution. These are often used in practice for microprocessor verification. We
present a low complexity and fully parallelized algorithm to check program
execution against the processor consistency model. In addition our algorithm is
general enough to support a number of consistency models without any
degradation in performance. An implementation of this algorithm is currently
used in practice to verify processors in the post silicon stage for multiple
architectures."
"Future nano-scale electronics built up from an Avogadro number of components
needs efficient, highly scalable, and robust means of communication in order to
be competitive with traditional silicon approaches. In recent years, the
Networks-on-Chip (NoC) paradigm emerged as a promising solution to interconnect
challenges in silicon-based electronics. Current NoC architectures are either
highly regular or fully customized, both of which represent implausible
assumptions for emerging bottom-up self-assembled molecular electronics that
are generally assumed to have a high degree of irregularity and imperfection.
Here, we pragmatically and experimentally investigate important design
trade-offs and properties of an irregular, abstract, yet physically plausible
3D small-world interconnect fabric that is inspired by modern network-on-chip
paradigms. We vary the framework's key parameters, such as the connectivity,
the number of switch nodes, the distribution of long- versus short-range
connections, and measure the network's relevant communication characteristics.
We further explore the robustness against link failures and the ability and
efficiency to solve a simple toy problem, the synchronization task. The results
confirm that (1) computation in irregular assemblies is a promising and
disruptive computing paradigm for self-assembled nano-scale electronics and (2)
that 3D small-world interconnect fabrics with a power-law decaying distribution
of shortcut lengths are physically plausible and have major advantages over
local 2D and 3D regular topologies."
"This paper presents a methodology to efficiently explore the design space of
communication adapters. In most digital signal processing (DSP) applications,
the overall performance of the system is significantly affected by
communication architectures, as a consequence the designers need specifically
optimized adapters. By explicitly modeling these communications within an
effective graph-theoretic model and analysis framework, we automatically
generate an optimized architecture, named Space-Time AdapteR (STAR). Our design
flow inputs a C description of Input/Output data scheduling, and user
requirements (throughput, latency, parallelism...), and formalizes
communication constraints through a Resource Constraints Graph (RCG). Design
space exploration is then performed through associated tools, to synthesize a
STAR component under time-to-market constraints. The proposed approach has been
tested to design an industrial data mixing block example: an Ultra-Wideband
interleaver."
"We propose a special computational device which uses light rays for solving
the subset-sum problem. The device has a graph-like representation and the
light is traversing it by following the routes given by the connections between
nodes. The nodes are connected by arcs in a special way which lets us to
generate all possible subsets of the given set. To each arc we assign either a
number from the given set or a predefined constant. When the light is passing
through an arc it is delayed by the amount of time indicated by the number
placed in that arc. At the destination node we will check if there is a ray
whose total delay is equal to the target value of the subset sum problem (plus
some constants)."
"Although prone to fabrication error, the nanowire crossbar is a promising
candidate component for next generation nanometer-scale circuits. In the
nanowire crossbar architecture, nanowires are addressed by controlling voltages
on the mesowires. For area efficiency, we are interested in the maximum number
of nanowires $N(m,e)$ that can be addressed by $m$ mesowires, in the face of up
to $e$ fabrication errors. Asymptotically tight bounds on $N(m,e)$ are
established in this paper. In particular, it is shown that $N(m,e) = \Theta(2^m
/ m^{e+1/2})$. Interesting observations are made on the equivalence between
this problem and the problem of constructing optimal EC/AUED codes,
superimposed distance codes, pooling designs, and diffbounded set systems.
Results in this paper also improve upon those in the EC/AUEC codes literature."
"For high throughput applications, turbo-like iterative decoders are
implemented with parallel architectures. However, to be efficient parallel
architectures require to avoid collision accesses i.e. concurrent read/write
accesses should not target the same memory block. This consideration applies to
the two main classes of turbo-like codes which are Low Density Parity Check
(LDPC) and Turbo-Codes. In this paper we propose a methodology which finds a
collision-free mapping of the variables in the memory banks and which optimizes
the resulting interleaving architecture. Finally, we show through a pedagogical
example the interest of our approach compared to state-of-the-art techniques."
"We present a mixed analog-digital spectrum sensing method that is especially
suited to the typical wideband setting of cognitive radio (CR). The advantages
of our system with respect to current architectures are threefold. First, our
analog front-end is fixed and does not involve scanning hardware. Second, both
the analog-to-digital conversion (ADC) and the digital signal processing (DSP)
rates are substantially below Nyquist. Finally, the sensing resources are
shared with the reception path of the CR, so that the lowrate streaming samples
can be used for communication purposes of the device, besides the sensing
functionality they provide. Combining these advantages leads to a real time map
of the spectrum with minimal use of mobile resources. Our approach is based on
the modulated wideband converter (MWC) system, which samples sparse wideband
inputs at sub-Nyquist rates. We report on results of hardware experiments,
conducted on an MWC prototype circuit, which affirm fast and accurate spectrum
sensing in parallel to CR communication."
"Polar codes provably achieve the symmetric capacity of a memoryless channel
while having an explicit construction. This work aims to increase the
throughput of polar decoder hardware by an order of magnitude relative to the
state of the art successive-cancellation decoder. We present an algorithm,
architecture, and FPGA implementation of a gigabit-per-second polar decoder."
"Independent of the technology, it is generally expected that future nanoscale
devices will be built from vast numbers of densely arranged devices that
exhibit high failure rates. Other than that, there is little consensus on what
type of technology and computing architecture holds most promises to go far
beyond today's top-down engineered silicon devices. Cellular automata (CA) have
been proposed in the past as a possible class of architectures to the von
Neumann computing architecture, which is not generally well suited for future
parallel and fine-grained nanoscale electronics. While the top-down engineered
semi-conducting technology favors regular and locally interconnected
structures, future bottom-up self-assembled devices tend to have irregular
structures because of the current lack precise control over these processes. In
this paper, we will assess random dynamical networks, namely Random Boolean
Networks (RBNs) and Random Threshold Networks (RTNs), as alternative computing
architectures and models for future information processing devices. We will
illustrate that--from a theoretical perspective--they offer superior properties
over classical CA-based architectures, such as inherent robustness as the
system scales up, more efficient information processing capabilities, and
manufacturing benefits for bottom-up designed devices, which motivates this
investigation. We will present recent results on the dynamic behavior and
robustness of such random dynamical networks while also including manufacturing
issues in the assessment."
"BCH (Bose-Chaudhuri-Hocquenghen) error correcting codes ([1]-[2]) are now
widely used in communication systems and digital technology. Direct LFSR(linear
feedback shifted register)-based encoding of a long BCH code suffers from
serial-in and serial-out limitation and large fanout effect of some XOR gates.
This makes the LFSR-based encoders of long BCH codes cannot keep up with the
data transmission speed in some applications. Several parallel long parallel
encoders for long cyclic codes have been proposed in [3]-[8]. The technique for
eliminating the large fanout effect by J-unfolding method and some algebraic
manipulation was presented in [7] and [8] . In this paper we propose a
CRT(Chinese Remainder Theorem)-based parallel architecture for long BCH
encoding. Our novel technique can be used to eliminate the fanout bottleneck.
The only restriction on the speed of long BCH encoding of our CRT-based
architecture is $log_2N$, where $N$ is the length of the BCH code."
"The recently-discovered polar codes are widely seen as a major breakthrough
in coding theory. These codes achieve the capacity of many important channels
under successive cancellation decoding. Motivated by the rapid progress in the
theory of polar codes, we propose a family of architectures for efficient
hardware implementation of successive cancellation decoders. We show that such
decoders can be implemented with O(n) processing elements and O(n) memory
elements, while providing constant throughput. We also propose a technique for
overlapping the decoding of several consecutive codewords, thereby achieving a
significant speed-up factor. We furthermore show that successive cancellation
decoding can be implemented in the logarithmic domain, thereby eliminating the
multiplication and division operations and greatly reducing the complexity of
each processing element."
"This paper describes the architecture, the development and the implementation
of Janus II, a new generation application-driven number cruncher optimized for
Monte Carlo simulations of spin systems (mainly spin glasses). This domain of
computational physics is a recognized grand challenge of high-performance
computing: the resources necessary to study in detail theoretical models that
can make contact with experimental data are by far beyond those available using
commodity computer systems. On the other hand, several specific features of the
associated algorithms suggest that unconventional computer architectures, which
can be implemented with available electronics technologies, may lead to order
of magnitude increases in performance, reducing to acceptable values on human
scales the time needed to carry out simulation campaigns that would take
centuries on commercially available machines. Janus II is one such machine,
recently developed and commissioned, that builds upon and improves on the
successful JANUS machine, which has been used for physics since 2008 and is
still in operation today. This paper describes in detail the motivations behind
the project, the computational requirements, the architecture and the
implementation of this new machine and compares its expected performances with
those of currently available commercial systems."
"Polar codes are the first error-correcting codes to provably achieve the
channel capacity but with infinite codelengths. For finite codelengths the
existing decoder architectures are limited in working frequency by the partial
sums computation unit. We explain in this paper how the partial sums
computation can be seen as a matrix multiplication. Then, an efficient hardware
implementation of this product is investigated. It has reduced logic resources
and interconnections. Formalized architectures, to compute partial sums and to
generate the bits of the generator matrix k^n, are presented. The proposed
architecture allows removing the multiplexing resources used to assigned to
each processing elements the required partial sums."
"Long polar codes can achieve the symmetric capacity of arbitrary binary-input
discrete memoryless channels under a low complexity successive cancelation (SC)
decoding algorithm. However, for polar codes with short and moderate code
length, the decoding performance of the SC algorithm is inferior. The cyclic
redundancy check (CRC) aided successive cancelation list (SCL) decoding
algorithm has better error performance than the SC algorithm for short or
moderate polar codes. In this paper, we propose an efficient list decoder
architecture for the CRC aided SCL algorithm, based on both algorithmic
reformulations and architectural techniques. In particular, an area efficient
message memory architecture is proposed to reduce the area of the proposed
decoder architecture. An efficient path pruning unit suitable for large list
size is also proposed. For a polar code of length 1024 and rate $\frac{1}{2}$,
when list size $L=2$ and 4, the proposed list decoder architecture is
implemented under a TSMC 90nm CMOS technology. Compared with the list decoders
in the literature, our decoder achieves 1.33 to 1.96 times hardware efficiency."
"Off-chip buses account for a significant portion of the total system power
consumed in embedded systems. Bus encoding schemes have been proposed to
minimize power dissipation, but none has been demonstrated to be optimal with
respect to any measure. In this paper, we give the first provably optimal and
explicit (polynomial-time constructible) families of memoryless codes for
minimizing bit transitions in off-chip buses. Our results imply that having
access to a clock does not make a memoryless encoding scheme that minimizes bit
transitions more powerful."
"This paper describes the design and simulation of an 8-bit dedicated
processor for calculating the Sine and Cosine of an Angle using CORDIC
Algorithm (COordinate Rotation DIgital Computer), a simple and efficient
algorithm to calculate hyperbolic and trigonometric functions. We have proposed
a dedicated processor system, modeled by writing appropriate programs in VHDL,
for calculating the Sine and Cosine of an angle. System simulation was carried
out using ModelSim 6.3f and Xilinx ISE Design Suite 12.3. A maximum frequency
of 81.353 MHz was reached with a minimum period of 12.292 ns. 126 (3%) slices
were used. This paper attempts to survey the existing CORDIC algorithm with an
eye towards implementation in Field Programmable Gate Arrays (FPGAs). A brief
description of the theory behind the algorithm and the derivation of the Sine
and Cosine of an angle using the CORDIC algorithm has been presented. The
system can be implemented using Spartan3 XC3S400 with Xilinx ISE 12.3 and VHDL."
"The recently-discovered polar codes are seen as a major breakthrough in
coding theory; they provably achieve the theoretical capacity of discrete
memoryless channels using the low complexity successive cancellation (SC)
decoding algorithm. Motivated by recent developments in polar coding theory, we
propose a family of efficient hardware implementations for SC polar decoders.
We show that such decoders can be implemented with O(n) processing elements,
O(n) memory elements, and can provide a constant throughput for a given target
clock frequency. Furthermore, we show that SC decoding can be implemented in
the logarithm domain, thereby eliminating costly multiplication and division
operations and reducing the complexity of each processing element greatly. We
also present a detailed architecture for an SC decoder and provide logic
synthesis results confirming the linear growth in complexity of the decoder as
the code length increases."
"The importance of embedded applications on image and video
processing,communication and cryptography domain has been taking a larger space
in current research era. Improvement of pictorial information for betterment of
human perception like deblurring, de-noising in several fields such as
satellite imaging, medical imaging etc are renewed research thrust.
Specifically we would like to elaborate our experience on the significance of
computer vision as one of the domains where hardware implemented algorithms
perform far better than those implemented through software. So far embedded
design engineers have successfully implemented their designs by means of
Application Specific Integrated Circuits (ASICs) and/or Digital Signal
Processors (DSP), however with the advancement of VLSI technology a very
powerful hardware device namely the Field Programmable Gate Array (FPGA)
combining the key advantages of ASICs and DSPs was developed which have the
possibility of reprogramming making them a very attractive device for rapid
prototyping.Communication of image and video data in multiple FPGA is no longer
far away from the thrust of secured transmission among them, and then the
relevance of cryptography is indeed unavoidable. This paper shows how the
Xilinx hardware development platform as well Mathworks Matlab can be used to
develop hardware based computer vision algorithms and its corresponding crypto
transmission channel between multiple FPGA platform from a system level
approach, making it favourable for developing a hardware-software co-design
environment."
"Modern Graphics Processing Units (GPUs) are now considered accelerators for
general purpose computation. A tight interaction between the GPU and the
interconnection network is the strategy to express the full potential on
capability computing of a multi-GPU system on large HPC clusters; that is the
reason why an efficient and scalable interconnect is a key technology to
finally deliver GPUs for scientific HPC. In this paper we show the latest
architectural and performance improvement of the APEnet+ network fabric, a
FPGA-based PCIe board with 6 fully bidirectional off-board links with 34 Gbps
of raw bandwidth per direction, and X8 Gen2 bandwidth towards the host PC. The
board implements a Remote Direct Memory Access (RDMA) protocol that leverages
upon peer-to-peer (P2P) capabilities of Fermi- and Kepler-class NVIDIA GPUs to
obtain real zero-copy, low-latency GPU-to-GPU transfers. Finally, we report on
the development activities for 2013 focusing on the adoption of the latest
generation 28 nm FPGAs and the preliminary tests performed on this new
platform."
"Two multiplierless algorithms are proposed for 4x4 approximate-DCT for
transform coding in digital video. Computational architectures for 1-D/2-D
realisations are implemented using Xilinx FPGA devices. CMOS synthesis at the
45 nm node indicate real-time operation at 1 GHz yielding 4x4 block rates of
125 MHz at less than 120 mW of dynamic power consumption."
"This volume contains the papers accepted at the First International Workshop
on FPGAs for Software Programmers (FSP 2014), held in Munich, Germany,
September 1st, 2014. FSP 2014 was co-located with the International Conference
on Field Programmable Logic and Applications (FPL)."
"We focus on the metric sorter unit of successive cancellation list decoders
for polar codes, which lies on the critical path in all current hardware
implementations of the decoder. We review existing metric sorter architectures
and we propose two new architectures that exploit the structure of the path
metrics in a log-likelihood ratio based formulation of successive cancellation
list decoding. Our synthesis results show that, for the list size of $L=32$,
our first proposed sorter is $14\%$ faster and $45\%$ smaller than existing
sorters, while for smaller list sizes, our second sorter has a higher delay in
return for up to $36\%$ reduction in the area."
"In the construction of exascale computing systems energy efficiency and power
consumption are two of the major challenges. Low-power high performance
embedded systems are of increasing interest as building blocks for large scale
high- performance systems. However, extracting maximum performance out of such
systems presents many challenges. Various aspects from the hardware
architecture to the programming models used need to be explored. The Epiphany
architecture integrates low-power RISC cores on a 2D mesh network and promises
up to 70 GFLOPS/Watt of processing efficiency. However, with just 32 KB of
memory per eCore for storing both data and code, and only low level inter-core
communication support, programming the Epiphany system presents several
challenges. In this paper we evaluate the performance of the Epiphany system
for a variety of basic compute and communication operations. Guided by this
data we explore strategies for implementing scientific applications on memory
constrained low-powered devices such as the Epiphany. With future systems
expected to house thousands of cores in a single chip, the merits of such
architectures as a path to exascale is compared to other competing systems."
"This volume contains the papers accepted at the DATE Friday Workshop on
Heterogeneous Architectures and Design Methods for Embedded Image Systems (HIS
2015), held in Grenoble, France, March 13, 2015. HIS 2015 was co-located with
the Conference on Design, Automation and Test in Europe (DATE)."
"We propose without loss of generality strategies to achieve a high-throughput
FPGA-based architecture for a QC-LDPC code based on a circulant-1 identity
matrix construction. We present a novel representation of the parity-check
matrix (PCM) providing a multi-fold throughput gain. Splitting of the node
processing algorithm enables us to achieve pipelining of blocks and hence
layers. By partitioning the PCM into not only layers but superlayers we derive
an upper bound on the pipelining depth for the compact representation. To
validate the architecture, a decoder for the IEEE 802.11n (2012) QC-LDPC is
implemented on the Xilinx Kintex-7 FPGA with the help of the FPGA IP compiler
[2] available in the NI LabVIEW Communication System Design Suite (CSDS) which
offers an automated and systematic compilation flow where an optimized hardware
implementation from the LDPC algorithm was generated in approximately 3
minutes, achieving an overall throughput of 608Mb/s (at 260MHz). As per our
knowledge this is the fastest implementation of the IEEE 802.11n QC-LDPC
decoder using an algorithmic compiler."
"We consider the problem of constructing fast and small binary adder circuits.
Among widely-used adders, the Kogge-Stone adder is often considered the
fastest, because it computes the carry bits for two $n$-bit numbers (where $n$
is a power of two) with a depth of $2\log_2 n$ logic gates, size $4 n\log_2 n$,
and all fan-outs bounded by two. Fan-outs of more than two are avoided, because
they lead to the insertion of repeaters for repowering the signal and
additional depth in the physical implementation. However, the depth bound of
the Kogge-Stone adder is off by a factor of two from the lower bound of $\log_2
n$. This bound is achieved asymptotically in two separate constructions by
Brent and Krapchenko. Brent's construction gives neither a bound on the fan-out
nor the size, while Krapchenko's adder has linear size, but can have up to
linear fan-out. With a fan-out bound of two, neither construction achieves a
depth of less than $2 \log_2 n$. In a further approach, Brent and Kung proposed
an adder with linear size and fan-out two, but twice the depth of the
Kogge-Stone adder. These results are 33-43 years old and no substantial
theoretical improvement for has been made since then.
  In this paper we integrate the individual advantages of all previous adder
circuits into a new family of full adders, the first to improve on the depth
bound of $2\log_2 n$ while maintaining a fan-out bound of two. Our adders
achieve an asymptotically optimum logic gate depth of $\log_2 n + o(\log_2 n)$
and linear size $\mathcal {O}(n)$."
"This volume contains the papers accepted at the Second International Workshop
on FPGAs for Software Programmers (FSP 2015), held in London, United Kingdom,
September 1st, 2015. FSP 2015 was co-located with the International Conference
on Field Programmable Logic and Applications (FPL)."
"In recent years, robots are required to be autonomous and their robotic
software are sophisticated. Robots have a problem of insufficient performance,
since it cannot equip with a high-performance microprocessor due to
battery-power operation. On the other hand, FPGA devices can accelerate
specific functions in a robot system without increasing power consumption by
implementing customized circuits. But it is difficult to introduce FPGA devices
into a robot due to large development cost of an FPGA circuit compared to
software. Therefore, in this study, we propose an FPGA component technology for
an easy integration of an FPGA into robots, which is compliant with ROS (Robot
Operating System). As a case study, we designed ROS-compliant FPGA component of
image labeling using Xilinx Zynq platform. The developed ROS-component FPGA
component performs 1.7 times faster compared to the ordinary ROS software
component."
"Stream computation is one of the approaches suitable for FPGA-based custom
computing due to its high throughput capability brought by pipelining with
regular memory access. To increase performance of iterative stream computation,
we can exploit both temporal and spatial parallelism by deepening and
duplicating pipelines, respectively. However, the performance is constrained by
several factors including available hardware resources on FPGA, an external
memory bandwidth, and utilization of pipeline stages, and therefore we need to
find the best mix of the different parallelism to achieve the highest
performance per power. In this paper, we present a domain-specific language
(DSL) based design space exploration for temporally and/or spatially parallel
stream computation with FPGA. We define a DSL where we can easily design a
hierarchical structure of parallel stream computation with abstract description
of computation. For iterative stream computation of fluid dynamics simulation,
we design hardware structures with a different mix of the temporal and spatial
parallelism. By measuring the performance and the power consumption, we find
the best among them."
"Successive-cancellation list (SCL) decoding is an algorithm that provides
very good error-correction performance for polar codes. However, its hardware
implementation requires a large amount of memory, mainly to store intermediate
results. In this paper, a partitioned SCL algorithm is proposed to reduce the
large memory requirements of the conventional SCL algorithm. The decoder tree
is broken into partitions that are decoded separately. We show that with
careful selection of list sizes and number of partitions, the proposed
algorithm can outperform conventional SCL while requiring less memory."
"This paper presents the usage of the Reed Solomon Codes as the Forward Error
Correction (FEC) unit of the Hybrid Automatic Repeat Request (ARQ) methods.
Parametric and flexible FPGA implementation details of such Erasure-Only RS
decoders with high symbol lengths (e.g. GF(2^32)) have been presented. The
design is based on the GF(2m) multiplier logic core operating at a single clock
cycle, where the resource utilization and throughput are both directly
proportional to the number of these cores. For a fixed implementation, the
throughput inversely decreases with the number of erasures to be corrected.
Implementation in Zynq7020 SoC device of an example GF(2^32)-RS Decoder capable
of correcting 64-erasures with a single multiplier resulted in 1641-LUTs and
188-FFs achieving 15Mbps, whereas the design with 8 multipliers resulted in
6128-LUTs and 628-FFs achieving 100Mbps."
"Promoting energy efficiency to a first class system design goal is an
important research challenge. Although more energy-efficient hardware can be
designed, it is software that controls the hardware; for a given system the
potential for energy savings is likely to be much greater at the higher levels
of abstraction in the system stack. Thus the greatest savings are expected from
energy-aware software development, which is the vision of the EU ENTRA project.
This article presents the concept of energy transparency as a foundation for
energy-aware software development. We show how energy modelling of hardware is
combined with static analysis to allow the programmer to understand the energy
consumption of a program without executing it, thus enabling exploration of the
design space taking energy into consideration. The paper concludes by
summarising the current and future challenges identified in the ENTRA project."
"Convolutional neural networks (CNNs) have revolutionized the world of
computer vision over the last few years, pushing image classification beyond
human accuracy. The computational effort of today's CNNs requires power-hungry
parallel processors or GP-GPUs. Recent developments in CNN accelerators for
system-on-chip integration have reduced energy consumption significantly.
Unfortunately, even these highly optimized devices are above the power envelope
imposed by mobile and deeply embedded applications and face hard limitations
caused by CNN weight I/O and storage. This prevents the adoption of CNNs in
future ultra-low power Internet of Things end-nodes for near-sensor analytics.
Recent algorithmic and theoretical advancements enable competitive
classification accuracy even when limiting CNNs to binary (+1/-1) weights
during training. These new findings bring major optimization opportunities in
the arithmetic core by removing the need for expensive multiplications, as well
as reducing I/O bandwidth and storage. In this work, we present an accelerator
optimized for binary-weight CNNs that achieves 1510 GOp/s at 1.2 V on a core
area of only 1.33 MGE (Million Gate Equivalent) or 0.19 mm$^2$ and with a power
dissipation of 895 {\mu}W in UMC 65 nm technology at 0.6 V. Our accelerator
significantly outperforms the state-of-the-art in terms of energy and area
efficiency achieving 61.2 TOp/s/W@0.6 V and 1135 GOp/s/MGE@1.2 V, respectively."
"Near-data processing (NDP) refers to augmenting memory or storage with
processing power. Despite its potential for acceleration computing and reducing
power requirements, only limited progress has been made in popularizing NDP for
various reasons. Recently, two major changes have occurred that have ignited
renewed interest and caused a resurgence of NDP. The first is the success of
machine learning (ML), which often demands a great deal of computation for
training, requiring frequent transfers of big data. The second is the
popularity of NAND flash-based solid-state drives (SSDs) containing multicore
processors that can accommodate extra computation for data processing. In this
paper, we evaluate the potential of NDP for ML using a new SSD platform that
allows us to simulate instorage processing (ISP) of ML workloads. Our platform
(named ISP-ML) is a full-fledged simulator of a realistic multi-channel SSD
that can execute various ML algorithms using data stored in the SSD. To conduct
a thorough performance analysis and an in-depth comparison with alternative
techniques, we focus on a specific algorithm: stochastic gradient descent
(SGD), which is the de facto standard for training differentiable models such
as logistic regression and neural networks. We implement and compare three SGD
variants (synchronous, Downpour, and elastic averaging) using ISP-ML,
exploiting the multiple NAND channels to parallelize SGD. In addition, we
compare the performance of ISP and that of conventional in-host processing,
revealing the advantages of ISP. Based on the advantages and limitations
identified through our experiments, we further discuss directions for future
research on ISP for accelerating ML."
"Powerful Forward Error Correction (FEC) schemes are used in optical
communications to achieve bit-error rates below $10^{-15}$. These FECs follow
one of two approaches: concatenation of simpler hard-decision codes or usage of
inherently powerful soft-decision codes. The first approach yields lower Net
Coding Gains (NCGs), but can usually work at higher code rates and have lower
complexity decoders. In this work, we propose a novel FEC scheme based on a
product code and a post-processing technique. It can achieve an NCG of 9.52~dB
at a BER of $10^{-15}$ and 9.96~dB at a BER of $10^{-18}$, an error-correction
performance that sits between that of current hard-decision and soft-decision
FECs. A decoder architecture is designed, tested on FPGA and synthesized in 65
nm CMOS technology: its 164 bits/cycle worst-case information throughput can
reach 100 Gb/s at the achieved frequency of 609~MHz. Its complexity is shown to
be lower than that of hard-decision decoders in literature, and an order of
magnitude lower than the estimated complexity of soft-decision decoders."
"Flexibility at hardware level is the main driving force behind adaptive
systems whose aim is to realise microarhitecture deconfiguration 'online'. This
feature allows the software/hardware stack to tolerate drastic changes of the
workload in data centres. With emerge of FPGA reconfigurablity this technology
is becoming a mainstream computing paradigm. Adaptivity is usually accompanied
by the high-level tools to facilitate multi-dimensional space exploration. An
essential aspect in this space is memory orchestration where on-chip and
off-chip memory distribution significantly influences the architecture in
coping with the critical spatial and timing constraints, e.g. Place and Route.
This paper proposes a memory smart technique for a particular class of adaptive
systems: Elastic Circuits which enjoy slack elasticity at fine level of
granularity. We explore retiming of a set of popular benchmarks via
investigating the memory distribution within and among accelerators. The area,
performance and power patterns are adopted by our high-level synthesis
framework, with respect to the behaviour of the input descriptions, to improve
the quality of the synthesised elastic circuits."
"An ultra-high throughput low-density parity check (LDPC) decoder with an
unrolled full-parallel architecture is proposed, which achieves the highest
decoding throughput compared to previously reported LDPC decoders in
literature. The decoder benefits from a serial message-transfer approach
between the decoding stages to alleviate the well-known routing congestion
problem in parallel LDPC decoders. Furthermore, a finite-alphabet message
passing algorithm is employed to replace the variable node update rule of
standard min-sum decoders with optimized look-up tables, designed in a way that
maximize the mutual information between decoding messages. The proposed
algorithm results in an architecture with reduced bit-width messages, leading
to a significantly higher decoding throughput and to a lower area as compared
to a min-sum decoder when serial message-transfer is used. The architecture is
placed and routed for the standard min-sum reference decoder and for the
proposed finite-alphabet decoder using a custom pseudo-hierarchical backend
design strategy to further alleviate routing congestions and to handle the
large design. Post-layout results show that the finite-alphabet decoder with
the serial message-transfer architecture achieves a throughput as large as 594
Gbps with an area of 16.2 mm$^2$ and dissipates an average power of 22.7 pJ per
decoded bit in a 28 nm FD-SOI library. Compared to the reference min-sum
decoder, this corresponds to 3.3 times smaller area and 2 times better energy
efficiency."
"Many architects believe that major improvements in cost-energy-performance
must now come from domain-specific hardware. This paper evaluates a custom
ASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since
2015 that accelerates the inference phase of neural networks (NN). The heart of
the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak
throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed
on-chip memory. The TPU's deterministic execution model is a better match to
the 99th-percentile response-time requirement of our NN applications than are
the time-varying optimizations of CPUs and GPUs (caches, out-of-order
execution, multithreading, multiprocessing, prefetching, ...) that help average
throughput more than guaranteed latency. The lack of such features helps
explain why, despite having myriad MACs and a big memory, the TPU is relatively
small and low power. We compare the TPU to a server-class Intel Haswell CPU and
an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters.
Our workload, written in the high-level TensorFlow framework, uses production
NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters'
NN inference demand. Despite low utilization for some applications, the TPU is
on average about 15X - 30X faster than its contemporary GPU or CPU, with
TOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the
TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and
200X the CPU."
"Achieving strong real-time guarantees in multi-core platforms is challenging
due to the extensive hardware resource sharing in the memory hierarchy. Modern
platforms and OS's, however, provide no means to appropriately handle memory
regions that are crucial for real-time performance. In this paper, we propose a
new OS-level abstraction, namely Deterministic Memory, to define memory regions
that are specially handled by the OS and the hardware to exhibit strong
real-time guarantees.
  We show that the deterministic memory abstraction can be introduced in the OS
at the granularity of single memory pages by exploiting existing hardware
support. When deterministic memory pages are accessed, the attribute is
propagated through all the levels of the memory hierarchy. Clearly, the
hardware needs to be designed to ensure real-time handling of deterministic
memory requests. To illustrate the potentialities of the new abstraction, we
also propose a novel design for a shared cache controller that takes advantage
of deterministic memory. Minimum cache space is guaranteed for deterministic
memory, while unused cache space is left available to non-real-time
applications.
  We implemented OS support for deterministic memory in the Linux kernel; and
we evaluated the proposed hardware modifications in a cycle-accurate
full-system simulator. We study the effectiveness of our approach on a set of
synthetic and real benchmarks. Results show that it is possible to achieve (i)
temporal determinism as strong as with traditional way-based cache
partitioning; and (ii) giving 50% of the private partition space, on average,
to the non-real-time applications."
"An algebraic integer (AI) based time-multiplexed row-parallel architecture
and two final-reconstruction step (FRS) algorithms are proposed for the
implementation of bivariate AI-encoded 2-D discrete cosine transform (DCT). The
architecture directly realizes an error-free 2-D DCT without using FRSs between
row-column transforms, leading to an 8$\times$8 2-D DCT which is entirely free
of quantization errors in AI basis. As a result, the user-selectable accuracy
for each of the coefficients in the FRS facilitates each of the 64 coefficients
to have its precision set independently of others, avoiding the leakage of
quantization noise between channels as is the case for published DCT designs.
The proposed FRS uses two approaches based on (i) optimized Dempster-Macleod
multipliers and (ii) expansion factor scaling. This architecture enables
low-noise high-dynamic range applications in digital video processing that
requires full control of the finite-precision computation of the 2-D DCT. The
proposed architectures and FRS techniques are experimentally verified and
validated using hardware implementations that are physically realized and
verified on FPGA chip. Six designs, for 4- and 8-bit input word sizes, using
the two proposed FRS schemes, have been designed, simulated, physically
implemented and measured. The maximum clock rate and block-rate achieved among
8-bit input designs are 307.787 MHz and 38.47 MHz, respectively, implying a
pixel rate of 8$\times$307.787$\approx$2.462 GHz if eventually embedded in a
real-time video-processing system. The equivalent frame rate is about 1187.35
Hz for the image size of 1920$\times$1080. All implementations are functional
on a Xilinx Virtex-6 XC6VLX240T FPGA device."