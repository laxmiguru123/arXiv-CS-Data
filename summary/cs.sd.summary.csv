summary
"While traditional implementations of variable-length digital delay lines are
based on a circular buffer accessed by two pointers, we propose an
implementation where a single fractional pointer is used both for read and
write operations. On modern general-purpose architectures, the proposed method
is nearly as efficient as the popularinterpolated circular buffer, and it
behaves well for delay-length modulations commonly found in digital audio
effects. The physical interpretation of the new implementation shows that it is
suitable for simulating tension or density modulations in wave-propagating
media."
"Computing practice today depends on visual output to drive almost all user
interaction. Other senses, such as audition, may be totally neglected, or used
tangentially, or used in highly restricted specialized ways. We have excellent
audio rendering through D-A conversion, but we lack rich general facilities for
modeling and manipulating sound comparable in quality and flexibility to
graphics. We need co-ordinated research in several disciplines to improve the
use of sound as an interactive information channel.
  Incremental and separate improvements in synthesis, analysis, speech
processing, audiology, acoustics, music, etc. will not alone produce the
radical progress that we seek in sonic practice. We also need to create a new
central topic of study in digital audio research. The new topic will assimilate
the contributions of different disciplines on a common foundation. The key
central concept that we lack is sound as a general-purpose information channel.
We must investigate the structure of this information channel, which is driven
by the co-operative development of auditory perception and physical sound
production. Particular audible encodings, such as speech and music, illuminate
sonic information by example, but they are no more sufficient for a
characterization than typography is sufficient for a characterization of visual
information."
"A rectangular enclosure has such an even distribution of resonances that it
can be accurately and efficiently modelled using a feedback delay network.
Conversely, a non rectangular shape such as a sphere has a distribution of
resonances that challenges the construction of an efficient model. This work
proposes an extension of the already known feedback delay network structure to
model the resonant properties of a sphere. A specific frequency distribution of
resonances can be approximated, up to a certain frequency, by inserting an
allpass filter of moderate order after each delay line of a feedback delay
network. The structure used for rectangular boxes is therefore augmented with a
set of allpass filters allowing parametric control over the enclosure size and
the boundary properties. This work was motivated by informal listening tests
which have shown that it is possible to identify a basic shape just from the
distribution of its audible resonances."
"This paper describes a method for decomposing steady-state instrument data
into excitation and formant filter components. The input data, taken from
several series of recordings of acoustical instruments is analyzed in the
frequency domain, and for each series a model is built, which most accurately
represents the data as a source-filter system. The source part is taken to be a
harmonic excitation system with frequency-invariant magnitudes, and the filter
part is considered to be responsible for all spectral inhomogenieties. This
method has been applied to the SHARC database of steady state instrument data
to create source-filter models for a large number of acoustical instruments.
Subsequent use of such models can have a wide variety of applications,
including improvements to wavetable and physical modeling synthesis, high
quality pitch shifting, and creation of ""hybrid"" instrument timbres."
"Modal synthesis is an important area of physical modeling whose exploration
in the past has been held back by a large number of control parameters, the
scarcity of general-purpose design tools and the difficulty of obtaining the
computational power required for real-time synthesis. This paper presents an
overview of a flexible software framework facilitating the design and control
of instruments based on modal synthesis. The framework is designed as a
hierarchy of polymorphic synthesis objects, representing modal structures of
various complexity. As a method of generalizing all interactions among the
elements of a modal system, an abstract notion of {\it energy} is introduced,
and a set of energy transfer functions is provided. Such abstraction leads to a
design where the dynamics of interactions can be largely separated from the
specifics of particular modal structures, yielding an easily configurable and
expandable system. A real-time version of the framework has been implemented as
a set of C++ classes along with an integrating shell and a GUI, and is
currently being used to design and play modal instruments, as well as to survey
fundamental properties of various modal algorithms."
"In this paper, a method of pitch tracking based on variance minimization of
locally periodic subsamples of an acoustic signal is presented. Replicates
along the length of the periodically sampled data of the signal vector are
taken and locally averaged sample variances are minimized to estimate the
fundamental frequency. Using this method, pitch tracking of any text
independent voiced signal is possible for different speakers."
"This paper describes an experimental system designed for development of real
time voice synthesis applications. The system is composed from a DSP
coprocessor card, equipped with an TMS320C25 or TMS320C50 chip, voice
acquisition module (ADDA2),host computer (IBM-PC compatible), software specific
tools."
"An effective way to increase the noise robustness of automatic speech
recognition is to label noisy speech features as either reliable or unreliable
(missing) prior to decoding, and to replace the missing ones by clean speech
estimates. We present a novel method to obtain such clean speech estimates.
Unlike previous imputation frameworks which work on a frame-by-frame basis, our
method focuses on exploiting information from a large time-context. Using a
sliding window approach, denoised speech representations are constructed using
a sparse representation of the reliable features in an overcomplete basis of
fixed-length exemplar fragments. We demonstrate the potential of our approach
with experiments on the AURORA-2 connected digit database."
"In this paper, we are presenting a new model for interactive music. Unlike
most interactive systems, our model is based on file organization, but does not
require digital audio treatments. This model includes a definition of a
constraints system and its solver. The products of this project are intended
for the general public, inexperienced users, as well as professional musicians,
and will be distributed commercially. We are here presenting three products of
this project. The difficulty of this project is to design a technology and
software products for interactive music which must be easy to use by the
general public and by professional composers."
This paper has been withdrawn by the author ali pourmohammad.
"In this paper, an improved strategy for automated text dependent speaker
identification system has been proposed in noisy environment. The
identification process incorporates the Neuro- Genetic hybrid algorithm with
cepstral based features. To remove the background noise from the source
utterance, wiener filter has been used. Different speech pre-processing
techniques such as start-end point detection algorithm, pre-emphasis filtering,
frame blocking and windowing have been used to process the speech utterances.
RCC, MFCC, MFCC, MFCC, LPC and LPCC have been used to extract the features.
After feature extraction of the speech, Neuro-Genetic hybrid algorithm has been
used in the learning and identification purposes. Features are extracted by
using different techniques to optimize the performance of the identification.
According to the VALID speech database, the highest speaker identification rate
of 100.000 percent for studio environment and 82.33 percent for office
environmental conditions have been achieved in the close set text dependent
speaker identification system."
"Speech analyzing in special periods of time has been presented in this paper.
One of the most important periods in signal processing is near to Zero. By this
paper, we analyze noise speech signals when these signals are near to Zero. Our
strategy is defining some subfunctions and compress histograms when a noise
speech signal is in a special period. It can be so useful for wavelet signal
processing and spoken systems analyzing."
"We are looking for a mathematical model of monophonic sounds with independent
time and phase dimensions. With such a model we can resynthesise a sound with
arbitrarily modulated frequency and progress of the timbre. We propose such a
model and show that it exactly fulfils some natural properties, like a kind of
time-invariance, robustness against non-harmonic frequencies, envelope
preservation, and inclusion of plain resampling as a special case. The
resulting algorithm is efficient and allows to process data in a streaming
manner with phase and shape modulation at sample rate, what we demonstrate with
an implementation in the functional language Haskell. It allows a wide range of
applications, namely pitch shifting and time scaling, creative FM synthesis
effects, compression of monophonic sounds, generating loops for sampled sounds,
synthesise sounds similar to wavetable synthesis, or making ultrasound audible."
"The objective of this paper is to understand the critical parameters that
need to be addressed while designing a guitar tuner. The focus of the design
lies in developing a suitable algorithm to accurately detect the fundamental
frequency of a plucked guitar string from its frequency spectrum. A
userfriendly graphical interface is developed using Matlab to allow any user to
easily tune his guitar using the developed program."
"Among environmental sounds, we have chosen to study a class of action-related
impact sounds: automobile door closure sounds. We propose to describe these
sounds using a model composed of perceptual properties. The development of the
perceptual model was derived from the evaluation of many door closure sounds
measured under controlled laboratory listening conditions. However, listening
to such sounds normally occurs within a natural context, which probably
modifies their perception. We therefore need to study differences between the
real situation and the laboratory situation by following standard practices in
order to specify the precise listening conditions and observe the influence of
previous learning, expectations, action-perception interactions, and attention
given to sounds. Our process consists in doing in situ experiments that are
compared with specific laboratory experiments in order to isolate certain
influential, context dependent components."
"The dichotic method of hearing sound adapts in the region of musical harmony.
The algorithm of the separation of the being dissonant voices into several
separate groups is proposed. For an increase in the pleasantness of chords the
different groups of voices are heard out through the different channels of
headphones. Is created two demonstration program for PC. Keywords: music,
harmony, chord, dichotic listening, dissonance, consonance, headphones,
pleasantness, midi."
"This work introduces an economic solution for the problems of sound
insulation of recording studios. Sound insulation at wall resonance frequency
is weak. Instead of acoustical treatment, a digital filter is used to eliminate
the effects of wall resonance and coincidence phenomena on recording of speech.
Sound insulation of studio is measured to calculate the wall resonance
frequency and the coincidence frequency. Pole /zero placement technique is used
to calculate the IIR filter coefficients. The digital filter is designed,
simulated and implemented. The proposed system is used to treat these problems
and it is shown to be effective in recording the noisy speech. In this work
digital signal processing is used instead of the acoustic treatment to
eliminate the effect of noise at the studio wall resonance. This technique is
cheap and effective in canceling the noise at the desired frequencies. Field
Programmable Gate Array (FPGA) is used for hardware implementation of the
proposed filter structure which provides fast and cheap solution for processing
real time audio signals. The implementation is carried out using Spartan chip
from Xinlinx achieving higher performance than commercially available software
solutions."
"""Hopscotch"" is a world-wide game for children to play since the times in the
ancient Roman Empire and China. Here we present a study mainly focused on the
research and discussion of the application on the children's well-know
edutainment via the physical interactive design to provide the sensing of the
times for the conventional hopscotch, which is a new type of experiment for the
technology aided edutainment. The innovated hopscotch music game involves the
sound samples of various animals and the characters of cartoon, and the
algorithmic composition via the development of the music technology based
interactive game, to gradually make the children perceive the world of digits,
sound, and music. It can guide the growing children's personality and character
from disorder into clarity. Furthermore, the traditional teaching materials can
be improved via the implementation of the electrical sensing devices,
electrical I/O module, and the computer music program Max/MSP, to integrate the
interactive computer music with the interactive and immersive soundscapes
composition, and the teaching tool with educational gaming is completely
accomplished eventually."
"This paper addresses the problem of infants' cry fundamental frequency
estimation. The fundamental frequency is estimated using a modified simple
inverse filtering tracking (SIFT) algorithm. The performance of the modified
SIFT is studied using a real database of infants' cry. It is shown that the
algorithm is capable of overcoming the problem of under-estimation and
over-estimation of the cry fundamental frequency, with an estimation accuracy
of 6.15% and 3.75%, for hyperphonated and phonated cry segments, respectively.
Some typical examples of the fundamental frequency contour in typical cases of
pathological and healthy cry signals are presented and discussed."
"The paper presents methods for evaluating the accuracy of alignments between
transcriptions and audio recordings. The methods have been applied to the
Spoken British National Corpus, which is an extensive and varied corpus of
natural unscripted speech. Early results show good agreement with human ratings
of alignment accuracy. The methods also provide an indication of the location
of likely alignment problems; this should allow efficient manual examination of
large corpora. Automatic checking of such alignments is crucial when analysing
any very large corpus, since even the best current speech alignment systems
will occasionally make serious errors. The methods described here use a hybrid
approach based on statistics of the speech signal itself, statistics of the
labels being evaluated, and statistics linking the two."
"A new method has been developed to adjust volume automatically on all audio
devices equipped with at least one microphone, including mobile phones,
personal media players, headsets, and car radios, that might be used in noisy
environments, such as crowds, cars, and outdoors. The method uses a patented
set of algorithms, implemented on the chips in such devices, to preserve
constant intelligibility of speech in noisy environments, rather than constant
signal-to-noise ratio. The algorithms analyze the noise background in real time
and compensate only for fluctuating noise in the frequency domain and the time
domain that interferes with intelligibility of speech. Advantages of this
method of controlling volume include: Controlling volume without sacrificing
clarity; adjusting only for persistent speech-interference noise; smoothing
volume fluctuations; and eliminating static-like bursts caused by noise spikes.
Practical human-factors approaches to implementing these algorithms in mobile
phones are discussed."
"In many application of noise cancellation, the changes in signal
characteristics could be quite fast. This requires the utilization of adaptive
algorithms, which converge rapidly. Least Mean Squares (LMS) adaptive filters
have been used in a wide range of signal processing application. The Recursive
Least Squares (RLS) algorithm has established itself as the ""ultimate"" adaptive
filtering algorithm in the sense that it is the adaptive filter exhibiting the
best convergence behavior. Unfortunately, practical implementations of the
algorithm are often associated with high computational complexity and/or poor
numerical properties. Recently adaptive filtering was presented that was based
on Matching Pursuits, have a nice tradeoff between complexity and the
convergence speed. This paper describes a new approach for noise cancellation
in speech enhancement using the new adaptive filtering algorithm named fast
affine projection algorithm (FAPA). The simulation results demonstrate the good
performance of the FAPA in attenuating the noise."
"In many application of noise cancellation, the changes in signal
characteristics could be quite fast. This requires the utilization of adaptive
algorithms, which converge rapidly. Least Mean Squares (LMS) and Normalized
Least Mean Squares (NLMS) adaptive filters have been used in a wide range of
signal processing application because of its simplicity in computation and
implementation. The Recursive Least Squares (RLS) algorithm has established
itself as the ""ultimate"" adaptive filtering algorithm in the sense that it is
the adaptive filter exhibiting the best convergence behavior. Unfortunately,
practical implementations of the algorithm are often associated with high
computational complexity and/or poor numerical properties. Recently adaptive
filtering was presented, have a nice tradeoff between complexity and the
convergence speed. This paper describes a new approach for noise cancellation
in speech enhancement using the two new adaptive filtering algorithms named
fast affine projection algorithm and fast Euclidean direction search algorithms
for attenuating noise in speech signals. The simulation results demonstrate the
good performance of the two new algorithms in attenuating the noise."
"This paper considers methods for audio display in a CAVE-type virtual reality
theater, a 3 m cube with displays covering all six rigid faces. Headphones are
possible since the user's headgear continuously measures ear positions, but
loudspeakers are preferable since they enhance the sense of total immersion.
The proposed solution consists of open-loop acoustic point control. The
transfer function, a matrix of room frequency responses from the loudspeakers
to the ears of the user, is inverted using multi-channel inversion methods, to
create exactly the desired sound field at the user's ears. The inverse transfer
function is constructed from impulse responses simulated by the image source
method. This technique is validated by measuring a 2x2 matrix transfer
function, simulating a transfer function with the same geometry, and filtering
the measured transfer function through the inverse of the simulation. Since
accuracy of the image source method decreases with time, inversion performance
is improved by windowing the simulated response prior to inversion. Parameters
of the simulation and inversion are adjusted to minimize residual reverberant
energy; the best-case dereverberation ratio is 10 dB."
"Change detection within an audio stream is an important task in several
domains, such as classification and segmentation of a sound or of a music
piece, as well as indexing of broadcast news or surveillance applications. In
this paper we propose two novel methods for spectral change detection without
any assumption about the input sound: they are both based on the evaluation of
information measures applied to a time- frequency representation of the signal,
and in particular to the spectrogram. The class of measures we consider, the
R\'enyi entropies, are obtained by extending the Shannon entropy definition: a
biasing of the spectrogram coefficients is realized through the dependence of
such measures on a parameter, which allows refined results compared to those
obtained with standard divergences. These methods provide a low computational
cost and are well-suited as a support for higher level analysis, segmentation
and classification algorithms."
"In this paper we propose a method for automatic local time adap- tation of
the spectrogram of an audio signal, based on its decomposition within a Gabor
multi-frame. The sparsity of the analyses within each individual frame is
evaluated through the R\'enyi entropies measures. According to the sparsity of
the decompositions, an optimal resolution and a reduced multi-frame are
determined, defining an adapted spectrogram with variable resolution and hop
size. The composition of such a reduced multi-frame allows an immediate
definition of a dual frame: re-synthesis techniques for this adapted analysis
are easily derived by the traditional phase vocoder scheme."
"We propose a method for automatic local time-adaptation of the spectrogram of
audio signals: it is based on the decomposition of a signal within a Gabor
multi-frame through the STFT operator. The sparsity of the analysis in every
individual frame of the multi-frame is evaluated through the R\'enyi entropy
measures: the best local resolution is determined minimizing the entropy
values. The overall spectrogram of the signal we obtain thus provides local
optimal resolution adaptively evolving over time. We give examples of the
performance of our algorithm with an instrumental sound and a synthetic one,
showing the improvement in spectrogram displaying obtained with an automatic
adaptation of the resolution. The analysis operator is invertible, thus leading
to a perfect reconstruction of the original signal through the analysis
coefficients."
"We present an algorithm for sound analysis and resynthesis with local
automatic adaptation of time-frequency resolution. There exists several
algorithms allowing to adapt the analysis window depending on its time or
frequency location; in what follows we propose a method which select the
optimal resolution depending on both time and frequency. We consider an
approach that we denote as analysis-weighting, from the point of view of Gabor
frame theory. We analyze in particular the case of different adaptive
time-varying resolutions within two complementary frequency bands; this is a
typical case where perfect signal reconstruction cannot in general be achieved
with fast algorithms, causing a certain error to be minimized. We provide
examples of adaptive analyses of a music sound, and outline several
possibilities that this work opens."
"Very short computer programs, sometimes consisting of as few as three
arithmetic operations in an infinite loop, can generate data that sounds like
music when output as raw PCM audio. The space of such programs was recently
explored by dozens of individuals within various on-line communities. This
paper discusses the programs resulting from this exploratory work and
highlights some rather unusual methods they use for synthesizing sound and
generating musical structure."
"We consider the problem of online audio source separation. Existing
algorithms adopt either a sliding block approach or a stochastic gradient
approach, which is faster but less accurate. Also, they rely either on spatial
cues or on spectral cues and cannot separate certain mixtures. In this paper,
we design a general online audio source separation framework that combines both
approaches and both types of cues. The model parameters are estimated in the
Maximum Likelihood (ML) sense using a Generalised Expectation Maximisation
(GEM) algorithm with multiplicative updates. The separation performance is
evaluated as a function of the block size and the step size and compared to
that of an offline algorithm."
"The sensitivity of human ear is dependent on frequency which is nonlinearly
resolved across the audio spectrum .Now to improve the recognition performance
in a similar non linear approach requires a front -end design, suggested by
empirical evidences. A popular alternative to linear prediction based analysis
is therefore filter bank analysis since this provides a much more
straightforward route to obtain the desired non-linear frequency resolution.
MEL filter bank and BARK filter bank are two popular filter bank analysis
techniques. This paper presents FPGA based implementation of MEL filter bank
and BARK filter bank with different bandwidths and different signal spectrum
ranges. The designs have been implemented using VHDL, simulated and verified
using Xilinx 11.1.For each filter bank, the basic building block is implemented
in Spartan 3E. A comparative study among these two mentioned filter banks is
also done in this paper."
"In this paper, the concept of an adaptation algorithm is proposed, which can
be used to blindly adapt the microphone array geometry of a humanoid robot such
that the performance of the underlying signal separation algorithm is improved.
As a decisive feature, an online performance measure for blind source
separation is introduced which allows a robust and reliable estimation of the
instantaneous separation performance based on currently observable data.
Experimental results from a simulated environment confirm the efficacy of the
concept."
"An acoustic testing approach based on the concept of a microphone sensor
surrounding the product under test is proposed. Microphone signals are
processed simultaneously by a test system computer, according to the objective
of the test. The spatial and frequency domain selectivity features of this
method are examined. Sound-spatial visualization algorithm is observed. A test
system design based on the concept of a microphone surrounding the tested
product has the potential to improve distortion measurement accuracy."
"The rapidly growing adoption of sensor-enabled smartphones has greatly fueled
the proliferation of applications that use phone sensors to monitor user
behavior. A central sensor among these is the microphone which enables, for
instance, the detection of valence in speech, or the identification of
speakers. Deploying multiple of these applications on a mobile device to
continuously monitor the audio environment allows for the acquisition of a
diverse range of sound-related contextual inferences. However, the cumulative
processing burden critically impacts the phone battery.
  To address this problem, we propose DSP.Ear - an integrated sensing system
that takes advantage of the latest low-power DSP co-processor technology in
commodity mobile devices to enable the continuous and simultaneous operation of
multiple established algorithms that perform complex audio inferences. The
system extracts emotions from voice, estimates the number of people in a room,
identifies the speakers, and detects commonly found ambient sounds, while
critically incurring little overhead to the device battery. This is achieved
through a series of pipeline optimizations that allow the computation to remain
largely on the DSP. Through detailed evaluation of our prototype implementation
we show that, by exploiting a smartphone's co-processor, DSP.Ear achieves a 3
to 7 times increase in the battery lifetime compared to a solution that uses
only the phone's main processor. In addition, DSP.Ear is 2 to 3 times more
power efficient than a naive DSP solution without optimizations. We further
analyze a large-scale dataset from 1320 Android users to show that in about
80-90% of the daily usage instances DSP.Ear is able to sustain a full day of
operation (even in the presence of other smartphone workloads) with a single
battery charge."
"This dissertation covers a single-processor approach to the speech processing
pipeline of bilateral Cochlear Implants (CIs). The use of only a single
processor to provide binaural stimulation signals overcomes the synchronization
problem, which is an existing challenging problem in the deployment of
bilateral CI devices. The developed single-processor speech processing pipeline
provides CI users with a sense of directionality. Its non-synchronization
feature as well as low computational and memory requirements make it a suitable
solution for actual deployment. A speech enhancement framework is developed
that incorporates different non-Euclidean speech distortion criteria and
different noise environments. This framework not only allows the design of
environment-optimized parameters but also enables a user-specific solution
where the anthropometric measurements of an individual user are incorporated
into the training process to obtain individualized bilateral parameters. The
developed techniques are primarily meant for bilateral CIs, however, they are
general purpose in the sense that they are also applicable to binaural hearing
aids, bimodal devices having hearing aid in one ear and cochlear implant in the
other ear as well as dual-channel speech enhancement applications. Extensive
experiments have shown the effectiveness of the developed solution in six
commonly encountered noise environments compared to a similar one-channel
pipeline when using two separate processors or when using independent
sequential processing."
"The notes which play the most important and second most important roles in
expressing a raga are called Vadi and Samvadi swars respectively in (North)
Indian Classical music. Like Bageshree, Bhairavi, Shankara, Hamir and Kalingra,
Rageshree is another controversial raga so far as the choice of Vadi-Samvadi
selection is concerned where there are two different opinions. In the present
work, a two minute vocal recording of raga Rageshree is subjected to a careful
statistical analysis. Our analysis is broken into three phases: first half,
middle half and last half. Under a multinomial model set up holding appreciably
in the first two phases, only one opinion is found acceptable. In the last
phase the distribution seems to be quasi multinomial, characterized by an
unstable nature of relative occurrence of pitch of all the notes and although
the note whose relative occurrence of pitch suddenly shoots is the Vadi swar
selected from our analysis of the first two phases, we take it as an outlier
demanding a separate treatment like any other in statistics. Selection of
Vadi-Samvadi notes in a quasi-multinomial set up is still an open research
problem. An interesting musical cocktail is proposed, however, embedding
several ideas like melodic property of notes, note combinations and pitch
movements between notes, using some weighted combination of psychological and
statistical stability of notes along with watching carefully the sudden shoot
of one or more notes whenever there is enough evidence that multinomial model
has broken down."
"Time-frequency representations such as the spectrogram are commonly used to
analyze signals having a time-varying distribution of spectral energy, but the
spectrogram is constrained by an unfortunate tradeoff between resolution in
time and frequency. A method of achieving high-resolution spectral
representations has been independently introduced by several parties. The
technique has been variously named reassignment and remapping, but while the
implementations have differed in details, they are all based on the same
theoretical and mathematical foundation. In this work, we present a brief
history of work on the method we will call the method of time-frequency
reassignment, and present a unified mathematical description of the technique
and its derivation. We will focus on the development of time-frequency
reassignment in the context of the spectrogram, and conclude with a discussion
of some current applications of the reassigned spectrogram."
"Using the AURORA-2 digit recognition task, we show that recognition
accuracies obtained with classical, SNR based oracle masks can be substantially
improved by using a state-dependent mask estimation technique."
"Music genre classification is an essential tool for music information
retrieval systems and it has been finding critical applications in various
media platforms. Two important problems of the automatic music genre
classification are feature extraction and classifier design. This paper
investigates inter-genre similarity modelling (IGS) to improve the performance
of automatic music genre classification. Inter-genre similarity information is
extracted over the mis-classified feature population. Once the inter-genre
similarity is modelled, elimination of the inter-genre similarity reduces the
inter-genre confusion and improves the identification rates. Inter-genre
similarity modelling is further improved with iterative IGS modelling(IIGS) and
score modelling for IGS elimination(SMIGS). Experimental results with promising
classification improvements are provided."
"Speech signals of the letter 'zha' in Tamil language of 3 males and 3 females
were coded using an improved version of Linear Predictive Coding (LPC). The
sampling frequency was at 16 kHz and the bit rate was at 15450 bits per second,
where the original bit rate was at 128000 bits per second with the help of wave
surfer audio tool. The output LPC cepstrum is implemented in first order three
state Hidden Markov Model(HMM) chain."
"Biometric authentication techniques are more consistent and efficient than
conventional authentication techniques and can be used in monitoring,
transaction authentication, information retrieval, access control, forensics,
etc. In this paper, we have presented a detailed comparative analysis between
Principle Component Analysis (PCA) and Independent Component Analysis (ICA)
which are used for feature extraction on the basis of different Artificial
Neural Network (ANN) such as Back Propagation (BP), Radial Basis Function (RBF)
and Learning Vector Quantization (LVQ). In this paper, we have chosen ""TULIPS1
database, (Movellan, 1995)"" which is a small audiovisual database of 12
subjects saying the first 4 digits in English for the incorporation of above
methods. The six geometric lip features i.e. height of the outer corners of the
mouth, width of the outer corners of the mouth, height of the inner corners of
the mouth, width of the inner corners of the mouth, height of the upper lip,
and height of the lower lip which extracts the identity relevant information
are considered for the research work. After the comprehensive analysis and
evaluation a maximum of 91.07% accuracy in speaker recognition is achieved
using PCA and RBF and 87.36% accuracy is achieved using ICA and RBF. Speaker
identification has a wide scope of applications such as access control,
monitoring, transaction authentication, information retrieval, forensics, etc."
"In this paper, we ask what properties makes a large corpus more or less
useful. We suggest that size, by itself, should not be the ultimate goal of
building a corpus. Large-scale corpora are considered desirable because they
offer statistical stability and rich variation. But this rich variation means
more factors to control and evaluate, which can limit the advantages of size.
We discuss the use of multi-channel data to complement large-scale speech
corpora. Even though multi-channel data may limit the scale of a corpus (due to
the complex and labor-intensive nature of data collection) they can offer
information that allows us to tease apart various factors related to speech
production."
"In this paper we consider the generation of discrete white noise. Despite
this seems to be a simple problem, common noise generator implementations do
not deliver comparable results at different sampling rates. First we define
what we mean with ""comparable results"". From this we conclude, that the
variance of the random variables shall grow proportionally to the sampling
rate. Eventually we consider how noise behaves under common signal
transformations, such as frequency filters, quantisation and impulse generation
and we explore how these signal transformations must be designed in order
generate sampling-rate-aware results when applied to white noise."
"In this paper, envelope detection of speech is discussed to distinguish the
pathological cases of speech disabled children. The speech signal samples of
children of age between five to eight years are considered for the present
study. These speech signals are digitized and are used to determine the speech
envelope. The envelope is subjected to ratio mean analysis to estimate the
disability. This analysis is conducted on ten speech signal samples which are
related to both place of articulation and manner of articulation. Overall
speech disability of a pathological subject is estimated based on the results
of above analysis."
"This paper presents a new method on the use of the gammachirp auditory filter
based on a continuous wavelet analysis. The gammachirp auditory filter is
designed to provide a spectrum reflecting the spectral properties of the
cochlea, which is responsible for frequency analysis in the human auditory
system. The impulse response of the theoretical gammachirp auditory filter that
has been developed by Irino and Patterson can be used as the kernel for wavelet
transform which approximates the frequency response of the cochlea. This study
implements the gammachirp auditory filter described by Irino as an analytical
wavelet and examines its application to a different speech signals.
  The obtained results will be compared with those obtained by two other
predefined wavelet families that are Morlet and Mexican Hat. The results show
that the gammachirp wavelet family gives results that are comparable to ones
obtained by Morlet and Mexican Hat wavelet family."
"Most music theory books are like medieval medical textbooks: they contain
unjustified superstition, non-reasoning, and funny symbols glorified by Latin
phrases. How does music, in particular harmony, actually work, presented as a
real, scientific theory of music?
  The core to our approach is to consider not only the Physical phenomena of
nature but also the Computational phenomena of any machine that must make sense
of sound, such as the human brain. In particular we derive the following three
fundamental phenomena of music:
  * the Major Scale,
  * the Standard Chord Dictionary, and
  * the difference in feeling between the Major and Minor Triads.
  While the Major Scale has been independently derived before by others in a
similar manner [Helmholtz1863, Birkhoff1933], I believe the derivation of the
Standard Chord Dictionary as well as the difference in feeling between the
Major and Minor Triads to be original.
  We show to be incomplete the theory of the heretofore agreed-upon authority
on this subject, 19th-century Physicist Hermann Helmholtz [Helmholtz1863]: he
says notes are in ""concord"" because the sound playing them together is ""less
worse"" than that of some other notes. But note that, in this theory, more notes
can only penalize, some merely less than others, and so the most harmonious
sound should be a single note by itself(!) and harmony would not exist as a
phenomenon of music at all.
  I intend this article to be satisfying to scientists as an original
contribution to science and art, yet I also intend it to be approachable by
musicians and other curious members of the general public who may have long
wondered at the curious properties of tonal music and been frustrated by the
lack of satisfying, readable exposition on the subject. Therefore I have
written in a deliberately plain and conversational style, avoiding
unnecessarily formal language."
"Analysis of speech for recognition of stress is important for identification
of emotional state of person. This can be done using 'Linear Techniques', which
has different parameters like pitch, vocal tract spectrum, formant frequencies,
Duration, MFCC etc. which are used for extraction of features from speech.
TEO-CB-Auto-Env is the method which is non-linear method of features
extraction. Analysis is done using TU-Berlin (Technical University of Berlin)
German database. Here emotion recognition is done for different emotions like
neutral, happy, disgust, sad, boredom and anger. Emotion recognition is used in
lie detector, database access systems, and in military for recognition of
soldiers' emotion identification during the war."
"The high-intensity, repetitive noise associated with functional magnetic
resonance imaging hinders on-line monitoring of subjects' speech and/or
recording speech signals suitable for off-line analysis. The proposed algorithm
enhances the speech signal by suppressing the scanner noise in the signal
recorded by a single-channel microphone. Significant increases in
signal-to-noise ratio are achieved using an adaptive filter that combines time
and frequency domain elements. In addition to providing a recording suitable
for speech analysis, such a real-time system provides an alternative means (to,
e.g., the ""panic ball"") for communication between the patient and the operator
during image acquisition."
"Cochlear implant devices are known to exist since a long time. The purpose of
the present work is to develop a speech algorithm for obtaining robust speech.
In this paper, the technique of cochlear implant is first introduced, followed
by discussions of some of the existing techniques available for obtaining
speech. The next section introduces a new technique for obtaining robust
speech. The key feature of this technique lies in the use of the advantages of
an integrated approach involving the use of an estimation technique such as a
kalman filter with non linear filter bank strategy, using Dual Resonance Non
Linear(DRNL) and Single Side Band(SSB) Encoding method. A comparative study of
the proposed method with the existing method indicates that the proposed method
performs well compared to the existing method."
"In this paper, we address the problem of blind separation of speech mixtures.
We propose a new blind speech separation system, which integrates a perceptual
filterbank and independent component analysis (ICA) and using kurtosis
criterion. The perceptual filterbank was designed by adjusting undecimated
wavelet packet decomposition (UWPD) tree in order to accord to critical band
characteristics of psycho-acoustic model. Our proposed technique consists on
transforming the observations signals into an adequate representation using
UWPD and Kurtosis maximization criterion in a new preprocessing step in order
to increase the non-Gaussianity which is a pre-requirement for ICA. Experiments
were carried out with the instantaneous mixture of two speech sources using two
sensors. The obtained results show that the proposed method gives a
considerable improvement when compared with FastICA and other techniques."
"We study the problem of separating audio sources from a single linear
mixture. The goal is to find a decomposition of the single channel spectrogram
into a sum of individual contributions associated to a certain number of
sources. In this paper, we consider an informed source separation problem in
which the input spectrogram is partly annotated. We propose a convex
formulation that relies on a nuclear norm penalty to induce low rank for the
contributions. We show experimentally that solving this model with a simple
subgradient method outperforms a previously introduced nonnegative matrix
factorization (NMF) technique, both in terms of source separation quality and
computation time."
"The extraction of a desired speech signal from a noisy environment has become
a challenging issue. In the recent years, the scientific community has
particularly focused on multichannel techniques which are dealt with in this
review. In fact, this study tries to classify these multichannel techniques
into three main ones: Beamforming, Independent Com-ponent Analysis (ICA) and
Time Frequency (T-F) masking. This paper also highlights their advantages and
drawbacks. However these previously mentioned techniques could not afford
satisfactory results. This fact leads to the idea that a combination of those
techniques, which is depicted along this study, may probably provide more
efficient results. In-deed, giving the fact that those approaches are still be
considered as being not totally efficient, has led us to review these mentioned
above in the hope that further researches will provide this domain with
suitable innovations."
"Usable speech criteria are proposed to extract minimally corrupted speech for
speaker identification (SID) in co-channel speech. In co-channel speech, either
speaker can randomly appear as the stronger speaker or the weaker one at a
time. Hence, the extracted usable segments are separated in time and need to be
organized into speaker streams for SID. In this paper, we focus to organize
extracted usable speech segment into a single stream for the same speaker by
speaker assignment system. For this, we develop model-based speaker assignment
method based on posterior probability and exhaustive search algorithm.
Evaluation of this method is performed on TIMIT database. The system is
evaluated on co-channel speech and results show a significant improvement."
"Many applications of speech communication and speaker identification suffer
from the problem of co-channel speech. This paper deals with a multi-resolution
dyadic wavelet transform method for usable segments of co-channel speech
detection that could be processed by a speaker identification system.
Evaluation of this method is performed on TIMIT database referring to the
Target to Interferer Ratio measure. Co-channel speech is constructed by mixing
all possible gender speakers. Results do not show much difference for different
mixtures. For the overall mixtures 95.76% of usable speech is correctly
detected with false alarms of 29.65%."
"In musical performances with expressive tempo modulation, the tempo variation
can be modelled as a sequence of tempo arcs. Previous authors have used this
idea to estimate series of piecewise arc segments from data. In this paper we
describe a probabilistic model for a time-series process of this nature, and
use this to perform inference of single- and multi-level arc processes from
data. We describe an efficient Viterbi-like process for MAP inference of arcs.
Our approach is score-agnostic, and together with efficient inference allows
for online analysis of performances including improvisations, and can predict
immediate future tempo trajectories."
"Segregating an audio mixture containing multiple simultaneous bird sounds is
a challenging task. However, birdsong often contains rapid pitch modulations,
and these modulations carry information which may be of use in automatic
recognition. In this paper we demonstrate that an improved spectrogram
representation, based on the distribution derivative method, leads to improved
performance of a segregation algorithm which uses a Markov renewal process
model to track vocalisation patterns consisting of singing and silences."
"Clipping or saturation in audio signals is a very common problem in signal
processing, for which, in the severe case, there is still no satisfactory
solution. In such case, there is a tremendous loss of information, and
traditional methods fail to appropriately recover the signal. We propose a
novel approach for this signal restoration problem based on the framework of
Iterative Hard Thresholding. This approach, which enforces the consistency of
the reconstructed signal with the clipped observations, shows superior
performance in comparison to the state-of-the-art declipping algorithms. This
is confirmed on synthetic and on actual high-dimensional audio data processing,
both on SNR and on subjective user listening evaluations."
"We present in this paper a new approach for polyphonic music transcription
using evolution strategies (ES). Automatic music transcription is a complex
process that still remains an open challenge. Using an audio signal to be
transcribed as target for our ES, information needed to generate a MIDI file
can be extracted from this latter one. Many techniques presented in the
literature at present exist and a few of them have applied evolutionary
algorithms to address this problem in the context of considering it as a search
space problem. However, ES have never been applied until now. The experiments
showed that by using these machines learning tools, some shortcomings presented
by other evolutionary algorithms based approaches for transcription can be
solved. They include the computation cost and the time for convergence. As
evolution strategies use self-adapting parameters, we show in this paper that
by correctly tuning the value of its strategy parameter that controls the
standard deviation, a fast convergence can be triggered toward the optima,
which from the results performs the transcription of the music with good
accuracy and in a short time. In the same context, the computation task is
tackled using parallelization techniques thus reducing the computation time and
the transcription time in the overall."
"In this paper a generalized postfilter algorithm design issues are presented.
This postfilter is used to jointly suppress late reverberation, residual echo,
and background noise. When residual echo and noise are suppressed, the best
result obtains by suppressing both interferences together after the Acoustic
echo cancellation (AEC). The main advantage of this approach is that the
residual echo and noise suppression does not suffer from the existence of a
strong acoustic echo component. Furthermore, the Acoustic echo cancellation
(AEC) does not suffer from the time-varying noise suppression. A disadvantage
is that the input signal of the Acoustic echo cancellation (AEC) has a low
signal-to-noise ratio (SNR). To overcome this problem, algorithms have been
proposed where, apart from the joint suppression, a noise-reduced signal is
used to adapt the echo canceller."
"The GTZAN dataset appears in at least 100 published works, and is the
most-used public dataset for evaluation in machine listening research for music
genre recognition (MGR). Our recent work, however, shows GTZAN has several
faults (repetitions, mislabelings, and distortions), which challenge the
interpretability of any result derived using it. In this article, we disprove
the claims that all MGR systems are affected in the same ways by these faults,
and that the performances of MGR systems in GTZAN are still meaningfully
comparable since they all face the same faults. We identify and analyze the
contents of GTZAN, and provide a catalog of its faults. We review how GTZAN has
been used in MGR research, and find few indications that its faults have been
known and considered. Finally, we rigorously study the effects of its faults on
evaluating five different MGR systems. The lesson is not to banish GTZAN, but
to use it with consideration of its contents."
"The perception of consonance/dissonance of musical harmonies is strongly
correlated with their periodicity. This is shown in this article by
consistently applying recent results from psychophysics and neuroacoustics,
namely that the just noticeable difference between pitches for humans is about
1% for the musically important low frequency range and that periodicities of
complex chords can be detected in the human brain. Based thereon, the concepts
of relative and logarithmic periodicity with smoothing are introduced as
powerful measures of harmoniousness. The presented results correlate
significantly with empirical investigations on the perception of chords. Even
for scales, plausible results are obtained. For example, all classical church
modes appear in the front ranks of all theoretically possible seven-tone
scales."
"We consider the ability of a very simple feed-forward neural network to
discriminate phonemes based on just relative power spectrum. The network
consists of two neurons with symmetric nonlinear response over a spectral
range. The output of the neurons is subsequently fed to a comparator. We show
that often this is enough to achieve complete separation of data. We compare
the performance of found discriminants with that of more general neurons. Our
conclusion is that not much is gained in passing to real-valued weights. More
likely higher number of neurons and preprocessing of input will yield better
discrimination results. The networks considered are directly amenable to
hardware (neuromorphic) designs. Other advantages include interpretability,
guarantees of performance on unseen data and low Kolmogorov complexity."
"Audio effect implementation on random musical signal is a basic application
of digital signal processors. In this paper, the compatibility features of
MATLAB R2008a with Code Composer Studio 3.3 has been exploited to develop
Simulink models which when emulated on TMS320C6713 DSK generate real time audio
effects. Each design has been done by two different asynchronous scheduling
techniques: (i) Idle task Scheduling and (ii) DSP/BIOS task Scheduling. A basic
COCOMO analysis has been done for the generated code to justify the industrial
viability of this design approach.
  KEYWORDS: Musical signal processing, Real time Audio effects, Echo, Stress
Generation, Reverberation, Reverberated Chorus, Real Time Scheduling."
"This paper addresses the problem of sound-source localization from time-delay
estimates using arbitrarily-shaped non-coplanar microphone arrays. A novel
geometric formulation is proposed, together with a thorough algebraic analysis
and a global optimization solver. The proposed model is thoroughly described
and evaluated. The geometric analysis, stemming from the direct acoustic
propagation model, leads to necessary and sufficient conditions for a set of
time delays to correspond to a unique position in the source space. Such sets
of time delays are referred to as feasible sets. We formally prove that every
feasible set corresponds to exactly one position in the source space, whose
value can be recovered using a closed-form localization mapping. Therefore we
seek for the optimal feasible set of time delays given, as input, the received
microphone signals. This time delay estimation problem is naturally cast into a
programming task, constrained by the feasibility conditions derived from the
geometric analysis. A global branch-and-bound optimization technique is
proposed to solve the problem at hand, hence estimating the best set of
feasible time delays and, subsequently, localizing the sound source. Extensive
experiments with both simulated and real data are reported; we compare our
methodology to four state-of-the-art techniques. This comparison clearly shows
that the proposed method combined with the branch-and-bound algorithm
outperforms existing methods. These in-depth geometric understanding, practical
algorithms, and encouraging results, open several opportunities for future
work."
"Birdsong often contains large amounts of rapid frequency modulation (FM). It
is believed that the use or otherwise of FM is adaptive to the acoustic
environment, and also that there are specific social uses of FM such as trills
in aggressive territorial encounters. Yet temporal fine detail of FM is often
absent or obscured in standard audio signal analysis methods such as Fourier
analysis or linear prediction. Hence it is important to consider high
resolution signal processing techniques for analysis of FM in bird
vocalisations. If such methods can be applied at big data scales, this offers a
further advantage as large datasets become available.
  We introduce methods from the signal processing literature which can go
beyond spectrogram representations to analyse the fine modulations present in a
signal at very short timescales. Focusing primarily on the genus Phylloscopus,
we investigate which of a set of four analysis methods most strongly captures
the species signal encoded in birdsong. In order to find tools useful in
practical analysis of large databases, we also study the computational time
taken by the methods, and their robustness to additive noise and MP3
compression.
  We find three methods which can robustly represent species-correlated FM
attributes, and that the simplest method tested also appears to perform the
best. We find that features representing the extremes of FM encode species
identity supplementary to that captured in frequency features, whereas
bandwidth features do not encode additional information.
  Large-scale FM analysis can efficiently extract information useful for
bioacoustic studies, in addition to measures more commonly used to characterise
vocalisations."
"L'accent est plac\'e dans cet article sur la structure hi\'erarchique,
l'aspect parcimonieux de la repr\'esentation de l'information sonore, la tr\`es
grande dimension des caract\'eristiques ainsi que sur l'ind\'ependance des
caract\'eristiques permettant de d\'efinir les composantes des objets sonores.
Les notions d'objet sonore et de repr\'esentation neuronale sont d'abord
introduites, puis illustr\'ees avec une application en analyse de signaux
sonores vari\'es: parole, musique et environnements naturels ext\'erieurs.
Finalement, un nouveau syst\`eme de reconnaissance automatique de parole est
propos\'e. Celui-ci est compar\'e \`a un syst\`eme statistique conventionnel.
Il montre tr\`es clairement que l'analyse par objets sonores introduit une
grande polyvalence et robustesse en reconnaissance de parole. Cette
int\'egration des connaissances en neurosciences et traitement des signaux
acoustiques ouvre de nouvelles perspectives dans le domaine de la
reconnaissance de signaux acoustiques.
  The emphasis is put on the hierarchical structure, independence and
sparseness aspects of auditory signal representations in high-dimensional
spaces, so as to define the components of auditory objects. The concept of an
auditory object and its neural representation is introduced. An illustrative
application then follows, consisting in the analysis of various auditory
signals: speech, music and natural outdoor environments. A new automatic speech
recognition (ASR) system is then proposed and compared to a conventional
statistical system. The proposed system clearly shows that an object-based
analysis introduces a great flexibility and robustness for the task of speech
recognition. The integration of knowledge from neuroscience and acoustic signal
processing brings new ways of thinking to the field of classification of
acoustic signals."
"The performance of audio source separation from underdetermined convolutive
mixture assuming known mixing filters can be significantly improved by using an
analysis sparse prior optimized by a reweighting l1 scheme and a wideband
datafidelity term, as demonstrated by a recent article. In this letter, we show
that the performance can be improved even more significantly by exploiting a
low-rank prior on the source spectrograms.We present a new algorithm to
estimate the sources based on i) an analysis sparse prior, ii) a reweighting
scheme so as to increase the sparsity, iii) a wideband data-fidelity term in a
constrained form, and iv) a low-rank constraint on the source spectrograms.
Evaluation on reverberant music mixtures shows that the resulting algorithm
improves state-of-the-art methods by more than 2 dB of signal-to-distortion
ratio."
"This paper introduces a simple method for producing multichannel MIDI music
that is based on randomness and simple probabilities. One distinctive feature
of the method is that it produces and sends in parallel to the sound card more
than one unsynchronized channels by exploiting the multi-threading capabilities
of general purpose programming languages. As consequence the derived sound
offers a quite ``full"" and ``unpredictable"" acoustic experience to the
listener. Subsequently the paper reports the results of an evaluation with
users. The results were very surprising: the majority of users responded that
they could tolerate this music in various occasions."
"This paper proposes a hybrid approach for co-channel speech segregation. HMM
(hidden Markov model) is used to track the pitches of 2 talkers. The resulting
pitch tracks are then enriched with the prominent pitch. The enriched tracks
are correctly grouped using pitch continuity. Medium frame harmonics are used
to extract the second pitch for frames with only one pitch deduced using the
previous steps. Finally, the pitch tracks are input to CASA (computational
auditory scene analysis) to segregate the mixed speech. The center frequency
range of the gamma tone filter banks is maximized to reduce the overlap between
the channels filtered for better segregation. Experiments were conducted using
this hybrid approach on the speech separation challenge database and compared
to the single (non-hybrid) approaches, i.e. signal processing and CASA. Results
show that using the hybrid approach outperforms the single approaches."
"In this paper we address the problems of modeling the acoustic space
generated by a full-spectrum sound source and of using the learned model for
the localization and separation of multiple sources that simultaneously emit
sparse-spectrum sounds. We lay theoretical and methodological grounds in order
to introduce the binaural manifold paradigm. We perform an in-depth study of
the latent low-dimensional structure of the high-dimensional interaural
spectral data, based on a corpus recorded with a human-like audiomotor robot
head. A non-linear dimensionality reduction technique is used to show that
these data lie on a two-dimensional (2D) smooth manifold parameterized by the
motor states of the listener, or equivalently, the sound source directions. We
propose a probabilistic piecewise affine mapping model (PPAM) specifically
designed to deal with high-dimensional data exhibiting an intrinsic piecewise
linear structure. We derive a closed-form expectation-maximization (EM)
procedure for estimating the model parameters, followed by Bayes inversion for
obtaining the full posterior density function of a sound source direction. We
extend this solution to deal with missing data and redundancy in real world
spectrograms, and hence for 2D localization of natural sound sources such as
speech. We further generalize the model to the challenging case of multiple
sound sources and we propose a variational EM framework. The associated
algorithm, referred to as variational EM for source separation and localization
(VESSL) yields a Bayesian estimation of the 2D locations and time-frequency
masks of all the sources. Comparisons of the proposed approach with several
existing methods reveal that the combination of acoustic-space learning with
Bayesian inference enables our method to outperform state-of-the-art methods."
"A new method for designing non-uniform filter-banks for acoustic echo
cancellation is proposed. In the method, the analysis prototype filter design
is framed as a convex optimization problem that maximizes the signal-to-alias
ratio (SAR) in the analysis banks. Since each sub-band has a different
bandwidth, the contribution to the overall SAR from each analysis bank is taken
into account during optimization. To increase the degrees of freedom during
optimization, no constraints are imposed on the phase or group delay of the
filters; at the same time, low delay is achieved by ensuring that the resulting
filters are minimum phase. Experimental results show that the filter bank
designed using the proposed method results in a sub-band adaptive filter with a
much better echo return loss enhancement (ERLE) when compared with existing
design methods."
"Sparse signal models are in the focus of recent developments in narrowband
DOA estimation. Applying these methods to localizing audio sources, however, is
challenging due to the wideband nature of the signals. The common approach of
processing all frequency bands separately and fusing the results is costly and
can introduce errors in the solution. We show how these problems can be
overcome by decomposing the wavefield of a circular microphone array and using
circular harmonic coefficients instead of time-frequency data for sparse DOA
estimation. As a result, we present the super-resolution localization method
WASCHL (Wideband Audio Sparse Circular Harmonics Localizer) that is inherently
frequency-coherent and highly efficient from a computational point of view."
"This article deals with the use of optimal lattice and optimal window in
Discrete Gabor Transform computation. In the case of a generalized Gaussian
window, extending earlier contributions, we introduce an additional local
window adaptation technique for non-stationary signals. We illustrate our
approach and the earlier one by addressing three time-frequency analysis
problems to show the improvements achieved by the use of optimal lattice and
window: close frequencies distinction, frequency estimation and SNR estimation.
The results are presented, when possible, with real world audio signals."
"This paper describes a novel technique for promoting sparsity in the modified
filtered-x algorithms required for active noise control. The proposed
algorithms are based on recent techniques incorporating approximations to the
\ell_0-norm in the cost functions that are used to derive adaptive filtering
algorithms. In particular, zero-attracting and reweighted zero-attracting
filtered-x adaptive algorithms are developed and considered for active noise
control problems. The results of simulations indicate that the proposed
techniques improve the convergence of the existing modified algorithm in the
case where the primary and secondary paths exhibit a degree of sparsity."
"The goal of this article is to present interactive didactic software for
analog to digital conversion using PCM method. After a short introduction
regarding vocal signal processing we present some method for analog to digital
conversion. The didactic software is an applet that can be direct accessed by
any interested person."
"This paper proposes an efficient method based on the steered-response power
(SRP) technique for sound source localization using microphone arrays: the
volumetric SRP (V-SRP). As compared to the SRP, by deploying a sparser
volumetric grid, the V-SRP achieves a significant reduction of the
computational complexity without sacrificing the accuracy of the location
estimates. By appending a fine search step to the V-SRP, its refined version
(RV-SRP) improves on the compromise between complexity and accuracy.
Experiments conducted in both simulated- and real-data scenarios demonstrate
the benefits of the proposed approaches. Specifically, the RV-SRP is shown to
outperform the SRP in accuracy at a computational cost of about ten times
lower."
"The objective of the present work is to propose a method to automatically
detect polarity of the speech signals by estimating instants of significant
excitation of the vocaltract and the cosine phase of the analytic signal
representation. The phase changes in the analytic signal around the Hilbert
envelope (HE) peaks are found to vary according to the polarity of the given
speech signal. The relevant HE peaks for the Hilbert phase analysis are
selected by estimating the instants of significant excitation in speech. The
speech polarity identification rate obtained for the proposed method is almost
equal to the state of the art residual skewness method for speech polarity
detection. The proposed method also provides the same results for the polarity
detection in electro-glottogram signals. Finally, the robustness of the
proposed method is confirmed from the reduced detection error rates obtained in
noisy environments with various signal to noise ratios (SNRs). The MATLAB codes
used for implementing the proposed method are available for download from the
following link: http://nlp.amrita.edu:8080/TTS/polarityprograms.zip"
"We propose a novel sparse representation for heavily underdetermined
multichannel sound mixtures, i.e., with much more sources than microphones. The
proposed approach operates in the complex Fourier domain, thus preserving
spatial characteristics carried by phase differences. We derive a
generalization of K-SVD which jointly estimates a dictionary capturing both
spectral and spatial features, a sparse activation matrix, and all
instantaneous source phases from a set of signal examples. The dictionary can
then be used to extract the learned signal from a new input mixture. The method
is applied to the challenging problem of ego-noise reduction for robot
audition. We demonstrate its superiority relative to conventional
dictionary-based techniques using recordings made in a real room."
"The performance of a speaker recognition system decreases when the speaker is
under stress or emotion. In this paper we explore and identify a mechanism that
enables use of inherent stress-in-speech or speaking style information present
in speech of a person as additional cues for speaker recognition. We quantify
the the inherent stress present in the speech of a speaker mainly using 3
features, namely, pitch, amplitude and duration (together called PAD) We
experimentally observe that the PAD vectors of similar phones in different
words of a speaker are close to each other in the three dimensional (PAD) space
confirming that the way a speaker stresses different syllables in their speech
is unique to them, thus we propose the use of PAD based speaking style of a
speaker as an additional feature for speaker recognition applications."
"Detection of transitions between broad phonetic classes in a speech signal is
an important problem which has applications such as landmark detection and
segmentation. The proposed hierarchical method detects silence to non-silence
transitions, high amplitude (mostly sonorants) to low ampli- tude (mostly
fricatives/affricates/stop bursts) transitions and vice-versa. A subset of the
extremum (minimum or maximum) samples between every pair of successive
zero-crossings is selected above a second pass threshold, from each bandpass
filtered speech signal frame. Relative to the mid-point (reference) of a frame,
locations of the first and the last extrema lie on either side, if the speech
signal belongs to a homogeneous segment; else, both these locations lie on the
left or the right side of the reference, indicating a transition frame. When
tested on the entire TIMIT database, of the transitions detected, 93.6% are
within a tolerance of 20 ms from the hand labeled boundaries. Sonorant,
unvoiced non-sonorant and silence classes and their respective onsets are
detected with an accuracy of about 83.5% for the same tolerance. The results
are as good as, and in some respects better than the state-of-the-art methods
for similar tasks."
"Linear prediction (LP) technique estimates an optimum all-pole filter of a
given order for a frame of speech signal. The coefficients of the all-pole
filter, 1/A(z) are referred to as LP coefficients (LPCs). The gain of the
inverse of the all-pole filter, A(z) at z = 1, i.e, at frequency = 0, A(1)
corresponds to the sum of LPCs, which has the property of being lower (higher)
than a threshold for the sonorants (fricatives). When the inverse-tan of A(1),
denoted as T(1), is used a feature and tested on the sonorant and fricative
frames of the entire TIMIT database, an accuracy of 99.07% is obtained. Hence,
we refer to T(1) as sonorant-fricative discrimination index (SFDI). This
property has also been tested for its robustness for additive white noise and
on the telephone quality speech of the NTIMIT database. These results are
comparable to, or in some respects, better than the state-of-the-art methods
proposed for a similar task. Such a property may be used for segmenting a
speech signal or for non-uniform frame-rate analysis."
"Usually, hearing impaired people use hearing aids which are implemented with
speech enhancement algorithms. Estimation of speech and estimation of nose are
the components in single channel speech enhancement system. The main objective
of any speech enhancement algorithm is estimation of noise power spectrum for
non stationary environment. VAD (Voice Activity Detector) is used to identify
speech pauses and during these pauses only estimation of noise. MMSE (Minimum
Mean Square Error) speech enhancement algorithm did not enhance the
intelligibility, quality and listener fatigues are the perceptual aspects of
speech. Novel evaluation approach SR (Signal to Residual spectrum ratio) based
on uncertainty parameter introduced for the benefits of hearing impaired people
in non stationary environments to control distortions. By estimation and
updating of noise based on division of original pure signal into three parts
such as pure speech, quasi speech and non speech frames based on multiple
threshold conditions. Different values of SR and LLR demonstrate the amount of
attenuation and amplification distortions. The proposed method will compared
with any one method WAT(Weighted Average Technique) Hence by using parameters
SR (signal to residual spectrum ratio) and LLR (log like hood ratio), MMSE
(Minim Mean Square Error) in terms of segmented SNR and LLR."
"Relative impulse responses between microphones are usually long and dense due
to the reverberant acoustic environment. Estimating them from short and noisy
recordings poses a long-standing challenge of audio signal processing. In this
paper we apply a novel strategy based on ideas of Compressed Sensing. Relative
transfer function (RTF) corresponding to the relative impulse response can
often be estimated accurately from noisy data but only for certain frequencies.
This means that often only an incomplete measurement of the RTF is available. A
complete RTF estimate can be obtained through finding its sparsest
representation in the time-domain: that is, through computing the sparsest
among the corresponding relative impulse responses. Based on this approach, we
propose to estimate the RTF from noisy data in three steps. First, the RTF is
estimated using any conventional method such as the non-stationarity-based
estimator by Gannot et al. or through Blind Source Separation. Second,
frequencies are determined for which the RTF estimate appears to be accurate.
Third, the RTF is reconstructed through solving a weighted $\ell_1$ convex
program, which we propose to solve via a computationally efficient variant of
the SpaRSA (Sparse Reconstruction by Separable Approximation) algorithm. An
extensive experimental study with real-world recordings has been conducted. It
has been shown that the proposed method is capable of improving many
conventional estimators used as the first step in most situations."
"Conventional NMF methods for source separation factorize the matrix of
spectral magnitudes. Spectral Phase is not included in the decomposition
process of these methods. However, phase of the speech mixture is generally
used in reconstructing the target speech signal. This results in undesired
traces of interfering sources in the target signal. In this paper the spectral
phase is incorporated in the decomposition process itself. Additionally, the
complex matrix factorization problem is reduced to an NMF problem using simple
transformations. This results in effective separation of speech mixtures since
both magnitude and phase are utilized jointly in the separation process.
Improvement in source separation results are demonstrated using objective
quality evaluations on the GRID corpus."
"This work explores nonparametric methods which aim at synthesizing audio from
low-dimensionnal acoustic features typically used in MIR frameworks. Several
issues prevent this task to be straightforwardly achieved. Such features are
designed for analysis and not for synthesis, thus favoring high-level
description over easily inverted acoustic representation. Whereas some previous
studies already considered the problem of synthesizing audio from features such
as Mel-Frequency Cepstral Coefficients, they mainly relied on the explicit
formula used to compute those features in order to inverse them. Here, we
instead adopt a simple blind approach, where arbitrary sets of features can be
used during synthesis and where reconstruction is exemplar-based. After testing
the approach on a speech synthesis from well known features problem, we apply
it to the more complex task of inverting songs from the Million Song Dataset.
What makes this task harder is twofold. First, that features are irregularly
spaced in the temporal domain according to an onset-based segmentation. Second
the exact method used to compute these features is unknown, although the
features for new audio can be computed using their API as a black-box. In this
paper, we detail these difficulties and present a framework to nonetheless
attempting such synthesis by concatenating audio samples from a training
dataset, whose features have been computed beforehand. Samples are selected at
the segment level, in the feature space with a simple nearest neighbor search.
Additionnal constraints can then be defined to enhance the synthesis
pertinence. Preliminary experiments are presented using RWC and GTZAN audio
datasets to synthesize tracks from the Million Song Dataset."
"Head-related impulse responses (HRIRs) are subject-dependent and
direction-dependent filters used in spatial audio synthesis. They describe the
scattering response of the head, torso, and pinnae of the subject. We propose a
structural factorization of the HRIRs into a product of non-negative and
Toeplitz matrices; the factorization is based on a novel extension of a
non-negative matrix factorization algorithm. As a result, the HRIR becomes
expressible as a convolution between a direction-independent \emph{resonance}
filter and a direction-dependent \emph{reflection} filter. Further, the
reflection filter can be made \emph{sparse} with minimal HRIR distortion. The
described factorization is shown to be applicable to the arbitrary source
signal case and allows one to employ time-domain convolution at a computational
cost lower than using convolution in the frequency domain."
"This paper presents a new approach for a vocoder design based on full
frequency masking by octaves in addition to a technique for spectral filling
via beta probability distribution. Some psycho-acoustic characteristics of
human hearing - inaudibility masking in frequency and phase - are used as a
basis for the proposed algorithm. The results confirm that this technique may
be useful to save bandwidth in applications requiring intelligibility. It is
recommended for the legal eavesdropping of long voice conversations."
"The estimation of the time- and frequency-dependent coherent-to-diffuse power
ratio (CDR) from the measured spatial coherence between two omnidirectional
microphones is investigated. Known CDR estimators are formulated in a common
framework, illustrated using a geometric interpretation in the complex plane,
and investigated with respect to bias and robustness towards model errors.
Several novel unbiased CDR estimators are proposed, and it is shown that
knowledge of either the direction of arrival (DOA) of the target source or the
coherence of the noise field is sufficient for unbiased CDR estimation. The
validity of the model for the application of CDR estimates to dereverberation
is investigated using measured and simulated impulse responses. A CDR-based
dereverberation system is presented and evaluated using signal-based quality
measures as well as automatic speech recognition accuracy. The results show
that the proposed unbiased estimators have a practical advantage over existing
estimators, and that the proposed DOA-independent estimator can be used for
effective blind dereverberation."
"The purpose of this study is to investigate how humans interpret musical
scores expressively, and then design machines that sing like humans. We
consider six factors that have a strong influence on the expression of human
singing. The factors are related to the acoustic, phonetic, and musical
features of a real singing signal. Given real singing voices recorded following
the MIDI scores and lyrics, our analysis module can extract the expression
parameters from the real singing signals semi-automatically. The expression
parameters are used to control the singing voice synthesis (SVS) system for
Mandarin Chinese, which is based on the harmonic plus noise model (HNM). The
results of perceptual experiments show that integrating the expression factors
into the SVS system yields a notable improvement in perceptual naturalness,
clearness, and expressiveness. By one-to-one mapping of the real singing signal
and expression controls to the synthesizer, our SVS system can simulate the
interpretation of a real singer with the timbre of a speaker."
"Loudspeaker-based spatial audio reproduction schemes are increasingly used
for evaluating hearing aids in complex acoustic conditions. To further
establish the feasibility of this approach, this study investigated the
interaction between spatial resolution of different reproduction methods and
technical and perceptual hearing aid performance measures using computer
simulations. Three spatial audio reproduction methods -- discrete speakers,
vector base amplitude panning and higher order ambisonics -- were compared in
regular circular loudspeaker arrays with 4 to 72 channels. The influence of
reproduction method and array size on performance measures of representative
multi-microphone hearing aid algorithm classes with spatially distributed
microphones and a representative single channel noise-reduction algorithm was
analyzed. Algorithm classes differed in their way of analyzing and exploiting
spatial properties of the sound field, requiring different accuracy of sound
field reproduction. Performance measures included beam pattern analysis,
signal-to-noise ratio analysis, perceptual localization prediction, and quality
modeling. The results show performance differences and interaction effects
between reproduction method and algorithm class that may be used for guidance
when selecting the appropriate method and number of speakers for specific tasks
in hearing aid research."
"This paper describes an online algorithm for enhancing monaural noisy speech.
Firstly, a novel phase-corrected low-delay gammatone filterbank is derived for
signal subband decomposition and resynthesis; the subband signals are then
analyzed frame by frame. Secondly, a novel feature named periodicity degree
(PD) is proposed to be used for detecting and estimating the fundamental period
(P0) in each frame and for estimating the signal-to-noise ratio (SNR) in each
frame-subband signal unit. The PD is calculated in each unit as the
multiplication of the normalized autocorrelation and the comb filter ratio, and
shown to be robust in various low-SNR conditions. Thirdly, the noise energy
level in each signal unit is estimated recursively based on the estimated SNR
for units with high PD and based on the noisy signal energy level for units
with low PD. Then the a priori SNR is estimated using a decision-directed
approach with the estimated noise level. Finally, a revised Wiener gain is
calculated, smoothed, and applied to each unit; the processed units are summed
across subbands and frames to form the enhanced signal. The P0 detection
accuracy of the algorithm was evaluated on two corpora and showed comparable
performance on one corpus and better performance on the other corpus when
compared to a recently published pitch detection algorithm. The speech
enhancement effect of the algorithm was evaluated on one corpus with two
objective criteria and showed better performance in one highly non-stationary
noise and comparable performance in two other noises when compared to a
state-of-the-art statistical-model based algorithm."
"Many current paradigms for acoustic event detection (AED) are not adapted to
the organic variability of natural sounds, and/or they assume a limit on the
number of simultaneous sources: often only one source, or one source of each
type, may be active. These aspects are highly undesirable for applications such
as bird population monitoring. We introduce a simple method modelling the
onsets, durations and offsets of acoustic events to avoid intrinsic limits on
polyphony or on inter-event temporal patterns. We evaluate the method in a case
study with over 3000 zebra finch calls. In comparison against a HMM-based
method we find it more accurate at recovering acoustic events, and more robust
for estimating calling rates."
"Joint audio-visual speaker tracking requires that the locations of
microphones and cameras are known and that they are given in a common
coordinate system. Sensor self-localization algorithms, however, are usually
separately developed for either the acoustic or the visual modality and return
their positions in a modality specific coordinate system, often with an unknown
rotation, scaling and translation between the two. In this paper we propose two
techniques to determine the positions of acoustic sensors in a common
coordinate system, based on audio-visual correlates, i.e., events that are
localized by both, microphones and cameras separately. The first approach maps
the output of an acoustic self-calibration algorithm by estimating rotation,
scale and translation to the visual coordinate system, while the second solves
a joint system of equations with acoustic and visual directions of arrival as
input. The evaluation of the two strategies reveals that joint calibration
outperforms the mapping approach and achieves an overall calibration error of
0.20m even in reverberant environments."
"The short-time Fourier transform (STFT) provides the foundation of
binary-mask based audio source separation approaches. In computing a
spectrogram, the STFT window size parameterizes the trade-off between time and
frequency resolution. However, it is not yet known how this parameter affects
the operation of the binary mask in terms of separation quality for real-world
signals such as speech or music. Here, we demonstrate that the trade-off
between time and frequency in the STFT, used to perform ideal binary mask
separation, depends upon the types of source that are to be separated. In
particular, we demonstrate that different window sizes are optimal for
separating different combinations of speech and musical signals. Our findings
have broad implications for machine audition and machine learning in general."
"Audio source separation is a difficult machine learning problem and
performance is measured by comparing extracted signals with the component
source signals. However, if separation is motivated by the ultimate goal of
re-mixing then complete separation is not necessary and hence separation
difficulty and separation quality are dependent on the nature of the re-mix.
Here, we use a convolutional deep neural network (DNN), trained to estimate
'ideal' binary masks for separating voice from music, to perform re-mixing of
the vocal balance by operating directly on the individual magnitude components
of the musical mixture spectrogram. Our results demonstrate that small changes
in vocal gain may be applied with very little distortion to the ultimate
re-mix. Our method may be useful for re-mixing existing mixes."
"This paper proposes an efficient parameterization of the Room Transfer
Function (RTF). Typically, the RTF rapidly varies with varying source and
receiver positions, hence requires an impractical number of point to point
measurements to characterize a given room. Therefore, we derive a novel RTF
parameterization that is robust to both receiver and source variations with the
following salient features: (i) The parameterization is given in terms of a
modal expansion of 3D basis functions. (ii) The aforementioned modal expansion
can be truncated at a finite number of modes given that the source and receiver
locations are from two sizeable spatial regions, which are arbitrarily
distributed. (iii) The parameter weights/coefficients are independent of the
source/receiver positions. Therefore, a finite set of coefficients is shown to
be capable of accurately calculating the RTF between any two arbitrary points
from a predefined spatial region where the source(s) lie and a pre-defined
spatial region where the receiver(s) lie. A practical method to measure the RTF
coefficients is also provided, which only requires a single microphone unit and
a single loudspeaker unit, given that the room characteristics remain
stationary over time. The accuracy of the above parameterization is verified
using appropriate simulation examples."
"This work investigates the empirical performance of the sparse synthesis
versus sparse analysis regularization for the ill-posed inverse problem of
audio declipping. We develop a versatile non-convex heuristics which can be
readily used with both data models. Based on this algorithm, we report that, in
most cases, the two models perform almost similarly in terms of signal
enhancement. However, the analysis version is shown to be amenable for real
time audio processing, when certain analysis operators are considered. Both
versions outperform state-of-the-art methods in the field, especially for the
severely saturated signals."
"Most previously proposed dual-channel coherent-to-diffuse-ratio (CDR)
estimators are based on a free-field model. When used for binaural signals,
e.g., for dereverberation in binaural hearing aids, their performance may
degrade due to the influence of the head, even when the direction-of-arrival of
the desired speaker is exactly known. In this paper, the head shadowing effect
is taken into account for CDR estimation by using a simplified model for the
frequency-dependent interaural time difference and a model for the binaural
coherence of the diffuse noise field. Evaluation of CDR-based dereverberation
with measured binaural impulse responses indicates that the proposed binaural
CDR estimators can improve PESQ scores."
"Sensitivity to ITDs is important for sound localization. Normal-hearing
listeners benefit from across-frequency processing, as seen with improved ITD
thresholds when consistent ITD cues are presented over a range of frequency
channels compared to when ITD information is only presented in a single
frequency channel. This study aimed to clarify whether cochlear-implant (CI)
listeners can make use of similar processing when being stimulated with
multiple interaural electrode pairs transmitting consistent ITD information.
ITD thresholds for unmodulated, 100-pulse-per-second pulse trains were measured
in seven bilateral CI listeners using research interfaces. Consistent ITDs were
presented at either one or two electrode pairs at different current levels,
allowing for comparisons at either constant level per component electrode or
equal overall loudness. Different tonotopic distances between the pairs were
tested in order to clarify the potential influence of channel interaction.
Comparison of ITD thresholds between double pairs and the respective single
pairs revealed systematic effects of tonotopic separation and current level. At
constant levels, performance with double-pair stimulation improved compared to
single-pair stimulation, but only for large tonotopic separation. Comparisons
at equal overall loudness revealed no benefit from presenting ITD information
at two electrode pairs for any tonotopic spacing. Irrespective of
electrode-pair configuration, ITD sensitivity improved with increasing current
level. Hence, the improved ITD sensitivity for double pairs found for a large
tonotopic separation and constant current levels seems to be due to increased
loudness. The overall data suggest that CI listeners can benefit from combining
consistent ITD information across multiple electrodes, provided sufficient
stimulus levels and that stimulating electrode pairs are widely spaced."
"We propose a natural way to generalize relative transfer functions (RTFs) to
more than one source. We first prove that such a generalization is not possible
using a single multichannel spectro-temporal observation, regardless of the
number of microphones. We then introduce a new transform for multichannel
multi-frame spectrograms, i.e., containing several channels and time frames in
each time-frequency bin. This transform allows a natural generalization which
satisfies the three key properties of RTFs, namely, they can be directly
estimated from observed signals, they capture spatial properties of the sources
and they do not depend on emitted signals. Through simulated experiments, we
show how this new method can localize multiple simultaneously active sound
sources using short spectro-temporal windows, without relying on source
separation."
"We introduce a novel low level feature for identifying cover songs which
quantifies the relative changes in the smoothed frequency spectrum of a song.
Our key insight is that a sliding window representation of a chunk of audio can
be viewed as a time-ordered point cloud in high dimensions. For corresponding
chunks of audio between different versions of the same song, these point clouds
are approximately rotated, translated, and scaled copies of each other. If we
treat MFCC embeddings as point clouds and cast the problem as a relative shape
sequence, we are able to correctly identify 42/80 cover songs in the ""Covers
80"" dataset. By contrast, all other work to date on cover songs exclusively
relies on matching note sequences from Chroma derived features."
"Reverberant sound fields are often modeled as isotropic. However, it has been
observed that spatial properties change during the decay of the sound field
energy, due to non-isotropic attenuation in non-ideal rooms. In this letter, a
model for the spatial coherence between two sensors in a decaying reverberant
sound field is developed for rectangular rooms. The modeled coherence function
depends on room dimensions, surface reflectivity and orientation of the sensor
pair, but is independent of the position of source and sensors in the room. The
model includes the spherically isotropic (diffuse) and cylindrically isotropic
sound field models as special cases."
"Conventional speaker localization algorithms, based merely on the received
microphone signals, are often sensitive to adverse conditions, such as: high
reverberation or low signal to noise ratio (SNR). In some scenarios, e.g. in
meeting rooms or cars, it can be assumed that the source position is confined
to a predefined area, and the acoustic parameters of the environment are
approximately fixed. Such scenarios give rise to the assumption that the
acoustic samples from the region of interest have a distinct geometrical
structure. In this paper, we show that the high dimensional acoustic samples
indeed lie on a low dimensional manifold and can be embedded into a low
dimensional space. Motivated by this result, we propose a semi-supervised
source localization algorithm which recovers the inverse mapping between the
acoustic samples and their corresponding locations. The idea is to use an
optimization framework based on manifold regularization, that involves
smoothness constraints of possible solutions with respect to the manifold. The
proposed algorithm, termed Manifold Regularization for Localization (MRL), is
implemented in an adaptive manner. The initialization is conducted with only
few labelled samples attached with their respective source locations, and then
the system is gradually adapted as new unlabelled samples (with unknown source
locations) are received. Experimental results show superior localization
performance when compared with a recently presented algorithm based on a
manifold learning approach and with the generalized cross-correlation (GCC)
algorithm as a baseline."
"A notation system was previously presented which can notate any rational
frequency in free Just Intonation. Transposition of music is carried out by
multiplying each member of a set of frequencies by a single frequency.
Transposition of JI notations up by a fixed amount requires multiplication to
be defined for any two notations. Transposition down requires inversion to be
defined for any notation, which allows division to also be defined for any two
notations. Each notation splits into four components which in decreasing size
order are octave, diatonic scale note, sharps or flats, rational comma
adjustment. Multiplication can be defined for each of the four notation
components. Since rational number multiplication is commutative, this leads to
a definition of multiplication for frequencies and thus notations. Examples of
notation inversion and multiplication are given. Examples of transposing
melodies are given. These are checked for accuracy using the rational numbers
which each notation represents. Calculation shortcuts are considered which make
notation operations quicker to carry out by hand. A question regarding whether
rational commas should be extended from 5-rough rational numbers to all
rational numbers is considered which would greatly simplify notation
multiplication. This approach is rejected since it leads to confusion about
octave number. The four component notation system is recommended instead.
Extensions to computer notation systems and stave representations are briefly
mentioned."
"We introduce a scattering representation for the analysis and classification
of sounds. It is locally translation-invariant, stable to deformations in time
and frequency, and has the ability to capture harmonic structures. The
scattering representation can be interpreted as a convolutional neural network
which cascades a wavelet transform in time and along a harmonic spiral. We
study its application for the analysis of the deformations of the source-filter
model."
"This paper addresses the problem of binaural localization of a single speech
source in noisy and reverberant environments. For a given binaural microphone
setup, the binaural response corresponding to the direct-path propagation of a
single source is a function of the source direction. In practice, this response
is contaminated by noise and reverberations. The direct-path relative transfer
function (DP-RTF) is defined as the ratio between the direct-path acoustic
transfer function of the two channels. We propose a method to estimate the
DP-RTF from the noisy and reverberant microphone signals in the short-time
Fourier transform domain. First, the convolutive transfer function
approximation is adopted to accurately represent the impulse response of the
sensors in the STFT domain. Second, the DP-RTF is estimated by using the auto-
and cross-power spectral densities at each frequency and over multiple frames.
In the presence of stationary noise, an inter-frame spectral subtraction
algorithm is proposed, which enables to achieve the estimation of noise-free
auto- and cross-power spectral densities. Finally, the estimated DP-RTFs are
concatenated across frequencies and used as a feature vector for the
localization of speech source. Experiments with both simulated and real data
show that the proposed localization method performs well, even under severe
adverse acoustic conditions, and outperforms state-of-the-art localization
methods under most of the acoustic conditions."
"This paper presents a novel method for extracting acoustic features that
characterise the background environment in audio recordings. These features are
based on the output of an alignment that fits multiple parallel
background--based Constrained Maximum Likelihood Linear Regression
transformations asynchronously to the input audio signal. With this setup, the
resulting features can track changes in the audio background like appearance
and disappearance of music, applause or laughter, independently of the speakers
in the foreground of the audio. The ability to provide this type of acoustic
description in audiovisual data has many potential applications, including
automatic classification of broadcast archives or improving automatic
transcription and subtitling. In this paper, the performance of these features
in a genre identification task in a set of 332 BBC shows is explored. The
proposed background--tracking features outperform short--term Perceptual Linear
Prediction features in this task using Gaussian Mixture Model classifiers (62%
vs 72% accuracy). The use of more complex classifiers, Hidden Markov Models and
Support Vector Machines, increases the performance of the system with the novel
background--tracking features to 79% and 81% in accuracy respectively."
"In this contribution to the 3rd CHiME Speech Separation and Recognition
Challenge (CHiME-3) we extend the acoustic front-end of the CHiME-3 baseline
speech recognition system by a coherence-based Wiener filter which is applied
to the output signal of the baseline beamformer. To compute the time- and
frequency-dependent postfilter gains the ratio between direct and diffuse
signal components at the output of the baseline beamformer is estimated and
used as approximation of the short-time signal-to-noise ratio. The proposed
spectral enhancement technique is evaluated with respect to word error rates of
the CHiME-3 challenge baseline speech recognition system using real speech
recorded in public environments. Results confirm the effectiveness of the
coherence-based postfilter when integrated into the front-end signal
enhancement."
"In this paper, we propose a universal background model, named universal
background sparse coding (UBSC), for speaker verification. The proposed method
trains an ensemble of clusterings by data resampling, and produces sparse codes
from the clusterings by one-nearest-neighbor optimization plus binarization.
The main advantage of UBSC is that it does not suffer from local minima and
does not make Gaussian assumptions on data distributions. We evaluated UBSC on
a clean speech corpus---TIMIT. We used the cosine similarity and inner product
similarity as the scoring methods of a trial. Experimental results show that
UBSC is comparable to Gaussian mixture model."
"Reverberation is damaging to both the quality and the intelligibility of a
speech signal. We propose a novel single-channel method of dereverberation
based on a linear filter in the Short Time Fourier Transform domain. Each
enhanced frame is constructed from a linear sum of nearby frames based on the
channel impulse response. The results show that the method can resolve any
reverberant signal with knowledge of the impulse response to a non-reverberant
signal."
"The estimation of the decay rate of a signal section is an integral component
of both blind and non-blind reverberation time estimation methods. Several
decay rate estimators have previously been proposed, based on, e.g., linear
regression and maximum-likelihood estimation. Unfortunately, most approaches
are sensitive to background noise, and/or are fairly demanding in terms of
computational complexity. This paper presents a low complexity decay rate
estimator, robust to stationary noise, for reverberation time estimation.
Simulations using artificial signals, and experiments with speech in
ventilation noise, demonstrate the performance and noise robustness of the
proposed method."
"This paper presents our contribution to the 3rd CHiME Speech Separation and
Recognition Challenge. Our system uses Bidirectional Long Short-Term Memory
(BLSTM) Recurrent Neural Networks (RNNs) for Single-channel Speech Enhancement
(SSE). Networks are trained to predict clean speech as well as noise features
from noisy speech features. In addition, the system applies two methods of
dereverberation on the 6-channel recordings of the challenge. The first is the
Phase-Error based Filtering (PEF) that uses time-varying phase-error filters
based on estimated time-difference of arrival of the speech source and the
phases of the microphone signals. The second is the Correlation Shaping (CS)
that applies a reduction of the long-term correlation energy in reverberant
speech. The Linear Prediction (LP) residual is processed to suppress the
long-term correlation. Furthermore, the system employs a LSTM Language Model
(LM) to perform N-best rescoring of recognition hypotheses. Using the proposed
methods, an improved Word Error Rate (WER) of 24.38% is achieved over the real
eval test set. This is around 25% relative improvement over the challenge
baseline."
"Several established parameters and metrics have been used to characterize the
acoustics of a room. The most important are the Direct-To-Reverberant Ratio
(DRR), the Reverberation Time (T60) and the reflection coefficient. The
acoustic characteristics of a room based on such parameters can be used to
predict the quality and intelligibility of speech signals in that room.
Recently, several important methods in speech enhancement and speech
recognition have been developed that show an increase in performance compared
to the predecessors but do require knowledge of one or more fundamental
acoustical parameters such as the T60. Traditionally, these parameters have
been estimated using carefully measured Acoustic Impulse Responses (AIRs).
However, in most applications it is not practical or even possible to measure
the acoustic impulse response. Consequently, there is increasing research
activity in the estimation of such parameters directly from speech and audio
signals. The aim of this challenge was to evaluate state-of-the-art algorithms
for blind acoustic parameter estimation from speech and to promote the emerging
area of research in this field. Participants evaluated their algorithms for T60
and DRR estimation against the 'ground truth' values provided with the
data-sets and presented the results in a paper describing the method used."
"Reverberation Time (T60) is an important measure for characterizing the
properties of a room. The author's T60 estimation algorithm was previously
tested on simulated data where the noise is artificially added to the speech
after convolution with a impulse responses simulated using the image method. We
test the algorithm on speech convolved with real recorded impulse responses and
noise from the same rooms from the Acoustic Characterization of Environments
(ACE) corpus and achieve results comparable results to those using simulated
data."
"Polyphonic music files were analyzed using the set of symbols that produced
the Minimal Entropy Description which we call the Fundamental Scale. This
allowed us to create a novel space to represent music pieces by developing: a)
a method to adjust a description from its original scale of observation to a
general scale, b) the concept of higher order entropy as the entropy associated
to the deviations of a frequency ranked symbol profile from a perfect Zipf
profile. We called this diversity index the ""2nd Order Entropy"". Applying these
methods to a variety of musical pieces showed how the space of ""symbolic
specific diversity-entropy"" and that of ""2nd order entropy"" captures
characteristics that are unique to each music type, style, composer and genre.
Some clustering of these properties around each musical category is shown. This
method allows to visualize a historic trajectory of academic music across this
space, from medieval to contemporary academic music. We show that description
of musical structures using entropy and symbolic diversity allows to
characterize traditional and popular expressions of music. These classification
techniques promise to be useful in other disciplines for pattern recognition
and machine learning, for example."
"In this study, we investigate a solution to reduce the effect of one-to-many
problem in voice conversion. One-to-many problem in VC happens when two very
similar speech segments in source speaker have corresponding speech segments in
target speaker that are not similar to each other. As a result, the mapper
function usually over-smoothes the generated features in order to be similar to
both target speech segments. In this study, we propose to equalize the formant
location of source-target frame pairs using dynamic frequency warping in order
to reduce the complexity. After the conversion, another dynamic frequency
warping is further applied to reverse the effect of formant location
equalization during the training. The subjective experiments showed that the
proposed approach improves the speech quality significantly."
"This paper addresses the problem of separating audio sources from
time-varying convolutive mixtures. We propose a probabilistic framework based
on the local complex-Gaussian model combined with non-negative matrix
factorization. The time-varying mixing filters are modeled by a continuous
temporal stochastic process. We present a variational expectation-maximization
(VEM) algorithm that employs a Kalman smoother to estimate the time-varying
mixing matrix, and that jointly estimate the source parameters. The sound
sources are then separated by Wiener filters constructed with the estimators
provided by the VEM algorithm. Extensive experiments on simulated data show
that the proposed method outperforms a block-wise version of a state-of-the-art
baseline method."
"We present a single channel data driven method for non-intrusive estimation
of full-band reverberation time and full-band direct-to-reverberant ratio. The
method extracts a number of features from reverberant speech and builds a model
using a recurrent neural network to estimate the reverberant acoustic
parameters. We explore three configurations by including different data and
also by combining the recurrent neural network estimates using a support vector
machine. Our best method to estimate DRR provides a Root Mean Square Deviation
(RMSD) of 3.84 dB and a RMSD of 43.19 % for T60 estimation."
"Blind estimation of acoustic room parameters such as the reverberation time
$T_\mathrm{60}$ and the direct-to-reverberation ratio ($\mathrm{DRR}$) is still
a challenging task, especially in case of blind estimation from reverberant
speech signals. In this work, a novel approach is proposed for joint estimation
of $T_\mathrm{60}$ and $\mathrm{DRR}$ from wideband speech in noisy conditions.
2D Gabor filters arranged in a filterbank are exploited for extracting
features, which are then used as input to a multi-layer perceptron (MLP). The
MLP output neurons correspond to specific pairs of $(T_\mathrm{60},
\mathrm{DRR})$ estimates; the output is integrated over time, and a simple
decision rule results in our estimate. The approach is applied to
single-microphone fullband speech signals provided by the Acoustic
Characterization of Environments (ACE) Challenge. Our approach outperforms the
baseline systems with median errors of close-to-zero and -1.5 dB for the
$T_\mathrm{60}$ and $\mathrm{DRR}$ estimates, respectively, while the
calculation of estimates is 5.8 times faster compared to the baseline."
"Reverberation, especially in large rooms, severely degrades speech
recognition performance and speech intelligibility. Since direct measurement of
room characteristics is usually not possible, blind estimation of
reverberation-related metrics such as the reverberation time (RT) and the
direct-to-reverberant energy ratio (DRR) can be valuable information to speech
recognition and enhancement algorithms operating in enclosed environments. The
objective of this work is to evaluate the performance of five variants of blind
RT and DRR estimators based on a modulation spectrum representation of
reverberant speech with single- and multi-channel speech data. These models are
all based on variants of the so-called Speech-to-Reverberation Modulation
Energy Ratio (SRMR). We show that these measures outperform a state-of-the-art
baseline based on maximum-likelihood estimation of sound decay rates in terms
of root-mean square error (RMSE), as well as Pearson correlation. Compared to
the baseline, the best proposed measure, called NSRMR_k , achieves a 23%
relative improvement in terms of RMSE and allows for relative correlation
improvements ranging from 13% to 47% for RT prediction."
"In this paper we present a single-microphone speech enhancement algorithm. A
hybrid approach is proposed merging the generative mixture of Gaussians (MoG)
model and the discriminative neural network (NN). The proposed algorithm is
executed in two phases, the training phase, which does not recur, and the test
phase. First, the noise-free speech power spectral density (PSD) is modeled as
a MoG, representing the phoneme based diversity in the speech signal. An NN is
then trained with phoneme labeled database for phoneme classification with
mel-frequency cepstral coefficients (MFCC) as the input features. Given the
phoneme classification results, a speech presence probability (SPP) is obtained
using both the generative and discriminative models. Soft spectral subtraction
is then executed while simultaneously, the noise estimation is updated. The
discriminative NN maintain the continuity of the speech and the generative
phoneme-based MoG preserves the speech spectral structure. Extensive
experimental study using real speech and noise signals is provided. We also
compare the proposed algorithm with alternative speech enhancement algorithms.
We show that we obtain a significant improvement over previous methods in terms
of both speech quality measures and speech recognition results."
"Direct-to-Reverberant Ratio (DRR) is an important measure for characterizing
the properties of a room. The recently proposed DRR Estimation using a
Null-Steered Beamformer (DENBE) algorithm was originally tested on simulated
data where noise was artificially added to the speech after convolution with
impulse responses simulated using the image-source method. This paper evaluates
the performance of this algorithm on speech convolved with measured impulse
responses and noise using the Acoustic Characterization of Environments (ACE)
Evaluation corpus. The fullband DRR estimation performance of the DENBE
algorithm exceeds that of the baselines in all Signal-to-Noise Ratios (SNRs)
and noise types. In addition, estimation of the DRR in one third-octave ISO
frequency bands is demonstrated."
"A dictionary learning based audio source classification algorithm is proposed
to classify a sample audio signal as one amongst a finite set of different
audio sources. Cosine similarity measure is used to select the atoms during
dictionary learning. Based on three objective measures proposed, namely, signal
to distortion ratio (SDR), the number of non-zero weights and the sum of
weights, a frame-wise source classification accuracy of 98.2% is obtained for
twelve different sources. Cent percent accuracy has been obtained using moving
SDR accumulated over six successive frames for ten of the audio sources tested,
while the two other sources require accumulation of 10 and 14 frames."
"This report introduces a new corpus of music, speech, and noise. This dataset
is suitable for training models for voice activity detection (VAD) and
music/speech discrimination. Our corpus is released under a flexible Creative
Commons license. The dataset consists of music from several genres, speech from
twelve languages, and a wide assortment of technical and non-technical noises.
We demonstrate use of this corpus for music/speech discrimination on Broadcast
news and VAD for speaker identification."
"This paper proposes a practical approach to estimate the
direct-to-reverberant energy ratio (DRR) using a spherical microphone array
without having knowledge of the source signal. We base our estimation on a
theoretical relationship between the DRR and the coherence estimation function
between coincident pressure and particle velocity. We discuss the proposed
method's ability to estimate the DRR in a wide variety of room sizes,
reverberation times and source receiver distances with appropriate examples.
Test results show that the method can estimate the room DRR for frequencies
between 199 - 2511 Hz, with $\pm$ 3 dB accuracy."
"A method for estimation of direct-to-reverberant ratio (DRR) using a
microphone array is proposed. The proposed method estimates the power spectral
density (PSD) of the direct sound and the reverberation using the algorithm
\textit{PSD estimation in beamspace} with a microphone array and calculates the
DRR of the observed signal. The speech corpus of the ACE (Acoustic
Characterisation of Environments) Challenge was utilised for evaluating the
practical feasibility of the proposed method. The experimental results revealed
that the proposed method was able to effectively estimate the DRR from a
recording of a reverberant speech signal which included various environmental
noise."
"This paper addresses the detection of periodic transients in vibration
signals for detecting faults in rotating machines. For this purpose, we present
a method to estimate periodic-group-sparse signals in noise. The method is
based on the formulation of a convex optimization problem. A fast iterative
algorithm is given for its solution. A simulated signal is formulated to verify
the performance of the proposed approach for periodic feature extraction. The
detection performance of comparative methods is compared with that of the
proposed approach via RMSE values and receiver operating characteristic (ROC)
curves. Finally, the proposed approach is applied to compound faults diagnosis
of motor bearings. The non-stationary vibration data were acquired from a
SpectraQuest's machinery fault simulator. The processed results show the
proposed approach can effectively detect and extract the useful features of
bearing outer race and inner race defect."
"This paper addresses the problem of extracting periodic oscillatory features
in vibration sig- nals for detecting faults in rotating machinery. To extract
the feature, we propose an approach in the short-time Fourier transform (STFT)
domain where the periodic oscillatory feature man- ifests itself as a
relatively sparse grid. To estimate the sparse grid, we formulate an
optimization problem using customized binary weights in the regularizer, where
the weights are formulated to promote periodicity. In order to solve the
proposed optimization problem, we develop an algorithm called augmented
Lagrangian majorization-minimization algorithm, which combines the split
augmented Lagrangian shrinkage algorithm (SALSA) with majorization-minimization
(MM), and is guaranteed to converge for both convex and non-convex formulation.
As examples, the proposed approach is applied to simulated data, and used as a
tool for diagnosing faults in bearings and gearboxes for real data, and
compared to some state-of-the-art methods. The results show the proposed
approach can effectively detect and extract the periodical oscillatory
features."
"Both harmonic and binaural signal properties are relevant for auditory
processing. To investigate how these cues combine in the auditory system,
detection thresholds for an 800-Hz tone masked by a diotic (i.e., identical
between the ears) harmonic complex tone were measured in six normal-hearing
subjects. The target tone was presented either diotically or with an interaural
phase difference (IPD) of 180 degree and in either harmonic or ""mistuned""
relationship to the diotic masker. Three different maskers were used, a
resolved and an unresolved complex tone (fundamental frequency: 160 and 40 Hz)
with four components below and above the target frequency and a broadband
unresolved complex tone with 12 additional components. The target IPD provided
release from masking in most masker conditions, whereas mistuning led to a
significant release from masking only in the diotic conditions with the
resolved and the narrowband unresolved maskers. A significant effect of
mistuning was neither found in the diotic condition with the wideband
unresolved masker nor in any of the dichotic conditions. An auditory model with
a single analysis frequency band and different binaural processing schemes was
employed to predict the data of the unresolved masker conditions. Sensitivity
to modulation cues was achieved by including an auditory-motivated modulation
filter in the processing pathway. The predictions of the diotic data were in
line with the experimental results and literature data in the narrowband
condition, but not in the broadband condition, suggesting that across-frequency
processing is involved in processing modulation information. The experimental
and model results in the dichotic conditions show that the binaural processor
cannot exploit modulation information in binaurally unmasked conditions."
"This contribution presents four algorithms developed by the authors for
single-channel fullband and subband T60 estimation within the ACE challenge.
The blind estimation of the fullband reverberation time (RT) by
maximum-likelihood (ML) estimation based on [15] is considered as baseline
approach. An improvement of this algorithm is devised where an energy-weighted
averaging of the upper subband RT estimates is performed using either a DCT or
1/3-octave filter-bank. The evaluation results show that this approach leads to
a lower variance for the estimation error in comparison to the baseline
approach at the price of an increased computational complexity. Moreover, a new
algorithm to estimate the subband RT is devised, where the RT estimates for the
lower octave subbands are extrapolated from the RT estimates of the upper
subbands by means of a simple model for the frequency-dependency of the subband
RT. The evaluation results of the ACE challenge reveal that this approach
allows to estimate the subband RT with an estimation error which is in a
similar range as for the presented fullband RT estimators."
"We propose a novel application based on acoustic-to-articulatory inversion
towards quality assessment of voice converted speech. The ability of humans to
speak effortlessly requires coordinated movements of various articulators,
muscles, etc. This effortless movement contributes towards naturalness,
intelligibility and speakers identity which is partially present in voice
converted speech. Hence, during voice conversion, the information related to
speech production is lost. In this paper, this loss is quantified for male
voice, by showing increase in RMSE error for voice converted speech followed by
showing decrease in mutual information. Similar results are obtained in case of
female voice. This observation is extended by showing that articulatory
features can be used as an objective measure. The effectiveness of proposed
measure over MCD is illustrated by comparing their correlation with Mean
Opinion Score."
"This paper resumes the results of a research conducted in a music production
situation Therefore, it is more a final lab report, a prospective methodology
then a scientific experience. The methodology we are presenting was developed
as an answer to a musical problem raised by the Italian composer Marta
Gentilucci. The problem was ""how to extract a temporal structure from a vowel
tremolo, on a tenuto (steady state) pitch."" The musical goal was to apply, in a
compositional context the vowel tremolo time structure on a tenuto pitch chord,
as a transposition control.In this context we decide to follow, to explore the
potential of low-level MPEG7 audio descriptors to build event detection
functions. One of the main problems using low-level audio descriptors in audio
analysis is the redundancy of information among them. We describe an ""ad hoc""
interactive methodology, based on side effect use of dimensionality reduction
by PCA, to choose a feature from a set of low-level audio descriptors, to be
used to detect a vowel tremolo rhythm. This methodology is supposed to be
interactive and easy enough to be used in a live creative context."
"Voice conversion methods have advanced rapidly over the last decade. Studies
have shown that speaker characteristics are captured by spectral feature as
well as various prosodic features. Most existing conversion methods focus on
the spectral feature as it directly represents the timbre characteristics,
while some conversion methods have focused only on the prosodic feature
represented by the fundamental frequency. In this paper, a comprehensive
framework using deep neural networks to convert both timbre and prosodic
features is proposed. The timbre feature is represented by a high-resolution
spectral feature. The prosodic features include F0, intensity and duration. It
is well known that DNN is useful as a tool to model high-dimensional features.
In this work, we show that DNN initialized by our proposed autoencoder
pretraining yields good quality DNN conversion models. This pretraining is
tailor-made for voice conversion and leverages on autoencoder to capture the
generic spectral shape of source speech. Additionally, our framework uses
segmental DNN models to capture the evolution of the prosodic features over
time. To reconstruct the converted speech, the spectral feature produced by the
DNN model is combined with the three prosodic features produced by the DNN
segmental models. Our experimental results show that the application of both
prosodic and high-resolution spectral features leads to quality converted
speech as measured by objective evaluation and subjective listening tests."
"We introduce the joint time-frequency scattering transform, a time shift
invariant descriptor of time-frequency structure for audio classification. It
is obtained by applying a two-dimensional wavelet transform in time and
log-frequency to a time-frequency wavelet scalogram. We show that this
descriptor successfully characterizes complex time-frequency phenomena such as
time-varying filters and frequency modulated excitations. State-of-the-art
results are achieved for signal reconstruction and phone segment classification
on the TIMIT dataset."
"The steered response power phase transform (SRP-PHAT) is a beamformer method
very attractive in acoustic localization applications due to its robustness in
reverberant environments. This paper presents a spatial grid design procedure,
called the geometrically sampled grid (GSG), which aims at computing the
spatial grid by taking into account the discrete sampling of time difference of
arrival (TDOA) functions and the desired spatial resolution. A new SRP-PHAT
localization algorithm based on the GSG method is also introduced. The proposed
method exploits the intersections of the discrete hyperboloids representing the
TDOA information domain of the sensor array, and projects the whole TDOA
information on the space search grid. The GSG method thus allows to design the
sampled spatial grid which represents the best search grid for a given sensor
array, it allows to perform a sensitivity analysis of the array and to
characterize its spatial localization accuracy, and it may assist the system
designer in the reconfiguration of the array. Experimental results using both
simulated data and real recordings show that the localization accuracy is
substantially improved both for high and for low spatial resolution, and that
it is closely related to the proposed power response sensitivity measure."
"A state-of-the-art 1D acoustic synthesizer has been previously developed, and
coupled to speaker-specific biomechanical models of oropharynx in ArtiSynth. As
expected, the formant frequencies of the synthesized vowel sounds were shown to
be different from those of the recorded audio. Such discrepancy was
hypothesized to be due to the simplified geometry of the vocal tract model as
well as the one dimensional implementation of Navier-Stokes equations. In this
paper, we calculate Helmholtz resonances of our vocal tract geometries using 3D
finite element method (FEM), and compare them with the formant frequencies
obtained from the 1D method and audio. We hope such comparison helps with
clarifying the limitations of our current models and/or speech synthesizer."
"In this paper, a Blind Source Separation (BSS) algorithm for multichannel
audio contents is proposed. Unlike common BSS algorithms targeting stereo audio
contents or microphone array signals, our technique is targeted at multichannel
audio such as 5.1 and 7.1ch audio. Since most multichannel audio object sources
are panned using the Inter-channel Loudness Difference (ILD), we employ the
ILVS (Inter-channel Loudness Vector Sum) concept to cluster common signals
(such as background music) from each channel. After separating the common
signals from each channel, we employ an Expectation Maximization (EM) algorithm
with a von-Mises distribution to successfully classify the clustering of sound
source objects and separate the audio signals from the original mixture. Our
proposed method can therefore separate common audio signals and object source
signals from multiple channels with reasonable quality. Our multichannel audio
content separation technique can be applied to an upmix system or a cinema
audio system requiring multichannel audio source separation."
"Human laugh is able to convey various kinds of meanings in human
communications. There exists various kinds of human laugh signal, for example:
vocalized laugh and non vocalized laugh. Following the theories of psychology,
among all the vocalized laugh type, rhythmic staccato-vocalization
significantly evokes the positive responses in the interactions. In this paper
we attempt to exploit this observation to detect human laugh occurrences, i.e.,
the laughter, in multiparty conversations from the AMI meeting corpus. First,
we separate the high energy frames from speech, leaving out the low energy
frames through power spectral density estimation. We borrow the algorithm of
rhythm detection from the area of music analysis to use that on the high energy
frames. Finally, we detect rhythmic laugh frames, analyzing the candidate
rhythmic frames using statistics. This novel approach for detection of
`positive' rhythmic human laughter performs better than the standard laughter
classification baseline."
"Gender recognition is an essential component of automatic speech recognition
and interactive voice response systems. Determining gender of the speaker
reduces the computational burden of such systems for any further processing.
Typical methods for gender recognition from speech largely depend on features
extraction and classification processes. The purpose of this study is to
evaluate the performance of various state-of-the-art classification methods
along with tuning their parameters for helping selection of the optimal
classification methods for gender recognition tasks. Five classification
schemes including k-nearest neighbor, na\""ive Bayes, multilayer perceptron,
random forest, and support vector machine are comprehensively evaluated for
determination of gender from telephonic speech using the Mel-frequency cepstral
coefficients. Different experiments were performed to determine the effects of
training data sizes, length of the speech streams, and parameter tuning on
classification performance. Results suggest that SVM is the best classifier
among all the five schemes for gender recognition."
"A method is proposed which enables one to produce musical compositions by
using transposition in place of harmonic progression. A transposition scale is
introduced to provide a set of intervals commensurate with the musical scale,
such as chromatic or just intonation scales. A sequence of intervals selected
from the transposition scale is used to shift instrument frequency at
predefined times during the composition which serves as a harmonic sequence of
a composition. A transposition sequence constructed in such a way can be
extended to a hierarchy of sequences. The fundamental sound frequency of an
instrument is obtained as a product of the base frequency, instrument key
factor, and a cumulative product of respective factors from all the harmonic
sequences. The multiplication factors are selected from subsets of rational
numbers, which form instrument scales and transposition scales of different
levels. Each harmonic sequence can be related to its own transposition scale,
or a single scale can be used for all levels. When composing for an orchestra
of instruments, harmonic sequences and instrument scales can be assigned
independently to each musical instrument. The method solves the problem of
using just intonation scale across multiple octaves as well as simplifies
writing of instrument scores."
"For most of the state-of-the-art speech enhancement techniques, a spectrogram
is usually preferred than the respective time-domain raw data since it reveals
more compact presentation together with conspicuous temporal information over a
long time span. However, the short-time Fourier transform (STFT) that creates
the spectrogram in general distorts the original signal and thereby limits the
capability of the associated speech enhancement techniques. In this study, we
propose a novel speech enhancement method that adopts the algorithms of
discrete wavelet packet transform (DWPT) and nonnegative matrix factorization
(NMF) in order to conquer the aforementioned limitation. In brief, the DWPT is
first applied to split a time-domain speech signal into a series of subband
signals without introducing any distortion. Then we exploit NMF to highlight
the speech component for each subband. Finally, the enhanced subband signals
are joined together via the inverse DWPT to reconstruct a noise-reduced signal
in time domain. We evaluate the proposed DWPT-NMF based speech enhancement
method on the MHINT task. Experimental results show that this new method
behaves very well in prompting speech quality and intelligibility and it
outperforms the convnenitional STFT-NMF based method."
"This paper addresses the problem of noise reduction with simultaneous
components extrac- tion in vibration signals for faults diagnosis of bearing.
The observed vibration signal is modeled as a summation of two components
contaminated by noise, and each component composes of repetitive transients. To
extract the two components simultaneously, an approach by solving an
optimization problem is proposed in this paper. The problem adopts convex
sparsity-based regularization scheme for decomposition, and non-convex
regularization is used to further promote the sparsity but preserving the
global convexity. A synthetic example is presented to illustrate the
performance of the proposed approach for repetitive feature extraction. The
performance and effectiveness of the proposed method are further demonstrated
by applying to compound faults and single fault diagnosis of a locomotive
bearing. The results show the proposed approach can effectively extract the
features of outer and inner race defects."
"Tabla, a percussion instrument, mainly used to accompany vocalists,
instrumentalists and dancers in every style of music from classical to light in
India, mainly used for keeping rhythm. This percussion instrument consists of
two drums played by two hands, structurally different and produces different
harmonic sounds. Earlier work has done labeling tabla strokes from real time
performances by testing neural networks and tree based classification methods.
The current work extends previous work by C. V. Raman and S. Kumar in 1920 on
spectrum modeling of tabla strokes. In this paper we have studied spectral
characteristics (by wavelet analysis by sub band coding method and using
torrence wavelet tool) of nine strokes from each of five tablas using Wavelet
transform. Wavelet analysis is now a common tool for analyzing localized
variations of power within a time series and to find the frequency distribution
in time frequency space. Statistically, we will look into the patterns depicted
by harmonics of different sub bands and the tablas. Distribution of dominant
frequencies at different sub-band of stroke signals, distribution of power and
behavior of harmonics are the important features, leads to categorization of
tabla."
"Even though chord roots constitute a fundamental concept in music theory,
existing models do not explain and determine them to full satisfaction. We
present a new method which takes sequential context into account to resolve
ambiguities and detect nonharmonic tones. We extract features from chord pairs
and use a decision tree to determine chord roots. This leads to a quantitative
improvement in correctness of the predicted roots in comparison to other
models. All this raises the question how much harmonic and nonharmonic tones
actually contribute to the perception of chord roots."
"Many audio applications rely on filter banks (FBs) to analyze, process, and
re-synthesize sounds. To approximate the auditory frequency resolution in the
signal chain, some applications rely on perceptually motivated FBs, the
gammatone FB being a popular example. However, most perceptually motivated FBs
only allow partial signal reconstruction at high redundancies and/or do not
have good resistance to sub-channel processing. This paper introduces an
oversampled perceptually motivated FB enabling perfect reconstruction,
efficient FB design, and adaptable redundancy. The filters are directly
constructed in the frequency domain and linearly distributed on a perceptual
frequency scale (e.g. ERB, Bark, or Mel scale). The proposed design allows for
various filter shapes, uniform or non-uniform FB setting, and large
down-sampling factors. For redundancies $\geq$ 3 perfect reconstruction is
achieved by computing the canonical dual FB analytically. For lower
redundancies perfect reconstruction is achieved using an iterative method.
Experiments show performance improvements of the proposed approach when
compared to the gammatone FB in terms of reconstruction error and resistance to
sub-channel processing, especially at low redundancies."
"Signal processing applications use sinusoidal modelling for speech synthesis,
speech coding, and audio coding. Estimation of the model parameters involves
non-linear optimisation methods, which can be very costly for real-time
applications. We propose a low-complexity iterative method that starts from
initial frequency estimates and converges rapidly. We show that for N sinusoids
in a frame of length L, the proposed method has a complexity of O(LN), which is
significantly less than the matching pursuits method. Furthermore, the proposed
method is shown to be more accurate than the matching pursuits and
time-frequency reassignment methods in our experiments."
"Smartglasses, in addition to their visual-output capabilities, often contain
acoustic sensors for receiving the user's voice. However, operation in noisy
environments may lead to significant degradation of the received signal. To
address this issue, we propose employing an acoustic sensor array which is
mounted on the eyeglasses frames. The signals from the array are processed by
an algorithm with the purpose of acquiring the user's desired near-filed speech
signal while suppressing noise signals originating from the environment. The
array is comprised of two AVSs which are located at the fore of the glasses'
temples. Each AVS consists of four collocated subsensors: one pressure sensor
(with an omnidirectional response) and three particle-velocity sensors (with
dipole responses) oriented in mutually orthogonal directions. The array
configuration is designed to boost the input power of the desired signal, and
to ensure that the characteristics of the noise at the different channels are
sufficiently diverse (lending towards more effective noise suppression). Since
changes in the array's position correspond to the desired speaker's movement,
the relative source-receiver position remains unchanged; hence, the need to
track fluctuations of the steering vector is avoided. Conversely, the spatial
statistics of the noise are subject to rapid and abrupt changes due to sudden
movement and rotation of the user's head. Consequently, the algorithm must be
capable of rapid adaptation. We propose an algorithm which incorporates
detection of the desired speech in the time-frequency domain, and employs this
information to adaptively update estimates of the noise statistics. Speech
detection plays a key role in ensuring the quality of the output signal. We
conduct controlled measurements of the array in noisy scenarios. The proposed
algorithm preforms favorably with respect to conventional algorithms."
"This report describes the use of a support vector machines with a novel
kernel, to determine the breathing rate and inhalation duration of a fire
fighter wearing a Self-Contained Breathing Apparatus. With this information, an
incident commander can monitor the firemen in his command for exhaustion and
ensure timely rotation of personnel to ensure overall fire fighter safety"
"Wireless telephone speech is usually limited to the 300-3400 Hz band, which
reduces its quality. There is thus a growing demand for wideband speech systems
that transmit from 50 Hz to 8000 Hz. This paper presents an algorithm to
generate wideband speech from narrowband speech using as low as 500 bits/s of
side information. The 50-300 Hz band is predicted from the narrowband signal. A
source-excitation model is used for the 3400-8000 Hz band, where the excitation
is extrapolated at the receiver, and the spectral envelope is transmitted.
Though some artifacts are present, the resulting wideband speech has enhanced
quality compared to narrowband speech."
"In the past few years, several case studies have illustrated that the use of
occupancy information in buildings leads to energy-efficient and low-cost HVAC
operation. The widely presented techniques for occupancy estimation include
temperature, humidity, CO2 concentration, image camera, motion sensor and
passive infrared (PIR) sensor. So far little studies have been reported in
literature to utilize audio and speech processing as indoor occupancy
prediction technique. With rapid advances of audio and speech processing
technologies, nowadays it is more feasible and attractive to integrate
audio-based signal processing component into smart buildings. In this work, we
propose to utilize audio processing techniques (i.e., speaker recognition and
background audio energy estimation) to estimate room occupancy (i.e., the
number of people inside a room). Theoretical analysis and simulation results
demonstrate the accuracy and effectiveness of this proposed occupancy
estimation technique. Based on the occupancy estimation, smart buildings will
adjust the thermostat setups and HVAC operations, thus, achieving greater
quality of service and drastic cost savings."
"Acoustic echo cancellation with stereo signals is generally an
under-determined problem because of the high coherence between the left and
right channels. In this paper, we present a novel method of significantly
reducing inter-channel coherence without affecting the audio quality. Our work
takes into account psychoacoustic masking and binaural auditory cues. The
proposed non-linear processing combines a shaped comb-allpass (SCAL) filter
with the injection of psychoacoustically masked noise. We show that the
proposed method performs significantly better than other known methods for
reducing inter-channel coherence."
"The Speex project has been started in 2002 to address the need for a free,
open-source speech codec. Speex is based on the Code Excited Linear Prediction
(CELP) algorithm and, unlike the previously existing Vorbis codec, is optimised
for transmitting speech for low latency communication over an unreliable packet
network. This paper presents an overview of Speex, the technology involved in
it and how it can be used in applications. The most recent developments in
Speex, such as the fixed-point port, acoustic echo cancellation and noise
suppression are also addressed."
"Sinusoidal parameter estimation is a computationally-intensive task, which
can pose problems for real-time implementations. In this paper, we propose a
low-complexity iterative method for estimating sinusoidal parameters that is
based on the linearisation of the model around an initial frequency estimate.
We show that for N sinusoids in a frame of length L, the proposed method has a
complexity of O(LN), which is significantly less than the matching pursuits
method. Furthermore, the proposed method is shown to be more accurate than the
matching pursuits and time frequency reassignment methods in our experiments."
"One key aspect of the CELP algorithm is that it shapes the coding noise using
a simple, yet effective, weighting filter. In this paper, we improve the noise
shaping of CELP using a more modern psychoacoustic model. This has the
significant advantage of improving the quality of an existing codec without the
need to change the bit-stream. More specifically, we improve the Speex CELP
codec by using the psychoacoustic model used in the Vorbis audio codec. The
results show a significant increase in quality, especially at high bit-rates,
where the improvement is equivalent to a 20% reduction in bit-rate. The
technique itself is not specific to Speex and could be applied to other CELP
codecs."
"Microphone array post-filters have demonstrated their ability to greatly
reduce noise at the output of a beamformer. However, current techniques only
consider a single source of interest, most of the time assuming stationary
background noise. We propose a microphone array post-filter that enhances the
signals produced by the separation of simultaneous sources using common source
separation algorithms. Our method is based on a loudness-domain optimal
spectral estimator and on the assumption that the noise can be described as the
sum of a stationary component and of a transient component that is due to
leakage between the channels of the initial source separation algorithm. The
system is evaluated in the context of mobile robotics and is shown to produce
better results than current post-filtering techniques, greatly reducing
interference while causing little distortion to the signal of interest, even at
very low SNR."
"In this paper, we address an important problem in high-quality audio
communication systems. Acoustic echo cancellation with stereo signals is
generally an under-determined problem because of the generally important
correlation that exists between the left and right channels. In this paper, we
present a novel method of significantly reducing that correlation without
affecting the audio quality. This method is perceptually motivated and combines
a shaped comb-allpass (SCAL) filter with the injection of psychoacoustically
masked noise. We show that the proposed method performs significantly better
than other known methods for channel decorrelation."
"Automatic speaker verification (ASV) technology is recently finding its way
to end-user applications for secure access to personal data, smart services or
physical facilities. Similar to other biometric technologies, speaker
verification is vulnerable to spoofing attacks where an attacker masquerades as
a particular target speaker via impersonation, replay, text-to-speech (TTS) or
voice conversion (VC) techniques to gain illegitimate access to the system. We
focus on TTS and VC that represent the most flexible, high-end spoofing
attacks. Most of the prior studies on synthesized or converted speech detection
report their findings using high-quality clean recordings. Meanwhile, the
performance of spoofing detectors in the presence of additive noise, an
important consideration in practical ASV implementations, remains largely
unknown. To this end, we analyze the suitability of state-of-the-art synthetic
speech detectors under additive noise with a special focus on front-end
features. Our comparison includes eight acoustic feature sets, five related to
spectral magnitude and three to spectral phase information. Our extensive
experiments on ASVSpoof 2015 corpus reveal several important findings. Firstly,
all the countermeasures break down even at relatively high signal-to-noise
ratios (SNRs) and fail to generalize to noisy conditions. Secondly, speech
enhancement is not found helpful. Thirdly, GMM back-end generally outperforms
the more involved i-vector back-end. Fourthly, concerning the compared
features, the Mel-frequency cepstral coefficients (MFCCs) and subband spectral
centroid magnitude coefficients (SCMCs) perform the best on average though the
winner method depends on SNR and noise type. Finally, a study with two score
fusion strategies shows that combining different feature based systems improves
recognition accuracy for known and unknown attacks in both clean and noisy
conditions."
"This paper presents an approach to model melodies (and music pieces in
general) as networks. Notes of a melody can be seen as nodes of a network that
are connected whenever these are played in sequence. This creates a directed
graph. By using complex network theory, it is possible to extract some main
metrics, typical of networks, that characterize the piece. Using this
framework, we provide an analysis on a set of guitar solos performed by main
musicians. The results of this study indicate that this model can have an
impact on multimedia applications such as music classification, identification,
and automatic music generation."
"Phase processing has been replaced by group delay processing for the
extraction of source and system parameters from speech. Group delay functions
are ill-behaved when the transfer function has zeros that are close to unit
circle in the z-domain. The modified group delay function addresses this
problem and has been successfully used for formant and monopitch estimation. In
this paper, modified group delay functions are used for multipitch estimation
in concurrent speech. The power spectrum of the speech is first flattened in
order to annihilate the system characteristics, while retaining the source
characteristics. Group delay analysis on this flattened spectrum picks the
predominant pitch in the first pass and a comb filter is used to filter out the
estimated pitch along with its harmonics. The residual spectrum is again
analyzed for the next candidate pitch estimate in the second pass. The final
pitch trajectories of the constituent speech utterances are formed using pitch
grouping and post processing techniques. The performance of the proposed
algorithm was evaluated on standard datasets using two metrics; pitch accuracy
and standard deviation of fine pitch error. Our results show that the proposed
algorithm is a promising pitch detection method in multipitch environment for
real speech recordings."
"In this paper, we propose a novelmethod to search for precise locations of
paired note onset and offset in a singing voice signal. In comparison with the
existing onset detection algorithms,our approach differs in two key respects.
First, we employ Correntropy, a generalized correlation function inspired from
Reyni's entropy, as a detection function to capture the instantaneous flux
while preserving insensitiveness to outliers. Next, a novel peak picking
algorithm is specially designed for this detection function. By calculating the
fitness of a pre-defined inverse hyperbolic kernel to a detection function, it
is possible to find an onset and its corresponding offset simultaneously.
Experimental results show that the proposed method achieves performance
significantly better than or comparable to other state-of-the-art techniques
for onset detection in singing voice."
"Many approaches have been used in bird species classification from their
sound in order to provide labels for the whole of a recording. However, a more
precise classification of each bird vocalization would be of great importance
to the use and management of sound archives and bird monitoring. In this work,
we introduce a technique that using a two step process can first automatically
detect all bird vocalizations and then, with the use of 'weakly' labelled
recordings, classify them. Evaluations of our proposed method show that it
achieves a correct classification of 61% when used in a synthetic dataset, and
up to 89% when the synthetic dataset only consists of vocalizations larger than
1000 pixels."
"Bird calls range from simple tones to rich dynamic multi-harmonic structures.
The more complex calls are very poorly understood at present, such as those of
the scientifically important corvid family (jackdaws, crows, ravens, etc.).
Individual birds can recognise familiar individuals from calls, but where in
the signal is this identity encoded? We studied the question by applying a
combination of feature representations to a dataset of jackdaw calls, including
linear predictive coding (LPC) and adaptive discrete Fourier transform (aDFT).
We demonstrate through a classification paradigm that we can strongly
outperform a standard spectrogram representation for identifying individuals,
and we apply metric learning to determine which time-frequency regions
contribute most strongly to robust individual identification. Computational
methods can help to direct our search for understanding of these complex
biological signals."
"In this work, a recently proposed Head-Related Transfer Function (HRTF)-based
Robust Least-Squares Frequency-Invariant (RLSFI) beamformer design is analyzed
with respect to its robustness against localization errors, which lead to a
mismatch between the HRTFs corresponding to the actual target source position
and the HRTFs which have been used for the beamformer design. The impact of
this mismatch on the performance of the HRTF-based RLSFI beamformer is
evaluated, including a comparison to the free-field-based beamformer design,
using signal-based measures and word error rates for an off-the-shelf speech
recognizer."
"Musical chords, harmonies or melodies in Just Intonation have note
frequencies which are described by a base frequency multiplied by rational
numbers. For any local section, these notes can be converted to some base
frequency multiplied by whole positive numbers. The structure of the chord can
be analysed mathematically by finding functions which are unchanged upon chord
transposition. These functions are are denoted invariant, and are important for
understanding the structure of harmony. Each chord described by whole numbers
has a greatest common divisor, GCD, and a lowest common multiple, LCM. The
ratio of these is denoted Complexity which is a positive whole number. The set
of divisors of Complexity give a subset of a p limit tone lattice and have both
a natural ordering and a multiplicative structure. The position and orientation
of the original chord, on the ordered set or on the lattice, give rise to many
other invariant functions including measures for otonality and utonality. Other
invariant functions can be constructed from: ratios between note pairs, prime
projections, weighted chords which incorporate loudness. Given a set of
conditions described by invariant functions, algorithms can be developed to
find all scales or chords meeting those conditions, allowing the classification
of consonant harmonies up to specified limits."
"Since the start of Indian cinema, a number of films have been made where a
particular song is based on a certain raga. These songs have been taking a
major role in spreading the essence of classical music to the common people,
who have no formal exposure to classical music. In this paper, we look to
explore what are the particular features of a certain raga which make it
understandable to common people and enrich the song to a great extent. For
this, we chose two common ragas of Hindustani classical music, namely Bhairav
and Mian ki Malhar which are known to have widespread application in popular
film music. We have taken 3 minute clips of these two ragas from the renderings
of two eminent maestros of Hindustani classical music. 3 min clips of ten (10)
widely popular songs of Bollywood films were selected for analysis. These were
analyzed with the help of a latest non linear analysis technique called
Multifractal Detrended Cross correlation Analysis (MFDXA). With this technique,
all parts of the Film music and the renderings from the eminent maestros are
analyzed to find out a cross correlation coefficient ({\gamma}x) which gives
the degree of correlation between these two signals. We hypothesize that the
parts which have the highest degree of cross correlation are the parts in which
that particular raga is established in the song. Also the variation of cross
correlation coefficient in the different parts of the two samples gives a
measure of the modulation that is executed by the singer. Thus, in nutshell we
try to study scientifically the amount of correlation that exists between the
raga and the same raga being utilized in Film music. This will help in
generating an automated algorithm through which a naive listener will relish
the flavor of a particular raga in a popular film song. The results are
discussed in detail."
"Hindustani classical music is entirely based on the Raga structures. In
Hindustani music, a Gharana or school refers to the adherence of a group of
musicians to a particular musical style of performing a certain raga. The
objective of this work was to find out if any characteristic acoustic cues
exist which discriminates a particular gharana from the other. Another
intriguing fact is if the artists of the same gharana keep their singing style
unchanged over generations or evolution of music takes place like everything
else in nature. In this work, we chose to study the similarities and
differences in singing style of some artists from at least four consecutive
generations representing four different gharanas using robust non-linear
methods. For this, alap parts of a particular raga sung by all the artists were
analyzed with the help of non linear multifractal analysis (MFDFA) technique.
The spectral width obtained from the MFDFA method gives an estimate of the
complexity of the signal. The observations give a cue in the direction to the
scientific recognition of guru-shisya parampara (teacher-student tradition) a
hitherto much-heard philosophical term. Moreover the variation in the
complexity patterns among various gharanas will give a hint of the
characteristic feature of that particular gharana as well as the effect of
globalization in the field of classical music happening through past few
decades."
"Speech recognition in adverse real-world environments is highly affected by
reverberation and nonstationary background noise. A well-known strategy to
reduce such undesired signal components in multi-microphone scenarios is
spatial filtering of the microphone signals. In this article, we demonstrate
that an additional coherence-based postfilter, which is applied to the
beamformer output signal to remove diffuse interference components from the
latter, is an effective means to further improve the recognition accuracy of
modern deep learning speech recognition systems. To this end, the recently
updated 3rd CHiME Speech Separation and Recognition Challenge (CHiME-3)
baseline speech recognition system is extended by a coherence-based postfilter
and the postfilter's impact on the word error rates is investigated for the
noisy environments provided by CHiME-3. To determine the time- and
frequency-dependent postfilter gains, we use a Direction-of-Arrival
(DOA)-dependent and a DOA-independent estimator of the coherent-to-diffuse
power ratio as an approximation of the short-time signal-to-noise ratio. Our
experiments show that incorporating coherence-based postfiltering into the
CHiME-3 baseline speech recognition system leads to a significant reduction of
the word error rate scores for the noisy and reverberant environments provided
as part of CHiME-3."
"This paper addresses the high dimensionality problem in blind source
separation (BSS), where the number of sources is greater than two. Two pairwise
iterative schemes are proposed to tackle this high dimensionality problem. The
two pairwise schemes realize nonparametric independent component analysis (ICA)
algorithms based on a new high-performance Convex CauchySchwarz Divergence
(CCSDIV). These two schemes enable fast and efficient demixing of sources in
real-world high dimensional source applications. Finally, the performance
superiority of the proposed schemes is demonstrated in metric-comparison with
FastICA, RobustICA, convex ICA (CICA), and other leading existing algorithms."
"Large music content libraries often comprise multiple versions of a piece of
music. To establish a link between different versions, automatic music
alignment methods map each position in one version to a corresponding position
in another version. Due to the leeway in interpreting a piece, any two versions
can differ significantly, for example, in terms of local tempo, articulation,
or playing style. For a given pair of versions, these differences can be
significant such that even state-of-the-art methods fail to identify a correct
alignment. In this paper, we present a novel method that increases the
robustness for difficult to align cases. Instead of aligning only pairs of
versions as done in previous methods, our method aligns multiple versions in a
joint manner. This way, the alignment can be computed by comparing each version
not only with one but with several versions, which stabilizes the comparison
and leads to an increase in alignment robustness. Using recordings from the
Mazurka Project, the alignment error for our proposed method was 14% lower on
average compared to a state-of-the-art method, with significantly less outliers
(standard deviation 53% lower)."
"In sensor array beamforming methods, a class of algorithms commonly used to
estimate the position of a radiating source, the diagonal loading of the
beamformer covariance matrix is generally used to improve computational
accuracy and localization robustness. This paper proposes a diagonal unloading
(DU) method which extends the conventional response power beamforming method by
imposing an additional constraint to the covariance matrix of the array output
vector. The regularization is obtained by subtracting a given amount of white
noise from the main diagonal of the covariance matrix. Specifically, the DU
beamformer aims at subtracting the signal subspace from the noisy signal space
and it is computed by constraining the regularized covariance matrix to be
negative definite. It is hence a data-dependent covariance matrix conditioning
method. We show how to calculate precisely the unloading parameter, and we
present an eigenvalue analysis for comparing the proposed DU beamforming, the
minimum variance distortionless response (MVDR) filter and the multiple signal
classification (MUSIC) method. Theoretical analysis and experiments with
acoustic sources demonstrate that the DU beamformer localization performance is
comparable to that of MVDR and MUSIC. Since the DU beamformer computational
cost is comparable to that of a conventional beamformer, the proposed method
can be attractive in array processing due to its simplicity, effectiveness and
computational efficiency."
"In this paper we consider the problem of speech enhancement in real-world
like conditions where multiple noises can simultaneously corrupt speech. Most
of the current literature on speech enhancement focus primarily on presence of
single noise in corrupted speech which is far from real-world environments.
Specifically, we deal with improving speech quality in office environment where
multiple stationary as well as non-stationary noises can be simultaneously
present in speech. We propose several strategies based on Deep Neural Networks
(DNN) for speech enhancement in these scenarios. We also investigate a DNN
training strategy based on psychoacoustic models from speech coding for
enhancement of noisy speech"
"In this paper, we propose a speaker-verification system based on maximum
likelihood linear regression (MLLR) super-vectors, for which speakers are
characterized by m-vectors. These vectors are obtained by a uniform
segmentation of the speaker MLLR super-vector using an overlapped sliding
window. We consider three approaches for MLLR transformation, based on the
conventional $1$-best automatic transcription, on the lattice word
transcription, or on a simple global universal background model (UBM). Session
variability compensation is performed in a post-processing module with
probabilistic linear discriminant analysis (PLDA) or the eigen factor radial
(EFR). Alternatively, we propose a cascade post-processing for the MLLR
super-vector based speaker-verification system.
  In this case, the m-vectors or MLLR super-vectors are first projected onto a
lower-dimensional vector space generated by linear discriminant analysis (LDA).
Next, PLDA session variability compensation and scoring is applied to the
reduced-dimensional vectors. This approach combines the advantages of both
techniques and makes the estimation of PLDA parameters easier. Experimental
results on telephone conversations of the NIST 2008 and 2010 speaker
recognition evaluation (SRE) indicate that the proposed m-vector system
performs significantly better than the conventional system based on the full
MLLR super-vectors. Cascade post-processing further reduces the error rate in
all cases. Finally, we present the results of fusion with a standard i-vector
system in the feature, as well as in the score domain, demonstrating that the
m-vector system is both competitive and complementary with it."
"Musical performance combines a wide range of pitches, nuances, and expressive
techniques. Audio-based classification of musical instruments thus requires to
build signal representations that are invariant to such transformations. This
article investigates the construction of learned convolutional architectures
for instrument recognition, given a limited amount of annotated training data.
In this context, we benchmark three different weight sharing strategies for
deep convolutional networks in the time-frequency domain: temporal kernels;
time-frequency kernels; and a linear combination of time-frequency kernels
which are one octave apart, akin to a Shepard pitch spiral. We provide an
acoustical interpretation of these strategies within the source-filter
framework of quasi-harmonic sounds with a fixed spectral envelope, which are
archetypal of musical notes. The best classification accuracy is obtained by
hybridizing all three convolutional layers into a single deep learning
architecture."
"In this paper, we present madmom, an open-source audio processing and music
information retrieval (MIR) library written in Python. madmom features a
concise, NumPy-compatible, object oriented design with simple calling
conventions and sensible default values for all parameters, which facilitates
fast prototyping of MIR applications. Prototypes can be seamlessly converted
into callable processing pipelines through madmom's concept of Processors,
callable objects that run transparently on multiple cores. Processors can also
be serialised, saved, and re-run to allow results to be easily reproduced
anywhere. Apart from low-level audio processing, madmom puts emphasis on
musically meaningful high-level features. Many of these incorporate machine
learning techniques and madmom provides a module that implements some in MIR
commonly used methods such as hidden Markov models and neural networks.
Additionally, madmom comes with several state-of-the-art MIR algorithms for
onset detection, beat, downbeat and meter tracking, tempo estimation, and piano
transcription. These can easily be incorporated into bigger MIR systems or run
as stand-alone programs."
"Nonnegative Matrix Factorization (NMF) is a powerful tool for decomposing
mixtures of audio signals in the Time-Frequency (TF) domain. In the source
separation framework, the phase recovery for each extracted component is
necessary for synthesizing time-domain signals. The Complex NMF (CNMF) model
aims to jointly estimate the spectrogram and the phase of the sources, but
requires to constrain the phase in order to produce satisfactory sounding
results. We propose to incorporate phase constraints based on signal models
within the CNMF framework: a \textit{phase unwrapping} constraint that enforces
a form of temporal coherence, and a constraint based on the \textit{repetition}
of audio events, which models the phases of the sources within onset frames. We
also provide an algorithm for estimating the model parameters. The experimental
results highlight the interest of including such constraints in the CNMF
framework for separating overlapping components in complex audio mixtures."
"This paper introduces a novel technique for reconstructing the phase of
modified spectrograms of audio signals. From the analysis of mixtures of
sinusoids we obtain relationships between phases of successive time frames in
the Time-Frequency (TF) domain. To obtain similar relationships over
frequencies, in particular within onset frames, we study an impulse model.
Instantaneous frequencies and attack times are estimated locally to encompass
the class of non-stationary signals such as vibratos. These techniques ensure
both the vertical coherence of partials (over frequencies) and the horizontal
coherence (over time). The method is tested on a variety of data and
demonstrates better performance than traditional consistency-based approaches.
We also introduce an audio restoration framework and observe that our technique
outperforms traditional methods."
"Phase recovery of modified spectrograms is a major issue in audio signal
processing applications, such as source separation. This paper introduces a
novel technique for estimating the phases of components in complex mixtures
within onset frames in the Time-Frequency (TF) domain. We propose to exploit
the phase repetitions from one onset frame to another. We introduce a reference
phase which characterizes a component independently of its activation times.
The onset phases of a component are then modeled as the sum of this reference
and an offset which is linearly dependent on the frequency. We derive a complex
mixture model within onset frames and we provide two algorithms for the
estimation of the model phase parameters. The model is estimated on
experimental data and this technique is integrated into an audio source
separation framework. The results demonstrate that this model is a promising
tool for exploiting phase repetitions, and point out its potential for
separating overlapping components in complex mixtures."
"Nonnegative Matrix Factorization (NMF) is a powerful tool for decomposing
mixtures of audio signals in the Time-Frequency (TF) domain. In applications
such as source separation, the phase recovery for each extracted component is a
major issue since it often leads to audible artifacts. In this paper, we
present a methodology for evaluating various NMF-based source separation
techniques involving phase reconstruction. For each model considered, a
comparison between two approaches (blind separation without prior information
and oracle separation with supervised model learning) is performed, in order to
inquire about the room for improvement for the estimation methods. Experimental
results show that the High Resolution NMF (HRNMF) model is particularly
promising, because it is able to take phases and correlations over time into
account with a great expressive power."
"This paper introduces a general and flexible framework for F0 and
aperiodicity (additive non periodic component) analysis, specifically intended
for high-quality speech synthesis and modification applications. The proposed
framework consists of three subsystems: instantaneous frequency estimator and
initial aperiodicity detector, F0 trajectory tracker, and F0 refinement and
aperiodicity extractor. A preliminary implementation of the proposed framework
substantially outperformed (by a factor of 10 in terms of RMS F0 estimation
error) existing F0 extractors in tracking ability of temporally varying F0
trajectories. The front end aperiodicity detector consists of a complex-valued
wavelet analysis filter with a highly selective temporal and spectral envelope.
This front end aperiodicity detector uses a new measure that quantifies the
deviation from periodicity. The measure is less sensitive to slow FM and AM and
closely correlates with the signal to noise ratio."
"The urban sound environment of New York City (NYC) can be, amongst other
things: loud, intrusive, exciting and dynamic. As indicated by the large
majority of noise complaints registered with the NYC 311 information/complaints
line, the urban sound environment has a profound effect on the quality of life
of the city's inhabitants. To monitor and ultimately understand these sonic
environments, a process of long-term acoustic measurement and analysis is
required. The traditional method of environmental acoustic monitoring utilizes
short term measurement periods using expensive equipment, setup and operated by
experienced and costly personnel. In this paper a different approach is
proposed to this application which implements a smart, low-cost, static,
acoustic sensing device based around consumer hardware. These devices can be
deployed in numerous and varied urban locations for long periods of time,
allowing for the collection of longitudinal urban acoustic data. The varied
environmental conditions of urban settings make for a challenge in gathering
calibrated sound pressure level data for prospective stakeholders. This paper
details the sensors' design, development and potential future applications,
with a focus on the calibration of the devices' Microelectromechanical systems
(MEMS) microphone in order to generate reliable decibel levels at the
type/class 2 level."
"We present Vibrato Nonnegative Tensor Factorization, an algorithm for
single-channel unsupervised audio source separation with an application to
separating instrumental or vocal sources with nonstationary pitch from music
recordings. Our approach extends Nonnegative Matrix Factorization for audio
modeling by including local estimates of frequency modulation as cues in the
separation. This permits the modeling and unsupervised separation of vibrato or
glissando musical sources, which is not possible with the basic matrix
factorization formulation.
  The algorithm factorizes a sparse nonnegative tensor comprising the audio
spectrogram and local frequency-slope-to-frequency ratios, which are estimated
at each time-frequency bin using the Distributed Derivative Method. The use of
local frequency modulations as separation cues is motivated by the principle of
common fate partial grouping from Auditory Scene Analysis, which hypothesizes
that each latent source in a mixture is characterized perceptually by coherent
frequency and amplitude modulations shared by its component partials. We derive
multiplicative factor updates by Minorization-Maximization, which guarantees
convergence to a local optimum by iteration. We then compare our method to the
baseline on two separation tasks: one considers synthetic vibrato notes, while
the other considers vibrato string instrument recordings."
"In this paper, we consider the problem of probabilistically modelling
symbolic music data. We introduce a representation which reduces polyphonic
music to a univariate categorical sequence. In this way, we are able to apply
state of the art natural language processing techniques, namely the long
short-term memory sequence model. The representation we employ permits
arbitrary rhythmic structure, which we assume to be given. We show that our
model is effective on four out of four piano roll based benchmark datasets. We
further improve our model by augmenting our training data set with
transpositions of the original pieces through all musical keys, thereby
convincingly advancing the state of the art on these benchmark problems. We
also fit models to music which is unconstrained in its rhythmic structure,
discuss the properties of this model, and provide musical samples which are
more sophisticated than previously possible with this class of recurrent neural
network sequence models. We also provide our newly preprocessed data set of non
piano-roll music data."
"This document provides the results of the tests of acoustic parameter
estimation algorithms on the Acoustic Characterization of Environments (ACE)
Challenge Evaluation dataset which were subsequently submitted and written up
into papers for the Proceedings of the ACE Challenge. This document is
supporting material for a forthcoming journal paper on the ACE Challenge which
will provide further analysis of the results."
"This paper presents a method to reconstruct the 3D structure of generic
convex rooms from sound signals. Differently from most of the previous
approaches, the method is fully uncalibrated in the sense that no knowledge
about the microphones and sources position is needed. Moreover, we demonstrate
that it is possible to bypass the well known echo labeling problem, allowing to
reconstruct the room shape in a reasonable computation time without the need of
additional hypotheses on the echoes order of arrival. Finally, the method is
intrinsically robust to outliers and missing data in the echoes detection,
allowing to work also in low SNR conditions. The proposed pipeline formalises
the problem in different steps such as time of arrival estimation, microphones
and sources localization and walls estimation. After providing a solution to
these different problems we present a global optimization approach that links
together all the problems in a single optimization function. The accuracy and
robustness of the method is assessed on a wide set of simulated setups and in a
challenging real scenario. Moreover we make freely available for a challenging
dataset for 3D room reconstruction with accurate ground truth in a real
scenario."
"Peer-led team learning (PLTL) is a model for teaching STEM courses where
small student groups meet periodically to collaboratively discuss coursework.
Automatic analysis of PLTL sessions would help education researchers to get
insight into how learning outcomes are impacted by individual participation,
group behavior, team dynamics, etc.. Towards this, speech and language
technology can help, and speaker diarization technology will lay the foundation
for analysis. In this study, a new corpus is established called CRSS-PLTL, that
contains speech data from 5 PLTL teams over a semester (10 sessions per team
with 5-to-8 participants in each team). In CRSS-PLTL, every participant wears a
LENA device (portable audio recorder) that provides multiple audio recordings
of the event. Our proposed solution is unsupervised and contains a new online
speaker change detection algorithm, termed G 3 algorithm in conjunction with
Hausdorff-distance based clustering to provide improved detection accuracy.
Additionally, we also exploit cross channel information to refine our
diarization hypothesis. The proposed system provides good improvements in
diarization error rate (DER) over the baseline LIUM system. We also present
higher level analysis such as the number of conversational turns taken in a
session, and speaking-time duration (participation) for each speaker."
"This study describes a binaural machine hearing system that is capable of
performing auditory stream segregation in scenarios where multiple sound
sources are present. The process of stream segregation refers to the capability
of human listeners to group acoustic signals into sets of distinct auditory
streams, corresponding to individual sound sources. The proposed computational
framework mimics this ability via a probabilistic clustering scheme for joint
localization and segregation. This scheme is based on mixtures of von Mises
distributions to model the angular positions of the sound sources surrounding
the listener. The distribution parameters are estimated using block-wise
processing of auditory cues extracted from binaural signals. Additionally, the
proposed system can conduct rotational head movements to improve localization
and stream segregation performance. Evaluation of the system is conducted in
scenarios containing multiple simultaneously active speech and non-speech
sounds placed at different positions relative to the listener."
"Characterizing sound field diffuseness has many practical applications, from
room acoustics analysis to speech enhancement and sound field reproduction. In
this paper we investigate how spherical microphone arrays (SMAs) can be used to
characterize diffuseness. Due to their specific geometry, SMAs are particularly
well suited for analyzing the spatial properties of sound fields. In
particular, the signals recorded by an SMA can be analyzed in the spherical
harmonic (SH) domain, which has special and desirable mathematical properties
when it comes to analyzing diffuse sound fields. We present a new measure of
diffuseness, the COMEDIE diffuseness estimate, which is based on the analysis
of the SH signal covariance matrix. This algorithm is suited for the estimation
of diffuseness arising either from the presence of multiple sources distributed
around the SMA or from the presence of a diffuse noise background. As well, we
introduce the concept of a diffuseness profile, which consists in measuring the
diffuseness for several SH orders simultaneously. Experimental results indicate
that diffuseness profiles better describe the properties of the sound field
than a single diffuseness measurement."
"In recent years, neural network approaches have shown superior performance to
conventional hand-made features in numerous application areas. In particular,
convolutional neural networks (ConvNets) exploit spatially local correlations
across input data to improve the performance of audio processing tasks, such as
speech recognition, musical chord recognition, and onset detection. Here we
apply ConvNet to acoustic scene classification, and show that the error rate
can be further decreased by using delta features in the frequency domain. We
propose a multiple-width frequency-delta (MWFD) data augmentation method that
uses static mel-spectrogram and frequency-delta features as individual input
examples. In addition, we describe a ConvNet output aggregation method designed
for MWFD augmentation, folded mean aggregation, which combines output
probabilities of static and MWFD features from the same analysis window using
multiplication first, rather than taking an average of all output
probabilities. We describe calculation results using the DCASE 2016 challenge
dataset, which shows that ConvNet outperforms both of the baseline system with
hand-crafted features and a deep neural network approach by around 7%. The
performance was further improved (by 5.7%) using the MWFD augmentation together
with folded mean aggregation. The system exhibited a classification accuracy of
0.831 when classifying 15 acoustic scenes."
"In this work, we propose a robust Head-Related Transfer Function (HRTF)-based
polynomial beamformer design which accounts for the influence of a humanoid
robot's head on the sound field. In addition, it allows for a flexible steering
of our previously proposed robust HRTF-based beamformer design. We evaluate the
HRTF-based polynomial beamformer design and compare it to the original
HRTF-based beamformer design by means of signal-independent measures as well as
word error rates of an off-the-shelf speech recognition system. Our results
confirm the effectiveness of the polynomial beamformer design, which makes it a
promising approach to robust beamforming for robot audition."
"In this paper we present our work on Task 1 Acoustic Scene Classi- fication
and Task 3 Sound Event Detection in Real Life Recordings. Among our experiments
we have low-level and high-level features, classifier optimization and other
heuristics specific to each task. Our performance for both tasks improved the
baseline from DCASE: for Task 1 we achieved an overall accuracy of 78.9%
compared to the baseline of 72.6% and for Task 3 we achieved a Segment-Based
Error Rate of 0.76 compared to the baseline of 0.91."
"This paper outlines preliminary steps towards the development of an audio-
based room-occupancy analysis model. Our approach borrows from speech
recognition tradition and is based on Gaussian Mixtures and Hidden Markov
Models. We analyze possible challenges encountered in the development of such a
model, and offer several solutions including feature design and prediction
strategies. We provide results obtained from experiments with audio data from a
retail store in Palo Alto, California. Model assessment is done via
leave-two-out Bootstrap and model convergence achieves good accuracy, thus
representing a contribution to multimodal people counting algorithms."
"This work presents a new toolkit for describing the acoustic properties of
the ocean environment before, during and after a sound event caused by an
underwater seismic air-gun. The toolkit uses existing sound measures, but
uniquely applies these to capture the early time period (actual pulse) and late
time period (reverberation and multiple arrivals). In total, 183 features are
produced for each air-gun sound. This toolkit was utilized on data retrieved
from a field deployment encompassing five marine autonomous recording units
during a 46-day seismic air-gun survey in Baffin Bay, Greenland. Using this
toolkit, a total of 147 million data points were identified from the Greenland
deployment recordings. The feasibility of extracting a large number of features
was then evaluated using two separate methods: a serial computer and a high
performance system. Results indicate that data extraction performance took an
estimated 216 hours for the serial system, and 18 hours for the high
performance computer. This paper provides an analytical description of the new
toolkit along with details for using it to identify relevant data."
"Source separation, which consists in decomposing data into meaningful
structured components, is an active research topic in many areas, such as music
and image signal processing, applied physics and text mining. In this paper, we
introduce the Positive $\alpha$-stable (P$\alpha$S) distributions to model the
latent sources, which are a subclass of the stable distributions family. They
notably permit us to model random variables that are both nonnegative and
impulsive. Considering the L\'evy distribution, the only P$\alpha$S
distribution whose density is tractable, we propose a mixture model called
L\'evy Nonnegative Matrix Factorization (L\'evy NMF). This model accounts for
low-rank structures in nonnegative data that possibly has high variability or
is corrupted by very adverse noise. The model parameters are estimated in a
maximum-likelihood sense. We also derive an estimator of the sources given the
parameters, which extends the validity of the generalized Wiener filtering to
the P$\alpha$S case. Experiments on synthetic data show that L\'evy NMF
compares favorably with state-of-the art techniques in terms of robustness to
impulsive noise. The analysis of two types of realistic signals is also
considered: musical spectrograms and fluorescence spectra of chemical species.
The results highlight the potential of the L\'evy NMF model for decomposing
nonnegative data."
"For audio source separation applications, it is common to estimate the
magnitude of the Time-Frequency (TF) representation of each source. In order to
recover a time-domain signal from a spectrogram for instance, it then becomes
necessary to recover the phase of the corresponding complex-valued Short-Time
Fourier Transform (STFT). Most authors in this field choose a Wiener-like
filtering approach which boils down to using the phase of the original mixture.
In this paper, a different standpoint is adopted. Many music events are
partially composed of slowly varying sinusoids and the STFT phase increment of
those frequency components takes a specific form. This allows phase recovery by
an unwrapping technique once a short-term frequency estimate has been obtained.
Herein, a whole iterative source separation procedure is proposed which builds
upon these results. It is tested on a variety of data, both synthetic and
realistic, and also with different source separation scenarios, oracle or non
oracle. In terms of SIR, SAR and SDR, the method achieves better performance
than consistency-based approaches. To complete the experimental analysis, sound
examples are provided which allow the reader to assess the interest of the
method regarding the improvement of sound quality."
"Many biological monitoring projects rely on acoustic detection of birds.
Despite increasingly large datasets, this detection is often manual or
semi-automatic, requiring manual tuning/postprocessing. We review the state of
the art in automatic bird sound detection, and identify a widespread need for
tuning-free and species-agnostic approaches. We introduce new datasets and an
IEEE research challenge to address this need, to make possible the development
of fully automatic algorithms for bird sound detection."
"Phased microphone arrays are used widely in the applications for acoustic
source localization. Deconvolution approaches such as DAMAS successfully
overcome the spatial resolution limit of the conventional delay-and-sum (DAS)
beamforming method. However deconvolution approaches require high computational
effort compared to conventional DAS beamforming method. This paper presents a
novel method that serves to improve the efficiency of DAMAS via wavelet
compression computational grid rather than via optimizing DAMAS algorithm. In
this method, the efficiency of DAMAS increases with compression ratio. This
method can thus save lots of run time in industrial applications for sound
source localization, particularly when sound sources are just located in a
small extent compared with scanning plane and a band of angular frequency needs
to be calculated. In addition, this method largely retains the spatial
resolution of DAMAS on original computational grid, although with a minor
deficiency that the occurrence probability of aliasing increasing slightly for
complicated sound source."
"Knowledge of the diffuse-field coherence between array sensors is a basic
assumption for a wide range of array processing applications. Explicit
relations previously existed only for omnidirectional and first-order
directional sensors, or a restricted arrangement of differential patterns. We
present a closed-form formulation of the theoretical coherence function between
arbitrary directionally band-limited sensors for the general cases that a) the
responses of the individual sensors are known or estimated, and the coherence
needs to be known for an arbitrary arrangement, and b) that no information on
the sensor directionality or on array geometry exists, but calibration
measurements around the array are available."
"A non-iterative method for the construction of the Short-Time Fourier
Transform (STFT) phase from the magnitude is presented. The method is based on
the direct relationship between the partial derivatives of the phase and the
logarithm of the magnitude of the un-sampled STFT with respect to the Gaussian
window. Although the theory holds in the continuous setting only, the
experiments show that the algorithm performs well even in the discretized
setting (Discrete Gabor transform) with low redundancy using the sampled
Gaussian window, the truncated Gaussian window and even other compactly
supported windows like the Hann window.
  Due to the non-iterative nature, the algorithm is very fast and it is
suitable for long audio signals. Moreover, solutions of iterative phase
reconstruction algorithms can be improved considerably by initializing them
with the phase estimate provided by the present algorithm.
  We present an extensive comparison with the state-of-the-art algorithms in a
reproducible manner."
"The sources separated by most single channel audio source separation
techniques are usually distorted and each separated source contains residual
signals from the other sources. To tackle this problem, we propose to enhance
the separated sources to decrease the distortion and interference between the
separated sources using deep neural networks (DNNs). Two different DNNs are
used in this work. The first DNN is used to separate the sources from the mixed
signal. The second DNN is used to enhance the separated signals. To consider
the interactions between the separated sources, we propose to use a single DNN
to enhance all the separated sources together. To reduce the residual signals
of one source from the other separated sources (interference), we train the DNN
for enhancement discriminatively to maximize the dissimilarity between the
predicted sources. The experimental results show that using discriminative
enhancement decreases the distortion and interference between the separated
sources."
"We present a neural network that can act as an equivalent to a Non-Negative
Matrix Factorization (NMF), and further show how it can be used to perform
supervised source separation. Due to the extensibility of this approach we show
how we can achieve better source separation performance as compared to
NMF-based methods, and propose a variety of derivative architectures that can
be used for further improvements."
"This work presents a method for estimation of the acoustic intensity, the
energy density and the associated sound field diffuseness around the origin,
when the sound field is weighted with a spatial filter. The method permits
energetic DOA estimation and sound field characterization focused in a specific
angular region determined by the beam pattern of the spatial filter. The
formulation of the estimators is presented and their behavior is analyzed for
the fundamental cases useful in parametric sound field models of a single plane
wave, a uniform diffuse field and a mixture of the two."
"In live and studio recordings unexpected sound events often lead to
interferences in the signal. For non-stationary interferences, sound source
separation techniques can be used to reduce the interference level in the
recording. In this context, we present a novel approach combining the strengths
of two algorithmic families: NMF and KAM. The recent KAM approach applies
robust statistics on frames selected by a source-specific kernel to perform
source separation. Based on semi-supervised NMF, we extend this approach in two
ways. First, we locate the interference in the recording based on detected NMF
activity. Second, we improve the kernel-based frame selection by incorporating
an NMF-based estimate of the clean music signal. Further, we introduce a
temporal context in the kernel, taking some musical structure into account. Our
experiments show improved separation quality for our proposed method over a
state-of-the-art approach for interference reduction."
"Peer-Led Team Learning (PLTL) is a structured learning model where a team
leader is appointed to facilitate collaborative problem solving among students
for Science, Technology, Engineering and Mathematics (STEM) courses. This paper
presents an informed HMM-based speaker diarization system. The minimum duration
of short conversationalturns and number of participating students were fed as
side information to the HMM system. A modified form of Bayesian Information
Criterion (BIC) was used for iterative merging and re-segmentation. Finally, we
used the diarization output to compute a novel dominance score based on
unsupervised acoustic analysis."
"We propose an online estimated dictionary based single channel speech
enhancement algorithm, which focuses on low rank and sparse matrix
decomposition. In this proposed algorithm, a noisy speech spectral matrix is
considered as the summation of low rank background noise components and an
activation of the online speech dictionary, on which both low rank and sparsity
constraints are imposed. This decomposition takes the advantage of local
estimated dictionary high expressiveness on speech components. The local
dictionary can be obtained through estimating the speech presence probability
by applying Expectation Maximal algorithm, in which a generalized Gamma prior
for speech magnitude spectrum is used. The evaluation results show that the
proposed algorithm achieves significant improvements when compared to four
other speech enhancement algorithms."
"The sampling of sound fields involves the measurement of spatially dependent
room impulse responses, where the Nyquist-Shannon sampling theorem applies in
both the temporal and spatial domain. Therefore, sampling inside a volume of
interest requires a huge number of sampling points in space, which comes along
with further difficulties such as exact microphone positioning and calibration
of multiple microphones. In this paper, we present a method for measuring sound
fields using moving microphones whose trajectories are known to the algorithm.
At that, the number of microphones is customizable by trading measurement
effort against sampling time. Through spatial interpolation of the dynamic
measurements, a system of linear equations is set up which allows for the
reconstruction of the entire sound field inside the volume of interest."
"In this study, we propose a modulation decoupling based single channel speech
enhancement subspace framework, in which the spectrogram of noisy speech is
decoupled as the product of a spectral envelop subspace and a spectral details
subspace. This decoupling approach provides a method to specifically work on
elimination of those noises that greatly affect the intelligibility. Two
supervised low-rank and sparse decomposition schemes are developed in the
spectral envelop subspace to obtain a robust recovery of speech components. A
Bayesian formulation of non-negative factorization is used to learn the speech
dictionary from the spectral envelop subspace of clean speech samples. In the
spectral details subspace, a standard robust principal component analysis is
implemented to extract the speech components. The validation results show that
compared with four speech enhancement algorithms, including MMSE-SPP, NMF-RPCA,
RPCA, and LARC, the proposed MS based algorithms achieve satisfactory
performance on improving perceptual quality, and especially speech
intelligibility."
"This paper introduces a new framework for supervised sound source
localization referred to as virtually-supervised learning. An acoustic shoe-box
room simulator is used to generate a large number of binaural single-source
audio scenes. These scenes are used to build a dataset of spatial binaural
features annotated with acoustic properties such as the 3D source position and
the walls' absorption coefficients. A probabilistic high- to low-dimensional
regression framework is used to learn a mapping from these features to the
acoustic properties. Results indicate that this mapping successfully estimates
the azimuth and elevation of new sources, but also their range and even the
walls' absorption coefficients solely based on binaural signals. Results also
reveal that incorporating random-diffusion effects in the data significantly
improves the estimation of all parameters."
"A judicious combination of dictionary learning methods, block sparsity and
source recovery algorithm are used in a hierarchical manner to identify the
noises and the speakers from a noisy conversation between two people.
Conversations are simulated using speech from two speakers, each with a
different background noise, with varied SNR values, down to -10 dB. Ten each of
randomly chosen male and female speakers from the TIMIT database and all the
noise sources from the NOISEX database are used for the simulations. For
speaker identification, the relative value of weights recovered is used to
select an appropriately small subset of the test data, assumed to contain
speech. This novel choice of using varied amounts of test data results in an
improvement in the speaker recognition rate of around 15% at SNR of 0 dB.
Speech and noise are separated using dictionaries of the estimated speaker and
noise, and an improvement of signal to distortion ratios of up to 10% is
achieved at SNR of 0 dB. K-medoid and cosine similarity based dictionary
learning methods lead to better recognition of the background noise and the
speaker. Experiments are also conducted on cases, where either the background
noise or the speaker is outside the set of trained dictionaries. In such cases,
adaptive dictionary learning leads to performance comparable to the other case
of complete dictionaries."
"In this paper, a two-stage dual tree complex wavelet packet transform
(DTCWPT) based speech enhancement algorithm has been proposed, in which a
speech presence probability (SPP) estimator and a generalized minimum mean
squared error (MMSE) estimator are developed. To overcome the drawback of
signal distortions caused by down sampling of WPT, a two-stage analytic
decomposition concatenating undecimated WPT (UWPT) and decimated WPT is
employed. An SPP estimator in the DTCWPT domain is derived based on a
generalized Gamma distribution of speech, and Gaussian noise assumption. The
validation results show that the proposed algorithm can obtain enhanced
perceptual evaluation of speech quality (PESQ), and segmental signal-to-noise
ratio (SegSNR) at low SNR nonstationary noise, compared with other four
state-of-the-art speech enhancement algorithms, including optimally modified
LSA (OM-LSA), soft masking using a posteriori SNR uncertainty (SMPO), a
posteriori SPP based MMSE estimation (MMSE-SPP), and adaptive Bayesian wavelet
thresholding (BWT)."
"Audio tagging aims to assign one or several tags to an audio clip. Most of
the datasets are weakly labelled, which means only the tags of the clip are
known, without knowing the occurrence time of the tags. The labeling of an
audio clip is often based on the audio events in the clip and no event level
label is provided to the user. Previous works have used the bag of frames model
assume the tags occur all the time, which is not the case in practice. We
propose a joint detection-classification (JDC) model to detect and classify the
audio clip simultaneously. The JDC model has the ability to attend to
informative and ignore uninformative sounds. Then only informative regions are
used for classification. Experimental results on the ""CHiME Home"" dataset show
that the JDC model reduces the equal error rate (EER) from 19.0% to 16.9%. More
interestingly, the audio event detector is trained successfully without needing
the event level label."
"In this paper, robust detection, tracking and geometry estimation methods are
developed and combined into a system for estimating time-difference estimates,
microphone localization and sound source movement. No assumptions on the 3D
locations of the microphones and sound sources are made. The system is capable
of tracking continuously moving sound sources in an reverberant environment.
The multi-path components are explicitly tracked and used in the geometry
estimation parts. The system is based on matching between pairs of channels
using GCC-PHAT. Instead of taking a single maximum at each time instant from
each such pair, we select the four strongest local maxima. This produce a set
of hypothesis to work with in the subsequent steps, where consistency
constraints between the channels and time-continuity constraints are exploited.
In the paper it demonstrated how such detections can be used to estimate
microphone positions, sound source movement and room geometry. The methods are
tested and verified using real data from several reverberant environments. The
evaluation demonstrated accuracy in the order of few millimeters."
"This paper explores how the in- and out-domain probabilistic linear
discriminant analysis (PLDA) speaker verification behave when enrolment and
verification lengths are reduced. Experiment studies have found that when
full-length utterance is used for evaluation, in-domain PLDA approach shows
more than 28% improvement in EER and DCF values over out-domain PLDA approach
and when short utterances are used for evaluation, the performance gain of
in-domain speaker verification reduces at an increasing rate. Novel modified
inter dataset variability (IDV) compensation is used to compensate the mismatch
between in- and out-domain data and IDV-compensated out-domain PLDA shows
respectively 26% and 14% improvement over out-domain PLDA speaker verification
when SWB and NIST data are respectively used for S normalization. When the
evaluation utterance length is reduced, the performance gain by IDV also
reduces as short utterance evaluation data i-vectors have more variations due
to phonetic variations when compared to the dataset mismatch between in- and
out-domain data."
"This paper investigates the effects of limited speech data in the context of
speaker verification using deep neural network (DNN) approach. Being able to
reduce the length of required speech data is important to the development of
speaker verification system in real world applications. The experimental
studies have found that DNN-senone-based Gaussian probabilistic linear
discriminant analysis (GPLDA) system respectively achieves above 50% and 18%
improvements in EER values over GMM-UBM GPLDA system on NIST 2010
coreext-coreext and truncated 15sec-15sec evaluation conditions. Further when
GPLDA model is trained on short-length utterances (30sec) rather than
full-length utterances (2min), DNN-senone GPLDA system achieves above 7%
improvement in EER values on truncated 15sec-15sec condition. This is because
short length development i-vectors have speaker, session and phonetic variation
and GPLDA is able to robustly model those variations. For several real world
applications, longer utterances (2min) can be used for enrollment and shorter
utterances (15sec) are required for verification, and in those conditions,
DNN-senone GPLDA system achieves above 26% improvement in EER values over
GMM-UBM GPLDA systems."
"Objective of this work is to integrate high performance computing (HPC)
technologies and bioacoustics data-mining capabilities by offering a
MATLAB-based toolbox called Raven-X. Raven-X will provide a
hardware-independent solution, for processing large acoustic datasets - the
toolkit will be available to the community at no cost. This goal will be
achieved by leveraging prior work done which successfully deployed MATLAB based
HPC tools within Cornell University's Bioacoustics Research Program (BRP).
These tools enabled commonly available multi-core computers to process data at
accelerated rates to detect and classify whale sounds in large multi-channel
sound archives. Through this collaboration, we will expand on this effort which
was featured through Mathworks research and industry forums incorporate new
cutting-edge detectors and classifiers, and disseminate Raven-X to the broader
bioacoustics community."
"A novel non-negative matrix factorization (NMF) based subband decomposition
in frequency spatial domain for acoustic source localization using a microphone
array is introduced. The proposed method decomposes source and noise subband
and emphasises source dominant frequency bins for more accurate source
representation. By employing NMF, delay basis vectors and their subband
information in frequency spatial domain for each frame is extracted. The
proposed algorithm is evaluated in both simulated noise and real noise with a
speech corpus database. Experimental results clearly indicate that the
algorithm performs more accurately than other conventional algorithms under
both reverberant and noisy acoustic environments."
"The problem of source localization with ad hoc microphone networks in noisy
and reverberant enclosures, given a training set of prerecorded measurements,
is addressed in this paper. The training set is assumed to consist of a limited
number of labelled measurements, attached with corresponding positions, and a
larger amount of unlabelled measurements from unknown locations. However,
microphone calibration is not required. We use a Bayesian inference approach
for estimating a function that maps measurement-based feature vectors to the
corresponding positions. The central issue is how to combine the information
provided by the different microphones in a unified statistical framework. To
address this challenge, we model this function using a Gaussian process with a
covariance function that encapsulates both the connections between pairs of
microphones and the relations among the samples in the training set. The
parameters of the process are estimated by optimizing a maximum likelihood (ML)
criterion. In addition, a recursive adaptation mechanism is derived where the
new streaming measurements are used to update the model. Performance is
demonstrated for 2-D localization of both simulated data and real-life
recordings in a variety of reverberation and noise levels."
"For more than the past twenty years, Csound has been one of the leaders in
the world of the computer music research, implementing innovative synthesis
methods and making them available beyond the academic environments from which
they often arise, and into the hands of musicians and sound designers
throughout the world. In its present state, Csound offers an efficient
environment for sound experimentation, allowing the user to work with almost
any known sound synthesis or signal processing method through its vast
collection of ready-made opcodes. But despite all this potential, the shared
resource of Csound instruments still lacks quality reproductions of well-known
synthesizers; even with its ability to generate commercial standard user
interfaces and with the possibility to compile Csound instruments in such as
fashion so that they can be used with no knowledge of Csound code. To fill this
gap, the authors have implemented two commercial-style synthesizers as VST
plug-ins using the Csound front-end ""Cabbage"". This paper describes their
architecture and some of the Csound specific challenges involved in the
development of fully featured synthesizers."
"This paper analyses the short utterance probabilistic linear discriminant
analysis (PLDA) speaker verification with utterance partitioning and short
utterance variance (SUV) modelling approaches. Experimental studies have found
that instead of using single long-utterance as enrolment data, if long enrolled
utterance is partitioned into multiple short utterances and average of short
utterance i-vectors is used as enrolled data, that improves the Gaussian PLDA
(GPLDA) speaker verification. This is because short utterance i-vectors have
speaker, session and utterance variations, and utterance-partitioning approach
compensates the utterance variation. Subsequently, SUV-PLDA is also studied
with utterance partitioning approach, and utterance partitioning-based
SUV-GPLDA system shows relative improvement of 9% and 16% in EER for NIST 2008
and NIST 2010 truncated 10sec-10sec evaluation condition as utterance
partitioning approach compensates the utterance variation and SUV modelling
approach compensates the mismatch between full-length development data and
short-length evaluation data."
"Acoustic reflector localization is an important issue in audio signal
processing, with direct applications in spatial audio, scene reconstruction,
and source separation. Several methods have recently been proposed to estimate
the 3D positions of acoustic reflectors given room impulse responses (RIRs). In
this article, we categorize these methods as ""image-source reversion"", which
localizes the image source before finding the reflector position, and ""direct
localization"", which localizes the reflector without intermediate steps. We
present five new contributions. First, an onset detector, called the clustered
dynamic programming projected phase-slope algorithm, is proposed to
automatically extract the time of arrival for early reflections within the RIRs
of a compact microphone array. Second, we propose an image-source reversion
method that uses the RIRs from a single loudspeaker. It is constructed by
combining an image source locator (the image source direction and range (ISDAR)
algorithm), and a reflector locator (using the loudspeaker-image bisection
(LIB) algorithm). Third, two variants of it, exploiting multiple loudspeakers,
are proposed. Fourth, we present a direct localization method, the ellipsoid
tangent sample consensus (ETSAC), exploiting ellipsoid properties to localize
the reflector. Finally, systematic experiments on simulated and measured RIRs
are presented, comparing the proposed methods with the state-of-the-art. ETSAC
generates errors lower than the alternative methods compared through our
datasets. Nevertheless, the ISDAR-LIB combination performs well and has a run
time 200 times faster than ETSAC."
"Infant speech perception and learning is modeled using Echo State Network
classification and Reinforcement Learning. Ambient speech for the modeled
infant learner is created using the speech synthesizer Vocaltractlab. An
auditory system is trained to recognize vowel sounds from a series of speakers
of different anatomies in Vocaltractlab. Having formed perceptual targets, the
infant uses Reinforcement Learning to imitate his ambient speech. A possible
way of bridging the problem of speaker normalisation is proposed, using direct
imitation but also including a caregiver who listens to the infants sounds and
imitates those that sound vowel-like."
"This paper addresses the problem of multiple-speaker localization in noisy
and reverberant environments, using binaural recordings of an acoustic scene. A
Gaussian mixture model (GMM) is adopted, whose components correspond to all the
possible candidate source locations defined on a grid. After optimizing the
GMM-based objective function, given an observed set of binaural features, both
the number of sources and their locations are estimated by selecting the GMM
components with the largest priors. This is achieved by enforcing a sparse
solution, thus favoring a small number of speakers with respect to the large
number of initial candidate source locations. An entropy-based penalty term is
added to the likelihood, thus imposing sparsity over the set of GMM priors. In
addition, the direct-path relative transfer function (DP-RTF) is used to build
robust binaural features. The DP-RTF, recently proposed for single-source
localization, was shown to be robust to reverberations, since it encodes
inter-channel information corresponding to the direct-path of sound
propagation. In this paper, we extend the DP-RTF estimation to the case of
multiple sources. In the short-time Fourier transform domain, a consistency
test is proposed to check whether a set of consecutive frames is associated to
the same source or not. Reliable DP-RTF features are selected from the frames
that pass the consistency test to be used for source localization. Experiments
carried out using both simulation data and real data gathered with a robotic
head confirm the efficiency of the proposed multi-source localization method."
"It is often required to extract the sound of an objective instrument played
in concert with other instruments. Microphone array is one of the effective
ways to enhance a sound from a specific direction. However it is not effective
in an echoic room such as concert hall. The pickup microphone attached on the
specific musical instrument is often employed to obtain the sound exclusively
from other instrumental sounds. The obtained timbre differ from the one we hear
at the usual listening position. The purpose of this paper is to propose a new
method of sound separation that utilizes the piezoelectric device attached on
the body of the instrument. The signal from the attached device has a different
spectrum from the sound heard by the audience but has the same frequency
components as the instrumental sound. Our idea is to use the device signal as a
modifier of the sound focusing filter applied to the microphone sound at the
listening position. The proposed method firstly estimates the frequency
components of the signal from the piezoelectric device. The frequency
characteristics for filtering the microphone sound are changed so that it pass
the estimated frequency components. Thus we can extractthe target sound without
distortion. The proposed method is a sort of dynamic sparseness approach. It
was found that SNR is improved by 8.7dB through the experiments."
"In this paper, we investigate the effectiveness of two-stage classification
strategies in detecting north Atlantic right whale upcalls. Time-frequency
measurements of data from passive acoustic monitoring devices are evaluated as
images. Vocalization spectrograms are preprocessed for noise reduction and tone
removal. First stage of the algorithm eliminates non-upcalls by an energy
detection algorithm. In the second stage, two sets of features are extracted
from the remaining signals using contour-based and texture based methods. The
former is based on extraction of time-frequency features from upcall contours,
and the latter employs a Local Binary Pattern operator to extract
distinguishing texture features of the upcalls. Subsequently evaluation phase
is carried out by using several classifiers to assess the effectiveness of both
the contour-based and texture-based features for upcall detection. Experimental
results with the data set provided by the Cornell University Bioacoustics
Research Program reveal that classifiers show accuracy improvements of 3% to 4%
when using LBP features over time-frequency features. Classifiers such as the
Linear Discriminant Analysis, Support Vector Machine, and TreeBagger achieve
high upcall detection rates with LBP features."
"In North-Indian-Music-System(NIMS),tabla is mostly used as percussive
accompaniment for vocal-music in polyphonic-compositions. The human auditory
system uses perceptual grouping of musical-elements and easily filters the
tabla component, thereby decoding prominent rhythmic features like tala, tempo
from a polyphonic composition. For Western music, lots of work have been
reported for automated drum analysis of polyphonic composition. However,
attempts at computational analysis of tala by separating the tabla-signal from
mixed signal in NIMS have not been successful. Tabla is played with two
components - right and left. The right-hand component has frequency overlap
with voice and other instruments. So, tala analysis of polyphonic-composition,
by accurately separating the tabla-signal from the mixture is a baffling task,
therefore an area of challenge. In this work we propose a novel technique for
successfully detecting tala using left-tabla signal, producing meaningful
results because the left-tabla normally doesn't have frequency overlap with
voice and other instruments. North-Indian-rhythm follows complex cyclic
pattern, against linear approach of Western-rhythm. We have exploited this
cyclic property along with stressed and non-stressed methods of playing
tabla-strokes to extract a characteristic pattern from the left-tabla strokes,
which, after matching with the grammar of tala-system, determines the tala and
tempo of the composition. A large number of
polyphonic(vocal+tabla+other-instruments) compositions has been analyzed with
the methodology and the result clearly reveals the effectiveness of proposed
techniques."
"Jazz guitar solos are improvised melody lines played on one instrument on top
of a chordal accompaniment (comping). As the improvisation happens
spontaneously, a reference score is non-existent, only a lead sheet. There are
situations, however, when one would like to have the original melody lines in
the form of notated music, see the Real Book. The motivation is either for the
purpose of practice and imitation or for musical analysis. In this work, an
automatic transcriber for jazz guitar solos is developed. It resorts to a very
intuitive representation of tonal music signals: the pitchgram. No
instrument-specific modeling is involved, so the transcriber should be
applicable to other pitched instruments as well. Neither is there the need to
learn any note profiles prior to or during the transcription. Essentially, the
proposed transcriber is a decision tree, thus a classifier, with a depth of 3.
It has a (very) low computational complexity and can be run on-line. The
decision rules can be refined or extended with no or little musical education.
The transcriber's performance is evaluated on a set of ten jazz solo excerpts
and compared with a state-of-the-art transcription system for the guitar plus
PYIN. We achieve an improvement of 34% w.r.t. the reference system and 19%
w.r.t. PYIN in terms of the F-measure. Another measure of accuracy, the error
score, attests that the number of erroneous pitch detections is reduced by more
than 50% w.r.t. the reference system and by 45% w.r.t. PYIN."
"The scattering framework offers an optimal hierarchical convolutional
decomposition according to its kernels. Convolutional Neural Net (CNN) can be
seen as an optimal kernel decomposition, nevertheless it requires large amount
of training data to learn its kernels. We propose a trade-off between these two
approaches: a Chirplet kernel as an efficient Q constant bioacoustic
representation to pretrain CNN. First we motivate Chirplet bioinspired auditory
representation. Second we give the first algorithm (and code) of a Fast
Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of
FCT on large environmental data base: months of Orca recordings, and 1000 Birds
species from the LifeClef challenge. Fourth, we validate FCT on the vowels
subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN
when it pretrains low level layers: it reduces training duration by -28\% for
birds classification, and by -26% for vowels classification. Scores are also
enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average
Precision on birds, and +2.3\% of vowel accuracy against raw audio CNN. We
conclude on perspectives on tonotopic FCT deep machine listening, and
inter-species bioacoustic transfer learning to generalise the representation of
animal communication systems."
"Research in collaborative music learning is subject to unresolved problems
demanding new technological solutions. One such problem poses the suppression
of the accompaniment in a live recording of a performance during practice,
which can be for the purposes of self-assessment or further machine-aided
analysis. Being able to separate a solo from the accompaniment allows to create
learning agents that may act as personal tutors and help the apprentice improve
his or her technique. First, we start from the classical adaptive noise
cancelling approach, and adjust it to the problem at hand. In a second step, we
compare some adaptive and Wiener filtering approaches and assess their
performances on the task. Our findings underpin that adaptive filtering is
inapt of dealing with music signals and that Wiener filtering in the short-time
Fourier transform domain is a much more effective approach. In addition, it is
very cheap if carried out in the frequency bands of auditory filters. A
double-output extension based on maximal-ratio combining is also proposed."
"One key step in audio signal processing is to transform the raw signal into
representations that are efficient for encoding the original information.
Traditionally, people transform the audio into spectral representations, as a
function of frequency, amplitude and phase transformation. In this work, we
take a purely data-driven approach to understand the temporal dynamics of audio
at the raw signal level. We maximize the information extracted from the raw
signal through a deep convolutional neural network (CNN) model. Our CNN model
is trained on the urbansound8k dataset. We discover that salient audio patterns
embedded in the raw waveforms can be efficiently extracted through a
combination of nonlinear filters learned by the CNN model."
"This text offers a personal and very subjective view on the current situation
of Music Information Research (MIR). Motivated by the desire to build systems
with a somewhat deeper understanding of music than the ones we currently have,
I try to sketch a number of challenges for the next decade of MIR research,
grouped around six simple truths about music that are probably generally agreed
on, but often ignored in everyday research."
"In this paper we present FRIDA---an algorithm for estimating directions of
arrival of multiple wideband sound sources. FRIDA combines multi-band
information coherently and achieves state-of-the-art resolution at extremely
low signal-to-noise ratios. It works for arbitrary array layouts, but unlike
the various steered response power and subspace methods, it does not require a
grid search. FRIDA leverages recent advances in sampling signals with a finite
rate of innovation. It is based on the insight that for any array layout, the
entries of the spatial covariance matrix can be linearly transformed into a
uniformly sampled sum of sinusoids."
"Musical frequencies in Just Intonation are comprised of rational numbers. The
structure of rational numbers is determined by prime factorisations. Just
Intonation frequencies can be split into two components. The larger component
uses only integer powers of the first two primes, 2 and 3. The smaller
component decomposes into a series of microtonal adjustments, one for each
prime number 5 and above present in the original frequency. The larger 3-limit
component can be notated using scientific pitch notation modified to use
Pythagorean tuning. The microtonal adjustments can be notated using rational
commas which are built up from prime commas. This gives a notation system for
the whole of free-JI, called Rational Comma Notation. RCN is compact since all
microtonal adjustments can be represented by a single notational unit based on
a rational number. RCN has different versions depending on the choice of
algorithm to assign a prime comma to each prime number. Two existing algorithms
SAG and KG2 are found in the literature. A novel algorithm DR is developed
based on discussion of mathematical and musical criteria for algorithm design.
Results for DR are presented for primes below 1400. Some observations are made
about these results and their applications, including shorthand notation and
pitch class lattices. Results for DR are compared with those for SAG and KG2.
Translation is possible between any two free-JI notations and any two versions
of RCN since they all represent the same underlying set of rational numbers."
"A cost effective approach to remote monitoring of protected areas such as
marine reserves and restricted naval waters is to use passive sonar to detect,
classify, localize, and track marine vessel activity (including small boats and
autonomous underwater vehicles). Cepstral analysis of underwater acoustic data
enables the time delay between the direct path arrival and the first multipath
arrival to be measured, which in turn enables estimation of the instantaneous
range of the source (a small boat). However, this conventional method is
limited to ranges where the Lloyd's mirror effect (interference pattern formed
between the direct and first multipath arrivals) is discernible. This paper
proposes the use of convolutional neural networks (CNNs) for the joint
detection and ranging of broadband acoustic noise sources such as marine
vessels in conjunction with a data augmentation approach for improving network
performance in varied signal-to-noise ratio (SNR) situations. Performance is
compared with a conventional passive sonar ranging method for monitoring marine
vessel activity using real data from a single hydrophone mounted above the sea
floor. It is shown that CNNs operating on cepstrum data are able to detect the
presence and estimate the range of transiting vessels at greater distances than
the conventional method."
"In this paper, we investigate DCTNet for audio signal classification. Its
output feature is related to Cohen's class of time-frequency distributions. We
introduce the use of adaptive DCTNet (A-DCTNet) for audio signals feature
extraction. The A-DCTNet applies the idea of constant-Q transform, with its
center frequencies of filterbanks geometrically spaced. The A-DCTNet is
adaptive to different acoustic scales, and it can better capture low frequency
acoustic information that is sensitive to human audio perception than features
such as Mel-frequency spectral coefficients (MFSC). We use features extracted
by the A-DCTNet as input for classifiers. Experimental results show that the
A-DCTNet and Recurrent Neural Networks (RNN) achieve state-of-the-art
performance in bird song classification rate, and improve artist identification
accuracy in music data. They demonstrate A-DCTNet's applicability to signal
processing problems."
"Some glottal analysis approaches based upon linear prediction or complex
cepstrum approaches have been proved to be effective to estimate glottal source
from real speech utterances. We propose a new approach employing both an
all-pole odd-order linear prediction to provide a coarse estimation and phase
decomposition based causality/anti-causality separation to generate further
refinements. The obtained measures show that this method improved performance
in terms of reducing source-filter separation in estimation of glottal flow
pulses (GFP). No glottal model fitting is required by this method, thus it has
wide and flexible adaptation to retain fidelity of speakers's vocal features
with computationally affordable resource. The method is evaluated on real
speech utterances to validate it."
"The use of deep learning to solve problems in literary arts has been a recent
trend that has gained a lot of attention and automated generation of music has
been an active area. This project deals with the generation of music using raw
audio files in the frequency domain relying on various LSTM architectures.
Fully connected and convolutional layers are used along with LSTM's to capture
rich features in the frequency domain and increase the quality of music
generated. The work is focused on unconstrained music generation and uses no
information about musical structure(notes or chords) to aid learning.The music
generated from various architectures are compared using blind fold tests. Using
the raw audio to train models is the direction to tapping the enormous amount
of mp3 files that exist over the internet without requiring the manual effort
to make structured MIDI files. Moreover, not all audio files can be represented
with MIDI files making the study of these models an interesting prospect to the
future of such models."
"In this demo we show a novel approach to score following. Instead of relying
on some symbolic representation, we are using a multi-modal convolutional
neural network to match the incoming audio stream directly to sheet music
images. This approach is in an early stage and should be seen as proof of
concept. Nonetheless, the audience will have the opportunity to test our
implementation themselves via 3 simple piano pieces."
"We propose a new algorithm for time stretching music signals based on the
theory of nonstationary Gabor frames. The algorithm extends the techniques of
the classical phase vocoder by incorporating adaptive time-frequency
representations and adaptive phase locking. Applying a preliminary onset
detection algorithm, the obtained time-frequency representation implies good
time resolution for the onsets and good frequency resolution for the sinusoidal
components.
  The phase estimates are done only at peak channels using quadratic
interpolation and the remaining phases are then locked to the values of the
peaks in an adaptive manner. In contrast to previous attempts we let the number
of frequency channels vary over time in order to obtain a low redundancy of the
corresponding transform. We show that with a redundancy comparable to that of
the phase vocoder we can greatly reduce artefacts such as phasiness and
transient smearing. The algorithm is tested on both synthetic and real world
signals and compared with state of the art algorithms in a reproducible manner."
"This paper describes the LIA speaker recognition system developed for the
Speaker Recognition Evaluation (SRE) campaign. Eight sub-systems are developed,
all based on a state-of-the-art approach: i-vector/PLDA which represents the
mainstream technique in text-independent speaker recognition. These sub-systems
differ: on the acoustic feature extraction front-end (MFCC, PLP), at the
i-vector extraction stage (UBM, DNN or two-feats posteriors) and finally on the
data-shifting (IDVC, mean-shifting). The submitted system is a fusion at the
score-level of these eight sub-systems."
"This paper describes a computational model of loudness variations in
expressive ensemble performance. The model predicts and explains the continuous
variation of loudness as a function of information extracted automatically from
the written score. Although such models have been proposed for expressive
performance in solo instruments, this is (to the best of our knowledge) the
first attempt to define a model for expressive performance in ensembles. To
that end, we extend an existing model that was designed to model expressive
piano performances, and describe the additional steps necessary for the model
to deal with scores of arbitrary instrumentation, including orchestral scores.
We test both linear and non-linear variants of the extended model n a data set
of audio recordings of symphonic music, in a leave-one-out setting. The
experiments reveal that the most successful model variant is a recurrent,
non-linear model. Even if the accuracy of the predicted loudness varies from
one recording to another, in several cases the model explains well over 50% of
the variance in loudness."
"We introduce a novel approach to studying animal behaviour and the context in
which it occurs, through the use of microphone backpacks carried on the backs
of individual free-flying birds. These sensors are increasingly used by animal
behaviour researchers to study individual vocalisations of freely behaving
animals, even in the field. However such devices may record more than an
animals vocal behaviour, and have the potential to be used for investigating
specific activities (movement) and context (background) within which
vocalisations occur. To facilitate this approach, we investigate the automatic
annotation of such recordings through two different sound scene analysis
paradigms: a scene-classification method using feature learning, and an
event-detection method using probabilistic latent component analysis (PLCA). We
analyse recordings made with Eurasian jackdaws (Corvus monedula) in both
captive and field settings. Results are comparable with the state of the art in
sound scene analysis; we find that the current recognition quality level
enables scalable automatic annotation of audio logger data, given partial
annotation, but also find that individual differences between animals and/or
their backpacks limit the generalisation from one individual to another. we
consider the interrelation of 'scenes' and 'events' in this particular task,
and issues of temporal resolution."
"In this work, we propose a two-dimensional Head-Related Transfer Function
(HRTF)-based robust beamformer design for robot audition, which allows for
explicit control of the beamformer response for the entire three-dimensional
sound field surrounding a humanoid robot. We evaluate the proposed method by
means of both signal-independent and signal-dependent measures in a robot
audition scenario. Our results confirm the effectiveness of the proposed
two-dimensional HRTF-based beamformer design, compared to our previously
published one-dimensional HRTF-based beamformer design, which was carried out
for a fixed elevation angle only."
"This paper addresses the problem of Target Activity Detection (TAD) for
binaural listening devices. TAD denotes the problem of robustly detecting the
activity of a target speaker in a harsh acoustic environment, which comprises
interfering speakers and noise (cocktail party scenario). In previous work, it
has been shown that employing a Feed-forward Neural Network (FNN) for detecting
the target speaker activity is a promising approach to combine the advantage of
different TAD features (used as network inputs). In this contribution, we
exploit a larger context window for TAD and compare the performance of FNNs and
Recurrent Neural Networks (RNNs) with an explicit focus on small network
topologies as desirable for embedded acoustic signal processing systems. More
specifically, the investigations include a comparison between three different
types of RNNs, namely plain RNNs, Long Short-Term Memories, and Gated Recurrent
Units. The results indicate that all versions of RNNs outperform FNNs for the
task of TAD."
"There is a common observation that audio event classification is easier to
deal with than detection. So far, this observation has been accepted as a fact
and we lack of a careful analysis. In this paper, we reason the rationale
behind this fact and, more importantly, leverage them to benefit the audio
event detection task. We present an improved detection pipeline in which a
verification step is appended to augment a detection system. This step employs
a high-quality event classifier to postprocess the benign event hypotheses
outputted by the detection system and reject false alarms. To demonstrate the
effectiveness of the proposed pipeline, we implement and pair up different
event detectors based on the most common detection schemes and various event
classifiers, ranging from the standard bag-of-words model to the
state-of-the-art bank-of-regressors one. Experimental results on the ITC-Irst
dataset show significant improvements to detection performance. More
importantly, these improvements are consistent for all detector-classifier
combinations."
"Traditional speech enhancement techniques modify the magnitude of a speech in
time-frequency domain, and use the phase of a noisy speech to resynthesize a
time domain speech. This work proposes a complex-valued Gaussian process latent
variable model (CGPLVM) to enhance directly the complex-valued noisy spectrum,
modifying not only the magnitude but also the phase. The main idea that
underlies the developed method is the modeling of short-time Fourier transform
(STFT) coefficients across the time frames of a speech as a proper complex
Gaussian process (GP) with noise added. The proposed method is based on
projecting the spectrum into a low-dimensional subspace. The likelihood
criterion is used to optimize the hyperparameters of the model. Experiments
were carried out on the CHTTL database, which contains the digits zero to nine
in Mandarin. Several standard measures are used to demonstrate that the
proposed method outperforms baseline methods."
"The higher order differential energy operator (DEO), denoted via
$\Upsilon_k(x)$, is an extension to the second order famous Teager-Kaiser
operator. The DEO helps measuring the higher order gauge of energy of a signal
which is useful for AM-FM demodulation. However, the energy criterion defined
by the DEO is not compliant with the presumption of positivity of energy. In
this paper we introduce a higher order operator called Positive Differential
Energy Operator (PDEO). This operator which can be obtained using alternative
recursive relations, resolves the energy sign problem. The simulations
demonstrate that the proposed operator can outperform DEOs in terms of Average
Signal to Error Ratio (ASER) in AM/FM demodulation."
"Several recent polyphonic music transcription systems have utilized deep
neural networks to achieve state of the art results on various benchmark
datasets, pushing the envelope on framewise and note-level performance
measures. Unfortunately we can observe a sort of glass ceiling effect. To
investigate this effect, we provide a detailed analysis of the particular kinds
of errors that state of the art deep neural transcription systems make, when
trained and tested on a piano transcription task. We are ultimately forced to
draw a rather disheartening conclusion: the networks seem to learn combinations
of notes, and have a hard time generalizing to unseen combinations of notes.
Furthermore, we speculate on various means to alleviate this situation."
"This research was conducted to develop a method to identify voice utterance.
For voice utterance that encounters change caused by aging factor, with the
interval of 10 to 25 years. The change of voice utterance influenced by aging
factor might be extracted by MFCC (Mel Frequency Cepstrum Coefficient).
However, the level of the compatibility of the feature may be dropped down to
55%. While the ones which do not encounter it may reach 95%. To improve the
compatibility of the changing voice feature influenced by aging factor, then
the method of the more specific feature extraction is developed: which is by
separating the voice into several channels, suggested as MFCC multichannel,
consisting of multichannel 5 filterbank (M5FB), multichannel 2 filterbank
(M2FB) and multichannel 1 filterbank (M1FB). The result of the test shows that
for model M5FB and M2FB have the highest score in the level of compatibility
with 85% and 82% with 25 years interval. While model M5FB gets the highest
score of 86% for 10 years time interval."
"Musical source separation methods exploit source-specific spectral
characteristics to facilitate the decomposition process. Kernel Additive
Modelling (KAM) models a source applying robust statistics to time-frequency
bins as specified by a source-specific kernel, a function defining similarity
between bins. Kernels in existing approaches are typically defined using
metrics between single time frames. In the presence of noise and other sound
sources information from a single-frame, however, turns out to be unreliable
and often incorrect frames are selected as similar. In this paper, we
incorporate a temporal context into the kernel to provide additional
information stabilizing the similarity search. Evaluated in the context of
vocal separation, our simple extension led to a considerable improvement in
separation quality compared to previous kernels."
"The mechanism proposed here is for real-time speaker change detection in
conversations, which firstly trains a neural network text-independent speaker
classifier using in-domain speaker data. Through the network, features of
conversational speech from out-of-domain speakers are then converted into
likelihood vectors, i.e. similarity scores comparing to the in-domain speakers.
These transformed features demonstrate very distinctive patterns, which
facilitates differentiating speakers and enable speaker change detection with
some straight-forward distance metrics. The speaker classifier and the speaker
change detector are trained/tested using speech of the first 200 (in-domain)
and the remaining 126 (out-of-domain) male speakers in TIMIT respectively. For
the speaker classification, 100% accuracy at a 200 speaker size is achieved on
any testing file, given the speech duration is at least 0.97 seconds. For the
speaker change detection using speaker classification outputs, performance
based on 0.5, 1, and 2 seconds of inspection intervals were evaluated in terms
of error rate and F1 score, using synthesized data by concatenating speech from
various speakers. It captures close to 97% of the changes by comparing the
current second of speech with the previous second, which is very competitive
among literature using other methods."
"This work presents a novel framework based on feed-forward neural network for
text-independent speaker classification and verification, two related systems
of speaker recognition. With optimized features and model training, it achieves
100% classification rate in classification and less than 6% Equal Error Rate
(ERR), using merely about 1 second and 5 seconds of data respectively. Features
with stricter Voice Active Detection (VAD) than the regular one for speech
recognition ensure extracting stronger voiced portion for speaker recognition,
speaker-level mean and variance normalization helps to eliminate the
discrepancy between samples from the same speaker. Both are proven to improve
the system performance. In building the neural network speaker classifier, the
network structure parameters are optimized with grid search and dynamically
reduced regularization parameters are used to avoid training terminated in
local minimum. It enables the training goes further with lower cost. In speaker
verification, performance is improved with prediction score normalization,
which rewards the speaker identity indices with distinct peaks and penalizes
the weak ones with high scores but more competitors, and speaker-specific
thresholding, which significantly reduces ERR in the ROC curve. TIMIT corpus
with 8K sampling rate is used here. First 200 male speakers are used to train
and test the classification performance. The testing files of them are used as
in-domain registered speakers, while data from the remaining 126 male speakers
are used as out-of-domain speakers, i.e. imposters in speaker verification."
"We formulated and implemented a procedure to generate aliasing-free
excitation source signals. It uses a new antialiasing filter in the continuous
time domain followed by an IIR digital filter for response equalization. We
introduced a cosine-series-based general design procedure for the new
antialiasing function. We applied this new procedure to implement the
antialiased Fujisaki-Ljungqvist model. We also applied it to revise our
previous implementation of the antialiased Fant-Liljencrants model. A
combination of these signals and a lattice implementation of the time varying
vocal tract model provides a reliable and flexible basis to test fo extractors
and source aperiodicity analysis methods. MATLAB implementations of these
antialiased excitation source models are available as part of our open source
tools for speech science."
"This computer science master thesis aims at modelling the nonlinearities of a
loudspeaker. A piecewise linear approximation is initially explored and then we
present a nonlinear Volterra model to simulate the behavior of the system. The
general theory of continuous and discrete Volterra series is summarised. A
Normalized Least Mean Square algorithm is used to determine the Volterra series
to third order. We also present as inverted system which is trained with the
same algorithm. Training data for the models were collected measuring a
physical speaker using a laser interferometer. Results indicate a decrease in
Mean Squared Error compared to the linear model with a dependency on the
particular test signal, the order and the parameters of the model."
"We demonstrate the capabilities of nonlinear Volterra models to simulate the
behavior of an audio system and compare them to linear filters. In this paper a
nonlinear model of an audio system based on Volterra series is presented and
Normalized Least Mean Square algorithm is used to determine the Volterra series
to third order. Training data for the models were collected measuring a
physical speaker using a laser interferometer. We explore several training
signals and filter's parameters. Results indicate a decrease in Mean Squared
Error compared to the linear model with a dependency on the particular test
signal, the order and the parameters of the model."
"Acoustic beamforming with a microphone array represents an adequate
technology for remote acoustic surveillance, as the system has no mechanical
parts and it has moderate size. However, in order to accomplish real
implementation, several challenges need to be addressed, such as array
geometry, microphone characteristics, and the digital beamforming algorithms.
This paper presents a simulated analysis on the effect of the array geometry in
the beamforming response. Two geometries are considered, namely, the linear and
the circular geometry. The analysis is performed with computer simulations to
mimic reality. The future steps comprise the construction of the physical
microphone array, and the software implementation on a multichannel digital
signal processing (DSP) system."
"For enhancing noisy signals, pre-trained single-channel speech enhancement
schemes exploit prior knowledge about the shape of typical speech structures.
This knowledge is obtained from training data for which methods from machine
learning are used, e.g., Mixtures of Gaussians, nonnegative matrix
factorization, and deep neural networks. If only speech envelopes are employed
as prior speech knowledge, e.g., to meet requirements in terms of computational
complexity and memory consumption, Wiener-like enhancement filters will not be
able to reduce noise components between speech spectral harmonics. In this
paper, we highlight the role of clean speech estimators that employ
super-Gaussian speech priors in particular for pre- trained approaches when
spectral envelope models are used. In the 2000s, such estimators have been
considered by many researchers for improving non-trained enhancement schemes.
However, while the benefit of super-Gaussian clean speech estimators in
non-trained enhancement schemes is limited, we point out that these estimators
make a much larger difference for enhancement schemes that employ pre-trained
envelope models. We show that for such pre-trained enhancements schemes super-
Gaussian estimators allow for a suppression of annoying residual noises which
are not reduced using Gaussian filters such as the Wiener filter. As a
consequence, considerable improvements in terms of Perceptual Evaluation of
Speech Quality and segmental signal-to-noise ratios are achieved."
"Audio tagging aims to perform multi-label classification on audio chunks and
it is a newly proposed task in the Detection and Classification of Acoustic
Scenes and Events 2016 (DCASE 2016) challenge. This task encourages research
efforts to better analyze and understand the content of the huge amounts of
audio data on the web. The difficulty in audio tagging is that it only has a
chunk-level label without a frame-level label. This paper presents a weakly
supervised method to not only predict the tags but also indicate the temporal
locations of the occurred acoustic events. The attention scheme is found to be
effective in identifying the important frames while ignoring the unrelated
frames. The proposed framework is a deep convolutional recurrent model with two
auxiliary modules: an attention module and a localization module. The proposed
algorithm was evaluated on the Task 4 of DCASE 2016 challenge. State-of-the-art
performance was achieved on the evaluation set with equal error rate (EER)
reduced from 0.13 to 0.11, compared with the convolutional recurrent baseline
system."
"The focus of this work is to study how to efficiently tailor Convolutional
Neural Networks (CNNs) towards learning timbre representations from log-mel
magnitude spectrograms. We first review the trends when designing CNN
architectures. Through this literature overview we discuss which are the
crucial points to consider for efficiently learning timbre representations
using CNNs. From this discussion we propose a design strategy meant to capture
the relevant time-frequency contexts for learning timbre, which permits using
domain knowledge for designing architectures. In addition, one of our main
goals is to design efficient CNN architectures -- what reduces the risk of
these models to over-fit, since CNNs' number of parameters is minimized.
Several architectures based on the design principles we propose are
successfully assessed for different research tasks related to timbre: singing
voice phoneme classification, musical instrument recognition and music
auto-tagging."
"Signal amplitude envelope allows to obtain information on the signal features
for different applications. It is commonly agreed that the envelope is a signal
that varies slowly and it should pass the prominent peaks of the data smoothly.
It has been widely used in sound analysis and also in different variables of
physiological data for animal and human studies. In order to get signal
envelope, a simple algorithm is proposed based on peak detection and it was
implemented with python libraries. This method can be used for different
applications in sound or in general in time series analysis for signals of
different origin or frequency content. As well, some aspects on the parameter
selection are discussed here to adapt the same method for different
applications and some traditional methods are also revisited."
"With ever-increasing number of car-mounted electric devices and their
complexity, audio classification is increasingly important for the automotive
industry as a fundamental tool for human-device interactions. Existing
approaches for audio classification, however, fall short as the unique and
dynamic audio characteristics of in-vehicle environments are not appropriately
taken into account. In this paper, we develop an audio classification system
that classifies an audio stream into music, speech, speech+music, and noise,
adaptably depending on driving environments including highway, local road,
crowded city, and stopped vehicle. More than 420 minutes of audio data
including various genres of music, speech, speech+music, and noise are
collected from diverse driving environments. The results demonstrate that the
proposed approach improves the average classification accuracy up to 166%, and
64% for speech, and speech+music, respectively, compared with a non-adaptive
approach in our experimental settings."
"We propose a multi-objective framework to learn both secondary targets not
directly related to the intended task of speech enhancement (SE) and the
primary target of the clean log-power spectra (LPS) features to be used
directly for constructing the enhanced speech signals. In deep neural network
(DNN) based SE we introduce an auxiliary structure to learn secondary
continuous features, such as mel-frequency cepstral coefficients (MFCCs), and
categorical information, such as the ideal binary mask (IBM), and integrate it
into the original DNN architecture for joint optimization of all the
parameters. This joint estimation scheme imposes additional constraints not
available in the direct prediction of LPS, and potentially improves the
learning of the primary target. Furthermore, the learned secondary information
as a byproduct can be used for other purposes, e.g., the IBM-based
post-processing in this work. A series of experiments show that joint LPS and
MFCC learning improves the SE performance, and IBM-based post-processing
further enhances listening quality of the reconstructed speech."
"Deep learning techniques have been used recently to tackle the audio source
separation problem. In this work, we propose to use deep convolution denoising
auto-encoders (CDAEs) for monaural audio source separation. We use as many
CDAEs as the number of sources to be separated from the mixed signal. Each CDAE
is trained to separate one source and treats the other sources as background
noise. The main idea is to allow each CDAE to learn suitable time-frequency
filters and features to its corresponding source. Our experimental results show
that CDAEs perform source separation slightly better than the deep feedforward
neural networks (FNNs) even with a much less number of parameters than FNNs."
"In this study we present a Deep Mixture of Experts (DMoE) neural-network
architecture for single microphone speech enhancement. By contrast to most
speech enhancement algorithms that overlook the speech variability mainly
caused by phoneme structure, our framework comprises a set of deep neural
networks (DNNs), each one of which is an 'expert' in enhancing a given speech
type corresponding to a phoneme. A gating DNN determines which expert is
assigned to a given speech segment. A speech presence probability (SPP) is then
obtained as a weighted average of the expert SPP decisions, with the weights
determined by the gating DNN. A soft spectral attenuation, based on the SPP, is
then applied to enhance the noisy speech signal. The experts and the gating
components of the DMoE network are trained jointly. As part of the training,
speech clustering into different subsets is performed in an unsupervised
manner. Therefore, unlike previous methods, a phoneme-labeled database is not
required for the training procedure. A series of experiments with different
noise types verified the applicability of the new algorithm to the task of
speech enhancement. The proposed scheme outperforms other schemes that either
do not consider phoneme structure or use a simpler training methodology."
"The automatic speaker identification procedure is used to extract features
that help to identify the components of the acoustic signal by discarding all
the other stuff like background noise, emotion, hesitation, etc. The acoustic
signal is generated by a human that is filtered by the shape of the vocal
tract, including tongue, teeth, etc. The shape of the vocal tract determines
and produced, what signal comes out in real time. The analytically develops
shape of the vocal tract, which exhibits envelop for the short time power
spectrum. The ASR needs efficient way of extracting features from the acoustic
signal that is used effectively to makes the shape of the individual vocal
tract. To identify any acoustic signal in the large collection of acoustic
signal i.e. corpora, it needs dimension compactness of total variability space
by using the GMM mean super vector. This work presents the efficient way to
implement dimension compactness in total variability space and using cosine
distance scoring to predict a fast output score for small size utterance."
"Speaker Identification process is to identify a particular vocal cord from a
set of existing speakers. In the speaker identification processes, unknown
speaker voice sample targets each of the existing speakers present in the
system and gives a predication. The predication may be more than one existing
known speaker voice and is very close to the unknown speaker voice. The model
is a Gaussian mixture model built by the extracted acoustic feature vectors
from voice. The i-vector based dimension compression mapping function of the
channel depended speaker, and super vector give better predicted scores
according to cosine distance scoring associated with the order pair of
speakers. In the order pair, the first coordinate is the unknown speaker i.e.
test speaker, and the second coordinates is the existing known speaker i.e.
target speaker. This paper presents the enhancement of the prediction based on
i- vector in compare to the normalized set of predicted score. In the
simulation, known speaker voices are collected through different channels and
in different languages. In the testing, the GMM voice models, and GMM based
i-Vector speaker voice models of the known speakers are used among the numbers
of clusters in the test data set."
"Sound propagation encompasses various acoustic phenomena including
reverberation. Current virtual acoustic methods, ranging from parametric
filters to physically-accurate solvers, can simulate reverberation with varying
degrees of fidelity. We investigate the effects of reverberant sounds generated
using different propagation algorithms on acoustic distance perception, i.e.,
how faraway humans perceive a sound source. In particular, we evaluate two
classes of methods for real-time sound propagation in dynamic scenes based on
parametric filters and ray tracing. Our study shows that the more accurate
method shows less distance compression as compared to the approximate,
filter-based method. This suggests that accurate reverberation in VR results in
a better reproduction of acoustic distances. We also quantify the levels of
distance compression introduced by different propagation methods in a virtual
environment."
"Peer-Led Team Learning (PLTL) is a learning methodology where a peer-leader
co-ordinate a small-group of students to collaboratively solve technical
problems. PLTL have been adopted for various science, engineering, technology
and maths courses in several US universities. This paper proposed and evaluated
a speech system for behavioral analysis of PLTL groups. It could help in
identifying the best practices for PLTL. The CRSS-PLTL corpus was used for
evaluation of developed algorithms. In this paper, we developed a robust speech
activity detection (SAD) by fusing the outputs of a DNN-based pitch extractor
and an unsupervised SAD based on voicing measures. Robust speaker diarization
system consisted of bottleneck features (from stacked autoencoder) and informed
HMM-based joint segmentation and clustering system. Behavioral characteristics
such as participation, dominance, emphasis, curiosity and engagement were
extracted by acoustic analyses of speech segments belonging to all students. We
proposed a novel method for detecting question inflection and performed equal
error rate analysis on PLTL corpus. In addition, a robust approach for
detecting emphasized speech regions was also proposed. Further, we performed
exploratory data analysis for understanding the distortion present in CRSS-PLTL
corpus as it was collected in naturalistic scenario. The ground-truth Likert
scale ratings were used for capturing the team dynamics in terms of student's
responses to a variety of evaluation questions. Results suggested the
applicability of proposed system for behavioral analysis of small-group
conversations such as PLTL, work-place meetings etc.. Keywords- Behavioral
Speech Processing, Bottleneck Features, Curiosity, Deep Neural Network,
Dominance, Auto-encoder, Emphasis, Engagement, Peer-Led Team Learning, Speaker
Diarization, Small-group Conversations"
"In competitive sports it is often very hard to quantify the performance. A
player to score or overtake may depend on only millesimal of seconds or
millimeters. In racquet sports like tennis, table tennis and squash many events
will occur in a short time duration, whose recording and analysis can help
reveal the differences in performance. In this paper we show that it is
possible to architect a framework that utilizes the characteristic sound
patterns to precisely classify the types of and localize the positions of these
events. From these basic information the shot types and the ball speed along
the trajectories can be estimated. Comparing these estimates with the optimal
speed and target the precision of the shot can be defined. The detailed shot
statistics and precision information significantly enriches and improves data
available today. Feeding them back to the players and the coaches facilitates
to describe playing performance objectively and to improve strategy skills. The
framework is implemented, its hardware and software components are installed
and tested in a squash court."
"We propose a robust two-dimensional polynomial beamformer design method,
formulated as a convex optimization problem, which allows for flexible steering
of a previously proposed data-independent robust beamformer in both azimuth and
elevation direction.~As an exemplary application, the proposed two-dimensional
polynomial beamformer design is applied to a twelve-element microphone array,
integrated into the head of a humanoid robot. To account for the effects of the
robot's head on the sound field, measured head-related transfer functions are
integrated into the optimization problem as steering vectors. The
two-dimensional polynomial beamformer design is evaluated using
signal-independent and signal-dependent measures. The results confirm that the
proposed polynomial beamformer design approximates the original fixed
beamformer design very accurately, which makes it an attractive approach for
robust real-time data-independent beamforming."
"Source separation and other audio applications have traditionally relied on
the use of short-time Fourier transforms as a front-end frequency domain
representation step. The unavailability of a neural network equivalent to
forward and inverse transforms hinders the implementation of end-to-end
learning systems for these applications. We present an auto-encoder neural
network that can act as an equivalent to short-time front-end transforms. We
demonstrate the ability of the network to learn optimal, real-valued basis
functions directly from the raw waveform of a signal and further show how it
can be used as an adaptive front-end for supervised source separation. In terms
of separation performance, these transforms outperform their Fourier
counterparts with an optimal frequency domain representation that is defined on
a highly reduced set of basis functions and tailored to the input."
"Advances in virtual reality have generated substantial interest in accurately
reproducing and storing spatial audio in the higher order ambisonics (HOA)
representation, given its rendering flexibility. Recent standardization for HOA
compression adopted a framework wherein HOA data are decomposed into principal
components that are then encoded by standard audio coding, i.e., frequency
domain quantization and entropy coding to exploit psychoacoustic redundancy. A
noted shortcoming of this approach is the occasional mismatch in principal
components across blocks, and the resulting suboptimal transitions in the data
fed to the audio coder. Instead, we propose a framework where singular value
decomposition (SVD) is performed after transformation to the frequency domain
via the modified discrete cosine transform (MDCT). This framework not only
ensures smooth transition across blocks, but also enables frequency dependent
SVD for better energy compaction. Moreover, we introduce a novel noise
substitution technique to compensate for suppressed ambient energy in discarded
higher order ambisonics channels, which significantly enhances the perceptual
quality of the reconstructed HOA signal. Objective and subjective evaluation
results provide evidence for the effectiveness of the proposed framework in
terms of both higher compression gains and better perceptual quality, compared
to existing methods."
"The goal of this thesis was to implement a tool that, given a digital audio
input, can extract and represent rhythm and musical time. The purpose of the
tool is to help develop better models of rhythm for real-time computer based
performance and composition. This analysis tool, Riddim, uses Independent
Subspace Analysis (ISA) and a robust onset detection scheme to separate and
detect salient rhythmic and timing information from different sonic sources
within the input. This information is then represented in a format that can be
used by a variety of algorithms that interpret timing information to infer
rhythmic and musical structure. A secondary objective of this work is a ""proof
of concept"" as a non-real-time rhythm analysis system based on ISA. This is a
necessary step since ultimately it is desirable to incorporate this
functionality in a real-time plug-in for live performance and improvisation."
"Human categorization of sound seems predominantly based on sound source
properties. To estimate these source properties we propose a novel sound
analysis method, which separates sound into different sonic textures: tones,
pulses, and broadband noises. The audible presence of tones or pulses
corresponds to more extended cochleagram patterns than can be expected on the
basis of correlations introduced by the gammachirp filterbank alone. We design
tract features to respond to these extended patterns, and use these to identify
areas of the time-frequency plane as tonal, pulsal, and noisy. Where an area is
marked as noisy if it is neither tonal nor pulsal. To investigate whether a
similar separation indeed underlies human perceptual organization we introduce
tract based descriptors: tonality, pulsality, and noisiness. These descriptors
keep track of either the total energy or the cochleagram area marked as
respectively tonal, pulsal, and noisy. Energy based tonality and pulsality is
strongly correlated with the first perceptual dimension of human subjects,
while energy based noisiness correlates moderately with the second perceptual
dimension. We conclude that harmonic, impact and continuous process sounds can
be largely separated with energy based tonality, pulsality and noisiness."
"A short overview demystifying the midi audio format is presented. The goal is
to explain the file structure and how the instructions are used to produce a
music signal, both in the case of monophonic signals as for polyphonic signals."
"A serious problem for automated music generation is to propose the model that
could reproduce sophisticated temporal and melodic patterns that would
correspond to the style of the training input. We propose a new architecture of
an artificial neural network that helps to deal with such tasks. The proposed
approach is based on a long short-term memory language model combined with
variational recurrent autoencoder. These methods have certain advantages when
dealing with temporally rich inputs. The proposed architecture comprises this
features and helps to generate results of higher complexity and diversity."
"Time-frequency representations are important for the analysis of time series.
We have developed an online time-series analysis system and equipped it to
reliably handle re-alignment in the time-frequency plane. The system can deal
with issues like invalid regions in time-frequency representations and
discontinuities in data transmissions, making it suitable for on-line
processing in real-world situations. In retrospect the whole problem can be
considered to be a generalization of ideas present in overlap-and-add
filtering, but then for time-frequency representations and including the
calculation of non-causal features. Here we present our design for
time-frequency representation fission and fusion rules. We present these rules
in the context of two typical use cases, which facilitate understanding of the
underlying choices."
"In large-scale wireless acoustic sensor networks (WASNs), many of the sensors
will only have a marginal contribution to a certain estimation task. Involving
all sensors increases the energy budget unnecessarily and decreases the
lifetime of the WASN. Using microphone subset selection, also termed as sensor
selection, the most informative sensors can be chosen from a set of candidate
sensors to achieve a prescribed inference performance. In this paper, we
consider microphone subset selection for minimum variance distortionless
response (MVDR) beamformer based noise reduction. The best subset of sensors is
determined by minimizing the transmission cost while constraining the output
noise power (or signal-to-noise ratio). Assuming the statistical information on
correlation matrices of the sensor measurements is available, the sensor
selection problem for this model-driven scheme is first solved by utilizing
convex optimization techniques. In addition, to avoid estimating the statistics
related to all the candidate sensors beforehand, we also propose a data-driven
approach to select the best subset using a greedy strategy. The performance of
the greedy algorithm converges to that of the model-driven method, while it
displays advantages in dynamic scenarios as well as on computational
complexity. Compared to a sparse MVDR or radius-based beamformer, experiments
show that the proposed methods can guarantee the desired performance with
significantly less transmission costs."
"We study the problem of dictionary learning for signals that can be
represented as polynomials or polynomial matrices, such as convolutive signals
with time delays or acoustic impulse responses. Recently, we developed a method
for polynomial dictionary learning based on the fact that a polynomial matrix
can be expressed as a polynomial with matrix coefficients, where the
coefficient of the polynomial at each time lag is a scalar matrix. However, a
polynomial matrix can be also equally represented as a matrix with polynomial
elements. In this paper, we develop an alternative method for learning a
polynomial dictionary and a sparse representation method for polynomial signal
reconstruction based on this model. The proposed methods can be used directly
to operate on the polynomial matrix without having to access its coefficients
matrices. We demonstrate the performance of the proposed method for acoustic
impulse response modeling."
"Conversational speech not only contains several variants of neutral speech
but is also prominently interlaced with several speaker generated non-speech
sounds such as laughter and breath. A robust speaker recognition system should
be capable of recognizing a speaker irrespective of these variations in his
speech. An understanding of whether the speaker-specific information
represented by these variations is similar or not helps build a good speaker
recognition system. In this paper, speaker variations captured by neutral
speech of a speaker is analyzed by considering speech-laugh (a variant of
neutral speech) and laughter (non-speech) sounds of the speaker. We study an
i-vector-based speaker recognition system trained only on neutral speech and
evaluate its performance on speech-laugh and laughter. Further, we analyze the
effect of including laughter sounds during training of an i-vector-basedspeaker
recognition system. Our experimental results show that the inclusion of
laughter sounds during training seem to provide complementary speaker-specific
information which results in an overall improved performance of the speaker
recognition system, especially on the utterances with speech-laugh segments."
"When a signal is recorded in an enclosed room, it typically gets affected by
reverberation. This degradation represents a problem when dealing with audio
signals, particularly in the field of speech signal processing, such as
automatic speech recognition. Although there are some approaches to deal with
this issue that are quite satisfactory under certain conditions, constructing a
method that works well in a general context still poses a significant
challenge. In this article, we propose a method based on convolutive
nonnegative matrix factorization that mixes two penalizers in order to impose
certain characteristics over the time-frequency components of the restored
signal and the reverberant components. An algorithm for implementing the method
is described and tested. Comparisons of the results against those obtained with
state of the art methods are presented, showing significant improvement."
"For practical automatic speaker verification (ASV) systems, replay attack
poses a true risk. By replaying a pre-recorded speech signal of the genuine
speaker, ASV systems tend to be easily fooled. An effective replay detection
method is therefore highly desirable. In this study, we investigate a major
difficulty in replay detection: the over-fitting problem caused by variability
factors in speech signal. An F-ratio probing tool is proposed and three
variability factors are investigated using this tool: speaker identity, speech
content and playback & recording device. The analysis shows that device is the
most influential factor that contributes the highest over-fitting risk. A
frequency warping approach is studied to alleviate the over-fitting problem, as
verified on the ASV-spoof 2017 database."
"We introduce a simple and linear SNR (strictly speaking, periodic to random
power ratio) estimator (0dB to 80dB without additional
calibration/linearization) for providing reliable descriptions of aperiodicity
in speech corpus. The main idea of this method is to estimate the background
random noise level without directly extracting the background noise. The
proposed method is applicable to a wide variety of time windowing functions
with very low sidelobe levels. The estimate combines the frequency derivative
and the time-frequency derivative of the mapping from filter center frequency
to the output instantaneous frequency. This procedure can replace the
periodicity detection and aperiodicity estimation subsystems of recently
introduced open source vocoder, YANG vocoder. Source code of MATLAB
implementation of this method will also be open sourced."
"In this paper, we propose a noise robust bottleneck feature representation
which is generated by an adversarial network (AN). The AN includes two cascade
connected networks, an encoding network (EN) and a discriminative network (DN).
Mel-frequency cepstral coefficients (MFCCs) of clean and noisy speech are used
as input to the EN and the output of the EN is used as the noise robust
feature. The EN and DN are trained in turn, namely, when training the DN, noise
types are selected as the training labels and when training the EN, all labels
are set as the same, i.e., the clean speech label, which aims to make the AN
features invariant to noise and thus achieve noise robustness. We evaluate the
performance of the proposed feature on a Gaussian Mixture Model-Universal
Background Model based speaker verification system, and make comparison to MFCC
features of speech enhanced by short-time spectral amplitude minimum mean
square error (STSA-MMSE) and deep neural network-based speech enhancement
(DNN-SE) methods. Experimental results on the RSR2015 database show that the
proposed AN bottleneck feature (AN-BN) dramatically outperforms the STSA-MMSE
and DNN-SE based MFCCs for different noise types and signal-to-noise ratios.
Furthermore, the AN-BN feature is able to improve the speaker verification
performance under the clean condition."
"This paper addresses the problems of blind channel identification and
multichannel equalization for speech dereverberation and noise reduction. The
time-domain cross-relation method is not suitable for blind room impulse
response identification, due to the near-common zeros of the long impulse
responses. We extend the cross-relation method to the short-time Fourier
transform (STFT) domain, in which the time-domain impulse responses are
approximately represented by the convolutive transfer functions (CTFs) with
much less coefficients. The CTFs suffer from the common zeros caused by the
oversampled STFT. We propose to identify CTFs based on the STFT with the
oversampled signals and the critical sampled CTFs, which is a good compromise
between the frequency aliasing of the signals and the common zeros problem of
CTFs. In addition, a normalization of the CTFs is proposed to remove the gain
ambiguity across sub-bands. In the STFT domain, the identified CTFs is used for
multichannel equalization, in which the sparsity of speech signals is
exploited. We propose to perform inverse filtering by minimizing the
$\ell_1$-norm of the source signal with the relaxed $\ell_2$-norm fitting error
between the micophone signals and the convolution of the estimated source
signal and the CTFs used as a constraint. This method is advantageous in that
the noise can be reduced by relaxing the $\ell_2$-norm to a tolerance
corresponding to the noise power, and the tolerance can be automatically set.
The experiments confirm the efficiency of the proposed method even under
conditions with high reverberation levels and intense noise."
"Estimating audio and musical signals from single channel mixtures often, if
not always, involves a transformation of the mixture signal to the
time-frequency (T-F) domain in which a masking operation takes place. Masking
is realized as an element-wise multiplication of the mixture signal's T-F
representation with a ratio of computed sources' spectrogram. Studies have
shown that the performance of the overall source estimation scheme is subject
to the sparsity and disjointness properties of a given T-F representation. In
this work we investigate the potential of an optimized pseudo quadrature mirror
filter-bank (PQMF), as a T-F representation for music source separation tasks.
Experimental results, suggest that the PQMF maintains the aforementioned
desirable properties and can be regarded as an alternative for representing
mixtures of musical signals."
"This paper describes the submission to fixed condition of NIST SRE 2016 by
Sharif University of Technology (SUT) team. We provide a full description of
the systems that were included in our submission. We start with an overview of
the datasets that were used for training and development. It is followed by
describing front-ends which contain different VAD and feature types. UBM and
i-vector extractor training are the next details in this paper. As one of the
important steps in system preparation, preconditioning the i-vectors are
explained in more details. Then, we describe the classifier and score
normalization methods. And finally, some results on SRE16 evaluation dataset
are reported and analyzed."
"Currently, most speech processing techniques use magnitude spectrograms as
front-end and are therefore by default discarding part of the signal: the
phase. In order to overcome this limitation, we propose an end-to-end learning
method for speech denoising based on Wavenet. The proposed model adaptation
retains Wavenet's powerful acoustic modeling capabilities, while significantly
reducing its time-complexity by eliminating its autoregressive nature.
Specifically, the model makes use of non-causal, dilated convolutions and
predicts target fields instead of a single target sample. These modifications
make the model highly parallelizable during both training and inference.
Furthermore, we propose a novel energy-conserving loss that directly operates
on the raw audio level. This loss also considers the quality of the estimated
background-noise signal (computed by applying a parameterless operation to the
input) during training. This direct link to the input enforces conserving the
energy of the signal throughout the pipeline. Both computational and perceptual
evaluations indicate that the proposed method is preferred to Wiener filtering,
a common method based on processing the magnitude spectrogram."
"A filter for universal real-time prediction of band-limited signals is
presented. The filter consists of multiple time-delayed feedback terms in order
to accomplish anticipatory coupling, which again leads to a negative group
delay for frequencies in the baseband. The universality of the filter arises
from its property that it does not rely on a specific model of the signal.
Specifically, as long as the signal to be predicted is band-limited with a
known cutoff frequency, the filter order, the only parameter of the filter,
follows and the filter predicts the signal in real time up to a prediction
horizon that depends on the cutoff frequency, too. It is worked out in detail
how signal prediction arises from the negative group delay of the filter. Its
properties, including stability, are investigated theoretically, by numerical
simulations, and by application to a physiological signal. Possible control and
signal processing applications of this filter are discussed."
"It is well known that recognizers personalized to each user are much more
effective than user-independent recognizers. With the popularity of smartphones
today, although it is not difficult to collect a large set of audio data for
each user, it is difficult to transcribe it. However, it is now possible to
automatically discover acoustic tokens from unlabeled personal data in an
unsupervised way. We therefore propose a multi-task deep learning framework
called a phoneme-token deep neural network (PTDNN), jointly trained from
unsupervised acoustic tokens discovered from unlabeled data and very limited
transcribed data for personalized acoustic modeling. We term this scenario
""weakly supervised"". The underlying intuition is that the high degree of
similarity between the HMM states of acoustic token models and phoneme models
may help them learn from each other in this multi-task learning framework.
Initial experiments performed over a personalized audio data set recorded from
Facebook posts demonstrated that very good improvements can be achieved in both
frame accuracy and word accuracy over popularly-considered baselines such as
fDLR, speaker code and lightly supervised adaptation. This approach complements
existing speaker adaptation approaches and can be used jointly with such
techniques to yield improved results."
"Most existing datasets for speaker identification contain samples obtained
under quite constrained conditions, and are usually hand-annotated, hence
limited in size. The goal of this paper is to generate a large scale
text-independent speaker identification dataset collected 'in the wild'. We
make two contributions. First, we propose a fully automated pipeline based on
computer vision techniques to create the dataset from open-source media. Our
pipeline involves obtaining videos from YouTube; performing active speaker
verification using a two-stream synchronization Convolutional Neural Network
(CNN), and confirming the identity of the speaker using CNN based facial
recognition. We use this pipeline to curate VoxCeleb which contains hundreds of
thousands of 'real world' utterances for over 1,000 celebrities. Our second
contribution is to apply and compare various state of the art speaker
identification techniques on our dataset to establish baseline performance. We
show that a CNN based architecture obtains the best performance for both
identification and verification."
"There are many methods proposed for the detection of impulsive sounds in
literature. Most of them are complex and require adaptation to ambient noise.
In this paper we propose a very simple and efficient method to detect impulsive
sounds. Although we use energy like most of the others to determine impulsive
sounds, the way we calculate the energy is quite different. Also our
calculation is immune to ambient noise and does not require any limit or
adaptation. We could detect impulsive sounds embedded in various kinds of
noises by using this formula.
  As our ultimate aim is to detect gunshots, next phase of impulsive sound
detection is gunshot recognition phase. Detected impulsive sounds are fed into
recognition phase in which we can decide on gunshots with high success rate."
"In this paper, a novel multitaper modified group delay function-based
representation for speech signals is proposed. With a set of phoneme-based
experiments, it is shown that the proposed method performs better that an
existing multitaper magnitude (MT-MAG) estimation technique, in terms of
variance and MSE, both in spectral- and cepstral-domains. In particular, the
performance of MT-MOGDF is found to be the best with the Thomson tapers.
Additionally, the utility of the MT-MOGDF technique is highlighted in a speaker
recognition experimental setup, where an improvement of around $20\%$ compared
to the next-best technique is obtained. Moreover, the computational
requirements of the proposed technique is comparable to that of MT-MAG. The
proposed feature can be used in for many speech-related applications; in
particular, it is best suited among those that require information of speaker
and speech."
"In this paper, Suprasegmental Hidden Markov Models (SPHMMs) have been used to
enhance the recognition performance of text-dependent speaker identification in
the shouted environment. Our speech database consists of two databases: our
collected database and the Speech Under Simulated and Actual Stress (SUSAS)
database. Our results show that SPHMMs significantly enhance speaker
identification performance compared to Second-Order Circular Hidden Markov
Models (CHMM2s) in the shouted environment. Using our collected database,
speaker identification performance in this environment is 68% and 75% based on
CHMM2s and SPHMMs respectively. Using the SUSAS database, speaker
identification performance in the same environment is 71% and 79% based on
CHMM2s and SPHMMs respectively."
"It is known that the performance of speaker identification systems is high
under the neutral talking condition; however, the performance deteriorates
under the shouted talking condition. In this paper, second-order circular
hidden Markov models (CHMM2s) have been proposed and implemented to enhance the
performance of isolated-word text-dependent speaker identification systems
under the shouted talking condition. Our results show that CHMM2s significantly
improve speaker identification performance under such a condition compared to
the first-order left-to-right hidden Markov models (LTRHMM1s), second-order
left-to-right hidden Markov models (LTRHMM2s), and the first-order circular
hidden Markov models (CHMM1s). Under the shouted talking condition, our results
show that the average speaker identification performance is 23% based on
LTRHMM1s, 59% based on LTRHMM2s, and 60% based on CHMM1s. On the other hand,
the average speaker identification performance under the same talking condition
based on CHMM2s is 72%."
"Speaker identification performance is almost perfect in neutral talking
environments; however, the performance is deteriorated significantly in shouted
talking environments. This work is devoted to proposing, implementing and
evaluating new models called Second-Order Circular Suprasegmental Hidden Markov
Models (CSPHMM2s) to alleviate the deteriorated performance in the shouted
talking environments. These proposed models possess the characteristics of both
Circular Suprasegmental Hidden Markov Models (CSPHMMs) and Second-Order
Suprasegmental Hidden Markov Models (SPHMM2s). The results of this work show
that CSPHMM2s outperform each of: First-Order Left-to-Right Suprasegmental
Hidden Markov Models (LTRSPHMM1s), Second-Order Left-to-Right Suprasegmental
Hidden Markov Models (LTRSPHMM2s) and First-Order Circular Suprasegmental
Hidden Markov Models (CSPHMM1s) in the shouted talking environments. In such
talking environments and using our collected speech database, average speaker
identification performance based on LTRSPHMM1s, LTRSPHMM2s, CSPHMM1s and
CSPHMM2s is 74.6%, 78.4%, 78.7% and 83.4%, respectively. Speaker identification
performance obtained based on CSPHMM2s is close to that obtained based on
subjective assessment by human listeners."
"This work is aimed at exploiting Second-Order Circular Suprasegmental Hidden
Markov Models (CSPHMM2s) as classifiers to enhance talking condition
recognition in stressful and emotional talking environments (completely two
separate environments). The stressful talking environment that has been used in
this work uses Speech Under Simulated and Actual Stress (SUSAS) database, while
the emotional talking environment uses Emotional Prosody Speech and Transcripts
(EPST) database. The achieved results of this work using Mel-Frequency Cepstral
Coefficients (MFCCs) demonstrate that CSPHMM2s outperform each of Hidden Markov
Models (HMMs), Second-Order Circular Hidden Markov Models (CHMM2s), and
Suprasegmental Hidden Markov Models (SPHMMs) in enhancing talking condition
recognition in the stressful and emotional talking environments. The results
also show that the performance of talking condition recognition in stressful
talking environments leads that in emotional talking environments by 3.67%
based on CSPHMM2s. Our results obtained in subjective evaluation by human
judges fall within 2.14% and 3.08% of those obtained, respectively, in
stressful and emotional talking environments based on CSPHMM2s."
"The importance of speaking style authentication from human speech is gaining
an increasing attention and concern from the engineering community. The
importance comes from the demand to enhance both the naturalness and efficiency
of spoken language human-machine interface. Our work in this research focuses
on proposing, implementing, and testing speaker-dependent and text-dependent
speaking style authentication (verification) systems that accept or reject the
identity claim of a speaking style based on suprasegmental hidden Markov models
(SPHMMs). Based on using SPHMMs, our results show that the average speaking
style authentication performance is: 99%, 37%, 85%, 60%, 61%, 59%, 41%, 61%,
and 57% belonging respectively to the speaking styles: neutral, shouted, slow,
loud, soft, fast, angry, happy, and fearful."
"This work aims at investigating and analyzing speaker identification in each
unbiased and biased emotional talking environments based on a classifier called
Suprasegmental Hidden Markov Models (SPHMMs). The first talking environment is
unbiased towards any emotion, while the second talking environment is biased
towards different emotions. Each of these talking environments is made up of
six distinct emotions. These emotions are neutral, angry, sad, happy, disgust
and fear. The investigation and analysis of this work show that speaker
identification performance in the biased talking environment is superior to
that in the unbiased talking environment. The obtained results in this work are
close to those achieved in subjective assessment by human judges."
"In this paper, second-order hidden Markov model (HMM2) has been used and
implemented to improve the recognition performance of text-dependent speaker
identification systems under neutral talking condition. Our results show that
HMM2 improves the recognition performance under neutral talking condition
compared to the first-order hidden Markov model (HMM1). The recognition
performance has been improved by 9%."
"Speaker recognition performance in emotional talking environments is not as
high as it is in neutral talking environments. This work focuses on proposing,
implementing, and evaluating a new approach to enhance the performance in
emotional talking environments. The new proposed approach is based on
identifying the unknown speaker using both his/her gender and emotion cues.
Both Hidden Markov Models (HMMs) and Suprasegmental Hidden Markov Models
(SPHMMs) have been used as classifiers in this work. This approach has been
tested on our collected emotional speech database which is composed of six
emotions. The results of this work show that speaker identification performance
based on using both gender and emotion cues is higher than that based on using
gender cues only, emotion cues only, and neither gender nor emotion cues by
7.22%, 4.45%, and 19.56%, respectively. This work also shows that the optimum
speaker identification performance takes place when the classifiers are
completely biased towards suprasegmental models and no impact of acoustic
models in the emotional talking environments. The achieved average speaker
identification performance based on the new proposed approach falls within
2.35% of that obtained in subjective evaluation by human judges."
"Usually, people talk neutrally in environments where there are no abnormal
talking conditions such as stress and emotion. Other emotional conditions that
might affect people talking tone like happiness, anger, and sadness. Such
emotions are directly affected by the patient health status. In neutral talking
environments, speakers can be easily verified, however, in emotional talking
environments, speakers cannot be easily verified as in neutral talking ones.
Consequently, speaker verification systems do not perform well in emotional
talking environments as they do in neutral talking environments. In this work,
a two-stage approach has been employed and evaluated to improve speaker
verification performance in emotional talking environments. This approach
employs speaker emotion cues (text-independent and emotion-dependent speaker
verification problem) based on both Hidden Markov Models (HMMs) and
Suprasegmental Hidden Markov Models (SPHMMs) as classifiers. The approach is
comprised of two cascaded stages that combines and integrates emotion
recognizer and speaker recognizer into one recognizer. The architecture has
been tested on two different and separate emotional speech databases: our
collected database and Emotional Prosody Speech and Transcripts database. The
results of this work show that the proposed approach gives promising results
with a significant improvement over previous studies and other approaches such
as emotion-independent speaker verification approach and emotion-dependent
speaker verification approach based completely on HMMs."
"In this work we propose, implement, and evaluate novel models called
Third-Order Hidden Markov Models (HMM3s) to enhance low performance of
text-independent speaker identification in shouted talking environments. The
proposed models have been tested on our collected speech database using
Mel-Frequency Cepstral Coefficients (MFCCs). Our results demonstrate that HMM3s
significantly improve speaker identification performance in such talking
environments by 11.3% and 166.7% compared to second-order hidden Markov models
(HMM2s) and first-order hidden Markov models (HMM1s), respectively. The
achieved results based on the proposed models are close to those obtained in
subjective assessment by human listeners."
"In this research, we model and analyze the vocal tract under normal and
stressful talking conditions. This research answers the question of the
degradation in the recognition performance of text-dependent speaker
identification under stressful talking conditions. This research can be used
(for future research) to improve the recognition performance under stressful
talking conditions."
"This work focuses on Emirati speaker verification systems in neutral talking
environments based on each of First-Order Hidden Markov Models (HMM1s),
Second-Order Hidden Markov Models (HMM2s), and Third-Order Hidden Markov Models
(HMM3s) as classifiers. These systems have been evaluated on our collected
Emirati speech database which is comprised of 25 male and 25 female Emirati
speakers using Mel-Frequency Cepstral Coefficients (MFCCs) as extracted
features. Our results show that HMM3s outperform each of HMM1s and HMM2s for a
text-independent Emirati speaker verification. The obtained results based on
HMM3s are close to those achieved in subjective assessment by human listeners."
"This work focuses on enhancing the performance of text-dependent and
speaker-dependent talking condition identification systems using second-order
hidden Markov models (HMM2s). Our results show that the talking condition
identification performance based on HMM2s has been improved significantly
compared to first-order hidden Markov models (HMM1s). Our talking conditions in
this work are neutral, shouted, loud, angry, happy, and fear."
"The work of this research is devoted to studying and enhancing talking
condition recognition in stressful and emotional talking environments
(completely two separate environments) based on three different and separate
classifiers. The three classifiers are: Hidden Markov Models (HMMs),
Second-Order Circular Hidden Markov Models (CHMM2s) and Suprasegmental Hidden
Markov Models (SPHMMs). The stressful talking environments that have been used
in this work are composed of neutral, shouted, slow, loud, soft and fast
talking conditions, while the emotional talking environments are made up of
neutral, angry, sad, happy, disgust and fear emotions. The achieved results in
the current work show that SPHMMs lead each of HMMs and CHMM2s in improving
talking condition recognition in stressful and emotional talking environments.
The results also demonstrate that talking condition recognition in stressful
talking environments outperforms that in emotional talking environments by
2.7%, 1.8% and 3.3% based on HMMs, CHMM2s and SPHMMs, respectively. Based on
subjective assessment by human judges, the recognition performance of stressful
talking conditions leads that of emotional ones by 5.2%."
"It is well known that speaker identification yields very high performance in
a neutral talking environment, on the other hand, the performance has been
sharply declined in a shouted talking environment. This work aims at proposing,
implementing, and evaluating novel Third-Order Circular Suprasegmental Hidden
Markov Models (CSPHMM3s) to improve the low performance of text-independent
speaker identification in a shouted talking environment. CSPHMM3s possess
combined characteristics of: Circular Hidden Markov Models (CHMMs), Third-Order
Hidden Markov Models (HMM3s), and Suprasegmental Hidden Markov Models (SPHMMs).
Our results show that CSPHMM3s are superior to each of: First-Order
Left-to-Right Suprasegmental Hidden Markov Models (LTRSPHMM1s), Second-Order
Left-to-Right Suprasegmental Hidden Markov Models (LTRSPHMM2s), Third-Order
Left-to-Right Suprasegmental Hidden Markov Models (LTRSPHMM3s), First-Order
Circular Suprasegmental Hidden Markov Models (CSPHMM1s), and Second-Order
Circular Suprasegmental Hidden Markov Models (CSPHMM2s) in a shouted talking
environment. Using our collected speech database, average speaker
identification performance in a shouted talking environment based on
LTRSPHMM1s, LTRSPHMM2s, LTRSPHMM3s, CSPHMM1s, CSPHMM2s, and CSPHMM3s is 74.6%,
78.4%, 81.7%, 78.7%, 83.4%, and 85.8%, respectively. Speaker identification
performance that has been achieved based on CSPHMM3s is close to that attained
based on subjective assessment by human listeners."
"The paper presents the pch2csd project, focused on converting patches of
popular Clavia Nord Modular G2 synthesizer into code of Csound language. Now
discontinued, Nord Modular G2 left a lot of interesting patches for sound
synthesis and algorithmic composition. To give this heritage a new life, we
created our project with the hope for being able to simulate the original sound
and behavior of Nord Modular."
"In this paper, we aim at improving the performance of synthesized speech in
statistical parametric speech synthesis (SPSS) based on a generative
adversarial network (GAN). In particular, we propose a novel architecture
combining the traditional acoustic loss function and the GAN's discriminative
loss under a multi-task learning (MTL) framework. The mean squared error (MSE)
is usually used to estimate the parameters of deep neural networks, which only
considers the numerical difference between the raw audio and the synthesized
one. To mitigate this problem, we introduce the GAN as a second task to
determine if the input is a natural speech with specific conditions. In this
MTL framework, the MSE optimization improves the stability of GAN, and at the
same time GAN produces samples with a distribution closer to natural speech.
Listening tests show that the multi-task architecture can generate more natural
speech that satisfies human perception than the conventional methods."
"This paper presents algorithms for modulation-domain speech enhancement using
a Kalman filter. The algorithms are derived using two alternative statistical
models for the speech and noise spectral coefficients. The proposed models
incorporate the estimated dynamics of the spectral amplitudes of speech and
noise into the MMSE estimation of the amplitude spectrum of the clean speech.
Both models assume that the speech and noise are additive in the complex
domain. The difference between the two algorithms is that the the first
algorithm models only the spectral dynamics of the clean speech while the
second algorithm jointly models the spectral dynamics of both speech and noise.
In the first algorithm, a closed-form estimator is derived under the assumption
that speech amplitudes follow a form of generalized Gamma distribution and the
noise amplitudes follow Gaussian distribution. In the second algorithm, in
order to include the dynamics of noise amplitudes with that of speech
amplitudes, we propose a statistical ""Gaussring"" model that comprises a mixture
of Gaussians whose centres lie in a circle on the complex plane. The
performance of the proposed algorithms are evaluated using the perceptual
evaluation of speech quality (PESQ) measure and segmental SNR measure and shown
to give a consistent improvement over a wide range of SNRs when compared to
competitive algorithms."
"This paper proposes a new method for calculating joint-state posteriors of
mixed-audio features using deep neural networks to be used in factorial speech
processing models. The joint-state posterior information is required in
factorial models to perform joint-decoding. The novelty of this work is its
architecture which enables the network to infer joint-state posteriors from the
pairs of state posteriors of stereo features. This paper defines an objective
function to solve an underdetermined system of equations, which is used by the
network for extracting joint-state posteriors. It develops the required
expressions for fine-tuning the network in a unified way. The experiments
compare the proposed network decoding results to those of the vector Taylor
series method and show 2.3% absolute performance improvement in the monaural
speech separation and recognition challenge. This achievement is substantial
when we consider the simplicity of joint-state posterior extraction provided by
deep neural networks."
"This paper introduces a new score-informed method for the segmentation of
jingju a cappella singing phrase into syllables. The proposed method estimates
the most likely sequence of syllable boundaries given the estimated syllable
onset detection function (ODF) and its score. Throughout the paper, we first
examine the jingju syllables structure and propose a definition of the term
""syllable onset"". Then, we identify which are the challenges that jingju a
cappella singing poses. Further, we investigate how to improve the syllable ODF
estimation with convolutional neural networks (CNNs). We propose a novel CNN
architecture that allows to efficiently capture different time-frequency scales
for estimating syllable onsets. In addition, we propose using a score-informed
Viterbi algorithm -instead of thresholding the onset function-, because the
available musical knowledge we have (the score) can be used to inform the
Viterbi algorithm in order to overcome the identified challenges. The proposed
method outperforms the state-of-the-art in syllable segmentation for jingju a
cappella singing. We further provide an analysis of the segmentation errors
which points possible research directions."
"We approach the singing phrase audio to score matching problem by using
phonetic and duration information - with a focus on studying the jingju a
cappella singing case. We argue that, due to the existence of a basic melodic
contour for each mode in jingju music, only using melodic information (such as
pitch contour) will result in an ambiguous matching. This leads us to propose a
matching approach based on the use of phonetic and duration information.
Phonetic information is extracted with an acoustic model shaped with our data,
and duration information is considered with the Hidden Markov Models (HMMs)
variants we investigate. We build a model for each lyric path in our scores and
we achieve the matching by ranking the posterior probabilities of the decoded
most likely state sequences. Three acoustic models are investigated: (i)
convolutional neural networks (CNNs), (ii) deep neural networks (DNNs) and
(iii) Gaussian mixture models (GMMs). Also, two duration models are compared:
(i) hidden semi-Markov model (HSMM) and (ii) post-processor duration model.
Results show that CNNs perform better in our (small) audio dataset and also
that HSMM outperforms the post-processor duration model."
"Estimation of the location of sound sources is usually done using microphone
arrays. Such settings provide an environment where we know the difference
between the received signals among different microphones in the terms of phase
or attenuation, which enables localization of the sound sources. In our
solution we exploit the properties of the room transfer function in order to
localize a sound source inside a room with only one microphone. The shape of
the room and the position of the microphone are assumed to be known. The design
guidelines and limitations of the sensing matrix are given. Implementation is
based on the sparsity in the terms of voxels in a room that are occupied by a
source. What is especially interesting about our solution is that we provide
localization of the sound sources not only in the horizontal plane, but in the
terms of the 3D coordinates inside the room."
"Traditional intelligent fault diagnosis of rolling bearings work well only
under a common assumption that the labeled training data (source domain) and
unlabeled testing data (target domain) are drawn from the same distribution.
When the distribution changes, most fault diagnosis models need to be rebuilt
from scratch using newly recollected labeled training data. However, it is
expensive or impossible to annotate huge amount of training data to rebuild
such new model. Meanwhile, large amounts of labeled training data have not been
fully utilized yet, which is apparently a waste of resources. As one of the
important research directions of transfer learning, domain adaptation (DA)
typically aims at minimizing the differences between distributions of different
domains in order to minimize the cross-domain prediction error by taking full
advantage of information coming from both source and target domains. In this
paper, we present one of the first studies on unsupervised DA in the field of
fault diagnosis of rolling bearings under varying working conditions and a
novel diagnosis strategy based on unsupervised DA using subspace alignment (SA)
is proposed. After processed by unsupervised DA with SA, the distributions of
training data and testing data become close and the classifier trained on
training data can be used to classify the testing data. Experimental results on
the 60 domain adaptation diagnosis problems under varying working condition in
Case Western Reserve benchmark data and 12 domain adaptation diagnosis problems
under varying working conditions in our new data are given to demonstrate the
effectiveness of the proposed method. The proposed methods can effectively
distinguish not only bearing faults categories but also fault severities."
"An elastic ideal 2D propagation medium, i.e., a membrane, can be simulated by
models discretizing the wave equation on the time-space grid (finite difference
methods), or locally discretizing the solution of the wave equation (waveguide
meshes). The two approaches provide equivalent computational structures, and
introduce numerical dispersion that induces a misalignment of the modes from
their theoretical positions. Prior literature shows that dispersion can be
arbitrarily reduced by oversizing and oversampling the mesh, or by adpting
offline warping techniques. In this paper we propose to reduce numerical
dispersion by embedding warping elements, i.e., properly tuned allpass filters,
in the structure. The resulting model exhibits a significant reduction in
dispersion, and requires less computational resources than a regular mesh
structure having comparable accuracy."
"Music composition using digital audio sequence editors is increasingly
performed in a visual workspace where sound complexes are built from discrete
sound objects, called gestures that are arranged in time and space to generate
a continuous composition. The visual workspace, common to most industry
standard audio loop sequencing software, is premised on the arrangement of
gestures defined with geometric shape properties. Here, one aspect of fractal
set theory was validated using audio-frequency sets to evaluate self-affine
scaling behavior when new sound complexes are built through union and
intersection operations on discrete musical gestures. Results showed that
intersection of two sets revealed lower complexity compared with the union
operator, meaning that the intersection of two sound gestures is an almost
disjoint set, and in accord with formal logic. These results are also discussed
with reference to fuzzy sets, cellular automata, nanotechnology and
self-organization to further explore the link between sequenced notation and
complexity."
"Voiced segments of speech are assumed to be composed of non-stationary
acoustic objects which can be described as stationary response of a
non-stationary fundamental drive (FD) process and which are furthermore suited
to reconstruct the hidden FD by using a voice adapted (self-consistent)
part-tone decomposition of the speech signal. The universality and robustness
of human pitch perception encourages the reconstruction of a band-limited FD in
the frequency range of the pitch. The self-consistent decomposition of voiced
continuants generates several part-tones which can be confirmed to be
topologically equivalent to corresponding acoustic modes of the excitation on
the transmitter side. As topologically equivalent image of a glottal master
oscillator, the self-consistent FD is suited to serve as low frequency part of
the basic time-scale separation of auditive perception and to describe the
broadband voiced excitation as entrained (synchronized) and/or modulated
primary response. Being guided by the acoustic correlates of pitch and loudness
perception, the time-scale separation avoids the conventional assumption of
stationary excitation and represents the basic decoding step of an advanced
precision transmission protocol of self-consistent (voiced) acoustic objects.
The present study is focussed on the adaptation of the trajectories (contours)
of the centre filter frequency of the part-tones to the chirp of the glottal
master oscillator."
"We investigate the symmetric Kullback-Leibler (KL2) distance in speaker
clustering and its unreported effects for differently-sized feature matrices.
Speaker data is represented as Mel Frequency Cepstral Coefficient (MFCC)
vectors, and features are compared using the KL2 metric to form clusters of
speech segments for each speaker. We make two observations with respect to
clustering based on KL2: 1.) The accuracy of clustering is strongly dependent
on the absolute lengths of the speech segments and their extracted feature
vectors. 2.) The accuracy of the similarity measure strongly degrades with the
length of the shorter of the two speech segments. These effects of length can
be attributed to the measure of covariance used in KL2. We demonstrate an
empirical correction of this sample-size effect that increases clustering
accuracy. We draw parallels to two Vector Quantization-based (VQ) similarity
measures, one which exhibits an equivalent effect of sample size, and the
second being less influenced by it."
"We introduce a novel and inexpensive approach for the temporal alignment of
speech to highly imperfect transcripts from automatic speech recognition (ASR).
Transcripts are generated for extended lecture and presentation videos, which
in some cases feature more than 30 speakers with different accents, resulting
in highly varying transcription qualities. In our approach we detect a subset
of phonemes in the speech track, and align them to the sequence of phonemes
extracted from the transcript. We report on the results for 4 speech-transcript
sets ranging from 22 to 108 minutes. The alignment performance is promising,
showing a correct matching of phonemes within 10, 20, 30 second error margins
for more than 60%, 75%, 90% of text, respectively, on average."
"Speech recognition based on the syllable segment is discussed in this paper.
The principal search methods in space of states for the speech recognition
problem by segment-syllabic parameters trajectory synthesis are investigated.
Recognition as comparison the parameters trajectories in chosen speech units on
the sections of the segmented speech is realized. Some experimental results are
given and discussed."
"An integrated and automated smart structures approach for structural health
monitoring is presented, utilizing an array of piezoelectric transducers
attached to or embedded within the structure for both actuation and sensing.
The system actively interrogates the structure via broadband excitation of
multiple actuators across a desired frequency range. The structure's vibration
signature is then characterized by computing the transfer functions between
each actuator/sensor pair, and compared to the baseline signature. Experimental
results applying the system to local area damage detection in a MD Explorer
rotorcraft composite flexbeam are presented."
"A raga is a melodic structure with fixed notes and a set of rules
characterizing a certain mood endorsed through performance. By a vadi swar is
meant that note which plays the most significant role in expressing the raga. A
samvadi swar similarly is the second most significant note. However, the
determination of their significance has an element of subjectivity and hence we
are motivated to find some truths through an objective analysis. The paper
proposes a probabilistic method of note detection and demonstrates how the
relative frequency (relative number of occurrences of the pitch) of the more
important notes stabilize far more quickly than that of others. In addition, a
count for distinct transitory and similar looking non-transitory (fundamental)
frequency movements (but possibly embedding distinct emotions!) between the
notes is also taken depicting the varnalankars or musical ornaments decorating
the notes and note sequences as rendered by the artist. They reflect certain
structural properties of the ragas. Several case studies are presented."
"In this paper, a novel method of designing a codebook for noise robust
speaker identification purpose utilizing Genetic Algorithm has been proposed.
Wiener filter has been used to remove the background noises from the source
speech utterances. Speech features have been extracted using standard speech
parameterization method such as LPC, LPCC, RCC, MFCC, (delta)MFCC and
(delta)(delta) MFCC. For each of these techniques, the performance of the
proposed system has been compared. In this codebook design method, Genetic
Algorithm has the capability of getting global optimal result and hence
improves the quality of the codebook. Comparing with the NOIZEOUS speech
database, the experimental result shows that 79.62 percent accuracy has been
achieved."
"The paper offers a solution to the centuries-old puzzle - why the major
chords are perceived as happy and the minor chords as sad - based on the
information theory of emotions. A theory and a formula of musical emotions were
created. They define the sign and the amplitude of the utilitarian emotional
coloration of separate major and minor chords through relative pitches of
constituent sounds. Keywords: chord, major, minor, the formula of musical
emotions, the information theory of emotions."
"Digital pulse width modulation has been considered for high-fidelity and
high-efficiency audio amplifiers for several years. It has been shown that the
distortion can be reduced and the implementation of the system can be
simplified if the switching frequency is much higher than the Nyquist rate of
the modulating waveform. Hence, the input digital source is normally upsampled
to a higher frequency. It was also proved that converting uniform samples to
natural samples will decrease the harmonic distortion. Thus, in this paper, we
examine a new approach that combines upsampling, digital interpolation and
natural sampling conversion. This approach uses poly-phase implementation of
the digital interpolation filter and digital differentiators. We will show that
the structure consists of an FIR-type linear stage and a nonlinear stage. Some
spectral simulation results of a pulse width modulation system based on this
approach will also be presented. Finally, we will discuss the improvement of
the new approach over old algorithms."
"This paper introduces and motivates the use of hybrid robust feature
extraction technique for spoken language identification (LID) system. The
speech recognizers use a parametric form of a signal to get the most important
distinguishable features of speech signal for recognition task. In this paper
Mel-frequency cepstral coefficients (MFCC), Perceptual linear prediction
coefficients (PLP) along with two hybrid features are used for language
Identification. Two hybrid features, Bark Frequency Cepstral Coefficients
(BFCC) and Revised Perceptual Linear Prediction Coefficients (RPLP) were
obtained from combination of MFCC and PLP. Two different classifiers, Vector
Quantization (VQ) with Dynamic Time Warping (DTW) and Gaussian Mixture Model
(GMM) were used for classification. The experiment shows better identification
rate using hybrid feature extraction techniques compared to conventional
feature extraction methods.BFCC has shown better performance than MFCC with
both classifiers. RPLP along with GMM has shown best identification performance
among all feature extraction techniques."
"To improve the performance of speaker identification systems, an effective
and robust method is proposed to extract speech features, capable of operating
in noisy environment. Based on the time-frequency multi-resolution property of
wavelet transform, the input speech signal is decomposed into various frequency
channels. For capturing the characteristic of the signal, the Mel-Frequency
Cepstral Coefficients (MFCCs) of the wavelet channels are calculated. Hidden
Markov Models (HMMs) were used for the recognition stage as they give better
recognition for the speaker's features than Dynamic Time Warping (DTW).
Comparison of the proposed approach with the MFCCs conventional feature
extraction method shows that the proposed method not only effectively reduces
the influence of noise, but also improves recognition. A recognition rate of
99.3% was obtained using the proposed feature extraction technique compared to
98.7% using the MFCCs. When the test patterns were corrupted by additive white
Gaussian noise with 20 dB S/N ratio, the recognition rate was 97.3% using the
proposed method compared to 93.3% using the MFCCs."
"An important problem to be solved in modeling head-related impulse responses
(HRIRs) is how to individualize HRIRs so that they are suitable for a listener.
We modeled the entire magnitude head-related transfer functions (HRTFs), in
frequency domain, for sound sources on horizontal plane of 37 subjects using
principal components analysis (PCA). The individual magnitude HRTFs could be
modeled adequately well by a linear combination of only ten orthonormal basis
functions. The goal of this research was to establish multiple linear
regression (MLR) between weights of basis functions obtained from PCA and fewer
anthropometric measurements in order to individualize a given listener's HRTFs
with his or her own anthropomety. We proposed here an improved
individualization method based on MLR of weights of basis functions by
utilizing 8 chosen out of 27 anthropometric measurements. Our objective
experiments' results show a superior performance than that of our previous work
on individualizing minimum phase HRIRs and also better than similar research.
The proposed individualization method shows that the individualized magnitude
HRTFs could approximated well the the original ones with small error. Moving
sound employing the reconstructed HRIRs could be perceived as if it was moving
around the horizontal plane."
"This paper describes an effective unsupervised speaker indexing approach. We
suggest a two stage algorithm to speed-up the state-of-the-art algorithm based
on the Bayesian Information Criterion (BIC). In the first stage of the merging
process a computationally cheap method based on the vector quantization (VQ) is
used. Then in the second stage a more computational expensive technique based
on the BIC is applied. In the speaker indexing task a turning parameter or a
threshold is used. We suggest an on-line procedure to define the value of a
turning parameter without using development data. The results are evaluated
using 10 hours of audio data."
"In certain applications it is useful to fit multinomial distributions to
observed data with a penalty term that encourages sparsity. For example, in
probabilistic latent audio source decomposition one may wish to encode the
assumption that only a few latent sources are active at any given time. The
standard heuristic of applying an L1 penalty is not an option when fitting the
parameters to a multinomial distribution, which are constrained to sum to 1. An
alternative is to use a penalty term that encourages low-entropy solutions,
which corresponds to maximum a posteriori (MAP) parameter estimation with an
entropic prior. The lack of conjugacy between the entropic prior and the
multinomial distribution complicates this approach. In this report I propose a
simple iterative algorithm for MAP estimation of multinomial distributions with
sparsity-inducing entropic priors."
"Music is a string of some of the notes out of 12 notes (Sa, Komal_re, Re,
Komal_ga, Ga, Ma, Kari_ma, Pa, Komal_dha, Dha, Komal_ni, Ni) and their
harmonics. Each note corresponds to a particular frequency. When such strings
are encoded to form discrete sequences, different frequencies present in the
music corresponds to different amplitude levels (value) of the discrete
sequence. Initially, a class of discrete sequences has been generated using
logistic map. All these discrete sequences have at most n-different amplitude
levels (value) (depending on the particular raga). Without loss of generality,
we have chosen two discrete sequences of two types of Indian raga viz. Bhairabi
and Bhupali having same number of amplitude levels to obtain/search close
relatives from the class. The relative / closeness can be assured through
correlation coefficient.The search is unbiased, random and non-adaptive. The
obtained string is that which maximally resembles the given two sequences. The
same can be thought of as a music composition of the given two strings. It is
to be noted that all these string are fractal string which can be persuaded by
fractal dimension."
"We overview current problems of audio retrieval and time-series subsequence
matching. We discuss the usage of subsequence matching approaches in audio data
processing, especially in automatic speech recognition (ASR) area and we aim at
improving performance of the retrieval process. To overcome the problems known
from the time-series area like the occurrence of implementation bias and data
bias we present a Subsequence Matching Framework as a tool for fast
prototyping, building, and testing similarity search subsequence matching
applications. The framework is build on top of MESSIF (Metric Similarity Search
Implementation Framework) and thus the subsequence matching algorithms can
exploit advanced similarity indexes in order to significantly increase their
query processing performance. To prove our concept we provide a design of
query-by-example spoken term detection type of application with the usage of
phonetic posteriograms and subsequence matching approach."
"Phonology typically describes speech in terms of discrete signs like
features. The field of intonational phonology uses discrete accents to describe
intonation and prosody. But, are such representations useful? The results of
mimicry experiments indicate that discrete signs are not a useful
representation of the shape of intonation contours. Human behaviour seems to be
better represented by a attractors where memory retains substantial fine detail
about an utterance. There is no evidence that discrete abstract representations
that might be formed that have an effect on the speech that is subsequently
produced. This paper also discusses conditions under which a discrete phonology
can arise from an attractor model and why - for intonation - attractors can be
inferred without the implying a discrete phonology."
"This paper proposes a voice morphing system for people suffering from
Laryngectomy, which is the surgical removal of all or part of the larynx or the
voice box, particularly performed in cases of laryngeal cancer. A primitive
method of achieving voice morphing is by extracting the source's vocal
coefficients and then converting them into the target speaker's vocal
parameters. In this paper, we deploy Gaussian Mixture Models (GMM) for mapping
the coefficients from source to destination. However, the use of the
traditional/conventional GMM-based mapping approach results in the problem of
over-smoothening of the converted voice. Thus, we hereby propose a unique
method to perform efficient voice morphing and conversion based on GMM,which
overcomes the traditional-method effects of over-smoothening. It uses a
technique of glottal waveform separation and prediction of excitations and
hence the result shows that not only over-smoothening is eliminated but also
the transformed vocal tract parameters match with the target. Moreover, the
synthesized speech thus obtained is found to be of a sufficiently high quality.
Thus, voice morphing based on a unique GMM approach has been proposed and also
critically evaluated based on various subjective and objective evaluation
parameters. Further, an application of voice morphing for Laryngectomees which
deploys this unique approach has been recommended by this paper."
"We introduce a free and open dataset of 7690 audio clips sampled from the
field-recording tag in the Freesound audio archive. The dataset is designed for
use in research related to data mining in audio archives of field recordings /
soundscapes. Audio is standardised, and audio and metadata are Creative Commons
licensed. We describe the data preparation process, characterise the dataset
descriptively, and illustrate its use through an auto-tagging experiment."
"This paper investigates a non-negative matrix factorization (NMF)-based
approach to the semi-supervised single-channel speech enhancement problem where
only non-stationary additive noise signals are given. The proposed method
relies on sinusoidal model of speech production which is integrated inside NMF
framework using linear constraints on dictionary atoms. This method is further
developed to regularize harmonic amplitudes. Simple multiplicative algorithms
are presented. The experimental evaluation was made on TIMIT corpus mixed with
various types of noise. It has been shown that the proposed method outperforms
some of the state-of-the-art noise suppression techniques in terms of
signal-to-noise ratio."
"This article introduces an effective generalization of the polar flavor of
the Fourier Theorem based on a new method of analysis. Under the premises of
the new theory an ample class of functions become viable as bases, with the
further advantage of using the same basis for analysis and reconstruction. In
fact other tools, like the wavelets, admit specially built nonorthogonal bases
but require different bases for analysis and reconstruction (biorthogonal and
dual bases) and vectorial coordinates; this renders those systems unintuitive
and computing intensive. As an example of the advantages of the new
generalization of the Fourier Theorem, this paper introduces a novel method for
the synthesis that is based on frequency-phase series of square waves (the
equivalent of the polar Fourier Theorem but for nonorthogonal bases). The
resulting synthesizer is very efficient needing only few components, frugal in
terms of computing needs, and viable for many applications."
"The new formulas, which determine sign and amplitude of utilitarian emotions,
are proposed on the basis of the information theory of emotions. In area of
perception of musical chords the force of emotions depends on the relative
pitch of sounds of major and minor chords.
  Is advanced hypothesis that in the perception of a musical chord in the
psyche caused by the subject value of some objective function L. This function
is expressed directly through the proportion of the pitch of chord. Major
chords are expressed as the straight proportions, which generate idea about an
increase in the objective function (L>1) and are caused positive utilitarian
emotions. Minor chords are expressed as the inverse proportion, which generate
idea about the decrease of objective function (L<1) and are caused negative
utilitarian emotions.
  The formula of musical emotions is advanced: Pwe = log(L) =
(1/M)*log(n1*n2*n3* ... *nM), where M is a quantity of voices of chord, ni -
integer number (or reciprocal fraction) from the pitch proportion, which
corresponds to the i-th voice of chord.
  Confined experimental check is produced. The limits of the applicability of
the formula of musical emotions are investigated.
  Keywords: sound, music, chord, major, minor, emotions, the formula of musical
emotions, the information theory of emotions."
"This paper presents a theory by which idealized models of auditory receptive
fields can be derived in a principled axiomatic manner, from a set of
structural properties to enable invariance of receptive field responses under
natural sound transformations and ensure internal consistency between
spectro-temporal receptive fields at different temporal and spectral scales.
  For defining a time-frequency transformation of a purely temporal sound
signal, it is shown that the framework allows for a new way of deriving the
Gabor and Gammatone filters as well as a novel family of generalized Gammatone
filters, with additional degrees of freedom to obtain different trade-offs
between the spectral selectivity and the temporal delay of time-causal temporal
window functions.
  When applied to the definition of a second-layer of receptive fields from a
spectrogram, it is shown that the framework leads to two canonical families of
spectro-temporal receptive fields, in terms of spectro-temporal derivatives of
either spectro-temporal Gaussian kernels for non-causal time or the combination
of a time-causal generalized Gammatone filter over the temporal domain and a
Gaussian filter over the logspectral domain. For each filter family, the
spectro-temporal receptive fields can be either separable over the
time-frequency domain or be adapted to local glissando transformations that
represent variations in logarithmic frequencies over time. Within each domain
of either non-causal or time-causal time, these receptive field families are
derived by uniqueness from the assumptions.
  It is demonstrated how the presented framework allows for computation of
basic auditory features for audio processing and that it leads to predictions
about auditory receptive fields with good qualitative similarity to biological
receptive fields measured in the inferior colliculus (ICC) and primary auditory
cortex (A1) of mammals."
"This paper addresses the problem of ad hoc microphone array calibration where
only partial information about the distances between microphones is available.
We construct a matrix consisting of the pairwise distances and propose to
estimate the missing entries based on a novel Euclidean distance matrix
completion algorithm by alternative low-rank matrix completion and projection
onto the Euclidean distance space. This approach confines the recovered matrix
to the EDM cone at each iteration of the matrix completion algorithm. The
theoretical guarantees of the calibration performance are obtained considering
the random and locally structured missing entries as well as the measurement
noise on the known distances. This study elucidates the links between the
calibration error and the number of microphones along with the noise level and
the ratio of missing distances. Thorough experiments on real data recordings
and simulated setups are conducted to demonstrate these theoretical insights. A
significant improvement is achieved by the proposed Euclidean distance matrix
completion algorithm over the state-of-the-art techniques for ad hoc microphone
array calibration."
"We present a formal language for assigning pitches to strings for fingered
multi-string instruments, particularly the six-string guitar. Given the
instrument's tuning (the strings' open pitches) and the compass of the fingers
of the hand stopping the strings, the formalism yields a framework for
simultaneously optimizing three things: the mapping of pitches to strings, the
choice of instrument tuning, and the key of the composition. Final optimization
relies on heuristics idiomatic to the tuning, the particular musical style, and
the performer's proficiency."
"Feature extraction plays an important role as a front-end processing block in
speaker identification (SI) process. Most of the SI systems utilize like
Mel-Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP),
Linear Predictive Cepstral Coefficients (LPCC), as a feature for representing
speech signal. Their derivations are based on short term processing of speech
signal and they try to capture the vocal tract information ignoring the
contribution from the vocal cord. Vocal cord cues are equally important in SI
context, as the information like pitch frequency, phase in the residual signal,
etc could convey important speaker specific attributes and are complementary to
the information contained in spectral feature sets. In this paper we propose a
novel feature set extracted from the residual signal of LP modeling.
Higher-order statistical moments are used here to find the nonlinear
relationship in residual signal. To get the advantages of complementarity vocal
cord based decision score is fused with the vocal tract based score. The
experimental results on two public databases show that fused mode system
outperforms single spectral features."
"For several years now, the ITU-T's Perceptual Evaluation of Speech Quality
(PESQ) has been the reference for objective speech quality assessment. It is
widely deployed in commercial QoE measurement products, and it has been well
studied in the literature. While PESQ does provide reasonably good correlation
with subjective scores for VoIP applications, the algorithm itself is not
usable in a real-time context, since it requires a reference signal, which is
usually not available in normal conditions. In this paper we provide an
alternative technique for estimating PESQ scores in a single-sided fashion,
based on the Pseudo Subjective Quality Assessment (PSQA) technique."
"This paper presents a new approach for classification of dysfluent and fluent
speech using Mel-Frequency Cepstral Coefficient (MFCC). The speech is fluent
when person's speech flows easily and smoothly. Sounds combine into syllable,
syllables mix together into words and words link into sentences with little
effort. When someone's speech is dysfluent, it is irregular and does not flow
effortlessly. Therefore, a dysfluency is a break in the smooth, meaningful flow
of speech. Stuttering is one such disorder in which the fluent flow of speech
is disrupted by occurrences of dysfluencies such as repetitions, prolongations,
interjections and so on. In this work we have considered three types of
dysfluencies such as repetition, prolongation and interjection to characterize
dysfluent speech. After obtaining dysfluent and fluent speech, the speech
signals are analyzed in order to extract MFCC features. The k-Nearest Neighbor
(k-NN) and Support Vector Machine (SVM) classifiers are used to classify the
speech as dysfluent and fluent speech. The 80% of the data is used for training
and 20% for testing. The average accuracy of 86.67% and 93.34% is obtained for
dysfluent and fluent speech respectively."
"The experimental two-microphone transfer function method (TMTF) is adapted to
the numerical framework to compute the radiation and input impedances of
three-dimensional vocal tracts of elliptical cross section. In its simplest
version, the TMTF method only requires measuring the acoustic pressure at two
points in an impedance duct and the postprocessing of the corresponding
transfer function. However, some considerations are to be taken into account
when using the TMTF method in the numerical context, which constitute the main
objective of this paper. In particular, the importance of including absorption
at the impedance duct walls to avoid lengthy numerical simulations is discussed
and analytical complex axial wave numbers for elliptical ducts are derived for
this purpose. It is also shown how the plane wave restriction of the TMTF
method can be circumvented to some extent by appropriate location of the
virtual microphones, thus extending the method frequency range of validity.
Virtual microphone spacing is also discussed on the basis of the so called
singularity factor. Numerical examples include the computation of the radiation
impedance of vowels /a/, /i/ and /u/ and the input impedance of vowel /a/, for
simplified vocal tracts of circular and elliptical cross sections."
"The time domain waveform of a speech signal carries all of the auditory
information. From the phonological point of view, it little can be said on the
basis of the waveform itself. However, past research in mathematics, acoustics,
and speech technology have provided many methods for converting data that can
be considered as information if interpreted correctly. In order to find some
statistically relevant information from incoming data, it is important to have
mechanisms for reducing the information of each segment in the audio signal
into a relatively small number of parameters, or features. These features
should describe each segment in such a characteristic way that other similar
segments can be grouped together by comparing their features. There are
enormous interesting and exceptional ways to describe the speech signal in
terms of parameters. Though, they all have their strengths and weaknesses, we
have presented some of the most used methods with their importance."
"Form about four decades human beings have been dreaming of an intelligent
machine which can master the natural speech. In its simplest form, this machine
should consist of two subsystems, namely automatic speech recognition (ASR) and
speech understanding (SU). The goal of ASR is to transcribe natural speech
while SU is to understand the meaning of the transcription. Recognizing and
understanding a spoken sentence is obviously a knowledge-intensive process,
which must take into account all variable information about the speech
communication process, from acoustics to semantics and pragmatics. While
developing an Automatic Speech Recognition System, it is observed that some
adverse conditions degrade the performance of the Speech Recognition System. In
this contribution, speech enhancement system is introduced for enhancing speech
signals corrupted by additive noise and improving the performance of Automatic
Speech Recognizers in noisy conditions. Automatic speech recognition
experiments show that replacing noisy speech signals by the corresponding
enhanced speech signals leads to an improvement in the recognition accuracies.
The amount of improvement varies with the type of the corrupting noise."
"Acoustical mismatch among training and testing phases degrades outstandingly
speech recognition results. This problem has limited the development of
real-world nonspecific applications, as testing conditions are highly variant
or even unpredictable during the training process. Therefore the background
noise has to be removed from the noisy speech signal to increase the signal
intelligibility and to reduce the listener fatigue. Enhancement techniques
applied, as pre-processing stages; to the systems remarkably improve
recognition results. In this paper, a novel approach is used to enhance the
perceived quality of the speech signal when the additive noise cannot be
directly controlled. Instead of controlling the background noise, we propose to
reinforce the speech signal so that it can be heard more clearly in noisy
environments. The subjective evaluation shows that the proposed method improves
perceptual quality of speech in various noisy environments. As in some cases
speaking may be more convenient than typing, even for rapid typists: many
mathematical symbols are missing from the keyboard but can be easily spoken and
recognized. Therefore, the proposed system can be used in an application
designed for mathematical symbol recognition (especially symbols not available
on the keyboard) in schools."
"Speech is a natural form of communication for human beings, and computers
with the ability to understand speech and speak with a human voice are expected
to contribute to the development of more natural man-machine interfaces.
Computers with this kind of ability are gradually becoming a reality, through
the evolution of speech recognition technologies. Speech is being an important
mode of interaction with computers. In this paper Feature extraction is
implemented using well-known Mel-Frequency Cepstral Coefficients (MFCC).Pattern
matching is done using Dynamic time warping (DTW) algorithm."
"We formulate a phonetic-prosodic space based on attributes as perceptual
observables, rather than articulatory specifications. We propose an alphabet as
markers in the phonetic subspace, aiming for a resolution sufficient to support
recognition of all spoken languages. The prosodic subspace is made up of
directly measurable physical variables. With the proposed alphabet, traditional
diphthongs naturally generalize to a broader class of language-neutral
phonotactic constraints, indicating a correlation structure similar to that of
the traditional sonority-based syllable. We define a stochastic structure on
the phone strings based on this diphthongal constraint, and show how a specific
spoken language can be defined as a specific set of probability distributions
of this stochastic structure. Furthermore, phonological variations within a
spoken language can be modeled as varying probability distributions restricted
to the phonetic subspace, conditioned on different values in the prosodic
subspace."
"We address the problem of sound representation and classification and present
results of a comparative study in the context of a domestic robotic scenario. A
dataset of sounds was recorded in realistic conditions (background noise,
presence of several sound sources, reverberations, etc.) using the humanoid
robot NAO. An extended benchmark is carried out to test a variety of
representations combined with several classifiers. We provide results obtained
with the annotated dataset and we assess the methods quantitatively on the
basis of their classification scores, computation times and memory
requirements. The annotated dataset is publicly available at
https://team.inria.fr/perception/nard/."
"This paper addresses the challenging scenario for the distant-talking control
of a music playback device, a common portable speaker with four small
loudspeakers in close proximity to one microphone. The user controls the device
through voice, where the speech-to-music ratio can be as low as -30 dB during
music playback. We propose a speech enhancement front-end that relies on known
robust methods for echo cancellation, double-talk detection, and noise
suppression, as well as a novel adaptive quasi-binary mask that is well suited
for speech recognition. The optimization of the system is then formulated as a
large scale nonlinear programming problem where the recognition rate is
maximized and the optimal values for the system parameters are found through a
genetic algorithm. We validate our methodology by testing over the TIMIT
database for different music playback levels and noise types. Finally, we show
that the proposed front-end allows a natural interaction with the device for
limited-vocabulary voice commands."
"The trend in media consumption towards streaming and portability offers new
challenges and opportunities for signal processing in audio and acoustics. The
most significant embodiment of this trend is that most music consumption now
happens on-the-go which has recently led to an explosion in headphone sales and
small portable speakers. In particular, premium headphones offer a gateway for
a younger generation to experience high quality sound. Additionally, through
technologies incorporating head-related transfer functions headphones can also
offer unique new experiences in gaming, augmented reality, and surround sound
listening. Home audio has also seen a transition to smaller sound systems in
the form of sound bars. This speaker configuration offers many exciting
challenges for surround sound reproduction which has traditionally used five
speakers surrounding the listener. Furthermore, modern home entertainment
systems offer more than just content delivery; users now expect wireless and
connected smart devices with video conferencing, gaming, and other interactive
capabilities. With this comes challenges for voice interaction at a distance
and in demanding conditions, e.g., during content playback, and opportunities
for new smart interactive experiences based on awareness of environment and
user biometrics."
"Automatic species classification of birds from their sound is a computational
tool of increasing importance in ecology, conservation monitoring and vocal
communication studies. To make classification useful in practice, it is crucial
to improve its accuracy while ensuring that it can run at big data scales. Many
approaches use acoustic measures based on spectrogram-type data, such as the
Mel-frequency cepstral coefficient (MFCC) features which represent a
manually-designed summary of spectral information. However, recent work in
machine learning has demonstrated that features learnt automatically from data
can often outperform manually-designed feature transforms. Feature learning can
be performed at large scale and ""unsupervised"", meaning it requires no manual
data labelling, yet it can improve performance on ""supervised"" tasks such as
classification. In this work we introduce a technique for feature learning from
large volumes of bird sound recordings, inspired by techniques that have proven
useful in other domains. We experimentally compare twelve different feature
representations derived from the Mel spectrum (of which six use this
technique), using four large and diverse databases of bird vocalisations, with
a random forest classifier. We demonstrate that MFCCs are of limited power in
this context, leading to worse performance than the raw Mel spectral data.
Conversely, we demonstrate that unsupervised feature learning provides a
substantial boost over MFCCs and Mel spectra without adding computational
complexity after the model has been trained. The boost is particularly notable
for single-label classification tasks at large scale. The spectro-temporal
activations learned through our procedure resemble spectro-temporal receptive
fields calculated from avian primary auditory forebrain."
"The potential use of non-linear speech features has not been investigated for
music analysis although other commonly used speech features like Mel Frequency
Ceptral Coefficients (MFCC) and pitch have been used extensively. In this
paper, we assume an audio signal to be a sum of modulated sinusoidal and then
use the energy separation algorithm to decompose the audio into amplitude and
frequency modulation components using the non-linear Teager-Kaiser energy
operator. We first identify the distribution of these non-linear features for
music only and voice only segments in the audio signal in different Mel spaced
frequency bands and show that they have the ability to discriminate. The
proposed method based on Kullback-Leibler divergence measure is evaluated using
a set of Indian classical songs from three different artists. Experimental
results show that the discrimination ability is evident in certain low and mid
frequency bands (200 - 1500 Hz)."
"Recognition of speech, and in particular the ability to generalize and learn
from small sets of labelled examples like humans do, depends on an appropriate
representation of the acoustic input. We formulate the problem of finding
robust speech features for supervised learning with small sample complexity as
a problem of learning representations of the signal that are maximally
invariant to intraclass transformations and deformations. We propose an
extension of a theory for unsupervised learning of invariant visual
representations to the auditory domain and empirically evaluate its validity
for voiced speech sound classification. Our version of the theory requires the
memory-based, unsupervised storage of acoustic templates -- such as specific
phones or words -- together with all the transformations of each that normally
occur. A quasi-invariant representation for a speech segment can be obtained by
projecting it to each template orbit, i.e., the set of transformed signals, and
computing the associated one-dimensional empirical probability distributions.
The computations can be performed by modules of filtering and pooling, and
extended to hierarchical architectures. In this paper, we apply a single-layer,
multicomponent representation for phonemes and demonstrate improved accuracy
and decreased sample complexity for vowel classification compared to standard
spectral, cepstral and perceptual features."
"In late 2011, Fado was elevated to the oral and intangible heritage of
humanity by UNESCO. This study aims to develop a tool for automatic detection
of Fado music based on the audio signal. To do this, frequency spectrum-related
characteristics were captured form the audio signal: in addition to the Mel
Frequency Cepstral Coefficients (MFCCs) and the energy of the signal, the
signal was further analysed in two frequency ranges, providing additional
information. Tests were run both in a 10-fold cross-validation setup (97.6%
accuracy), and in a traditional train/test setup (95.8% accuracy). The good
results reflect the fact that Fado is a very distinctive musical style."
"Several speaker identification systems are giving good performance with clean
speech but are affected by the degradations introduced by noisy audio
conditions. To deal with this problem, we investigate the use of complementary
information at different levels for computing a combined match score for the
unknown speaker. In this work, we observe the effect of two supervised machine
learning approaches including support vectors machines (SVM) and na\""ive bayes
(NB). We define two feature vector sets based on mel frequency cepstral
coefficients (MFCC) and relative spectral perceptual linear predictive
coefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture
Model (GMM). Several ways of combining these information sources give
significant improvements in a text-independent speaker identification task
using a very large telephone degraded NTIMIT database."
"Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used
speech features in most speech and speaker recognition applications. In this
paper, we study the effect of resampling a speech signal on these speech
features. We first derive a relationship between the MFCC param- eters of the
resampled speech and the MFCC parameters of the original speech. We propose six
methods of calculating the MFCC parameters of downsampled speech by
transforming the Mel filter bank used to com- pute MFCC of the original speech.
We then experimentally compute the MFCC parameters of the down sampled speech
using the proposed meth- ods and compute the Pearson coefficient between the
MFCC parameters of the downsampled speech and that of the original speech to
identify the most effective choice of Mel-filter band that enables the computed
MFCC of the resampled speech to be as close as possible to the original speech
sample MFCC."
"An efficient, and intuitive algorithm is presented for the identification of
speakers from a long dataset (like YouTube long discussion, Cocktail party
recorded audio or video).The goal of automatic speaker identification is to
identify the number of different speakers and prepare a model for that speaker
by extraction, characterization and speaker-specific information contained in
the speech signal. It has many diverse application specially in the field of
Surveillance, Immigrations at Airport, cyber security, transcription in
multi-source of similar sound source, where it is difficult to assign
transcription arbitrary. The most commonly speech parametrization used in
speaker verification, K-mean, cepstral analysis, is detailed. Gaussian mixture
modeling, which is the speaker modeling technique is then explained. Gaussian
mixture models (GMM), perhaps the most robust machine learning algorithm has
been introduced examine and judge carefully speaker identification in text
independent. The application or employment of Gaussian mixture models for
monitoring & Analysing speaker identity is encouraged by the familiarity,
awareness, or understanding gained through experience that Gaussian spectrum
depict the characteristics of speaker's spectral conformational pattern and
remarkable ability of GMM to construct capricious densities after that we
illustrate 'Expectation maximization' an iterative algorithm which takes some
arbitrary value in initial estimation and carry on the iterative process until
the convergence of value is observed,so by doing various number of experiments
we are able to obtain 79 ~ 82% of identification rate using Vector quantization
and 85 ~ 92.6% of identification rate using GMM modeling by Expectation
maximization parameter estimation depending on variation of parameter."
"In this article we present an account of the state-of-the-art in acoustic
scene classification (ASC), the task of classifying environments from the
sounds they produce. Starting from a historical review of previous research in
this area, we define a general framework for ASC and present different imple-
mentations of its components. We then describe a range of different algorithms
submitted for a data challenge that was held to provide a general and fair
benchmark for ASC techniques. The dataset recorded for this purpose is
presented, along with the performance metrics that are used to evaluate the
algorithms and statistical significance tests to compare the submitted methods.
We use a baseline method that employs MFCCS, GMMS and a maximum likelihood
criterion as a benchmark, and only find sufficient evidence to conclude that
three algorithms significantly outperform it. We also evaluate the human
classification accuracy in performing a similar classification task. The best
performing algorithm achieves a mean accuracy that matches the median accuracy
obtained by humans, and common pairs of classes are misclassified by both
computers and humans. However, all acoustic scenes are correctly classified by
at least some individuals, while there are scenes that are misclassified by all
algorithms."
"Automatic image tagging has been a long standing problem, it mainly relies on
image recognition techniques of which the accuracy is still not satisfying.
This paper attempts to explore out-of-band sensing base on the mobile phone to
sense the people in a picture while the picture is being taken and create name
tags on-the-fly. The major challenges pertain to two aspects - ""Who"" and
""Which"". (1) ""Who"": discriminating people who are in the picture from those
that are not; (2) ""Which"": correlating each name tag with its corresponding
people in the picture. We propose an accurate acoustic scheme applying on the
mobile phones, which leverages the Doppler effect of sound wave to address
these two challenges. As a proof of concept, we implement the scheme on 7
android phones and take pictures in various real-life scenarios with people
positioning in different ways. Extensive experiments show that the accuracy of
tag correlation is above 85% within 3m for picturing."
"The ""bag-of-frames"" approach (BOF), which encodes audio signals as the
long-term statistical distribution of short-term spectral features, is commonly
regarded as an effective and sufficient way to represent environmental sound
recordings (soundscapes) since its introduction in an influential 2007 article.
The present paper describes a concep-tual replication of this seminal article
using several new soundscape datasets, with results strongly questioning the
adequacy of the BOF approach for the task. We show that the good accuracy
originally re-ported with BOF likely result from a particularly thankful
dataset with low within-class variability, and that for more realistic
datasets, BOF in fact does not perform significantly better than a mere
one-point av-erage of the signal's features. Soundscape modeling, therefore,
may not be the closed case it was once thought to be. Progress, we ar-gue,
could lie in reconsidering the problem of considering individual acoustical
events within each soundscape."
"Notes, ornaments and intervals are examples of basic musical elements. Their
representation as discrete-time digital audio plays a central role in software
for music creation and design. Nevertheless, there is no systematic relation,
in analytical terms, of these musical elements to the sonic samples. Such a
compendium enables scientific experiments in precise and trustful ways, among
educational and artistic uses. This paper presents a comprehensive description
of music in digital audio, within an unified approach. Musical elements, like
pitch, duration and timbre are expressed by equations on sample level. This
quantitatively relates characteristics of the discrete-time signal to musical
qualities. Internal variations, e.g. tremolos, vibratos and spectral
fluctuations, are also considered to operate within a note. Moreover, the
generation of musical structures such as rhythmic meter, pitch intervals and
cycles, are attained canonically with notes. The availability of these
resources in scripts is provided in public domain within the \massa\ toolbox -
Music and Audio in Sequences and Samples. Authors observe that the
implementation of sample-domain analytical results as open source can encourage
concise research. As further illustrated in the paper, \massa\ has already been
employed by users for diverse purposes, including acoustics experimentation,
art and education. The efficacy of these physical descriptions was confirmed by
the synthesis of small musical pieces. As shown, it is possible to synthesize
whole albums through collage of scripts and parametrization."
"In this report we describe an ongoing line of research for solving
single-channel source separation problems. Many monaural signal decomposition
techniques proposed in the literature operate on a feature space consisting of
a time-frequency representation of the input data. A challenge faced by these
approaches is to effectively exploit the temporal dependencies of the signals
at scales larger than the duration of a time-frame. In this work we propose to
tackle this problem by modeling the signals using a time-frequency
representation with multiple temporal resolutions. The proposed representation
consists of a pyramid of wavelet scattering operators, which generalizes
Constant Q Transforms (CQT) with extra layers of convolution and complex
modulus. We first show that learning standard models with this multi-resolution
setting improves source separation results over fixed-resolution methods. As
study case, we use Non-Negative Matrix Factorizations (NMF) that has been
widely considered in many audio application. Then, we investigate the inclusion
of the proposed multi-resolution setting into a discriminative training regime.
We discuss several alternatives using different deep neural network
architectures."
"An algorithm involving Mel-Frequency Cepstral Coefficients (MFCCs) is
provided to perform signal feature extraction for the task of speaker accent
recognition. Then different classifiers are compared based on the MFCC feature.
For each signal, the mean vector of MFCC matrix is used as an input vector for
pattern recognition. A sample of 330 signals, containing 165 US voice and 165
non-US voice, is analyzed. By comparison, k-nearest neighbors yield the highest
average test accuracy, after using a cross-validation of size 500, and least
time being used in the computation"
"The application of Compressive sensing approach to the speech and musical
signals is considered in this paper. Compressive sensing (CS) is a new approach
to the signal sampling that allows signal reconstruction from a small set of
randomly acquired samples. This method is developed for the signals that
exhibit the sparsity in a certain domain. Here we have observed two sparsity
domains: discrete Fourier and discrete cosine transform domain. Furthermore,
two different types of audio signals are analyzed in terms of sparsity and CS
performance - musical and speech signals. Comparative analysis of the CS
reconstruction using different number of signal samples is performed in the two
domains of sparsity. It is shown that the CS can be successfully applied to
both, musical and speech signals, but the speech signals are more demanding in
terms of the number of observations. Also, our results show that discrete
cosine transform domain allows better reconstruction using lower number of
observations, compared to the Fourier transform domain, for both types of
signals."
"An acoustic reverberator consisting of a network of delay lines connected via
scattering junctions is proposed. All parameters of the reverberator are
derived from physical properties of the enclosure it simulates. It allows for
simulation of unequal and frequency-dependent wall absorption, as well as
directional sources and microphones. The reverberator renders the first-order
reflections exactly, while making progressively coarser approximations of
higher-order reflections. The rate of energy decay is close to that obtained
with the image method (IM) and consistent with the predictions of Sabine and
Eyring equations. The time evolution of the normalized echo density, which was
previously shown to be correlated with the perceived texture of reverberation,
is also close to that of IM. However, its computational complexity is one to
two orders of magnitude lower, comparable to the computational complexity of a
feedback delay network (FDN), and its memory requirements are negligible."
"Audio fingerprinting, also named as audio hashing, has been well-known as a
powerful technique to perform audio identification and synchronization. It
basically involves two major steps: fingerprint (voice pattern) design and
matching search. While the first step concerns the derivation of a robust and
compact audio signature, the second step usually requires knowledge about
database and quick-search algorithms. Though this technique offers a wide range
of real-world applications, to the best of the authors' knowledge, a
comprehensive survey of existing algorithms appeared more than eight years ago.
Thus, in this paper, we present a more up-to-date review and, for emphasizing
on the audio signal processing aspect, we focus our state-of-the-art survey on
the fingerprint design step for which various audio features and their
tractable statistical models are discussed."
"In this paper, we present a latent variable (LV) framework to identify all
the speakers and their keywords given a multi-speaker mixture signal. We
introduce two separate LVs to denote active speakers and the keywords uttered.
The dependency of a spoken keyword on the speaker is modeled through a
conditional probability mass function. The distribution of the mixture signal
is expressed in terms of the LV mass functions and speaker-specific-keyword
models. The proposed framework admits stochastic models, representing the
probability density function of the observation vectors given that a particular
speaker uttered a specific keyword, as speaker-specific-keyword models. The LV
mass functions are estimated in a Maximum Likelihood framework using the
Expectation Maximization (EM) algorithm. The active speakers and their keywords
are detected as modes of the joint distribution of the two LVs. In mixture
signals, containing two speakers uttering the keywords simultaneously, the
proposed framework achieves an accuracy of 82% for detecting both the speakers
and their respective keywords, using Student's-t mixture models as
speaker-specific-keyword models."
"Dysarthria is malfunctioning of motor speech caused by faintness in the human
nervous system. It is characterized by the slurred speech along with physical
impairment which restricts their communication and creates the lack of
confidence and affects the lifestyle. This paper attempt to increase the
efficiency of Automatic Speech Recognition (ASR) system for unimpaired speech
signal. It describes state of art of research into improving ASR for speakers
with dysarthria by means of incorporated knowledge of their speech production.
Hybridized approach for feature extraction and acoustic modelling technique
along with evolutionary algorithm is proposed for increasing the efficiency of
the overall system. Here number of feature vectors are varied and tested the
system performance. It is observed that system performance is boosted by
genetic algorithm. System with 16 acoustic features optimized with genetic
algorithm has obtained highest recognition rate of 98.28% with training time of
5:30:17."
"This paper proposes a deep denoising auto-encoder technique to extract better
acoustic features for speech synthesis. The technique allows us to
automatically extract low-dimensional features from high dimensional spectral
features in a non-linear, data-driven, unsupervised way. We compared the new
stochastic feature extractor with conventional mel-cepstral analysis in
analysis-by-synthesis and text-to-speech experiments. Our results confirm that
the proposed method increases the quality of synthetic speech in both
experiments."
"In this research endeavor, it was hypothesized that the sound produced by
animals during their vocalizations can be used as identifiers of the animal
breed or species even if they sound the same to unaided human ear. To test this
hypothesis, three artificial neural networks (ANNs) were developed using
bioacoustics properties as inputs for the respective automatic identification
of 13 bird species, eight dog breeds, and 11 frog species. Recorded
vocalizations of these animals were collected and processed using several known
signal processing techniques to convert the respective sounds into computable
bioacoustics values. The converted values of the vocalizations, together with
the breed or species identifications, were used to train the ANNs following a
ten-fold cross validation technique. Tests show that the respective ANNs can
correctly identify 71.43\% of the birds, 94.44\% of the dogs, and 90.91\% of
the frogs. This result show that bioacoustics and ANN can be used to
automatically determine animal breeds and species, which together could be a
promising automated tool for animal identification, biodiversity determination,
animal conservation, and other animal welfare efforts."
"Many existing speaker verification systems are reported to be vulnerable
against different spoofing attacks, for example speaker-adapted speech
synthesis, voice conversion, play back, etc. In order to detect these spoofed
speech signals as a countermeasure, we propose a score level fusion approach
with several different i-vector subsystems. We show that the acoustic level
Mel-frequency cepstral coefficients (MFCC) features, the phase level modified
group delay cepstral coefficients (MGDCC) and the phonetic level phoneme
posterior probability (PPP) tandem features are effective for the
countermeasure. Furthermore, feature level fusion of these features before
i-vector modeling also enhance the performance. A polynomial kernel support
vector machine is adopted as the supervised classifier. In order to enhance the
generalizability of the countermeasure, we also adopted the cosine similarity
and PLDA scoring as one-class classifications methods. By combining the
proposed i-vector subsystems with the OpenSMILE baseline which covers the
acoustic and prosodic information further improves the final performance. The
proposed fusion system achieves 0.29% and 3.26% EER on the development and test
set of the database provided by the INTERSPEECH 2015 automatic speaker
verification spoofing and countermeasures challenge."
"In this paper we propose a technique for spectral envelope estimation using
maximum values in the sub-bands of Fourier magnitude spectrum (MSASB). Most
other methods in the literature parametrize spectral envelope in cepstral
domain such as Mel-generalized cepstrum etc. Such cepstral domain
representations, although compact, are not readily interpretable. This
difficulty is overcome by our method which parametrizes in the spectral domain
itself. In our experiments, spectral envelope estimated using MSASB method was
incorporated in the STRAIGHT vocoder. Both objective and subjective results of
analysis-by-synthesis indicate that the proposed method is comparable to
STRAIGHT. We also evaluate the effectiveness of the proposed parametrization in
a statistical parametric speech synthesis framework using deep neural networks."
"This paper addresses the problem of audio scenes classification and
contributes to the state of the art by proposing a novel feature. We build this
feature by considering histogram of gradients (HOG) of time-frequency
representation of an audio scene. Contrarily to classical audio features like
MFCC, we make the hypothesis that histogram of gradients are able to encode
some relevant informations in a time-frequency {representation:} namely, the
local direction of variation (in time and frequency) of the signal spectral
power. In addition, in order to gain more invariance and robustness, histogram
of gradients are locally pooled. We have evaluated the relevance of {the novel
feature} by comparing its performances with state-of-the-art competitors, on
several datasets, including a novel one that we provide, as part of our
contribution. This dataset, that we make publicly available, involves $19$
classes and contains about $900$ minutes of audio scene recording. We thus
believe that it may be the next standard dataset for evaluating audio scene
classification algorithms. Our comparison results clearly show that our
HOG-based features outperform its competitors"
"This paper proposes an efficient reconfigurable hardware design for speech
enhancement based on multi band spectral subtraction algorithm and involving
both magnitude and phase components. Our proposed design is novel as it
estimates environmental noise from speech adaptively utilizing both magnitude
and phase components of the speech spectrum. We performed multi-band spectrum
subtraction by dividing the noisy speech spectrum into different non-uniform
frequency bands having varying signal to noise ratio (SNR) and subtracting the
estimated noise from each of these frequency bands. This results to the
elimination of noise from both high SNR and low SNR signal components for all
the frequency bands. We have coined our proposed speech enhancement technique
as Multi Band Magnitude Phase Spectral Subtraction (MBMPSS). The magnitude and
phase operations are executed concurrently exploiting the parallel logic blocks
of Field Programmable Gate Array (FPGA), thus increasing the throughput of the
system to a great extent. We have implemented our design on Spartan6 Lx45 FPGA
and presented the implementation result in terms of resource utilization and
delay information for the different blocks of our design. To the best of our
best knowledge, this is a new type of hardware design for speech enhancement
application and also a first of its kind implementation on reconfigurable
hardware. We have used benchmark audio data for the evaluation of the proposed
hardware and the experimental results show that our hardware shows a better SNR
value compared to the existing state of the art research works."
"This work focuses on the topic of melodic characterization and similarity in
a specific musical repertoire: a cappella flamenco singing, more specifically
in debla and martinete styles. We propose the combination of manual and
automatic description. First, we use a state-of-the-art automatic transcription
method to account for general melodic similarity from music recordings. Second,
we define a specific set of representative mid-level melodic features, which
are manually labeled by flamenco experts. Both approaches are then contrasted
and combined into a global similarity measure. This similarity measure is
assessed by inspecting the clusters obtained through phylogenetic algorithms
algorithms and by relating similarity to categorization in terms of style.
Finally, we discuss the advantage of combining automatic and expert annotations
as well as the need to include repertoire-specific descriptions for meaningful
melodic characterization in traditional music collections."
"We discuss post-processing of speech that has been recorded during Magnetic
Resonance Imaging (MRI) of the vocal tract. Such speech recordings are
contaminated by high levels of acoustic noise from the MRI scanner. Also, the
frequency response of the sound signal path is not flat as a result of severe
restrictions on recording instrumentation due to MRI technology.
  The post-processing algorithm for noise reduction is based on adaptive
spectral filtering. The speech material consists of samples of prolonged vowel
productions that are used for validation of the post-processing algorithm. The
comparison data is recorded in anechoic chamber from the same test subject.
Formant analysis is carried out for the post-processed speech and the
comparison data. Artificially noise-contaminated vowel samples are used for
validation experiments to determine performance of the algorithm where using
true data would be difficult.
  The properties of recording instrumentation or the post-processing algorithm
do not explain the consistent frequency dependent discrepancy between formant
data from experiments during MRI and in anechoic chamber. It is shown that the
discrepancy is statistically significant, in particular, where it is largest at
1 kHz and 2 kHz. The reflecting surfaces of the MRI head and neck coil are
suspected to change the speech acoustics which results in ""external formants""
at these frequencies. However, the role of test subject adaptation to noise and
constrained space acoustics during an MRI examination cannot be ruled out."
"This paper presents the contribution to the third 'CHiME' speech separation
and recognition challenge including both front-end signal processing and
back-end speech recognition. In the front-end, Multi-channel Wiener filter
(MWF) is designed to achieve background noise reduction. Different from
traditional MWF, optimized parameter for the tradeoff between noise reduction
and target signal distortion is built according to the desired noise reduction
level. In the back-end, several techniques are taken advantage to improve the
noisy Automatic Speech Recognition (ASR) performance including Deep Neural
Network (DNN), Convolutional Neural Network (CNN) and Long short-term memory
(LSTM) using medium vocabulary, Lattice rescoring with a big vocabulary
language model finite state transducer, and ROVER scheme. Experimental results
show the proposed system combining front-end and back-end is effective to
improve the ASR performance."
"In this paper, the Lingban entry to the third 'CHiME' speech separation and
recognition challenge is presented. A time-frequency masking based speech
enhancement front-end is proposed to suppress the environmental noise utilizing
multi-channel coherence and spatial cues. The state-of-the-art speech
recognition techniques, namely recurrent neural network based acoustic and
language modeling, state space minimum Bayes risk based discriminative acoustic
modeling, and i-vector based acoustic condition modeling, are carefully
integrated into the speech recognition back-end. To further improve the system
performance by fully exploiting the advantages of different technologies, the
final recognition results are obtained by lattice combination and rescoring.
Evaluations carried out on the official dataset prove the effectiveness of the
proposed systems. Comparing with the best baseline result, the proposed system
obtains consistent improvements with over 57% relative word error rate
reduction on the real-data test set."
"A dedicated algorithm for sparse spectral representation of music sound is
presented. The goal is to enable the representation of a piece of music signal,
as a linear superposition of as few spectral components as possible. A
representation of this nature is said to be sparse. In the present context
sparsity is accomplished by greedy selection of the spectral components, from
an overcomplete set called a dictionary. The proposed algorithm is tailored to
be applied with trigonometric dictionaries. Its distinctive feature being that
it avoids the need for the actual construction of the whole dictionary, by
implementing the required operations via the Fast Fourier Transform. The
achieved sparsity is theoretically equivalent to that rendered by the
Orthogonal Matching Pursuit method. The contribution of the proposed dedicated
implementation is to extend the applicability of the standard Orthogonal
Matching Pursuit algorithm, by reducing its storage and computational demands.
The suitability of the approach for producing sparse spectral models is
illustrated by comparison with the traditional method, in the line of the Short
Time Fourier Transform, involving only the corresponding orthonormal
trigonometric basis."
"In the present paper, details are given on the implementation of a
wavelet-based analysis tailored to the processing of acoustical signals. The
family of the suitable wavelets (`Reimann wavelets') are obtained in the time
domain from a Fourier transform, extracted in Ref.~\cite{r1} after invoking
theoretical principles and time-frequency localisation constraints. A scheme is
set forth to determine the optimal values of the parameters of this type of
wavelet on the basis of the goodness of the reproduction of a $30$-s audio file
containing harmonic signals corresponding to six successive $A$ notes of the
chromatic musical scale, from $A_2$ to $A_7$. The quality of the reproduction
over about six and a half octaves is investigated. Finally, details are given
on the incorporation of the re-assignment method in the analysis framework, as
the means a) to determine the important contributions of the wavelet transforms
and b) to suppress noise present in the signal."
"State-of-the-art statistical parametric speech synthesis (SPSS) generally
uses a vocoder to represent speech signals and parameterize them into features
for subsequent modeling. Magnitude spectrum has been a dominant feature over
the years. Although perceptual studies have shown that phase spectrum is
essential to the quality of synthesized speech, it is often ignored by using a
minimum phase filter during synthesis and the speech quality suffers. To bypass
this bottleneck in vocoded speech, this paper proposes a phase-embedded
waveform representation framework and establishes a magnitude-phase joint
modeling platform for high-quality SPSS. Our experiments on waveform
reconstruction show that the performance is better than that of the widely-used
STRAIGHT. Furthermore, the proposed modeling and synthesis platform outperforms
a leading-edge, vocoded, deep bidirectional long short-term memory recurrent
neural network (DBLSTM-RNN)-based baseline system in various objective
evaluation metrics conducted."
"Multilingual spoken dialogue systems have gained prominence in the recent
past necessitating the requirement for a front-end Language Identification
(LID) system. Most of the existing LID systems rely on modeling the language
discriminative information from low-level acoustic features. Due to the
variabilities of speech (speaker and emotional variabilities, etc.),
large-scale LID systems developed using low-level acoustic features suffer from
a degradation in the performance. In this approach, we have attempted to model
the higher level language discriminative phonotactic information for developing
an LID system. In this paper, the input speech signal is tokenized to phone
sequences by using a language independent phone recognizer. The language
discriminative phonotactic information in the obtained phone sequences are
modeled using statistical and recurrent neural network based language modeling
approaches. As this approach, relies on higher level phonotactical information
it is more robust to variabilities of speech. Proposed approach is
computationally light weight, highly scalable and it can be used in complement
with the existing LID systems."
"Flamenco is a music tradition from Southern Spain which attracts a growing
community of enthusiasts around the world. Its unique melodic and rhythmic
elements, the typically spontaneous and improvised interpretation and its
diversity regarding styles make this still largely undocumented art form a
particularly interesting material for musicological studies. In prior works it
has already been demonstrated that research on computational analysis of
flamenco music, despite it being a relatively new field, can provide powerful
tools for the discovery and diffusion of this genre. In this paper we present
corpusCOFLA, a data framework for the development of such computational tools.
The proposed collection of audio recordings and meta-data serves as a pool for
creating annotated subsets which can be used in development and evaluation of
algorithms for specific music information retrieval tasks. First, we describe
the design criteria for the corpus creation and then provide various examples
of subsets drawn from the corpus. We showcase possible research applications in
the context of computational study of flamenco music and give perspectives
regarding further development of the corpus."
"Automatic note-level transcription is considered one of the most challenging
tasks in music information retrieval. The specific case of flamenco singing
transcription poses a particular challenge due to its complex melodic
progressions, intonation inaccuracies, the use of a high degree of
ornamentation and the presence of guitar accompaniment. In this study, we
explore the limitations of existing state of the art transcription systems for
the case of flamenco singing and propose a specific solution for this genre: We
first extract the predominant melody and apply a novel contour filtering
process to eliminate segments of the pitch contour which originate from the
guitar accompaniment. We formulate a set of onset detection functions based on
volume and pitch characteristics to segment the resulting vocal pitch contour
into discrete note events. A quantised pitch label is assigned to each note
event by combining global pitch class probabilities with local pitch contour
statistics. The proposed system outperforms state of the art singing
transcription systems with respect to voicing accuracy, onset detection and
overall performance when evaluated on flamenco singing datasets."
"Indian twin drums mainly bayan and dayan (tabla) are the most important
percussion instruments in India popularly used for keeping rhythm. It is a twin
percussion/drum instrument of which the right hand drum is called dayan and the
left hand drum is called bayan. Tabla strokes are commonly called as `bol',
constitutes a series of syllables. In this study we have studied the timbre
characteristics of nine strokes from each of five different tablas. Timbre
parameters were calculated from LTAS of each stroke signals. Study of timbre
characteristics is one of the most important deterministic approach for
analyzing tabla and its stroke characteristics. Statistical correlations among
timbre parameters were measured and also through factor analysis we get to know
about the parameters of timbre analysis which are closely related. Tabla
strokes have unique harmonic and timbral characteristics at mid frequency range
and have no uniqueness at low frequency ranges."
"The popular i-vector model represents speakers as low-dimensional continuous
vectors (i-vectors), and hence it is a way of continuous speaker embedding. In
this paper, we investigate binary speaker embedding, which transforms i-vectors
to binary vectors (codes) by a hash function. We start from locality sensitive
hashing (LSH), a simple binarization approach where binary codes are derived
from a set of random hash functions. A potential problem of LSH is that the
randomly sampled hash functions might be suboptimal. We therefore propose an
improved Hamming distance learning approach, where the hash function is learned
by a variable-sized block training that projects each dimension of the original
i-vectors to variable-sized binary codes independently. Our experiments show
that binary speaker embedding can deliver competitive or even better results on
both speaker verification and identification tasks, while the memory usage and
the computation cost are significantly reduced."
"Probabilistic linear discriminant analysis (PLDA) is a popular normalization
approach for the i-vector model, and has delivered state-of-the-art performance
in speaker recognition. A potential problem of the PLDA model, however, is that
it essentially assumes Gaussian distributions over speaker vectors, which is
not always true in practice. Additionally, the objective function is not
directly related to the goal of the task, e.g., discriminating true speakers
and imposters. In this paper, we propose a max-margin metric learning approach
to solve the problems. It learns a linear transform with a criterion that the
margin between target and imposter trials are maximized. Experiments conducted
on the SRE08 core test show that compared to PLDA, the new approach can obtain
comparable or even better performance, though the scoring is simply a cosine
computation."
"Rolling element bearing faults in rotating systems are observed as impulses
in the vibration signals, which are usually buried in noises. In order to
effectively detect the fault of bearings, a novel spectrum searching method is
proposed. The structural information of spectrum (SIOS) on a predefined basis
is constructed through a searching algorithm, such that the harmonics of
impulses generated by faults can be clearly identified and analyzed. Local
peaks of the spectrum are located on a certain bin of the basis, and then the
SIOS can interpret the spectrum via the number and energy of harmonics related
to frequency bins of the basis. Finally bearings can be diagnosed based on the
SIOS by identifying its dominant components. Mathematical formulation is
developed to guarantee the correct construction of the SISO through searching.
The effectiveness of the proposed method is verified with a simulation signal
and a benchmark study of bearings."
"The promising performance of Deep Learning (DL) in speech recognition has
motivated the use of DL in other speech technology applications such as speaker
recognition. Given i-vectors as inputs, the authors proposed an impostor
selection algorithm and a universal model adaptation process in a hybrid system
based on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) to
discriminatively model each target speaker. In order to have more insight into
the behavior of DL techniques in both single and multi-session speaker
enrollment tasks, some experiments have been carried out in this paper in both
scenarios. Additionally, the parameters of the global model, referred to as
universal DBN (UDBN), are normalized before adaptation. UDBN normalization
facilitates training DNNs specifically with more than one hidden layer.
Experiments are performed on the NIST SRE 2006 corpus. It is shown that the
proposed impostor selection algorithm and UDBN adaptation process enhance the
performance of conventional DNNs 8-20 % and 16-20 % in terms of EER for the
single and multi-session tasks, respectively. In both scenarios, the proposed
architectures outperform the baseline systems obtaining up to 17 % reduction in
EER."
"A new musical instrument classification method using convolutional neural
networks (CNNs) is presented in this paper. Unlike the traditional methods, we
investigated a scheme for classifying musical instruments using the learned
features from CNNs. To create the learned features from CNNs, we not only used
a conventional spectrogram image, but also proposed multiresolution recurrence
plots (MRPs) that contain the phase information of a raw input signal.
Consequently, we fed the characteristic timbre of the particular instrument
into a neural network, which cannot be extracted using a phase-blinded
representations such as a spectrogram. By combining our proposed MRPs and
spectrogram images with a multi-column network, the performance of our proposed
classifier system improves over a system that uses only a spectrogram.
Furthermore, the proposed classifier also outperforms the baseline result from
traditional handcrafted features and classifiers."
"This article has been withdrawn by arXiv administrators because the submitter
did not have the legal authority to grant the license applied to the work."
"We present a new representation of harmonic sounds that linearizes the
dynamics of pitch and spectral envelope, while remaining stable to deformations
in the time-frequency plane. It is an instance of the scattering transform, a
generic operator which cascades wavelet convolutions and modulus
nonlinearities. It is derived from the pitch spiral, in that convolutions are
successively performed in time, log-frequency, and octave index. We give a
closed-form approximation of spiral scattering coefficients for a nonstationary
generalization of the harmonic source-filter model."
"In this paper, we propose a neural-based coding scheme in which an artificial
neural network is exploited to automatically compress and decompress speech
signals by a trainable approach. Having a two-stage training phase, the system
can be fully specified to each speech frame and have robust performance across
different speakers and wide range of spoken utterances. Indeed, Frame-based
nonlinear predictive coding (FNPC) would code a frame in the procedure of
training to predict the frame samples. The motivating objective is to analyze
the system behavior in regenerating not only the envelope of spectra, but also
the spectra phase. This scheme has been evaluated in time and discrete cosine
transform (DCT) domains and the output of predicted phonemes show the
potentiality of the FNPC to reconstruct complicated signals. The experiments
were conducted on three voiced plosive phonemes, b/d/g/ in time and DCT domains
versus the number of neurons in the hidden layer. Experiments approve the FNPC
capability as an automatic coding system by which /b/d/g/ phonemes have been
reproduced with a good accuracy. Evaluations revealed that the performance of
FNPC system, trained to predict DCT coefficients is more desirable,
particularly for frames with the wider distribution of energy, compared to time
samples."
"Recent developments in speech synthesis have produced systems capable of
outcome intelligible speech, but now researchers strive to create models that
more accurately mimic human voices. One such development is the incorporation
of multiple linguistic styles in various languages and accents.
  HMM-based Speech Synthesis is of great interest to many researchers, due to
its ability to produce sophisticated features with small footprint. Despite
such progress, its quality has not yet reached the level of the predominant
unit-selection approaches that choose and concatenate recordings of real
speech. Recent efforts have been made in the direction of improving these
systems.
  In this paper we present the application of Long-Short Term Memory Deep
Neural Networks as a Postfiltering step of HMM-based speech synthesis, in order
to obtain closer spectral characteristics to those of natural speech. The
results show how HMM-voices could be improved using this approach."
"With increasing quality requirements for multimedia communications, audio
codecs must maintain both high quality and low delay. Typically, audio codecs
offer either low delay or high quality, but rarely both. We propose a codec
that simultaneously addresses both these requirements, with a delay of only 8.7
ms at 44.1 kHz. It uses gain-shape algebraic vector quantisation in the
frequency domain with time-domain pitch prediction. We demonstrate that the
proposed codec operating at 48 kbit/s and 64 kbit/s out-performs both G.722.1C
and MP3 and has quality comparable to AAC-LD, despite having less than one
fourth of the algorithmic delay of these codecs."
"In this paper we present a research on identification of audio recording
devices from background noise, thus providing a method for forensics. The audio
signal is the sum of speech signal and noise signal. Usually, people pay more
attention to speech signal, because it carries the information to deliver. So a
great amount of researches have been dedicated to getting higher
Signal-Noise-Ratio (SNR). There are many speech enhancement algorithms to
improve the quality of the speech, which can be seen as reducing the noise.
However, noises can be regarded as the intrinsic fingerprint traces of an audio
recording device. These digital traces can be characterized and identified by
new machine learning techniques. Therefore, in our research, we use the noise
as the intrinsic features. As for the identification, multiple classifiers of
deep learning methods are used and compared. The identification result shows
that the method of getting feature vector from the noise of each device and
identifying them with deep learning techniques is viable, and well-preformed."
"Researches have shown accent classification can be improved by integrating
semantic information into pure acoustic approach. In this work, we combine
phonetic knowledge, such as vowels, with enhanced acoustic features to build an
improved accent classification system. The classifier is based on Gaussian
Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual
Linear Predictive (PLP) features. The features are further optimized by
Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant
Analysis (HLDA). Using 7 major types of accented speech from the Foreign
Accented English (FAE) corpus, the system achieves classification accuracy 54%
with input test data as short as 20 seconds, which is competitive to the state
of the art in this field."
"One of the main difficulties in echo cancellation is the fact that the
learning rate needs to vary according to conditions such as double-talk and
echo path change. In this paper we propose a new method of varying the learning
rate of a frequency-domain echo canceller. This method is based on the
derivation of the optimal learning rate of the NLMS algorithm in the presence
of noise. The method is evaluated in conjunction with the multidelay block
frequency domain (MDF) adaptive filter. We demonstrate that it performs better
than current double-talk detection techniques and is simple to implement."
"Various algorithms for text-independent speaker recognition have been
developed through the decades, aiming to improve both accuracy and efficiency.
This paper presents a novel PCA/LDA-based approach that is faster than
traditional statistical model-based methods and achieves competitive results.
First, the performance based on only PCA and only LDA is measured; then a mixed
model, taking advantages of both methods, is introduced. A subset of the TIMIT
corpus composed of 200 male speakers, is used for enrollment, validation and
testing. The best results achieve 100%; 96% and 95% classification rate at
population level 50; 100 and 200, using 39-dimensional MFCC features with delta
and double delta. These results are based on 12-second text-independent speech
for training and 4-second data for test. These are comparable to the
conventional MFCC-GMM methods, but require significantly less time to train and
operate."
"Systems based on automatic speech recognition (ASR) technology can provide
important functionality in computer assisted language learning applications.
This is a young but growing area of research motivated by the large number of
students studying foreign languages. Here we propose a Hidden Markov Model
(HMM)-based method to detect mispronunciations. Exploiting the specific dialog
scripting employed in language learning software, HMMs are trained for
different pronunciations. New adaptive features have been developed and
obtained through an adaptive warping of the frequency scale prior to computing
the cepstral coefficients. The optimization criterion used for the warping
function is to maximize separation of two major groups of pronunciations
(native and non-native) in terms of classification rate. Experimental results
show that the adaptive frequency scale yields a better coefficient
representation leading to higher classification rates in comparison with
conventional HMMs using Mel-frequency cepstral coefficients."
"This document proposes a bandwidth extension system producing a wideband
signal from a narrowband speech signal. The extension is performed
independently for high and low frequencies. High-frequency extension uses the
excitation-filter model. Extension of the excitation is performed in the time
domain using a non-linear function, while the spectral envelope is extended in
the cepstral domain using a multi-layer perceptron. Low-band extension is based
on the sinusoidal model. The amplitude of sinusoids is also estimated using a
multi-layer perceptron.
  The results show that the sound quality after extension is higher than that
of narrowband speech, with a significant variation across listeners. Some of
the techniques, including excitation extension, are of interest in the field of
speech coding.
  -----
  Le pr\'esent m\'emoire propose un syst\`eme d'extension de la bande
permettant de produire un signal en bande AM \`a partir d'un signal de parole
en bande t\'el\'ephonique. L'extension est effectu\'ee de fa\c{c}on
ind\'ependante pour les hautes fr\'equences et les basses fr\'equences.
L'extension des hautes fr\'equences utilise le mod\`ele filtre-excitation.
L'extension de l'excitation est r\'ealis\'ee dans le domaine temporel par une
fonction non lin\'eaire, alors que l'extension de l'enveloppe spectrale
s'effectue dans le domaine cepstral par un perceptron multi-couches.
L'extension de la bande basse utilise le mod\`ele sinuso\""idal. L'amplitude des
sinuso\""ides est aussi estim\'ee par un perceptron multi-couches.
  Les r\'esultats obtenus montrent que la qualit\'e sonore apr\`es extension
est sup\'erieure \`a celle de la bande t\'el\'ephonique, avec une importante
diff\'erence entre les auditeurs. Certaines techniques d\'evelopp\'ees, dont
l'extension de l'excitation, pr\'esentent un certain int\'er\^et pour le
domaine du codage de la parole."
"One of the main difficulties in echo cancellation is the fact that the
learning rate needs to vary according to conditions such as double-talk and
echo path change. Several methods have been proposed to vary the learning. In
this paper we propose a new closed-loop method where the learning rate is
proportional to a misalignment parameter, which is in turn estimated based on a
gradient adaptive approach. The method is presented in the context of a
multidelay block frequency domain (MDF) echo canceller. We demonstrate that the
proposed algorithm outperforms current popular double-talk detection techniques
by up to 6 dB."
"The vector representations of fixed dimensionality for words (in text)
offered by Word2Vec have been shown to be very useful in many application
scenarios, in particular due to the semantic information they carry. This paper
proposes a parallel version, the Audio Word2Vec. It offers the vector
representations of fixed dimensionality for variable-length audio segments.
These vector representations are shown to describe the sequential phonetic
structures of the audio segments to a good degree, with very attractive real
world applications such as query-by-example Spoken Term Detection (STD). In
this STD application, the proposed approach significantly outperformed the
conventional Dynamic Time Warping (DTW) based approaches at significantly lower
computation requirements. We propose unsupervised learning of Audio Word2Vec
from audio data without human annotation using Sequence-to-sequence Audoencoder
(SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM)
units: the first RNN (encoder) maps the input audio sequence into a vector
representation of fixed dimensionality, and the second RNN (decoder) maps the
representation back to the input audio sequence. The two RNNs are jointly
trained by minimizing the reconstruction error. Denoising Sequence-to-sequence
Autoencoder (DSA) is furthered proposed offering more robust learning."
"Conceptual blending is a powerful tool for computational creativity where,
for example, the properties of two harmonic spaces may be combined in a
consistent manner to produce a novel harmonic space. However, deciding about
the importance of property features in the input spaces and evaluating the
results of conceptual blending is a nontrivial task. In the specific case of
musical harmony, defining the salient features of chord transitions and
evaluating invented harmonic spaces requires deep musicological background
knowledge. In this paper, we propose a creative tool that helps musicologists
to evaluate and to enhance harmonic innovation. This tool allows a music expert
to specify arguments over given transition properties. These arguments are then
considered by the system when defining combinations of features in an
idiom-blending process. A music expert can assess whether the new harmonic
idiom makes musicological sense and re-adjust the arguments (selection of
features) to explore alternative blends that can potentially produce better
harmonic spaces. We conclude with a discussion of future work that would
further automate the harmonisation process."
"Now-a-days, speech-based biometric systems such as automatic speaker
verification (ASV) are highly prone to spoofing attacks by an imposture. With
recent development in various voice conversion (VC) and speech synthesis (SS)
algorithms, these spoofing attacks can pose a serious potential threat to the
current state-of-the-art ASV systems. To impede such attacks and enhance the
security of the ASV systems, the development of efficient anti-spoofing
algorithms is essential that can differentiate synthetic or converted speech
from natural or human speech. In this paper, we propose a set of novel speech
features for detecting spoofing attacks. The proposed features are computed
using alternative frequency-warping technique and formant-specific block
transformation of filter bank log energies. We have evaluated existing and
proposed features against several kinds of synthetic speech data from ASVspoof
2015 corpora. The results show that the proposed techniques outperform existing
approaches for various spoofing attack detection task. The techniques
investigated in this paper can also accurately classify natural and synthetic
speech as equal error rates (EERs) of 0% have been achieved."
"This paper presents a new method of singing voice analysis that performs
mutually-dependent singing voice separation and vocal fundamental frequency
(F0) estimation. Vocal F0 estimation is considered to become easier if singing
voices can be separated from a music audio signal, and vocal F0 contours are
useful for singing voice separation. This calls for an approach that improves
the performance of each of these tasks by using the results of the other. The
proposed method first performs robust principal component analysis (RPCA) for
roughly extracting singing voices from a target music audio signal. The F0
contour of the main melody is then estimated from the separated singing voices
by finding the optimal temporal path over an F0 saliency spectrogram. Finally,
the singing voices are separated again more accurately by combining a
conventional time-frequency mask given by RPCA with another mask that passes
only the harmonic structures of the estimated F0s. Experimental results showed
that the proposed method significantly improved the performances of both
singing voice separation and vocal F0 estimation. The proposed method also
outperformed all the other methods of singing voice separation submitted to an
international music analysis competition called MIREX 2014."
"In this paper, we study several microphone channel selection and weighting
methods for robust automatic speech recognition (ASR) in noisy conditions. For
channel selection, we investigate two methods based on the maximum likelihood
(ML) criterion and minimum autoencoder reconstruction criterion, respectively.
For channel weighting, we produce enhanced log Mel filterbank coefficients as a
weighted sum of the coefficients of all channels. The weights of the channels
are estimated by using the ML criterion with constraints. We evaluate the
proposed methods on the CHiME-3 noisy ASR task. Experiments show that channel
weighting significantly outperforms channel selection due to its higher
flexibility. Furthermore, on real test data in which different channels have
different gains of the target signal, the channel weighting method performs
equally well or better than the MVDR beamforming, despite the fact that the
channel weighting does not make use of the phase delay information which is
normally used in beamforming."
"Most current very low bit rate (VLBR) speech coding systems use hidden Markov
model (HMM) based speech recognition/synthesis techniques. This allows
transmission of information (such as phonemes) segment by segment that
decreases the bit rate. However, the encoder based on a phoneme speech
recognition may create bursts of segmental errors. Segmental errors are further
propagated to optional suprasegmental (such as syllable) information coding.
Together with the errors of voicing detection in pitch parametrization,
HMM-based speech coding creates speech discontinuities and unnatural speech
sound artefacts.
  In this paper, we propose a novel VLBR speech coding framework based on
neural networks (NNs) for end-to-end speech analysis and synthesis without
HMMs. The speech coding framework relies on phonological (sub-phonetic)
representation of speech, and it is designed as a composition of deep and
spiking NNs: a bank of phonological analysers at the transmitter, and a
phonological synthesizer at the receiver, both realised as deep NNs, and a
spiking NN as an incremental and robust encoder of syllable boundaries for
coding of continuous fundamental frequency (F0). A combination of phonological
features defines much more sound patterns than phonetic features defined by
HMM-based speech coders, and the finer analysis/synthesis code contributes into
smoother encoded speech. Listeners significantly prefer the NN-based approach
due to fewer discontinuities and speech artefacts of the encoded speech. A
single forward pass is required during the speech encoding and decoding. The
proposed VLBR speech coding operates at a bit rate of approximately 360 bits/s."
"We propose a novel method for Acoustic Event Detection (AED). In contrast to
speech, sounds coming from acoustic events may be produced by a wide variety of
sources. Furthermore, distinguishing them often requires analyzing an extended
time period due to the lack of a clear sub-word unit. In order to incorporate
the long-time frequency structure for AED, we introduce a convolutional neural
network (CNN) with a large input field. In contrast to previous works, this
enables to train audio event detection end-to-end. Our architecture is inspired
by the success of VGGNet and uses small, 3x3 convolutions, but more depth than
previous methods in AED. In order to prevent over-fitting and to take full
advantage of the modeling capabilities of our network, we further propose a
novel data augmentation method to introduce data variation. Experimental
results show that our CNN significantly outperforms state of the art methods
including Bag of Audio Words (BoAW) and classical CNNs, achieving a 16%
absolute improvement."
"Previous accent classification research focused mainly on detecting accents
with pure acoustic information without recognizing accented speech. This work
combines phonetic knowledge such as vowels with acoustic information to build
Guassian Mixture Model (GMM) classifier with Perceptual Linear Predictive (PLP)
features, optimized by Hetroscedastic Linear Discriminant Analysis (HLDA). With
input about 20-second accented speech, this system achieves classification rate
of 51% on a 7-way classification system focusing on the major types of accents
in English, which is competitive to the state-of-the-art results in this field."
"We apply deep learning methods, specifically long short-term memory (LSTM)
networks, to music transcription modelling and composition. We build and train
LSTM networks using approximately 23,000 music transcriptions expressed with a
high-level vocabulary (ABC notation), and use them to generate new
transcriptions. Our practical aim is to create music transcription models
useful in particular contexts of music composition. We present results from
three perspectives: 1) at the population level, comparing descriptive
statistics of the set of training transcriptions and generated transcriptions;
2) at the individual level, examining how a generated transcription reflects
the conventions of a music practice in the training transcriptions (Celtic
folk); 3) at the application level, using the system for idea generation in
music composition. We make our datasets, software and sound examples open and
available: \url{https://github.com/IraKorshunova/folk-rnn}."
"Non-negative Matrix Factorization (NMF) has already been applied to learn
speaker characterizations from single or non-simultaneous speech for speaker
recognition applications. It is also known for its good performance in (blind)
source separation for simultaneous speech. This paper explains how NMF can be
used to jointly solve the two problems in a multichannel speaker recognizer for
simultaneous speech. It is shown how state-of-the-art multichannel NMF for
blind source separation can be easily extended to incorporate speaker
recognition. Experiments on the CHiME corpus show that this method outperforms
the sequential approach of first applying source separation, followed by
speaker recognition that uses state-of-the-art i-vector techniques."
"Distortion of the underlying speech is a common problem for single-channel
speech enhancement algorithms, and hinders such methods from being used more
extensively. A dictionary based speech enhancement method that emphasizes
preserving the underlying speech is proposed. Spectral patches of clean speech
are sampled and clustered to train a dictionary. Given a noisy speech spectral
patch, the best matching dictionary entry is selected and used to estimate the
noise power at each time-frequency bin. The noise estimation step is formulated
as an outlier detection problem, where the noise at each bin is assumed present
only if it is an outlier to the corresponding bin of the best matching
dictionary entry. This framework assigns higher priority in removing spectral
elements that strongly deviate from a typical spoken unit stored in the trained
dictionary. Even without the aid of a separate noise model, this method can
achieve significant noise reduction for various non-stationary noises, while
effectively preserving the underlying speech in more challenging noisy
environments."
"We introduce the use of DCTNet, an efficient approximation and alternative to
PCANet, for acoustic signal classification. In PCANet, the eigenfunctions of
the local sample covariance matrix (PCA) are used as filterbanks for
convolution and feature extraction. When the eigenfunctions are well
approximated by the Discrete Cosine Transform (DCT) functions, each layer of of
PCANet and DCTNet is essentially a time-frequency representation. We relate
DCTNet to spectral feature representation methods, such as the the short time
Fourier transform (STFT), spectrogram and linear frequency spectral
coefficients (LFSC). Experimental results on whale vocalization data show that
DCTNet improves classification rate, demonstrating DCTNet's applicability to
signal processing problems such as underwater acoustics."
"In this paper, we present a novel state of the art system for automatic
downbeat tracking from music signals. The audio signal is first segmented in
frames which are synchronized at the tatum level of the music. We then extract
different kind of features based on harmony, melody, rhythm and bass content to
feed convolutional neural networks that are adapted to take advantage of each
feature characteristics. This ensemble of neural networks is combined to obtain
one downbeat likelihood per tatum. The downbeat sequence is finally decoded
with a flexible and efficient temporal model which takes advantage of the
metrical continuity of a song. We then perform an evaluation of our system on a
large base of 9 datasets, compare its performance to 4 other published
algorithms and obtain a significant increase of 16.8 percent points compared to
the second best system, for altogether a moderate cost in test and training.
The influence of each step of the method is studied to show its strengths and
shortcomings."
"We present a content-based automatic music tagging algorithm using fully
convolutional neural networks (FCNs). We evaluate different architectures
consisting of 2D convolutional layers and subsampling layers only. In the
experiments, we measure the AUC-ROC scores of the architectures with different
complexities and input types using the MagnaTagATune dataset, where a 4-layer
architecture shows state-of-the-art performance with mel-spectrogram input.
Furthermore, we evaluated the performances of the architectures with varying
the number of layers on a larger dataset (Million Song Dataset), and found that
deeper models outperformed the 4-layer architecture. The experiments show that
mel-spectrogram is an effective time-frequency representation for automatic
tagging and that more complex models benefit from more training data."
"Given a musical audio recording, the goal of automatic music transcription is
to determine a score-like representation of the piece underlying the recording.
Despite significant interest within the research community, several studies
have reported on a 'glass ceiling' effect, an apparent limit on the
transcription accuracy that current methods seem incapable of overcoming. In
this paper, we explore how much this effect can be mitigated by focusing on a
specific instrument class and making use of additional information on the
recording conditions available in studio or home recording scenarios. In
particular, exploiting the availability of single note recordings for the
instrument in use we develop a novel signal model employing variable-length
spectro-temporal patterns as its central building blocks - tailored for pitched
percussive instruments such as the piano. Temporal dependencies between
spectral templates are modeled, resembling characteristics of factorial scaled
hidden Markov models (FS-HMM) and other methods combining Non-Negative Matrix
Factorization with Markov processes. In contrast to FS-HMMs, our parameter
estimation is developed in a global, relaxed form within the extensible
alternating direction method of multipliers (ADMM) framework, which enables the
systematic combination of basic regularizers propagating sparsity and local
stationarity in note activity with more complex regularizers imposing temporal
semantics. The proposed method achieves an f-measure of 93-95% for note onsets
on pieces recorded on a Yamaha Disklavier (MAPS DB)."
"In this document, we introduce a new dataset designed for training machine
learning models of symbolic music data. Five datasets are provided, one of
which is from a newly collected corpus of 20K midi files. We describe our
preprocessing and cleaning pipeline, which includes the exclusion of a number
of files based on scores from a previously developed probabilistic machine
learning model. We also define training, testing and validation splits for the
new dataset, based on a clustering scheme which we also describe. Some simple
histograms are included."
"In this paper we propose methods to extract geographically relevant
information in a multimedia recording using its audio. Our method primarily is
based on the fact that urban acoustic environment consists of a variety of
sounds. Hence, location information can be inferred from the composition of
sound events/classes present in the audio. More specifically, we adopt matrix
factorization techniques to obtain semantic content of recording in terms of
different sound classes. These semantic information are then combined to
identify the location of recording."
"In this paper, we describe a statistical parametric speech synthesis approach
with unit-level acoustic representation. In conventional deep neural network
based speech synthesis, the input text features are repeated for the entire
duration of phoneme for mapping text and speech parameters. This mapping is
learnt at the frame-level which is the de-facto acoustic representation.
However much of this computational requirement can be drastically reduced if
every unit can be represented with a fixed-dimensional representation. Using
recurrent neural network based auto-encoder, we show that it is indeed possible
to map units of varying duration to a single vector. We then use this acoustic
representation at unit-level to synthesize speech using deep neural network
based statistical parametric speech synthesis technique. Results show that the
proposed approach is able to synthesize at the same quality as the conventional
frame based approach at a highly reduced computational cost."
"Acoustic models based on long short-term memory recurrent neural networks
(LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and
showed significant improvements in naturalness and latency over those based on
hidden Markov models (HMMs). This paper describes further optimizations of
LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization,
multi-frame inference, and robust inference using an {\epsilon}-contaminated
Gaussian loss function. Experimental results in subjective listening tests show
that these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based
SPSS in runtime speed while maintaining naturalness. Evaluations between
LSTM-RNN- based SPSS and HMM-driven unit selection speech synthesis are also
presented."
"Audio carries substantial information about the content of our surroundings.
The content has been explored at the semantic level using acoustic concepts,
but rarely on concept pairs such as happy crowd and angry crowd. Concept pairs
convey unique information and complement other audio and multimedia
applications. Hence, in this work we explored for the first time the
classification's performance of acoustic concepts pairs. For this study, we
introduce the AudioSentiBank corpus, which is a large-scale folksology
containing over 1,123 adjective and verb noun pairs. Our contribution consists
on providing the research corpus, the benchmark for classification of acoustic
concept pairs, and an analysis on the pairs."
"This paper presents a novel two-phase method for audio representation,
Discriminative and Compact Audio Representation (DCAR), and evaluates its
performance at detecting events in consumer-produced videos. In the first phase
of DCAR, each audio track is modeled using a Gaussian mixture model (GMM) that
includes several components to capture the variability within that track. The
second phase takes into account both global structure and local structure. In
this phase, the components are rendered more discriminative and compact by
formulating an optimization problem on Grassmannian manifolds, which we found
represents the structure of audio effectively.
  Our experiments used the YLI-MED dataset (an open TRECVID-style video corpus
based on YFCC100M), which includes ten events. The results show that the
proposed DCAR representation consistently outperforms state-of-the-art audio
representations. DCAR's advantage over i-vector, mv-vector, and GMM
representations is significant for both easier and harder discrimination tasks.
We discuss how these performance differences across easy and hard cases follow
from how each type of model leverages (or doesn't leverage) the intrinsic
structure of the data. Furthermore, DCAR shows a particularly notable accuracy
advantage on events where humans have more difficulty classifying the videos,
i.e., events with lower mean annotator confidence."
"One of the most important problems in audio event detection research is
absence of benchmark results for comparison with any proposed method. Different
works consider different sets of events and datasets which makes it difficult
to comprehensively analyze any novel method with an existing one. In this paper
we propose to establish results for audio event recognition on two recent
publicly-available datasets. In particular we use Gaussian Mixture model based
feature representation and combine them with linear as well as non-linear
kernel Support Vector Machines."
"In recent years identity-vector (i-vector) based speaker verification (SV)
systems have become very successful. Nevertheless, environmental noise and
speech duration variability still have a significant effect on degrading the
performance of these systems. In many real-life applications, duration of
recordings are very short; as a result, extracted i-vectors cannot reliably
represent the attributes of the speaker. Here, we investigate the effect of
speech duration on the performance of three state-of-the-art speaker
recognition systems. In addition, using a variety of available score fusion
methods, we investigate the effect of score fusion for those speaker
verification techniques to benefit from the performance difference of different
methods under different enrollment and test speech duration conditions. This
technique performed significantly better than the baseline score fusion
methods."
"A non-invasive method for the monitoring of heart activity can help to reduce
the deaths caused by heart disorders such as stroke, arrhythmia and heart
attack. The human voice can be considered as a biometric data that can be used
for estimation of heart rate. In this paper, we propose a method for estimating
the heart rate from human speech dynamically using voice signal analysis and by
the development of an empirical linear predictor model. The correlation between
the voice signal and heart rate are established by classifiers and prediction
of the heart rates with or without emotions are done using linear models. The
prediction accuracy was tested using the data collected from 15 subjects, it is
about 4050 samples of speech signals and corresponding electrocardiogram
samples. The proposed approach can use for early non-invasive detection of
heart rate changes that can be correlated to an emotional state of the
individual and also can be used as a tool for diagnosis of heart conditions in
real-time situations."
"In this paper we propose a new binaural beamforming technique which can be
seen as a relaxation of the linearly constrained minimum variance (LCMV)
framework. The proposed method can achieve simultaneous noise reduction and
exact binaural cue preservation of the target source, similar to the binaural
minimum variance distortionless response (BMVDR) method. However, unlike BMVDR,
the proposed method is also able to preserve the binaural cues of multiple
interferers to a certain predefined accuracy. Specifically, it is able to
control the trade-off between noise reduction and binaural cue preservation of
the interferers by using a separate trade-off parameter per interferer.
Moreover, we provide a robust way of selecting these trade-off parameters in
such a way that the preservation accuracy for the binaural cues of the
interferers is always better than the corresponding ones of the BMVDR. The
relaxation of the constraints in the proposed method achieves approximate
binaural cue preservation of more interferers than other previously presented
LCMV-based binaural beamforming methods that use strict equality constraints."
"This paper introduces WaveNet, a deep neural network for generating raw audio
waveforms. The model is fully probabilistic and autoregressive, with the
predictive distribution for each audio sample conditioned on all previous ones;
nonetheless we show that it can be efficiently trained on data with tens of
thousands of samples per second of audio. When applied to text-to-speech, it
yields state-of-the-art performance, with human listeners rating it as
significantly more natural sounding than the best parametric and concatenative
systems for both English and Mandarin. A single WaveNet can capture the
characteristics of many different speakers with equal fidelity, and can switch
between them by conditioning on the speaker identity. When trained to model
music, we find that it generates novel and often highly realistic musical
fragments. We also show that it can be employed as a discriminative model,
returning promising results for phoneme recognition."
"TristouNet is a neural network architecture based on Long Short-Term Memory
recurrent networks, meant to project speech sequences into a fixed-dimensional
euclidean space. Thanks to the triplet loss paradigm used for training, the
resulting sequence embeddings can be compared directly with the euclidean
distance, for speaker comparison purposes. Experiments on short (between 500ms
and 5s) speech turn comparison and speaker change detection show that
TristouNet brings significant improvements over the current state-of-the-art
techniques for both tasks."
"Using a known speaker-intrinsic normalization procedure, formant data are
scaled by the reciprocal of the geometric mean of the first three formant
frequencies. This reduces the influence of the talker but results in a
distorted vowel space. The proposed speaker-extrinsic procedure re-scales the
normalized values by the mean formant values of vowels. When tested on the
formant data of vowels published by Peterson and Barney, the combined approach
leads to well separated clusters by reducing the spread due to talkers. The
proposed procedure performs better than two top-ranked normalization procedures
based on the accuracy of vowel classification as the objective measure."
"In language recognition, the task of rejecting/differentiating closely spaced
versus acoustically far spaced languages remains a major challenge. For
confusable closely spaced languages, the system needs longer input test
duration material to obtain sufficient information to distinguish between
languages. Alternatively, if languages are distinct and not
acoustically/linguistically similar to others, duration is not a sufficient
remedy. The solution proposed here is to explore duration distribution analysis
for near/far languages based on the Language Recognition i-Vector Machine
Learning Challenge 2015 (LRiMLC15) database. Using this knowledge, we propose a
likelihood ratio based fusion approach that leveraged both score and duration
information. The experimental results show that the use of duration and score
fusion improves language recognition performance by 5% relative in LRiMLC15
cost."
"In this paper, a new statistic feature of the discrete short-time amplitude
spectrum is discovered by experiments for the signals of unvoiced
pronunciation. For the random-varying short-time spectrum, this feature reveals
the relationship between the amplitude's average and its standard for every
frequency component. On the other hand, the association between the amplitude
distributions for different frequency components is also studied. A new model
representing such association is inspired by the normalized histogram of
amplitude. By mathematical analysis, the new statistic feature discovered is
proved to be necessary evidence which supports the proposed model, and also can
be direct evidence for the widely used hypothesis of ""identical distribution of
amplitude for all frequencies""."
"This paper presents results on Speaker Recognition (SR) for children's
speech, using the OGI Kids corpus and GMM-UBM and GMM-SVM SR systems. Regions
of the spectrum containing important speaker information for children are
identified by conducting SR experiments over 21 frequency bands. As for adults,
the spectrum can be split into four regions, with the first (containing primary
vocal tract resonance information) and third (corresponding to high frequency
speech sounds) being most useful for SR. However, the frequencies at which
these regions occur are from 11% to 38% higher for children. It is also noted
that subband SR rates are lower for younger children. Finally results are
presented of SR experiments to identify a child in a class (30 children,
similar age) and school (288 children, varying ages). Class performance depends
on age, with accuracy varying from 90% for young children to 99% for older
children. The identification rate achieved for a child in a school is 81%."
"This work focuses on reliable detection of bird sound emissions as recorded
in the open field. Acoustic detection of avian sounds can be used for the
automatized monitoring of multiple bird taxa and querying in long-term
recordings for species of interest for researchers, conservation practitioners,
and decision makers. Recordings in the wild can be very noisy due to the
exposure of the microphones to a large number of audio sources originating from
all distances and directions, the number and identity of which cannot be known
a-priori. The co-existence of the target vocalizations with abiotic
interferences in an unconstrained environment is inefficiently treated by
current approaches of audio signal enhancement. A technique that would spot
only bird vocalization while ignoring other audio sources is of prime
importance. These difficulties are tackled in this work, presenting a deep
autoencoder that maps the audio spectrogram of bird vocalizations to its
corresponding binary mask that encircles the spectral blobs of vocalizations
while suppressing other audio sources. The procedure requires minimum human
attendance, it is very fast during execution, thus suitable to scan massive
volumes of data, in order to analyze them, evaluate insights and hypotheses,
identify patterns of bird activity that, hopefully, finally lead to design
policies on biodiversity issues."
"PLDA is a popular normalization approach for the i-vector model, and it has
delivered state-of-the-art performance in speaker verification. However, PLDA
training requires a large amount of labeled development data, which is highly
expensive in most cases. A possible approach to mitigate the problem is various
unsupervised adaptation methods, which use unlabeled data to adapt the PLDA
scattering matrices to the target domain.
  In this paper, we present a new `local training' approach that utilizes
inaccurate but much cheaper local labels to train the PLDA model. These local
labels discriminate speakers within a single conversion only, and so are much
easier to obtain compared to the normal `global labels'. Our experiments show
that the proposed approach can deliver significant performance improvement,
particularly with limited globally-labeled data."
"This paper presents a unified model to perform language and speaker
recognition simultaneously and altogether. The model is based on a multi-task
recurrent neural network where the output of one task is fed as the input of
the other, leading to a collaborative learning framework that can improve both
language and speaker recognition by borrowing information from each other. Our
experiments demonstrated that the multi-task model outperforms the
task-specific models on both tasks."
"Most existing methods in binaural sound source localization rely on some kind
of aggregation of phase-and level-difference cues in the time-frequency plane.
While different ag-gregation schemes exist, they are often heuristic and suffer
in adverse noise conditions. In this paper, we introduce the rectified binaural
ratio as a new feature for sound source local-ization. We show that for
Gaussian-process point source signals corrupted by stationary Gaussian noise,
this ratio follows a complex t-distribution with explicit parameters. This new
formulation provides a principled and statistically sound way to aggregate
binaural features in the presence of noise. We subsequently derive two simple
and efficient methods for robust relative transfer function and time-delay
estimation. Experiments on heavily corrupted simulated and speech signals
demonstrate the robustness of the proposed scheme."
"We consider the problem of estimating the phases of K mixed complex signals
from a multichannel observation, when the mixing matrix and signal magnitudes
are known. This problem can be cast as a non-convex quadratically constrained
quadratic program which is known to be NP-hard in general. We propose three
approaches to tackle it: a heuristic method, an alternate minimization method,
and a convex relaxation into a semi-definite program. The last two approaches
are showed to outperform the oracle multichannel Wiener filter in
under-determined informed source separation tasks, using simulated and speech
signals. The convex relaxation approach yields best results, including the
potential for exact source separation in under-determined settings."
"Besides spoken words, speech signals also carry information about speaker
gender, age, and emotional state which can be used in a variety of speech
analysis applications. In this paper, a divide and conquer strategy for
ensemble classification has been proposed to recognize emotions in speech.
Intrinsic hierarchy in emotions has been utilized to construct an emotions
tree, which assisted in breaking down the emotion recognition task into smaller
sub tasks. The proposed framework generates predictions in three phases.
Firstly, emotions are detected in the input speech signal by classifying it as
neutral or emotional. If the speech is classified as emotional, then in the
second phase, it is further classified into positive and negative classes.
Finally, individual positive or negative emotions are identified based on the
outcomes of the previous stages. Several experiments have been performed on a
widely used benchmark dataset. The proposed method was able to achieve improved
recognition rates as compared to several other approaches."
"Speaker verification systems are vulnerable to spoofing attacks which
presents a major problem in their real-life deployment. To date, most of the
proposed synthetic speech detectors (SSDs) have weighted the importance of
different segments of speech equally. However, different attack methods have
different strengths and weaknesses and the traces that they leave may be short
or long term acoustic artifacts. Moreover, those may occur for only particular
phonemes or sounds. Here, we propose three algorithms that weigh
likelihood-ratio scores of individual frames, phonemes, and sound-classes
depending on their importance for the SSD. Significant improvement over the
baseline system has been obtained for known attack methods that were used in
training the SSDs. However, improvement with unknown attack types was not
substantial. Thus, the type of distortions that were caused by the unknown
systems were different and could not be captured better with the proposed SSD
compared to the baseline SSD."
"This review chapter aims to strengthen the link between frame theory and
signal processing tasks in psychoacoustics. On the one side, the basic concepts
of frame theory are presented and some proofs are provided to explain those
concepts in some detail. The goal is to reveal to hearing scientists how this
mathematical theory could be relevant for their research. In particular, we
focus on frame theory in a filter bank approach, which is probably the most
relevant view-point for audio signal processing. On the other side, basic
psychoacoustic concepts are presented to stimulate mathematicians to apply
their knowledge in this field."
"This paper describes a system designed as part of an interactive VR opera,
which immerses a real-time composer and an audience (via a network) in the
historical location of Gobeklitepe, in southern Turkey during an imaginary
scenario set in the Pre-Pottery Neolithic period (8500-5500 BCE), viewed by
some to be the earliest example of a temple, or observatory. In this scene
music is generated, where the harmonic material is determined based on
observations of light variation from pulsating stars, that would have
theoretically been overhead on the 1st of October 8000 BC at 23:00 and animal
calls based on the reliefs in the temple. Based on the observations of the
stars V465 Per, HD 217860, 16 Lac, BG CVn and KIC 6382916, frequency
collections were derived and applied to the generation of musical sound and
notation sequences within a custom VR environment using a novel method
incorporating spectralist techniques. Parameters controlling this 'resynthesis'
can be manipulated by the performer using a Leap Motion controller and Oculus
Rift HMD, yielding both sonic and visual results in the environment. The final
opera is to be viewed via Google Cardboard and delivered over the Internet.
This entire process aims to pose questions about real-time composition through
time distortion and invoke a sense of wonder and meaningfulness through a
ritualistic experience."
"We present a novel framework for generating pop music. Our model is a
hierarchical Recurrent Neural Network, where the layers and the structure of
the hierarchy encode our prior knowledge about how pop music is composed. In
particular, the bottom layers generate the melody, while the higher levels
produce the drums and chords. We conduct several human studies that show strong
preference of our generated music over that produced by the recent method by
Google. We additionally show two applications of our framework: neural dancing
and karaoke, as well as neural story singing."
"Music holds a significant cultural role in social identity and in the
encouragement of socialization. Technology, by the destruction of physical and
cultural distance, has lead to many changes in musical themes and the complete
loss of forms. Yet, it also allows for the preservation and distribution of
music from societies without a history of written sheet music. This paper
presents early work on a tool for musicians and ethnomusicologists to
transcribe sheet music from monophonic voiced pieces for preservation and
distribution. Using FFT, the system detects the pitch frequencies, also other
methods detect note durations, tempo, time signatures and generates sheet
music. The final system is able to be used in mobile platforms allowing the
user to take recordings and produce sheet music in situ to a performance."
"Despite the overwhelming success of deep learning in various speech
processing tasks, the problem of separating simultaneous speakers in a mixture
remains challenging. Two major difficulties in such systems are the arbitrary
source permutation and unknown number of sources in the mixture. We propose a
novel deep learning framework for single channel speech separation by creating
attractor points in high dimensional embedding space of the acoustic signals
which pull together the time-frequency bins corresponding to each source.
Attractor points in this study are created by finding the centroids of the
sources in the embedding space, which are subsequently used to determine the
similarity of each bin in the mixture to each source. The network is then
trained to minimize the reconstruction error of each source by optimizing the
embeddings. The proposed model is different from prior works in that it
implements an end-to-end training, and it does not depend on the number of
sources in the mixture. Two strategies are explored in the test time, K-means
and fixed attractor points, where the latter requires no post-processing and
can be implemented in real-time. We evaluated our system on Wall Street Journal
dataset and show 5.49\% improvement over the previous state-of-the-art methods."
"Designing appropriate features for acoustic event recognition tasks is an
active field of research. Expressive features should both improve the
performance of the tasks and also be interpret-able. Currently, heuristically
designed features based on the domain knowledge requires tremendous effort in
hand-crafting, while features extracted through deep network are difficult for
human to interpret. In this work, we explore the experience guided learning
method for designing acoustic features. This is a novel hybrid approach
combining both domain knowledge and purely data driven feature designing. Based
on the procedure of log Mel-filter banks, we design a filter bank learning
layer. We concatenate this layer with a convolutional neural network (CNN)
model. After training the network, the weight of the filter bank learning layer
is extracted to facilitate the design of acoustic features. We smooth the
trained weight of the learning layer and re-initialize it in filter bank
learning layer as audio feature extractor. For the environmental sound
recognition task based on the Urban- sound8K dataset, the experience guided
learning leads to a 2% accuracy improvement compared with the fixed feature
extractors (the log Mel-filter bank). The shape of the new filter banks are
visualized and explained to prove the effectiveness of the feature design
process."
"The works of Rabindranath Tagore have been sung by various artistes over
generations spanning over almost 100 years. there are few songs which were
popular in the early years and have been able to retain their popularity over
the years while some others have faded away. In this study we look to find cues
for the singing style of these songs which have kept them alive for all these
years. For this we took 3 min clip of four Tagore songs which have been sung by
five generation of artistes over 100 years and analyze them with the help of
latest nonlinear techniques Multifractal Detrended Fluctuation Analysis
(MFDFA). The multifractal spectral width is a manifestation of the inherent
complexity of the signal and may prove to be an important parameter to identify
the singing style of particular generation of singers and how this style varies
over different generations. The results are discussed in detail."
"In North Indian Classical Music, raga forms the basic structure over which
individual improvisations is performed by an artist based on his/her
creativity. The Alap is the opening section of a typical Hindustani Music (HM)
performance, where the raga is introduced and the paths of its development are
revealed using all the notes used in that particular raga and allowed
transitions between them with proper distribution over time. In India,
corresponding to each raga, several emotional flavors are listed, namely erotic
love, pathetic, devotional, comic, horrific, repugnant, heroic, fantastic,
furious, peaceful. The detection of emotional cues from Hindustani Classical
music is a demanding task due to the inherent ambiguity present in the
different ragas, which makes it difficult to identify any particular emotion
from a certain raga. In this study we took the help of a high resolution
mathematical microscope (MFDFA or Multifractal Detrended Fluctuation Analysis)
to procure information about the inherent complexities and time series
fluctuations that constitute an acoustic signal. With the help of this
technique, 3 min alap portion of six conventional ragas of Hindustani classical
music namely, Darbari Kanada, Yaman, Mian ki Malhar, Durga, Jay Jayanti and
Hamswadhani played in three different musical instruments were analyzed. The
results are discussed in detail."
"We introduce the Free Music Archive (FMA), an open and easily accessible
dataset which can be used to evaluate several tasks in music information
retrieval (MIR), a field concerned with browsing, searching, and organizing
large music collections. The community's growing interest in feature and
end-to-end learning is however restrained by the limited availability of large
audio datasets. By releasing the FMA, we hope to foster research which will
improve the state-of-the-art and hopefully surpass the performance ceiling
observed in e.g. genre recognition (MGR). The data is made of 106,574 tracks,
16,341 artists, 14,854 albums, arranged in a hierarchical taxonomy of 161
genres, for a total of 343 days of audio and 917 GiB, all under permissive
Creative Commons licenses. It features metadata like song title, album, artist
and genres; user data like play counts, favorites, and comments; free-form text
like description, biography, and tags; together with full-length, high-quality
audio, and some pre-computed features. We propose a train/validation/test split
and three subsets: a genre-balanced set of 8,000 tracks from 8 major genres, a
genre-unbalanced set of 25,000 tracks from 16 genres, and a 98 GiB version with
clips trimmed to 30s. This paper describes the dataset and how it was created,
proposes some tasks like music classification and annotation or recommendation,
and evaluates some baselines for MGR. Code, data, and usage examples are
available at https://github.com/mdeff/fma."
"Many people enjoy classical symphonic music. Its diverse instrumentation
makes for a rich listening experience. This diversity adds to the conductor's
expressive freedom to shape the sound according to their imagination. As a
result, the same piece may sound quite differently from one conductor to
another. Differences in interpretation may be noticeable subjectively to
listeners, but they are sometimes hard to pinpoint, presumably because of the
acoustic complexity of the sound. We describe a computational model that
interprets dynamics---expressive loudness variations in performances---in terms
of the musical score, highlighting differences between performances of the same
piece. We demonstrate experimentally that the model has predictive power, and
give examples of conductor ideosyncrasies found by using the model as an
explanatory tool. Although the present model is still in active development, it
may pave the road for a consumer-oriented companion to interactive classical
music understanding."
"State-of-the-art i-vector based speaker verification relies on variants of
Probabilistic Linear Discriminant Analysis (PLDA) for discriminant analysis. We
are mainly motivated by the recent work of the joint Bayesian (JB) method,
which is originally proposed for discriminant analysis in face verification. We
apply JB to speaker verification and make three contributions beyond the
original JB. 1) In contrast to the EM iterations with approximated statistics
in the original JB, the EM iterations with exact statistics are employed and
give better performance. 2) We propose to do simultaneous diagonalization (SD)
of the within-class and between-class covariance matrices to achieve efficient
testing, which has broader application scope than the SVD-based efficient
testing method in the original JB. 3) We scrutinize similarities and
differences between various Gaussian PLDAs and JB, complementing the previous
analysis of comparing JB only with Prince-Elder PLDA. Extensive experiments are
conducted on NIST SRE10 core condition 5, empirically validating the
superiority of JB with faster convergence rate and 9-13% EER reduction compared
with state-of-the-art PLDA."
"We explore frame-level audio feature learning for chord recognition using
artificial neural networks. We present the argument that chroma vectors
potentially hold enough information to model harmonic content of audio for
chord recognition, but that standard chroma extractors compute too noisy
features. This leads us to propose a learned chroma feature extractor based on
artificial neural networks. It is trained to compute chroma features that
encode harmonic information important for chord recognition, while being robust
to irrelevant interferences. We achieve this by feeding the network an audio
spectrum with context instead of a single frame as input. This way, the network
can learn to selectively compensate noise and resolve harmonic ambiguities.
  We compare the resulting features to hand-crafted ones by using a simple
linear frame-wise classifier for chord recognition on various data sets. The
results show that the learned feature extractor produces superior chroma
vectors for chord recognition."
"In an attempt at exploring the limitations of simple approaches to the task
of piano transcription (as usually defined in MIR), we conduct an in-depth
analysis of neural network-based framewise transcription. We systematically
compare different popular input representations for transcription systems to
determine the ones most suitable for use with neural networks. Exploiting
recent advances in training techniques and new regularizers, and taking into
account hyper-parameter tuning, we show that it is possible, by simple
bottom-up frame-wise processing, to obtain a piano transcriber that outperforms
the current published state of the art on the publicly available MAPS dataset
-- without any complex post-processing steps. Thus, we propose this simple
approach as a new baseline for this dataset, for future transcription research
to build on and improve."
"In this paper, we propose an end-to-end neural network (NN) based EEG-speech
(NES) modeling framework, in which three network structures are developed to
map imagined EEG signals to phonemes. The proposed NES models incorporate a
language model based EEG feature extraction layer, an acoustic feature mapping
layer, and a restricted Boltzmann machine (RBM) based the feature learning
layer. The NES models can jointly realize the representation of multichannel
EEG signals and the projection of acoustic speech signals. Among three proposed
NES models, two augmented networks utilize spoken EEG signals as either bias or
gate information to strengthen the feature learning and translation of imagined
EEG signals. Experimental results show that all three proposed NES models
outperform the baseline support vector machine (SVM) method on EEG-speech
classification. With respect to binary classification, our approach achieves
comparable results relative to deep believe network approach."
"This paper introduces a new paradigm for sound source lo-calization referred
to as virtual acoustic space traveling (VAST) and presents a first dataset
designed for this purpose. Existing sound source localization methods are
either based on an approximate physical model (physics-driven) or on a
specific-purpose calibration set (data-driven). With VAST, the idea is to learn
a mapping from audio features to desired audio properties using a massive
dataset of simulated room impulse responses. This virtual dataset is designed
to be maximally representative of the potential audio scenes that the
considered system may be evolving in, while remaining reasonably compact. We
show that virtually-learned mappings on this dataset generalize to real data,
overcoming some intrinsic limitations of traditional binaural sound
localization methods based on time differences of arrival."
"In this paper we propose a novel model for unconditional audio generation
based on generating one audio sample at a time. We show that our model, which
profits from combining memory-less modules, namely autoregressive multilayer
perceptrons, and stateful recurrent neural networks in a hierarchical structure
is able to capture underlying sources of variations in the temporal sequences
over very long time spans, on three datasets of different nature. Human
evaluation on the generated samples indicate that our model is preferred over
competing models. We also show how each component of the model contributes to
the exhibited performance."
"This work aims to investigate the use of deep neural network to detect
commercial hobby drones in real-life environments by analyzing their sound
data. The purpose of work is to contribute to a system for detecting drones
used for malicious purposes, such as for terrorism. Specifically, we present a
method capable of detecting the presence of commercial hobby drones as a binary
classification problem based on sound event detection. We recorded the sound
produced by a few popular commercial hobby drones, and then augmented this data
with diverse environmental sound data to remedy the scarcity of drone sound
data in diverse environments. We investigated the effectiveness of
state-of-the-art event sound classification methods, i.e., a Gaussian Mixture
Model (GMM), Convolutional Neural Network (CNN), and Recurrent Neural Network
(RNN), for drone sound detection. Our empirical results, which were obtained
with a testing dataset collected on an urban street, confirmed the
effectiveness of these models for operating in a real environment. In summary,
our RNN models showed the best detection performance with an F-Score of 0.8009
with 240 ms of input audio with a short processing time, indicating their
applicability to real-time detection systems."
"Most of the previous approaches to lyrics-to-audio alignment used a
pre-developed automatic speech recognition (ASR) system that innately suffered
from several difficulties to adapt the speech model to individual singers. A
significant aspect missing in previous works is the self-learnability of
repetitive vowel patterns in the singing voice, where the vowel part used is
more consistent than the consonant part. Based on this, our system first learns
a discriminative subspace of vowel sequences, based on weighted symmetric
non-negative matrix factorization (WS-NMF), by taking the self-similarity of a
standard acoustic feature as an input. Then, we make use of canonical time
warping (CTW), derived from a recent computer vision technique, to find an
optimal spatiotemporal transformation between the text and the acoustic
sequences. Experiments with Korean and English data sets showed that deploying
this method after a pre-developed, unsupervised, singing source separation
achieved more promising results than other state-of-the-art unsupervised
approaches and an existing ASR-based system."
"Hidden Markov model based various phoneme recognition methods for Bengali
language is reviewed. Automatic phoneme recognition for Bengali language using
multilayer neural network is reviewed. Usefulness of multilayer neural network
over single layer neural network is discussed. Bangla phonetic feature table
construction and enhancement for Bengali speech recognition is also discussed.
Comparison among these methods is discussed."
"Chord recognition systems use temporal models to post-process frame-wise
chord preditions from acoustic models. Traditionally, first-order models such
as Hidden Markov Models were used for this task, with recent works suggesting
to apply Recurrent Neural Networks instead. Due to their ability to learn
longer-term dependencies, these models are supposed to learn and to apply
musical knowledge, instead of just smoothing the output of the acoustic model.
In this paper, we argue that learning complex temporal models at the level of
audio frames is futile on principle, and that non-Markovian models do not
perform better than their first-order counterparts. We support our argument
through three experiments on the McGill Billboard dataset. The first two show
1) that when learning complex temporal models at the frame level, improvements
in chord sequence modelling are marginal; and 2) that these improvements do not
translate when applied within a full chord recognition system. The third, still
rather preliminary experiment gives first indications that the use of complex
sequential models for chord prediction at higher temporal levels might be more
promising."
"Korea University Intelligent Signal Processing Lab. (KU-ISPL) developed
speaker recognition system for SRE16 fixed training condition. Data for
evaluation trials are collected from outside North America, spoken in Tagalog
and Cantonese while training data only is spoken English. Thus, main issue for
SRE16 is compensating the discrepancy between different languages. As
development dataset which is spoken in Cebuano and Mandarin, we could prepare
the evaluation trials through preliminary experiments to compensate the
language mismatched condition. Our team developed 4 different approaches to
extract i-vectors and applied state-of-the-art techniques as backend. To
compensate language mismatch, we investigated and endeavored unique method such
as unsupervised language clustering, inter language variability compensation
and gender/language dependent score normalization."
"A class of methods based on multichannel linear prediction (MCLP) can achieve
effective blind dereverberation of a source, when the source is observed with a
microphone array. We propose an inventive use of MCLP as a pre-processing step
for blind source separation with a microphone array. We show theoretically
that, under certain assumptions, such pre-processing reduces the original blind
reverberant source separation problem to a non-reverberant one, which in turn
can be effectively tackled using existing methods. We demonstrate our claims
using real recordings obtained with an eight-microphone circular array in
reverberant environments."
"We introduce in this work an efficient approach for audio scene
classification using deep recurrent neural networks. An audio scene is firstly
transformed into a sequence of high-level label tree embedding feature vectors.
The vector sequence is then divided into multiple subsequences on which a deep
GRU-based recurrent neural network is trained for sequence-to-label
classification. The global predicted label for the entire sequence is finally
obtained via aggregation of subsequence classification outputs. We will show
that our approach obtains an F1-score of 97.7% on the LITIS Rouen dataset,
which is the largest dataset publicly available for the task. Compared to the
best previously reported result on the dataset, our approach is able to reduce
the relative classification error by 35.3%."
"The field of speech recognition is in the midst of a paradigm shift:
end-to-end neural networks are challenging the dominance of hidden Markov
models as a core technology. Using an attention mechanism in a recurrent
encoder-decoder architecture solves the dynamic time alignment problem,
allowing joint end-to-end training of the acoustic and language modeling
components. In this paper we extend the end-to-end framework to encompass
microphone array signal processing for noise suppression and speech enhancement
within the acoustic encoding network. This allows the beamforming components to
be optimized jointly within the recognition architecture to improve the
end-to-end speech recognition objective. Experiments on the noisy speech
benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system
outperformed the attention-based baseline with input from a conventional
adaptive beamformer."
"Psychiatric illnesses are often associated with multiple symptoms, whose
severity must be graded for accurate diagnosis and treatment. This grading is
usually done by trained clinicians based on human observations and judgments
made within doctor-patient sessions. Current research provides sufficient
reason to expect that the human voice may carry biomarkers or signatures of
many, if not all, these symptoms. Based on this conjecture, we explore the
possibility of objectively and automatically grading the symptoms of
psychiatric illnesses with reference to various standard psychiatric rating
scales. Using acoustic data from several clinician-patient interviews within
hospital settings, we use non-parametric models to learn and predict the
relations between symptom-ratings and voice. In the process, we show that
different articulatory-phonetic units of speech are able to capture the effects
of different symptoms differently, and use this to establish a plausible
methodology that could be employed for automatically grading psychiatric
symptoms for clinical purposes."
"In this paper we propose the utterance-level Permutation Invariant Training
(uPIT) technique. uPIT is a practically applicable, end-to-end, deep learning
based solution for speaker independent multi-talker speech separation.
Specifically, uPIT extends the recently proposed Permutation Invariant Training
(PIT) technique with an utterance-level cost function, hence eliminating the
need for solving an additional permutation problem during inference, which is
otherwise required by frame-level PIT. We achieve this using Recurrent Neural
Networks (RNNs) that, during training, minimize the utterance-level separation
error, hence forcing separated frames belonging to the same speaker to be
aligned to the same output stream. In practice, this allows RNNs, trained with
uPIT, to separate multi-talker mixed speech without any prior knowledge of
signal duration, number of speakers, speaker identity or gender. We evaluated
uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks
and found that uPIT outperforms techniques based on Non-negative Matrix
Factorization (NMF) and Computational Auditory Scene Analysis (CASA), and
compares favorably with Deep Clustering (DPCL) and the Deep Attractor Network
(DANet). Furthermore, we found that models trained with uPIT generalize well to
unseen speakers and languages. Finally, we found that a single model, trained
with uPIT, can handle both two-speaker, and three-speaker speech mixtures."
"The term gestalt has been widely used in the field of psychology which
defined the perception of human mind to group any object not in part but as a
unified whole. Music in general is polytonic i.e. a combination of a number of
pure tones (frequencies) mixed together in a manner that sounds harmonius. The
study of human brain response due to different frequency groups of acoustic
signal can give us an excellent insight regarding the neural and functional
architecture of brain functions. In this work we have tried to analyze the
effect of different frequency bands of music on the various frequency rhythms
of human brain obtained from EEG data of 5 participants. Four (4) widely
popular Rabindrasangeet clips were subjected to Wavelet Transform method for
extracting five resonant frequency bands from the original music signal. These
resonant frequency bands were presented to the subjects as auditory stimulus
and EEG signals recorded simultaneously in 19 different locations of the brain.
The recorded EEG signals were noise cleaned and subjected to Multifractal
Detrended Fluctuation Analysis (MFDFA) technique on the alpha, theta and gamma
frequency range. Thus, we obtained the complexity values (in the form of
multifractal spectral width) in alpha, theta and gamma EEG rhythms
corresponding to different frequency bands of music. We obtain frequency
specific arousal based response in different lobes of brain as well as in
specific EEG bands corresponding to musical stimuli. This revelation can be of
immense importance when it comes to the field of cognitive music therapy."
"Environmental sound detection is a challenging application of machine
learning because of the noisy nature of the signal, and the small amount of
(labeled) data that is typically available. This work thus presents a
comparison of several state-of-the-art Deep Learning models on the IEEE
challenge on Detection and Classification of Acoustic Scenes and Events (DCASE)
2016 challenge task and data, classifying sounds into one of fifteen common
indoor and outdoor acoustic scenes, such as bus, cafe, car, city center, forest
path, library, train, etc. In total, 13 hours of stereo audio recordings are
available, making this one of the largest datasets available. We perform
experiments on six sets of features, including standard Mel-frequency cepstral
coefficients (MFCC), Binaural MFCC, log Mel-spectrum and two different large-
scale temporal pooling features extracted using OpenSMILE. On these features,
we apply five models: Gaussian Mixture Model (GMM), Deep Neural Network (DNN),
Recurrent Neural Network (RNN), Convolutional Deep Neural Net- work (CNN) and
i-vector. Using the late-fusion approach, we improve the performance of the
baseline 72.5% by 15.6% in 4-fold Cross Validation (CV) avg. accuracy and 11%
in test accuracy, which matches the best result of the DCASE 2016 challenge.
With large feature sets, deep neural network models out- perform traditional
methods and achieve the best performance among all the studied methods.
Consistent with other work, the best performing single model is the
non-temporal DNN model, which we take as evidence that sounds in the DCASE
challenge do not exhibit strong temporal dynamics."
"Most existing neural network models for music generation use recurrent neural
networks. However, the recent WaveNet model proposed by DeepMind shows that
convolutional neural networks (CNNs) can also generate realistic musical
waveforms in the audio domain. Following this light, we investigate using CNNs
for generating melody (a series of MIDI notes) one bar after another in the
symbolic domain. In addition to the generator, we use a discriminator to learn
the distributions of melodies, making it a generative adversarial network
(GAN). Moreover, we propose a novel conditional mechanism to exploit available
prior knowledge, so that the model can generate melodies either from scratch,
by following a chord sequence, or by conditioning on the melody of previous
bars (e.g. a priming melody), among other possibilities. The resulting model,
named MidiNet, can be expanded to generate music with multiple MIDI channels
(i.e. tracks). We conduct a user study to compare the melody of eight-bar long
generated by MidiNet and by Google's MelodyRNN models, each time using the same
priming melody. Result shows that MidiNet performs comparably with MelodyRNN
models in being realistic and pleasant to listen to, yet MidiNet's melodies are
reported to be much more interesting."
"In this paper, we propose a novel technique for direct recognition of
multiple speech streams given the single channel of mixed speech, without first
separating them. Our technique is based on permutation invariant training (PIT)
for automatic speech recognition (ASR). In PIT-ASR, we compute the average
cross entropy (CE) over all frames in the whole utterance for each possible
output-target assignment, pick the one with the minimum CE, and optimize for
that assignment. PIT-ASR forces all the frames of the same speaker to be
aligned with the same output layer. This strategy elegantly solves the label
permutation problem and speaker tracing problem in one shot. Our experiments on
artificially mixed AMI data showed that the proposed approach is very
promising."
"In this paper, we present a time-contrastive learning (TCL)based unsupervised
bottleneck (BN) feature extraction method for speech signals with an
application to speaker verification. The method exploits the temporal structure
of a speech signal and more specifically, it trains deep neural networks (DNNs)
to discriminate temporal events obtained by uniformly segmenting the signal
without using any label information, in contrast to conventional DNN based BN
feature extraction methods that train DNNs using labeled data to discriminate
speakers or passphrases or phones or a combination of them. We consider
different strategies for TCL and its combination with transfer learning.
Experimental results on the RSR2015 database show that the TCL method is
superior to the conventional speaker and pass-phrase discriminant BN feature
and Mel-frequency cepstral coefficients (MFCCs) feature for text-dependent
speaker verification. The unsupervised TCL method further has the advantage of
being able to leverage the huge amount of unlabeled data that are often
available in real life."
"A convolution neural network (CNN) based classification method for broadband
DOA estimation is proposed, where the phase component of the short-time Fourier
transform coefficients of the received microphone signals are directly fed into
the CNN and the features required for DOA estimation are learnt during
training. Since only the phase component of the input is used, the CNN can be
trained with synthesized noise signals, thereby making the preparation of the
training data set easier compared to using speech signals. Through experimental
evaluation, the ability of the proposed noise trained CNN framework to
generalize to speech sources is demonstrated. In addition, the robustness of
the system to noise, small perturbations in microphone positions, as well as
its ability to adapt to different acoustic conditions is investigated using
experiments with simulated and real data."
"In this paper, we will provide a comparison between uniform and random
sampling for speech and music signals. There are various sampling and recovery
methods for audio signals. Here, we only investigate uniform and random schemes
for sampling and basic low-pass filtering and iterative method with adaptive
thresholding for recovery. The simulation results indicate that uniform
sampling with cubic spline interpolation outperforms other sampling and
recovery methods."
"In this chapter we explain briefly the fundamentals of the interactive scores
formalism. Then we develop a solution for implementing the ECO machine by
mixing petri nets and constraints propagation. We also present another solution
for implementing the ECO machine using concurrent constraint programming.
Finally, we present an extension of interactive score with conditional
branching."
"In this study an Artificial Neural Network was trained to classify musical
instruments, using audio samples transformed to the frequency domain. Different
features of the sound, in both time and frequency domain, were analyzed and
compared in relation to how much information that could be derived from that
limited data. The study concluded that in comparison with the base experiment,
that had an accuracy of 93.5%, using the attack only resulted in 80.2% and the
initial 100 Hz in 64.2%."
"While both the data volume and heterogeneity of the digital music content is
huge, it has become increasingly important and convenient to build a
recommendation or search system to facilitate surfacing these content to the
user or consumer community. Most of the recommendation models fall into two
primary species, collaborative filtering based and content based approaches.
Variants of instantiations of collaborative filtering approach suffer from the
common issues of so called ""cold start"" and ""long tail"" problems where there is
not much user interaction data to reveal user opinions or affinities on the
content and also the distortion towards the popular content. Content-based
approaches are sometimes limited by the richness of the available content data
resulting in a heavily biased and coarse recommendation result. In recent
years, the deep neural network has enjoyed a great success in large-scale image
and video recognitions. In this paper, we propose and experiment using deep
convolutional neural network to imitate how human brain processes hierarchical
structures in the auditory signals, such as music, speech, etc., at various
timescales. This approach can be used to discover the latent factor models of
the music based upon acoustic hyper-images that are extracted from the raw
audio waves of music. These latent embeddings can be used either as features to
feed to subsequent models, such as collaborative filtering, or to build
similarity metrics between songs, or to classify music based on the labels for
training such as genre, mood, sentiment, etc."
"There is increasing interest in the use of animal-like robots in applications
such as companionship and pet therapy. However, in the majority of cases it is
only the robot's physical appearance that mimics a given animal. In contrast,
MiRo is the first commercial biomimetic robot to be based on a hardware and
software architecture that is modelled on the biological brain. This paper
describes how MiRo's vocalisation system was designed, not using pre-recorded
animal sounds, but based on the implementation of a real-time parametric
general-purpose mammalian vocal synthesiser tailored to the specific physical
characteristics of the robot. The novel outcome has been the creation of an
'appropriate' voice for MiRo that is perfectly aligned to the physical and
behavioural affordances of the robot, thereby avoiding the 'uncanny valley'
effect and contributing strongly to the effectiveness of MiRo as an interactive
device."
"Deep learning approaches are still not very common in the speaker
verification field. We investigate the possibility of using deep residual
convolutional neural network with spectrograms as an input features in the
text-dependent speaker verification task. Despite the fact that we were not
able to surpass the baseline system in quality, we achieved a quite good
results for such a new approach getting an 5.23% ERR on the RSR2015 evaluation
part. Fusion of the baseline and proposed systems outperformed the best
individual system by 18% relatively."
"In this paper, the uncertainty is defined as the mean square error between a
given enhanced noisy observation vector and the corresponding clean one. Then,
a DNN is trained by using enhanced noisy observation vectors as input and the
uncertainty as output with a training database. In testing, the DNN receives an
enhanced noisy observation vector and delivers the estimated uncertainty. This
uncertainty in employed in combination with a weighted DNN-HMM based speech
recognition system and compared with an existing estimation of the noise
cancelling uncertainty variance based on an additive noise model. Experiments
were carried out with Aurora-4 task. Results with clean, multi-noise and
multi-condition training are presented."
"We show that a Modular Neural Network (MNN) can combine various speech
enhancement modules, each of which is a Deep Neural Network (DNN) specialized
on a particular enhancement job. Differently from an ordinary ensemble
technique that averages variations in models, the propose MNN selects the best
module for the unseen test signal to produce a greedy ensemble. We see this as
Collaborative Deep Learning (CDL), because it can reuse various already-trained
DNN models without any further refining. In the proposed MNN selecting the best
module during run time is challenging. To this end, we employ a speech
AutoEncoder (AE) as an arbitrator, whose input and output are trained to be as
similar as possible if its input is clean speech. Therefore, the AE can gauge
the quality of the module-specific denoised result by seeing its AE
reconstruction error, e.g. low error means that the module output is similar to
clean speech. We propose an MNN structure with various modules that are
specialized on dealing with a specific noise type, gender, and input
Signal-to-Noise Ratio (SNR) value, and empirically prove that it almost always
works better than an arbitrarily chosen DNN module and sometimes as good as an
oracle result."
"The stethoscope is a well-known and widely available diagnostic instrument.
In recent years, many innovative solutions for recording and viewing sounds
from a stethoscope have become available. However, to fully utilize such
devices, there is a need for an automated approach for detecting abnormal lung
sounds, which is better than the existing methods that typically have been
developed and evaluated using a small and non-diverse dataset.
  We propose a machine learning based approach for detecting crackles in lung
sounds recorded using a stethoscope in a large health survey. Our method is
trained and evaluated using 209 files with crackles classified by expert
listeners. Our analysis pipeline is based on features extracted from small
windows in audio files. We evaluated several feature extraction methods and
classifiers. We evaluated the pipeline using a training set of 175 crackle
windows and 208 normal windows.
  We found and evaluated a 5-dimenstional vector with four features from the
time domain and one from the spectrum domain. We evaluated several classifiers
and found SVM with a Radial Basis Function Kernel to perform best for our
5-dimensional feature vector. Our approach had a precision of 86% and recall of
84% for classifying a crackle in a window, which is more accurate than found in
studies of health personnel. The low-dimensional feature vector makes the SVM
very fast. The model can be trained on a regular computer in 1.44 seconds, and
319 crackles can be classified in 1.08 seconds.
  Our approach detects and visualizes individual crackles in recorded audio
files. It is accurate, fast, and has low resource requirements. The approach is
therefore well suited for deployment on smart devices and phones or as a web
application. It can be used to train health personnel or as part of a
smartphone application for Bluetooth stethoscopes."
"Speech signals are complex intermingling of various informative factors, and
this information blending makes decoding any of the individual factors
extremely difficult. A natural idea is to factorize each speech frame into
independent factors, though it turns out to be even more difficult than
decoding each individual factor. A major encumbrance is that the speaker trait,
a major factor in speech signals, has been suspected to be a long-term
distributional pattern and so not identifiable at the frame level. In this
paper, we demonstrated that the speaker factor is also a short-time spectral
pattern and can be largely identified with just a few frames using a simple
deep neural network (DNN). This discovery motivated a cascade deep
factorization (CDF) framework that infers speech factors in a sequential way,
and factors previously inferred are used as conditional variables when
inferring other factors. Our experiment on an automatic emotion recognition
(AER) task demonstrated that this approach can effectively factorize speech
signals, and using these factors, the original speech spectrum can be recovered
with high accuracy. This factorization and reconstruction approach provides a
novel tool for many speech processing tasks."
"This paper studies the detection of bird calls in audio segments using
stacked convolutional and recurrent neural networks. Data augmentation by
blocks mixing and domain adaptation using a novel method of test mixing are
proposed and evaluated in regard to making the method robust to unseen data.
The contributions of two kinds of acoustic features (dominant frequency and log
mel-band energy) and their combinations are studied in the context of bird
audio detection. Our best achieved AUC measure on five cross-validations of the
development data is 95.5% and 88.1% on the unseen evaluation data."
"This paper proposes to use low-level spatial features extracted from
multichannel audio for sound event detection. We extend the convolutional
recurrent neural network to handle more than one type of these multichannel
features by learning from each of them separately in the initial stages. We
show that instead of concatenating the features of each channel into a single
feature vector the network learns sound events in multichannel audio better
when they are presented as separate layers of a volume. Using the proposed
spatial features over monaural features on the same network gives an absolute
F-score improvement of 6.1% on the publicly available TUT-SED 2016 dataset and
2.7% on the TUT-SED 2009 dataset that is fifteen times larger."
"This paper studies the emotion recognition from musical tracks in the
2-dimensional valence-arousal (V-A) emotional space. We propose a method based
on convolutional (CNN) and recurrent neural networks (RNN), having
significantly fewer parameters compared with the state-of-the-art method for
the same task. We utilize one CNN layer followed by two branches of RNNs
trained separately for arousal and valence. The method was evaluated using the
'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for
arousal and 0.268 for valence, which is the best result reported on this
dataset."
"In this paper, we propose the use of spatial and harmonic features in
combination with long short term memory (LSTM) recurrent neural network (RNN)
for automatic sound event detection (SED) task. Real life sound recordings
typically have many overlapping sound events, making it hard to recognize with
just mono channel audio. Human listeners have been successfully recognizing the
mixture of overlapping sound events using pitch cues and exploiting the stereo
(multichannel) audio signal available at their ears to spatially localize these
events. Traditionally SED systems have only been using mono channel audio,
motivated by the human listener we propose to extend them to use multichannel
audio. The proposed SED system is compared against the state of the art mono
channel method on the development subset of TUT sound events detection 2016
database. The usage of spatial and harmonic features are shown to improve the
performance of SED."
"In this work we describe and evaluate methods to learn musical embeddings.
Each embedding is a vector that represents four contiguous beats of music and
is derived from a symbolic representation. We consider autoencoding-based
methods including denoising autoencoders, and context reconstruction, and
evaluate the resulting embeddings on a forward prediction and a classification
task."
"In Acoustic Scene Classification (ASC) two major approaches have been
followed . While one utilizes engineered features such as
mel-frequency-cepstral-coefficients (MFCCs), the other uses learned features
that are the outcome of an optimization algorithm. I-vectors are the result of
a modeling technique that usually takes engineered features as input. It has
been shown that standard MFCCs extracted from monaural audio signals lead to
i-vectors that exhibit poor performance, especially on indoor acoustic scenes.
At the same time, Convolutional Neural Networks (CNNs) are well known for their
ability to learn features by optimizing their filters. They have been applied
on ASC and have shown promising results. In this paper, we first propose a
novel multi-channel i-vector extraction and scoring scheme for ASC, improving
their performance on indoor and outdoor scenes. Second, we propose a CNN
architecture that achieves promising ASC results. Further, we show that
i-vectors and CNNs capture complementary information from acoustic scenes.
Finally, we propose a hybrid system for ASC using multi-channel i-vectors and
CNNs by utilizing a score fusion technique. Using our method, we participated
in the ASC task of the DCASE-2016 challenge. Our hybrid approach achieved 1 st
rank among 49 submissions, substantially improving the previous state of the
art."
"End-to-end learning treats the entire system as a whole adaptable black box,
which, if sufficient data are available, may learn a system that works very
well for the target task. This principle has recently been applied to several
prototype research on speaker verification (SV), where the feature learning and
classifier are learned together with an objective function that is consistent
with the evaluation metric. An opposite approach to end-to-end is feature
learning, which firstly trains a feature learning model, and then constructs a
back-end classifier separately to perform SV. Recently, both approaches
achieved significant performance gains on SV, mainly attributed to the smart
utilization of deep neural networks. However, the two approaches have not been
carefully compared, and their respective advantages have not been well
discussed. In this paper, we compare the end-to-end and feature learning
approaches on a text-independent SV task. Our experiments on a dataset sampled
from the Fisher database and involving 5,000 speakers demonstrated that the
feature learning approach outperformed the end-to-end approach. This is a
strong support for the feature learning approach, at least with data and
computation resources similar to ours."
"This paper proposes a speaker recognition (SRE) task with trivial speech
events, such as cough and laugh. These trivial events are ubiquitous in
conversations and less subjected to intentional change, therefore offering
valuable particularities to discover the genuine speaker from disguised speech.
However, trivial events are often short and idiocratic in spectral patterns,
making SRE extremely difficult. Fortunately, we found a very powerful deep
feature learning structure that can extract highly speaker-sensitive features.
By employing this tool, we studied the SRE performance on three types of
trivial events: cough, laugh and ""Wei"" (a short Chinese ""Hello""). The results
show that there is rich speaker information within these trivial events, even
for cough that is intuitively less speaker distinguishable. With the deep
feature approach, the EER can reach 10%-14% with the three trivial events,
despite their extremely short durations (0.2-1.0 seconds)."
"Existing speaker verification (SV) systems often suffer from performance
degradation if there is any language mismatch between model training, speaker
enrollment, and test. A major cause of this degradation is that most existing
SV methods rely on a probabilistic model to infer the speaker factor, so any
significant change on the distribution of the speech signal will impact the
inference. Recently, we proposed a deep learning model that can learn how to
extract the speaker factor by a deep neural network (DNN). By this feature
learning, an SV system can be constructed with a very simple back-end model. In
this paper, we investigate the robustness of the feature-based SV system in
situations with language mismatch. Our experiments were conducted on a complex
cross-lingual scenario, where the model training was in English, and the
enrollment and test were in Chinese or Uyghur. The experiments demonstrated
that the feature-based system outperformed the i-vector system with a large
margin, particularly with language mismatch between enrollment and test."
"The modeling of speech can be used for speech synthesis and speech
recognition. We present a speech analysis method based on pole-zero modeling of
speech with mixed block sparse and Gaussian excitation. By using a pole-zero
model, instead of the all-pole model, a better spectral fitting can be
expected. Moreover, motivated by the block sparse glottal flow excitation
during voiced speech and the white noise excitation for unvoiced speech, we
model the excitation sequence as a combination of block sparse signals and
white noise. A variational EM (VEM) method is proposed for estimating the
posterior PDFs of the block sparse residuals and point estimates of mod- elling
parameters within a sparse Bayesian learning framework. Compared to
conventional pole-zero and all-pole based methods, experimental results show
that the proposed method has lower spectral distortion and good performance in
reconstructing of the block sparse excitation."
"This paper presents a new approach in understanding how deep neural networks
(DNNs) work by applying homomorphic signal processing techniques. Focusing on
the task of multi-pitch estimation (MPE), this paper demonstrates the
equivalence relation between a generalized cepstrum and a DNN in terms of their
structures and functionality. Such an equivalence relation, together with pitch
perception theories and the recently established
rectified-correlations-on-a-sphere (RECOS) filter analysis, provide an
alternative way in explaining the role of the nonlinear activation function and
the multi-layer structure, both of which exist in a cepstrum and a DNN. To
validate the efficacy of this new approach, a new feature designed in the same
fashion is proposed for pitch salience function. The new feature outperforms
the one-layer spectrum in the MPE task and, as predicted, it addresses the
issue of the missing fundamental effect and also achieves better robustness to
noise."
"In this paper a feature extractor based on Gabor frames and Mallat's
scattering transform, called Gabor scattering, is introduced. This feature
extractor is applied to a simple signal model for audio signals, i.e. a class
of tones consisting of fundamental frequency and its multiples and an according
envelope. Within different layers, different invariances to certain signal
features occur. In this paper we give a mathematical explanation for the first
and the second layer which are illustrated by numerical examples. Deformation
stability of this feature extractor will be shown by using a decoupling
technique, previously suggested for the scattering transform of Cartoon
functions. Here it is used to see if the feature extractor is robust to changes
in spectral shape and frequency modulation."
"High-accuracy speech recognition is especially challenging when large
datasets are not available. It is possible to bridge this gap with careful and
knowledge-driven parsing combined with the biologically inspired CNN and the
learning guarantees of the Vapnik Chervonenkis (VC) theory. This work presents
a Shallow-CNN-HTSVM (Hierarchical Tree Support Vector Machine classifier)
architecture which uses a predefined knowledge-based set of rules with
statistical machine learning techniques. Here we show that gross errors present
even in state-of-the-art systems can be avoided and that an accurate acoustic
model can be built in a hierarchical fashion. The CNN-HTSVM acoustic model
outperforms traditional GMM-HMM models and the HTSVM structure outperforms a
MLP multi-class classifier. More importantly we isolate the performance of the
acoustic model and provide results on both the frame and phoneme level
considering the true robustness of the model. We show that even with a small
amount of data accurate and robust recognition rates can be obtained."
"This manifesto paper will introduce machine listening intelligence, an
integrated research framework for acoustic and musical signals modelling, based
on signal processing, deep learning and computational musicology."
"A central goal in automatic music transcription is to detect individual note
events in music recordings. An important variant is instrument-dependent music
transcription where methods can use calibration data for the instruments in
use. However, despite the additional information, results rarely exceed an
f-measure of 80%. As a potential explanation, the transcription problem can be
shown to be badly conditioned and thus relies on appropriate regularization. A
recently proposed method employs a mixture of simple, convex regularizers (to
stabilize the parameter estimation process) and more complex terms (to
encourage more meaningful structure). In this paper, we present two extensions
to this method. First, we integrate a computational loudness model to better
differentiate real from spurious note detections. Second, we employ
(Bidirectional) Long Short Term Memory networks to re-weight the likelihood of
detected note constellations. Despite their simplicity, our two extensions lead
to a drop of about 35% in note error rate compared to the state-of-the-art."
"Multichannel linear filters, such as the Multichannel Wiener Filter (MWF) and
the Generalized Eigenvalue (GEV) beamformer are popular signal processing
techniques which can improve the speech recognition performance. In this paper,
we present an experimental study on these linear filters in a specific speech
recognition task, namely the CHiME-4 challenge, which features real recordings
in multiple noisy environments. Specifically, the rank-1 MWF is employed for
noise reduction and a new constant residual noise power constraint is derived
which benefits the recognition performance. To fulfill the underlying rank-1
assumption, the speech covariance matrix is reconstructed based on eigenvectors
or generalized eigenvectors. Then the rank-1 constrained MWF is evaluated with
alternative multichannel linear filters under the same framework, which
involves a Bidirectional Long Short-Term Memory (BLSTM) network for mask
estimation. The proposed filter outperforms alternative ones, leading up to a
40% relative Word Error Rate (WER) reduction compared with the baseline
Weighted Delay and Sum (WDAS) beamformer on the real test set, and a 15%
relative WER reduction compared with the GEV-BAN method. The results also
suggest that the speech recognition accuracy correlates more with the feature
variance than with the noise reduction or the speech distortion level."
"The buildup and release of a sense of tension is one of the most essential
aspects of the process of listening to music. A veridical computational model
of perceived musical tension would be an important ingredient for many music
informatics applications. The present paper presents a new approach to
modelling harmonic tension based on a distributed representation of chords. The
starting hypothesis is that harmonic tension as perceived by human listeners is
related, among other things, to the expectedness of harmonic units (chords) in
their local harmonic context. We train a word2vec-type neural network to learn
a vector space that captures contextual similarity and expectedness, and define
a quantitative measure of harmonic tension on top of this. To assess the
veridicality of the model, we compare its outputs on a number of well-defined
chord classes and cadential contexts to results from pertinent empirical
studies in music psychology. Statistical analysis shows that the model's
predictions conform very well with empirical evidence obtained from human
listeners."
"The goal of this contribution is to use a parametric speech synthesis system
for reducing background noise and other interferences from recorded speech
signals. In a first step, Hidden Markov Models of the synthesis system are
trained.
  Two adequate training corpora consisting of text and corresponding speech
files have been set up and cleared of various faults, including inaudible
utterances or incorrect assignments between audio and text data. Those are
tested and compared against each other regarding e.g. flaws in the synthesized
speech, it's naturalness and intelligibility. Thus different voices have been
synthesized, whose quality depends less on the number of training samples used,
but much more on the cleanliness and signal-to-noise ratio of those.
Generalized voice models have been used for synthesis and the results greatly
differ between the two speech corpora.
  Tests regarding the adaptation to different speakers show that a resemblance
to the original speaker is audible throughout all recordings, yet the
synthesized voices sound robotic and unnatural in smaller parts. The spoken
text, however, is usually intelligible, which shows that the models are working
well.
  In a novel approach, speech is synthesized using side information of the
original audio signal, particularly the pitch frequency. Results show an
increase of speech quality and intelligibility in comparison to speech
synthesized solely from text, up to the point of being nearly indistinguishable
from the original."
"Despite the recent success of deep learning for many speech processing tasks,
single-microphone speech separation remains challenging for two main reasons.
One reason is the arbitrary order of the target and masker speakers in the
mixture (permutation problem), and the second is the unknown number of speakers
in the mixture (output dimension problem). We propose a novel deep learning
framework for speech separation that addresses both of these important issues.
We use a neural network to project the time-frequency representation of the
mixture signal into a high-dimensional embedding space. A reference point
(attractor) is created in the embedding space to pull together all the
time-frequency bins that belong to that speaker. The attractor point for a
speaker is formed by finding the centroid of the source in the embedding space
which is then used to determine the source assignment. We propose three methods
for finding the attractor points for each source, including unsupervised
clustering, fixed attractor points, and fixed anchor points in the embedding
space that guide the estimation of attractor points. The objective function for
the network is standard signal reconstruction error which enables end-to-end
operation during both the training and test phases. We evaluate our system on
the Wall Street Journal dataset (WSJ0) on two and three speaker mixtures, and
report comparable or better performance in comparison with other deep learning
methods for speech separation."
"Text-dependent speaker verification is becoming popular in the speaker
recognition society. However, the conventional i-vector framework which has
been successful for speaker identification and other similar tasks works
relatively poorly in this task. Researchers have proposed several new methods
to improve performance, but it is still unclear that which model is the best
choice, especially when the pass-phrases are prompted during enrollment and
test. In this paper, we introduce four modeling methods and compare their
performance on the newly published RedDots dataset. To further explore the
influence of different frame alignments, Viterbi and forward-backward
algorithms are both used in the HMM-based models. Several bottleneck features
are also investigated. Our experiments show that, by explicitly modeling the
lexical content, the HMM-based modeling achieves good results in the
fixed-phrase condition. In the prompted-phrase condition, GMM-HMM and
i-vector/HMM are not as successful. In both conditions, the forward-backward
algorithm brings more benefits to the i-vector/HMM system. Additionally, we
also find that even though bottleneck features perform well for
text-independent speaker verification, they do not outperform MFCCs on the most
challenging Imposter-Correct trials on RedDots."
"The work presented here applies deep learning to the task of automated
cardiac auscultation, i.e. recognizing abnormalities in heart sounds. We
describe an automated heart sound classification algorithm that combines the
use of time-frequency heat map representations with a deep convolutional neural
network (CNN). Given the cost-sensitive nature of misclassification, our CNN
architecture is trained using a modified loss function that directly optimizes
the trade-off between sensitivity and specificity. We evaluated our algorithm
at the 2016 PhysioNet Computing in Cardiology challenge where the objective was
to accurately classify normal and abnormal heart sounds from single, short,
potentially noisy recordings. Our entry to the challenge achieved a final
specificity of 0.95, sensitivity of 0.73 and overall score of 0.84. We achieved
the greatest specificity score out of all challenge entries and, using just a
single CNN, our algorithm differed in overall score by only 0.02 compared to
the top place finisher, which used an ensemble approach."
"Tonal structure is in part conveyed by statistical regularities between
musical events, and research has shown that computational models reflect tonal
structure in music by capturing these regularities in schematic constructs like
pitch histograms. Of the few studies that model the acquisition of perceptual
learning from musical data, most have employed self-organizing models that
learn a topology of static descriptions of musical contexts. Also, the stimuli
used to train these models are often symbolic rather than acoustically faithful
representations of musical material. In this work we investigate whether
sequential predictive models of musical memory (specifically, recurrent neural
networks), trained on audio from commercial CD recordings, induce tonal
knowledge in a similar manner to listeners (as shown in behavioral studies in
music perception). Our experiments indicate that various types of recurrent
neural networks produce musical expectations that clearly convey tonal
structure. Furthermore, the results imply that although implicit knowledge of
tonal structure is a necessary condition for accurate musical expectation, the
most accurate predictive models also use other cues beyond the tonal structure
of the musical context."
"Acoustic Event Classification (AEC) has become a significant task for
machines to perceive the surrounding auditory scene. However, extracting
effective representations that capture the underlying characteristics of the
acoustic events is still challenging. Previous methods mainly focused on
designing the audio features in a 'hand-crafted' manner. Interestingly,
data-learnt features have been recently reported to show better performance. Up
to now, these were only considered on the frame-level. In this paper, we
propose an unsupervised learning framework to learn a vector representation of
an audio sequence for AEC. This framework consists of a Recurrent Neural
Network (RNN) encoder and a RNN decoder, which respectively transforms the
variable-length audio sequence into a fixed-length vector and reconstructs the
input sequence on the generated vector. After training the encoder-decoder, we
feed the audio sequences to the encoder and then take the learnt vectors as the
audio sequence representations. Compared with previous methods, the proposed
method can not only deal with the problem of arbitrary-lengths of audio
streams, but also learn the salient information of the sequence. Extensive
evaluation on a large-size acoustic event database is performed, and the
empirical results demonstrate that the learnt audio sequence representation
yields a significant performance improvement by a large margin compared with
other state-of-the-art hand-crafted sequence features for AEC."
"Speech emotion recognition (SER) is to study the formation and change of
speaker's emotional state from the speech signal perspective, so as to make the
interaction between human and computer more intelligent. SER is a challenging
task that has encountered the problem of less training data and low prediction
accuracy. Here we propose a data augmentation algorithm based on the imaging
principle of the retina and convex lens, to acquire the different sizes of
spectrogram and increase the amount of training data by changing the distance
between the spectrogram and the convex lens. Meanwhile, with the help of deep
learning to get the high-level features, we propose the Deep Retinal
Convolution Neural Networks (DRCNNs) for SER and achieve the average accuracy
over 99%. The experimental results indicate that DRCNNs outperforms the
previous studies in terms of both the number of emotions and the accuracy of
recognition. Predictably, our results will dramatically improve human-computer
interaction."
"This paper describes an object-oriented approach to music composition and
sound design. The approach unifies the processes of music making and instrument
building by using similar logic, objects, and procedures. The composition
modules use an abstract representation of musical data, which can be easily
mapped onto different synthesis languages or a traditionally notated score. An
abstract base class is used to derive classes on different time scales. Objects
can be related to act across time scales, as well as across an entire piece,
and relationships between similar objects can replicate traditional music
operations or introduce new ones. The DISCO (Digital Instrument for
Sonification and Composition) system is an open-ended work in progress."
"This article describes a collaborative project between researchers in the
Mathematics and Computer Science Division at Argonne National Laboratory and
the Computer Music Project of the University of Illinois at Urbana-Champaign.
The project focuses on the use of sound for the exploration and analysis of
complex data sets in scientific computing. The article addresses digital sound
synthesis in the context of DIASS (Digital Instrument for Additive Sound
Synthesis) and sound visualization in a virtual-reality environment by means of
M4CAVE. It describes the procedures and preliminary results of some experiments
in scientific sonification and sound visualization."
"We present a fully automatic method for music classification, based only on
compression of strings that represent the music pieces. The method uses no
background knowledge about music whatsoever: it is completely general and can,
without change, be used in different areas like linguistic classification and
genomics. It is based on an ideal theory of the information content in
individual objects (Kolmogorov complexity), information distance, and a
universal similarity metric. Experiments show that the method distinguishes
reasonably well between various musical genres and can even cluster pieces by
composer."
"This paper presents a novel application of speech emotion recognition:
estimation of the level of conversational engagement between users of a voice
communication system. We begin by using machine learning techniques, such as
the support vector machine (SVM), to classify users' emotions as expressed in
individual utterances. However, this alone fails to model the temporal and
interactive aspects of conversational engagement. We therefore propose the use
of a multilevel structure based on coupled hidden Markov models (HMM) to
estimate engagement levels in continuous natural speech. The first level is
comprised of SVM-based classifiers that recognize emotional states, which could
be (e.g.) discrete emotion types or arousal/valence levels. A high-level HMM
then uses these emotional states as input, estimating users' engagement in
conversation by decoding the internal states of the HMM. We report experimental
results obtained by applying our algorithms to the LDC Emotional Prosody and
CallFriend speech corpora."
"GENESIS3 is the new version of the GENESIS software environment for musical
creation by means of mass-interaction physics network modeling. It was
designed, and developed from scratch, in hindsight of more than 10 years
working on and using the previous version. We take the opportunity of this
birth to provide in this article (1) an analysis of the peculiarities in
GENESIS, aiming at highlighting its core ?software paradigm?; and (2) an update
on the features of the new version as compared to the last."
"As an applicaton of sonification, a simulation of the early universe was
developed to portray a phase transition that occurred shortly after the Big
Bang. The Standard Model of particle physics postulates that a hypothetical
particle, the Higgs boson, is responsible for the breaking of the symmetry
between the electromagnetic force and the weak force. This phase transition may
have been responsible for triggering Baryogenesis, the generation of an
abundance of matter over anti-matter. This hypothesis is known as Electroweak
Baryogenesis. In this simulation, aspects of bubble nucleation in Standard
Model Electroweak Baryogenesis were examined and modeled using Mathematica, and
sonified using SuperCollider3. The resulting simulation, which has been used
for pedagogical purposes by one of the authors, suggests interesting
possibilities for the integration of science and aesthetics as well as auditory
perception. The sonification component in particular also had the unexpected
benefit of being useful in debugging the Mathematica code."
"Representations in the auditory cortex might be based on mechanisms similar
to the visual ventral stream; modules for building invariance to
transformations and multiple layers for compositionality and selectivity. In
this paper we propose the use of such computational modules for extracting
invariant and discriminative audio representations. Building on a theory of
invariance in hierarchical architectures, we propose a novel, mid-level
representation for acoustical signals, using the empirical distributions of
projections on a set of templates and their transformations. Under the
assumption that, by construction, this dictionary of templates is composed from
similar classes, and samples the orbit of variance-inducing signal
transformations (such as shift and scale), the resulting signature is
theoretically guaranteed to be unique, invariant to transformations and stable
to deformations. Modules of projection and pooling can then constitute layers
of deep networks, for learning composite representations. We present the main
theoretical and computational aspects of a framework for unsupervised learning
of invariant audio representations, empirically evaluated on music genre
classification."
"Audio restoration is effectively achieved by using low complexity algorithm
called AMP. This algorithm has fast convergence and has lower computation
intensity making it suitable for audio recovery problems. This paper focuses on
restoring an audio signal by using VLSI architecture called AMP-M that
implements AMP algorithm. This architecture employs MAC unit with fixed bit
Wallace tree multiplier, FFT-MUX and various memory units (RAM) for audio
restoration. VLSI and FPGA implementation results shows that reduced area, high
throughput, low power is achieved making it suitable for real time audio
recovery problems. Prominent examples are Magnetic Resonance Imaging (MRI),
Radar and Wireless Communications."
"Despite surveillance systems are becoming increasingly ubiquitous in our
living environment, automated surveillance, currently based on video sensory
modality and machine intelligence, lacks most of the time the robustness and
reliability required in several real applications. To tackle this issue, audio
sensory devices have been taken into account, both alone or in combination with
video, giving birth, in the last decade, to a considerable amount of research.
In this paper audio-based automated surveillance methods are organized into a
comprehensive survey: a general taxonomy, inspired by the more widespread video
surveillance field, is proposed in order to systematically describe the methods
covering background subtraction, event classification, object tracking and
situation analysis. For each of these tasks, all the significant works are
reviewed, detailing their pros and cons and the context for which they have
been proposed. Moreover, a specific section is devoted to audio features,
discussing their expressiveness and their employment in the above described
tasks. Differently, from other surveys on audio processing and analysis, the
present one is specifically targeted to automated surveillance, highlighting
the target applications of each described methods and providing the reader
tables and schemes useful to retrieve the most suited algorithms for a specific
requirement."
"We present a new system for simultaneous estimation of keys, chords, and bass
notes from music audio. It makes use of a novel chromagram representation of
audio that takes perception of loudness into account. Furthermore, it is fully
based on machine learning (instead of expert knowledge), such that it is
potentially applicable to a wider range of genres as long as training data is
available. As compared to other models, the proposed system is fast and memory
efficient, while achieving state-of-the-art performance."
"Recognition systems are commonly designed to authenticate users at the access
control levels of a system. A number of voice recognition methods have been
developed using a pitch estimation process which are very vulnerable in low
Signal to Noise Ratio (SNR) environments thus, these programs fail to provide
the desired level of accuracy and robustness. Also, most text independent
speaker recognition programs are incapable of coping with unauthorized attempts
to gain access by tampering with the samples or reference database. The
proposed text-independent voice recognition system makes use of multilevel
cryptography to preserve data integrity while in transit or storage. Encryption
and decryption follow a transform based approach layered with pseudorandom
noise addition whereas for pitch detection, a modified version of the
autocorrelation pitch extraction algorithm is used. The experimental results
show that the proposed algorithm can decrypt the signal under test with
exponentially reducing Mean Square Error over an increasing range of SNR.
Further, it outperforms the conventional algorithms in actual identification
tasks even in noisy environments. The recognition rate thus obtained using the
proposed method is compared with other conventional methods used for speaker
identification."
"Musical counterpoint, a musical technique in which two or more independent
melodies are played simultaneously with the goal of creating harmony, has been
around since the baroque era. However, to our knowledge computational
generation of aesthetically pleasing linear counterpoint based on subjective
fitness assessment has not been explored by the evolutionary computation
community (although generation using objective fitness has been attempted in
quite a few cases). The independence of contrapuntal melodies and the
subjective nature of musical aesthetics provide an excellent platform for the
application of genetic algorithms. In this paper, a genetic algorithm approach
to generating contrapuntal melodies is explained, with a description of the
various musical heuristics used and of how variable-length chromosome strings
are used to avoid generating ""jerky"" rhythms and melodic phrases, as well as
how subjectivity is incorporated into the algorithm's fitness measures. Next,
results from empirical testing of the algorithm are presented, with a focus on
how a user's musical sophistication influences their experience. Lastly,
further musical and compositional applications of the algorithm are discussed
along with planned future work on the algorithm."
"This work examines a semi-blind single-channel source separation problem. Our
specific aim is to separate one source whose local structure is approximately
known, from another a priori unspecified background source, given only a single
linear combination of the two sources. We propose a separation technique based
on local sparse approximations along the lines of recent efforts in sparse
representations and dictionary learning. A key feature of our procedure is the
online learning of dictionaries (using only the data itself) to sparsely model
the background source, which facilitates its separation from the
partially-known source. Our approach is applicable to source separation
problems in various application domains; here, we demonstrate the performance
of our proposed approach via simulation on a stylized audio source separation
task."
"In our work we define a new algebra of operators as a substitute for fuzzy
logic. Its primary purpose is for construction of binary discriminators for
phonemes based on spectral content. It is optimized for design of
non-parametric computational circuits, and makes uses of 4 operations: $\min$,
$\max$, the difference and generalized additively homogenuous means."
"$KS$-algebra consists of expressions constructed with four kinds operations,
the minimum, maximum, difference and additively homogeneous generalized means.
Five families of $Z$-classifiers are investigated on binary classification
tasks between English phonemes. It is shown that the classifiers are able to
reflect well known formant characteristics of vowels, while having very small
Kolmogoroff's complexity."
"In a sensor network with remote sensor devices, it is important to have a
method that can accurately localize a sound event with a small amount of data
transmitted from the sensors. In this paper, we propose a novel method for
localization of a sound source using compressive sensing. Instead of sampling a
large amount of data at the Nyquist sampling rate in time domain, the acoustic
sensors take compressive measurements integrated in time. The compressive
measurements can be used to accurately compute the location of a sound source."
"A scattering transform defines a locally translation invariant representation
which is stable to time-warping deformations. It extends MFCC representations
by computing modulation spectrum coefficients of multiple orders, through
cascades of wavelet convolutions and modulus operators. Second-order scattering
coefficients characterize transient phenomena such as attacks and amplitude
modulation. A frequency transposition invariant representation is obtained by
applying a scattering transform along log-frequency. State-the-of-art
classification results are obtained for musical genre and phone classification
on GTZAN and TIMIT databases, respectively."
"In this paper, we complete the study of the geometry of the TDOA map that
encodes the noiseless model for the localization of a source from the range
differences between three receivers in a plane, by computing the Cartesian
equation of the bifurcation curve in terms of the positions of the receivers.
From that equation, we can compute its real asymptotic lines. The present
manuscript completes the analysis of [Inverse Problems, Vol. 30, Number 3,
Pages 035004]. Our result is useful to check if a source belongs or is closed
to the bifurcation curve, where the localization in a noisy scenario is
ambiguous."
"Generally audio news broadcast on radio is com- posed of music, commercials,
news from correspondents and recorded statements in addition to the actual news
read by the newsreader. When news transcripts are available, automatic
segmentation of audio news broadcast to time align the audio with the text
transcription to build frugal speech corpora is essential. We address the
problem of identifying segmentation in the audio news broadcast corresponding
to the news read by the newsreader so that they can be mapped to the text
transcripts. The existing techniques produce sub-optimal solutions when used to
extract newsreader read segments. In this paper, we propose a new technique
which is able to identify the acoustic change points reliably using an acoustic
Self Similarity Matrix (SSM). We describe the two pass technique in detail and
verify its performance on real audio news broadcast of All India Radio for
different languages."
"The paper presents the capability of an HMM-based TTS system to produce
Bengali speech. In this synthesis method, trajectories of speech parameters are
generated from the trained Hidden Markov Models. A final speech waveform is
synthesized from those speech parameters. In our experiments, spectral
properties were represented by Mel Cepstrum Coefficients. Both the training and
synthesis issues are investigated in this paper using annotated Bengali speech
database. Experimental evaluation depicts that the developed text-to-speech
system is capable of producing adequately natural speech in terms of
intelligibility and intonation for Bengali."
"We present the concept of an acoustic rake receiver---a microphone beamformer
that uses echoes to improve the noise and interference suppression. The rake
idea is well-known in wireless communications; it involves constructively
combining different multipath components that arrive at the receiver antennas.
Unlike spread-spectrum signals used in wireless communications, speech signals
are not orthogonal to their shifts. Therefore, we focus on the spatial
structure, rather than temporal. Instead of explicitly estimating the channel,
we create correspondences between early echoes in time and image sources in
space. These multiple sources of the desired and the interfering signal offer
additional spatial diversity that we can exploit in the beamformer design.
  We present several ""intuitive"" and optimal formulations of acoustic rake
receivers, and show theoretically and numerically that the rake formulation of
the maximum signal-to-interference-and-noise beamformer offers significant
performance boosts in terms of noise and interference suppression. Beyond
signal-to-noise ratio, we observe gains in terms of the \emph{perceptual
evaluation of speech quality} (PESQ) metric for the speech quality. We
accompany the paper by the complete simulation and processing chain written in
Python. The code and the sound samples are available online at
\url{http://lcav.github.io/AcousticRakeReceiver/}."
"This paper addresses the problem of localizing audio sources using binaural
measurements. We propose a supervised formulation that simultaneously localizes
multiple sources at different locations. The approach is intrinsically
efficient because, contrary to prior work, it relies neither on source
separation, nor on monaural segregation. The method starts with a training
stage that establishes a locally-linear Gaussian regression model between the
directional coordinates of all the sources and the auditory features extracted
from binaural measurements. While fixed-length wide-spectrum sounds (white
noise) are used for training to reliably estimate the model parameters, we show
that the testing (localization) can be extended to variable-length
sparse-spectrum sounds (such as speech), thus enabling a wide range of
realistic applications. Indeed, we demonstrate that the method can be used for
audio-visual fusion, namely to map speech signals onto images and hence to
spatially align the audio and visual modalities, thus enabling to discriminate
between speaking and non-speaking faces. We release a novel corpus of real-room
recordings that allow quantitative evaluation of the co-localization method in
the presence of one or two sound sources. Experiments demonstrate increased
accuracy and speed relative to several state-of-the-art methods."
"We trained a Siamese network with multi-task same/different information on a
speech dataset, and found that it was possible to share a network for both
tasks without a loss in performance. The first task was to discriminate between
two same or different words, and the second was to discriminate between two
same or different talkers."
"This paper proposes a novel framework for unsupervised audio source
separation using a deep autoencoder. The characteristics of unknown source
signals mixed in the mixed input is automatically by properly configured
autoencoders implemented by a network with many layers, and separated by
clustering the coefficient vectors in the code layer. By investigating the
weight vectors to the final target, representation layer, the primitive
components of the audio signals in the frequency domain are observed. By
clustering the activation coefficients in the code layer, the previously
unknown source signals are segregated. The original source sounds are then
separated and reconstructed by using code vectors which belong to different
clusters. The restored sounds are not perfect but yield promising results for
the possibility in the success of many practical applications."
"A new algorithm for voice automatic syllabic splitting in the Portuguese
language is proposed, which is based on the envelope of the speech signal of
the input audio file. A computational implementation in MatlabTM is presented
and made available at the URL
http://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its
straightforwardness, the proposed method is very attractive for embedded
systems (e.g. i-phones). It can also be used as a screen to assist more
sophisticated methods. Voice excerpts containing more than one syllable and
identified by the same envelope are named as super-syllables and they are
subsequently separated. The results indicate which samples corresponds to the
beginning and end of each detected syllable. Preliminary tests were performed
to fifty words at an identification rate circa 70% (further improvements may be
incorporated to treat particular phonemes). This algorithm is also useful in
voice command systems, as a tool in the teaching of Portuguese language or even
for patients with speech pathology."
"A system is presented that segments, clusters and predicts musical audio in
an unsupervised manner, adjusting the number of (timbre) clusters
instantaneously to the audio input. A sequence learning algorithm adapts its
structure to a dynamically changing clustering tree. The flow of the system is
as follows: 1) segmentation by onset detection, 2) timbre representation of
each segment by Mel frequency cepstrum coefficients, 3) discretization by
incremental clustering, yielding a tree of different sound classes (e.g.
instruments) that can grow or shrink on the fly driven by the instantaneous
sound events, resulting in a discrete symbol sequence, 4) extraction of
statistical regularities of the symbol sequence, using hierarchical N-grams and
the newly introduced conceptual Boltzmann machine, and 5) prediction of the
next sound event in the sequence. The system's robustness is assessed with
respect to complexity and noisiness of the signal. Clustering in isolation
yields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singing
voice and drums. Onset detection jointly with clustering achieve an ARI of
81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% /
39.2%."
"From a machine learning perspective, the human ability localize sounds can be
modeled as a non-parametric and non-linear regression problem between binaural
spectral features of sound received at the ears (input) and their sound-source
directions (output). The input features can be summarized in terms of the
individual's head-related transfer functions (HRTFs) which measure the spectral
response between the listener's eardrum and an external point in $3$D. Based on
these viewpoints, two related problems are considered: how can one achieve an
optimal sampling of measurements for training sound-source localization (SSL)
models, and how can SSL models be used to infer the subject's HRTFs in
listening tests. First, we develop a class of binaural SSL models based on
Gaussian process regression and solve a \emph{forward selection} problem that
finds a subset of input-output samples that best generalize to all SSL
directions. Second, we use an \emph{active-learning} approach that updates an
online SSL model for inferring the subject's SSL errors via headphones and a
graphical user interface. Experiments show that only a small fraction of HRTFs
are required for $5^{\circ}$ localization accuracy and that the learned HRTFs
are localized closer to their intended directions than non-individualized
HRTFs."
"Monaural source separation is important for many real world applications. It
is challenging because, with only a single channel of information available,
without any constraints, an infinite number of solutions are possible. In this
paper, we explore joint optimization of masking functions and deep recurrent
neural networks for monaural source separation tasks, including monaural speech
separation, monaural singing voice separation, and speech denoising. The joint
optimization of the deep recurrent neural networks with an extra masking layer
enforces a reconstruction constraint. Moreover, we explore a discriminative
criterion for training neural networks to further enhance the separation
performance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT
datasets for speech separation, singing voice separation, and speech denoising
tasks, respectively. Our approaches achieve 2.30--4.98 dB SDR gain compared to
NMF models in the speech separation task, 2.30--2.48 dB GNSDR gain and
4.32--5.42 dB GSIR gain compared to existing models in the singing voice
separation task, and outperform NMF and DNN baselines in the speech denoising
task."
"Given the large number of new musical tracks released each year, automated
approaches to plagiarism detection are essential to help us track potential
violations of copyright. Most current approaches to plagiarism detection are
based on musical similarity measures, which typically ignore the issue of
polyphony in music. We present a novel feature space for audio derived from
compositional modelling techniques, commonly used in signal separation, that
provides a mechanism to account for polyphony without incurring an inordinate
amount of computational overhead. We employ this feature representation in
conjunction with traditional audio feature representations in a classification
framework which uses an ensemble of distance features to characterize pairs of
songs as being plagiarized or not. Our experiments on a database of about 3000
musical track pairs show that the new feature space characterization produces
significant improvements over standard baselines."
"In the process of recording, storage and transmission of time-domain audio
signals, errors may be introduced that are difficult to correct in an
unsupervised way. Here, we train a convolutional deep neural network to
re-synthesize input time-domain speech signals at its output layer. We then use
this abstract transformation, which we call a deep transform (DT), to perform
probabilistic re-synthesis on further speech (of the same speaker) which has
been degraded. Using the convolutive DT, we demonstrate the recovery of speech
audio that has been subject to extreme degradation. This approach may be useful
for correction of errors in communications devices."
"In cocktail party listening scenarios, the human brain is able to separate
competing speech signals. However, the signal processing implemented by the
brain to perform cocktail party listening is not well understood. Here, we
trained two separate convolutive autoencoder deep neural networks (DNN) to
separate monaural and binaural mixtures of two concurrent speech streams. We
then used these DNNs as convolutive deep transform (CDT) devices to perform
probabilistic re-synthesis. The CDTs operated directly in the time-domain. Our
simulations demonstrate that very simple neural networks are capable of
exploiting monaural and binaural information available in a cocktail party
listening scenario."
"Separation of competing speech is a key challenge in signal processing and a
feat routinely performed by the human auditory brain. A long standing benchmark
of the spectrogram approach to source separation is known as the ideal binary
mask. Here, we train a convolutional deep neural network, on a two-speaker
cocktail party problem, to make probabilistic predictions about binary masks.
Our results approach ideal binary mask performance, illustrating that
relatively simple deep neural networks are capable of robust binary mask
prediction. We also illustrate the trade-off between prediction statistics and
separation quality."
"Convolutional deep neural networks (DNN) are state of the art in many
engineering problems but have not yet addressed the issue of how to deal with
complex spectrograms. Here, we use circular statistics to provide a convenient
probabilistic estimate of spectrogram phase in a complex convolutional DNN. In
a typical cocktail party source separation scenario, we trained a convolutional
DNN to re-synthesize the complex spectrograms of two source speech signals
given a complex spectrogram of the monaural mixture - a discriminative deep
transform (DT). We then used this complex convolutional DT to obtain
probabilistic estimates of the magnitude and phase components of the source
spectrograms. Our separation results are on a par with equivalent binary-mask
based non-complex separation approaches."
"Identification and extraction of singing voice from within musical mixtures
is a key challenge in source separation and machine audition. Recently, deep
neural networks (DNN) have been used to estimate 'ideal' binary masks for
carefully controlled cocktail party speech separation problems. However, it is
not yet known whether these methods are capable of generalizing to the
discrimination of voice and non-voice in the context of musical mixtures. Here,
we trained a convolutional DNN (of around a billion parameters) to provide
probabilistic estimates of the ideal binary mask for separation of vocal sounds
from real-world musical mixtures. We contrast our DNN results with more
traditional linear methods. Our approach may be useful for automatic removal of
vocal sounds from musical mixtures for 'karaoke' type applications."
"The Teager-Kaiser energy operator (TKO) belongs to a class of autocorrelators
and their linear combination that can track the instantaneous energy of a
nonstationary sinusoidal signal source. TKO-based monocomponent AM-FM
demodulation algorithms work under the basic assumption that the operator
outputs are always positive. In the absence of noise, this is assured for pure
sinusoidal inputs and the instantaneous property is also guaranteed. Noise
invalidates both of these, particularly under small signal conditions.
Post-detection filtering and thresholding are of use to reestablish these at
the cost of some time to acquire. Key questions are: (a) how many samples must
one use and (b) how much noise power at the detector input can one tolerate.
Results of study of the role of delay and the limits imposed by additive
Gaussian noise are presented along with the computation of the cumulants and
probability density functions of the individual quadratic forms and their
ratios."
"Recognizing emotion from speech has become one the active research themes in
speech processing and in applications based on human-computer interaction. This
paper conducts an experimental study on recognizing emotions from human speech.
The emotions considered for the experiments include neutral, anger, joy and
sadness. The distinuishability of emotional features in speech were studied
first followed by emotion classification performed on a custom dataset. The
classification was performed for different classifiers. One of the main feature
attribute considered in the prepared dataset was the peak-to-peak distance
obtained from the graphical representation of the speech signals. After
performing the classification tests on a dataset formed from 30 different
subjects, it was found that for getting better accuracy, one should consider
the data collected from one person rather than considering the data from a
group of people."
"This paper presents the Speech Technology Center (STC) systems submitted to
Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof)
Challenge 2015. In this work we investigate different acoustic feature spaces
to determine reliable and robust countermeasures against spoofing attacks. In
addition to the commonly used front-end MFCC features we explored features
derived from phase spectrum and features based on applying the multiresolution
wavelet transform. Similar to state-of-the-art ASV systems, we used the
standard TV-JFA approach for probability modelling in spoofing detection
systems. Experiments performed on the development and evaluation datasets of
the Challenge demonstrate that the use of phase-related and wavelet-based
features provides a substantial input into the efficiency of the resulting STC
systems. In our research we also focused on the comparison of the linear (SVM)
and nonlinear (DBN) classifiers."
"Most speech enhancement algorithms make use of the short-time Fourier
transform (STFT), which is a simple and flexible time-frequency decomposition
that estimates the short-time spectrum of a signal. However, the duration of
short STFT frames are inherently limited by the nonstationarity of speech
signals. The main contribution of this paper is a demonstration of speech
enhancement and automatic speech recognition in the presence of reverberation
and noise by extending the length of analysis windows. We accomplish this
extension by performing enhancement in the short-time fan-chirp transform
(STFChT) domain, an overcomplete time-frequency representation that is coherent
with speech signals over longer analysis window durations than the STFT. This
extended coherence is gained by using a linear model of fundamental frequency
variation of voiced speech signals. Our approach centers around using a
single-channel minimum mean-square error log-spectral amplitude (MMSE-LSA)
estimator proposed by Habets, which scales coefficients in a time-frequency
domain to suppress noise and reverberation. In the case of multiple
microphones, we preprocess the data with either a minimum variance
distortionless response (MVDR) beamformer, or a delay-and-sum beamformer (DSB).
We evaluate our algorithm on both speech enhancement and recognition tasks for
the REVERB challenge dataset. Compared to the same processing done in the STFT
domain, our approach achieves significant improvement in terms of objective
enhancement metrics (including PESQ---the ITU-T standard measurement for speech
quality). In terms of automatic speech recognition (ASR) performance as
measured by word error rate (WER), our experiments indicate that the STFT with
a long window is more effective for ASR."
"In this manuscript, we formulate the problem of denoising Time Differences of
Arrival (TDOAs) in the TDOA space, i.e. the Euclidean space spanned by TDOA
measurements. The method consists of pre-processing the TDOAs with the purpose
of reducing the measurement noise. The complete set of TDOAs (i.e., TDOAs
computed at all microphone pairs) is known to form a redundant set, which lies
on a linear subspace in the TDOA space. Noise, however, prevents TDOAs from
lying exactly on this subspace. We therefore show that TDOA denoising can be
seen as a projection operation that suppresses the component of the noise that
is orthogonal to that linear subspace. We then generalize the projection
operator also to the cases where the set of TDOAs is incomplete. We
analytically show that this operator improves the localization accuracy, and we
further confirm that via simulation."
"We approach the challenging problem of generating highlights from sports
broadcasts utilizing audio information only. A language-independent,
multi-stage classification approach is employed for detection of key acoustic
events which then act as a platform for summarization of highlight scenes.
Objective results and human experience indicate that our system is highly
efficient."
"Traditional methods to tackle many music information retrieval tasks
typically follow a two-step architecture: feature engineering followed by a
simple learning algorithm. In these ""shallow"" architectures, feature
engineering and learning are typically disjoint and unrelated. Additionally,
feature engineering is difficult, and typically depends on extensive domain
expertise.
  In this paper, we present an application of convolutional neural networks for
the task of automatic musical instrument identification. In this model, feature
extraction and learning algorithms are trained together in an end-to-end
fashion. We show that a convolutional neural network trained on raw audio can
achieve performance surpassing traditional methods that rely on hand-crafted
features."
"A codec for compression of music signals is proposed. The method belongs to
the class of transform lossy compression. It is conceived to be applied in the
high quality recovery range though. The transformation, endowing the codec with
its distinctive feature, relies on the ability to construct high quality sparse
approximation of music signals. This is achieved by a redundant trigonometric
dictionary and a dedicated pursuit strategy. The potential of the approach is
illustrated by comparison with the OGG Vorbis format, on a sample consisting of
clips of melodic music. The comparison evidences remarkable improvements in
compression performance for the identical quality of the decompressed signal."
"We introduce a novel family of adaptive robust equalizers for highly
challenging underwater acoustic (UWA) channel equalization. Since the
underwater environment is highly non-stationary and subjected to impulsive
noise, we use adaptive filtering techniques based on a relative logarithmic
cost function inspired by the competitive methods from the online learning
literature. To improve the convergence performance of the conventional linear
equalization methods, while mitigating the stability issues, we intrinsically
combine different norms of the error in the cost function, using logarithmic
functions. Hence, we achieve a comparable convergence performance to least mean
fourth (LMF) equalizer, while significantly enhancing the stability performance
in such an adverse communication medium. We demonstrate the performance of our
algorithms through highly realistic experiments performed on accurately
simulated underwater acoustic channels."
"This paper discusses real-time alignment of audio signals of music
performance to the corresponding score (a.k.a. score following) which can
handle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips)
in performances. This type of score following is particularly useful in
automatic accompaniment for practices and rehearsals, where errors and
repeats/skips are often made. Simple extensions of the algorithms previously
proposed in the literature are not applicable in these situations for scores of
practical length due to the problem of large computational complexity. To cope
with this problem, we present two hidden Markov models of monophonic
performance with errors and arbitrary repeats/skips, and derive efficient
score-following algorithms with an assumption that the prior probability
distributions of score positions before and after repeats/skips are independent
from each other. We confirmed real-time operation of the algorithms with music
scores of practical length (around 10000 notes) on a modern laptop and their
tracking ability to the input performance within 0.7 s on average after
repeats/skips in clarinet performance data. Further improvements and extension
for polyphonic signals are also discussed."
"Categorization is crucial for content description in archiving of music
signals. On many occasions, human brain fails to classify the instruments
properly just by listening to their sounds which is evident from the human
response data collected during our experiment. Some previous attempts to
categorize several musical instruments using various linear analysis methods
required a number of parameters to be determined. In this work, we attempted to
categorize a number of string instruments according to their mode of playing
using latest-state-of-the-art robust non-linear methods. For this, 30 second
sound signals of 26 different string instruments from all over the world were
analyzed with the help of non linear multifractal analysis (MFDFA) technique.
The spectral width obtained from the MFDFA method gives an estimate of the
complexity of the signal. From the variation of spectral width, we observed
distinct clustering among the string instruments according to their mode of
playing. Also there is an indication that similarity in the structural
configuration of the instruments is playing a major role in the clustering of
their spectral width. The observations and implications are discussed in
detail."
"Can music be represented as a meaningful geometric and topological object? In
this paper, we propose a strategy to describe some music features as a
polyhedral surface obtained by a simplicial interpretation of the
\textit{Tonnetz}. The \textit{Tonnetz} is a graph largely used in computational
musicology to describe the harmonic relationships of notes in equal tuning. In
particular, we use persistent homology in order to describe the
\textit{persistent} properties of music encoded in the aforementioned model.
Both the relevance and the characteristics of this approach are discussed by
analyzing some paradigmatic compositional styles. Eventually, the task of
automatic music style classification is addressed by computing the hierarchical
clustering of the topological fingerprints associated with some collections of
compositions."
"OBJECTIVE: We aim to extract and denoise the attended speaker in a noisy,
two-speaker acoustic scenario, relying on microphone array recordings from a
binaural hearing aid, which are complemented with electroencephalography (EEG)
recordings to infer the speaker of interest. METHODS: In this study, we propose
a modular processing flow that first extracts the two speech envelopes from the
microphone recordings, then selects the attended speech envelope based on the
EEG, and finally uses this envelope to inform a multi-channel speech separation
and denoising algorithm. RESULTS: Strong suppression of interfering
(unattended) speech and background noise is achieved, while the attended speech
is preserved. Furthermore, EEG-based auditory attention detection (AAD) is
shown to be robust to the use of noisy speech signals. CONCLUSIONS: Our results
show that AAD-based speaker extraction from microphone array recordings is
feasible and robust, even in noisy acoustic environments, and without access to
the clean speech signals to perform EEG-based AAD. SIGNIFICANCE: Current
research on AAD always assumes the availability of the clean speech signals,
which limits the applicability in real settings. We have extended this research
to detect the attended speaker even when only microphone recordings with noisy
speech mixtures are available. This is an enabling ingredient for new
brain-computer interfaces and effective filtering schemes in neuro-steered
hearing prostheses. Here, we provide a first proof of concept for EEG-informed
attended speaker extraction and denoising."
"We propose two novel techniques --- stacking bottleneck features and minimum
generation error training criterion --- to improve the performance of deep
neural network (DNN)-based speech synthesis. The techniques address the related
issues of frame-by-frame independence and ignorance of the relationship between
static and dynamic features, within current typical DNN-based synthesis
frameworks. Stacking bottleneck features, which are an acoustically--informed
linguistic representation, provides an efficient way to include more detailed
linguistic context at the input. The minimum generation error training
criterion minimises overall output trajectory error across an utterance, rather
than minimising the error per frame independently, and thus takes into account
the interaction between static and dynamic features. The two techniques can be
easily combined to further improve performance. We present both objective and
subjective results that demonstrate the effectiveness of the proposed
techniques. The subjective results show that combining the two techniques leads
to significantly more natural synthetic speech than from conventional DNN or
long short-term memory (LSTM) recurrent neural network (RNN) systems."
"In this paper we describe the recent advancements made in the IBM i-vector
speaker recognition system for conversational speech. In particular, we
identify key techniques that contribute to significant improvements in
performance of our system, and quantify their contributions. The techniques
include: 1) a nearest-neighbor discriminant analysis (NDA) approach that is
formulated to alleviate some of the limitations associated with the
conventional linear discriminant analysis (LDA) that assumes Gaussian
class-conditional distributions, 2) the application of speaker- and
channel-adapted features, which are derived from an automatic speech
recognition (ASR) system, for speaker recognition, and 3) the use of a deep
neural network (DNN) acoustic model with a large number of output units (~10k
senones) to compute the frame-level soft alignments required in the i-vector
estimation process. We evaluate these techniques on the NIST 2010 speaker
recognition evaluation (SRE) extended core conditions involving telephone and
microphone trials. Experimental results indicate that: 1) the NDA is more
effective (up to 35% relative improvement in terms of EER) than the traditional
parametric LDA for speaker recognition, 2) when compared to raw acoustic
features (e.g., MFCCs), the ASR speaker-adapted features provide gains in
speaker recognition performance, and 3) increasing the number of output units
in the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k)
provides consistent improvements in performance (for example from 37% to 57%
relative EER gains over our baseline GMM i-vector system). To our knowledge,
results reported in this paper represent the best performances published to
date on the NIST SRE 2010 extended core tasks."
"This paper presents a method for detecting mispronunciations with the aim of
improving Computer Assisted Language Learning (CALL) tools used by foreign
language learners. The algorithm is based on Principle Component Analysis
(PCA). It is hierarchical with each successive step refining the estimate to
classify the test word as being either mispronounced or correct. Preprocessing
before detection, like normalization and time-scale modification, is
implemented to guarantee uniformity of the feature vectors input to the
detection system. The performance using various features including spectrograms
and Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated.
Best results were obtained using MFCCs, achieving up to 99% accuracy in word
verification and 93% in native/non-native classification. Compared with Hidden
Markov Models (HMMs) which are used pervasively in recognition application,
this particular approach is computational efficient and effective when training
data is limited."
"Blind methods often separate or identify signals or signal subspaces up to an
unknown scaling factor. Sometimes it is necessary to cope with the scaling
ambiguity, which can be done through reconstructing signals as they are
received by sensors, because scales of the sensor responses (images) have known
physical interpretations. In this paper, we analyze two approaches that are
widely used for computing the sensor responses, especially, in Frequency-Domain
Independent Component Analysis. One approach is the least-squares projection,
while the other one assumes a regular mixing matrix and computes its inverse.
Both estimators are invariant to the unknown scaling. Although frequently used,
their differences were not studied yet. A goal of this work is to fill this
gap. The estimators are compared through a theoretical study, perturbation
analysis and simulations. We point to the fact that the estimators are
equivalent when the separated signal subspaces are orthogonal, and vice versa.
Two applications are shown, one of which demonstrates a case where the
estimators yield substantially different results."
"In this paper we present an approach to polyphonic sound event detection in
real life recordings based on bi-directional long short term memory (BLSTM)
recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to
map acoustic features of a mixture signal consisting of sounds from multiple
classes, to binary activity indicators of each event class. Our method is
tested on a large database of real-life recordings, with 61 classes (e.g.
music, car, speech) from 10 different everyday contexts. The proposed method
outperforms previous approaches by a large margin, and the results are further
improved using data augmentation techniques. Overall, our system reports an
average F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a
relative improvement over previous state-of-the-art approach of 6.8% and 15.1%
respectively."
"We introduce a new learned descriptor for audio signals which is efficient
for event representation. The entries of the descriptor are produced by
evaluating a set of regressors on the input signal. The regressors are
class-specific and trained using the random regression forests framework. Given
an input signal, each regressor estimates the onset and offset positions of the
target event. The estimation confidence scores output by a regressor are then
used to quantify how the target event aligns with the temporal structure of the
corresponding category. Our proposed descriptor has two advantages. First, it
is compact, i.e. the dimensionality of the descriptor is equal to the number of
event classes. Second, we show that even simple linear classification models,
trained on our descriptor, yield better accuracies on audio event
classification task than not only the nonlinear baselines but also the
state-of-the-art results."
"Acoustic event detection is essential for content analysis and description of
multimedia recordings. The majority of current literature on the topic learns
the detectors through fully-supervised techniques employing strongly labeled
data. However, the labels available for majority of multimedia data are
generally weak and do not provide sufficient detail for such methods to be
employed. In this paper we propose a framework for learning acoustic event
detectors using only weakly labeled data. We first show that audio event
detection using weak labels can be formulated as an Multiple Instance Learning
problem. We then suggest two frameworks for solving multiple-instance learning,
one based on support vector machines, and the other on neural networks. The
proposed methods can help in removing the time consuming and expensive process
of manually annotating data to facilitate fully supervised learning. Moreover,
it can not only detect events in a recording but can also provide temporal
locations of events in the recording. This helps in obtaining a complete
description of the recording and is notable since temporal information was
never known in the first place in weakly labeled data."
"Identifying musical instruments in polyphonic music recordings is a
challenging but important problem in the field of music information retrieval.
It enables music search by instrument, helps recognize musical genres, or can
make music transcription easier and more accurate. In this paper, we present a
convolutional neural network framework for predominant instrument recognition
in real-world polyphonic music. We train our network from fixed-length music
excerpts with a single-labeled predominant instrument and estimate an arbitrary
number of predominant instruments from an audio signal with a variable length.
To obtain the audio-excerpt-wise result, we aggregate multiple outputs from
sliding windows over the test audio. In doing so, we investigated two different
aggregation methods: one takes the average for each instrument and the other
takes the instrument-wise sum followed by normalization. In addition, we
conducted extensive experiments on several important factors that affect the
performance, including analysis window size, identification threshold, and
activation functions for neural networks to find the optimal set of parameters.
Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we
found that convolutional neural networks are more robust than conventional
methods that exploit spectral features and source separation with support
vector machines. Experimental results showed that the proposed convolutional
network architecture obtained an F1 measure of 0.602 for micro and 0.503 for
macro, respectively, achieving 19.6% and 16.4% in performance improvement
compared with other state-of-the-art algorithms."
"Building systems that possess the sensitivity and intelligence to identify
and describe high-level attributes in music audio signals continues to be an
elusive goal, but one that surely has broad and deep implications for a wide
variety of applications. Hundreds of papers have so far been published toward
this goal, and great progress appears to have been made. Some systems produce
remarkable accuracies at recognising high-level semantic concepts, such as
music style, genre and mood. However, it might be that these numbers do not
mean what they seem. In this paper, we take a state-of-the-art music content
analysis system and investigate what causes it to achieve exceptionally high
performance in a benchmark music audio dataset. We dissect the system to
understand its operation, determine its sensitivities and limitations, and
predict the kinds of knowledge it could and could not possess about music. We
perform a series of experiments to illuminate what the system has actually
learned to do, and to what extent it is performing the intended music listening
task. Our results demonstrate how the initial manifestation of music
intelligence in this state-of-the-art can be deceptive. Our work provides
constructive directions toward developing music content analysis systems that
can address the music information and creation needs of real-world users."
"This paper presents a novel BigEAR big data framework that employs
psychological audio processing chain (PAPC) to process smartphone-based
acoustic big data collected when the user performs social conversations in
naturalistic scenarios. The overarching goal of BigEAR is to identify moods of
the wearer from various activities such as laughing, singing, crying, arguing,
and sighing. These annotations are based on ground truth relevant for
psychologists who intend to monitor/infer the social context of individuals
coping with breast cancer. We pursued a case study on couples coping with
breast cancer to know how the conversations affect emotional and social well
being. In the state-of-the-art methods, psychologists and their team have to
hear the audio recordings for making these inferences by subjective evaluations
that not only are time-consuming and costly, but also demand manual data coding
for thousands of audio files. The BigEAR framework automates the audio
analysis. We computed the accuracy of BigEAR with respect to the ground truth
obtained from a human rater. Our approach yielded overall average accuracy of
88.76% on real-world data from couples coping with breast cancer."
"Audio Event Detection is an important task for content analysis of multimedia
data. Most of the current works on detection of audio events is driven through
supervised learning approaches. We propose a weakly supervised learning
framework which can make use of the tremendous amount of web multimedia data
with significantly reduced annotation effort and expense. Specifically, we use
several multiple instance learning algorithms to show that audio event
detection through weak labels is feasible. We also propose a novel scalable
multiple instance learning algorithm and show that its competitive with other
multiple instance learning algorithms for audio event detection tasks."
"This paper addresses a question about music cognition: how do we derive
polymetric structures. A preference rule system is presented which is
implemented into a drum computer. The preference rule system allows inferring
local polymetric structures, like two-over-three and three-over-two. By
analyzing the micro-timing of West African percussion music a timing pattern
consisting of six pulses was discovered. It integrates binary and ternary
rhythmic feels. The presented drum computer integrates the discovered
superimposed polymetric swing (timing and velocity) appropriate to the rhythmic
sequence the user inputs. For binary sequences, the amount of binary swing is
increased and for ternary sequences, the ternary swing is increased."
"Adding an emotions using prosody manipulation method for Indonesian text to
speech system. Text To Speech (TTS) is a system that can convert text in one
language into speech, accordance with the reading of the text in the language
used. The focus of this research is a natural sounding concept, the make
""humanize"" for the pronunciation of voice synthesis system Text To Speech.
Humans have emotions / intonation that may affect the sound produced. The main
requirement for the system used Text To Speech in this research is eSpeak, the
database MBROLA using id1, Human Speech Corpus database from a website that
summarizes the words with the highest frequency (Most Common Words) used in a
country. And there are 3 types of emotional / intonation designed base. There
is a happy, angry and sad emotion. Method for develop the emotional filter is
manipulate the relevant features of prosody (especially pitch and duration
value) using a predetermined rate factor that has been established by analyzing
the differences between the standard output Text To Speech and voice recording
with emotional prosody / a particular intonation. The test results for the
perception tests of Human Speech Corpus for happy emotion is 95 %, 96.25 % for
angry emotion and 98.75 % for sad emotions. For perception test system carried
by intelligibility and naturalness test. Intelligibility test for the accuracy
of sound with the original sentence is 93.3%, and for clarity rate for each
sentence is 62.8%. For naturalness, accuracy emotional election amounted to
75.6 % for happy emotion, 73.3 % for angry emotion, and 60 % for sad emotions.
  -----
  Text To Speech (TTS) merupakan suatu sistem yang dapat mengonversi teks dalam
format suatu bahasa menjadi ucapan sesuai dengan pembacaan teks dalam bahasa
yang digunakan."
"This report describes our submissions to Task2 and Task3 of the DCASE 2016
challenge. The systems aim at dealing with the detection of overlapping audio
events in continuous streams, where the detectors are based on random decision
forests. The proposed forests are jointly trained for classification and
regression simultaneously. Initially, the training is classification-oriented
to encourage the trees to select discriminative features from overlapping
mixtures to separate positive audio segments from the negative ones. The
regression phase is then carried out to let the positive audio segments vote
for the event onsets and offsets, and therefore model the temporal structure of
audio events. One random decision forest is specifically trained for each event
category of interest. Experimental results on the development data show that
our systems significantly outperform the baseline on the Task2 evaluation while
they are inferior to the baseline in the Task3 evaluation."
"Environmental audio tagging aims to predict only the presence or absence of
certain acoustic events in the interested acoustic scene. In this paper we make
contributions to audio tagging in two parts, respectively, acoustic modeling
and feature learning. We propose to use a shrinking deep neural network (DNN)
framework incorporating unsupervised feature learning to handle the multi-label
classification task. For the acoustic modeling, a large set of contextual
frames of the chunk are fed into the DNN to perform a multi-label
classification for the expected tags, considering that only chunk (or
utterance) level rather than frame-level labels are available. Dropout and
background noise aware training are also adopted to improve the generalization
capability of the DNNs. For the unsupervised feature learning, we propose to
use a symmetric or asymmetric deep de-noising auto-encoder (sDAE or aDAE) to
generate new data-driven features from the Mel-Filter Banks (MFBs) features.
The new features, which are smoothed against background noise and more compact
with contextual information, can further improve the performance of the DNN
baseline. Compared with the standard Gaussian Mixture Model (GMM) baseline of
the DCASE 2016 audio tagging challenge, our proposed method obtains a
significant equal error rate (EER) reduction from 0.21 to 0.13 on the
development set. The proposed aDAE system can get a relative 6.7% EER reduction
compared with the strong DNN baseline on the development set. Finally, the
results also show that our approach obtains the state-of-the-art performance
with 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while
EER of the first prize of this challenge is 0.17."
"In this paper, we present a deep neural network (DNN)-based acoustic scene
classification framework. Two hierarchical learning methods are proposed to
improve the DNN baseline performance by incorporating the hierarchical taxonomy
information of environmental sounds. Firstly, the parameters of the DNN are
initialized by the proposed hierarchical pre-training. Multi-level objective
function is then adopted to add more constraint on the cross-entropy based loss
function. A series of experiments were conducted on the Task1 of the Detection
and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The
final DNN-based system achieved a 22.9% relative improvement on average scene
classification error as compared with the Gaussian Mixture Model (GMM)-based
benchmark system across four standard folds."
"In the context of the Internet of Things (IoT), sound sensing applications
are required to run on embedded platforms where notions of product pricing and
form factor impose hard constraints on the available computing power. Whereas
Automatic Environmental Sound Recognition (AESR) algorithms are most often
developed with limited consideration for computational cost, this article seeks
which AESR algorithm can make the most of a limited amount of computing power
by comparing the sound classification performance em as a function of its
computational cost. Results suggest that Deep Neural Networks yield the best
ratio of sound classification accuracy across a range of computational costs,
while Gaussian Mixture Models offer a reasonable accuracy at a consistently
small cost, and Support Vector Machines stand between both in terms of
compromise between accuracy and computational cost."
"We present a novel method for the compensation of long duration data gaps in
audio signals, in particular music. The concealment of such signal defects is
based on a graph that encodes signal structure in terms of time-persistent
spectral similarity. A suitable candidate segment for the substitution of the
lost content is proposed by an intuitive optimization scheme and smoothly
inserted into the gap. Extensive listening tests show that the proposed
algorithm provides highly promising results when applied to a variety of
real-world music signals."
"In this paper, the design of a computationally efficient variable bandpass
digital filter is presented. The center frequency and bandwidth of this filter
can be changed online without updating the filter coefficients. The warped
filters, obtained by replacing each unit delay of a digital filter with an
allpass filter, are widely used for various audio processing applications.
However, warped filters fail to provide variable bandwidth bandpass responses
for a given center frequency using first order allpass transformation. To
overcome this drawback, our design is accomplished by combining warped filter
with the coefficient decimation technique. The design example shows that the
proposed variable digital filter is simple to design and offers a total gate
count reduction of 36% and 65% over the warped filters compared to the designs
presented in [3] and [1] respectively"
"The ability of deep convolutional neural networks (CNN) to learn
discriminative spectro-temporal patterns makes them well suited to
environmental sound classification. However, the relative scarcity of labeled
data has impeded the exploitation of this family of high-capacity models. This
study has two primary contributions: first, we propose a deep convolutional
neural network architecture for environmental sound classification. Second, we
propose the use of audio data augmentation for overcoming the problem of data
scarcity and explore the influence of different augmentations on the
performance of the proposed CNN architecture. Combined with data augmentation,
the proposed model produces state-of-the-art results for environmental sound
classification. We show that the improved performance stems from the
combination of a deep, high-capacity model and an augmented training set: this
combination outperforms both the proposed CNN without augmentation and a
""shallow"" dictionary learning model with augmentation. Finally, we examine the
influence of each augmentation on the model's classification accuracy for each
class, and observe that the accuracy for each class is influenced differently
by each augmentation, suggesting that the performance of the model could be
improved further by applying class-conditional data augmentation."
"Audio Event Detection (AED) aims to recognize sounds within audio and video
recordings. AED employs machine learning algorithms commonly trained and tested
on annotated datasets. However, available datasets are limited in number of
samples and hence it is difficult to model acoustic diversity. Therefore, we
propose combining labeled audio from a dataset and unlabeled audio from the web
to improve the sound models. The audio event detectors are trained on the
labeled audio and ran on the unlabeled audio downloaded from YouTube. Whenever
the detectors recognized any of the known sounds with high confidence, the
unlabeled audio was use to re-train the detectors. The performance of the
re-trained detectors is compared to the one from the original detectors using
the annotated test set. Results showed an improvement of the AED, and uncovered
challenges of using web audio from videos."
"In this paper we describe approaches for discovering acoustic concepts and
relations in text. The first major goal is to be able to identify text phrases
which contain a notion of audibility and can be termed as a sound or an
acoustic concept. We also propose a method to define an acoustic scene through
a set of sound concepts. We use pattern matching and parts of speech tags to
generate sound concepts from large scale text corpora. We use dependency
parsing and LSTM recurrent neural network to predict a set of sound concepts
for a given acoustic scene. These methods are not only helpful in creating an
acoustic knowledge base but in the future can also directly help acoustic event
and scene detection research."
"Decision making is an important component in a speaker verification system.
For the conventional GMM-UBM architecture, the decision is usually conducted
based on the log likelihood ratio of the test utterance against the GMM of the
claimed speaker and the UBM. This single-score decision is simple but tends to
be sensitive to the complex variations in speech signals (e.g. text content,
channel, speaking style, etc.). In this paper, we propose a decision making
approach based on multiple scores derived from a set of cohort GMMs (cohort
scores). Importantly, these cohort scores are not simply averaged as in
conventional cohort methods; instead, we employ a powerful discriminative model
as the decision maker. Experimental results show that the proposed method
delivers substantial performance improvement over the baseline system,
especially when a deep neural network (DNN) is used as the decision maker, and
the DNN input involves some statistical features derived from the cohort
scores."
"Convolutional Neural Networks (CNNs) have proven very effective in image
classification and show promise for audio. We use various CNN architectures to
classify the soundtracks of a dataset of 70M training videos (5.24 million
hours) with 30,871 video-level labels. We examine fully connected Deep Neural
Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We
investigate varying the size of both training set and label vocabulary, finding
that analogs of the CNNs used in image classification do well on our audio
classification task, and larger training and label sets help up to a point. A
model using embeddings from these classifiers does much better than raw
features on the Audio Set [5] Acoustic Event Detection (AED) classification
task."
"Learning acoustic models directly from the raw waveform data with minimal
processing is challenging. Current waveform-based models have generally used
very few (~2) convolutional layers, which might be insufficient for building
high-level discriminative features. In this work, we propose very deep
convolutional neural networks (CNNs) that directly use time-domain waveforms as
inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over
very long sequences (e.g., vector of size 32000), necessary for processing
acoustic waveforms. This is achieved through batch normalization, residual
learning, and a careful design of down-sampling in the initial layers. Our
networks are fully convolutional, without the use of fully connected layers and
dropout, to maximize representation learning. We use a large receptive field in
the first convolutional layer to mimic bandpass filters, but very small
receptive fields subsequently to control the model capacity. We demonstrate the
performance gains with the deeper models. Our evaluation shows that the CNN
with 18 weight layers outperform the CNN with 3 weight layers by over 15% in
absolute accuracy for an environmental sound recognition task and matches the
performance of models using log-mel features."
"Notes in a musical piece are building blocks employed in non-random ways to
create melodies. It is the ""interaction"" among a limited amount of notes that
allows constructing the variety of musical compositions that have been written
in centuries and within different cultures. Networks are a modeling tool that
is commonly employed to represent a set of entities interacting in some way.
Thus, notes composing a melody can be seen as nodes of a network that are
connected whenever these are played in sequence. The outcome of such a process
results in a directed graph. By using complex network theory, some main metrics
of musical graphs can be measured, which characterize the related musical
pieces. In this paper, we define a framework to represent melodies as networks.
Then, we provide an analysis on a set of guitar solos performed by main
musicians. Results of this study indicate that the presented model can have an
impact on audio and multimedia applications such as music classification,
identification, e-learning, automatic music generation, multimedia
entertainment."
"This paper presents NetWorks (NW), an interactive music generation system
that uses a hierarchically clustered scale free network to generate music that
ranges from orderly to chaotic. NW was inspired by the Honing Theory of
creativity, according to which human-like creativity hinges on (1) the ability
to self-organize and maintain dynamics at the 'edge of chaos' using something
akin to 'psychological entropy', and (2) the capacity to shift between analytic
and associative processing modes. At the 'edge of chaos', NW generates patterns
that exhibit emergent complexity through coherent development at low, mid, and
high levels of musical organization, and often suggests goal seeking behaviour.
The architecture consists of four 16-node modules: one each for pitch,
velocity, duration, and entry delay. The Core allows users to define how nodes
are connected, and rules that determine when and how nodes respond to their
inputs. The Mapping Layer allows users to map node output values to MIDI data
that is routed to software instruments in a digital audio workstation. By
shifting between bottom-up and top-down NW shifts between analytic and
associative processing modes."
"Based on API call sequences, semantic-aware and machine learning (ML) based
malware classifiers can be built for malware detection or classification.
Previous works concentrate on crafting and extracting various features from
malware binaries, disassembled binaries or API calls via static or dynamic
analysis and resorting to ML to build classifiers. However, they tend to
involve too much feature engineering and fail to provide interpretability. We
solve these two problems with the recent advances in deep learning: 1)
RNN-based autoencoders (RNN-AEs) can automatically learn low-dimensional
representation of a malware from its raw API call sequence. 2) Multiple
decoders can be trained under different supervisions to give more information,
other than the class or family label of a malware. Inspired by the works of
document classification and automatic sentence summarization, each API call
sequence can be regarded as a sentence. In this paper, we make the first
attempt to build a multi-task malware learning model based on API call
sequences. The model consists of two decoders, one for malware classification
and one for $\emph{file access pattern}$ (FAP) generation given the API call
sequence of a malware. We base our model on the general seq2seq framework.
Experiments show that our model can give competitive classification results as
well as insightful FAP information."
"In this work, a Bayesian approach to speaker normalization is proposed to
compensate for the degradation in performance of a speaker independent speech
recognition system. The speaker normalization method proposed herein uses the
technique of vocal tract length normalization (VTLN). The VTLN parameters are
estimated using a novel Bayesian approach which utilizes the Gibbs sampler, a
special type of Markov Chain Monte Carlo method. Additionally the
hyperparameters are estimated using maximum likelihood approach. This model is
used assuming that human vocal tract can be modeled as a tube of uniform cross
section. It captures the variation in length of the vocal tract of different
speakers more effectively, than the linear model used in literature. The work
has also investigated different methods like minimization of Mean Square Error
(MSE) and Mean Absolute Error (MAE) for the estimation of VTLN parameters. Both
single pass and two pass approaches are then used to build a VTLN based speech
recognizer. Experimental results on recognition of vowels and Hindi phrases
from a medium vocabulary indicate that the Bayesian method improves the
performance by a considerable margin."
"In this letter, we propose enhanced factored three way restricted Boltzmann
machines (EFTW-RBMs) for speech detection. The proposed model incorporates
conditional feature learning by multiplying the dynamical state of the third
unit, which allows a modulation over the visible-hidden node pairs. Instead of
stacking previous frames of speech as the third unit in a recursive manner, the
correlation related weighting coefficients are assigned to the contextual
neighboring frames. Specifically, a threshold function is designed to capture
the long-term features and blend the globally stored speech structure. A
factored low rank approximation is introduced to reduce the parameters of the
three-dimensional interaction tensor, on which non-negative constraint is
imposed to address the sparsity characteristic. The validations through the
area-under-ROC-curve (AUC) and signal distortion ratio (SDR) show that our
approach outperforms several existing 1D and 2D (i.e., time and time-frequency
domain) speech detection algorithms in various noisy environments."
"This paper presents the Intelligent Voice (IV) system submitted to the NIST
2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this
year was on developing speaker recognition technology which is robust for novel
languages that are much more heterogeneous than those used in the current
state-of-the-art, using significantly less training data, that does not contain
meta-data from those languages. The system is based on the state-of-the-art
i-vector/PLDA which is developed on the fixed training condition, and the
results are reported on the protocol defined on the development set of the
challenge."
"This paper presents an efficient implementation of the Wavenet generation
process called Fast Wavenet. Compared to a naive implementation that has
complexity O(2^L) (L denotes the number of layers in the network), our proposed
approach removes redundant convolution operations by caching previous
calculations, thereby reducing the complexity to O(L) time. Timing experiments
show significant advantages of our fast implementation over a naive one. While
this method is presented for Wavenet, the same scheme can be applied anytime
one wants to perform autoregressive generation or online prediction using a
model with dilated convolution layers. The code for our method is publicly
available."
"Heart diseases constitute a global health burden, and the problem is
exacerbated by the error-prone nature of listening to and interpreting heart
sounds. This motivates the development of automated classification to screen
for abnormal heart sounds. Existing machine learning-based systems achieve
accurate classification of heart sound recordings but rely on expert features
that have not been thoroughly evaluated on noisy recordings. Here we propose a
segmental convolutional neural network architecture that achieves automatic
feature learning from noisy heart sound recordings. Our experiments show that
our best model, trained on noisy recording segments acquired with an existing
hidden semi-markov model-based approach, attains a classification accuracy of
87.5% on the 2016 PhysioNet/CinC Challenge dataset, compared to the 84.6%
accuracy of the state-of-the-art statistical classifier trained and evaluated
on the same dataset. Our results indicate the potential of using neural
network-based methods to increase the accuracy of automated classification of
heart sound recordings for improved screening of heart diseases."
"Several methods exist for a computer to generate music based on data
including Markov chains, recurrent neural networks, recombinancy, and grammars.
We explore the use of unit selection and concatenation as a means of generating
music using a procedure based on ranking, where, we consider a unit to be a
variable length number of measures of music. We first examine whether a unit
selection method, that is restricted to a finite size unit library, can be
sufficient for encompassing a wide spectrum of music. We do this by developing
a deep autoencoder that encodes a musical input and reconstructs the input by
selecting from the library. We then describe a generative model that combines a
deep structured semantic model (DSSM) with an LSTM to predict the next unit,
where units consist of four, two, and one measures of music. We evaluate the
generative model using objective metrics including mean rank and accuracy and
with a subjective listening test in which expert musicians are asked to
complete a forced-choiced ranking task. We compare our model to a note-level
generative baseline that consists of a stacked LSTM trained to predict forward
by one note."
"We introduce a method for imposing higher-level structure on generated,
polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a
generative model is combined with gradient descent constraint optimization to
provide further control over the generation process. Among other things, this
allows for the use of a ""template"" piece, from which some structural properties
can be extracted, and transferred as constraints to newly generated material.
The sampling process is guided with Simulated Annealing in order to avoid local
optima, and find solutions that both satisfy the constraints, and are
relatively stable with respect to the C-RBM. Results show that with this
approach it is possible to control the higher level self-similarity structure,
the meter, as well as tonal properties of the resulting musical piece while
preserving its local musical coherence."
"This paper demonstrates the feasibility of learning to retrieve short
snippets of sheet music (images) when given a short query excerpt of music
(audio) -- and vice versa --, without any symbolic representation of music or
scores. This would be highly useful in many content-based musical retrieval
scenarios. Our approach is based on Deep Canonical Correlation Analysis (DCCA)
and learns correlated latent spaces allowing for cross-modality retrieval in
both directions. Initial experiments with relatively simple monophonic music
show promising results."
"Most of the existing studies on voice conversion (VC) are conducted in
acoustically matched conditions between source and target signal. However, the
robustness of VC methods in presence of mismatch remains unknown. In this
paper, we report a comparative analysis of different VC techniques under
mismatched conditions. The extensive experiments with five different VC
techniques on CMU ARCTIC corpus suggest that performance of VC methods
substantially degrades in noisy conditions. We have found that bilinear
frequency warping with amplitude scaling (BLFWAS) outperforms other methods in
most of the noisy conditions. We further explore the suitability of different
speech enhancement techniques for robust conversion. The objective evaluation
results indicate that spectral subtraction and log minimum mean square error
(logMMSE) based speech enhancement techniques can be used to improve the
performance in specific noisy conditions."
"With the development of speech synthesis techniques, automatic speaker
verification systems face the serious challenge of spoofing attack. In order to
improve the reliability of speaker verification systems, we develop a new
filter bank based cepstral feature, deep neural network filter bank cepstral
coefficients (DNN-FBCC), to distinguish between natural and spoofed speech. The
deep neural network filter bank is automatically generated by training a filter
bank neural network (FBNN) using natural and synthetic speech. By adding
restrictions on the training rules, the learned weight matrix of FBNN is
band-limited and sorted by frequency, similar to the normal filter bank. Unlike
the manually designed filter bank, the learned filter bank has different filter
shapes in different channels, which can capture the differences between natural
and synthetic speech more effectively. The experimental results on the ASVspoof
{2015} database show that the Gaussian mixture model maximum-likelihood
(GMM-ML) classifier trained by the new feature performs better than the
state-of-the-art linear frequency cepstral coefficients (LFCC) based
classifier, especially on detecting unknown attacks."
"Environmental audio tagging is a newly proposed task to predict the presence
or absence of a specific audio event in a chunk. Deep neural network (DNN)
based methods have been successfully adopted for predicting the audio tags in
the domestic audio scene. In this paper, we propose to use a convolutional
neural network (CNN) to extract robust features from mel-filter banks (MFBs),
spectrograms or even raw waveforms for audio tagging. Gated recurrent unit
(GRU) based recurrent neural networks (RNNs) are then cascaded to model the
long-term temporal structure of the audio signal. To complement the input
information, an auxiliary CNN is designed to learn on the spatial features of
stereo recordings. We evaluate our proposed methods on Task 4 (audio tagging)
of the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE
2016) challenge. Compared with our recent DNN-based method, the proposed
structure can reduce the equal error rate (EER) from 0.13 to 0.11 on the
development set. The spatial features can further reduce the EER to 0.10. The
performance of the end-to-end learning on raw waveforms is also comparable.
Finally, on the evaluation set, we get the state-of-the-art performance with
0.12 EER while the performance of the best existing system is 0.15 EER."
"Recently, the end-to-end approach that learns hierarchical representations
from raw data using deep convolutional neural networks has been successfully
explored in the image, text and speech domains. This approach was applied to
musical signals as well but has been not fully explored yet. To this end, we
propose sample-level deep convolutional neural networks which learn
representations from very small grains of waveforms (e.g. 2 or 3 samples)
beyond typical frame-level input representations. Our experiments show how deep
architectures with sample-level filters improve the accuracy in music
auto-tagging and they provide results comparable to previous state-of-the-art
performances for the Magnatagatune dataset and Million Song Dataset. In
addition, we visualize filters learned in a sample-level DCNN in each layer to
identify hierarchically learned features and show that they are sensitive to
log-scaled frequency along layer, such as mel-frequency spectrogram that is
widely used in music classification systems."
"Bird sounds possess distinctive spectral structure which may exhibit small
shifts in spectrum depending on the bird species and environmental conditions.
In this paper, we propose using convolutional recurrent neural networks on the
task of automated bird audio detection in real-life environments. In the
proposed method, convolutional layers extract high dimensional, local frequency
shift invariant features, while recurrent layers capture longer term
dependencies between the features extracted from short time frames. This method
achieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data
and obtains the second place in the Bird Audio Detection challenge."
"In this paper we analyze the gate activation signals inside the gated
recurrent neural networks, and find the temporal structure of such signals is
highly correlated with the phoneme boundaries. This correlation is further
verified by a set of experiments for phoneme segmentation, in which better
results compared to standard approaches were obtained."
"Speech enhancement (SE) aims to reduce noise in speech signals. Most SE
techniques focus on addressing audio information only.In this work, inspired by
multimodal learning, which utilizes data from different modalities, and the
recent success of convolutional neural networks (CNNs) in SE, we propose an
audio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual
streams into a unified network model.In the proposed AVDCNN SE model,audio and
visual features are first processed using individual CNNs, and then, fused into
a joint network to generate enhanced speech at an output layer. The AVDCNN
model is trained in an end-to-end manner, and parameters are jointly learned
through back-propagation. We evaluate enhanced speech using five objective
criteria. Results show that the AVDCNN yields notably better performance as
compared to an audio-only CNN-based SE model, confirming the effectiveness of
integrating visual information into the SE process."
"Being able to predict whether a song can be a hit has impor- tant
applications in the music industry. Although it is true that the popularity of
a song can be greatly affected by exter- nal factors such as social and
commercial influences, to which degree audio features computed from musical
signals (whom we regard as internal factors) can predict song popularity is an
interesting research question on its own. Motivated by the recent success of
deep learning techniques, we attempt to ex- tend previous work on hit song
prediction by jointly learning the audio features and prediction models using
deep learning. Specifically, we experiment with a convolutional neural net-
work model that takes the primitive mel-spectrogram as the input for feature
learning, a more advanced JYnet model that uses an external song dataset for
supervised pre-training and auto-tagging, and the combination of these two
models. We also consider the inception model to characterize audio infor-
mation in different scales. Our experiments suggest that deep structures are
indeed more accurate than shallow structures in predicting the popularity of
either Chinese or Western Pop songs in Taiwan. We also use the tags predicted
by JYnet to gain insights into the result of different models."
"In this paper, we design a system in order to perform the real-time beat
tracking for an audio signal. We use Onset Strength Signal (OSS) to detect the
onsets and estimate the tempos. Then, we form Cumulative Beat Strength Signal
(CBSS) by taking advantage of OSS and estimated tempos. Next, we perform peak
detection by extracting the periodic sequence of beats among all CBSS peaks. In
simulations, we can see that our proposed algorithm, Online Beat TrAckINg
(OBTAIN), outperforms state-of-art results in terms of prediction accuracy
while maintaining comparable and practical computational complexity. The
real-time performance is tractable visually as illustrated in the simulations."
"Voice conversion (VC) using sequence-to-sequence learning of context
posterior probabilities is proposed. Conventional VC using shared context
posterior probabilities predicts target speech parameters from the context
posterior probabilities estimated from the source speech parameters. Although
conventional VC can be built from non-parallel data, it is difficult to convert
speaker individuality such as phonetic property and speaking rate contained in
the posterior probabilities because the source posterior probabilities are
directly used for predicting target speech parameters. In this work, we assume
that the training data partly include parallel speech data and propose
sequence-to-sequence learning between the source and target posterior
probabilities. The conversion models perform non-linear and variable-length
transformation from the source probability sequence to the target one. Further,
we propose a joint training algorithm for the modules. In contrast to
conventional VC, which separately trains the speech recognition that estimates
posterior probabilities and the speech synthesis that predicts target speech
parameters, our proposed method jointly trains these modules along with the
proposed probability conversion modules. Experimental results demonstrate that
our approach outperforms the conventional VC."
"This paper presents sampling-based speech parameter generation using
moment-matching networks for Deep Neural Network (DNN)-based speech synthesis.
Although people never produce exactly the same speech even if we try to express
the same linguistic and para-linguistic information, typical statistical speech
synthesis produces completely the same speech, i.e., there is no
inter-utterance variation in synthetic speech. To give synthetic speech natural
inter-utterance variation, this paper builds DNN acoustic models that make it
possible to randomly sample speech parameters. The DNNs are trained so that
they make the moments of generated speech parameters close to those of natural
speech parameters. Since the variation of speech parameters is compressed into
a low-dimensional simple prior noise vector, our algorithm has lower
computation cost than direct sampling of speech parameters. As the first step
towards generating synthetic speech that has natural inter-utterance variation,
this paper investigates whether or not the proposed sampling-based generation
deteriorates synthetic speech quality. In evaluation, we compare speech quality
of conventional maximum likelihood-based generation and proposed sampling-based
generation. The result demonstrates the proposed generation causes no
degradation in speech quality."
"We present a new model for singing synthesis based on a modified version of
the WaveNet architecture. Instead of modeling raw waveform, we model features
produced by a parametric vocoder that separates the influence of pitch and
timbre. This allows conveniently modifying pitch to match any target melody,
facilitates training on more modest dataset sizes, and significantly reduces
training and generation times. Our model makes frame-wise predictions using
mixture density outputs rather than categorical outputs in order to reduce the
required parameter count. As we found overfitting to be an issue with the
relatively small datasets used in our experiments, we propose a method to
regularize the model and make the autoregressive generation process more robust
to prediction errors. Using a simple multi-stream architecture, harmonic,
aperiodic and voiced/unvoiced components can all be predicted in a coherent
manner. We compare our method to existing parametric statistical and
state-of-the-art concatenative methods using quantitative metrics and a
listening test. While naive implementations of the autoregressive generation
algorithm tend to be inefficient, using a smart algorithm we can greatly speed
up the process and obtain a system that's competitive in both speed and
quality."
"Recently deep neural networks (DNNs) have been used to learn speaker
features. However, the quality of the learned features is not sufficiently
good, so a complex back-end model, either neural or probabilistic, has to be
used to address the residual uncertainty when applied to speaker verification,
just as with raw features. This paper presents a convolutional time-delay deep
neural network structure (CT-DNN) for speaker feature learning. Our
experimental results on the Fisher database demonstrated that this CT-DNN can
produce high-quality speaker features: even with a single feature (0.3 seconds
including the context), the EER can be as low as 7.68%. This effectively
confirmed that the speaker trait is largely a deterministic short-time property
rather than a long-time distributional pattern, and therefore can be extracted
from just dozens of frames."
"We propose an algorithm to separate simultaneously speaking persons from each
other, the ""cocktail party problem"", using a single microphone. Our approach
involves a deep recurrent neural networks regression to a vector space that is
descriptive of independent speakers. Such a vector space can embed empirically
determined speaker characteristics and is optimized by distinguishing between
speaker masks. We call this technique source-contrastive estimation. The
methodology is inspired by negative sampling, which has seen success in natural
language processing, where an embedding is learned by correlating and
de-correlating a given input vector with output weights. Although the matrix
determined by the output weights is dependent on a set of known speakers, we
only use the input vectors during inference. Doing so will ensure that source
separation is explicitly speaker-independent. Our approach is similar to recent
deep neural network clustering and permutation-invariant training research; we
use weighted spectral features and masks to augment individual speaker
frequencies while filtering out other speakers. We avoid, however, the severe
computational burden of other approaches with our technique. Furthermore, by
training a vector space rather than combinations of different speakers or
differences thereof, we avoid the so-called permutation problem during
training. Our algorithm offers an intuitive, computationally efficient response
to the cocktail party problem, and most importantly boasts better empirical
performance than other current techniques."
"This paper presents the Speech Technology Center (STC) replay attack
detection systems proposed for Automatic Speaker Verification Spoofing and
Countermeasures Challenge 2017. In this study we focused on comparison of
different spoofing detection approaches. These were GMM based methods, high
level features extraction with simple classifier and deep learning frameworks.
Experiments performed on the development and evaluation parts of the challenge
dataset demonstrated stable efficiency of deep learning approaches in case of
changing acoustic conditions. At the same time SVM classifier with high level
features provided a substantial input in the efficiency of the resulting STC
systems according to the fusion systems results."
"Growing interest in automatic speaker verification (ASV)systems has lead to
significant quality improvement of spoofing attackson them. Many research works
confirm that despite the low equal er-ror rate (EER) ASV systems are still
vulnerable to spoofing attacks. Inthis work we overview different acoustic
feature spaces and classifiersto determine reliable and robust countermeasures
against spoofing at-tacks. We compared several spoofing detection systems,
presented so far,on the development and evaluation datasets of the Automatic
SpeakerVerification Spoofing and Countermeasures (ASVspoof) Challenge
2015.Experimental results presented in this paper demonstrate that the useof
magnitude and phase information combination provides a substantialinput into
the efficiency of the spoofing detection systems. Also wavelet-based features
show impressive results in terms of equal error rate. Inour overview we compare
spoofing performance for systems based on dif-ferent classifiers. Comparison
results demonstrate that the linear SVMclassifier outperforms the conventional
GMM approach. However, manyresearchers inspired by the great success of deep
neural networks (DNN)approaches in the automatic speech recognition, applied
DNN in thespoofing detection task and obtained quite low EER for known and
un-known type of spoofing attacks."
"New system for i-vector speaker recognition based on variational autoencoder
(VAE) is investigated. VAE is a promising approach for developing accurate deep
nonlinear generative models of complex data. Experiments show that VAE provides
speaker embedding and can be effectively trained in an unsupervised manner. LLR
estimate for VAE is developed. Experiments on NIST SRE 2010 data demonstrate
its correctness. Additionally, we show that the performance of VAE-based system
in the i-vectors space is close to that of the diagonal PLDA. Several
interesting results are also observed in the experiments with $\beta$-VAE. In
particular, we found that for $\beta\ll 1$, VAE can be trained to capture the
features of complex input data distributions in an effective way, which is hard
to obtain in the standard VAE ($\beta=1$)."
"Eliminating the negative effect of non-stationary environmental noise is a
long-standing research topic for automatic speech recognition but still remains
an important challenge. Data-driven supervised approaches, especially the ones
based on deep neural networks, have recently emerged as potential alternatives
to traditional unsupervised approaches and with sufficient training, can
alleviate the shortcomings of the unsupervised methods in various real-life
acoustic environments. In this light, we review recently developed,
representative deep learning approaches for tackling non-stationary additive
and convolutional degradation of speech with the aim of providing guidelines
for those involved in the development of environmentally robust speech
recognition systems. We separately discuss single- and multi-channel techniques
developed for the front-end and back-end of speech recognition systems, as well
as joint front-end and back-end training frameworks."
"We introduce Kapre, Keras layers for audio and music signal preprocessing.
Music research using deep neural networks requires a heavy and tedious
preprocessing stage, for which audio processing parameters are often ignored in
parameter optimisation. To solve this problem, Kapre implements time-frequency
conversions, normalisation, and data augmentation as Keras layers. We report
simple benchmark results, showing real-time on-GPU preprocessing adds a
reasonable amount of computation."
"Music tag words that describe music audio by text have different levels of
abstraction. Taking this issue into account, we propose a music classification
approach that aggregates multi-level and multi-scale features using pre-trained
feature extractors. In particular, the feature extractors are trained in
sample-level deep convolutional neural networks using raw waveforms. We show
that this approach achieves state-of-the-art results on several music
classification datasets."
"This study deals with the classification of Instrumentals and Songs in a
bigger musical database than what was used in all previous studies. Songs are
musical pieces containing singing voice, contrary to Instrumentals. This
research tackles the imbalance between the number of Instrumentals and the
numerous Songs present in industrial musical databases. Our work considers the
low precision of automatically generated playlists from content-based audio
retrieval. Indeed, previous works failed to address an efficient Instrumental
detection algorithm. We set up three experiments to assess the flaws of
previous works on an original and bigger musical database. This paper posits a
new approach that uses the presence probability of a frame's predicted singing
voice to deduce the track tag, i.e., whether the track is an Instrumental or a
Song. The main novelties are twofold. Firstly, we propose two sets of features
at the track scale based on frame predictions. Secondly, we propose a paradigm
that focuses on minority classes in musical databases and thus enhances general
user satisfaction. The suggested approach has a better Instrumental detection.
Thus, it can be used to generate thematic playlists with up to three times less
false positives than existing playlists. Furthermore, we provide the source
code to guarantee reproducible research. We also propose clues for further
research toward faultless playlists for other tags."
"We present a semantic vector space model for capturing complex polyphonic
musical context. A word2vec model based on a skip-gram representation with
negative sampling was used to model slices of music from a dataset of
Beethoven's piano sonatas. A visualization of the reduced vector space using
t-distributed stochastic neighbor embedding shows that the resulting embedded
vector space captures tonal relationships, even without any explicit
information about the musical contents of the slices. Secondly, an excerpt of
the Moonlight Sonata from Beethoven was altered by replacing slices based on
context similarity. The resulting music shows that the selected slice based on
similar word2vec context also has a relatively short tonal distance from the
original slice."
"Long Short-Term Memory networks (LSTMs) can be trained to realize inverse
control of physics-based sound synthesizers. Physics-based sound synthesizers
simulate the laws of physics to produce output sound according to input gesture
signals. When a user's gestures are measured in real time, she or he can use
them to control physics-based sound synthesizers, thereby creating simulated
virtual instruments. An intriguing question is how to program a computer to
learn to play such physics-based models. This work demonstrates that LSTMs can
be trained to accomplish this inverse control task with four physics-based
sound synthesizers."
"The increasing accuracy of automatic chord estimation systems, the
availability of vast amounts of heterogeneous reference annotations, and
insights from annotator subjectivity research make chord label personalization
increasingly important. Nevertheless, automatic chord estimation systems are
historically exclusively trained and evaluated on a single reference
annotation. We introduce a first approach to automatic chord label
personalization by modeling subjectivity through deep learning of a harmonic
interval-based chord label representation. After integrating these
representations from multiple annotators, we can accurately personalize chord
labels for individual annotators from a single model and the annotators' chord
label vocabulary. Furthermore, we show that chord personalization using
multiple reference annotations outperforms using a single reference annotation."
"Convolutional neural networks (CNNs) have been successfully applied on both
discriminative and generative modeling for music-related tasks. For a
particular task, the trained CNN contains information representing the decision
making or the abstracting process. One can hope to manipulate existing music
based on this 'informed' network and create music with new features
corresponding to the knowledge obtained by the network. In this paper, we
propose a method to utilize the stored information from a CNN trained on
musical genre classification task. The network was composed of three
convolutional layers, and was trained to classify five-second song clips into
five different genres. After training, randomly selected clips were modified by
maximizing the sum of outputs from the network layers. In addition to the
potential of such CNNs to produce interesting audio transformation, more
information about the network and the original music could be obtained from the
analysis of the generated features since these features indicate how the
network 'understands' the music."
"We propose a novel neural network model for music signal processing using
vector product neurons and dimensionality transformations. Here, the inputs are
first mapped from real values into three-dimensional vectors then fed into a
three-dimensional vector product neural network where the inputs, outputs, and
weights are all three-dimensional values. Next, the final outputs are mapped
back to the reals. Two methods for dimensionality transformation are proposed,
one via context windows and the other via spectral coloring. Experimental
results on the iKala dataset for blind singing voice separation confirm the
efficacy of our model."
"Presented is a method of generating a full drum kit part for a provided
kick-drum sequence. A sequence to sequence neural network model used in natural
language translation was adopted to encode multiple musical styles and an
online survey was developed to test different techniques for sampling the
output of the softmax function. The strongest results were found using a
sampling technique that drew from the three most probable outputs at each
subdivision of the drum pattern but the consistency of output was found to be
heavily dependent on style."
"One of the decisions that arise when designing a neural network for any
application is how the data should be represented in order to be presented to,
and possibly generated by, a neural network. For audio, the choice is less
obvious than it seems to be for visual images, and a variety of representations
have been used for different applications including the raw digitized sample
stream, hand-crafted features, machine discovered features, MFCCs and variants
that include deltas, and a variety of spectral representations. This paper
reviews some of these representations and issues that arise, focusing
particularly on spectrograms for generating audio using neural networks for
style transfer."
"This paper deals with the problem of audio source separation. To handle the
complex and ill-posed nature of the problems of audio source separation, the
current state-of-the-art approaches employ deep neural networks to obtain
instrumental spectra from a mixture. In this study, we propose a novel network
architecture that extends the recently developed densely connected
convolutional network (DenseNet), which has shown excellent results on image
classification tasks. To deal with the specific problem of audio source
separation, an up-sampling layer, block skip connection and band-dedicated
dense blocks are incorporated on top of DenseNet. The proposed approach takes
advantage of long contextual information and outperforms state-of-the-art
results on SiSEC 2016 competition by a large margin in terms of
signal-to-distortion ratio. Moreover, the proposed architecture requires
significantly fewer parameters and considerably less training time compared
with other methods."
"We present the first approach to automated audio captioning. We employ an
encoder-decoder scheme with an alignment model in between. The input to the
encoder is a sequence of log mel-band energies calculated from an audio file,
while the output is a sequence of words, i.e. a caption. The encoder is a
multi-layered, bi-directional gated recurrent unit (GRU) and the decoder a
multi-layered GRU with a classification layer connected to the last GRU of the
decoder. The classification layer and the alignment model are fully connected
layers with shared weights between timesteps. The proposed method is evaluated
using data drawn from a commercial sound effects library, ProSound Effects. The
resulting captions were rated through metrics utilized in machine translation
and image captioning fields. Results from metrics show that the proposed method
can predict words appearing in the original caption, but not always correctly
ordered."
"The development of audio event recognition models requires labeled training
data, which are generally hard to obtain. One promising source of recordings of
audio events is the large amount of multimedia data on the web. In particular,
if the audio content analysis must itself be performed on web audio, it is
important to train the recognizers themselves from such data. Training from
these web data, however, poses several challenges, the most important being the
availability of labels : labels, if any, that may be obtained for the data are
generally {\em weak}, and not of the kind conventionally required for training
detectors or classifiers. We propose that learning algorithms that can exploit
weak labels offer an effective method to learn from web data. We then propose a
robust and efficient deep convolutional neural network (CNN) based framework to
learn audio event recognizers from weakly labeled data. The proposed method can
train from and analyze recordings of variable length in an efficient manner and
outperforms a network trained with {\em strongly labeled} web data by a
considerable margin."
"The goal of this study is the automatic detection of onsets of the singing
voice in polyphonic audio recordings. Starting with a hypothesis that the
knowledge of the current position in a metrical cycle (i.e. metrical accent)
can improve the accuracy of vocal note onset detection, we propose a novel
probabilistic model to jointly track beats and vocal note onsets. The proposed
model extends a state of the art model for beat and meter tracking, in which
a-priori probability of a note at a specific metrical accent interacts with the
probability of observing a vocal note onset. We carry out an evaluation on a
varied collection of multi-instrument datasets from two music traditions
(English popular music and Turkish makam) with different types of metrical
cycles and singing styles. Results confirm that the proposed model reasonably
improves vocal note onset detection accuracy compared to a baseline model that
does not take metrical position into account."
"Although great progresses have been made in automatic speech recognition
(ASR), significant performance degradation is still observed when recognizing
multi-talker mixed speech. In this paper, we propose and evaluate several
architectures to address this problem under the assumption that only a single
channel of mixed signal is available. Our technique extends permutation
invariant training (PIT) by introducing the front-end feature separation module
with the minimum mean square error (MSE) criterion and the back-end recognition
module with the minimum cross entropy (CE) criterion. More specifically, during
training we compute the average MSE or CE over the whole utterance for each
possible utterance-level output-target assignment, pick the one with the
minimum MSE or CE, and optimize for that assignment. This strategy elegantly
solves the label permutation problem observed in the deep learning based
multi-talker mixed speech separation and recognition systems. The proposed
architectures are evaluated and compared on an artificially mixed AMI dataset
with both two- and three-talker mixed speech. The experimental results indicate
that our proposed architectures can cut the word error rate (WER) by 45.0% and
25.0% relatively against the state-of-the-art single-talker speech recognition
system across all speakers when their energies are comparable, for two- and
three-talker mixed speech, respectively. To our knowledge, this is the first
work on the multi-talker mixed speech recognition on the challenging
speaker-independent spontaneous large vocabulary continuous speech task."
"MARF is an open-source research platform and a collection of
voice/sound/speech/text and natural language processing (NLP) algorithms
written in Java and arranged into a modular and extensible framework
facilitating addition of new algorithms. MARF can run distributively over the
network and may act as a library in applications or be used as a source for
learning and extension. A few example applications are provided to show how to
use the framework. There is an API reference in the Javadoc format as well as
this set of accompanying notes with the detailed description of the
architectural design, algorithms, and applications. MARF and its applications
are released under a BSD-style license and is hosted at SourceForge.net. This
document provides the details and the insight on the internals of MARF and some
of the mentioned applications."
"Popular music is a key cultural expression that has captured listeners'
attention for ages. Many of the structural regularities underlying musical
discourse are yet to be discovered and, accordingly, their historical evolution
remains formally unknown. Here we unveil a number of patterns and metrics
characterizing the generic usage of primary musical facets such as pitch,
timbre, and loudness in contemporary western popular music. Many of these
patterns and metrics have been consistently stable for a period of more than
fifty years, thus pointing towards a great degree of conventionalism.
Nonetheless, we prove important changes or trends related to the restriction of
pitch transitions, the homogenization of the timbral palette, and the growing
loudness levels. This suggests that our perception of the new would be rooted
on these changing characteristics. Hence, an old tune could perfectly sound
novel and fashionable, provided that it consisted of common harmonic
progressions, changed the instrumentation, and increased the average loudness."
"It is well known that speaker verification systems are subject to spoofing
attacks. The Automatic Speaker Verification Spoofing and Countermeasures
Challenge -- ASVSpoof2015 -- provides a standard spoofing database, containing
attacks based on synthetic speech, along with a protocol for experiments. This
paper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based
on deep neural networks, working both as a classifier and as a feature
extraction module for a GMM and a SVM classifier. Results show the validity of
this approach, achieving less than 0.5\% EER for known attacks."
"Consonance is related to the perception of pleasantness arising from a
combination of sounds and has been approached quantitatively using mathematical
relations, physics, information theory, and psychoacoustics. Tonal consonance
is present in timbre, musical tuning, harmony, and melody, and it is used for
conveying sensations, perceptions, and emotions in music. It involves the
physical properties of sound waves and is used to study melody and harmony
through musical intervals and chords. From the perspective of complexity, the
macroscopic properties of a system with many parts frequently rely on the
statistical properties of its constituent elements. Here we show how the tonal
consonance parameters for complex tones can be used to study complexity in
music. We apply this formalism to melody, showing that melodic lines in musical
pieces can be described in terms of the physical properties of melodic
intervals and the existence of an entropy extremalization principle subject to
psychoacoustic macroscopic constraints with musical meaning. This result
connects the human perception of consonance with the complexity of human
creativity in music through the physical properties of the musical stimulus."