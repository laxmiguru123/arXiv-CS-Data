summary
"This paper presents a multiscale decomposition algorithm. Unlike standard
wavelet transforms, the proposed operator is both linear and shift invariant.
The central idea is to obtain shift invariance by averaging the aligned wavelet
transform projections over all circular shifts of the signal. It is shown how
the same transform can be obtained by a linear filter bank."
"We give a systematic, abstract formulation of the image normalization method
as applied to a general group of image transformations, and then illustrate the
abstract analysis by applying it to the hierarchy of viewing transformations of
a planar object."
"This paper presents an invariant under scaling and linear brightness change.
The invariant is based on differentials and therefore is a local feature.
Rotationally invariant 2-d differential Gaussian operators up to third order
are proposed for the implementation of the invariant. The performance is
analyzed by simulating a camera zoom-out."
"We describe a simple, but efficient algorithm for the generation of dilated
contours from bilevel images. The initial part of the contour extraction is
explained to be a good candidate for parallel computer code generation. The
remainder of the algorithm is of linear nature."
"Fractal image compression, Culik's image compression and zerotree prediction
coding of wavelet image decomposition coefficients succeed only because typical
images being compressed possess a significant degree of self-similarity.
Besides the common concept, these methods turn out to be even more tightly
related, to the point of algorithmical reducibility of one technique to
another. The goal of the present paper is to demonstrate these relations.
  The paper offers a plain-term interpretation of Culik's image compression, in
regular image processing terms, without resorting to finite state machines and
similar lofty language. The interpretation is shown to be algorithmically
related to an IFS fractal image compression method: an IFS can be exactly
transformed into Culik's image code. Using this transformation, we will prove
that in a self-similar (part of an) image any zero wavelet coefficient is the
root of a zerotree, or its branch.
  The paper discusses the zerotree coding of (wavelet/projection) coefficients
as a common predictor/corrector, applied vertically through different layers of
a multiresolutional decomposition, rather than within the same view. This
interpretation leads to an insight into the evolution of image compression
techniques: from a causal single-layer prediction, to non-causal same-view
predictions (wavelet decomposition among others) and to a causal cross-layer
prediction (zero-trees, Culik's method)."
"This paper presents invariants under gamma correction and similarity
transformations. The invariants are local features based on differentials which
are implemented using derivatives of the Gaussian. The use of the proposed
invariant representation is shown to yield improved correlation results in a
template matching scenario."
"This work deals with content-based video indexing. Our viewpoint is
semi-automatic analysis of compressed video. We consider the possible
applications of motion analysis and moving object detection : assisting moving
object indexing, summarising videos, and allowing image and motion queries. We
propose an approach based on interest points. As first results, we test and
compare the stability of different types of interest point detectors in
compressed sequences."
"The paper has established and verified the theory prevailing widely among
image and pattern recognition specialists that the bottom-up indirect regional
matching process is the more stable and the more robust than the global
matching process against concentrated types of noise represented by clutter,
outlier or occlusion in the imagery. We have demonstrated this by analyzing the
effect of concentrated noise on a typical decision making process of a
simplified two candidate voting model where our theorem establishes the lower
bounds to a critical breakdown point of election (or decision) result by the
bottom-up matching process are greater than the exact bound of the global
matching process implying that the former regional process is capable of
accommodating a higher level of noise than the latter global process before the
result of decision overturns. We present a convincing experimental verification
supporting not only the theory by a white-black flag recognition problem in the
presence of localized noise but also the validity of the conjecture by a facial
recognition problem that the theorem remains valid for other decision making
processes involving an important dimension-reducing transform such as principal
component analysis or a Gabor transform."
"A Bayesian classifier that up-weights the differences in the attribute values
is discussed. Using four popular datasets from the UCI repository, some
interesting features of the network are illustrated. The network is suitable
for classification problems."
"The difference-boosting algorithm is used on letters dataset from the UCI
repository to classify distorted raster images of English alphabets. In
contrast to rather complex networks, the difference-boosting is found to
produce comparable or better classification efficiency on this complex problem."
"We present a new method to transform the spectral pixel information of a
micrograph into an affine geometric description, which allows us to analyze the
morphology of granular materials. We use spectral and pulse-coupled neural
network based segmentation techniques to generate blobs, and a newly developed
algorithm to extract dilated contours. A constrained Delaunay tesselation of
the contour points results in a triangular mesh. This mesh is the basic
ingredient of the Chodal Axis Transform, which provides a morphological
decomposition of shapes. Such decomposition allows for grain separation and the
efficient computation of the statistical features of granular materials."
"The problem of searching for a model-based scene interpretation is analyzed
within a probabilistic framework. Object models are formulated as generative
models for range data of the scene. A new statistical criterion, the truncated
object probability, is introduced to infer an optimal sequence of object
hypotheses to be evaluated for their match to the data. The truncated
probability is partly determined by prior knowledge of the objects and partly
learned from data. Some experiments on sequence quality and object segmentation
and recognition from stereo data are presented. The article recovers classic
concepts from object recognition (grouping, geometric hashing, alignment) from
the probabilistic perspective and adds insight into the optimal ordering of
object hypotheses for evaluation. Moreover, it introduces point-relation
densities, a key component of the truncated probability, as statistical models
of local surface shape."
"We study theoretical and computational aspects of the least squares fit (LSF)
of circles and circular arcs. First we discuss the existence and uniqueness of
LSF and various parametrization schemes. Then we evaluate several popular
circle fitting algorithms and propose a new one that surpasses the existing
methods in reliability. We also discuss and compare direct (algebraic) circle
fits."
"We study the problem of fitting parametrized curves to noisy data. Under
certain assumptions (known as Cartesian and radial functional models), we
derive asymptotic expressions for the bias and the covariance matrix of the
parameter estimates. We also extend Kanatani's version of the Cramer-Rao lower
bound, which he proved for unbiased estimates only, to more general estimates
that include many popular algorithms (most notably, the orthogonal least
squares and algebraic fits). We then show that the gradient-weighted algebraic
fit is statistically efficient and describe all other statistically efficient
algebraic fits."
"Most algorithms in 3D computer vision rely on the pinhole camera model
because of its simplicity, whereas virtually all imaging devices introduce
certain amount of nonlinear distortion, where the radial distortion is the most
severe part. Common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
radial distortion model with an easy analytical undistortion formula, which
also belongs to the polynomial approximation category. Experimental results are
presented to show that with this radial distortion model, satisfactory accuracy
is achieved. An application of the new radial distortion model is non-iterative
yellow line alignment with a calibrated camera on ODIS, a robot built in our
CSOIS."
"Common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
radial distortion model with an easy analytical undistortion formula, which
also belongs to the polynomial approximation category. Experimental results are
presented to show that with this radial distortion model, satisfactory accuracy
is achieved."
"The common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
class of rational radial distortion models with easy analytical undistortion
formulae. Experimental results are presented to show that with this class of
rational radial distortion models, satisfactory and comparable accuracy is
achieved."
"The common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
piecewise radial distortion model with easy analytical undistortion formula.
The motivation for seeking a piecewise radial distortion model is that, when a
camera is resulted in a low quality during manufacturing, the nonlinear radial
distortion can be complex. Using low order polynomials to approximate the
radial distortion might not be precise enough. On the other hand, higher order
polynomials suffer from the inverse problem. With the new piecewise radial
distortion function, more flexibility is obtained and the radial undistortion
can be performed analytically. Experimental results are presented to show that
with this new piecewise radial distortion model, better performance is achieved
than that using the single function. Furthermore, a comparable performance with
the conventional polynomial model using 2 coefficients can also be
accomplished."
"The task of camera calibration is to estimate the intrinsic and extrinsic
parameters of a camera model. Though there are some restricted techniques to
infer the 3-D information about the scene from uncalibrated cameras, effective
camera calibration procedures will open up the possibility of using a wide
range of existing algorithms for 3-D reconstruction and recognition.
  The applications of camera calibration include vision-based metrology, robust
visual platooning and visual docking of mobile robots where the depth
information is important."
"The commonly used radial distortion model for camera calibration is in fact
an assumption or a restriction. In practice, camera distortion could happen in
a general geometrical manner that is not limited to the radial sense. This
paper proposes a simplified geometrical distortion modeling method by using two
different radial distortion functions in the two image axes. A family of
simplified geometric distortion models is proposed, which are either simple
polynomials or the rational functions of polynomials. Analytical geometric
undistortion is possible using two of the distortion functions discussed in
this paper and their performance can be improved by applying a piecewise
fitting idea. Our experimental results show that the geometrical distortion
models always perform better than their radial distortion counterparts.
Furthermore, the proposed geometric modeling method is more appropriate for
cameras whose distortion is not perfectly radially symmetric around the center
of distortion."
"In the paper will be presented a safety and security system based on
fingerprint technology. The results suggest a new scenario where the new cars
can use a fingerprint sensor integrated in car handle to allow access and in
the dashboard as starter button."
"In the paper will be presented a safety system based on iridology. The
results suggest a new scenario where the security problem in supervised and
unsupervised areas can be treat with the present system and the iris image
recognition."
"For many tracking and surveillance applications, background subtraction
provides an effective means of segmenting objects moving in front of a static
background. Researchers have traditionally used combinations of morphological
operations to remove the noise inherent in the background-subtracted result.
Such techniques can effectively isolate foreground objects, but tend to lose
fidelity around the borders of the segmentation, especially for noisy input.
This paper explores the use of a minimum graph cut algorithm to segment the
foreground, resulting in qualitatively and quantitiatively cleaner
segmentations. Experiments on both artificial and real data show that the
graph-based method reduces the error around segmented foreground objects. A
MATLAB code implementation is available at
http://www.cs.smith.edu/~nhowe/research/code/#fgseg"
"A method of temporal factor prognosis of TE (tick-borne encephalitis)
infection has been developed. The high precision of the prognosis results for a
number of geographical regions of Primorsky Krai has been achieved. The method
can be applied not only to epidemiological research but also to others."
"Despite encouraging recent progresses in ensemble approaches, classification
methods seem to have reached a plateau in development. Further advances depend
on a better understanding of geometrical and topological characteristics of
point sets in high-dimensional spaces, the preservation of such characteristics
under feature transformations and sampling processes, and their interaction
with geometrical models used in classifiers. We discuss an attempt to measure
such properties from data sets and relate them to classifier accuracies."
"This publication presents methods for face detection, analysis and
recognition: fast normalized cross-correlation (fast correlation coefficient)
between multiple templates based face pre-detection method, method for
detection of exact face contour based on snakes and Generalized Gradient Vector
Flow field, method for combining recognition algorithms based on Cumulative
Match Characteristics in order to increase recognition speed and accuracy, and
face recognition method based on Principal Component Analysis of the Wavelet
Packet Decomposition allowing to use PCA - based recognition method with large
number of training images. For all the methods are presented experimental
results and comparisons of speed and accuracy with large face databases."
"This paper presents a blind detection and compensation technique for camera
lens geometric distortions. The lens distortion introduces higher-order
correlations in the frequency domain and in turn it can be detected using
higher-order spectral analysis tools without assuming any specific calibration
target. The existing blind lens distortion removal method only considered a
single-coefficient radial distortion model. In this paper, two coefficients are
considered to model approximately the geometric distortion. All the models
considered have analytical closed-form inverse formulae."
"We study image compression by a separable wavelet basis
$\big\{\psi(2^{k_1}x-i)\psi(2^{k_2}y-j),$ $\phi(x-i)\psi(2^{k_2}y-j),$
$\psi(2^{k_1}(x-i)\phi(y-j),$ $\phi(x-i)\phi(y-i)\big\},$ where $k_1, k_2 \in
\mathbb{Z}_+$; $i,j\in\mathbb{Z}$; and $\phi,\psi$ are elements of a standard
biorthogonal wavelet basis in $L_2(\mathbb{R})$. Because $k_1\ne k_2$, the
supports of the basis elements are rectangles, and the corresponding transform
is known as the {\em rectangular wavelet transform}. We prove that if
one-dimensional wavelet basis has $M$ dual vanishing moments then the rate of
approximation by $N$ coefficients of rectangular wavelet transform is
$\mathcal{O}(N^{-M}\log^C N)$ for functions with mixed derivative of order $M$
in each direction.
  The square wavelet transform yields the approximation rate is
$\mathcal{O}(N^{-M/2})$ for functions with all derivatives of the total order
$M$. Thus, the rectangular wavelet transform can outperform the square one if
an image has a mixed derivative. We provide experimental comparison of image
compression which shows that rectangular wavelet transform outperform the
square one."
"The Gradient Vector Flow (GVF) is a vector diffusion approach based on
Partial Differential Equations (PDEs). This method has been applied together
with snake models for boundary extraction medical images segmentation. The key
idea is to use a diffusion-reaction PDE to generate a new external force field
that makes snake models less sensitivity to initialization as well as improves
the snake's ability to move into boundary concavities. In this paper, we
firstly review basic results about convergence and numerical analysis of usual
GVF schemes. We point out that GVF presents numerical problems due to
discontinuities image intensity. This point is considered from a practical
viewpoint from which the GVF parameters must follow a relationship in order to
improve numerical convergence. Besides, we present an analytical analysis of
the GVF dependency from the parameters values. Also, we observe that the method
can be used for multiply connected domains by just imposing the suitable
boundary condition. In the experimental results we verify these theoretical
points and demonstrate the utility of GVF on a segmentation approach that we
have developed based on snakes."
"Image information content is known to be a complicated and controvercial
problem. This paper posits a new image information content definition.
Following the theory of Solomonoff-Kolmogorov-Chaitin's complexity, we define
image information content as a set of descriptions of imafe data structures.
Three levels of such description can be generally distinguished: 1)the global
level, where the coarse structure of the entire scene is initially outlined; 2)
the intermediate level, where structures of separate, non-overlapping image
regions usually associated with individual scene objects are deliniated; and 3)
the low-level description, where local image structures observed in a limited
and restricted field of view are resolved. A technique for creating such image
information content descriptors is developed. Its algorithm is presented and
elucidated with some examples, which demonstrate the effectiveness of the
proposed approach."
"In this paper we present an unconventional image segmentation approach which
is devised to meet the requirements of image understanding and pattern
recognition tasks. Generally image understanding assumes interplay of two
sub-processes: image information content discovery and image information
content interpretation. Despite of its widespread use, the notion of ""image
information content"" is still ill defined, intuitive, and ambiguous. Most
often, it is used in the Shannon's sense, which means information content
assessment averaged over the whole signal ensemble. Humans, however,rarely
resort to such estimates. They are very effective in decomposing images into
their meaningful constituents and focusing attention to the perceptually
relevant image parts. We posit that following the latest findings in human
attention vision studies and the concepts of Kolmogorov's complexity theory an
unorthodox segmentation approach can be proposed that provides effective image
decomposition to information preserving image fragments well suited for
subsequent image interpretation. We provide some illustrative examples,
demonstrating effectiveness of this approach."
"We present an automatic face verification system inspired by known properties
of biological systems. In the proposed algorithm the whole image is converted
from the spatial to polar frequency domain by a Fourier-Bessel Transform (FBT).
Using the whole image is compared to the case where only face image regions
(local analysis) are considered. The resulting representations are embedded in
a dissimilarity space, where each image is represented by its distance to all
the other images, and a Pseudo-Fisher discriminator is built. Verification test
results on the FERET database showed that the local-based algorithm outperforms
the global-FBT version. The local-FBT algorithm performed as state-of-the-art
methods under different testing conditions, indicating that the proposed system
is highly robust for expression, age, and illumination variations. We also
evaluated the performance of the proposed system under strong occlusion
conditions and found that it is highly robust for up to 50% of face occlusion.
Finally, we automated completely the verification system by implementing face
and eye detection algorithms. Under this condition, the local approach was only
slightly superior to the global approach."
"A novel biologically motivated face recognition algorithm based on polar
frequency is presented. Polar frequency descriptors are extracted from face
images by Fourier-Bessel transform (FBT). Next, the Euclidean distance between
all images is computed and each image is now represented by its dissimilarity
to the other images. A Pseudo-Fisher Linear Discriminant was built on this
dissimilarity space. The performance of Discrete Fourier transform (DFT)
descriptors, and a combination of both feature types was also evaluated. The
algorithms were tested on a 40- and 1196-subjects face database (ORL and FERET,
respectively). With 5 images per subject in the training and test datasets,
error rate on the ORL database was 3.8, 1.25 and 0.2% for the FBT, DFT, and the
combined classifier, respectively, as compared to 2.6% achieved by the best
previous algorithm. The most informative polar frequency features were
concentrated at low-to-medium angular frequencies coupled to low radial
frequencies. On the FERET database, where an affine normalization
pre-processing was applied, the FBT algorithm outperformed only the PCA in a
rank recognition test. However, it achieved performance comparable to
state-of-the-art methods when evaluated by verification tests. These results
indicate the high informative value of the polar frequency content of face
images in relation to recognition and verification tasks, and that the
Cartesian frequency content can complement information about the subjects'
identity, but possibly only when the images are not pre-normalized. Possible
implications for human face recognition are discussed."
"We present a novel local-based face verification system whose components are
analogous to those of biological systems. In the proposed system, after global
registration and normalization, three eye regions are converted from the
spatial to polar frequency domain by a Fourier-Bessel Transform. The resulting
representations are embedded in a dissimilarity space, where each image is
represented by its distance to all the other images. In this dissimilarity
space a Pseudo-Fisher discriminator is built. ROC and equal error rate
verification test results on the FERET database showed that the system
performed at least as state-of-the-art methods and better than a system based
on polar Fourier features. The local-based system is especially robust to
facial expression and age variations, but sensitive to registration errors."
"We present a method for automated segmentation of the vasculature in retinal
images. The method produces segmentations by classifying each image pixel as
vessel or non-vessel, based on the pixel's feature vector. Feature vectors are
composed of the pixel's intensity and continuous two-dimensional Morlet wavelet
transform responses taken at multiple scales. The Morlet wavelet is capable of
tuning to specific frequencies, thus allowing noise filtering and vessel
enhancement in a single step. We use a Bayesian classifier with
class-conditional probability density functions (likelihoods) described as
Gaussian mixtures, yielding a fast classification, while being able to model
complex decision surfaces and compare its performance with the linear minimum
squared error classifier. The probability distributions are estimated based on
a training set of labeled pixels obtained from manual segmentations. The
method's performance is evaluated on publicly available DRIVE and STARE
databases of manually labeled non-mydriatic images. On the DRIVE database, it
achieves an area under the receiver operating characteristic (ROC) curve of
0.9598, being slightly superior than that presented by the method of Staal et
al."
"In this paper, a decision support system for ship identification is
presented. The system receives as input a silhouette of the vessel to be
identified, previously extracted from a side view of the object. This view
could have been acquired with imaging sensors operating at different spectral
ranges (CCD, FLIR, image intensifier). The input silhouette is preprocessed and
compared to those stored in a database, retrieving a small number of potential
matches ranked by their similarity to the target silhouette. This set of
potential matches is presented to the system operator, who makes the final ship
identification. This system makes use of an evolved version of the Curvature
Scale Space (CSS) representation. In the proposed approach, it is curvature
extrema, instead of zero crossings, that are tracked during silhouette
evolution, hence improving robustness and enabling to cope successfully with
cases where the standard CCS representation is found to be unstable. Also, the
use of local curvature was replaced with the more robust concept of lobe
concavity, with significant additional gains in performance. Experimental
results on actual operational imagery prove the excellent performance and
robustness of the developed method."
"Metal melting on release after explosion is a physical system far from
quilibrium. A complete physical model of this system does not exist, because
many interrelated effects have to be considered. General methodology needs to
be developed so as to describe and understand physical phenomena involved.
  The high noise of the data, moving blur of images, the high degree of
uncertainty due to the different types of sensors, and the information
entangled and hidden inside the noisy images makes reasoning about the physical
processes very difficult. Major problems include proper information extraction
and the problem of reconstruction, as well as prediction of the missing data.
In this paper, several techniques addressing the first problem are given,
building the basis for tackling the second problem."
"A novel algorithm for tunable compression to within the precision of
reproduction targets, or storage, is proposed. The new algorithm is termed the
`Perceptron Algorithm', which utilises simple existing concepts in a novel way,
has multiple immediate commercial application aspects as well as it opens up a
multitude of fronts in computational science and technology. The aims of this
paper are to present the concepts underlying the algorithm, observations by its
application to some example cases, and the identification of a multitude of
potential areas of applications such as: image compression by orders of
magnitude, signal compression including sound as well, image analysis in a
multilayered detailed analysis, pattern recognition and matching and rapid
database searching (e.g. face recognition), motion analysis, biomedical
applications e.g. in MRI and CAT scan image analysis and compression, as well
as hints on the link of these ideas to the way how biological memory might work
leading to new points of view in neural computation. Commercial applications of
immediate interest are the compression of images at the source (e.g.
photographic equipment, scanners, satellite imaging systems), DVD film
compression, pay-per-view downloads acceleration and many others identified in
the present paper at its conclusion and future work section."
"This short article presents an alternative view of high resolution imaging
from various sources with the aim of the discovery of potential sites of
archaeological importance, or sites that exhibit `anomalies' such that they may
merit closer inspection and analysis. It is conjectured, and to a certain
extent demonstrated here, that it is possible for advanced civilizations to
factor in erosion by natural processes into a large scale design so that main
features be preserved even with the passage of millions of years. Alternatively
viewed, even without such intent embedded in a design left for posterity, it is
possible that a gigantic construction may naturally decay in such a way that
even cataclysmic (massive) events may leave sufficient information intact with
the passage of time, provided one changes the point of view from high
resolution images to enhanced blurred renderings of the sites in question."
"A novel algorithm is proposed for segmenting an image into multiple levels
using its mean and variance. Starting from the extreme pixel values at both
ends of the histogram plot, the algorithm is applied recursively on sub-ranges
computed from the previous step, so as to find a threshold level and a new
sub-range for the next step, until no significant improvement in image quality
can be achieved. The method makes use of the fact that a number of
distributions tend towards Dirac delta function, peaking at the mean, in the
limiting condition of vanishing variance. The procedure naturally provides for
variable size segmentation with bigger blocks near the extreme pixel values and
finer divisions around the mean or other chosen value for better visualization.
Experiments on a variety of images show that the new algorithm effectively
segments the image in computationally very less time."
"We present an algorithm that enables one to perform locally adaptive block
thresholding, while maintaining image continuity. Images are divided into
sub-images based some standard image attributes and thresholding technique is
employed over the sub-images. The present algorithm makes use of the thresholds
of neighboring sub-images to calculate a range of values. The image continuity
is taken care by choosing the threshold of the sub-image under consideration to
lie within the above range. After examining the average range values for
various sub-image sizes of a variety of images, it was found that the range of
acceptable threshold values is substantially high, justifying our assumption of
exploiting the freedom of range for bringing out local details."
"This communication describes a representation of images as a set of edges
characterized by their position and orientation. This representation allows the
comparison of two images and the computation of their similarity. The first
step in this computation of similarity is the seach of a geometrical basis of
the two dimensional space where the two images are represented simultaneously
after transformation of one of them. Presently, this simultaneous
representation takes into account a shift and a scaling ; it may be extended to
rotations or other global geometrical transformations. An elementary
probabilistic computation shows that a sufficient but not excessive number of
trials (a few tens) ensures that the exhibition of this common basis is
guaranteed in spite of possible errors in the detection of edges. When this
first step is performed, the search of similarity between the two images
reduces to counting the coincidence of edges in the two images. The approach
may be applied to many problems of pattern matching ; it was checked on face
recognition."
"In this paper, we focus on Fourier analysis and holographic transforms for
signal representation. For instance, in the case of image processing, the
holographic representation has the property that an arbitrary portion of the
transformed image enables reconstruction of the whole image with details
missing. We focus on holographic representation defined through the Fourier
Transforms. Thus, We firstly review some results in Fourier transform and
Fourier series. Next, we review the Discrete Holographic Fourier Transform
(DHFT) for image representation. Then, we describe the contributions of our
work. We show a simple scheme for progressive transmission based on the DHFT.
Next, we propose the Continuous Holographic Fourier Transform (CHFT) and
discuss some theoretical aspects of it for 1D signals. Finally, some testes are
presented in the experimental results"
"Feature extraction and matching are among central problems of computer
vision. It is inefficent to search features over all locations and scales.
Neurophysiological evidence shows that to locate objects in a digital image the
human visual system employs visual attention to a specific object while
ignoring others. The brain also has a mechanism to search from coarse to fine.
In this paper, we present a feature extractor and an associated hierarchical
searching model to simulate such processes. With the hierarchical
representation of the object, coarse scanning is done through the matching of
the larger scale and precise localization is conducted through the matching of
the smaller scale. Experimental results justify the proposed model in its
effectiveness and efficiency to localize features."
"In this article we propose a novel face recognition method based on Principal
Component Analysis (PCA) and Log-Gabor filters. The main advantages of the
proposed method are its simple implementation, training, and very high
recognition accuracy. For recognition experiments we used 5151 face images of
1311 persons from different sets of the FERET and AR databases that allow to
analyze how recognition accuracy is affected by the change of facial
expressions, illumination, and aging. Recognition experiments with the FERET
database (containing photographs of 1196 persons) showed that our method can
achieve maximal 97-98% first one recognition rate and 0.3-0.4% Equal Error
Rate. The experiments also showed that the accuracy of our method is less
affected by eye location errors and used image normalization method than of
traditional PCA -based recognition method."
"In this article we propose a method for the recognition of faces with
different facial expressions. For recognition we extract feature vectors by
using log-Gabor filters of multiple orientations and scales. Using sliding
window algorithm and variances -based masking these features are extracted at
image regions that are less affected by the changes of facial expressions.
Extracted features are passed to the Principal Component Analysis (PCA) -based
recognition method. The results of face recognition experiments using
expression variant faces showed that the proposed method could achieve higher
recognition accuracy than many other methods. For development and testing we
used facial images from the AR and FERET databases. Using facial photographs of
more than one thousand persons from the FERET database the proposed method
achieved 96.6-98.9% first one recognition rate and 0.2-0.6% Equal Error Rate
(EER)."
"Regularization functionals that lower level set boundary length when used
with L^1 fidelity functionals on signal de-noising on images create artifacts.
These are (i) rounding of corners, (ii) shrinking of radii, (iii) shrinking of
cusps, and (iv) non-smoothing of staircasing. Regularity functionals based upon
total curvature of level set boundaries do not create artifacts (i) and (ii).
An adjusted fidelity term based on the flat norm on the current (a
distributional graph) representing the density of curvature of level sets
boundaries can minimize (iii) by weighting the position of a cusp. A regularity
term to eliminate staircasing can be based upon the mass of the current
representing the graph of an image function or its second derivatives.
Densities on the Grassmann bundle of the Grassmann bundle of the ambient space
of the graph can be used to identify patterns, textures, occlusion and lines."
"Raster images can have a range of various distortions connected to their
raster structure. Upsampling them might in effect substantially yield the
raster structure of the original image, known as aliasing. The upsampling
itself may introduce aliasing into the upsampled image as well. The presented
method attempts to remove the aliasing using frequency filters based on the
discrete fast Fourier transform, and applied directionally in certain regions
placed along the edges in the image.
  As opposed to some anisotropic smoothing methods, the presented algorithm
aims to selectively reduce only the aliasing, preserving the sharpness of image
details.
  The method can be used as a post--processing filter along with various
upsampling algorithms. It was experimentally shown that the method can improve
the visual quality of the upsampled images."
"In this paper, we are interested in the application to video segmentation of
the discrete shape optimization problem involving the shape weighted perimeter
and an additional term depending on a parameter. Based on recent works and in
particular the one of Darbon and Sigelle, we justify the equivalence of the
shape optimization problem and a weighted total variation regularization. For
solving this problem, we adapt the projection algorithm proposed recently for
solving the basic TV regularization problem. Another solution to the shape
optimization investigated here is the graph cut technique. Both methods have
the advantage to lead to a global minimum. Since we can distinguish moving
objects from static elements of a scene by analyzing norm of the optical flow
vectors, we choose the optical flow norm as initial data. In order to have the
contour as close as possible to an edge in the image, we use a classical edge
detector function as the weight of the weighted total variation. This model has
been used in one of our former works. We also apply the same methods to a video
segmentation model used by Jehan-Besson, Barlaud and Aubert. In this case, only
standard perimeter is incorporated in the shape functional. We also propose
another way for finding moving objects by using an a contrario detection of
objects on the image obtained by solving the Rudin-Osher-Fatemi Total Variation
regularization problem.We can notice the segmentation can be associated to a
level set in the former methods."
"We present conditional expression (CE) for finding blurs convolved in given
images. The CE is given in terms of the zero-values of the blurs evaluated at
multi-point. The CE can detect multiple blur all at once. We illustrate the
multiple blur-detection by using a test image."
"A simple search method for finding a blur convolved in a given image is
presented. The method can be easily extended to a large blur. The method has
been experimentally tested with a model blurred image."
"We developed novel conditional expressions (CEs) for Lane and Bates' blind
deconvolution. The CEs are given in term of the derivatives of the zero-values
of the z-transform of given images. The CEs make it possible to automatically
detect multiple blur convolved in the given images all at once without
performing any analysis of the zero-sheets of the given images. We illustrate
the multiple blur-detection by the CEs for a model image"
"In this paper, we propose a global method for estimating the motion of a
camera which films a static scene. Our approach is direct, fast and robust, and
deals with adjacent frames of a sequence. It is based on a quadratic
approximation of the deformation between two images, in the case of a scene
with constant depth in the camera coordinate system. This condition is very
restrictive but we show that provided translation and depth inverse variations
are small enough, the error on optical flow involved by the approximation of
depths by a constant is small. In this context, we propose a new model of
camera motion, that allows to separate the image deformation in a similarity
and a ``purely'' projective application, due to change of optical axis
direction. This model leads to a quadratic approximation of image deformation
that we estimate with an M-estimator; we can immediatly deduce camera motion
parameters."
"Many image processing problems involve identifying the region in the image
domain occupied by a given entity in the scene. Automatic solution of these
problems requires models that incorporate significant prior knowledge about the
shape of the region. Many methods for including such knowledge run into
difficulties when the topology of the region is unknown a priori, for example
when the entity is composed of an unknown number of similar objects.
Higher-order active contours (HOACs) represent one method for the modelling of
non-trivial prior knowledge about shape without necessarily constraining region
topology, via the inclusion of non-local interactions between region boundary
points in the energy defining the model. The case of an unknown number of
circular objects arises in a number of domains, e.g. medical, biological,
nanotechnological, and remote sensing imagery. Regions composed of an a priori
unknown number of circles may be referred to as a `gas of circles'. In this
report, we present a HOAC model of a `gas of circles'. In order to guarantee
stable circles, we conduct a stability analysis via a functional Taylor
expansion of the HOAC energy around a circular shape. This analysis fixes one
of the model parameters in terms of the others and constrains the rest. In
conjunction with a suitable likelihood energy, we apply the model to the
extraction of tree crowns from aerial imagery, and show that the new model
outperforms other techniques."
"Irregular pyramids are made of a stack of successively reduced graphs
embedded in the plane. Such pyramids are used within the segmentation framework
to encode a hierarchy of partitions. The different graph models used within the
irregular pyramid framework encode different types of relationships between
regions. This paper compares different graph models used within the irregular
pyramid framework according to a set of relationships between regions. We also
define a new algorithm based on a pyramid of combinatorial maps which allows to
determine if one region contains the other using only local calculus."
"The aim of this study is to detect man-made cartographic objects in
high-resolution satellite images. New generation satellites offer a sub-metric
spatial resolution, in which it is possible (and necessary) to develop methods
at object level rather than at pixel level, and to exploit structural features
of objects. With this aim, a method to generate structural object models from
manually segmented images has been developed. To generate the model from
non-segmented images, extraction of the objects from the sample images is
required. A hybrid method of extraction (both in terms of input sources and
segmentation algorithms) is proposed: A region based segmentation is applied on
a 10 meter resolution multi-spectral image. The result is used as marker in a
""marker-controlled watershed method using edges"" on a 2.5 meter resolution
panchromatic image. Very promising results have been obtained even on images
where the limits of the target objects are not apparent."
"There is a huge amount of historical documents in libraries and in various
National Archives that have not been exploited electronically. Although
automatic reading of complete pages remains, in most cases, a long-term
objective, tasks such as word spotting, text/image alignment, authentication
and extraction of specific fields are in use today. For all these tasks, a
major step is document segmentation into text lines. Because of the low quality
and the complexity of these documents (background noise, artifacts due to
aging, interfering lines),automatic text line segmentation remains an open
research field. The objective of this paper is to present a survey of existing
methods, developed during the last decade, and dedicated to documents of
historical interest."
"We present a novel approach for the derivation of PDE modeling
curvature-driven flows for matrix-valued data. This approach is based on the
Riemannian geometry of the manifold of Symmetric Positive Definite Matrices
Pos(n)."
"We propose a new algorithm to the problem of polygonal curve approximation
based on a multiresolution approach. This algorithm is suboptimal but still
maintains some optimality between successive levels of resolution using dynamic
programming. We show theoretically and experimentally that this algorithm has a
linear complexity in time and space. We experimentally compare the outcomes of
our algorithm to the optimal ""full search"" dynamic programming solution and
finally to classical merge and split approaches. The experimental evaluations
confirm the theoretical derivations and show that the proposed approach
evaluated on 2D coastal maps either show a lower time complexity or provide
polygonal approximations closer to the input discrete curves."
"This paper presents deformable templates as a tool for segmentation and
localization of biological structures in medical images. Structures are
represented by a prototype template, combined with a parametric warp mapping
used to deform the original shape. The localization procedure is achieved using
a multi-stage, multi-resolution algorithm de-signed to reduce computational
complexity and time. The algorithm initially identifies regions in the image
most likely to contain the desired objects and then examines these regions at
progressively increasing resolutions. The final stage of the algorithm involves
warping the prototype template to match the localized objects. The algorithm is
presented along with the results of four example applications using MRI, x-ray
and ultrasound images."
"Nuclear medicine (NM) images inherently suffer from large amounts of noise
and blur. The purpose of this research is to reduce the noise and blur while
maintaining image integrity for improved diagnosis. The proposed solution is to
increase image quality after the standard pre- and post-processing undertaken
by a gamma camera system. Mean Field Annealing (MFA) is the image processing
technique used in this research. It is a computational iterative technique that
makes use of the Point Spread Function (PSF) and the noise associated with the
NM image. MFA is applied to NM images with the objective of reducing noise
while not compromising edge integrity. Using a sharpening filter as a
post-processing technique (after MFA) yields image enhancement of planar NM
images."
"This paper explores a comparative study of both the linear and kernel
implementations of three of the most popular Appearance-based Face Recognition
projection classes, these being the methodologies of Principal Component
Analysis, Linear Discriminant Analysis and Independent Component Analysis. The
experimental procedure provides a platform of equal working conditions and
examines the ten algorithms in the categories of expression, illumination,
occlusion and temporal delay. The results are then evaluated based on a
sequential combination of assessment tools that facilitate both intuitive and
statistical decisiveness among the intra and interclass comparisons. The best
categorical algorithms are then incorporated into a hybrid methodology, where
the advantageous effects of fusion strategies are considered."
"Subtraction of aligned images is a means to assess changes in a wide variety
of clinical applications. In this paper we explore the information theoretical
origin of Mutual Information (MI), which is based on Shannon's entropy.However,
the interpretation of standard MI registration as a communication channel
suggests that MI is too restrictive a criterion. In this paper the concept of
Mutual Information (MI) is extended to (Normalized) Focussed Mutual Information
(FMI) to incorporate prior knowledge to overcome some shortcomings of MI. We
use this to develop new methodologies to successfully address specific
registration problems, the follow-up of dental restorations, cephalometry, and
the monitoring of implants."
"This article describes the implementation of a system designed to
automatically detect the presence of pulmonary embolism in lung scans. These
images are firstly segmented, before alignment and feature extraction using
PCA. The neural network was trained using the Hybrid Monte Carlo method,
resulting in a committee of 250 neural networks and good results are obtained."
"A recent paper \cite{CaeCaeSchBar06} proposed a provably optimal, polynomial
time method for performing near-isometric point pattern matching by means of
exact probabilistic inference in a chordal graphical model. Their fundamental
result is that the chordal graph in question is shown to be globally rigid,
implying that exact inference provides the same matching solution as exact
inference in a complete graphical model. This implies that the algorithm is
optimal when there is no noise in the point patterns. In this paper, we present
a new graph which is also globally rigid but has an advantage over the graph
proposed in \cite{CaeCaeSchBar06}: its maximal clique size is smaller,
rendering inference significantly more efficient. However, our graph is not
chordal and thus standard Junction Tree algorithms cannot be directly applied.
Nevertheless, we show that loopy belief propagation in such a graph converges
to the optimal solution. This allows us to retain the optimality guarantee in
the noiseless case, while substantially reducing both memory requirements and
processing time. Our experimental results show that the accuracy of the
proposed solution is indistinguishable from that of \cite{CaeCaeSchBar06} when
there is noise in the point patterns."
"In this paper, we use belief-propagation techniques to develop fast
algorithms for image inpainting. Unlike traditional gradient-based approaches,
which may require many iterations to converge, our techniques achieve
competitive results after only a few iterations. On the other hand, while
belief-propagation techniques are often unable to deal with high-order models
due to the explosion in the size of messages, we avoid this problem by
approximating our high-order prior model using a Gaussian mixture. By using
such an approximation, we are able to inpaint images quickly while at the same
time retaining good visual results."
"In this paper, we firstly modify a parameter in affinity propagation (AP) to
improve its convergence ability, and then, we apply it to vector quantization
(VQ) codebook design problem. In order to improve the quality of the resulted
codebook, we combine the improved AP (IAP) with the conventional LBG algorithm
to generate an effective algorithm call IAP-LBG. According to the experimental
results, the proposed method not only enhances the convergence abilities but
also is capable of providing higher-quality codebooks than conventional LBG
method."
"Although the recognition of isolated handwritten digits has been a research
topic for many years, it continues to be of interest for the research community
and for commercial applications. We show that despite the maturity of the
field, different approaches still deliver results that vary enough to allow
improvements by using their combination. We do so by choosing four
well-motivated state-of-the-art recognition systems for which results on the
standard MNIST benchmark are available. When comparing the errors made, we
observe that the errors made differ between all four systems, suggesting the
use of classifier combination. We then determine the error rate of a
hypothetical system that combines the output of the four systems. The result
obtained in this manner is an error rate of 0.35% on the MNIST data, the best
result published so far. We furthermore discuss the statistical significance of
the combined result and of the results of the individual classifiers."
"We present a hierarchical method for segmenting text areas in natural images.
The method assumes that the text is written with a contrasting color on a more
or less uniform background. But no assumption is made regarding the language or
character set used to write the text. In particular, the text can contain
simple graphics or symbols. The key feature of our approach is that we first
concentrate on finding the background of the text, before testing whether there
is actually text on the background. Since uniform areas are easy to find in
natural images, and since text backgrounds define areas which contain ""holes""
(where the text is written) we thus look for uniform areas containing ""holes""
and label them as text backgrounds candidates. Each candidate area is then
further tested for the presence of text within its convex hull. We tested our
method on a database of 65 images including English and Urdu text. The method
correctly segmented all the text areas in 63 of these images, and in only 4 of
these were areas that do not contain text also segmented."
"We show the potential for classifying images of mixtures of aggregate, based
themselves on varying, albeit well-defined, sizes and shapes, in order to
provide a far more effective approach compared to the classification of
individual sizes and shapes. While a dominant (additive, stationary) Gaussian
noise component in image data will ensure that wavelet coefficients are of
Gaussian distribution, long tailed distributions (symptomatic, for example, of
extreme values) may well hold in practice for wavelet coefficients. Energy (2nd
order moment) has often been used for image characterization for image
content-based retrieval, and higher order moments may be important also, not
least for capturing long tailed distributional behavior. In this work, we
assess 2nd, 3rd and 4th order moments of multiresolution transform -- wavelet
and curvelet transform -- coefficients as features. As analysis methodology,
taking account of image types, multiresolution transforms, and moments of
coefficients in the scales or bands, we use correspondence analysis as well as
k-nearest neighbors supervised classification."
"We present the SAMMI lightweight object detection method which has a high
level of accuracy and robustness, and which is able to operate in an
environment with a large number of cameras. Background modeling is based on DCT
coefficients provided by cameras. Foreground detection uses similarity in
temporal characteristics of adjacent blocks of pixels, which is a
computationally inexpensive way to make use of object coherence. Scene model
updating uses the approximated median method for improved performance.
Evaluation at pixel level and application level shows that SAMMI object
detection performs better and faster than the conventional Mixture of Gaussians
method."
"The method of a linear high dynamic range imaging using solid-state
photosensors with Bayer colour filters array is provided in this paper. Using
information from neighbour pixels, it is possible to reconstruct linear images
with wide dynamic range from the oversaturated images. Bayer colour filters
array is considered as an array of neutral filters in a quasimonochromatic
light. If the camera's response function to the desirable light source is known
then one can calculate correction coefficients to reconstruct oversaturated
images. Reconstructed images are linearized in order to provide a linear high
dynamic range images for optical-digital imaging systems. The calibration
procedure for obtaining the camera's response function to the desired light
source is described. Experimental results of the reconstruction of the images
from the oversaturated images are presented for red, green, and blue
quasimonochromatic light sources. Quantitative analysis of the accuracy of the
reconstructed images is provided."
"The goal of this paper is to estimate directly the rotation and translation
between two stereoscopic images with the help of five homologous points. The
methodology presented does not mix the rotation and translation parameters,
which is comparably an important advantage over the methods using the
well-known essential matrix. This results in correct behavior and accuracy for
situations otherwise known as quite unfavorable, such as planar scenes, or
panoramic sets of images (with a null base length), while providing quite
comparable results for more ""standard"" cases. The resolution of the algebraic
polynomials resulting from the modeling of the coplanarity constraint is made
with the help of powerful algebraic solver tools (the Groebner bases and the
Rational Univariate Representation)."
"Colour and coarseness of skin are visually different. When image processing
is involved in the skin analysis, it is important to quantitatively evaluate
such differences using texture features. In this paper, we discuss a texture
analysis and measurements based on a statistical approach to the pattern
recognition. Grain size and anisotropy are evaluated with proper diagrams. The
possibility to determine the presence of pattern defects is also discussed."
"Most search engines index the textual content of documents in digital
libraries. However, scholarly articles frequently report important findings in
figures for visual impact and the contents of these figures are not indexed.
These contents are often invaluable to the researcher in various fields, for
the purposes of direct comparison with their own work. Therefore, searching for
figures and extracting figure data are important problems. To the best of our
knowledge, there exists no tool to automatically extract data from figures in
digital documents. If we can extract data from these images automatically and
store them in a database, an end-user can query and combine data from multiple
digital documents simultaneously and efficiently. We propose a framework based
on image analysis and machine learning to extract information from 2-D plot
images and store them in a database. The proposed algorithm identifies a 2-D
plot and extracts the axis labels, legend and the data points from the 2-D
plot. We also segregate overlapping shapes that correspond to different data
points. We demonstrate performance of individual algorithms, using a
combination of generated and real-life images."
"It is now well established that sparse signal models are well suited to
restoration tasks and can effectively be learned from audio, image, and video
data. Recent research has been aimed at learning discriminative sparse models
instead of purely reconstructive ones. This paper proposes a new step in that
direction, with a novel sparse representation for signals belonging to
different classes in terms of a shared dictionary and multiple class-decision
functions. The linear variant of the proposed model admits a simple
probabilistic interpretation, while its most general variant admits an
interpretation in terms of kernels. An optimization framework for learning all
the components of the proposed model is presented, along with experimental
results on standard handwritten digit and texture classification tasks."
"Black box models of technical systems are purely descriptive. They do not
explain why a system works the way it does. Thus, black box models are
insufficient for some problems. But there are numerous applications, for
example, in control engineering, for which a black box model is absolutely
sufficient. In this article, we describe a general stochastic framework with
which such models can be built easily and fully automated by observation.
Furthermore, we give a practical example and show how this framework can be
used to model and control a motorcar powertrain."
"Graph kernels methods are based on an implicit embedding of graphs within a
vector space of large dimension. This implicit embedding allows to apply to
graphs methods which where until recently solely reserved to numerical data.
Within the shape classification framework, graphs are often produced by a
skeletonization step which is sensitive to noise. We propose in this paper to
integrate the robustness to structural noise by using a kernel based on a bag
of path where each path is associated to a hierarchy encoding successive
simplifications of the path. Several experiments prove the robustness and the
flexibility of our approach compared to alternative shape classification
methods."
"In this paper we present a simple and robust method for self-correction of
camera distortion using single images of scenes which contain straight lines.
Since the most common distortion can be modelled as radial distortion, we
illustrate the method using the Harris radial distortion model, but the method
is applicable to any distortion model. The method is based on transforming the
edgels of the distorted image to a 1-D angular Hough space, and optimizing the
distortion correction parameters which minimize the entropy of the
corresponding normalized histogram. Properly corrected imagery will have fewer
curved lines, and therefore less spread in Hough space. Since the method does
not rely on any image structure beyond the existence of edgels sharing some
common orientations and does not use edge fitting, it is applicable to a wide
variety of image types. For instance, it can be applied equally well to images
of texture with weak but dominant orientations, or images with strong vanishing
points. Finally, the method is performed on both synthetic and real data
revealing that it is particularly robust to noise."
"We consider the problem of classification of an object given multiple
observations that possibly include different transformations. The possible
transformations of the object generally span a low-dimensional manifold in the
original signal space. We propose to take advantage of this manifold structure
for the effective classification of the object represented by the observation
set. In particular, we design a low complexity solution that is able to exploit
the properties of the data manifolds with a graph-based algorithm. Hence, we
formulate the computation of the unknown label matrix as a smoothing process on
the manifold under the constraint that all observations represent an object of
one single class. It results into a discrete optimization problem, which can be
solved by an efficient and low complexity algorithm. We demonstrate the
performance of the proposed graph-based algorithm in the classification of sets
of multiple images. Moreover, we show its high potential in video-based face
recognition, where it outperforms state-of-the-art solutions that fall short of
exploiting the manifold structure of the face image data sets."
"This paper addresses the problem of 3D face recognition using simultaneous
sparse approximations on the sphere. The 3D face point clouds are first aligned
with a novel and fully automated registration process. They are then
represented as signals on the 2D sphere in order to preserve depth and geometry
information. Next, we implement a dimensionality reduction process with
simultaneous sparse approximations and subspace projection. It permits to
represent each 3D face by only a few spherical functions that are able to
capture the salient facial characteristics, and hence to preserve the
discriminant facial information. We eventually perform recognition by effective
matching in the reduced space, where Linear Discriminant Analysis can be
further activated for improved recognition performance. The 3D face recognition
algorithm is evaluated on the FRGC v.1.0 data set, where it is shown to
outperform classical state-of-the-art solutions that work with depth images."
"Statistical pattern recognition methods based on the Coherence Length Diagram
(CLD) have been proposed for medical image analyses, such as quantitative
characterisation of human skin textures, and for polarized light microscopy of
liquid crystal textures. Further investigations are made on image maps
originated from such diagram and some examples related to irregularity of
microstructures are shown."
"In this paper, we propose a new approach for keypoint-based object detection.
Traditional keypoint-based methods consist in classifying individual points and
using pose estimation to discard misclassifications. Since a single point
carries no relational features, such methods inherently restrict the usage of
structural information to the pose estimation phase. Therefore, the classifier
considers purely appearance-based feature vectors, thus requiring
computationally expensive feature extraction or complex probabilistic modelling
to achieve satisfactory robustness. In contrast, our approach consists in
classifying graphs of keypoints, which incorporates structural information
during the classification phase and allows the extraction of simpler feature
vectors that are naturally robust. In the present work, 3-vertices graphs have
been considered, though the methodology is general and larger order graphs may
be adopted. Successful experimental results obtained for real-time object
detection in video sequences are reported."
This paper has been withdrawn by the author ali pourmohammad.
"This paper proposes an algorithm for image processing, obtained by adapting
to image maps the definitions of two well-known physical quantities. These
quantities are the dipole and quadrupole moments of a charge distribution. We
will see how it is possible to define dipole and quadrupole moments for the
gray-tone maps and apply them in the development of algorithms for edge
detection."
"Instead of evaluating the gradient field of the brightness map of an image,
we propose the use of dipole vectors. This approach is obtained by adapting to
the image gray-tone distribution the definition of the dipole moment of charge
distributions. We will show how to evaluate the dipoles and obtain a vector
field, which can be a good alternative to the gradient field in pattern
recognition."
"By a ""covering"" we mean a Gaussian mixture model fit to observed data.
Approximations of the Bayes factor can be availed of to judge model fit to the
data within a given Gaussian mixture model. Between families of Gaussian
mixture models, we propose the R\'enyi quadratic entropy as an excellent and
tractable model comparison framework. We exemplify this using the segmentation
of an MRI image volume, based (1) on a direct Gaussian mixture model applied to
the marginal distribution function, and (2) Gaussian model fit through k-means
applied to the 4D multivalued image volume furnished by the wavelet transform.
Visual preference for one model over another is not immediate. The R\'enyi
quadratic entropy allows us to show clearly that one of these modelings is
superior to the other."
"Multi-manifold modeling is increasingly used in segmentation and data
representation tasks in computer vision and related fields. While the general
problem, modeling data by mixtures of manifolds, is very challenging, several
approaches exist for modeling data by mixtures of affine subspaces (which is
often referred to as hybrid linear modeling). We translate some important
instances of multi-manifold modeling to hybrid linear modeling in embedded
spaces, without explicitly performing the embedding but applying the kernel
trick. The resulting algorithm, Kernel Spectral Curvature Clustering, uses
kernels at two levels - both as an implicit embedding method to linearize
nonflat manifolds and as a principled method to convert a multiway affinity
problem into a spectral clustering one. We demonstrate the effectiveness of the
method by comparing it with other state-of-the-art methods on both synthetic
data and a real-world problem of segmenting multiple motions from two
perspective camera views."
"We apply the Spectral Curvature Clustering (SCC) algorithm to a benchmark
database of 155 motion sequences, and show that it outperforms all other
state-of-the-art methods. The average misclassification rate by SCC is 1.41%
for sequences having two motions and 4.85% for three motions."
"A method to extract and recognize isolated characters in license plates is
proposed. In extraction stage, the proposed method detects isolated characters
by using Difference-of-Gaussian (DOG) function, The DOG function, similar to
Laplacian of Gaussian function, was proven to produce the most stable image
features compared to a range of other possible image functions. The candidate
characters are extracted by doing connected component analysis on different
scale DOG images. In recognition stage, a novel feature vector named
accumulated gradient projection vector (AGPV) is used to compare the candidate
character with the standard ones. The AGPV is calculated by first projecting
pixels of similar gradient orientations onto specific axes, and then
accumulates the projected gradient magnitudes by each axis. In the experiments,
the AGPVs are proven to be invariant from image scaling and rotation, and
robust to noise and illumination change."
"The size and geometry of the prostate are known to be pivotal quantities used
by clinicians to assess the condition of the gland during prostate cancer
screening. As an alternative to palpation, an increasing number of methods for
estimation of the above-mentioned quantities are based on using imagery data of
prostate. The necessity to process large volumes of such data creates a need
for automatic segmentation tools which would allow the estimation to be carried
out with maximum accuracy and efficiency. In particular, the use of transrectal
ultrasound (TRUS) imaging in prostate cancer screening seems to be becoming a
standard clinical practice due to the high benefit-to-cost ratio of this
imaging modality. Unfortunately, the segmentation of TRUS images is still
hampered by relatively low contrast and reduced SNR of the images, thereby
requiring the segmentation algorithms to incorporate prior knowledge about the
geometry of the gland. In this paper, a novel approach to the problem of
segmenting the TRUS images is described. The proposed approach is based on the
concept of distribution tracking, which provides a unified framework for
modeling and fusing image-related and morphological features of the prostate.
Moreover, the same framework allows the segmentation to be regularized via
using a new type of ""weak"" shape priors, which minimally bias the estimation
procedure, while rendering the latter stable and robust."
"The problem of reconstruction of digital images from their degraded
measurements is regarded as a problem of central importance in various fields
of engineering and imaging sciences. In such cases, the degradation is
typically caused by the resolution limitations of an imaging device in use
and/or by the destructive influence of measurement noise. Specifically, when
the noise obeys a Poisson probability law, standard approaches to the problem
of image reconstruction are based on using fixed-point algorithms which follow
the methodology first proposed by Richardson and Lucy. The practice of using
these methods, however, shows that their convergence properties tend to
deteriorate at relatively high noise levels. Accordingly, in the present paper,
a novel method for de-noising and/or de-blurring of digital images corrupted by
Poisson noise is introduced. The proposed method is derived under the
assumption that the image of interest can be sparsely represented in the domain
of a linear transform. Consequently, a shrinkage-based iterative procedure is
proposed, which guarantees the solution to converge to the global maximizer of
an associated maximum-a-posteriori criterion. It is shown in a series of both
computer-simulated and real-life experiments that the proposed method
outperforms a number of existing alternatives in terms of stability, precision,
and computational efficiency."
"A new fangled method for ship wake detection in synthetic aperture radar
(SAR) images is explored here. Most of the detection procedure applies the
Radon transform as its properties outfit more than any other transformation for
the detection purpose. But still it holds problems when the transform is
applied to an image with a high level of noise. Here this paper articulates the
combination between the radon transformation and the shrinkage methods which
increase the mode of wake detection process. The latter shrinkage method with
RT maximize the signal to noise ratio hence it leads to most optimal detection
of lines in the SAR images. The originality mainly works on the denoising
segment of the proposed algorithm. Experimental work outs are carried over both
in simulated and real SAR images. The detection process is more adequate with
the proposed method and improves better than the conventional methods."
"This paper presents an algorithm which aims to assist the radiologist in
identifying breast cancer at its earlier stages. It combines several image
processing techniques like image negative, thresholding and segmentation
techniques for detection of tumor in mammograms. The algorithm is verified by
using mammograms from Mammographic Image Analysis Society. The results obtained
by applying these techniques are described."
"The paper describes an image processing for a non-photorealistic rendering.
The algorithm is based on a random choice of a set of pixels from those ot the
original image and substitution of them with colour spots. An iterative
procedure is applied to cover, at a desired level, the canvas. The resulting
effect mimics the impressionist painting and Pointillism."
"Recognition of iris based on Visible Light (VL) imaging is a difficult
problem because of the light reflection from the cornea. Nonetheless, pigment
melanin provides a rich feature source in VL, unavailable in Near-Infrared
(NIR) imaging. This is due to biological spectroscopy of eumelanin, a chemical
not stimulated in NIR. In this case, a plausible solution to observe such
patterns may be provided by an adaptive procedure using a variational technique
on the image histogram. To describe the patterns, a shape analysis method is
used to derive feature-code for each subject. An important question is how much
the melanin patterns, extracted from VL, are independent of iris texture in
NIR. With this question in mind, the present investigation proposes fusion of
features extracted from NIR and VL to boost the recognition performance. We
have collected our own database (UTIRIS) consisting of both NIR and VL images
of 158 eyes of 79 individuals. This investigation demonstrates that the
proposed algorithm is highly sensitive to the patterns of cromophores and
improves the iris recognition rate."
"Multiview 3D face modeling has attracted increasing attention recently and
has become one of the potential avenues in future video systems. We aim to make
more reliable and robust automatic feature extraction and natural 3D feature
construction from 2D features detected on a pair of frontal and profile view
face images. We propose several heuristic algorithms to minimize possible
errors introduced by prevalent nonperfect orthogonal condition and noncoherent
luminance. In our approach, we first extract the 2D features that are visible
to both cameras in both views. Then, we estimate the coordinates of the
features in the hidden profile view based on the visible features extracted in
the two orthogonal views. Finally, based on the coordinates of the extracted
features, we deform a 3D generic model to perform the desired 3D clone
modeling. Present study proves the scope of resulted facial models for
practical applications like face recognition and facial animation."
"In this paper, we present a system for modelling vehicle motion in an urban
scene from low frame-rate aerial video. In particular, the scene is modelled as
a probability distribution over velocities at every pixel in the image.
  We describe the complete system for acquiring this model. The video is
captured from a helicopter and stabilized by warping the images to match an
orthorectified image of the area. A pixel classifier is applied to the
stabilized images, and the response is segmented to determine car locations and
orientations. The results are fed in to a tracking scheme which tracks cars for
three frames, creating tracklets. This allows the tracker to use a combination
of velocity, direction, appearance, and acceleration cues to keep only tracks
likely to be correct. Each tracklet provides a measurement of the car velocity
at every point along the tracklet's length, and these are then aggregated to
create a histogram of vehicle velocities at every pixel in the image.
  The results demonstrate that the velocity probability distribution prior can
be used to infer a variety of information about road lane directions, speed
limits, vehicle speeds and common trajectories, and traffic bottlenecks, as
well as providing a means of describing environmental knowledge about traffic
rules that can be used in tracking."
"Finding the three-dimensional representation of all or a part of a scene from
a single two dimensional image is a challenging task. In this paper we propose
a method for identifying the pose and location of objects with circular
protrusions in three dimensions from a single image and a 3d representation or
model of the object of interest. To do this, we present a method for
identifying ellipses and their properties quickly and reliably with a novel
technique that exploits intensity differences between objects and a geometric
technique for matching an ellipse in 2d to a circle in 3d.
  We apply these techniques to the specific problem of determining the pose and
location of vehicles, particularly cars, from a single image. We have achieved
excellent pose recovery performance on artificially generated car images and
show promising results on real vehicle images. We also make use of the ellipse
detection method to identify car wheels from images, with a very high
successful match rate."
"Varieties of noises are major problem in recognition of Electromyography
(EMG) signal. Hence, methods to remove noise become most significant in EMG
signal analysis. White Gaussian noise (WGN) is used to represent interference
in this paper. Generally, WGN is difficult to be removed using typical
filtering and solutions to remove WGN are limited. In addition, noise removal
is an important step before performing feature extraction, which is used in
EMG-based recognition. This research is aimed to present a novel feature that
tolerate with WGN. As a result, noise removal algorithm is not needed. Two
novel mean and median frequencies (MMNF and MMDF) are presented for robust
feature extraction. Sixteen existing features and two novelties are evaluated
in a noisy environment. WGN with various signal-to-noise ratios (SNRs), i.e.
20-0 dB, was added to the original EMG signal. The results showed that MMNF
performed very well especially in weak EMG signal compared with others. The
error of MMNF in weak EMG signal with very high noise, 0 dB SNR, is about 5-10
percent and closed by MMDF and Histogram, whereas the error of other features
is more than 20 percent. While in strong EMG signal, the error of MMNF is
better than those from other features. Moreover, the combination of MMNF,
Histrogram of EMG and Willison amplitude is used as feature vector in
classification task. The experimental result shows the better recognition
result in noisy environment than other success feature candidates. From the
above results demonstrate that MMNF can be used for new robust feature
extraction."
"We propose to use novel and classical audio and text signal-processing and
otherwise techniques for ""inexpensive"" fast writer identification tasks of
scanned hand-written documents ""visually"". The ""inexpensive"" refers to the
efficiency of the identification process in terms of CPU cycles while
preserving decent accuracy for preliminary identification. This is a
comparative study of multiple algorithm combinations in a pattern recognition
pipeline implemented in Java around an open-source Modular Audio Recognition
Framework (MARF) that can do a lot more beyond audio. We present our
preliminary experimental findings in such an identification task. We simulate
""visual"" identification by ""looking"" at the hand-written document as a whole
rather than trying to extract fine-grained features out of it prior
classification."
"This paper attempts to undertake the study of three types of noise such as
Salt and Pepper (SPN), Random variation Impulse Noise (RVIN), Speckle (SPKN).
Different noise densities have been removed between 10% to 60% by using five
types of filters as Mean Filter (MF), Adaptive Wiener Filter (AWF), Gaussian
Filter (GF), Standard Median Filter (SMF) and Adaptive Median Filter (AMF). The
same is applied to the Saturn remote sensing image and they are compared with
one another. The comparative study is conducted with the help of Mean Square
Errors (MSE) and Peak-Signal to Noise Ratio (PSNR). So as to choose the base
method for removal of noise from remote sensing image."
"Acquisition-to-acquisition signal intensity variations (non-standardness) are
inherent in MR images. Standardization is a post processing method for
correcting inter-subject intensity variations through transforming all images
from the given image gray scale into a standard gray scale wherein similar
intensities achieve similar tissue meanings. The lack of a standard image
intensity scale in MRI leads to many difficulties in tissue characterizability,
image display, and analysis, including image segmentation. This phenomenon has
been documented well; however, effects of standardization on medical image
registration have not been studied yet. In this paper, we investigate the
influence of intensity standardization in registration tasks with systematic
and analytic evaluations involving clinical MR images. We conducted nearly
20,000 clinical MR image registration experiments and evaluated the quality of
registrations both quantitatively and qualitatively. The evaluations show that
intensity variations between images degrades the accuracy of registration
performance. The results imply that the accuracy of image registration not only
depends on spatial and geometric similarity but also on the similarity of the
intensity values for the same tissues in different images."
"This paper investigates, using prior shape models and the concept of ball
scale (b-scale), ways of automatically recognizing objects in 3D images without
performing elaborate searches or optimization. That is, the goal is to place
the model in a single shot close to the right pose (position, orientation, and
scale) in a given image so that the model boundaries fall in the close vicinity
of object boundaries in the image. This is achieved via the following set of
key ideas: (a) A semi-automatic way of constructing a multi-object shape model
assembly. (b) A novel strategy of encoding, via b-scale, the pose relationship
between objects in the training images and their intensity patterns captured in
b-scale images. (c) A hierarchical mechanism of positioning the model, in a
one-shot way, in a given image from a knowledge of the learnt pose relationship
and the b-scale image of the given image to be segmented. The evaluation
results on a set of 20 routine clinical abdominal female and male CT data sets
indicate the following: (1) Incorporating a large number of objects improves
the recognition accuracy dramatically. (2) The recognition algorithm can be
thought as a hierarchical framework such that quick replacement of the model
assembly is defined as coarse recognition and delineation itself is known as
finest recognition. (3) Scale yields useful information about the relationship
between the model assembly and any given image such that the recognition
results in a placement of the model close to the actual pose without doing any
elaborate searches or optimization. (4) Effective object recognition can make
delineation most accurate."
"Microcalcifications in mammogram have been mainly targeted as a reliable
earliest sign of breast cancer and their early detection is vital to improve
its prognosis. Since their size is very small and may be easily overlooked by
the examining radiologist, computer-based detection output can assist the
radiologist to improve the diagnostic accuracy. In this paper, we have proposed
an algorithm for detecting microcalcification in mammogram. The proposed
microcalcification detection algorithm involves mammogram quality enhancement
using multirresolution analysis based on the dyadic wavelet transform and
microcalcification detection by fuzzy shell clustering. It may be possible to
detect nodular components such as microcalcification accurately by introducing
shape information. The effectiveness of the proposed algorithm for
microcalcification detection is confirmed by experimental results."
"Teleophthalmology holds a great potential to improve the quality, access, and
affordability in health care. For patients, it can reduce the need for travel
and provide the access to a superspecialist. Ophthalmology lends itself easily
to telemedicine as it is a largely image based diagnosis. The main goal of the
proposed system is to diagnose the type of disease in the retina and to
automatically detect and segment retinal diseases without human supervision or
interaction. The proposed system will diagnose the disease present in the
retina using a neural network based classifier.The extent of the disease spread
in the retina can be identified by extracting the textural features of the
retina. This system will diagnose the following type of diseases: Diabetic
Retinopathy and Drusen."
"In this paper offers a simple and lossless compression method for compression
of medical images. Method is based on wavelet decomposition of the medical
images followed by the correlation analysis of coefficients. The correlation
analyses are the basis of prediction equation for each sub band. Predictor
variable selection is performed through coefficient graphic method to avoid
multicollinearity problem and to achieve high prediction accuracy and
compression rate. The method is applied on MRI and CT images. Results show that
the proposed approach gives a high compression rate for MRI and CT images
comparing with state of the art methods."
"In this paper, an algorithm is proposed for Image Restoration. Such algorithm
is different from the traditional approaches in this area, by utilizing priors
that are learned from similar images. Original images and their degraded
versions by the known degradation operators are utilized for designing the
Quantization. The code vectors are designed using the blurred images. For each
such vector, the high frequency information obtained from the original images
is also available. During restoration, the high frequency information of a
given degraded image is estimated from its low frequency information based on
the artificial noise. For the restoration problem, a number of techniques are
designed corresponding to various versions of the blurring function. Given a
noisy and blurred image, one of the techniques is chosen based on a similarity
measure, therefore providing the identification of the blur. To make the
restoration process computationally efficient, the Quantization Nearest
Neighborhood approaches are utilized."
"For many machine learning algorithms such as $k$-Nearest Neighbor ($k$-NN)
classifiers and $ k $-means clustering, often their success heavily depends on
the metric used to calculate distances between different data points.
  An effective solution for defining such a metric is to learn it from a set of
labeled training samples. In this work, we propose a fast and scalable
algorithm to learn a Mahalanobis distance metric. By employing the principle of
margin maximization to achieve better generalization performances, this
algorithm formulates the metric learning as a convex optimization problem and a
positive semidefinite (psd) matrix is the unknown variable. a specialized
gradient descent method is proposed. our algorithm is much more efficient and
has a better performance in scalability compared with existing methods.
Experiments on benchmark data sets suggest that, compared with state-of-the-art
metric learning algorithms, our algorithm can achieve a comparable
classification accuracy with reduced computational complexity."
"Designing a Business Card Reader (BCR) for mobile devices is a challenge to
the researchers because of huge deformation in acquired images, multiplicity in
nature of the business cards and most importantly the computational constraints
of the mobile devices. This paper presents a text extraction method designed in
our work towards developing a BCR for mobile devices. At first, the background
of a camera captured image is eliminated at a coarse level. Then, various rule
based techniques are applied on the Connected Components (CC) to filter out the
noises and picture regions. The CCs identified as text are then binarized using
an adaptive but light-weight binarization technique. Experiments show that the
text extraction accuracy is around 98% for a wide range of resolutions with
varying computation time and memory requirements. The optimum performance is
achieved for the images of resolution 1024x768 pixels with text extraction
accuracy of 98.54% and, space and time requirements as 1.1 MB and 0.16 seconds
respectively."
"Business card images are of multiple natures as these often contain graphics,
pictures and texts of various fonts and sizes both in background and
foreground. So, the conventional binarization techniques designed for document
images can not be directly applied on mobile devices. In this paper, we have
presented a fast binarization technique for camera captured business card
images. A card image is split into small blocks. Some of these blocks are
classified as part of the background based on intensity variance. Then the
non-text regions are eliminated and the text ones are skew corrected and
binarized using a simple yet adaptive technique. Experiment shows that the
technique is fast, efficient and applicable for the mobile devices."
"This report presents properties of the Discrete Pulse Transform on
multi-dimensional arrays introduced by the authors two or so years ago. The
main result given here in Lemma 2.1 is also formulated in a paper to appear in
IEEE Transactions on Image Processing. However, the proof, being too technical,
was omitted there and hence it appears in full in this publication."
"Automatic License Plate Recognition (ALPR) is a challenging area of research
due to its importance to variety of commercial applications. The overall
problem may be subdivided into two key modules, firstly, localization of
license plates from vehicle images, and secondly, optical character recognition
of extracted license plates. In the current work, we have concentrated on the
first part of the problem, i.e., localization of license plate regions from
Indian commercial vehicles as a significant step towards development of a
complete ALPR system for Indian vehicles. The technique is based on color based
segmentation of vehicle images and identification of potential license plate
regions. True license plates are finally localized based on four spatial and
horizontal contrast features. The technique successfully localizes the actual
license plates in 73.4% images."
"Analysing human gait has found considerable interest in recent computer
vision research. So far, however, contributions to this topic exclusively dealt
with the tasks of person identification or activity recognition. In this paper,
we consider a different application for gait analysis and examine its use as a
means of deducing the physical well-being of people. The proposed method is
based on transforming the joint motion trajectories using wavelets to extract
spatio-temporal features which are then fed as input to a vector quantiser; a
self-organising map for classification of walking patterns of individuals with
and without pathology. We show that our proposed algorithm is successful in
extracting features that successfully discriminate between individuals with and
without locomotion impairment."
"This paper proposes a new technique based on nonlinear Adaptive Median filter
(AMF) for image restoration. Image denoising is a common procedure in digital
image processing aiming at the removal of noise, which may corrupt an image
during its acquisition or transmission, while retaining its quality. This
procedure is traditionally performed in the spatial or frequency domain by
filtering. The aim of image enhancement is to reconstruct the true image from
the corrupted image. The process of image acquisition frequently leads to
degradation and the quality of the digitized image becomes inferior to the
original image. Filtering is a technique for enhancing the image. Linear filter
is the filtering in which the value of an output pixel is a linear combination
of neighborhood values, which can produce blur in the image. Thus a variety of
smoothing techniques have been developed that are non linear. Median filter is
the one of the most popular non-linear filter. When considering a small
neighborhood it is highly efficient but for large window and in case of high
noise it gives rise to more blurring to image. The Centre Weighted Median (CWM)
filter has got a better average performance over the median filter [8]. However
the original pixel corrupted and noise reduction is substantial under high
noise condition. Hence this technique has also blurring affect on the image. To
illustrate the superiority of the proposed approach by overcoming the existing
problem, the proposed new scheme (AMF) Adaptive Median Filter has been
simulated along with the standard ones and various performance measures have
been compared."
"Augmenting human computer interaction with automated analysis and synthesis
of facial expressions is a goal towards which much research effort has been
devoted recently. Facial gesture recognition is one of the important component
of natural human-machine interfaces; it may also be used in behavioural
science, security systems and in clinical practice. Although humans recognise
facial expressions virtually without effort or delay, reliable expression
recognition by machine is still a challenge. The face expression recognition
problem is challenging because different individuals display the same
expression differently. This paper presents an overview of gesture recognition
in real time using the concepts of correlation and Mahalanobis distance.We
consider the six universal emotional categories namely joy, anger, fear,
disgust, sadness and surprise."
"Image denoising is getting more significance, especially in Computed
Tomography (CT), which is an important and most common modality in medical
imaging. This is mainly due to that the effectiveness of clinical diagnosis
using CT image lies on the image quality. The denoising technique for CT images
using window-based Multi-wavelet transformation and thresholding shows the
effectiveness in denoising, however, a drawback exists in selecting the closer
windows in the process of window-based multi-wavelet transformation and
thresholding. Generally, the windows of the duplicate noisy image that are
closer to each window of original noisy image are obtained by the checking them
sequentially. This leads to the possibility of missing out very closer windows
and so enhancement is required in the aforesaid process of the denoising
technique. In this paper, we propose a GA-based window selection methodology to
include the denoising technique. With the aid of the GA-based window selection
methodology, the windows of the duplicate noisy image that are very closer to
every window of the original noisy image are extracted in an effective manner.
By incorporating the proposed GA-based window selection methodology, the
denoising the CT image is performed effectively. Eventually, a comparison is
made between the denoising technique with and without the proposed GA-based
window selection methodology."
"Digital image plays a vital role in the early detection of cancers, such as
prostate cancer, breast cancer, lungs cancer, cervical cancer. Ultrasound
imaging method is also suitable for early detection of the abnormality of
fetus. The accurate detection of region of interest in ultrasound image is
crucial. Since the result of reflection, refraction and deflection of
ultrasound waves from different types of tissues with different acoustic
impedance. Usually, the contrast in ultrasound image is very low and weak edges
make the image difficult to identify the fetus region in the ultrasound image.
So the analysis of ultrasound image is more challenging one. We try to develop
a new algorithmic approach to solve the problem of non clarity and find
disorder of it. Generally there is no common enhancement approach for noise
reduction. This paper proposes different filtering techniques based on
statistical methods for the removal of various noise. The quality of the
enhanced images is measured by the statistical quantity measures:
Signal-to-Noise Ratio (SNR), Peak Signal-to-Noise Ratio (PSNR), and Root Mean
Square Error (RMSE)."
"Handwritten numeral recognition is in general a benchmark problem of Pattern
Recognition and Artificial Intelligence. Compared to the problem of printed
numeral recognition, the problem of handwritten numeral recognition is
compounded due to variations in shapes and sizes of handwritten characters.
Considering all these, the problem of handwritten numeral recognition is
addressed under the present work in respect to handwritten Arabic numerals.
Arabic is spoken throughout the Arab World and the fifth most popular language
in the world slightly before Portuguese and Bengali. For the present work, we
have developed a feature set of 88 features is designed to represent samples of
handwritten Arabic numerals for this work. It includes 72 shadow and 16 octant
features. A Multi Layer Perceptron (MLP) based classifier is used here for
recognition handwritten Arabic digits represented with the said feature set. On
experimentation with a database of 3000 samples, the technique yields an
average recognition rate of 94.93% evaluated after three-fold cross validation
of results. It is useful for applications related to OCR of handwritten Arabic
Digit and can also be extended to include OCR of handwritten characters of
Arabic alphabet."
"The work presents a comparative assessment of seven different feature sets
for recognition of handwritten Arabic numerals using a Multi Layer Perceptron
(MLP) based classifier. The seven feature sets employed here consist of shadow
features, octant centroids, longest runs, angular distances, effective spans,
dynamic centers of gravity, and some of their combinations. On experimentation
with a database of 3000 samples, the maximum recognition rate of 95.80% is
observed with both of two separate combinations of features. One of these
combinations consists of shadow and centriod features, i. e. 88 features in
all, and the other shadow, centroid and longest run features, i. e. 124
features in all. Out of these two, the former combination having a smaller
number of features is finally considered effective for applications related to
Optical Character Recognition (OCR) of handwritten Arabic numerals. The work
can also be extended to include OCR of handwritten characters of Arabic
alphabet."
"An approach to textures pattern recognition based on inverse resonance
filtration (IRF) is considered. A set of principal resonance harmonics of
textured image signal fluctuations eigen harmonic decomposition (EHD) is used
for the IRF design. It was shown that EHD is invariant to textured image linear
shift. The recognition of texture is made by transfer of its signal into
unstructured signal which simple statistical parameters can be used for texture
pattern recognition. Anomalous variations of this signal point on foreign
objects. Two methods of 2D EHD parameters estimation are considered with the
account of texture signal breaks presence. The first method is based on the
linear symmetry model that is not sensitive to signal phase jumps. The
condition of characteristic polynomial symmetry provides the model stationarity
and periodicity. Second method is based on the eigenvalues problem of matrices
pencil projection into principal vectors space of singular values decomposition
(SVD) of 2D correlation matrix. Two methods of classification of retrieval from
textured image foreign objects are offered."
"Text binarisation process classifies individual pixels as text or background
in the textual images. Binarization is necessary to bridge the gap between
localization and recognition by OCR. This paper presents Sliding window method
to binarise text from textual images with textured background. Suitable
preprocessing techniques are applied first to increase the contrast of the
image and blur the background noises due to textured background. Then Edges are
detected by iterative thresholding. Subsequently formed edge boxes are analyzed
to remove unwanted edges due to complex background and binarised by sliding
window approach based character size uniformity check algorithm. The proposed
method has been applied on localized region from heterogeneous textual images
and compared with Otsu, Niblack methods and shown encouraging performance of
the proposed method."
"Interest point detection is a common task in various computer vision
applications. Although a big variety of detector are developed so far
computational efficiency of interest point based image analysis remains to be
the problem. Current paper proposes a system-theoretic approach to interest
point detection. Starting from the analysis of interdependency between detector
and descriptor it is shown that given a descriptor it is possible to introduce
to notion of detector redundancy. Furthermore for each detector it is possible
to construct its irredundant and equivalent modification. Modified detector
possesses lower computational complexity and is preferable. It is also shown
that several known approaches to reduce computational complexity of image
registration can be generalized in terms of proposed theory."
"Principle objective of Image enhancement is to process an image so that
result is more suitable than original image for specific application. Digital
image enhancement techniques provide a multitude of choices for improving the
visual quality of images. Appropriate choice of such techniques is greatly
influenced by the imaging modality, task at hand and viewing conditions. This
paper will provide an overview of underlying concepts, along with algorithms
commonly used for image enhancement. The paper focuses on spatial domain
techniques for image enhancement, with particular reference to point processing
methods and histogram processing."
"Area of classifying satellite imagery has become a challenging task in
current era where there is tremendous growth in settlement i.e. construction of
buildings, roads, bridges, dam etc. This paper suggests an improvised k-means
and Artificial Neural Network (ANN) classifier for land-cover mapping of
Eastern Himalayan state Sikkim. The improvised k-means algorithm shows
satisfactory results compared to existing methods that includes k-Nearest
Neighbor and maximum likelihood classifier. The strength of the Artificial
Neural Network (ANN) classifier lies in the fact that they are fast and have
good recognition rate and it's capability of self-learning compared to other
classification algorithms has made it widely accepted. Classifier based on ANN
shows satisfactory and accurate result in comparison with the classical method."
"We provide a novel search technique, which uses a hierarchical model and a
mutual information gain heuristic to efficiently prune the search space when
localizing faces in images. We show exponential gains in computation over
traditional sliding window approaches, while keeping similar performance
levels."
"Fast evolution of Internet technologies has led to an explosive growth of
video data available in the public domain and created unprecedented challenges
in the analysis, organization, management, and control of such content. The
problems encountered in video analysis such as identifying a video in a large
database (e.g. detecting pirated content in YouTube), putting together video
fragments, finding similarities and common ancestry between different versions
of a video, have analogous counterpart problems in genetic research and
analysis of DNA and protein sequences. In this paper, we exploit the analogy
between genetic sequences and videos and propose an approach to video analysis
motivated by genomic research. Representing video information as video DNA
sequences and applying bioinformatic algorithms allows to search, match, and
compare videos in large-scale databases. We show an application for
content-based metadata mapping between versions of annotated video."
"The Coherence Length Diagram and the related maps have been shown to
represent a useful tool for image analysis. Setting threshold parameters is one
of the most important issues when dealing with such applications, as they
affect both the computability, which is outlined by the support map, and the
appearance of the coherence length diagram itself and of defect maps. A coupled
optimization analysis, returning a range for the basic (saturation) threshold,
and a histogram based method, yielding suitable values for a desired map
appearance, are proposed for an effective control of the analysis process."
"This paper presents multi-appearance fusion of Principal Component Analysis
(PCA) and generalization of Linear Discriminant Analysis (LDA) for multi-camera
view offline face recognition (verification) system. The generalization of LDA
has been extended to establish correlations between the face classes in the
transformed representation and this is called canonical covariate. The proposed
system uses Gabor filter banks for characterization of facial features by
spatial frequency, spatial locality and orientation to make compensate to the
variations of face instances occurred due to illumination, pose and facial
expression changes. Convolution of Gabor filter bank to face images produces
Gabor face representations with high dimensional feature vectors. PCA and
canonical covariate are then applied on the Gabor face representations to
reduce the high dimensional feature spaces into low dimensional Gabor
eigenfaces and Gabor canonical faces. Reduced eigenface vector and canonical
face vector are fused together using weighted mean fusion rule. Finally,
support vector machines (SVM) have trained with augmented fused set of features
and perform the recognition task. The system has been evaluated with UMIST face
database consisting of multiview faces. The experimental results demonstrate
the efficiency and robustness of the proposed system for multi-view face images
with high recognition rates. Complexity analysis of the proposed system is also
presented at the end of the experimental results."
"The objective of the paper is to recognize handwritten samples of lower case
Roman script using Tesseract open source Optical Character Recognition (OCR)
engine under Apache License 2.0. Handwritten data samples containing isolated
and free-flow text were collected from different users. Tesseract is trained
with user-specific data samples of both the categories of document pages to
generate separate user-models representing a unique language-set. Each such
language-set recognizes isolated and free-flow handwritten test samples
collected from the designated user. On a three user model, the system is
trained with 1844, 1535 and 1113 isolated handwritten character samples
collected from three different users and the performance is tested on 1133,
1186 and 1204 character samples, collected form the test sets of the three
users respectively. The user specific character level accuracies were obtained
as 87.92%, 81.53% and 65.71% respectively. The overall character-level accuracy
of the system is observed as 78.39%. The system fails to segment 10.96%
characters and erroneously classifies 10.65% characters on the overall dataset."
"In the present work, we have used Tesseract 2.01 open source Optical
Character Recognition (OCR) Engine under Apache License 2.0 for recognition of
handwriting samples of lower case Roman script. Handwritten isolated and
free-flow text samples were collected from multiple users. Tesseract is trained
to recognize user-specific handwriting samples of both the categories of
document pages. On a single user model, the system is trained with 1844
isolated handwritten characters and the performance is tested on 1133
characters, taken form the test set. The overall character-level accuracy of
the system is observed as 83.5%. The system fails to segment 5.56% characters
and erroneously classifies 10.94% characters."
"Objective of the current work is to develop an Optical Character Recognition
(OCR) engine for information Just In Time (iJIT) system that can be used for
recognition of handwritten textual annotations of lower case Roman script.
Tesseract open source OCR engine under Apache License 2.0 is used to develop
user-specific handwriting recognition models, viz., the language sets, for the
said system, where each user is identified by a unique identification tag
associated with the digital pen. To generate the language set for any user,
Tesseract is trained with labeled handwritten data samples of isolated and
free-flow texts of Roman script, collected exclusively from that user. The
designed system is tested on five different language sets with free- flow
handwritten annotations as test samples. The system could successfully segment
and subsequently recognize 87.92%, 81.53%, 92.88%, 86.75% and 90.80%
handwritten characters in the test samples of five different users."
"The objective of the paper is to recognize handwritten samples of basic
Bangla characters using Tesseract open source Optical Character Recognition
(OCR) engine under Apache License 2.0. Handwritten data samples containing
isolated Bangla basic characters and digits were collected from different
users. Tesseract is trained with user-specific data samples of document pages
to generate separate user-models representing a unique language-set. Each such
language-set recognizes isolated basic Bangla handwritten test samples
collected from the designated users. On a three user model, the system is
trained with 919, 928 and 648 isolated handwritten character and digit samples
and the performance is tested on 1527, 14116 and 1279 character and digit
samples, collected form the test datasets of the three users respectively. The
user specific character/digit recognition accuracies were obtained as 90.66%,
91.66% and 96.87% respectively. The overall basic character-level and digit
level accuracy of the system is observed as 92.15% and 97.37%. The system fails
to segment 12.33% characters and 15.96% digits and also erroneously classifies
7.85% characters and 2.63% on the overall dataset."
"The objective of the paper is to recognize handwritten samples of Roman
numerals using Tesseract open source Optical Character Recognition (OCR)
engine. Tesseract is trained with data samples of different persons to generate
one user-independent language model, representing the handwritten Roman
digit-set. The system is trained with 1226 digit samples collected form the
different users. The performance is tested on two different datasets, one
consisting of samples collected from the known users (those who prepared the
training data samples) and the other consisting of handwritten data samples of
unknown users. The overall recognition accuracy is obtained as 92.1% and 86.59%
on these test datasets respectively."
"Integrated Traffic Management Systems (ITMS) are now implemented in different
cities in India to primarily address the concerns of road-safety and security.
An automated Red Light Violation Detection System (RLVDS) is an integral part
of the ITMS. In our present work we have designed and developed a complete
system for generating the list of all stop-line violating vehicle images
automatically from video snapshots of road-side surveillance cameras. The
system first generates adaptive background images for each camera view,
subtracts captured images from the corresponding background images and analyses
potential occlusions over the stop-line in a traffic signal. Considering
round-the-clock operations in a real-life test environment, the developed
system could successfully track 92% images of vehicles with violations on the
stop-line in a ""Red"" traffic signal."
"Automatic License Plate Recognition system is a challenging area of research
now-a-days and binarization is an integral and most important part of it. In
case of a real life scenario, most of existing methods fail to properly
binarize the image of a vehicle in a congested road, captured through a CCD
camera. In the current work we have applied histogram equalization technique
over the complete image and also over different hierarchy of image
partitioning. A novel scheme is formulated for giving the membership value to
each pixel for each hierarchy of histogram equalization. Then the image is
binarized depending on the net membership value of each pixel. The technique is
exhaustively evaluated on the vehicle image dataset as well as the license
plate dataset, giving satisfactory performances."
"The hybrid linear modeling problem is to identify a set of d-dimensional
affine sets in a D-dimensional Euclidean space. It arises, for example, in
object tracking and structure from motion. The hybrid linear model can be
considered as the second simplest (behind linear) manifold model of data. In
this paper we will present a very simple geometric method for hybrid linear
modeling based on selecting a set of local best fit flats that minimize a
global l1 error measure. The size of the local neighborhoods is determined
automatically by the Jones' l2 beta numbers; it is proven under certain
geometric conditions that good local neighborhoods exist and are found by our
method. We also demonstrate how to use this algorithm for fast determination of
the number of affine subspaces. We give extensive experimental evidence
demonstrating the state of the art accuracy and speed of the algorithm on
synthetic and real hybrid linear data."
"The use of OCR in postal services is not yet universal and there are still
many countries that process mail sorting manually. Automated Arabic/Indian
numeral Optical Character Recognition (OCR) systems for Postal services are
being used in some countries, but still there are errors during the mail
sorting process, thus causing a reduction in efficiency. The need to
investigate fast and efficient recognition algorithms/systems is important so
as to correctly read the postal codes from mail addresses and to eliminate any
errors during the mail sorting stage. The objective of this study is to
recognize printed numerical postal codes from mail addresses. The proposed
system is a multistage hybrid system which consists of three different feature
extraction methods, i.e., binary, zoning, and fuzzy features, and three
different classifiers, i.e., Hamming Nets, Euclidean Distance, and Fuzzy Neural
Network Classifiers. The proposed system, systematically compares the
performance of each of these methods, and ensures that the numerals are
recognized correctly. Comprehensive results provide a very high recognition
rate, outperforming the other known developed methods in literature."
"This paper presents an efficient human recognition system based on vein
pattern from the palma dorsa. A new absorption based technique has been
proposed to collect good quality images with the help of a low cost camera and
light source. The system automatically detects the region of interest from the
image and does the necessary preprocessing to extract features. A Euclidean
Distance based matching technique has been used for making the decision. It has
been tested on a data set of 1750 image samples collected from 341 individuals.
The accuracy of the verification system is found to be 99.26% with false
rejection rate (FRR) of 0.03%."
"This article presents a new classification framework that can extract
individual features per class. The scheme is based on a model of incoherent
subspaces, each one associated to one class, and a model on how the elements in
a class are represented in this subspace. After the theoretical analysis an
alternate projection algorithm to find such a collection is developed. The
classification performance and speed of the proposed method is tested on the AR
and YaleB databases and compared to that of Fisher's LDA and a recent approach
based on on $\ell_1$ minimisation. Finally connections of the presented scheme
to already existing work are discussed and possible ways of extensions are
pointed out."
"We introduce the notion of Principal Component Analysis (PCA) of image
gradient orientations. As image data is typically noisy, but noise is
substantially different from Gaussian, traditional PCA of pixel intensities
very often fails to estimate reliably the low-dimensional subspace of a given
data population. We show that replacing intensities with gradient orientations
and the $\ell_2$ norm with a cosine-based distance measure offers, to some
extend, a remedy to this problem. Our scheme requires the eigen-decomposition
of a covariance matrix and is as computationally efficient as standard $\ell_2$
PCA. We demonstrate some of its favorable properties on robust subspace
estimation."
"This paper attempts to undertake the study of segmentation image techniques
by using five threshold methods as Mean method, P-tile method, Histogram
Dependent Technique (HDT), Edge Maximization Technique (EMT) and visual
Technique and they are compared with one another so as to choose the best
technique for threshold segmentation techniques image. These techniques applied
on three satellite images to choose base guesses for threshold segmentation
image."
"This paper aims at generating a new face based on the human like description
using a new concept. The FASY (FAce SYnthesis) System is a Face Database
Retrieval and new Face generation System that is under development. One of its
main features is the generation of the requested face when it is not found in
the existing database, which allows a continuous growing of the database also."
"This paper presents a novel approach to handle the challenges of face
recognition. In this work thermal face images are considered, which minimizes
the affect of illumination changes and occlusion due to moustache, beards,
adornments etc. The proposed approach registers the training and testing
thermal face images in polar coordinate, which is capable to handle
complicacies introduced by scaling and rotation. Polar images are projected
into eigenspace and finally classified using a multi-layer perceptron. In the
experiments we have used Object Tracking and Classification Beyond Visible
Spectrum (OTCBVS) database benchmark thermal face images. Experimental results
show that the proposed approach significantly improves the verification and
identification performance and the success rate is 97.05%."
"In this paper we describe a procedure to reduce the size of the input feature
vector. A complex pattern recognition problem like face recognition involves
huge dimension of input feature vector. To reduce that dimension here we have
used eigenspace projection (also called as Principal Component Analysis), which
is basically transformation of space. To reduce further we have applied feature
selection method to select indispensable features, which will remain in the
final feature vectors. Features those are not selected are removed from the
final feature vector considering them as redundant or superfluous. For
selection of features we have used the concept of reduct and core from rough
set theory. This method has shown very good performance. It is worth to mention
that in some cases the recognition rate increases with the decrease in the
feature vector dimension."
"Object detection is one of the key tasks in computer vision. The cascade
framework of Viola and Jones has become the de facto standard. A classifier in
each node of the cascade is required to achieve extremely high detection rates,
instead of low overall classification error. Although there are a few reported
methods addressing this requirement in the context of object detection, there
is no a principled feature selection method that explicitly takes into account
this asymmetric node learning objective. We provide such a boosting algorithm
in this work. It is inspired by the linear asymmetric classifier (LAC) of Wu et
al. in that our boosting algorithm optimizes a similar cost function. The new
totally-corrective boosting algorithm is implemented by the column generation
technique in convex optimization. Experimental results on face detection
suggest that our proposed boosting algorithms can improve the state-of-the-art
methods in detection performance."
"The ability to efficiently and accurately detect objects plays a very crucial
role for many computer vision tasks. Recently, offline object detectors have
shown a tremendous success. However, one major drawback of offline techniques
is that a complete set of training data has to be collected beforehand. In
addition, once learned, an offline detector can not make use of newly arriving
data. To alleviate these drawbacks, online learning has been adopted with the
following objectives: (1) the technique should be computationally and storage
efficient; (2) the updated classifier must maintain its high classification
accuracy. In this paper, we propose an effective and efficient framework for
learning an adaptive online greedy sparse linear discriminant analysis (GSLDA)
model. Unlike many existing online boosting detectors, which usually apply
exponential or logistic loss, our online algorithm makes use of LDA's learning
criterion that not only aims to maximize the class-separation criterion but
also incorporates the asymmetrical property of training data distributions. We
provide a better alternative for online boosting algorithms in the context of
training a visual object detector. We demonstrate the robustness and efficiency
of our methods on handwriting digit and face data sets. Our results confirm
that object detection tasks benefit significantly when trained in an online
manner."
"Maps are used to describe far-off places . It is an aid for navigation and
military strategies. Mapping of the lands are important and the mapping work is
based on (i). Natural resource management & development (ii). Information
technology ,(iii). Environmental development ,(iv). Facility management and
(v). e-governance. The Landuse / Landcover system espoused by almost all
Organisations and scientists, engineers and remote sensing community who are
involved in mapping of earth surface features, is a system which is derived
from the united States Geological Survey (USGS) LULC classification system. The
application of RS and GIS involves influential of homogeneous zones, drift
analysis of land use integration of new area changes or change detection
etc.,National Remote Sensing Agency(NRSA) Govt. of India has devised a
generalized LULC classification system respect to the Indian conditions based
on the various categories of Earth surface features , resolution of available
satellite data, capabilities of sensors and present and future applications.
The profusion information of the earth surface offered by the high resolution
satellite images for remote sensing applications. Using change detection
methodologies to extract the target changes in the areas from high resolution
images and rapidly updates geodatabase information processing.Traditionally,
classification approaches have focused on per-pixel technologies. Pixels within
areas assumed to be automatically homogeneous are analyzed independently."
"Segmentation of images holds an important position in the area of image
processing. It becomes more important whi le typically dealing with medical
images where presurgery and post surgery decisions are required for the purpose
of initiating and speeding up the recovery process. Segmentation of 3-D tumor
structures from magnetic resonance images (MRI) is a very challenging problem
due to the variability of tumor geometry and intensity patterns. Level set
evolution combining global smoothness with the flexibility of topology changes
offers significant advantages over the conventional statistical classification
followed by mathematical morphology. Level set evolution with constant
propagation needs to be initialized either completely inside or outside the
tumor and can leak through weak or missing boundary parts. Replacing the
constant propagation term by a statistical force overcomes these limitations
and results in a convergence to a stable solution. Using MR images presenting
tumors, probabilities for background and tumor regions are calculated from a
pre- and post-contrast difference image and mixture modeling fit of the
histogram. The whole image is used for initialization of the level set
evolution to segment the tumor boundaries."
"This philosophical paper proposes a modified version of the scientific
method, in which large databases are used instead of experimental observations
as the necessary empirical ingredient. This change in the source of the
empirical data allows the scientific method to be applied to several aspects of
physical reality that previously resisted systematic interrogation. Under the
new method, scientific theories are compared by instantiating them as
compression programs, and examining the codelengths they achieve on a database
of measurements related to a phenomenon of interest. Because of the
impossibility of compressing random data, ""real world"" data can only be
compressed by discovering and exploiting the empirical structure it exhibits.
The method also provides a new way of thinking about two longstanding issues in
the philosophy of science: the problem of induction and the problem of
demarcation.
  The second part of the paper proposes to reformulate computer vision as an
empirical science of visual reality, by applying the new method to large
databases of natural images. The immediate goal of the proposed reformulation
is to repair the chronic difficulties in evaluation experienced by the field of
computer vision. The reformulation should bring a wide range of benefits,
including a substantially increased degree of methodological rigor, the ability
to justify complex theories without overfitting, a scalable evaluation
paradigm, and the potential to make systematic progress. A crucial argument is
that the change is not especially drastic, because most computer vision tasks
can be reformulated as specialized image compression techniques. Finally, a
concrete proposal is discussed in which a database is produced by recording
from a roadside video camera, and compression is achieved by developing a
computational understanding of the appearance of moving cars."
"Content Based Image Retrieval (CBIR) systems based on shape using invariant
image moments, viz., Moment Invariants (MI) and Zernike Moments (ZM) are
available in the literature. MI and ZM are good at representing the shape
features of an image. However, non-orthogonality of MI and poor reconstruction
of ZM restrict their application in CBIR. Therefore, an efficient and
orthogonal moment based CBIR system is needed. Legendre Moments (LM) are
orthogonal, computationally faster, and can represent image shape features
compactly. CBIR system using Exact Legendre Moments (ELM) for gray scale images
is proposed in this work. Superiority of the proposed CBIR system is observed
over other moment based methods, viz., MI and ZM in terms of retrieval
efficiency and retrieval time. Further, the classification efficiency is
improved by employing Support Vector Machine (SVM) classifier. Improved
retrieval results are obtained over existing CBIR algorithm based on Stacked
Euler Vector (SERVE) combined with Modified Moment Invariants (MMI)."
"Wireless Capsule Endoscopy (WCE) is device to detect abnormalities in
colon,esophagus,small intestinal and stomach, to distinguish bleeding in WCE
images from non bleeding is a hard job by human reviewing and very time
consuming. Consequently, automation for classifying bleeding frames not only
will expedite the process but will reduce the burden on the doctors. Using the
purity of the red color we can detect the Bleeding areas in WCE images. But, we
could find various intensity of red color values in different parts of the
small intestinal,so it is not enough to depend on the red color feature alone.
We select RGB(Red,Green,Blue) because it takes raw level values and it is easy
to use. In this paper we will put range ratio color for each of R,G,and B.
Therefore, we divide each image into multiple pixels and apply the range ratio
color condition for each pixel. Then we count the number of the pixels that
achieved our condition. If the number of pixels grater than zero, then the
frame is classified as a bleeding type. Otherwise, it is a non-bleeding. Our
experimental results show that this method could achieve a very high accuracy
in detecting bleeding images for the different parts of the small intestinal"
"The physiological and behavioral trait is employed to develop biometric
authentication systems. The proposed work deals with the authentication of iris
and signature based on minimum variance criteria. The iris patterns are
preprocessed based on area of the connected components. The segmented image
used for authentication consists of the region with large variations in the
gray level values. The image region is split into quadtree components. The
components with minimum variance are determined from the training samples. Hu
moments are applied on the components. The summation of moment values
corresponding to minimum variance components are provided as input vector to
k-means and fuzzy kmeans classifiers. The best performance was obtained for MMU
database consisting of 45 subjects. The number of subjects with zero False
Rejection Rate [FRR] was 44 and number of subjects with zero False Acceptance
Rate [FAR] was 45. This paper addresses the computational load reduction in
off-line signature verification based on minimal features using k-means, fuzzy
k-means, k-nn, fuzzy k-nn and novel average-max approaches. FRR of 8.13% and
FAR of 10% was achieved using k-nn classifier. The signature is a biometric,
where variations in a genuine case, is a natural expectation. In the genuine
signature, certain parts of signature vary from one instance to another. The
system aims to provide simple, fast and robust system using less number of
features when compared to state of art works."
"The problem of image segmentation is known to become particularly challenging
in the case of partial occlusion of the object(s) of interest, background
clutter, and the presence of strong noise. To overcome this problem, the
present paper introduces a novel approach segmentation through the use of
""weak"" shape priors. Specifically, in the proposed method, an segmenting active
contour is constrained to converge to a configuration at which its geometric
parameters attain their empirical probability densities closely matching the
corresponding model densities that are learned based on training samples. It is
shown through numerical experiments that the proposed shape modeling can be
regarded as ""weak"" in the sense that it minimally influences the segmentation,
which is allowed to be dominated by data-related forces. On the other hand, the
priors provide sufficient constraints to regularize the convergence of
segmentation, while requiring substantially smaller training sets to yield less
biased results as compared to the case of PCA-based regularization methods. The
main advantages of the proposed technique over some existing alternatives is
demonstrated in a series of experiments."
"A difficult problem in clustering is how to handle data with a manifold
structure, i.e. data that is not shaped in the form of compact clouds of
points, forming arbitrary shapes or paths embedded in a high-dimensional space.
In this work we introduce the Penalized k-Nearest-Neighbor-Graph (PKNNG) based
metric, a new tool for evaluating distances in such cases. The new metric can
be used in combination with most clustering algorithms. The PKNNG metric is
based on a two-step procedure: first it constructs the k-Nearest-Neighbor-Graph
of the dataset of interest using a low k-value and then it adds edges with an
exponentially penalized weight for connecting the sub-graphs produced by the
first step. We discuss several possible schemes for connecting the different
sub-graphs. We use three artificial datasets in four different embedding
situations to evaluate the behavior of the new metric, including a comparison
among different clustering methods. We also evaluate the new metric in a real
world application, clustering the MNIST digits dataset. In all cases the PKNNG
metric shows promising clustering results."
"This paper presents an effective method for fingerprint verification based on
a data mining technique called minutiae clustering and a graph-theoretic
approach to analyze the process of fingerprint comparison to give a feature
space representation of minutiae and to produce a lower bound on the number of
detectably distinct fingerprints. The method also proving the invariance of
each individual fingerprint by using both the topological behavior of the
minutiae graph and also using a distance measure called Hausdorff distance.The
method provides a graph based index generation mechanism of fingerprint
biometric data. The self-organizing map neural network is also used for
classifying the fingerprints."
"A general framework for solving image inverse problems is introduced in this
paper. The approach is based on Gaussian mixture models, estimated via a
computationally efficient MAP-EM algorithm. A dual mathematical interpretation
of the proposed framework with structured sparse estimation is described, which
shows that the resulting piecewise linear estimate stabilizes the estimation
when compared to traditional sparse inverse problem techniques. This
interpretation also suggests an effective dictionary motivated initialization
for the MAP-EM algorithm. We demonstrate that in a number of image inverse
problems, including inpainting, zooming, and deblurring, the same algorithm
produces either equal, often significantly better, or very small margin worse
results than the best published ones, at a lower computational cost."
"This paper presents a survey of human action recognition approaches based on
visual data recorded from a single video camera. We propose an organizing
framework which puts in evidence the evolution of the area, with techniques
moving from heavily constrained motion capture scenarios towards more
challenging, realistic, ""in the wild"" videos. The proposed organization is
based on the representation used as input for the recognition task, emphasizing
the hypothesis assumed and thus, the constraints imposed on the type of video
that each technique is able to address. Expliciting the hypothesis and
constraints makes the framework particularly useful to select a method, given
an application. Another advantage of the proposed organization is that it
allows categorizing newest approaches seamlessly with traditional ones, while
providing an insightful perspective of the evolution of the action recognition
task up to now. That perspective is the basis for the discussion in the end of
the paper, where we also present the main open issues in the area."
"Minimization of boundary curvature is a classic regularization technique for
image segmentation in the presence of noisy image data. Techniques for
minimizing curvature have historically been derived from descent methods which
could be trapped in a local minimum and therefore required a good
initialization. Recently, combinatorial optimization techniques have been
applied to the optimization of curvature which provide a solution that achieves
nearly a global optimum. However, when applied to image segmentation these
methods required a meaningful data term. Unfortunately, for many images,
particularly medical images, it is difficult to find a meaningful data term.
Therefore, we propose to remove the data term completely and instead weight the
curvature locally, while still achieving a global optimum."
"Retrieving images from large and varied repositories using visual contents
has been one of major research items, but a challenging task in the image
management community. In this paper we present an efficient approach for
region-based image classification and retrieval using a fast multi-level neural
network model. The advantages of this neural model in image classification and
retrieval domain will be highlighted. The proposed approach accomplishes its
goal in three main steps. First, with the help of a mean-shift based
segmentation algorithm, significant regions of the image are isolated.
Secondly, color and texture features of each region are extracted by using
color moments and 2D wavelets decomposition technique. Thirdly the multi-level
neural classifier is trained in order to classify each region in a given image
into one of five predefined categories, i.e., ""Sky"", ""Building"", ""SandnRock"",
""Grass"" and ""Water"". Simulation results show that the proposed method is
promising in terms of classification and retrieval accuracy results. These
results compare favorably with the best published results obtained by other
state-of-the-art image retrieval techniques."
"One of the most visually demonstrable and straightforward uses of filtering
is in the field of Computer Vision. In this document we will try to outline the
issues encountered while designing and implementing a particle and kalman
filter based tracking system."
"Classification methods based on learning from examples have been widely
applied to character recognition from the 1990s and have brought forth
significant improvements of recognition accuracies. This class of methods
includes statistical methods, artificial neural networks, support vector
machines (SVM), multiple classifier combination, etc. In this paper, we discuss
the characteristics of the some classification methods that have been
successfully applied to handwritten Devnagari character recognition and results
of SVM and ANNs classification method, applied on Handwritten Devnagari
characters. After preprocessing the character image, we extracted shadow
features, chain code histogram features, view based features and longest run
features. These features are then fed to Neural classifier and in support
vector machine for classification. In neural classifier, we explored three ways
of combining decisions of four MLP's designed for four different features."
"This paper deals with a new method for recognition of offline Handwritten
non-compound Devnagari Characters in two stages. It uses two well known and
established pattern recognition techniques: one using neural networks and the
other one using minimum edit distance. Each of these techniques is applied on
different sets of characters for recognition. In the first stage, two sets of
features are computed and two classifiers are applied to get higher recognition
accuracy. Two MLP's are used separately to recognize the characters. For one of
the MLP's the characters are represented with their shadow features and for the
other chain code histogram feature is used. The decision of both MLP's is
combined using weighted majority scheme. Top three results produced by combined
MLP's in the first stage are used to calculate the relative difference values.
In the second stage, based on these relative differences character set is
divided into two. First set consists of the characters with distinct shapes and
second set consists of confused characters, which appear very similar in
shapes. Characters of distinct shapes of first set are classified using MLP.
Confused characters in second set are classified using minimum edit distance
method. Method of minimum edit distance makes use of corner detected in a
character image using modified Harris corner detection technique. Experiment on
this method is carried out on a database of 7154 samples. The overall
recognition is found to be 90.74%."
"In this paper a scheme for offline Handwritten Devnagari Character
Recognition is proposed, which uses different feature extraction methodologies
and recognition algorithms. The proposed system assumes no constraints in
writing style or size. First the character is preprocessed and features namely
: Chain code histogram and moment invariant features are extracted and fed to
Multilayer Perceptrons as a preliminary recognition step. Finally the results
of both MLP's are combined using weighted majority scheme. The proposed system
is tested on 1500 handwritten devnagari character database collected from
different people. It is observed that the proposed system achieves recognition
rates 98.03% for top 5 results and 89.46% for top 1 result."
"This work presents the application of weighted majority voting technique for
combination of classification decision obtained from three Multi_Layer
Perceptron(MLP) based classifiers for Recognition of Handwritten Devnagari
characters using three different feature sets. The features used are
intersection, shadow feature and chain code histogram features. Shadow features
are computed globally for character image while intersection features and chain
code histogram features are computed by dividing the character image into
different segments. On experimentation with a dataset of 4900 samples the
overall recognition rate observed is 92.16% as we considered top five choices
results. This method is compared with other recent methods for Handwritten
Devnagari Character Recognition and it has been observed that this approach has
better success rate than other methods."
"The paper presents a two stage classification approach for handwritten
devanagari characters The first stage is using structural properties like
shirorekha, spine in character and second stage exploits some intersection
features of characters which are fed to a feedforward neural network. Simple
histogram based method does not work for finding shirorekha, vertical bar
(Spine) in handwritten devnagari characters. So we designed a differential
distance based technique to find a near straight line for shirorekha and spine.
This approach has been tested for 50000 samples and we got 89.12% success"
"In this paper a method for recognition of handwritten devanagari characters
is described. Here, feature vector is constituted by accumulated directional
gradient changes in different segments, number of intersections points for the
character, type of spine present and type of shirorekha present in the
character. One Multi-layer Perceptron with conjugate-gradient training is used
to classify these feature vectors. This method is applied to a database with
1000 sample characters and the recognition rate obtained is 88.12%"
"A novel, generic scheme for off-line handwritten English alphabets character
images is proposed. The advantage of the technique is that it can be applied in
a generic manner to different applications and is expected to perform better in
uncertain and noisy environments. The recognition scheme is using a multilayer
perceptron(MLP) neural networks. The system was trained and tested on a
database of 300 samples of handwritten characters. For improved generalization
and to avoid overtraining, the whole available dataset has been divided into
two subsets: training set and test set. We achieved 99.10% and 94.15% correct
recognition rates on training and test sets respectively. The purposed scheme
is robust with respect to various writing styles and size as well as presence
of considerable noise."
"This paper aims at VLSI realization for generation of a new face from textual
description. The FASY (FAce SYnthesis) System is a Face Database Retrieval and
new Face generation System that is under development. One of its main features
is the generation of the requested face when it is not found in the existing
database. The new face generation system works in three steps - searching
phase, assembling phase and tuning phase. In this paper the tuning phase using
hardware description language and its implementation in a Field Programmable
Gate Array (FPGA) device is presented."
"This paper presents a novel type-2 Fuzzy logic System to define the Shape of
a facial component with the crisp output. This work is the part of our main
research effort to design a system (called FASY) which offers a novel face
construction approach based on the textual description and also extracts and
analyzes the facial components from a face image by an efficient technique. The
Fuzzy model, designed in this paper, takes crisp value of width and height of a
facial component and produces the crisp value of Shape for different facial
components. This method is designed using Matlab 6.5 and Visual Basic 6.0 and
tested with the facial components extracted from 200 male and female face
images of different ages from different face databases."
"Recognizing a face based on its attributes is an easy task for a human to
perform as it is a cognitive process. In recent years, Face Recognition is
achieved with different kinds of facial features which were used separately or
in a combined manner. Currently, Feature fusion methods and parallel methods
are the facial features used and performed by integrating multiple feature sets
at different levels. However, this integration and the combinational methods do
not guarantee better result. Hence to achieve better results, the feature
fusion model with multiple weighted facial attribute set is selected. For this
feature model, face images from predefined data set has been taken from
Olivetti Research Laboratory (ORL) and applied on different methods like
Principal Component Analysis (PCA) based Eigen feature extraction technique,
Discrete Cosine Transformation (DCT) based feature extraction technique,
Histogram Based Feature Extraction technique and Simple Intensity based
features. The extracted feature set obtained from these methods were compared
and tested for accuracy. In this work we have developed a model which will use
the above set of feature extraction techniques with different levels of weights
to attain better accuracy. The results show that the selection of optimum
weight for a particular feature will lead to improvement in recognition rate."
"Color space transformations are frequently used in image processing,
graphics, and visualization applications. In many cases, these transformations
are complex nonlinear functions, which prohibits their use in time-critical
applications. In this paper, we present a new approach called Minimax
Approximations for Color-space Transformations (MACT).We demonstrate MACT on
three commonly used color space transformations. Extensive experiments on a
large and diverse image set and comparisons with well-known multidimensional
lookup table interpolation methods show that MACT achieves an excellent balance
among four criteria: ease of implementation, memory usage, accuracy, and
computational speed."
"Accurately detecting pedestrians in images plays a critically important role
in many computer vision applications. Extraction of effective features is the
key to this task. Promising features should be discriminative, robust to
various variations and easy to compute. In this work, we present novel
features, termed dense center-symmetric local binary patterns (CS-LBP) and
pyramid center-symmetric local binary/ternary patterns (CS-LBP/LTP), for
pedestrian detection. The standard LBP proposed by Ojala et al. \cite{c4}
mainly captures the texture information. The proposed CS-LBP feature, in
contrast, captures the gradient information and some texture information.
Moreover, the proposed dense CS-LBP and the pyramid CS-LBP/LTP are easy to
implement and computationally efficient, which is desirable for real-time
applications. Experiments on the INRIA pedestrian dataset show that the dense
CS-LBP feature with linear supporct vector machines (SVMs) is comparable with
the histograms of oriented gradients (HOG) feature with linear SVMs, and the
pyramid CS-LBP/LTP features outperform both HOG features with linear SVMs and
the start-of-the-art pyramid HOG (PHOG) feature with the histogram intersection
kernel SVMs. We also demonstrate that the combination of our pyramid CS-LBP
feature and the PHOG feature could significantly improve the detection
performance-producing state-of-the-art accuracy on the INRIA pedestrian
dataset."
"Reduced ordering based vector filters have proved successful in removing
long-tailed noise from color images while preserving edges and fine image
details. These filters commonly utilize variants of the Minkowski distance to
order the color vectors with the aim of distinguishing between noisy and
noise-free vectors. In this paper, we review various alternative distance
measures and evaluate their performance on a large and diverse set of images
using several effectiveness and efficiency criteria. The results demonstrate
that there are in fact strong alternatives to the popular Minkowski metrics."
"Vector filters based on order-statistics have proved successful in removing
impulsive noise from color images while preserving edges and fine image
details. Among these filters, the ones that involve the cosine distance
function (directional filters) have particularly high computational
requirements, which limits their use in time critical applications. In this
paper, we introduce two methods to speed up these filters. Experiments on a
diverse set of color images show that the proposed methods provide substantial
computational gains without significant loss of accuracy."
"Vector operators based on robust order statistics have proved successful in
digital multichannel imaging applications, particularly color image filtering
and enhancement, in dealing with impulsive noise while preserving edges and
fine image details. These operators often have very high computational
requirements which limits their use in time-critical applications. This paper
introduces techniques to speed up vector filters using the minimax
approximation theory. Extensive experiments on a large and diverse set of color
images show that proposed approximations achieve an excellent balance among
ease of implementation, accuracy, and computational speed."
"In this paper, we present a fast switching filter for impulsive noise removal
from color images. The filter exploits the HSL color space, and is based on the
peer group concept, which allows for the fast detection of noise in a
neighborhood without resorting to pairwise distance computations between each
pixel. Experiments on large set of diverse images demonstrate that the proposed
approach is not only extremely fast, but also gives excellent results in
comparison to various state-of-the-art filters."
"In this paper, a comprehensive survey of 48 filters for impulsive noise
removal from color images is presented. The filters are formulated using a
uniform notation and categorized into 8 families. The performance of these
filters is compared on a large set of images that cover a variety of domains
using three effectiveness and one efficiency criteria. In order to ensure a
fair efficiency comparison, a fast and accurate approximation for the inverse
cosine function is introduced. In addition, commonly used distance measures
(Minkowski, angular, and directional-distance) are analyzed and evaluated.
Finally, suggestions are provided on how to choose a filter given certain
requirements."
"Dermoscopy is a non-invasive skin imaging technique, which permits
visualization of features of pigmented melanocytic neoplasms that are not
discernable by examination with the naked eye. One of the most important
features for the diagnosis of melanoma in dermoscopy images is the blue-white
veil (irregular, structureless areas of confluent blue pigmentation with an
overlying white ""ground-glass"" film). In this article, we present a machine
learning approach to the detection of blue-white veil and related structures in
dermoscopy images. The method involves contextual pixel classification using a
decision tree classifier. The percentage of blue-white areas detected in a
lesion combined with a simple shape descriptor yielded a sensitivity of 69.35%
and a specificity of 89.97% on a set of 545 dermoscopy images. The sensitivity
rises to 78.20% for detection of blue veil in those cases where it is a primary
feature for melanoma recognition."
"Background: Dermoscopy is one of the major imaging modalities used in the
diagnosis of melanoma and other pigmented skin lesions. Due to the difficulty
and subjectivity of human interpretation, dermoscopy image analysis has become
an important research area. One of the most important steps in dermoscopy image
analysis is the automated detection of lesion borders. Although numerous
methods have been developed for the detection of lesion borders, very few
studies were comprehensive in the evaluation of their results. Methods: In this
paper, we evaluate five recent border detection methods on a set of 90
dermoscopy images using three sets of dermatologist-drawn borders as the
ground-truth. In contrast to previous work, we utilize an objective measure,
the Normalized Probabilistic Rand Index, which takes into account the
variations in the ground-truth images. Conclusion: The results demonstrate that
the differences between four of the evaluated border detection methods are in
fact smaller than those predicted by the commonly used XOR measure."
"Background: Dermoscopy is one of the major imaging modalities used in the
diagnosis of melanoma and other pigmented skin lesions. Due to the difficulty
and subjectivity of human interpretation, automated analysis of dermoscopy
images has become an important research area. Border detection is often the
first step in this analysis. Methods: In this article, we present an
approximate lesion localization method that serves as a preprocessing step for
detecting borders in dermoscopy images. In this method, first the black frame
around the image is removed using an iterative algorithm. The approximate
location of the lesion is then determined using an ensemble of thresholding
algorithms. Results: The method is tested on a set of 428 dermoscopy images.
The localization error is quantified by a metric that uses dermatologist
determined borders as the ground truth. Conclusion: The results demonstrate
that the method presented here achieves both fast and accurate localization of
lesions in dermoscopy images."
"In this paper, Deterministic Cellular Automata (DCA) based video shot
classification and retrieval is proposed. The deterministic 2D Cellular
automata model captures the human facial expressions, both spontaneous and
posed. The determinism stems from the fact that the facial muscle actions are
standardized by the encodings of Facial Action Coding System (FACS) and Action
Units (AUs). Based on these encodings, we generate the set of evolutionary
update rules of the DCA for each facial expression. We consider a
Person-Independent Facial Expression Space (PIFES) to analyze the facial
expressions based on Partitioned 2D-Cellular Automata which capture the
dynamics of facial expressions and classify the shots based on it. Target video
shot is retrieved by comparing the similar expression is obtained for the query
frame's face with respect to the key faces expressions in the database video.
Consecutive key face expressions in the database that are highly similar to the
query frame's face, then the key faces are used to generate the set of
retrieved video shots from the database. A concrete example of its application
which realizes an affective interaction between the computer and the user is
proposed. In the affective interaction, the computer can recognize the facial
expression of any given video shot. This interaction endows the computer with
certain ability to adapt to the user's feedback."
"Image hashing is the process of associating a short vector of bits to an
image. The resulting summaries are useful in many applications including image
indexing, image authentication and pattern recognition. These hashes need to be
invariant under transformations of the image that result in similar visual
content, but should drastically differ for conceptually distinct contents. This
paper proposes an image hashing method that is invariant under rotation,
scaling and translation of the image. The gist of our approach relies on the
geometric characterization of salient point distribution in the image. This is
achieved by the definition of a ""saliency graph"" connecting these points
jointly with an image intensity function on the graph nodes. An invariant hash
is then obtained by considering the spectrum of this function in the
eigenvector basis of the Laplacian graph, that is, its graph Fourier transform.
Interestingly, this spectrum is invariant under any relabeling of the graph
nodes. The graph reveals geometric information of the image, making the hash
robust to image transformation, yet distinct for different visual content. The
efficiency of the proposed method is assessed on a set of MRI 2-D slices and on
a database of faces."
"Real-time object detection is one of the core problems in computer vision.
The cascade boosting framework proposed by Viola and Jones has become the
standard for this problem. In this framework, the learning goal for each node
is asymmetric, which is required to achieve a high detection rate and a
moderate false positive rate. We develop new boosting algorithms to address
this asymmetric learning problem. We show that our methods explicitly optimize
asymmetric loss objectives in a totally corrective fashion. The methods are
totally corrective in the sense that the coefficients of all selected weak
classifiers are updated at each iteration. In contract, conventional boosting
like AdaBoost is stage-wise in that only the current weak classifier's
coefficient is updated. At the heart of the totally corrective boosting is the
column generation technique. Experiments on face detection show that our
methods outperform the state-of-the-art asymmetric boosting methods."
"This paper deals with an improvement of vertex based nonlinear diffusion for
mesh denoising. This method directly filters the position of the vertices using
Laplace, reduced centered Gaussian and Rayleigh probability density functions
as diffusivities. The use of these PDFs improves the performance of a
vertex-based diffusion method which are adapted to the underlying mesh
structure. We also compare the proposed method to other mesh denoising methods
such as Laplacian flow, mean, median, min and the adaptive MMSE filtering. To
evaluate these methods of filtering, we use two error metrics. The first is
based on the vertices and the second is based on the normals. Experimental
results demonstrate the effectiveness of our proposed method in comparison with
the existing methods."
"Many algorithms for approximate nearest neighbor search in high-dimensional
spaces partition the data into clusters. At query time, in order to avoid
exhaustive search, an index selects the few (or a single) clusters nearest to
the query point. Clusters are often produced by the well-known $k$-means
approach since it has several desirable properties. On the downside, it tends
to produce clusters having quite different cardinalities. Imbalanced clusters
negatively impact both the variance and the expectation of query response
times. This paper proposes to modify $k$-means centroids to produce clusters
with more comparable sizes without sacrificing the desirable properties.
Experiments with a large scale collection of image descriptors show that our
algorithm significantly reduces the variance of response times without
seriously impacting the search quality."
"This project aims to create 3d model of the natural world and model changes
in it instantaneously. A framework for modeling instantaneous changes natural
scenes in real time using Lagrangian Particle Framework and a fluid-particle
grid approach is presented. This project is presented in the form of a
proof-based system where we show that the design is very much possible but
currently we only have selective scripts that accomplish the given job, a
complete software however is still under work. This research can be divided
into 3 distinct sections: the first one discusses a multi-camera rig that can
measure ego-motion accurately up to 88%, how this device becomes the backbone
of our framework, and some improvements devised to optimize a know framework
for depth maps and 3d structure estimation from a single still image called
make3d. The second part discusses the fluid-particle framework to model natural
scenes, presents some algorithms that we are using to accomplish this task and
we show how an application of our framework can extend make3d to model natural
scenes in real time. This part of the research constructs a bridge between
computer vision and computer graphics so that now ideas, answers and intuitions
that arose in the domain of computer graphics can now be applied to computer
vision and natural modeling. The final part of this research improves upon what
might become the first general purpose vision system using deep belief
architectures and provides another framework to improve the lower bound on
training images for boosting by using a variation of Restricted Boltzmann
machines (RBM). We also discuss other applications that might arise from our
work in these areas."
"We propose a mid-level image segmentation framework that combines multiple
figure-ground hypothesis (FG) constrained at different locations and scales,
into interpretations that tile the entire image. The problem is cast as
optimization over sets of maximal cliques sampled from the graph connecting
non-overlapping, putative figure-ground segment hypotheses. Potential functions
over cliques combine unary Gestalt-based figure quality scores and pairwise
compatibilities among spatially neighboring segments, constrained by
T-junctions and the boundary interface statistics resulting from projections of
real 3d scenes. Learning the model parameters is formulated as rank
optimization, alternating between sampling image tilings and optimizing their
potential function parameters. State of the art results are reported on both
the Berkeley and the VOC2009 segmentation dataset, where a 28% improvement was
achieved."
"This paper introduces a novel method for human face detection with its
orientation by using wavelet, principle component analysis (PCA) and redial
basis networks. The input image is analyzed by two-dimensional wavelet and a
two-dimensional stationary wavelet. The common goals concern are the image
clearance and simplification, which are parts of de-noising or compression. We
applied an effective procedure to reduce the dimension of the input vectors
using PCA. Radial Basis Function (RBF) neural network is then used as a
function approximation network to detect where either the input image is
contained a face or not and if there is a face exists then tell about its
orientation. We will show how RBF can perform well then back-propagation
algorithm and give some solution for better regularization of the RBF (GRNN)
network. Compared with traditional RBF networks, the proposed network
demonstrates better capability of approximation to underlying functions, faster
learning speed, better size of network, and high robustness to outliers."
"There is an abundant literature on face detection due to its important role
in many vision applications. Since Viola and Jones proposed the first real-time
AdaBoost based face detector, Haar-like features have been adopted as the
method of choice for frontal face detection. In this work, we show that simple
features other than Haar-like features can also be applied for training an
effective face detector. Since, single feature is not discriminative enough to
separate faces from difficult non-faces, we further improve the generalization
performance of our simple features by introducing feature co-occurrences. We
demonstrate that our proposed features yield a performance improvement compared
to Haar-like features. In addition, our findings indicate that features play a
crucial role in the ability of the system to generalize."
"Image segmentation has been a very active research topic in image analysis
area. Currently, most of the image segmentation algorithms are designed based
on the idea that images are partitioned into a set of regions preserving
homogeneous intra-regions and inhomogeneous inter-regions. However, human
visual intuition does not always follow this pattern. A new image segmentation
method named Visual-Hint Boundary to Segment (VHBS) is introduced, which is
more consistent with human perceptions. VHBS abides by two visual hint rules
based on human perceptions: (i) the global scale boundaries tend to be the real
boundaries of the objects; (ii) two adjacent regions with quite different
colors or textures tend to result in the real boundaries between them. It has
been demonstrated by experiments that, compared with traditional image
segmentation method, VHBS has better performance and also preserves higher
computational efficiency."
"Matching pursuit and K-SVD is demonstrated in the translation invariant
setting"
"We present here a first prototype of a ""Speed Limit Support"" Advance Driving
Assistance System (ADAS) producing permanent reliable information on the
current speed limit applicable to the vehicle. Such a module can be used either
for information of the driver, or could even serve for automatic setting of the
maximum speed of a smart Adaptive Cruise Control (ACC). Our system is based on
a joint interpretation of cartographic information (for static reference
information) with on-board vision, used for traffic sign detection and
recognition (including supplementary sub-signs) and visual road lines
localization (for detection of lane changes). The visual traffic sign detection
part is quite robust (90% global correct detection and recognition for main
speed signs, and 80% for exit-lane sub-signs detection). Our approach for joint
interpretation with cartography is original, and logic-based rather than
probability-based, which allows correct behaviour even in cases, which do
happen, when both vision and cartography may provide the same erroneous
information."
"The so-called factorization methods recover 3-D rigid structure from motion
by factorizing an observation matrix that collects 2-D projections of features.
These methods became popular due to their robustness - they use a large number
of views, which constrains adequately the solution - and computational
simplicity - the large number of unknowns is computed through an SVD, avoiding
non-linear optimization. However, they require that all the entries of the
observation matrix are known. This is unlikely to happen in practice, due to
self-occlusion and limited field of view. Also, when processing long videos,
regions that become occluded often appear again later. Current factorization
methods process these as new regions, leading to less accurate estimates of 3-D
structure. In this paper, we propose a global factorization method that infers
complete 3-D models directly from the 2-D projections in the entire set of
available video frames. Our method decides whether a region that has become
visible is a region that was seen before, or a previously unseen region, in a
global way, i.e., by seeking the simplest rigid object that describes well the
entire set of observations. This global approach increases significantly the
accuracy of the estimates of the 3-D shape of the scene and the 3-D motion of
the camera. Experiments with artificial and real videos illustrate the good
performance of our method."
"The majority of the approaches to the automatic recovery of a panoramic image
from a set of partial views are suboptimal in the sense that the input images
are aligned, or registered, pair by pair, e.g., consecutive frames of a video
clip. These approaches lead to propagation errors that may be very severe,
particularly when dealing with videos that show the same region at disjoint
time intervals. Although some authors have proposed a post-processing step to
reduce the registration errors in these situations, there have not been
attempts to compute the optimal solution, i.e., the registrations leading to
the panorama that best matches the entire set of partial views}. This is our
goal. In this paper, we use a generative model for the partial views of the
panorama and develop an algorithm to compute in an efficient way the Maximum
Likelihood estimate of all the unknowns involved: the parameters describing the
alignment of all the images and the panorama itself."
"In image analysis, many tasks require representing two-dimensional (2D)
shape, often specified by a set of 2D points, for comparison purposes. The
challenge of the representation is that it must not only capture the
characteristics of the shape but also be invariant to relevant transformations.
Invariance to geometric transformations, such as translation, rotation, and
scale, has received attention in the past, usually under the assumption that
the points are previously labeled, i.e., that the shape is characterized by an
ordered set of landmarks. However, in many practical scenarios, the points
describing the shape are obtained from automatic processes, e.g., edge or
corner detection, thus without labels or natural ordering. Obviously, the
combinatorial problem of computing the correspondences between the points of
two shapes in the presence of the aforementioned geometrical distortions
becomes a quagmire when the number of points is large. We circumvent this
problem by representing shapes in a way that is invariant to the permutation of
the landmarks, i.e., we represent bags of unlabeled 2D points. Within our
framework, a shape is mapped to an analytic function on the complex plane,
leading to what we call its analytic signature (ANSIG). To store an ANSIG, it
suffices to sample it along a closed contour in the complex plane. We show that
the ANSIG is a maximal invariant with respect to the permutation group, i.e.,
that different shapes have different ANSIGs and shapes that differ by a
permutation (or re-labeling) of the landmarks have the same ANSIG. We further
show how easy it is to factor out geometric transformations when comparing
shapes using the ANSIG representation. Finally, we illustrate these
capabilities with shape-based image classification experiments."
"When comparing 2D shapes, a key issue is their normalization. Translation and
scale are easily taken care of by removing the mean and normalizing the energy.
However, defining and computing the orientation of a 2D shape is not so simple.
In fact, although for elongated shapes the principal axis can be used to define
one of two possible orientations, there is no such tool for general shapes. As
we show in the paper, previous approaches fail to compute the orientation of
even noiseless observations of simple shapes. We address this problem. In the
paper, we show how to uniquely define the orientation of an arbitrary 2D shape,
in terms of what we call its Principal Moments. We show that a small subset of
these moments suffice to represent the underlying 2D shape and propose a new
method to efficiently compute the shape orientation: Principal Moment Analysis.
Finally, we discuss how this method can further be applied to normalize
grey-level images. Besides the theoretical proof of correctness, we describe
experiments demonstrating robustness to noise and illustrating the method with
real images."
"A new framework of compressive sensing (CS), namely statistical compressive
sensing (SCS), that aims at efficiently sampling a collection of signals that
follow a statistical distribution and achieving accurate reconstruction on
average, is introduced. For signals following a Gaussian distribution, with
Gaussian or Bernoulli sensing matrices of O(k) measurements, considerably
smaller than the O(k log(N/k)) required by conventional CS, where N is the
signal dimension, and with an optimal decoder implemented with linear
filtering, significantly faster than the pursuit decoders applied in
conventional CS, the error of SCS is shown tightly upper bounded by a constant
times the k-best term approximation error, with overwhelming probability. The
failure probability is also significantly smaller than that of conventional CS.
Stronger yet simpler results further show that for any sensing matrix, the
error of Gaussian SCS is upper bounded by a constant times the k-best term
approximation with probability one, and the bound constant can be efficiently
calculated. For signals following Gaussian mixture models, SCS with a piecewise
linear decoder is introduced and shown to produce for real images better
results than conventional CS based on sparse models."
"A collaborative framework for detecting the different sources in mixed
signals is presented in this paper. The approach is based on C-HiLasso, a
convex collaborative hierarchical sparse model, and proceeds as follows. First,
we build a structured dictionary for mixed signals by concatenating a set of
sub-dictionaries, each one of them learned to sparsely model one of a set of
possible classes. Then, the coding of the mixed signal is performed by
efficiently solving a convex optimization problem that combines standard
sparsity with group and collaborative sparsity. The present sources are
identified by looking at the sub-dictionaries automatically selected in the
coding. The collaborative filtering in C-HiLasso takes advantage of the
temporal/spatial redundancy in the mixed signals, letting collections of
samples collaborate in identifying the classes, while allowing individual
samples to have different internal sparse representations. This collaboration
is critical to further stabilize the sparse representation of signals, in
particular the class/sub-dictionary selection. The internal sparsity inside the
sub-dictionaries, as naturally incorporated by the hierarchical aspects of
C-HiLasso, is critical to make the model consistent with the essence of the
sub-dictionaries that have been trained for sparse representation of each
individual class. We present applications from speaker and instrument
identification and texture separation. In the case of audio signals, we use
sparse modeling to describe the short-term power spectrum envelopes of harmonic
sounds. The proposed pitch independent method automatically detects the number
of sources on a recording."
"In this paper we propose a vision system that performs image Super Resolution
(SR) with selectivity. Conventional SR techniques, either by multi-image fusion
or example-based construction, have failed to capitalize on the intrinsic
structural and semantic context in the image, and performed ""blind"" resolution
recovery to the entire image area. By comparison, we advocate example-based
selective SR whereby selectivity is exemplified in three aspects: region
selectivity (SR only at object regions), source selectivity (object SR with
trained object dictionaries), and refinement selectivity (object boundaries
refinement using matting). The proposed system takes over-segmented
low-resolution images as inputs, assimilates recent learning techniques of
sparse coding (SC) and grouped multi-task lasso (GMTL), and leads eventually to
a framework for joint figure-ground separation and interest object SR. The
efficiency of our framework is manifested in our experiments with subsets of
the VOC2009 and MSRC datasets. We also demonstrate several interesting vision
applications that can build on our system."
"This work presents a framework for tracking head movements and capturing the
movements of the mouth and both the eyebrows in real-time. We present a head
tracker which is a combination of a optical flow and a template based tracker.
The estimation of the optical flow head tracker is used as starting point for
the template tracker which fine-tunes the head estimation. This approach
together with re-updating the optical flow points prevents the head tracker
from drifting. This combination together with our switching scheme, makes our
tracker very robust against fast movement and motion-blur. We also propose a
way to reduce the influence of partial occlusion of the head. In both the
optical flow and the template based tracker we identify and exclude occluded
points."
"Accumulating evidence has shown that iron is involved in the mechanism
underlying many neurodegenerative diseases, such as Alzheimer's disease,
Parkinson's disease and Huntington's disease. Abnormal (higher) iron
accumulation has been detected in the brains of most neurodegenerative
patients, especially in the basal ganglia region. Presence of iron leads to
changes in MR signal in both magnitude and phase. Accordingly, tissues with
high iron concentration appear hypo-intense (darker than usual) in MR
contrasts. In this report, we proposed an improved binary hypointensity
description and a novel nonbinary hypointensity description based on principle
components analysis. Moreover, Kendall's rank correlation coefficient was used
to compare the complementary and redundant information provided by the two
methods in order to better understand the individual descriptions of iron
accumulation in the brain."
"Two types of combining strategies were evaluated namely combining skin
features and combining skin classifiers. Several combining rules were applied
where the outputs of the skin classifiers are combined using binary operators
such as the AND and the OR operators, ""Voting"", ""Sum of Weights"" and a new
neural network. Three chrominance components from the YCbCr colour space that
gave the highest correct detection on their single feature MLP were selected as
the combining parameters. A major issue in designing a MLP neural network is to
determine the optimal number of hidden units given a set of training patterns.
Therefore, a ""coarse to fine search"" method to find the number of neurons in
the hidden layer is proposed. The strategy of combining Cb/Cr and Cr features
improved the correct detection by 3.01% compared to the best single feature MLP
given by Cb-Cr. The strategy of combining the outputs of three skin classifiers
using the ""Sum of Weights"" rule further improved the correct detection by 4.38%
compared to the best single feature MLP."
"Due to huge deformation in the camera captured images, variety in nature of
the business cards and the computational constraints of the mobile devices,
design of an efficient Business Card Reader (BCR) is challenging to the
researchers. Extraction of text regions and segmenting them into characters is
one of such challenges. In this paper, we have presented an efficient character
segmentation technique for business card images captured by a cell-phone
camera, designed in our present work towards developing an efficient BCR. At
first, text regions are extracted from the card images and then the skewed ones
are corrected using a computationally efficient skew correction technique. At
last, these skew corrected text regions are segmented into lines and characters
based on horizontal and vertical histogram. Experiments show that the present
technique is efficient and applicable for mobile devices, and the mean
segmentation accuracy of 97.48% is achieved with 3 mega-pixel (500-600 dpi)
images. It takes only 1.1 seconds for segmentation including all the
preprocessing steps on a moderately powerful notebook (DualCore T2370, 1.73
GHz, 1GB RAM, 1MB L2 Cache)."
"Various applications of car plate recognition systems have been developed
using various kinds of methods and techniques by researchers all over the
world. The applications developed were only suitable for specific country due
to its standard specification endorsed by the transport department of
particular countries. The Road Transport Department of Malaysia also has
endorsed a specification for car plates that includes the font and size of
characters that must be followed by car owners. However, there are cases where
this specification is not followed. Several applications have been developed in
Malaysia to overcome this problem. However, there is still problem in achieving
100% recognition accuracy. This paper is mainly focused on conducting an
experiment using chain codes technique to perform recognition for different
types of fonts used in Malaysian car plates."
"A symmetrical model of color vision, the decoding model as a new version of
zone model, was introduced. The model adopts new continuous-valued logic and
works in a way very similar to the way a 3-8 decoder in a numerical circuit
works. By the decoding model, Young and Helmholtz's tri-pigment theory and
Hering's opponent theory are unified more naturally; opponent process, color
evolution, and color blindness are illustrated more concisely. According to the
decoding model, we can obtain a transform from RGB system to HSV system, which
is formally identical to the popular transform for computer graphics provided
by Smith (1978). Advantages, problems, and physiological tests of the decoding
model are also discussed."
"In this work, the possibilities for segmentation of cells from their
background and each other in digital image were tested, combined and improoved.
Lot of images with young, adult and mixture cells were able to prove the
quality of described algorithms. Proper segmentation is one of the main task of
image analysis and steps order differ from work to work, depending on input
images. Reply for biologicaly given question was looking for in this work,
including filtration, details emphasizing, segmentation and sphericity
computing. Order of algorithms and way to searching for them was also
described. Some questions and ideas for further work were mentioned in the
conclusion part."
"English Character Recognition (CR) has been extensively studied in the last
half century and progressed to a level, sufficient to produce technology driven
applications. But same is not the case for Indian languages which are
complicated in terms of structure and computations. Rapidly growing
computational power may enable the implementation of Indic CR methodologies.
Digital document processing is gaining popularity for application to office and
library automation, bank and postal services, publishing houses and
communication technology. Devnagari being the national language of India,
spoken by more than 500 million people, should be given special attention so
that document retrieval and analysis of rich ancient and modern Indian
literature can be effectively done. This article is intended to serve as a
guide and update for the readers, working in the Devnagari Optical Character
Recognition (DOCR) area. An overview of DOCR systems is presented and the
available DOCR techniques are reviewed. The current status of DOCR is discussed
and directions for future research are suggested."
"In this paper, we explore the use of the diffusion geometry framework for the
fusion of geometric and photometric information in local and global shape
descriptors. Our construction is based on the definition of a diffusion process
on the shape manifold embedded into a high-dimensional space where the
embedding coordinates represent the photometric information. Experimental
results show that such data fusion is useful in coping with different
challenges of shape analysis where pure geometric and pure photometric methods
fail."
"The richness of natural images makes the quest for optimal representations in
image processing and computer vision challenging. The latter observation has
not prevented the design of image representations, which trade off between
efficiency and complexity, while achieving accurate rendering of smooth regions
as well as reproducing faithful contours and textures. The most recent ones,
proposed in the past decade, share an hybrid heritage highlighting the
multiscale and oriented nature of edges and patterns in images. This paper
presents a panorama of the aforementioned literature on decompositions in
multiscale, multi-orientation bases or dictionaries. They typically exhibit
redundancy to improve sparsity in the transformed domain and sometimes its
invariance with respect to simple geometric deformations (translation,
rotation). Oriented multiscale dictionaries extend traditional wavelet
processing and may offer rotation invariance. Highly redundant dictionaries
require specific algorithms to simplify the search for an efficient (sparse)
representation. We also discuss the extension of multiscale geometric
decompositions to non-Euclidean domains such as the sphere or arbitrary meshed
surfaces. The etymology of panorama suggests an overview, based on a choice of
partially overlapping ""pictures"". We hope that this paper will contribute to
the appreciation and apprehension of a stream of current research directions in
image understanding."
"This report presents the results and details of a content-based image
retrieval project using the Top-surf descriptor. The experimental results are
preliminary, however, it shows the capability of deducing objects from parts of
the objects or from the objects that are similar. This paper uses a dataset
consisting of 1200 images of which 800 images are equally divided into 8
categories, namely airplane, beach, motorbike, forest, elephants, horses, bus
and building, while the other 400 images are randomly picked from the Internet.
The best results achieved are from building category."
"Bag-of-words model is implemented and tried on 10-class visual concept
detection problem. The experimental results show that ""DURF+ERT+SVM""
outperforms ""SIFT+ERT+SVM"" both in detection performance and computation
efficiency. Besides, combining DURF and SIFT results in even better detection
performance. Real-time object detection using SIFT and RANSAC is also tried on
simple objects, e.g. drink can, and good result is achieved."
"Facial expressions convey non-verbal cues, which play an important role in
interpersonal relations. Automatic recognition of human face based on facial
expression can be an important component of natural human-machine interface. It
may also be used in behavioural science. Although human can recognize the face
practically without any effort, but reliable face recognition by machine is a
challenge. This paper presents a new approach for recognizing the face of a
person considering the expressions of the same human face at different
instances of time. This methodology is developed combining Eigenface method for
feature extraction and modified k-Means clustering for identification of the
human face. This method endowed the face recognition without using the
conventional distance measure classifiers. Simulation results show that
proposed face recognition using perception of k-Means clustering is useful for
face images with different facial expressions."
"A new method is proposed to get image features' geometric information. Using
Gaussian as an input signal, a theoretical optimal solution to calculate
feature's affine shape is proposed. Based on analytic result of a feature
model, the method is different from conventional iterative approaches. From the
model, feature's parameters such as position, orientation, background
luminance, contrast, area and aspect ratio can be extracted. Tested with
synthesized and benchmark data, the method achieves or outperforms existing
approaches in term of accuracy, speed and stability. The method can detect
small, long or thin objects precisely, and works well under general conditions,
such as for low contrast, blurred or noisy images."
"Design of a fuzzy rule based classifier is proposed. The performance of the
classifier for multispectral satellite image classification is improved using
Dempster- Shafer theory of evidence that exploits information of the
neighboring pixels. The classifiers are tested rigorously with two known images
and their performance are found to be better than the results available in the
literature. We also demonstrate the improvement of performance while using D-S
theory along with fuzzy rule based classifiers over the basic fuzzy rule based
classifiers for all the test cases."
"We present a novel method that allows for measuring the quality of
diffusion-weighted MR images dependent on the image resolution and the image
noise. For this purpose, we introduce a new thresholding technique so that
noise and the signal can automatically be estimated from a single data set.
Thus, no user interaction as well as no double acquisition technique, which
requires a time-consuming proper geometrical registration, is needed. As a
coarser image resolution or slice thickness leads to a higher signal-to-noise
ratio (SNR), our benchmark determines a resolution-independent quality measure
so that images with different resolutions can be adequately compared. To
evaluate our method, a set of diffusion-weighted images from different vendors
is used. It is shown that the quality can efficiently be determined and that
the automatically computed SNR is comparable to the SNR which is measured
manually in a manually selected region of interest."
"In this paper, a new method for offline handwritten signature retrieval is
based on curvelet transform is proposed. Many applications in image processing
require similarity retrieval of an image from a large collection of images. In
such cases, image indexing becomes important for efficient organization and
retrieval of images. This paper addresses this issue in the context of a
database of handwritten signature images and describes a system for similarity
retrieval. The proposed system uses a curvelet based texture features
extraction. The performance of the system has been tested with an image
database of 180 signatures. The results obtained indicate that the proposed
system is able to identify signatures with great with accuracy even when a part
of a signature is missing."
"Template matching is one of the most prevalent pattern recognition methods
worldwide. It has found uses in most visual concept detection fields. In this
work, we investigate methods for improving template matching by adjusting the
weights of different regions of the template. We compare several weight maps
and test the methods using the FERET face test set in the context of human eye
detection."
"The main goal of the GEOMIR2K9 project is to create a software program that
is able to find similar scenic images clustered by geographical location and
sorted by similarity based only on their visual content. The user should be
able to input a query image, based on this given query image the program should
find relevant visual content and present this to the user in a meaningful way.
Technically the goal for the GEOMIR2K9 project is twofold. The first of these
two goals is to create a basic low level visual information retrieval system.
This includes feature extraction, post processing of the feature data and
classification/ clustering based on similarity with a strong focus on scenic
images. The second goal of this project is to provide the user with a novel and
suitable interface and visualization method so that the user may interact with
the retrieved images in a natural and meaningful way."
"We demonstrate the possibility of coding parts, features that are higher
level than boundaries, using a modified AT field after augmenting the
interaction term of the AT energy with a non-local term and weakening the
separation into boundary/not-boundary phases. The iteratively extracted parts
using the level curves with double point singularities are organized as a
proper binary tree. Inconsistencies due to non-generic configurations for level
curves as well as due to visual changes such as occlusion are successfully
handled once the tree is endowed with a probabilistic structure. The work is a
step in establishing the AT function as a bridge between low and high level
visual processing."
"Perception research provides strong evidence in favor of part based
representation of shapes in human visual system. Despite considerable
differences among different theories in terms of how part boundaries are found,
there is substantial agreement on that the process depends on many local and
global geometric factors. This poses an important challenge from the
computational point of view. In the first part of the chapter, I present a
novel decomposition method by taking both local and global interactions within
the shape domain into account. At the top of the partitioning hierarchy, the
shape gets split into two parts capturing, respectively, the gross structure
and the peripheral structure. The gross structure may be conceived as the least
deformable part of the shape which remains stable under visual transformations.
The peripheral structure includes limbs, protrusions, and boundary texture.
Such a separation is in accord with the behavior of the artists who start with
a gross shape and enrich it with details. The method is particularly
interesting from the computational point of view as it does not resort to any
geometric notions (e.g. curvature, convexity) explicitly. In the second part of
the chapter, I relate the new method to PDE based shape representation schemes."
"This paper presents a new axis-based shape representation scheme along with a
matching framework to address the problem of generic shape recognition. The
main idea is to define the relative spatial arrangement of local symmetry axes
and their metric properties in a shape centered coordinate frame. The resulting
descriptions are invariant to scale, rotation, small changes in viewpoint and
articulations. Symmetry points are extracted from a surface whose level curves
roughly mimic the motion by curvature. By increasing the amount of smoothing on
the evolving curve, only those symmetry axes that correspond to the most
prominent parts of a shape are extracted. The representation does not suffer
from the common instability problems of the traditional connected skeletons. It
captures the perceptual qualities of shapes well. Therefore finding the
similarities and the differences among shapes becomes easier. The matching
process gives highly successful results on a diverse database of 2D shapes."
"We present a new skeletal representation along with a matching framework to
address the deformable shape recognition problem. The disconnectedness arises
as a result of excessive regularization that we use to describe a shape at an
attainably coarse scale. Our motivation is to rely on the stable properties of
the shape instead of inaccurately measured secondary details. The new
representation does not suffer from the common instability problems of
traditional connected skeletons, and the matching process gives quite
successful results on a diverse database of 2D shapes. An important difference
of our approach from the conventional use of the skeleton is that we replace
the local coordinate frame with a global Euclidean frame supported by
additional mechanisms to handle articulations and local boundary deformations.
As a result, we can produce descriptions that are sensitive to any combination
of changes in scale, position, orientation and articulation, as well as
invariant ones."
"Despite the recent developments in spatiotemporal local features for action
recognition in video sequences, local color information has so far been
ignored. However, color has been proved an important element to the success of
automated recognition of objects and scenes. In this paper we extend the
space-time interest point descriptor STIP to take into account the color
information on the features' neighborhood. We compare the performance of our
color-aware version of STIP (which we have called HueSTIP) with the original
one."
"We present a method for nonrigid registration of 2-D geometric shapes. Our
contribution is twofold. First, we extend the classic chamfer-matching energy
to a variational functional. Secondly, we introduce a meshless deformation
model that can handle significant high-curvature deformations. We represent 2-D
shapes implicitly using distance transforms, and registration error is defined
based on the shape contours' mutual distances. In addition, we model global
shape deformation as an approximation blended from local deformation fields
using partition-of-unity. The global deformation field is regularized by
penalizing inconsistencies between local fields. The representation can be made
adaptive to shape's contour, leading to registration that is both flexible and
efficient. Finally, registration is achieved by minimizing a variational
chamfer-energy functional combined with the consistency regularizer. We
demonstrate the effectiveness of our method on a number of experiments."
"Gabor filters play an important role in many application areas for the
enhancement of various types of images and the extraction of Gabor features.
For the purpose of enhancing curved structures in noisy images, we introduce
curved Gabor filters which locally adapt their shape to the direction of flow.
These curved Gabor filters enable the choice of filter parameters which
increase the smoothing power without creating artifacts in the enhanced image.
In this paper, curved Gabor filters are applied to the curved ridge and valley
structure of low-quality fingerprint images. First, we combine two orientation
field estimation methods in order to obtain a more robust estimation for very
noisy images. Next, curved regions are constructed by following the respective
local orientation and they are used for estimating the local ridge frequency.
Lastly, curved Gabor filters are defined based on curved regions and they are
applied for the enhancement of low-quality fingerprint images. Experimental
results on the FVC2004 databases show improvements of this approach in
comparison to state-of-the-art enhancement methods."
"The success of many machine learning and pattern recognition methods relies
heavily upon the identification of an appropriate distance metric on the input
data. It is often beneficial to learn such a metric from the input training
data, instead of using a default one such as the Euclidean distance. In this
work, we propose a boosting-based technique, termed BoostMetric, for learning a
quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance
metric requires enforcing the constraint that the matrix parameter to the
metric remains positive definite. Semidefinite programming is often used to
enforce this constraint, but does not scale well and easy to implement.
BoostMetric is instead based on the observation that any positive semidefinite
matrix can be decomposed into a linear combination of trace-one rank-one
matrices. BoostMetric thus uses rank-one positive semidefinite matrices as weak
learners within an efficient and scalable boosting-based learning process. The
resulting methods are easy to implement, efficient, and can accommodate various
types of constraints. We extend traditional boosting algorithms in that its
weak learner is a positive semidefinite matrix with trace and rank being one
rather than a classifier or regressor. Experiments on various datasets
demonstrate that the proposed algorithms compare favorably to those
state-of-the-art methods in terms of classification accuracy and running time."
This paper has been withdrawn
"We propose a method that combines signals from many brain regions observed in
functional Magnetic Resonance Imaging (fMRI) to predict the subject's behavior
during a scanning session. Such predictions suffer from the huge number of
brain regions sampled on the voxel grid of standard fMRI data sets: the curse
of dimensionality. Dimensionality reduction is thus needed, but it is often
performed using a univariate feature selection procedure, that handles neither
the spatial structure of the images, nor the multivariate nature of the signal.
By introducing a hierarchical clustering of the brain volume that incorporates
connectivity constraints, we reduce the span of the possible spatial
configurations to a single tree of nested regions tailored to the signal. We
then prune the tree in a supervised setting, hence the name supervised
clustering, in order to extract a parcellation (division of the volume) such
that parcel-based signal averages best predict the target information.
Dimensionality reduction is thus achieved by feature agglomeration, and the
constructed features now provide a multi-scale representation of the signal.
Comparisons with reference methods on both simulated and real data show that
our approach yields higher prediction accuracy than standard voxel-based
approaches. Moreover, the method infers an explicit weighting of the regions
involved in the regression or classification task."
"Topological alignments and snakes are used in image processing, particularly
in locating object boundaries. Both of them have their own advantages and
limitations. To improve the overall image boundary detection system, we focused
on developing a novel algorithm for image processing. The algorithm we propose
to develop will based on the active contour method in conjunction with
topological alignments method to enhance the image detection approach. The
algorithm presents novel technique to incorporate the advantages of both
Topological Alignments and snakes. Where the initial segmentation by
Topological Alignments is firstly transformed into the input of the snake model
and begins its evolvement to the interested object boundary. The results show
that the algorithm can deal with low contrast images and shape cells,
demonstrate the segmentation accuracy under weak image boundaries, which
responsible for lacking accuracy in image detecting techniques. We have
achieved better segmentation and boundary detecting for the image, also the
ability of the system to improve the low contrast and deal with over and under
segmentation."
"Detection of geometric features in digital images is an important exercise in
image analysis and computer vision. The Hough Transform techniques for
detection of circles require a huge memory space for data processing hence
requiring a lot of time in computing the locations of the data space, writing
to and searching through the memory space. In this paper we propose a novel and
efficient scheme for detecting circles in edge-detected grayscale digital
images. We use Ant-system algorithm for this purpose which has not yet found
much application in this field. The main feature of this scheme is that it can
detect both intersecting as well as non-intersecting circles with a time
efficiency that makes it useful in real time applications. We build up an ant
system of new type which finds out closed loops in the image and then tests
them for circles."
"This papers introduces a new family of iris encoders which use 2-dimensional
Haar Wavelet Transform for noise attenuation, and Hilbert Transform to encode
the iris texture. In order to prove the usefulness of the newly proposed iris
encoding approach, the recognition results obtained by using these new encoders
are compared to those obtained using the classical Log- Gabor iris encoder.
Twelve tests involving single/multienrollment and conducted on Bath Iris Image
Database are presented here. One of these tests achieves an Equal Error Rate
comparable to the lowest value reported so far for this database. New Matlab
tools for iris image processing are also released together with this paper: a
second version of the Circular Fuzzy Iris Segmentator (CFIS2), a fast Log-Gabor
encoder and two Haar-Hilbert based encoders."
"This paper presents a new algorithm to track mobile objects in different
scene conditions. The main idea of the proposed tracker includes estimation,
multi-features similarity measures and trajectory filtering. A feature set
(distance, area, shape ratio, color histogram) is defined for each tracked
object to search for the best matching object. Its best matching object and its
state estimated by the Kalman filter are combined to update position and size
of the tracked object. However, the mobile object trajectories are usually
fragmented because of occlusions and misdetections. Therefore, we also propose
a trajectory filtering, named global tracker, aims at removing the noisy
trajectories and fusing the fragmented trajectories belonging to a same mobile
object. The method has been tested with five videos of different scene
conditions. Three of them are provided by the ETISEO benchmarking project
(http://www-sop.inria.fr/orion/ETISEO) in which the proposed tracker
performance has been compared with other seven tracking algorithms. The
advantages of our approach over the existing state of the art ones are: (i) no
prior knowledge information is required (e.g. no calibration and no contextual
models are needed), (ii) the tracker is more reliable by combining multiple
feature similarities, (iii) the tracker can perform in different scene
conditions: single/several mobile objects, weak/strong illumination,
indoor/outdoor scenes, (iv) a trajectory filtering is defined and applied to
improve the tracker performance, (v) the tracker performance outperforms many
algorithms of the state of the art."
"This paper presents a comparative study of two different methods, which are
based on fusion and polar transformation of visual and thermal images. Here,
investigation is done to handle the challenges of face recognition, which
include pose variations, changes in facial expression, partial occlusions,
variations in illumination, rotation through different angles, change in scale
etc. To overcome these obstacles we have implemented and thoroughly examined
two different fusion techniques through rigorous experimentation. In the first
method log-polar transformation is applied to the fused images obtained after
fusion of visual and thermal images whereas in second method fusion is applied
on log-polar transformed individual visual and thermal images. After this step,
which is thus obtained in one form or another, Principal Component Analysis
(PCA) is applied to reduce dimension of the fused images. Log-polar transformed
images are capable of handling complicacies introduced by scaling and rotation.
The main objective of employing fusion is to produce a fused image that
provides more detailed and reliable information, which is capable to overcome
the drawbacks present in the individual visual and thermal face images.
Finally, those reduced fused images are classified using a multilayer
perceptron neural network. The database used for the experiments conducted here
is Object Tracking and Classification Beyond Visible Spectrum (OTCBVS) database
benchmark thermal and visual face images. The second method has shown better
performance, which is 95.71% (maximum) and on an average 93.81% as correct
recognition rate."
"This paper demonstrates two different fusion techniques at two different
levels of a human face recognition process. The first one is called data fusion
at lower level and the second one is the decision fusion towards the end of the
recognition process. At first a data fusion is applied on visual and
corresponding thermal images to generate fused image. Data fusion is
implemented in the wavelet domain after decomposing the images through
Daubechies wavelet coefficients (db2). During the data fusion maximum of
approximate and other three details coefficients are merged together. After
that Principle Component Analysis (PCA) is applied over the fused coefficients
and finally two different artificial neural networks namely Multilayer
Perceptron(MLP) and Radial Basis Function(RBF) networks have been used
separately to classify the images. After that, for decision fusion based
decisions from both the classifiers are combined together using Bayesian
formulation. For experiments, IRIS thermal/visible Face Database has been used.
Experimental results show that the performance of multiple classifier system
along with decision fusion works well over the single classifier system."
"In this paper, we present a technique by which high-intensity feature vectors
extracted from the Gabor wavelet transformation of frontal face images, is
combined together with Independent Component Analysis (ICA) for enhanced face
recognition. Firstly, the high-intensity feature vectors are automatically
extracted using the local characteristics of each individual face from the
Gabor transformed images. Then ICA is applied on these locally extracted
high-intensity feature vectors of the facial images to obtain the independent
high intensity feature (IHIF) vectors. These IHIF forms the basis of the work.
Finally, the image classification is done using these IHIF vectors, which are
considered as representatives of the images. The importance behind implementing
ICA along with the high-intensity features of Gabor wavelet transformation is
twofold. On the one hand, selecting peaks of the Gabor transformed face images
exhibit strong characteristics of spatial locality, scale, and orientation
selectivity. Thus these images produce salient local features that are most
suitable for face recognition. On the other hand, as the ICA employs locally
salient features from the high informative facial parts, it reduces redundancy
and represents independent features explicitly. These independent features are
most useful for subsequent facial discrimination and associative recall. The
efficiency of IHIF method is demonstrated by the experiment on frontal facial
images dataset, selected from the FERET, FRAV2D, and the ORL database."
"Forensic applications like criminal investigations, terrorist identification
and National security issues require a strong fingerprint data base and
efficient identification system. In this paper we propose DWT based Fingerprint
Recognition using Non Minutiae (DWTFR) algorithm. Fingerprint image is
decomposed into multi resolution sub bands of LL, LH, HL and HH by applying 3
level DWT. The Dominant local orientation angle {\theta} and Coherence are
computed on LL band only. The Centre Area Features and Edge Parameters are
determined on each DWT level by considering all four sub bands. The comparison
of test fingerprint with database fingerprint is decided based on the Euclidean
Distance of all the features. It is observed that the values of FAR, FRR and
TSR are improved compared to the existing algorithm."
"Editing on digital images is ubiquitous. Identification of deliberately
modified facial images is a new challenge for face identification system. In
this paper, we address the problem of identification of a face or person from
heavily altered facial images. In this face identification problem, the input
to the system is a manipulated or transformed face image and the system reports
back the determined identity from a database of known individuals. Such a
system can be useful in mugshot identification in which mugshot database
contains two views (frontal and profile) of each criminal. We considered only
frontal view from the available database for face identification and the query
image is a manipulated face generated by face transformation software tool
available online. We propose SIFT features for efficient face identification in
this scenario. Further comparative analysis has been given with well known
eigenface approach. Experiments have been conducted with real case images to
evaluate the performance of both methods."
"A line of a bilingual document page may contain text words in regional
language and numerals in English. For Optical Character Recognition (OCR) of
such a document page, it is necessary to identify different script forms before
running an individual OCR system. In this paper, we have identified a tool of
morphological opening by reconstruction of an image in different directions and
regional descriptors for script identification at word level, based on the
observation that every text has a distinct visual appearance. The proposed
system is developed for three Indian major bilingual documents, Kannada, Telugu
and Devnagari containing English numerals. The nearest neighbour and k-nearest
neighbour algorithms are applied to classify new word images. The proposed
algorithm is tested on 2625 words with various font styles and sizes. The
results obtained are quite encouraging"
"Although radiologists can employ CAD systems to characterize malignancies,
pulmonary fibrosis and other chronic diseases; the design of imaging techniques
to quantify infectious diseases continue to lag behind. There exists a need to
create more CAD systems capable of detecting and quantifying characteristic
patterns often seen in respiratory tract infections such as influenza,
bacterial pneumonia, or tuborculosis. One of such patterns is Tree-in-bud (TIB)
which presents \textit{thickened} bronchial structures surrounding by clusters
of \textit{micro-nodules}. Automatic detection of TIB patterns is a challenging
task because of their weak boundary, noisy appearance, and small lesion size.
In this paper, we present two novel methods for automatically detecting TIB
patterns: (1) a fast localization of candidate patterns using information from
local scale of the images, and (2) a M\""{o}bius invariant feature extraction
method based on learned local shape and texture properties. A comparative
evaluation of the proposed methods is presented with a dataset of 39 laboratory
confirmed viral bronchiolitis human parainfluenza (HPIV) CTs and 21 normal lung
CTs. Experimental results demonstrate that the proposed CAD system can achieve
high detection rate with an overall accuracy of 90.96%."
"We present a fully automated method for top-down segmentation of the
pulmonary arterial tree in low-dose thoracic CT images. The main basal
pulmonary arteries are identified near the lung hilum by searching for
candidate vessels adjacent to known airways, identified by our previously
reported airway segmentation method. Model cylinders are iteratively fit to the
vessels to track them into the lungs. Vessel bifurcations are detected by
measuring the rate of change of vessel radii, and child vessels are segmented
by initiating new trackers at bifurcation points. Validation is accomplished
using our novel sparse surface (SS) evaluation metric. The SS metric was
designed to quantify the magnitude of the segmentation error per vessel while
significantly decreasing the manual marking burden for the human user. A total
of 210 arteries and 205 veins were manually marked across seven test cases.
134/210 arteries were correctly segmented, with a specificity for arteries of
90%, and average segmentation error of 0.15 mm. This fully-automated
segmentation is a promising method for improving lung nodule detection in
low-dose CT screening scans, by separating vessels from surrounding
iso-intensity objects."
"Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library."
"Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications."
"The performance of the Fingerprint recognition system will be more accurate
with respect of enhancement for the fingerprint images. In this paper we
develop a novel method for Fingerprint image contrast enhancement technique
based on the discrete wavelet transform (DWT) and singular value decomposition
(SVD) has been proposed. This technique is compared with conventional image
equalization techniques such as standard general histogram equalization and
local histogram equalization. An automatic histogram threshold approach based
on a fuzziness measure is presented. Then, using an index of fuzziness, a
similarity process is started to find the threshold point. A significant
contrast between ridges and valleys of the best, medium and poor finger image
features to extract from finger images and get maximum recognition rate using
fuzzy measures. The experimental results show the recognition of superiority of
the proposed method to get maximum performance up gradation to the
implementation of this approach."
"In this paper we propose a measure of anisotropy as a quality parameter to
estimate the amount of noise in noisy images. The anisotropy of an image can be
determined through a directional measure, using an appropriate statistical
distribution of the information contained in the image. This new measure is
achieved through a stack filtering paradigm. First, we define a local
directional entropy, based on the distribution of 0's and 1's in the
neigborhood of every pixel location of each stack level. Then the entropy
variation of this directional entropy is used to define an anisotropic measure.
The empirical results have shown that this measure can be regarded as an
excellent image noise indicator, which is particularly relevant for quality
assessment of denoising algorithms. The method has been evaluated with
artificial and real-world degraded images."
"We employ the face recognition technology developed in house at face.com to a
well accepted benchmark and show that without any tuning we are able to
considerably surpass state of the art results. Much of the improvement is
concentrated in the high-valued performance point of zero false positive
matches, where the obtained recall rate almost doubles the best reported result
to date. We discuss the various components and innovations of our system that
enable this significant performance gap. These components include extensive
utilization of an accurate 3D reconstructed shape model dealing with challenges
arising from pose and illumination. In addition, discriminative models based on
billions of faces are used in order to overcome aging and facial expression as
well as low light and overexposure. Finally, we identify a challenging set of
identification queries that might provide useful focus for future research."
"We give an algorithm that learns a representation of data through
compression. The algorithm 1) predicts bits sequentially from those previously
seen and 2) has a structure and a number of computations similar to an
autoencoder. The likelihood under the model can be calculated exactly, and
arithmetic coding can be used directly for compression. When training on digits
the algorithm learns filters similar to those of restricted boltzman machines
and denoising autoencoders. Independent samples can be drawn from the model by
a single sweep through the pixels. The algorithm has a good compression
performance when compared to other methods that work under random ordering of
pixels."
"This paper presents an automated system for human face recognition in a real
time background world for a large homemade dataset of persons face. The task is
very difficult as the real time background subtraction in an image is still a
challenge. Addition to this there is a huge variation in human face image in
terms of size, pose and expression. The system proposed collapses most of this
variance. To detect real time human face AdaBoost with Haar cascade is used and
a simple fast PCA and LDA is used to recognize the faces detected. The matched
face is then used to mark attendance in the laboratory, in our case. This
biometric system is a real time attendance system based on the human face
recognition with a simple and fast algorithms and gaining a high accuracy
rate.."
"We propose a novel algorithm for compressive imaging that exploits both the
sparsity and persistence across scales found in the 2D wavelet transform
coefficients of natural images. Like other recent works, we model wavelet
structure using a hidden Markov tree (HMT) but, unlike other works, ours is
based on loopy belief propagation (LBP). For LBP, we adopt a recently proposed
""turbo"" message passing schedule that alternates between exploitation of HMT
structure and exploitation of compressive-measurement structure. For the
latter, we leverage Donoho, Maleki, and Montanari's recently proposed
approximate message passing (AMP) algorithm. Experiments with a large image
database suggest that, relative to existing schemes, our turbo LBP approach
yields state-of-the-art reconstruction performance with substantial reduction
in complexity."
"There are many image fusion methods that can be used to produce
high-resolution mutlispectral images from a high-resolution panchromatic (PAN)
image and low-resolution multispectral (MS) of remote sensed images. This paper
attempts to undertake the study of image fusion techniques with different
Statistical techniques for image fusion as Local Mean Matching (LMM), Local
Mean and Variance Matching (LMVM), Regression variable substitution (RVS),
Local Correlation Modeling (LCM) and they are compared with one another so as
to choose the best technique, that can be applied on multi-resolution satellite
images. This paper also devotes to concentrate on the analytical techniques for
evaluating the quality of image fusion (F) by using various methods including
Standard Deviation (SD), Entropy(En), Correlation Coefficient (CC), Signal-to
Noise Ratio (SNR), Normalization Root Mean Square Error (NRMSE) and Deviation
Index (DI) to estimate the quality and degree of information improvement of a
fused image quantitatively."
"Sparse modeling is one of the efficient techniques for imaging that allows
recovering lost information. In this paper, we present a novel iterative
phase-retrieval algorithm using a sparse representation of the object amplitude
and phase. The algorithm is derived in terms of a constrained maximum
likelihood, where the wave field reconstruction is performed using a number of
noisy intensity-only observations with a zero-mean additive Gaussian noise. The
developed algorithm enables the optimal solution for the object wave field
reconstruction. Our goal is an improvement of the reconstruction quality with
respect to the conventional algorithms. Sparse regularization results in
advanced reconstruction accuracy, and numerical simulations demonstrate
significant enhancement of imaging."
"Object parsing and segmentation from point clouds are challenging tasks
because the relevant data is available only as thin structures along object
boundaries or other features, and is corrupted by large amounts of noise. To
handle this kind of data, flexible shape models are desired that can accurately
follow the object boundaries. Popular models such as Active Shape and Active
Appearance models lack the necessary flexibility for this task, while recent
approaches such as the Recursive Compositional Models make model
simplifications in order to obtain computational guarantees. This paper
investigates a hierarchical Bayesian model of shape and appearance in a
generative setting. The input data is explained by an object parsing layer,
which is a deformation of a hidden PCA shape model with Gaussian prior. The
paper also introduces a novel efficient inference algorithm that uses informed
data-driven proposals to initialize local searches for the hidden variables.
Applied to the problem of object parsing from structured point clouds such as
edge detection images, the proposed approach obtains state of the art parsing
errors on two standard datasets without using any intensity information."
"Until now, of highest relevance for remote sensing data processing and
analysis have been techniques for pixel level image fusion. So, This paper
attempts to undertake the study of Feature-Level based image fusion. For this
purpose, feature based fusion techniques, which are usually based on empirical
or heuristic rules, are employed. Hence, in this paper we consider feature
extraction (FE) for fusion. It aims at finding a transformation of the original
space that would produce such new features, which preserve or improve as much
as possible. This study introduces three different types of Image fusion
techniques including Principal Component Analysis based Feature Fusion (PCA),
Segment Fusion (SF) and Edge fusion (EF). This paper also devotes to
concentrate on the analytical techniques for evaluating the quality of image
fusion (F) by using various methods including (SD), (En), (CC), (SNR), (NRMSE)
and (DI) to estimate the quality and degree of information improvement of a
fused image quantitatively."
"Detecting the edges of objects within images is critical for quality image
processing. We present an edge-detecting technique that uses morphological
amoebas that adjust their shape based on variation in image contours. We
evaluate the method both quantitatively and qualitatively for edge detection of
images, and compare it to classic morphological methods. Our amoeba-based
edge-detection system performed better than the classic edge detectors."
"Human gait, which is a new biometric aimed to recognize individuals by the
way they walk have come to play an increasingly important role in visual
surveillance applications. In this paper a novel hybrid holistic approach is
proposed to show how behavioural walking characteristics can be used to
recognize unauthorized and suspicious persons when they enter a surveillance
area. Initially background is modelled from the input video captured from
cameras deployed for security and the foreground moving object in the
individual frames are segmented using the background subtraction algorithm.
Then gait representing spatial, temporal and wavelet components are extracted
and fused for training and testing multi class support vector machine models
(SVM). The proposed system is evaluated using side view videos of NLPR
database. The experimental results demonstrate that the proposed system
achieves a pleasing recognition rate and also the results indicate that the
classification ability of SVM with Radial Basis Function (RBF) is better than
with other kernel functions."
"In VQ based image compression technique has three major steps namely (i)
Codebook Design, (ii) VQ Encoding Process and (iii) VQ Decoding Process. The
performance of VQ based image compression technique depends upon the
constructed codebook. A widely used technique for VQ codebook design is the
Linde-Buzo-Gray (LBG) algorithm. However the performance of the standard LBG
algorithm is highly dependent on the choice of the initial codebook. In this
paper, we have proposed a simple and very effective approach for codebook
initialization for LBG algorithm. The simulation results show that the proposed
scheme is computationally efficient and gives expected performance as compared
to the standard LBG algorithm."
"Breast cancer is considered as one of a major health problem that constitutes
the strongest cause behind mortality among women in the world. So, in this
decade, breast cancer is the second most common type of cancer, in term of
appearance frequency, and the fifth most common cause of cancer related death.
In order to reduce the workload on radiologists, a variety of CAD systems;
Computer-Aided Diagnosis (CADi) and Computer-Aided Detection (CADe) have been
proposed. In this paper, we interested on CADe tool to help radiologist to
detect cancer. The proposed CADe is based on a three-step work flow; namely,
detection, analysis and classification. This paper deals with the problem of
automatic detection of Region Of Interest (ROI) based on Level Set approach
depended on edge and region criteria. This approach gives good visual
information from the radiologist. After that, the features extraction using
textures characteristics and the vector classification using Multilayer
Perception (MLP) and k-Nearest Neighbours (KNN) are adopted to distinguish
different ACR (American College of Radiology) classification. Moreover, we use
the Digital Database for Screening Mammography (DDSM) for experiments and these
results in term of accuracy varied between 60 % and 70% are acceptable and must
be ameliorated to aid radiologist."
"Object detection is a fundamental step for automated video analysis in many
vision applications. Object detection in a video is usually performed by object
detectors or background subtraction techniques. Often, an object detector
requires manually labeled examples to train a binary classifier, while
background subtraction needs a training sequence that contains no objects to
build a background model. To automate the analysis, object detection without a
separate training phase becomes a critical task. People have tried to tackle
this task by using motion information. But existing motion-based methods are
usually limited when coping with complex scenarios such as nonrigid motion and
dynamic background. In this paper, we show that above challenges can be
addressed in a unified framework named DEtecting Contiguous Outliers in the
LOw-rank Representation (DECOLOR). This formulation integrates object detection
and background learning into a single process of optimization, which can be
solved by an alternating algorithm efficiently. We explain the relations
between DECOLOR and other sparsity-based methods. Experiments on both simulated
data and real sequences demonstrate that DECOLOR outperforms the
state-of-the-art approaches and it can work effectively on a wide range of
complex scenarios."
"Many computer vision and image processing problems can be posed as solving
partial differential equations (PDEs). However, designing PDE system usually
requires high mathematical skills and good insight into the problems. In this
paper, we consider designing PDEs for various problems arising in computer
vision and image processing in a lazy manner: \emph{learning PDEs from real
data via data-based optimal control}. We first propose a general intelligent
PDE system which holds the basic translational and rotational invariance rule
for most vision problems. By introducing a PDE-constrained optimal control
framework, it is possible to use the training data resulting from multiple ways
(ground truth, results from other methods, and manual results from humans) to
learn PDEs for different computer vision tasks. The proposed optimal control
based training framework aims at learning a PDE-based regressor to approximate
the unknown (and usually nonlinear) mapping of different vision tasks. The
experimental results show that the learnt PDEs can solve different vision
problems reasonably well. In particular, we can obtain PDEs not only for
problems that traditional PDEs work well but also for problems that PDE-based
methods have never been tried before, due to the difficulty in describing those
problems in a mathematical way."
"The research work presented in this paper is to achieve the tissue
classification and automatically diagnosis the abnormal tumor region present in
Computed Tomography (CT) images using the wavelet based statistical texture
analysis method. Comparative studies of texture analysis method are performed
for the proposed wavelet based texture analysis method and Spatial Gray Level
Dependence Method (SGLDM). Our proposed system consists of four phases i)
Discrete Wavelet Decomposition (ii) Feature extraction (iii) Feature selection
(iv) Analysis of extracted texture features by classifier. A wavelet based
statistical texture feature set is derived from normal and tumor regions.
Genetic Algorithm (GA) is used to select the optimal texture features from the
set of extracted texture features. We construct the Support Vector Machine
(SVM) based classifier and evaluate the performance of classifier by comparing
the classification results of the SVM based classifier with the Back
Propagation Neural network classifier(BPN). The results of Support Vector
Machine (SVM), BPN classifiers for the texture analysis methods are evaluated
using Receiver Operating Characteristic (ROC) analysis. Experimental results
show that the classification accuracy of SVM is 96% for 10 fold cross
validation method. The system has been tested with a number of real Computed
Tomography brain images and has achieved satisfactory results."
"This paper proposes a simple, automatic and efficient clustering algorithm,
namely, Automatic Merging for Optimal Clusters (AMOC) which aims to generate
nearly optimal clusters for the given datasets automatically. The AMOC is an
extension to standard k-means with a two phase iterative procedure combining
certain validation techniques in order to find optimal clusters with automation
of merging of clusters. Experiments on both synthetic and real data have proved
that the proposed algorithm finds nearly optimal clustering structures in terms
of number of clusters, compactness and separation."
"Document segmentation is one of the critical phases in machine recognition of
any language. Correct segmentation of individual symbols decides the accuracy
of character recognition technique. It is used to decompose image of a sequence
of characters into sub images of individual symbols by segmenting lines and
words. Devnagari is the most popular script in India. It is used for writing
Hindi, Marathi, Sanskrit and Nepali languages. Moreover, Hindi is the third
most popular language in the world. Devnagari documents consist of vowels,
consonants and various modifiers. Hence proper segmentation of Devnagari word
is challenging. A simple histogram based approach to segment Devnagari
documents is proposed in this paper. Various challenges in segmentation of
Devnagari script are also discussed."
"Most image labeling problems such as segmentation and image reconstruction
are fundamentally ill-posed and suffer from ambiguities and noise. Higher order
image priors encode high level structural dependencies between pixels and are
key to overcoming these problems. However, these priors in general lead to
computationally intractable models. This paper addresses the problem of
discovering compact representations of higher order priors which allow
efficient inference. We propose a framework for solving this problem which uses
a recently proposed representation of higher order functions where they are
encoded as lower envelopes of linear functions. Maximum a Posterior inference
on our learned models reduces to minimizing a pairwise function of discrete
variables, which can be done approximately using standard methods. Although
this is a primarily theoretical paper, we also demonstrate the practical
effectiveness of our framework on the problem of learning a shape prior for
image segmentation and reconstruction. We show that our framework can learn a
compact representation that approximates a prior that encourages low curvature
shapes. We evaluate the approximation accuracy, discuss properties of the
trained model, and show various results for shape inpainting and image
segmentation."
"Computational photography involves sophisticated capture methods. A new trend
is to capture projection of higher dimensional visual signals such as videos,
multi-spectral data and lightfields on lower dimensional sensors. Carefully
designed capture methods exploit the sparsity of the underlying signal in a
transformed domain to reduce the number of measurements and use an appropriate
reconstruction method. Traditional progressive methods may capture successively
more detail using a sequence of simple projection basis, such as DCT or
wavelets and employ straightforward backprojection for reconstruction.
Randomized projection methods do not use any specific sequence and use L0
minimization for reconstruction. In this paper, we analyze the statistical
properties of natural images, videos, multi-spectral data and light-fields and
compare the effectiveness of progressive and random projections. We define
effectiveness by plotting reconstruction SNR against compression factor. The
key idea is a procedure to measure best-case effectiveness that is fast,
independent of specific hardware and independent of the reconstruction
procedure. We believe this is the first empirical study to compare different
lossy capture strategies without the complication of hardware or reconstruction
ambiguity. The scope is limited to linear non-adaptive sensing. The results
show that random projections produce significant advantages over other
projections only for higher dimensional signals, and suggest more research to
nascent adaptive and non-linear projection methods."
"We present an approach for the joint segmentation and grouping of similar
components in anisotropic 3D image data and use it to segment neural tissue in
serial sections electron microscopy (EM) images.
  We first construct a nested set of neuron segmentation hypotheses for each
slice. A conditional random field (CRF) then allows us to evaluate both the
compatibility of a specific segmentation and a specific inter-slice assignment
of neuron candidates with the underlying observations. The model is solved
optimally for an entire image stack simultaneously using integer linear
programming (ILP), which yields the maximum a posteriori solution in amortized
linear time in the number of slices.
  We evaluate the performance of our approach on an annotated sample of the
Drosophila larva neuropil and show that the consideration of different
segmentation hypotheses in each slice leads to a significant improvement in the
segmentation and assignment accuracy."
"We give a non-iterative solution to a particular case of the four-point
three-views pose problem when three camera centers are collinear. Using the
well-known Cayley representation of orthogonal matrices, we derive from the
epipolar constraints a system of three polynomial equations in three variables.
The eliminant of that system is a multiple of a 36th degree univariate
polynomial. The true (unique) solution to the problem can be expressed in terms
of one of real roots of that polynomial. Experiments on synthetic data confirm
that our method is robust enough even in case of planar configurations."
"This paper presents a complete Optical Character Recognition (OCR) system for
camera captured image/graphics embedded textual documents for handheld devices.
At first, text regions are extracted and skew corrected. Then, these regions
are binarized and segmented into lines and characters. Characters are passed
into the recognition module. Experimenting with a set of 100 business card
images, captured by cell phone camera, we have achieved a maximum recognition
accuracy of 92.74%. Compared to Tesseract, an open source desktop-based
powerful OCR engine, present recognition accuracy is worth contributing.
Moreover, the developed technique is computationally efficient and consumes low
memory so as to be applicable on handheld devices."
"Global voting schemes based on the Hough transform (HT) have been widely used
to robustly detect lines in images. However, since the votes do not take line
connectivity into account, these methods do not deal well with cluttered
images. In opposition, the so-called local methods enforce connectivity but
lack robustness to deal with challenging situations that occur in many
realistic scenarios, e.g., when line segments cross or when long segments are
corrupted. In this paper, we address the critical limitations of the HT as a
line segment extractor by incorporating connectivity in the voting process.
This is done by only accounting for the contributions of edge points lying in
increasingly larger neighborhoods and whose position and directional content
agree with potential line segments. As a result, our method, which we call
STRAIGHT (Segment exTRAction by connectivity-enforcInG HT), extracts the
longest connected segments in each location of the image, thus also integrating
into the HT voting process the usually separate step of individual segment
extraction. The usage of the Hough space mapping and a corresponding
hierarchical implementation make our approach computationally feasible. We
present experiments that illustrate, with synthetic and real images, how
STRAIGHT succeeds in extracting complete segments in several situations where
current methods fail."
"Matlab version 7.1 had been used to detect playing cards on a Casino table
and the suits and ranks of these cards had been identified. The process gives
an example of an application of computer vision to a problem where rectangular
objects are to be detected and the information content of the objects are
extracted out. In the case of playing cards, it is the suit and rank of each
card. The image processing system is done in two passes. Pass 1 detects
rectangular shapes and template matched with a template of the left and right
edges of the cards. Pass 2 extracts the suit and rank of the cards by matching
the top left portion of the card that contains both rank and suit information,
with stored templates of ranks and suits of the playing cards using a series of
if-then statements."
"In this article we study the digital homology groups of digital images which
are based on the singular homology groups of topological spaces in algebraic
topology. Specifically, we define a digitally standard $n$-simplex, a digitally
singular $n$-simplex, and the digital homology groups of digital images with
$k$-adjacency relations. We then construct a covariant functor from a category
of digital images and digitally continuous functions to the one of abelian
groups and group homomorphisms, and investigate some fundamental and
interesting properties of digital homology groups of digital images, such as
the digital version of the dimension axiom which is one of the
Eilenberg-Steenrod axioms."
"We describe an approach for segmenting an image into regions that correspond
to surfaces in the scene that are partially surrounded by the medium. It
integrates both appearance and motion statistics into a cost functional, that
is seeded with occluded regions and minimized efficiently by solving a linear
programming problem. Where a short observation time is insufficient to
determine whether the object is detachable, the results of the minimization can
be used to seed a more costly optimization based on a longer sequence of video
data. The result is an entirely unsupervised scheme to detect and segment an
arbitrary and unknown number of objects. We test our scheme to highlight the
potential, as well as limitations, of our approach."
"This contribution proposes a new approach towards developing a class of
probabilistic methods for classifying attributed graphs. The key concept is
random attributed graph, which is defined as an attributed graph whose nodes
and edges are annotated by random variables. Every node/edge has two random
processes associated with it- occurence probability and the probability
distribution over the attribute values. These are estimated within the maximum
likelihood framework. The likelihood of a random attributed graph to generate
an outcome graph is used as a feature for classification. The proposed approach
is fast and robust to noise."
"Motivated by an emerging theory of robust low-rank matrix representation, in
this paper, we introduce a novel solution for online rigid-body motion
registration. The goal is to develop algorithmic techniques that enable a
robust, real-time motion registration solution suitable for low-cost, portable
3-D camera devices. Assuming 3-D image features are tracked via a standard
tracker, the algorithm first utilizes Robust PCA to initialize a low-rank shape
representation of the rigid body. Robust PCA finds the global optimal solution
of the initialization, while its complexity is comparable to singular value
decomposition. In the online update stage, we propose a more efficient
algorithm for sparse subspace projection to sequentially project new feature
observations onto the shape subspace. The lightweight update stage guarantees
the real-time performance of the solution while maintaining good registration
even when the image sequence is contaminated by noise, gross data corruption,
outlying features, and missing data. The state-of-the-art accuracy of the
solution is validated through extensive simulation and a real-world experiment,
while the system enjoys one to two orders of magnitude speed-up compared to
well-established RANSAC solutions. The new algorithm will be released online to
aid peer evaluation."
"It is well-known that box filters can be efficiently computed using
pre-integrations and local finite-differences
[Crow1984,Heckbert1986,Viola2001]. By generalizing this idea and by combining
it with a non-standard variant of the Central Limit Theorem, a constant-time or
O(1) algorithm was proposed in [Chaudhury2010] that allowed one to perform
space-variant filtering using Gaussian-like kernels. The algorithm was based on
the observation that both isotropic and anisotropic Gaussians could be
approximated using certain bivariate splines called box splines. The attractive
feature of the algorithm was that it allowed one to continuously control the
shape and size (covariance) of the filter, and that it had a fixed
computational cost per pixel, irrespective of the size of the filter. The
algorithm, however, offered a limited control on the covariance and accuracy of
the Gaussian approximation. In this work, we propose some improvements by
appropriately modifying the algorithm in [Chaudhury2010]."
"We propose a Bayesian image super-resolution (SR) method with a causal
Gaussian Markov random field (MRF) prior. SR is a technique to estimate a
spatially high-resolution image from given multiple low-resolution images. An
MRF model with the line process supplies a preferable prior for natural images
with edges. We improve the existing image transformation model, the compound
MRF model, and its hyperparameter prior model. We also derive the optimal
estimator -- not the joint maximum a posteriori (MAP) or marginalized maximum
likelihood (ML), but the posterior mean (PM) -- from the objective function of
the L2-norm (mean square error) -based peak signal-to-noise ratio (PSNR). Point
estimates such as MAP and ML are generally not stable in ill-posed
high-dimensional problems because of overfitting, while PM is a stable
estimator because all the parameters in the model are evaluated as
distributions. The estimator is numerically determined by using variational
Bayes. Variational Bayes is a widely used method that approximately determines
a complicated posterior distribution, but it is generally hard to use because
it needs the conjugate prior. We solve this problem with simple Taylor
approximations. Experimental results have shown that the proposed method is
more accurate or comparable to existing methods."
"This paper describes an intelligent system ABHIVYAKTI, which would be
pervasive in nature and based on the Computer Vision. It would be very easy in
use and deployment. Elder and sick people who are not able to talk or walk,
they are dependent on other human beings and need continuous monitoring, while
our system provides flexibility to the sick or elder person to announce his or
her need to their caretaker by just showing a particular gesture with the
developed system, if the caretaker is not nearby. This system will use
fingertip detection techniques for acquiring gesture and Artificial Neural
Networks (ANNs) will be used for gesture recognition."
"This article describes a comprehensive system for surveillance and monitoring
applications. The development of an efficient real time video motion detection
system is motivated by their potential for deployment in the areas where
security is the main concern. The paper presents a platform for real time video
motion detection and subsequent generation of an alarm condition as set by the
parameters of the control system. The prototype consists of a mobile platform
mounted with RF camera which provides continuous feedback of the environment.
The received visual information is then analyzed by user for appropriate
control action, thus enabling the user to operate the system from a remote
location. The system is also equipped with the ability to process the image of
an object and generate control signals which are automatically transmitted to
the mobile platform to track the object."
"Recently, the face recognizers based on linear representations have been
shown to deliver state-of-the-art performance. In real-world applications,
however, face images usually suffer from expressions, disguises and random
occlusions. The problematic facial parts undermine the validity of the
linear-subspace assumption and thus the recognition performance deteriorates
significantly. In this work, we address the problem in a
learning-inference-mixed fashion. By observing that the linear-subspace
assumption is more reliable on certain face patches rather than on the holistic
face, some Bayesian Patch Representations (BPRs) are randomly generated and
interpreted according to the Bayes' theory. We then train an ensemble model
over the patch-representations by minimizing the empirical risk w.r.t the
""leave-one-out margins"". The obtained model is termed Optimal Representation
Ensemble (ORE), since it guarantees the optimality from the perspective of
Empirical Risk Minimization. To handle the unknown patterns in test faces, a
robust version of BPR is proposed by taking the non-face category into
consideration. Equipped with the Robust-BPRs, the inference ability of ORE is
increased dramatically and several record-breaking accuracies (99.9% on Yale-B
and 99.5% on AR) and desirable efficiencies (below 20 ms per face in Matlab)
are achieved. It also overwhelms other modular heuristics on the faces with
random occlusions, extreme expressions and disguises. Furthermore, to
accommodate immense BPRs sets, a boosting-like algorithm is also derived. The
boosted model, a.k.a Boosted-ORE, obtains similar performance to its prototype.
Besides the empirical superiorities, two desirable features of the proposed
methods, namely, the training-determined model-selection and the
data-weight-free boosting procedure, are also theoretically verified."
"In machine learning and computer vision, input images are often filtered to
increase data discriminability. In some situations, however, one may wish to
purposely decrease discriminability of one classification task (a ""distractor""
task), while simultaneously preserving information relevant to another (the
task-of-interest): For example, it may be important to mask the identity of
persons contained in face images before submitting them to a crowdsourcing site
(e.g., Mechanical Turk) when labeling them for certain facial attributes.
Another example is inter-dataset generalization: when training on a dataset
with a particular covariance structure among multiple attributes, it may be
useful to suppress one attribute while preserving another so that a trained
classifier does not learn spurious correlations between attributes. In this
paper we present an algorithm that finds optimal filters to give high
discriminability to one task while simultaneously giving low discriminability
to a distractor task. We present results showing the effectiveness of the
proposed technique on both simulated data and natural face images."
"Construction of a scale space with a convolution filter has been studied
extensively in the past. It has been proven that the only convolution kernel
that satisfies the scale space requirements is a Gaussian type. In this paper,
we consider a matrix of convolution filters introduced in [1] as a building
kernel for a scale space, and shows that we can construct a non-Gaussian scale
space with a $2\times 2$ matrix of filters. The paper derives sufficient
conditions for the matrix of filters for being a scale space kernel, and
present some numerical demonstrations."
"In this paper, a multi-resolution feature extraction algorithm for face
recognition is proposed based on two-dimensional discrete wavelet transform
(2D-DWT), which efficiently exploits the local spatial variations in a face
image. For the purpose of feature extraction, instead of considering the entire
face image, an entropy-based local band selection criterion is developed, which
selects high-informative horizontal segments from the face image. In order to
capture the local spatial variations within these highinformative horizontal
bands precisely, the horizontal band is segmented into several small spatial
modules. Dominant wavelet coefficients corresponding to each local region
residing inside those horizontal bands are selected as features. In the
selection of the dominant coefficients, a threshold criterion is proposed,
which not only drastically reduces the feature dimension but also provides high
within-class compactness and high between-class separability. A principal
component analysis is performed to further reduce the dimensionality of the
feature space. Extensive experimentation is carried out upon standard face
databases and a very high degree of recognition accuracy is achieved by the
proposed method in comparison to those obtained by some of the existing
methods."
"Shape is an important aspects in recognizing plants. Several approaches have
been introduced to identify objects, including plants. Combination of geometric
features such as aspect ratio, compactness, and dispersion, or moments such as
moment invariants were usually used toidentify plants. In this research, a
comparative experiment of 4 methods to identify plants using shape features was
accomplished. Two approaches have never been used in plants identification yet,
Zernike moments and Polar Fourier Transform (PFT), were incorporated. The
experimental comparison was done on 52 kinds of plants with various shapes. The
result, PFT gave best performance with 64% in accuracy and outperformed the
other methods."
"This paper proposed a method that combines Polar Fourier Transform, color
moments, and vein features to retrieve leaf images based on a leaf image. The
method is very useful to help people in recognizing foliage plants. Foliage
plants are plants that have various colors and unique patterns in the leaf.
Therefore, the colors and its patterns are information that should be counted
on in the processing of plant identification. To compare the performance of
retrieving system to other result, the experiments used Flavia dataset, which
is very popular in recognizing plants. The result shows that the method gave
better performance than PNN, SVM, and Fourier Transform. The method was also
tested using foliage plants with various colors. The accuracy was 90.80% for 50
kinds of plants."
"This manuscript describes the elements of a theory of information tailored to
control and decision tasks and specifically to visual data. The concept of
Actionable Information is described, that relates to a notion of information
championed by J. Gibson, and a notion of ""complete information"" that relates to
the minimal sufficient statistics of a complete representation. It is shown
that the ""actionable information gap"" between the two can be reduced by
exercising control on the sensing process. Thus, senging, control and
information are inextricably tied. This has consequences in the so-called
""signal-to-symbol barrier"" problem, as well as in the analysis and design of
active sensing systems. It has ramifications in vision-based control,
navigation, 3-D reconstruction and rendering, as well as detection,
localization, recognition and categorization of objects and scenes in live
video.
  This manuscript has been developed from a set of lecture notes for a summer
course at the First International Computer Vision Summer School (ICVSS) in
Scicli, Italy, in July of 2008. They were later expanded and amended for
subsequent lectures in the same School in July 2009. Starting on November 1,
2009, they were further expanded for a special topics course, CS269, taught at
UCLA in the Spring term of 2010."
"In this paper we present a general, flexible framework for learning mappings
from images to actions by interacting with the environment. The basic idea is
to introduce a feature-based image classifier in front of a reinforcement
learning algorithm. The classifier partitions the visual space according to the
presence or absence of few highly informative local descriptors that are
incrementally selected in a sequence of attempts to remove perceptual aliasing.
We also address the problem of fighting overfitting in such a greedy algorithm.
Finally, we show how high-level visual features can be generated when the power
of local descriptors is insufficient for completely disambiguating the aliased
states. This is done by building a hierarchy of composite features that consist
of recursive spatial combinations of visual features. We demonstrate the
efficacy of our algorithms by solving three visual navigation tasks and a
visual version of the classical Car on the Hill control problem."
"This paper provides a new algorithm for solving inverse problems, based on
the minimization of the $L^2$ norm and on the control of the Total Variation.
It consists in relaxing the role of the Total Variation in the classical Total
Variation minimization approach, which permits us to get better approximation
to the inverse problems. The numerical results on the deconvolution problem
show that our method outperforms some previous ones."
"Various and different methods can be used to produce high-resolution
multispectral images from high-resolution panchromatic image (PAN) and
low-resolution multispectral images (MS), mostly on the pixel level. However,
the jury is still out on the benefits of a fused image compared to its original
images. There is also a lack of measures for assessing the objective quality of
the spatial resolution for the fusion methods. Therefore, an objective quality
of the spatial resolution assessment for fusion images is required. So, this
study attempts to develop a new qualitative assessment to evaluate the spatial
quality of the pan sharpened images by many spatial quality metrics. Also, this
paper deals with a comparison of various image fusion techniques based on pixel
and feature fusion techniques."
"The paper will present a novel approach for solving face recognition problem.
Our method combines 2D Principal Component Analysis (2DPCA), one of the
prominent methods for extracting feature vectors, and Support Vector Machine
(SVM), the most powerful discriminative method for classification. Experiments
based on proposed method have been conducted on two public data sets FERET and
AT&T; the results show that the proposed method could improve the
classification rates."
"Fast and robust hand segmentation and tracking is an essential basis for
gesture recognition and thus an important component for contact-less
human-computer interaction (HCI). Hand gesture recognition based on 2D video
data has been intensively investigated. However, in practical scenarios purely
intensity based approaches suffer from uncontrollable environmental conditions
like cluttered background colors. In this paper we present a real-time hand
segmentation and tracking algorithm using Time-of-Flight (ToF) range cameras
and intensity data. The intensity and range information is fused into one pixel
value, representing its combined intensity-depth homogeneity. The scene is
hierarchically clustered using a GPU based parallel merging algorithm, allowing
a robust identification of both hands even for inhomogeneous backgrounds. After
the detection, both hands are tracked on the CPU. Our tracking algorithm can
cope with the situation that one hand is temporarily covered by the other hand."
"The acquisition of MRI images offers a trade-off in terms of acquisition
time, spatial/temporal resolution and signal-to-noise ratio (SNR). Thus, for
instance, increasing the time efficiency of MRI often comes at the expense of
reduced SNR. This, in turn, necessitates the use of post-processing tools for
noise rejection, which makes image de-noising an indispensable component of
computer assistance diagnosis. In the field of MRI, a multitude of image
de-noising methods have been proposed hitherto. In this paper, the application
of a particular class of de-noising algorithms - known as non-local mean (NLM)
filters - is investigated. Such filters have been recently applied for MRI data
enhancement and they have been shown to provide more accurate results as
compared to many alternative de-noising algorithms. Unfortunately, virtually
all existing methods for NLM filtering have been derived under the assumption
of additive white Gaussian (AWG) noise contamination. Since this assumption is
known to fail at low values of SNR, an alternative formulation of NLM filtering
is required, which would take into consideration the correct Rician statistics
of MRI noise. Accordingly, the contribution of the present paper is two-fold.
First, it points out some principal disadvantages of the earlier methods of NLM
filtering of MRI images and suggests means to rectify them. Second, the paper
introduces a new similarity measure for NLM filtering of MRI Images, which is
derived under bona fide statistical assumptions and results in more accurate
reconstruction of MR scans as compared to alternative NLM approaches. Finally,
the utility and viability of the proposed method is demonstrated through a
series of numerical experiments using both in silico and in vivo MRI data."
"We present Local Naive Bayes Nearest Neighbor, an improvement to the NBNN
image classification algorithm that increases classification accuracy and
improves its ability to scale to large numbers of object classes. The key
observation is that only the classes represented in the local neighborhood of a
descriptor contribute significantly and reliably to their posterior probability
estimates. Instead of maintaining a separate search structure for each class,
we merge all of the reference data together into one search structure, allowing
quick identification of a descriptor's local neighborhood. We show an increase
in classification accuracy when we ignore adjustments to the more distant
classes and show that the run time grows with the log of the number of classes
rather than linearly in the number of classes as did the original. This gives a
100 times speed-up over the original method on the Caltech 256 dataset. We also
provide the first head-to-head comparison of NBNN against spatial pyramid
methods using a common set of input features. We show that local NBNN
outperforms all previous NBNN based methods and the original spatial pyramid
model. However, we find that local NBNN, while competitive with, does not beat
state-of-the-art spatial pyramid methods that use local soft assignment and
max-pooling."
"In this paper we present a biorealistic model for the first part of the early
vision processing by incorporating memristive nanodevices. The architecture of
the proposed network is based on the organisation and functioning of the outer
plexiform layer (OPL) in the vertebrate retina. We demonstrate that memristive
devices are indeed a valuable building block for neuromorphic architectures, as
their highly non-linear and adaptive response could be exploited for
establishing ultra-dense networks with similar dynamics to their biological
counterparts. We particularly show that hexagonal memristive grids can be
employed for faithfully emulating the smoothing-effect occurring at the OPL for
enhancing the dynamic range of the system. In addition, we employ a
memristor-based thresholding scheme for detecting the edges of grayscale
images, while the proposed system is also evaluated for its adaptation and
fault tolerance capacity against different light or noise conditions as well as
distinct device yields."
"This paper introduces a statistical method to decide whether two blocks in a
pair of of images match reliably. The method ensures that the selected block
matches are unlikely to have occurred ""just by chance."" The new approach is
based on the definition of a simple but faithful statistical ""background model""
for image blocks learned from the image itself. A theorem guarantees that under
this model not more than a fixed number of wrong matches occurs (on average)
for the whole image. This fixed number (the number of false alarms) is the only
method parameter. Furthermore, the number of false alarms associated with each
match measures its reliability. This ""a contrario"" block-matching method,
however, cannot rule out false matches due to the presence of periodic objects
in the images. But it is successfully complemented by a parameterless
""self-similarity threshold."" Experimental evidence shows that the proposed
method also detects occlusions and incoherent motions due to vehicles and
pedestrians in non simultaneous stereo."
"We propose in this paper a tracking algorithm which is able to adapt itself
to different scene contexts. A feature pool is used to compute the matching
score between two detected objects. This feature pool includes 2D, 3D
displacement distances, 2D sizes, color histogram, histogram of oriented
gradient (HOG), color covariance and dominant color. An offline learning
process is proposed to search for useful features and to estimate their weights
for each context. In the online tracking process, a temporal window is defined
to establish the links between the detected objects. This enables to find the
object trajectories even if the objects are misdetected in some frames. A
trajectory filter is proposed to remove noisy trajectories. Experimentation on
different contexts is shown. The proposed tracker has been tested in videos
belonging to three public datasets and to the Caretaker European project. The
experimental results prove the effect of the proposed feature weight learning,
and the robustness of the proposed tracker compared to some methods in the
state of the art. The contributions of our approach over the state of the art
trackers are: (i) a robust tracking algorithm based on a feature pool, (ii) a
supervised learning scheme to learn feature weights for each context, (iii) a
new method to quantify the reliability of HOG descriptor, (iv) a combination of
color covariance and dominant color features with spatial pyramid distance to
manage the case of object occlusion."
"Crucial information barely visible to the human eye is often embedded in a
series of low-resolution images taken of the same scene. Super-resolution
enables the extraction of this information by reconstructing a single image, at
a high resolution than is present in any of the individual images. This is
particularly useful in forensic imaging, where the extraction of minute details
in an image can help to solve a crime. Super-resolution image restoration has
been one of the most important research areas in recent years which goals to
obtain a high resolution (HR) image from several low resolutions (LR) blurred,
noisy, under sampled and displaced images. Relation of the HR image and LR
images can be modeled by a linear system using a transformation matrix and
additive noise. However, a unique solution may not be available because of the
singularity of transformation matrix. To overcome this problem, POCS method has
been used. However, their performance is not good because the effect of noise
energy has been ignored. In this paper, we propose an adaptive regularization
approach based on the fact that the regularization parameter should be a linear
function of noise variance. The performance of the proposed approach has been
tested on several images and the obtained results demonstrate the superiority
of our approach compared with existing methods."
"This paper presents a novel reaction-diffusion (RD) method for implicit
active contours, which is completely free of the costly re-initialization
procedure in level set evolution (LSE). A diffusion term is introduced into
LSE, resulting in a RD-LSE equation, to which a piecewise constant solution can
be derived. In order to have a stable numerical solution of the RD based LSE,
we propose a two-step splitting method (TSSM) to iteratively solve the RD-LSE
equation: first iterating the LSE equation, and then solving the diffusion
equation. The second step regularizes the level set function obtained in the
first step to ensure stability, and thus the complex and costly
re-initialization procedure is completely eliminated from LSE. By successfully
applying diffusion to LSE, the RD-LSE model is stable by means of the simple
finite difference method, which is very easy to implement. The proposed RD
method can be generalized to solve the LSE for both variational level set
method and PDE-based level set method. The RD-LSE method shows very good
performance on boundary anti-leakage, and it can be readily extended to high
dimensional level set method. The extensive and promising experimental results
on synthetic and real images validate the effectiveness of the proposed RD-LSE
approach."
"This paper proposes a new procedure in order to improve the performance of
block matching and 3-D filtering (BM3D) image denoising algorithm. It is
demonstrated that it is possible to achieve a better performance than that of
BM3D algorithm in a variety of noise levels. This method changes BM3D algorithm
parameter values according to noise level, removes prefiltering, which is used
in high noise level; therefore Peak Signal-to-Noise Ratio (PSNR) and visual
quality get improved, and BM3D complexities and processing time are reduced.
This improved BM3D algorithm is extended and used to denoise satellite and
color filter array (CFA) images. Output results show that the performance has
upgraded in comparison with current methods of denoising satellite and CFA
images. In this regard this algorithm is compared with Adaptive PCA algorithm,
that has led to superior performance for denoising CFA images, on the subject
of PSNR and visual quality. Also the processing time has decreased
significantly."
"Clustering is a fundamental task in unsupervised learning. The focus of this
paper is the Correlation Clustering functional which combines positive and
negative affinities between the data points. The contribution of this paper is
two fold: (i) Provide a theoretic analysis of the functional. (ii) New
optimization algorithms which can cope with large scale problems (>100K
variables) that are infeasible using existing methods. Our theoretic analysis
provides a probabilistic generative interpretation for the functional, and
justifies its intrinsic ""model-selection"" capability. Furthermore, we draw an
analogy between optimizing this functional and the well known Potts energy
minimization. This analogy allows us to suggest several new optimization
algorithms, which exploit the intrinsic ""model-selection"" capability of the
functional to automatically recover the underlying number of clusters. We
compare our algorithms to existing methods on both synthetic and real data. In
addition we suggest two new applications that are made possible by our
algorithms: unsupervised face identification and interactive multi-object
segmentation by rough boundary delineation."
"Matching animal-like flexibility in recognition and the ability to quickly
incorporate new information remains difficult. Limits are yet to be adequately
addressed in neural models and recognition algorithms. This work proposes a
configuration for recognition that maintains the same function of conventional
algorithms but avoids combinatorial problems. Feedforward recognition
algorithms such as classical artificial neural networks and machine learning
algorithms are known to be subject to catastrophic interference and forgetting.
Modifying or learning new information (associations between patterns and
labels) causes loss of previously learned information. I demonstrate using
mathematical analysis how supervised generative models, with feedforward and
feedback connections, can emulate feedforward algorithms yet avoid catastrophic
interference and forgetting. Learned information in generative models is stored
in a more intuitive form that represents the fixed points or solutions of the
network and moreover displays similar difficulties as cognitive phenomena.
Brain-like capabilities and limits associated with generative models suggest
the brain may perform recognition and store information using a similar
approach. Because of the central role of recognition, progress understanding
the underlying principles may reveal significant insight on how to better study
and integrate with the brain."
"Combining information from various image features has become a standard
technique in concept recognition tasks. However, the optimal way of fusing the
resulting kernel functions is usually unknown in practical applications.
Multiple kernel learning (MKL) techniques allow to determine an optimal linear
combination of such similarity matrices. Classical approaches to MKL promote
sparse mixtures. Unfortunately, so-called 1-norm MKL variants are often
observed to be outperformed by an unweighted sum kernel. The contribution of
this paper is twofold: We apply a recently developed non-sparse MKL variant to
state-of-the-art concept recognition tasks within computer vision. We provide
insights on benefits and limits of non-sparse MKL and compare it against its
direct competitors, the sum kernel SVM and the sparse MKL. We report empirical
results for the PASCAL VOC 2009 Classification and ImageCLEF2010 Photo
Annotation challenge data sets. About to be submitted to PLoS ONE."
"A vehicle detection plays an important role in the traffic control at
signalised intersections. This paper introduces a vision-based algorithm for
vehicles presence recognition in detection zones. The algorithm uses linguistic
variables to evaluate local attributes of an input image. The image attributes
are categorised as vehicle, background or unknown features. Experimental
results on complex traffic scenes show that the proposed algorithm is effective
for a real-time vehicles detection."
"In this paper a vision-based vehicles recognition method is presented.
Proposed method uses fuzzy description of image segments for automatic
recognition of vehicles recorded in image data. The description takes into
account selected geometrical properties and shape coefficients determined for
segments of reference image (vehicle model). The proposed method was
implemented using reasoning system with fuzzy rules. A vehicles recognition
algorithm was developed based on the fuzzy rules describing shape and
arrangement of the image segments that correspond to visible parts of a
vehicle. An extension of the algorithm with set of fuzzy rules defined for
different reference images (and various vehicle shapes) enables vehicles
classification in traffic scenes. The devised method is suitable for
application in video sensors for road traffic control and surveillance systems."
"In this paper, we introduce a Reduced Reference Image Quality Assessment
(RRIQA) measure based on the natural image statistic approach. A new adaptive
transform called ""Tetrolet"" is applied to both reference and distorted images.
To model the marginal distribution of tetrolet coefficients Bessel K Forms
(BKF) density is proposed. Estimating the parameters of this distribution
allows to summarize the reference image with a small amount of side
information. Five distortion measures based on the BKF parameters of the
original and processed image are used to predict quality scores. A comparison
between these measures is presented showing a good consistency with human
judgment."
"A fundamental task in human chromosome analysis is chromosome segmentation.
Segmentation plays an important role in chromosome karyotyping. The first step
in segmentation is to remove intrusive objects such as stain debris and other
noises. The next step is detection of touching and overlapping chromosomes, and
the final step is separation of such chromosomes. Common methods for separation
between touching chromosomes are interactive and require human intervention for
correct separation between touching and overlapping chromosomes. In this paper,
a geometric-based method is used for automatic detection of touching and
overlapping chromosomes and separating them. The proposed scheme performs
segmentation in two phases. In the first phase, chromosome clusters are
detected using three geometric criteria, and in the second phase, chromosome
clusters are separated using a cut-line. Most of earlier methods did not work
properly in case of chromosome clusters that contained more than two
chromosomes. Our method, on the other hand, is quite efficient in separation of
such chromosome clusters. At each step, one separation will be performed and
this algorithm is repeated until all individual chromosomes are separated.
Another important point about the proposed method is that it uses the geometric
features of chromosomes which are independent of the type of images and it can
easily be applied to any type of images such as binary images and does not
require multispectral images as well. We have applied our method to a database
containing 62 touching and partially overlapping chromosomes and a success rate
of 91.9% is achieved."
"After the discovery that fixed points of loopy belief propagation coincide
with stationary points of the Bethe free energy, several researchers proposed
provably convergent algorithms to directly minimize the Bethe free energy.
These algorithms were formulated only for non-zero temperature (thus finding
fixed points of the sum-product algorithm) and their possible extension to zero
temperature is not obvious. We present the zero-temperature limit of the
double-loop algorithm by Heskes, which converges a max-product fixed point. The
inner loop of this algorithm is max-sum diffusion. Under certain conditions,
the algorithm combines the complementary advantages of the max-product belief
propagation and max-sum diffusion (LP relaxation): it yields good approximation
of both ground states and max-marginals."
"Transformation-invariant analysis of signals often requires the computation
of the distance from a test pattern to a transformation manifold. In
particular, the estimation of the distances between a transformed query signal
and several transformation manifolds representing different classes provides
essential information for the classification of the signal. In many
applications the computation of the exact distance to the manifold is costly,
whereas an efficient practical solution is the approximation of the manifold
distance with the aid of a manifold grid. In this paper, we consider a setting
with transformation manifolds of known parameterization. We first present an
algorithm for the selection of samples from a single manifold that permits to
minimize the average error in the manifold distance estimation. Then we propose
a method for the joint discretization of multiple manifolds that represent
different signal classes, where we optimize the transformation-invariant
classification accuracy yielded by the discrete manifold representation.
Experimental results show that sampling each manifold individually by
minimizing the manifold distance estimation error outperforms baseline sampling
solutions with respect to registration and classification accuracy. Performing
an additional joint optimization on all samples improves the classification
performance further. Moreover, given a fixed total number of samples to be
selected from all manifolds, an asymmetric distribution of samples to different
manifolds depending on their geometric structures may also increase the
classification accuracy in comparison with the equal distribution of samples."
"Manifold models provide low-dimensional representations that are useful for
processing and analyzing data in a transformation-invariant way. In this paper,
we study the problem of learning smooth pattern transformation manifolds from
image sets that represent observations of geometrically transformed signals. In
order to construct a manifold, we build a representative pattern whose
transformations accurately fit various input images. We examine two objectives
of the manifold building problem, namely, approximation and classification. For
the approximation problem, we propose a greedy method that constructs a
representative pattern by selecting analytic atoms from a continuous dictionary
manifold. We present a DC (Difference-of-Convex) optimization scheme that is
applicable to a wide range of transformation and dictionary models, and
demonstrate its application to transformation manifolds generated by rotation,
translation and anisotropic scaling of a reference pattern. Then, we generalize
this approach to a setting with multiple transformation manifolds, where each
manifold represents a different class of signals. We present an iterative
multiple manifold building algorithm such that the classification accuracy is
promoted in the learning of the representative patterns. Experimental results
suggest that the proposed methods yield high accuracy in the approximation and
classification of data compared to some reference methods, while the invariance
to geometric transformations is achieved due to the transformation manifold
model."
"A framework of online adaptive statistical compressed sensing is introduced
for signals following a mixture model. The scheme first uses non-adaptive
measurements, from which an online decoding scheme estimates the model
selection. As soon as a candidate model has been selected, an optimal sensing
scheme for the selected model continues to apply. The final signal
reconstruction is calculated from the ensemble of both the non-adaptive and the
adaptive measurements. For signals generated from a Gaussian mixture model, the
online adaptive sensing algorithm is given and its performance is analyzed. On
both synthetic and real image data, the proposed adaptive scheme considerably
reduces the average reconstruction error with respect to standard statistical
compressed sensing that uses fully random measurements, at a marginally
increased computational complexity."
"Personal identification problem has been a major field of research in recent
years. Biometrics-based technologies that exploit fingerprints, iris, face,
voice and palmprints, have been in the center of attention to solve this
problem. Palmprints can be used instead of fingerprints that have been of the
earliest of these biometrics technologies. A palm is covered with the same skin
as the fingertips but has a larger surface, giving us more information than the
fingertips. The major features of the palm are palm-lines, including principal
lines, wrinkles and ridges. Using these lines is one of the most popular
approaches towards solving the palmprint recognition problem. Another robust
feature is the wavelet energy of palms. In this paper we used a hybrid feature
which combines both of these features. %Moreover, multispectral analysis is
applied to improve the performance of the system. At the end, minimum distance
classifier is used to match test images with one of the training samples. The
proposed algorithm has been tested on a well-known multispectral palmprint
dataset and achieved an average accuracy of 98.8\%."
"Biometric based authentication for secured access to resources has gained
importance, due to their reliable, invariant and discriminating features.
Palmprint is one such biometric entity. Prior to classification and
identification registering a sample palmprint is an important activity. In this
paper we propose a computationally effective method for automated registration
of samples from PlolyU palmprint database. In our approach we preprocess the
sample and trace the border to find the nearest point from center of sample.
Angle between vector representing the nearest point and vector passing through
the center is used for automated palm sample registration. The angle of
inclination between start and end point of heart line and life line is used for
basic classification of palmprint samples in left class and right class."
"This paper presents a method for learning overcomplete dictionaries composed
of two modalities that describe a 3D scene: image intensity and scene depth. We
propose a novel Joint Basis Pursuit (JBP) algorithm that finds related sparse
features in two modalities using conic programming and integrate it into a
two-step dictionary learning algorithm. JBP differs from related convex
algorithms because it finds joint sparsity models with different atoms and
different coefficient values for intensity and depth. This is crucial for
recovering generative models where the same sparse underlying causes (3D
features) give rise to different signals (intensity and depth). We give a
theoretical bound for the sparse coefficient recovery error obtained by JBP,
and show experimentally that JBP is far superior to the state of the art Group
Lasso algorithm. When applied to the Middlebury depth-intensity database, our
learning algorithm converges to a set of related features, such as pairs of
depth and intensity edges or image textures and depth slants. Finally, we show
that the learned dictionary and JBP achieve the state of the art depth
inpainting performance on time-of-flight 3D data."
"In this paper, a salient region extraction method for creating picture
collage based on stereo vision is proposed. Picture collage is a kind of visual
image summary to arrange all input images on a given canvas, allowing overlay,
to maximize visible visual information. The salient regions of each image are
firstly extracted and represented as a depth map. The output picture collage
shows as many visible salient regions (without being overlaid by others) from
all images as possible. A very efficient Genetic algorithm is used here for the
optimization. The experimental results showed the superior performance of the
proposed method."
"A uniform distribution of the image force field around the object fasts the
convergence speed of the segmentation process. However, to achieve this aim, it
causes the force constructed from the heat diffusion model unable to indicate
the object boundaries accurately. The image force based on electrostatic field
model can perform an exact shape recovery. First, this study introduces a
fusion scheme of these two image forces, which is capable of extracting the
object boundary with high precision and fast speed. Until now, there is no
satisfied analysis about the relationship between Snakes and Geometric Active
Contours (GAC). The second contribution of this study addresses that the GAC
model can be deduced directly from Snakes model. It proves that each term in
GAC and Snakes is correspondent and has similar function. However, the two
models are expressed using different mathematics. Further, since losing the
ability of rotating the contour, adoption of level sets can limits the usage of
GAC in some circumstances."
"In this paper, a new adaptive noise reduction scheme for images corrupted by
impulse noise is presented. The proposed scheme efficiently identifies and
reduces salt and pepper noise. MAG (Mean Absolute Gradient) is used to identify
pixels which are most likely corrupted by salt and pepper noise that are
candidates for further median based noise reduction processing. Directional
filtering is then applied after noise reduction to achieve a good tradeoff
between detail preservation and noise removal. The proposed scheme can remove
salt and pepper noise with noise density as high as 90% and produce better
result in terms of qualitative and quantitative measures of images."
"This paper suggests a nonparametric scheme to find the sparse solution of the
underdetermined system of linear equations in the presence of unknown impulsive
or non-Gaussian noise. This approach is robust against any variations of the
noise model and its parameters. It is based on minimization of rank pseudo norm
of the residual signal and l_1-norm of the signal of interest, simultaneously.
We use the steepest descent method to find the sparse solution via an iterative
algorithm. Simulation results show that our proposed method outperforms the
existence methods like OMP, BP, Lasso, and BCS whenever the observation vector
is contaminated with measurement or environmental non-Gaussian noise with
unknown parameters. Furthermore, for low SNR condition, the proposed method has
better performance in the presence of Gaussian noise."
"Solving the Maximum a Posteriori on Markov Random Field, MRF-MAP, is a
prevailing method in recent interactive image segmentation tools. Although
mathematically explicit in its computational targets, and impressive for the
segmentation quality, MRF-MAP is hard to accomplish without the interactive
information from users. So it is rarely adopted in the automatic style up to
today. In this paper, we present an automatic image segmentation algorithm,
NegCut, based on the approximation to MRF-MAP. First we prove MRF-MAP is
NP-hard when the probabilistic models are unknown, and then present an
approximation function in the form of minimum cuts on graphs with negative
weights. Finally, the binary segmentation is taken from the largest eigenvector
of the target matrix, with a tuned version of the Lanczos eigensolver. It is
shown competitive at the segmentation quality in our experiments."
"We present an algorithm using transformation groups and their irreducible
representations to generate an orthogonal basis for a signal in the vector
space of the signal. It is shown that multiresolution analysis can be done with
amplitudes using a transformation group. G-lets is thus not a single transform,
but a group of linear transformations related by group theory. The algorithm
also specifies that a multiresolution and multiscale analysis for each
resolution is possible in terms of frequencies. Separation of low and high
frequency components of each amplitude resolution is facilitated by G-lets.
Using conjugacy classes of the transformation group, more than one set of basis
may be generated, giving a different perspective of the signal through each
basis. Applications for this algorithm include edge detection, feature
extraction, denoising, face recognition, compression, and more. We analyze this
algorithm using dihedral groups as an example. We demonstrate the results with
an ECG signal and the standard `Lena' image."
"This paper presents a new method for automatic quantification of ellipse-like
cells in images, an important and challenging problem that has been studied by
the computer vision community. The proposed method can be described by two main
steps. Initially, image segmentation based on the k-means algorithm is
performed to separate different types of cells from the background. Then, a
robust and efficient strategy is performed on the blob contour for touching
cells splitting. Due to the contour processing, the method achieves excellent
results of detection compared to manual detection performed by specialists."
"Shape is one of the most important visual attributes to characterize objects,
playing a important role in pattern recognition. There are various approaches
to extract relevant information of a shape. An approach widely used in shape
analysis is the complexity, and Fractal Dimension and Multi-Scale Fractal
Dimension are both well-known methodologies to estimate it. This papers
presents a comparative study between Fractal Dimension and Multi-Scale Fractal
Dimension in a shape analysis context. Through experimental comparison using a
shape database previously classified, both methods are compared. Different
parameters configuration of each method are considered and a discussion about
the results of each method is also presented."
"The calculus of variations applied to the image processing requires some
numerical models able to perform the variations of images and the extremization
of appropriate actions. To produce the variations of images, there are several
possibilities based on the brightness maps. Before a numerical model, I propose
an experimental approach, based on a tool of Gimp, GNU Image Manipulation
Program, in order to visualize how the image variations can be. After the
discussion of this tool, which is able to strongly increase the visibility of
images, the variations and a possible functional for the visibility are
proposed in the framework of a numerical model. The visibility functional is
analogous to the fringe visibility of the optical interference."
"This paper presents a new method for dynamic texture recognition based on
spatiotemporal Gabor filters. Dynamic textures have emerged as a new field of
investigation that extends the concept of self-similarity of texture image to
the spatiotemporal domain. To model a dynamic texture, we convolve the sequence
of images to a bank of spatiotemporal Gabor filters. For each response, a
feature vector is built by calculating the energy statistic. As far as the
authors know, this paper is the first to report an effective method for dynamic
texture recognition using spatiotemporal Gabor filters. We evaluate the
proposed method on two challenging databases and the experimental results
indicate that the proposed method is a robust approach for dynamic texture
recognition."
"Essentially a biometric system is a pattern recognition system which
recognizes a user by determining the authenticity of a specific anatomical or
behavioral characteristic possessed by the user. With the ever increasing
integration of computers and Internet into daily life style, it has become
necessary to protect sensitive and personal data. This paper proposes a
multimodal biometric system which incorporates more than one biometric trait to
attain higher security and to handle failure to enroll situations for some
users. This paper is aimed at investigating a multimodal biometric identity
system using Linear Discriminant Analysis as backbone to both facial and speech
recognition and implementing such system in real-time using SignalWAVE."
"The use of hierarchical Conditional Random Field model deal with the problem
of labeling images . At the time of labeling a new image, selection of the
nearest cluster and using the related CRF model to label this image. When one
give input image, one first use the CRF model to get initial pixel labels then
finding the cluster with most similar images. Then at last relabeling the input
image by the CRF model associated with this cluster. This paper presents a
approach to label and segment specific image having correct information."
"In this paper, we present a novel, learning-based, two-step super-resolution
(SR) algorithm well suited to solve the specially demanding problem of
obtaining SR estimates from short image sequences. The first step, devoted to
increase the sampling rate of the incoming images, is performed by fitting
linear combinations of functions generated from principal components (PC) to
reproduce locally the sparse projected image data, and using these models to
estimate image values at nodes of the high-resolution grid. PCs were obtained
from local image patches sampled at sub-pixel level, which were generated in
turn from a database of high-resolution images by application of a physically
realistic observation model. Continuity between local image models is enforced
by minimizing an adequate functional in the space of model coefficients. The
second step, dealing with restoration, is performed by a linear filter with
coefficients learned to restore residual interpolation artifacts in addition to
low-resolution blurring, providing an effective coupling between both steps of
the method. Results on a demanding five-image scanned sequence of graphics and
text are presented, showing the excellent performance of the proposed method
compared to several state-of-the-art two-step and Bayesian Maximum a Posteriori
SR algorithms."
"In this study we investigate the fast image filtering algorithm based on
Intro sort algorithm and fast noise reduction of infrared images. Main feature
of the proposed approach is that no prior knowledge of noise required. It is
developed based on Stefan- Boltzmann law and the Fourier law. We also
investigate the fast noise reduction approach that has advantage of less
computation load. In addition, it can retain edges, details, text information
even if the size of the window increases. Intro sort algorithm begins with
Quick sort and switches to heap sort when the recursion depth exceeds a level
based on the number of elements being sorted. This approach has the advantage
of fast noise reduction by reducing the comparison time. It also significantly
speed up the noise reduction process and can apply to real-time image
processing. This approach will extend the Infrared images applications for
medicine and video conferencing."
"Texture analysis is an important field of investigation that has received a
great deal of interest from computer vision community. In this paper, we
propose a novel approach for texture modeling based on partial differential
equation (PDE). Each image $f$ is decomposed into a family of derived
sub-images. $f$ is split into the $u$ component, obtained with anisotropic
diffusion, and the $v$ component which is calculated by the difference between
the original image and the $u$ component. After enhancing the texture attribute
$v$ of the image, Gabor features are computed as descriptors. We validate the
proposed approach on two texture datasets with high variability. We also
evaluate our approach on an important real-world application: leaf-texture
analysis. Experimental results indicate that our approach can be used to
produce higher classification rates and can be successfully employed for
different texture applications."
"Compressive sensing (CS) is a new approach for the acquisition and recovery
of sparse signals and images that enables sampling rates significantly below
the classical Nyquist rate. Despite significant progress in the theory and
methods of CS, little headway has been made in compressive video acquisition
and recovery. Video CS is complicated by the ephemeral nature of dynamic
events, which makes direct extensions of standard CS imaging architectures and
signal models difficult. In this paper, we develop a new framework for video CS
for dynamic textured scenes that models the evolution of the scene as a linear
dynamical system (LDS). This reduces the video recovery problem to first
estimating the model parameters of the LDS from compressive measurements, and
then reconstructing the image frames. We exploit the low-dimensional dynamic
parameters (the state sequence) and high-dimensional static parameters (the
observation matrix) of the LDS to devise a novel compressive measurement
strategy that measures only the dynamic part of the scene at each instant and
accumulates measurements over time to estimate the static parameters. This
enables us to lower the compressive measurement rate considerably. We validate
our approach with a range of experiments involving both video recovery, sensing
hyper-spectral data, and classification of dynamic scenes from compressive
data. Together, these applications demonstrate the effectiveness of the
approach."
"Image binarization is the process of separation of pixel values into two
groups, white as background and black as foreground. Thresholding plays a major
in binarization of images. Thresholding can be categorized into global
thresholding and local thresholding. In images with uniform contrast
distribution of background and foreground like document images, global
thresholding is more appropriate. In degraded document images, where
considerable background noise or variation in contrast and illumination exists,
there exists many pixels that cannot be easily classified as foreground or
background. In such cases, binarization with local thresholding is more
appropriate. This paper describes a locally adaptive thresholding technique
that removes background by using local mean and mean deviation. Normally the
local mean computational time depends on the window size. Our technique uses
integral sum image as a prior processing to calculate local mean. It does not
involve calculations of standard deviations as in other local adaptive
techniques. This along with the fact that calculations of mean is independent
of window size speed up the process as compared to other local thresholding
techniques."
"A framework for adaptive and non-adaptive statistical compressive sensing is
developed, where a statistical model replaces the standard sparsity model of
classical compressive sensing. We propose within this framework optimal
task-specific sensing protocols specifically and jointly designed for
classification and reconstruction. A two-step adaptive sensing paradigm is
developed, where online sensing is applied to detect the signal class in the
first step, followed by a reconstruction step adapted to the detected class and
the observed samples. The approach is based on information theory, here
tailored for Gaussian mixture models (GMMs), where an information-theoretic
objective relationship between the sensed signals and a representation of the
specific task of interest is maximized. Experimental results using synthetic
signals, Landsat satellite attributes, and natural images of different sizes
and with different noise levels show the improvements achieved using the
proposed framework when compared to more standard sensing protocols. The
underlying formulation can be applied beyond GMMs, at the price of higher
mathematical and computational complexity."
"The appearance of microcalcifications in mammograms is one of the early signs
of breast cancer. So, early detection of microcalcification clusters (MCCs) in
mammograms can be helpful for cancer diagnosis and better treatment of breast
cancer. In this paper a computer method has been proposed to support
radiologists in detection MCCs in digital mammography. First, in order to
facilitate and improve the detection step, mammogram images have been enhanced
with wavelet transformation and morphology operation. Then for segmentation of
suspicious MCCs, two methods have been investigated. The considered methods
are: adaptive threshold and watershed segmentation. Finally, the detected MCCs
areas in different algorithms will be compared to find out which segmentation
method is more appropriate for extracting MCCs in mammograms."
"3D motion tracking is a critical task in many computer vision applications.
Unsupervised markerless 3D motion tracking systems determine the most relevant
object in the screen and then track it by continuously estimating its
projection features (center and area) from the edge image and a point inside
the relevant object projection (namely, inner point), until the tracking fails.
Existing reliable object projection feature estimation techniques are based on
ray-casting or grid-filling from the inner point. These techniques assume the
edge image to be accurate. However, in real case scenarios, edge
miscalculations may arise from low contrast between the target object and its
surroundings or motion blur caused by low frame rates or fast moving target
objects. In this paper, we propose a barrier extension to casting-based
techniques that mitigates the effect of edge miscalculations."
"This paper presents the Discrete Wavelet based fusion techniques for
combining perceptually important image features. SPIHT (Set Partitioning in
Hierarchical Trees) algorithm is an efficient method for lossy and lossless
coding of fused image. This paper presents some modifications on the SPIHT
algorithm. It is based on the idea of insignificant correlation of wavelet
coefficient among the medium and high frequency sub bands. In RE-MSPIHT
algorithm, wavelet coefficients are scaled prior to SPIHT coding based on the
sub band importance, with the goal of minimizing the MSE."
"In practical applications, we often have to deal with high order data, such
as a grayscale image and a video sequence are intrinsically 2nd-order tensor
and 3rd-order tensor, respectively. For doing clustering or classification of
these high order data, it is a conventional way to vectorize these data before
hand, as PCA or FDA does, which often induce the curse of dimensionality
problem. For this reason, experts have developed many methods to deal with the
tensorial data, such as multilinear PCA, multilinear LDA, and so on. In this
paper, we still address the problem of high order data representation and
recognition, and propose to study the result of merging multilinear PCA and
multilinear LDA into one scenario, we name it \textbf{GDA} for the abbreviation
of Generalized Discriminant Analysis. To evaluate GDA, we perform a series of
experiments, and the experimental results demonstrate our GDA outperforms a
selection of competing methods such (2D)$^2$PCA, (2D)$^2$LDA, and MDA."
"This manuscript proposes a posterior mean (PM) super-resolution (SR) method
with a compound Gaussian Markov random field (MRF) prior. SR is a technique to
estimate a spatially high-resolution image from observed multiple
low-resolution images. A compound Gaussian MRF model provides a preferable
prior for natural images that preserves edges. PM is the optimal estimator for
the objective function of peak signal-to-noise ratio (PSNR). This estimator is
numerically determined by using variational Bayes (VB). We then solve the
conjugate prior problem on VB and the exponential-order calculation cost
problem of a compound Gaussian MRF prior with simple Taylor approximations. In
experiments, the proposed method roughly overcomes existing methods."
"Previous researches have demonstrated that the framework of dictionary
learning with sparse coding, in which signals are decomposed as linear
combinations of a few atoms of a learned dictionary, is well adept to
reconstruction issues. This framework has also been used for discrimination
tasks such as image classification. To achieve better performances of
classification, experts develop several methods to learn a discriminative
dictionary in a supervised manner. However, another issue is that when the data
become extremely large in scale, these methods will be no longer effective as
they are all batch-oriented approaches. For this reason, we propose a novel
online algorithm for discriminative dictionary learning, dubbed \textbf{ODDL}
in this paper. First, we introduce a linear classifier into the conventional
dictionary learning formulation and derive a discriminative dictionary learning
problem. Then, we exploit an online algorithm to solve the derived problem.
Unlike the most existing approaches which update dictionary and classifier
alternately via iteratively solving sub-problems, our approach directly
explores them jointly. Meanwhile, it can largely shorten the runtime for
training and is also particularly suitable for large-scale classification
issues. To evaluate the performance of the proposed ODDL approach in image
recognition, we conduct some experiments on three well-known benchmarks, and
the experimental results demonstrate ODDL is fairly promising for image
classification tasks."
"In 3D reconstruction, the recovery of the calibration parameters of the
cameras is paramount since it provides metric information about the observed
scene, e.g., measures of angles and ratios of distances. Autocalibration
enables the estimation of the camera parameters without using a calibration
device, but by enforcing simple constraints on the camera parameters. In the
absence of information about the internal camera parameters such as the focal
length and the principal point, the knowledge of the camera pixel shape is
usually the only available constraint. Given a projective reconstruction of a
rigid scene, we address the problem of the autocalibration of a minimal set of
cameras with known pixel shape and otherwise arbitrarily varying intrinsic and
extrinsic parameters. We propose an algorithm that only requires 5 cameras (the
theoretical minimum), thus halving the number of cameras required by previous
algorithms based on the same constraint. To this purpose, we introduce as our
basic geometric tool the six-line conic variety (SLCV), consisting in the set
of planes intersecting six given lines of 3D space in points of a conic. We
show that the set of solutions of the Euclidean upgrading problem for three
cameras with known pixel shape can be parameterized in a computationally
efficient way. This parameterization is then used to solve autocalibration from
five or more cameras, reducing the three-dimensional search space to a
two-dimensional one. We provide experiments with real images showing the good
performance of the technique."
"A wavelet scattering network computes a translation invariant image
representation, which is stable to deformations and preserves high frequency
information for classification. It cascades wavelet transform convolutions with
non-linear modulus and averaging operators. The first network layer outputs
SIFT-type descriptors whereas the next layers provide complementary invariant
information which improves classification. The mathematical analysis of wavelet
scattering networks explains important properties of deep convolution networks
for classification.
  A scattering representation of stationary processes incorporates higher order
moments and can thus discriminate textures having the same Fourier power
spectrum. State of the art classification results are obtained for handwritten
digits and texture discrimination, using a Gaussian kernel SVM and a generative
PCA classifier."
"In this paper, we implement and carry out the comparison of two methods of
computer-aided-detection of masses on mammograms. The two algorithms basically
consist of 3 steps each: segmentation, binarization and noise suppression using
different techniques for each step. A database of 60 images was used to compare
the performance of the two algorithms in terms of general detection efficiency,
conservation of size and shape of detected masses."
"This paper introduces a probabilistic graphical model for continuous action
recognition with two novel components: substructure transition model and
discriminative boundary model. The first component encodes the sparse and
global temporal transition prior between action primitives in state-space model
to handle the large spatial-temporal variations within an action class. The
second component enforces the action duration constraint in a discriminative
way to locate the transition boundaries between actions more accurately. The
two components are integrated into a unified graphical structure to enable
effective training and inference. Our comprehensive experimental results on
both public and in-house datasets show that, with the capability to incorporate
additional information that had not been explicitly or efficiently modeled by
previous methods, our proposed algorithm achieved significantly improved
performance for continuous action recognition."
"Pedicle screw insertion technique has made revolution in the surgical
treatment of spinal fractures and spinal disorders. Although X- ray fluoroscopy
based navigation is popular, there is risk of prolonged exposure to X- ray
radiation. Systems that have lower radiation risk are generally quite
expensive. The position and orientation of the drill is clinically very
important in pedicle screw fixation. In this paper, the position and
orientation of the marker on the drill is determined using pattern recognition
based methods, using geometric features, obtained from the input video sequence
taken from CCD camera. A search is then performed on the video frames after
preprocessing, to obtain the exact position and orientation of the drill.
Animated graphics, showing the instantaneous position and orientation of the
drill is then overlaid on the processed video for real time drill control and
navigation."
"This paper deals with enhancement of images with poor contrast and detection
of background. Proposes a frame work which is used to detect the background in
images characterized by poor contrast. Image enhancement has been carried out
by the two methods based on the Weber's law notion. The first method employs
information from image background analysis by blocks, while the second
transformation method utilizes the opening operation, closing operation, which
is employed to define the multi-background gray scale images. The complete
image processing is done using MATLAB simulation model. Finally, this paper is
organized as follows as Morphological transformation and Weber's law. Image
background approximation to the background by means of block analysis in
conjunction with transformations that enhance images with poor lighting. The
multibackground notion is introduced by means of the opening by reconstruction
shows a comparison among several techniques to improve contrast in images.
Finally, conclusions are presented."
"We present a rectangle-based segmentation algorithm that sets up a graph and
performs a graph cut to separate an object from the background. However,
graph-based algorithms distribute the graph's nodes uniformly and equidistantly
on the image. Then, a smoothness term is added to force the cut to prefer a
particular shape. This strategy does not allow the cut to prefer a certain
structure, especially when areas of the object are indistinguishable from the
background. We solve this problem by referring to a rectangle shape of the
object when sampling the graph nodes, i.e., the nodes are distributed
nonuniformly and non-equidistantly on the image. This strategy can be useful,
when areas of the object are indistinguishable from the background. For
evaluation, we focus on vertebrae images from Magnetic Resonance Imaging (MRI)
datasets to support the time consuming manual slice-by-slice segmentation
performed by physicians. The ground truth of the vertebrae boundaries were
manually extracted by two clinical experts (neurological surgeons) with several
years of experience in spine surgery and afterwards compared with the automatic
segmentation results of the proposed scheme yielding an average Dice Similarity
Coefficient (DSC) of 90.97\pm62.2%."
"A method to obtain three-dimensional data of real-world objects by
integrating their material properties is presented. The material properties are
defined by capturing the Reflectance Fields of the real-world objects. It is
shown, unlike conventional reconstruction methods, the method is able to use
the reflectance information to recover surface depth for objects having a
non-Lambertian surface reflectance. It is, for recovering 3D data of objects
exhibiting an anisotropic BRDF with an error less than 0.3%."
"In real world everything is an object which represents particular classes.
Every object can be fully described by its attributes. Any real world dataset
contains large number of attributes and objects. Classifiers give poor
performance when these huge datasets are given as input to it for proper
classification. So from these huge dataset most useful attributes need to be
extracted that contribute the maximum to the decision. In the paper, attribute
set is reduced by generating reducts using the indiscernibility relation of
Rough Set Theory (RST). The method measures similarity among the attributes
using relative indiscernibility relation and computes attribute similarity set.
Then the set is minimized and an attribute similarity table is constructed from
which attribute similar to maximum number of attributes is selected so that the
resultant minimum set of selected attributes (called reduct) cover all
attributes of the attribute similarity table. The method has been applied on
glass dataset collected from the UCI repository and the classification accuracy
is calculated by various classifiers. The result shows the efficiency of the
proposed method."
"Marker-based motion capture (MoCap) systems can be composed by several dozens
of cameras with the purpose of reconstructing the trajectories of hundreds of
targets. With a large amount of cameras it becomes interesting to determine the
optimal reconstruction strategy. For such aim it is of fundamental importance
to understand the information provided by different camera measurements and how
they are combined, i.e. how the reconstruction error changes by considering
different cameras. In this work, first, an approximation of the reconstruction
error variance is derived. The results obtained in some simulations suggest
that the proposed strategy allows to obtain a good approximation of the real
error variance with significant reduction of the computational time."
"This paper proposes a novel adaptive algorithm to extract facial feature
points automatically such as eyebrows corners, eyes corners, nostrils, nose
tip, and mouth corners in frontal view faces, which is based on cumulative
histogram approach by varying different threshold values. At first, the method
adopts the Viola-Jones face detector to detect the location of face and also
crops the face region in an image. From the concept of the human face
structure, the six relevant regions such as right eyebrow, left eyebrow, right
eye, left eye, nose, and mouth areas are cropped in a face image. Then the
histogram of each cropped relevant region is computed and its cumulative
histogram value is employed by varying different threshold values to create a
new filtering image in an adaptive way. The connected component of interested
area for each relevant filtering image is indicated our respective feature
region. A simple linear search algorithm for eyebrows, eyes and mouth filtering
images and contour algorithm for nose filtering image are applied to extract
our desired corner points automatically. The method was tested on a large BioID
frontal face database in different illuminations, expressions and lighting
conditions and the experimental results have achieved average success rates of
95.27%."
"In this paper we propose a graph-based data clustering algorithm which is
based on exact clustering of a minimum spanning tree in terms of a minimum
isoperimetry criteria. We show that our basic clustering algorithm runs in $O(n
\log n)$ and with post-processing in $O(n^2)$ (worst case) time where $n$ is
the size of the data set. We also show that our generalized graph model which
also allows the use of potentials at vertices can be used to extract a more
detailed pack of information as the {\it outlier profile} of the data set. In
this direction we show that our approach can be used to define the concept of
an outlier-set in a precise way and we propose approximation algorithms for
finding such sets. We also provide a comparative performance analysis of our
algorithm with other related ones and we show that the new clustering algorithm
(without the outlier extraction procedure) behaves quite effectively even on
hard benchmarks and handmade examples."
"This paper presents a novel Coprime Blurred Pair (CBP) model for visual
data-hiding for security in camera surveillance. While most previous approaches
have focused on completely encrypting the video stream, we introduce a spatial
encryption scheme by blurring the image/video contents to create a CBP. Our
goal is to obscure detail in public video streams by blurring while allowing
behavior to be recognized and to quickly deblur the stream so that details are
available if behavior is recognized as suspicious. We create a CBP by blurring
the same latent image with two unknown kernels. The two kernels are coprime
when mapped to bivariate polynomials in the z domain. To deblur the CBP we
first use the coprime constraint to approximate the kernels and sample the
bivariate CBP polynomials in one dimension on the unit circle. At each sample
point, we factor the 1D polynomial pair and compose the results into a 2D
kernel matrix. Finally, we compute the inverse Fast Fourier Transform (FFT) of
the kernel matrices to recover the coprime kernels and then the latent video
stream. It is therefore only possible to deblur the video stream if a user has
access to both streams. To improve the practicability of our algorithm, we
implement our algorithm using a graphics processing unit (GPU) to decrypt the
blurred video streams in real-time, and extensive experimental results
demonstrate that our new scheme can effectively protect sensitive identity
information in surveillance videos and faithfully reconstruct the unblurred
video stream when two blurred sequences are available."
"Research is taking place to find effective algorithms for content-based image
representation and description. There is a substantial amount of algorithms
available that use visual features (color, shape, texture). Shape feature has
attracted much attention from researchers that there are many shape
representation and description algorithms in literature. These shape image
representation and description algorithms are usually not application
independent or robust, making them undesirable for generic shape description.
This paper presents an object shape representation using Kernel Density Feature
Points Estimator (KDFPE). In this method, the density of feature points within
defined rings around the centroid of the image is obtained. The KDFPE is then
applied to the vector of the image. KDFPE is invariant to translation, scale
and rotation. This method of image representation shows improved retrieval rate
when compared to Density Histogram Feature Points (DHFP) method. Analytic
analysis is done to justify our method, which was compared with the DHFP to
prove its robustness."
"In depth from defocus (DFD), when images are captured with different camera
parameters, a relative magnification is induced between them. Image warping is
a simpler solution to account for magnification than seemingly more accurate
optical approaches. This work is an investigation into the effects of
magnification on the accuracy of DFD. We comment on issues regarding scaling
effect on relative blur computation. We statistically analyze accountability of
scale factor, commenting on the bias and efficiency of the estimator that does
not consider scale. We also discuss the effect of interpolation errors on blur
estimation in a warping based solution to handle magnification and carry out
experimental analysis to comment on the blur estimation accuracy."
"The automatic recognition of facial expressions has been an active research
topic since the early nineties. There have been several advances in the past
few years in terms of face detection and tracking, feature extraction
mechanisms and the techniques used for expression classification. This paper
surveys some of the published work since 2001 till date. The paper presents a
time-line view of the advances made in this field, the applications of
automatic face expression recognizers, the characteristics of an ideal system,
the databases that have been used and the advances made in terms of their
standardization and a detailed summary of the state of the art. The paper also
discusses facial parameterization using FACS Action Units (AUs) and MPEG-4
Facial Animation Parameters (FAPs) and the recent advances in face detection,
tracking and feature extraction methods. Notes have also been presented on
emotions, expressions and facial features, discussion on the six prototypic
expressions and the recent studies on expression classifiers. The paper ends
with a note on the challenges and the future work. This paper has been written
in a tutorial style with the intention of helping students and researchers who
are new to this field."
"This paper focuses on fruit defect detection and glare removal using
morphological operations, Glare removal can be considered as an important
preprocessing step as uneven lighting may introduce it in images, which hamper
the results produced through segmentation by Gabor filters .The problem of
glare in images is very pronounced sometimes due to the unusual reflectance
from the camera sensor or stray light entering, this method counteracts this
problem and makes the defect detection much more pronounced. Anisotropic
diffusion is used for further smoothening of the images and removing the high
energy regions in an image for better defect detection and makes the defects
more retrievable. Our algorithm is robust and scalable the employability of a
particular mask for glare removal has been checked and proved useful for
counteracting.this problem, anisotropic diffusion further enhances the defects
with its use further Optimal Gabor filter at various orientations is used for
defect detection."
"Robustness of embedded biometric systems is of prime importance with the
emergence of fourth generation communication devices and advancement in
security systems This paper presents the realization of such technologies which
demands reliable and error-free biometric identity verification systems. High
dimensional patterns are not permitted due to eigen-decomposition in high
dimensional image space and degeneration of scattering matrices in small size
sample. Generalization, dimensionality reduction and maximizing the margins are
controlled by minimizing weight vectors. Results show good pattern by
multimodal biometric system proposed in this paper. This paper is aimed at
investigating a biometric identity system using Principal Component Analysis
and Lindear Discriminant Analysis with K-Nearest Neighbor and implementing such
system in real-time using SignalWAVE."
"Developing a Bangla OCR requires bunch of algorithm and methods. There were
many effort went on for developing a Bangla OCR. But all of them failed to
provide an error free Bangla OCR. Each of them has some lacking. We discussed
about the problem scope of currently existing Bangla OCR's. In this paper, we
present the basic steps required for developing a Bangla OCR and a complete
workflow for development of a Bangla OCR with mentioning all the possible
algorithms required."
"In this paper we present a novel slanted-plane MRF model which reasons
jointly about occlusion boundaries as well as depth. We formulate the problem
as the one of inference in a hybrid MRF composed of both continuous (i.e.,
slanted 3D planes) and discrete (i.e., occlusion boundaries) random variables.
This allows us to define potentials encoding the ownership of the pixels that
compose the boundary between segments, as well as potentials encoding which
junctions are physically possible. Our approach outperforms the
state-of-the-art on Middlebury high resolution imagery as well as in the more
challenging KITTI dataset, while being more efficient than existing slanted
plane MRF-based methods, taking on average 2 minutes to perform inference on
high resolution imagery."
"Gender is an important demographic attribute of people. This paper provides a
survey of human gender recognition in computer vision. A review of approaches
exploiting information from face and whole body (either from a still image or
gait sequence) is presented. We highlight the challenges faced and survey the
representative methods of these approaches. Based on the results, good
performance have been achieved for datasets captured under controlled
environments, but there is still much work that can be done to improve the
robustness of gender recognition under real-life environments."
"This paper introduces a Bayesian image segmentation algorithm based on finite
mixtures. An EM algorithm is developed to estimate parameters of the Gaussian
mixtures. The finite mixture is a flexible and powerful probabilistic modeling
tool. It can be used to provide a model-based clustering in the field of
pattern recognition. However, the application of finite mixtures to image
segmentation presents some difficulties; especially it's sensible to noise. In
this paper we propose a variant of this method which aims to resolve this
problem. Our approach proceeds by the characterization of pixels by two
features: the first one describes the intrinsic properties of the pixel and the
second characterizes the neighborhood of pixel. Then the classification is made
on the base on adaptive distance which privileges the one or the other features
according to the spatial position of the pixel in the image. The obtained
results have shown a significant improvement of our approach compared to the
standard version of EM algorithm."
"The aim of this work is to develop a method for automatic segmentation of the
liver based on a priori knowledge of the image, such as location and shape of
the liver."
"In this paper, we propose an automatic analysis system for the Arabic
handwriting postal addresses recognition, by using the beta elliptical model.
Our system is divided into different steps: analysis, pre-processing and
classification. The first operation is the filtering of image. In the second,
we remove the border print, stamps and graphics. After locating the address on
the envelope, the address segmentation allows the extraction of postal code and
city name separately. The pre-processing system and the modeling approach are
based on two basic steps. The first step is the extraction of the temporal
order in the image of the handwritten trajectory. The second step is based on
the use of Beta-Elliptical model for the representation of handwritten script.
The recognition system is based on Graph-matching algorithm. Our modeling and
recognition approaches were validated by using the postal code and city names
extracted from the Tunisian postal envelopes data. The recognition rate
obtained is about 98%."
"In this paper, we have proposed an extended version of Absolute Moment Block
Truncation Coding (AMBTC) to compress images. Generally the elements of a
bitplane used in the variants of Block Truncation Coding (BTC) are of size 1
bit. But it has been extended to two bits in the proposed method. Number of
statistical moments preserved to reconstruct the compressed has also been
raised from 2 to 4. Hence, the quality of the reconstructed images has been
improved significantly from 33.62 to 38.12 with the increase in bpp by 1. The
increased bpp (3) is further reduced to 1.75in multiple levels: in one level,
by dropping 4 elements of the bitplane in such a away that the pixel values of
the dropped elements can easily be interpolated with out much of loss in the
quality, in level two, eight elements are dropped and reconstructed later and
in level three, the size of the statistical moments is reduced. The experiments
were carried over standard images of varying intensities. In all the cases, the
proposed method outperforms the existing AMBTC technique in terms of both PSNR
and bpp."
"This paper proposes a neural network approach based on Error Back Propagation
(EBP) for classification of different eye images. To reduce the complexity of
layered neural network the dimensions of input vectors are optimized using
Singular Value Decomposition (SVD). The main of this work is to provide for
best method for feature extraction and classification. The details of this
combined system named as SVD-EBP system, and results thereof are presented in
this paper.
  Keywords- Singular value decomposition(SVD), Error back Propagation(EBP)."
"In this paper, an approach to the problem of automatic facial feature
extraction from a still frontal posed image and classification and recognition
of facial expression and hence emotion and mood of a person is presented. Feed
forward back propagation neural network is used as a classifier for classifying
the expressions of supplied face into seven basic categories like surprise,
neutral, sad, disgust, fear, happy and angry. For face portion segmentation and
localization, morphological image processing operations are used. Permanent
facial features like eyebrows, eyes, mouth and nose are extracted using SUSAN
edge detection operator, facial geometry, edge projection analysis. Experiments
are carried out on JAFFE facial expression database and gives better
performance in terms of 100% accuracy for training set and 95.26% accuracy for
test set."
"Electronic toll collection (ETC) system has been a common trend used for toll
collection on toll road nowadays. The implementation of electronic toll
collection allows vehicles to travel at low or full speed during the toll
payment, which help to avoid the traffic delay at toll road. One of the major
components of an electronic toll collection is the automatic vehicle detection
and classification (AVDC) system which is important to classify the vehicle so
that the toll is charged according to the vehicle classes. Vision-based vehicle
classification system is one type of vehicle classification system which adopt
camera as the input sensing device for the system. This type of system has
advantage over the rest for it is cost efficient as low cost camera is used.
The implementation of vision-based vehicle classification system requires lower
initial investment cost and very suitable for the toll collection trend
migration in Malaysia from single ETC system to full-scale multi-lane free flow
(MLFF). This project includes the development of an image-based vehicle
classification system as an effort to seek for a robust vision-based vehicle
classification system. The techniques used in the system include
scale-invariant feature transform (SIFT) technique, Canny's edge detector,
K-means clustering as well as Euclidean distance matching. In this project, a
unique way to image description as matching medium is proposed. This
distinctiveness of method is analogous to the human DNA concept which is highly
unique. The system is evaluated on open datasets and return promising results."
"The watershed is a powerful tool for segmenting objects whose contours appear
as crest lines on a gradient image. The watershed transform associates to a
topographic surface a partition into catchment basins, defined as attraction
zones of a drop of water falling on the relief and following a line of steepest
descent. Unfortunately, catchment basins may overlap and do not form a
partition. Moreover, current watershed algorithms, being shortsighted, do not
correctly estimate the steepness of the downwards trajectories and overestimate
the overlapping zones of catchment basins. An arbitrary division of these zones
between adjacent catchment basin results in a poor localization of the
contours. We propose an algorithm without myopia, which considers the total
length of a trajectory for estimating its steepness. We first consider
topographic surfaces defined on node weighted graphs. The graphs are pruned in
order to eliminate all downwards trajectories which are not the steepest. An
iterative algorithm with simple neighborhood operations performs the pruning
and constructs the catchment basins. The algorithm is then adapted to gray tone
images. The graph structure itself is encoded as an image thanks to the fixed
neighborhood structure of grids. A pair of adaptative erosions and dilations
prune the graph and extend the catchment basins. As a result one obtains a
precise detection of the catchment basins and a graph of the steepest
trajectories. A last iterative algorithm allows to follow selected downwards
trajectories in order to detect particular structures such as rivers or thalweg
lines of the topographic surface."
"This paper present our new intensity chromaticity space-based feature
detection and matching algorithm. This approach utilizes hybridization of
wireless local area network and camera internal sensor which to receive signal
strength from a access point and the same time retrieve interest point
information from hallways. This information is combined by model fitting
approach in order to find the absolute of user target position. No conventional
searching algorithm is required, thus it is expected reducing the computational
complexity. Finally we present pre-experimental results to illustrate the
performance of the localization system for an indoor environment set-up."
"Many User interactive systems are proposed all methods are trying to
implement as a user friendly and various approaches proposed but most of the
systems not reached to the use specifications like user friendly systems with
user interest, all proposed method implemented basic techniques some are
improved methods also propose but not reaching to the user specifications. In
this proposed paper we concentrated on image retrieval system with in early
days many user interactive systems performed with basic concepts but such
systems are not reaching to the user specifications and not attracted to the
user so a lot of research interest in recent years with new specifications,
recent approaches have user is interested in friendly interacted methods are
expecting, many are concentrated for improvement in all methods. In this
proposed system we focus on the retrieval of images within a large image
collection based on color projections and different mathematical approaches are
introduced and applied for retrieval of images. before Appling proposed methods
images are sub grouping using threshold values, in this paper R G B color
combinations considered for retrieval of images, in proposed methods are
implemented and results are included, through results it is observed that we
obtaining efficient results comparatively previous and existing."
"By coding a query sample as a sparse linear combination of all training
samples and then classifying it by evaluating which class leads to the minimal
coding residual, sparse representation based classification (SRC) leads to
interesting results for robust face recognition. It is widely believed that the
l1- norm sparsity constraint on coding coefficients plays a key role in the
success of SRC, while its use of all training samples to collaboratively
represent the query sample is rather ignored. In this paper we discuss how SRC
works, and show that the collaborative representation mechanism used in SRC is
much more crucial to its success of face classification. The SRC is a special
case of collaborative representation based classification (CRC), which has
various instantiations by applying different norms to the coding residual and
coding coefficient. More specifically, the l1 or l2 norm characterization of
coding residual is related to the robustness of CRC to outlier facial pixels,
while the l1 or l2 norm characterization of coding coefficient is related to
the degree of discrimination of facial features. Extensive experiments were
conducted to verify the face recognition accuracy and efficiency of CRC with
different instantiations."
"Most sparse linear representation-based trackers need to solve a
computationally expensive L1-regularized optimization problem. To address this
problem, we propose a visual tracker based on non-sparse linear
representations, which admit an efficient closed-form solution without
sacrificing accuracy. Moreover, in order to capture the correlation information
between different feature dimensions, we learn a Mahalanobis distance metric in
an online fashion and incorporate the learned metric into the optimization
problem for obtaining the linear representation. We show that online metric
learning using proximity comparison significantly improves the robustness of
the tracking, especially on those sequences exhibiting drastic appearance
changes. Furthermore, in order to prevent the unbounded growth in the number of
training samples for the metric learning, we design a time-weighted reservoir
sampling method to maintain and update limited-sized foreground and background
sample buffers for balancing sample diversity and adaptability. Experimental
results on challenging videos demonstrate the effectiveness and robustness of
the proposed tracker."
"With the advancement of communication and security technologies, it has
become crucial to have robustness of embedded biometric systems. This paper
presents the realization of such technologies which demands reliable and
error-free biometric identity verification systems. High dimensional patterns
are not permitted due to eigen-decomposition in high dimensional feature space
and degeneration of scattering matrices in small size sample. Generalization,
dimensionality reduction and maximizing the margins are controlled by
minimizing weight vectors. Results show good pattern by multimodal biometric
system proposed in this paper. This paper is aimed at investigating a biometric
identity system using Support Vector Machines(SVMs) and Lindear Discriminant
Analysis(LDA) with MFCCs and implementing such system in real-time using
SignalWAVE."
"This paper work directly towards the improving the quality of the image for
the digital cameras and other visual capturing products. In this Paper, the
authors clearly defines the problems occurs in the CFA image. A different
methodology for removing the noise is discuses in the paper for color
correction and color balancing of the image. At the same time, the authors also
proposed a new methodology of providing denoisiing process before the
demosaickingfor the improving the image quality of CFA which is much efficient
then the other previous defined. The demosaicking process for producing the
colors in the image in a best way is also discuss."
"Estimating pose of the head is an important preprocessing step in many
pattern recognition and computer vision systems such as face recognition. Since
the performance of the face recognition systems is greatly affected by the
poses of the face, how to estimate the accurate pose of the face in human face
image is still a challenging problem. In this paper, we represent a novel
method for head pose estimation. To enhance the efficiency of the estimation we
use contourlet transform for feature extraction. Contourlet transform is
multi-resolution, multi-direction transform. In order to reduce the feature
space dimension and obtain appropriate features we use LDA (Linear Discriminant
Analysis) and PCA (Principal Component Analysis) to remove ineffcient features.
Then, we apply different classifiers such as k-nearest neighborhood (knn) and
minimum distance. We use the public available FERET database to evaluate the
performance of proposed method. Simulation results indicate the superior
robustness of the proposed method."
"We present a novel approach to background subtraction that is based on the
local shape of small image regions. In our approach, an image region centered
on a pixel is mod-eled using the local self-similarity descriptor. We aim at
obtaining a reliable change detection based on local shape change in an image
when foreground objects are moving. The method first builds a background model
and compares the local self-similarities between the background model and the
subsequent frames to distinguish background and foreground objects.
Post-processing is then used to refine the boundaries of moving objects.
Results show that this approach is promising as the foregrounds obtained are
com-plete, although they often include shadows."
"Conventional edge-based active contours often require the normal component of
an edge indicator function on the optimal contours to approximate zero, while
the tangential component can still be significant. In real images, the full
gradients of the edge indicator function along the object boundaries are often
small. Hence, the curve evolution of edge-based active contours can terminate
early before converging to the object boundaries with a careless contour
initialization. We propose a novel Geodesic Snakes (GeoSnakes) active contour
that requires the full gradients of the edge indicator to vanish at the optimal
solution. Besides, the conventional curve evolution approach for minimizing
active contour energy cannot fully solve the Euler-Lagrange (EL) equation of
our GeoSnakes active contour, causing a Pseudo Stationary Phenomenon (PSP). To
address the PSP problem, we propose an auxiliary curve evolution equation,
named the equilibrium flow (EF) equation. Based on the EF and the conventional
curve evolution, we obtain a solution to the full EL equation of GeoSnakes
active contour. Experimental results validate the proposed geometrical
interpretation of the early termination problem, and they also show that the
proposed method overcomes the problem."
"Model based methods to marker-free motion capture have a very high
computational overhead that make them unattractive. In this paper we describe a
method that improves on existing global optimization techniques to tracking
articulated objects. Our method improves on the state-of-the-art Annealed
Particle Filter (APF) by reusing samples across annealing layers and by using
an adaptive parametric density for diffusion. We compare the proposed method
with APF on a scalable problem and study how the two methods scale with the
dimensionality, multi-modality and the range of search. Then we perform
sensitivity analysis on the parameters of our algorithm and show that it
tolerates a wide range of parameter settings. We also show results on tracking
human pose from the widely-used Human Eva I dataset. Our results show that the
proposed method reduces the tracking error despite using less than 50% of the
computational resources as APF. The tracked output also shows a significant
qualitative improvement over APF as demonstrated through image and video
results."
"Many images nowadays are captured from behind the glasses and may have
certain stains discrepancy because of glass and must be processed to make
differentiation between the glass and objects behind it. This research paper
proposes an algorithm to remove the damaged or corrupted part of the image and
make it consistent with other part of the image and to segment objects behind
the glass. The damaged part is removed using total variation inpainting method
and segmentation is done using kmeans clustering, anisotropic diffusion and
watershed transformation. The final output is obtained by interpolation. This
algorithm can be useful to applications in which some part of the images are
corrupted due to data transmission or needs to segment objects from an image
for further processing."
"Feature extraction is one of the fundamental problems of character
recognition. The performance of character recognition system is depends on
proper feature extraction and correct classifier selection. In this article, a
rapid feature extraction method is proposed and named as Celled Projection (CP)
that compute the projection of each section formed through partitioning an
image. The recognition performance of the proposed method is compared with
other widely used feature extraction methods that are intensively studied for
many different scripts in literature. The experiments have been conducted using
Bangla handwritten numerals along with three different well known classifiers
which demonstrate comparable results including 94.12% recognition accuracy
using celled projection."
"Principle Component Analysis PCA is a classical feature extraction and data
representation technique widely used in pattern recognition. It is one of the
most successful techniques in face recognition. But it has drawback of high
computational especially for big size database. This paper conducts a study to
optimize the time complexity of PCA (eigenfaces) that does not affects the
recognition performance. The authors minimize the participated eigenvectors
which consequently decreases the computational time. A comparison is done to
compare the differences between the recognition time in the original algorithm
and in the enhanced algorithm. The performance of the original and the enhanced
proposed algorithm is tested on face94 face database. Experimental results show
that the recognition time is reduced by 35% by applying our proposed enhanced
algorithm. DET Curves are used to illustrate the experimental results."
"The ultimate aim of handwriting recognition is to make computers able to read
and/or authenticate human written texts, with a performance comparable to or
even better than that of humans. Reading means that the computer is given a
piece of handwriting and it provides the electronic transcription of that (e.g.
in ASCII format). Two types of handwriting: on-line and offline. The most
important purpose of off-line handwriting recognition is in protection systems
and authentication. Arabic Handwriting scripts are much more complicated in
comparison to Latin scripts. This paper introduces a simple and novel
methodology to authenticate Arabic handwriting characters. Reaching our aim, we
built our own character database. The research methodology depends on two
stages: The first is character extraction where preprocessing the word and then
apply segmentation process to obtain the character. The second is the character
recognition by matching the characters comprising the word with the letters in
the database. Our results ensure character recognition with 81%. We eliminate
FAR by using similarity percent between 45-55%. Our research is coded using
MATLAB."
"This Paper Analyze the performance of Unsymmetrical trimmed median, which is
used as detector for the detection of impulse noise, Gaussian noise and mixed
noise is proposed. The proposed algorithm uses a fixed 3x3 window for the
increasing noise densities. The pixels in the current window are arranged in
sorting order using a improved snake like sorting algorithm with reduced
comparator. The processed pixel is checked for the occurrence of outliers, if
the absolute difference between processed pixels is greater than fixed
threshold. Under high noise densities the processed pixel is also noisy hence
the median is checked using the above procedure. if found true then the pixel
is considered as noisy hence the corrupted pixel is replaced by the median of
the current processing window. If median is also noisy then replace the
corrupted pixel with unsymmetrical trimmed median else if the pixel is termed
uncorrupted and left unaltered. The proposed algorithm (PA) is tested on
varying detail images for various noises. The proposed algorithm effectively
removes the high density fixed value impulse noise, low density random valued
impulse noise, low density Gaussian noise and lower proportion of mixed noise.
The proposed algorithm is targeted on Xc3e5000-5fg900 FPGA using Xilinx 7.1
compiler version which requires less number of slices, optimum speed and low
power when compared to the other median finding architectures."
"In this paper, we propose a novel family of windowing technique to compute
Mel Frequency Cepstral Coefficient (MFCC) for automatic speaker recognition
from speech. The proposed method is based on fundamental property of discrete
time Fourier transform (DTFT) related to differentiation in frequency domain.
Classical windowing scheme such as Hamming window is modified to obtain
derivatives of discrete time Fourier transform coefficients. It has been
mathematically shown that the slope and phase of power spectrum are inherently
incorporated in newly computed cepstrum. Speaker recognition systems based on
our proposed family of window functions are shown to attain substantial and
consistent performance improvement over baseline single tapered Hamming window
as well as recently proposed multitaper windowing technique."
"A new line of research uses compression methods to measure the similarity
between signals. Two signals are considered similar if one can be compressed
significantly when the information of the other is known. The existing
compression-based similarity methods, although successful in the discrete one
dimensional domain, do not work well in the context of images. This paper
proposes a sparse representation-based approach to encode the information
content of an image using information from the other image, and uses the
compactness (sparsity) of the representation as a measure of its
compressibility (how much can the image be compressed) with respect to the
other image. The more sparse the representation of an image, the better it can
be compressed and the more it is similar to the other image. The efficacy of
the proposed measure is demonstrated through the high accuracies achieved in
image clustering, retrieval and classification."
"Hierarchical image segmentation provides region-oriented scalespace, i.e., a
set of image segmentations at different detail levels in which the
segmentations at finer levels are nested with respect to those at coarser
levels. Most image segmentation algorithms, such as region merging algorithms,
rely on a criterion for merging that does not lead to a hierarchy, and for
which the tuning of the parameters can be difficult. In this work, we propose a
hierarchical graph based image segmentation relying on a criterion popularized
by Felzenzwalb and Huttenlocher. We illustrate with both real and synthetic
images, showing efficiency, ease of use, and robustness of our method."
"This paper discusses a novel method for Facial Expression Recognition System
which performs facial expression analysis in a near real time from a live web
cam feed. Primary objectives were to get results in a near real time with light
invariant, person independent and pose invariant way. The system is composed of
two different entities trainer and evaluator. Each frame of video feed is
passed through a series of steps including haar classifiers, skin detection,
feature extraction, feature points tracking, creating a learned Support Vector
Machine model to classify emotions to achieve a tradeoff between accuracy and
result rate. A processing time of 100-120 ms per 10 frames was achieved with
accuracy of around 60%. We measure our accuracy in terms of variety of
interaction and classification scenarios. We conclude by discussing relevance
of our work to human computer interaction and exploring further measures that
can be taken."
"We have shown that the left side null space of the autoregression (AR) matrix
operator is the lexicographical presentation of the point spread function (PSF)
on condition the AR parameters are common for original and blurred images. The
method of inverse PSF evaluation with regularization functional as the function
of surface area is offered. The inverse PSF was used for primary image
estimation. Two methods of original image estimate optimization were designed
basing on maximum entropy generalization of sought and blurred images
conditional probability density and regularization. The first method uses
balanced variations of convolution and deconvolution transforms to obtaining
iterative schema of image optimization. The variations balance was defined by
dynamic regularization basing on condition of iteration process convergence.
The regularization has dynamic character because depends on current and
previous image estimate variations. The second method implements the
regularization of deconvolution optimization in curved space with metric
defined on image estimate surface. It is basing on target functional invariance
to fluctuations of optimal argument value. The given iterative schemas have
faster convergence in comparison with known ones, so they can be used for
reconstruction of high resolution images series in real time."
"Here I suggest the use of a 3D scanning and rendering to create some virtual
copies of ancient artifacts to study and compare them. In particular, this
approach could be interesting for some roman marble busts, two of which are
portraits of Julius Caesar, and the third is a realistic portrait of a man
recently found at Arles, France. The comparison of some images indicates that a
three-dimensional visualization is necessary."
"This paper presents an approach to detect and track groups of people in
video-surveillance applications, and to automatically recognize their behavior.
This method keeps track of individuals moving together by maintaining a spacial
and temporal group coherence. First, people are individually detected and
tracked. Second, their trajectories are analyzed over a temporal window and
clustered using the Mean-Shift algorithm. A coherence value describes how well
a set of people can be described as a group. Furthermore, we propose a formal
event description language. The group events recognition approach is
successfully validated on 4 camera views from 3 datasets: an airport, a subway,
a shopping center corridor and an entrance hall."
"In many domains that involve the use of sensors, such as robotics or sensor
networks, there are opportunities to use some form of active sensing to
disambiguate data from noisy or unreliable sensors. These disambiguating
actions typically take time and expend energy. One way to choose the next
disambiguating action is to select the action with the greatest expected
entropy reduction, or information gain. In this work, we consider active
sensing in aid of stereo vision for robotics. Stereo vision is a powerful
sensing technique for mobile robots, but it can fail in scenes that lack strong
texture. In such cases, a structured light source, such as vertical laser line
can be used for disambiguation. By treating the stereo matching problem as a
specially structured HMM-like graphical model, we demonstrate that for a scan
line with n columns and maximum stereo disparity d, the entropy minimizing aim
point for the laser can be selected in O(nd) time - cost no greater than the
stereo algorithm itself. In contrast, a typical HMM formulation would suggest
at least O(nd^2) time for the entropy calculation alone."
"Human activities comprise several sub-activities performed in a sequence and
involve interactions with various objects. This makes reasoning about the
object affordances a central task for activity recognition. In this work, we
consider the problem of jointly labeling the object affordances and human
activities from RGB-D videos. We frame the problem as a Markov Random Field
where the nodes represent objects and sub-activities, and the edges represent
the relationships between object affordances, their relations with
sub-activities, and their evolution over time. We formulate the learning
problem using a structural SVM approach, where labeling over various alternate
temporal segmentations are considered as latent variables. We tested our method
on a dataset comprising 120 activity videos collected from four subjects, and
obtained an end-to-end precision of 81.8% and recall of 80.0% for labeling the
activities."
"Fingerprint reconstruction is one of the most well-known and publicized
biometrics. Because of their uniqueness and consistency over time, fingerprints
have been used for identification over a century, more recently becoming
automated due to advancements in computed capabilities. Fingerprint
reconstruction is popular because of the inherent ease of acquisition, the
numerous sources (e.g. ten fingers) available for collection, and their
established use and collections by law enforcement and immigration.
Fingerprints have always been the most practical and positive means of
identification. Offenders, being well aware of this, have been coming up with
ways to escape identification by that means. Erasing left over fingerprints,
using gloves, fingerprint forgery; are certain examples of methods tried by
them, over the years. Failing to prevent themselves, they moved to an extent of
mutilating their finger skin pattern, to remain unidentified. This article is
based upon obliteration of finger ridge patterns and discusses some known cases
in relation to the same, in chronological order; highlighting the reasons why
offenders go to an extent of performing such act. The paper gives an overview
of different methods and performance measurement of the fingerprint
reconstruction."
"Biometric time and attendance system is one of the most successful
applications of biometric technology. One of the main advantage of a biometric
time and attendance system is it avoids ""buddy-punching"". Buddy punching was a
major loophole which will be exploiting in the traditional time attendance
systems. Fingerprint recognition is an established field today, but still
identifying individual from a set of enrolled fingerprints is a time taking
process. Most fingerprint-based biometric systems store the minutiae template
of a user in the database. It has been traditionally assumed that the minutiae
template of a user does not reveal any information about the original
fingerprint. This belief has now been shown to be false; several algorithms
have been proposed that can reconstruct fingerprint images from minutiae
templates. In this paper, a novel fingerprint reconstruction algorithm is
proposed to reconstruct the phase image, which is then converted into the
grayscale image. The proposed reconstruction algorithm reconstructs the phase
image from minutiae. The proposed reconstruction algorithm is used to automate
the whole process of taking attendance, manually which is a laborious and
troublesome work and waste a lot of time, with its managing and maintaining the
records for a period of time is also a burdensome task. The proposed
reconstruction algorithm has been evaluated with respect to the success rates
of type-I attack (match the reconstructed fingerprint against the original
fingerprint) and type-II attack (match the reconstructed fingerprint against
different impressions of the original fingerprint) using a commercial
fingerprint recognition system. Given the reconstructed image from our
algorithm, we show that both types of attacks can be effectively launched
against a fingerprint recognition system."
"In the paper the optimal image segmentation by means of piecewise constant
approximations is considered. The optimality is defined by a minimum value of
the total squared error or by equivalent value of standard deviation of the
approximation from the image. The optimal approximations are defined
independently on the method of their obtaining and might be generated in
different algorithms. We investigate the computation of the optimal
approximation on the grounds of stability with respect to a given set of
modifications. To obtain the optimal approximation the Mumford-Shuh model is
generalized and developed, which in the computational part is combined with the
Otsu method in multi-thresholding version. The proposed solution is proved
analytically and experimentally on the example of the standard image."
"This paper presents the performance of different blockbased discrete cosine
transform (DCT) algorithms for compressing color image. In this RGB component
of color image are converted to YCbCr before DCT transform is applied. Y is
luminance component;Cb and Cr are chrominance components of the image. The
modification of the image data is done based on the classification of image
blocks to edge blocks and non-edge blocks, then the edge block of the image is
compressed with low compression and the nonedge blocks is compressed with high
compression. The analysis results have indicated that the performance of the
suggested method is much better, where the constructed images are less
distorted and compressed with higher factor."
"The paper presents two edge grouping algorithms for finding a closed contour
starting from a particular edge point and enclosing a fixation point. Both
algorithms search a shortest simple cycle in \textit{an angularly ordered
graph} derived from an edge image where a vertex is an end point of a contour
fragment and an undirected arc is drawn between a pair of end-points whose
visual angle from the fixation point is less than a threshold value, which is
set to $\pi/2$ in our experiments. The first algorithm restricts the search
space by disregarding arcs that cross the line extending from the fixation
point to the starting point. The second algorithm improves the solution of the
first algorithm in a greedy manner. The algorithms were tested with a large
number of natural images with manually placed fixation and starting points. The
results are promising."
"A copy-move forgery is created by copying and pasting content within the same
image, and potentially post-processing it. In recent years, the detection of
copy-move forgeries has become one of the most actively researched topics in
blind image forensics. A considerable number of different algorithms have been
proposed focusing on different types of postprocessed copies. In this paper, we
aim to answer which copy-move forgery detection algorithms and processing steps
(e.g., matching, filtering, outlier detection, affine transformation
estimation) perform best in various postprocessing scenarios. The focus of our
analysis is to evaluate the performance of previously proposed feature sets. We
achieve this by casting existing algorithms in a common pipeline. In this
paper, we examined the 15 most prominent feature sets. We analyzed the
detection performance on a per-image basis and on a per-pixel basis. We created
a challenging real-world copy-move dataset, and a software framework for
systematic image manipulation. Experiments show, that the keypoint-based
features SIFT and SURF, as well as the block-based DCT, DWT, KPCA, PCA and
Zernike features perform very well. These feature sets exhibit the best
robustness against various noise sources and downsampling, while reliably
identifying the copied regions."
"Extensive research efforts have been dedicated to 3D model retrieval in
recent decades. Recently, view-based methods have attracted much research
attention due to the high discriminative property of multi-views for 3D object
representation. In this report, we summarize the view-based 3D model methods
and provide the further research trends. This paper focuses on the scheme for
matching between multiple views of 3D models and the application of
bag-of-visual-words method in 3D model retrieval. For matching between multiple
views, the many-to-many matching, probabilistic matching and semisupervised
learning methods are introduced. For bag-of-visual-words application in 3D
model retrieval, we first briefly review the bag-of-visual-words works on
multimedia and computer vision tasks, where the visual dictionary has been
detailed introduced. Then a series of 3D model retrieval methods by using
bag-of-visual-words description are surveyed in this paper. At last, we
summarize the further research content in view-based 3D model retrieval."
"Recently, total variation (TV) based minimization algorithms have achieved
great success in compressive sensing (CS) recovery for natural images due to
its virtue of preserving edges. However, the use of TV is not able to recover
the fine details and textures, and often suffers from undesirable staircase
artifact. To reduce these effects, this letter presents an improved TV based
image CS recovery algorithm by introducing a new nonlocal regularization
constraint into CS optimization problem. The nonlocal regularization is built
on the well known nonlocal means (NLM) filtering and takes advantage of
self-similarity in images, which helps to suppress the staircase effect and
restore the fine details. Furthermore, an efficient augmented Lagrangian based
algorithm is developed to solve the above combined TV and nonlocal
regularization constrained problem. Experimental results demonstrate that the
proposed algorithm achieves significant performance improvements over the
state-of-the-art TV based algorithm in both PSNR and visual perception."
"Learning-based image super-resolution aims to reconstruct high-frequency (HF)
details from the prior model trained by a set of high- and low-resolution image
patches. In this paper, HF to be estimated is considered as a combination of
two components: main high-frequency (MHF) and residual high-frequency (RHF),
and we propose a novel image super-resolution method via dual-dictionary
learning and sparse representation, which consists of the main dictionary
learning and the residual dictionary learning, to recover MHF and RHF
respectively. Extensive experimental results on test images validate that by
employing the proposed two-layer progressive scheme, more image details can be
recovered and much better results can be achieved than the state-of-the-art
algorithms in terms of both PSNR and visual perception."
"Context categorization is a fundamental pre-requisite for multi-domain
multimedia content analysis applications in order to manage contextual
information in an efficient manner. In this paper, we introduce a new color
image context categorization method (DITEC) based on the trace transform. The
problem of dimensionality reduction of the obtained trace transform signal is
addressed through statistical descriptors that keep the underlying information.
These extracted features offer a highly discriminant behavior for content
categorization. The theoretical properties of the method are analyzed and
validated experimentally through two different datasets."
"This paper presents a novel approach to recognize Grantha, an ancient script
in South India and converting it to Malayalam, a prevalent language in South
India using online character recognition mechanism. The motivation behind this
work owes its credit to (i) developing a mechanism to recognize Grantha script
in this modern world and (ii) affirming the strong connection among Grantha and
Malayalam. A framework for the recognition of Grantha script using online
character recognition is designed and implemented. The features extracted from
the Grantha script comprises mainly of time-domain features based on writing
direction and curvature. The recognized characters are mapped to corresponding
Malayalam characters. The framework was tested on a bed of medium length
manuscripts containing 9-12 sample lines and printed pages of a book titled
Soundarya Lahari writtenin Grantha by Sri Adi Shankara to recognize the words
and sentences. The manuscript recognition rates with the system are for Grantha
as 92.11%, Old Malayalam 90.82% and for new Malayalam script 89.56%. The
recognition rates of pages of the printed book are for Grantha as 96.16%, Old
Malayalam script 95.22% and new Malayalam script as 92.32% respectively. These
results show the efficiency of the developed system."
"The most significant problem may be undesirable effects for the spectral
signatures of fused images as well as the benefits of using fused images mostly
compared to their source images were acquired at the same time by one sensor.
They may or may not be suitable for the fusion of other images. It becomes
therefore increasingly important to investigate techniques that allow
multi-sensor, multi-date image fusion to make final conclusions can be drawn on
the most suitable method of fusion. So, In this study we present a new method
Segmentation Fusion method (SF) for remotely sensed images is presented by
considering the physical characteristics of sensors, which uses a feature level
processing paradigm. In a particularly, attempts to test the proposed method
performance on 10 multi-sensor images and comparing it with different fusion
techniques for estimating the quality and degree of information improvement
quantitatively by using various spatial and spectral metrics."
"This article presents a new distance for measuring shape dissimilarity
between objects. Recent publications introduced the use of eigenvalues of the
Laplace operator as compact shape descriptors. Here, we revisit the eigenvalues
to define a proper distance, called Weighted Spectral Distance (WESD), for
quantifying shape dissimilarity. The definition of WESD is derived through
analysing the heat-trace. This analysis provides the proposed distance an
intuitive meaning and mathematically links it to the intrinsic geometry of
objects. We analyse the resulting distance definition, present and prove its
important theoretical properties. Some of these properties include: i) WESD is
defined over the entire sequence of eigenvalues yet it is guaranteed to
converge, ii) it is a pseudometric, iii) it is accurately approximated with a
finite number of eigenvalues, and iv) it can be mapped to the [0,1) interval.
Lastly, experiments conducted on synthetic and real objects are presented.
These experiments highlight the practical benefits of WESD for applications in
vision and medical image analysis."
"A framework for unsupervised group activity analysis from a single video is
here presented. Our working hypothesis is that human actions lie on a union of
low-dimensional subspaces, and thus can be efficiently modeled as sparse linear
combinations of atoms from a learned dictionary representing the action's
primitives. Contrary to prior art, and with the primary goal of spatio-temporal
action grouping, in this work only one single video segment is available for
both unsupervised learning and analysis without any prior training information.
After extracting simple features at a single spatio-temporal scale, we learn a
dictionary for each individual in the video during each short time lapse. These
dictionaries allow us to compare the individuals' actions by producing an
affinity matrix which contains sufficient discriminative information about the
actions in the scene leading to grouping with simple and efficient tools. With
diverse publicly available real videos, we demonstrate the effectiveness of the
proposed framework and its robustness to cluttered backgrounds, changes of
human appearance, and action variability."
"We have benchmarked the maximum obtainable recognition accuracy on various
word image datasets using manual segmentation and a currently available
commercial OCR. We have developed a Matlab program, with graphical user
interface, for semi-automated pixel level segmentation of word images. We
discuss the advantages of pixel level annotation. We have covered five
databases adding up to over 3600 word images. These word images have been
cropped from camera captured scene, born-digital and street view images. We
recognize the segmented word image using the trial version of Nuance Omnipage
OCR. We also discuss, how the degradations introduced during acquisition or
inaccuracies introduced during creation of word images affect the recognition
of the word present in the image. Word images for different kinds of
degradations and correction for slant and curvy nature of words are also
discussed. The word recognition rates obtained on ICDAR 2003, Sign evaluation,
Street view, Born-digital and ICDAR 2011 datasets are 83.9%, 89.3%, 79.6%,
88.5% and 86.7% respectively."
"In the feature classification domain, the choice of data affects widely the
results. For the Hyperspectral image, the bands dont all contain the
information; some bands are irrelevant like those affected by various
atmospheric effects, see Figure.4, and decrease the classification accuracy.
And there exist redundant bands to complicate the learning system and product
incorrect prediction [14]. Even the bands contain enough information about the
scene they may can't predict the classes correctly if the dimension of space
images, see Figure.3, is so large that needs many cases to detect the
relationship between the bands and the scene (Hughes phenomenon) [10]. We can
reduce the dimensionality of hyperspectral images by selecting only the
relevant bands (feature selection or subset selection methodology), or
extracting, from the original bands, new bands containing the maximal
information about the classes, using any functions, logical or numerical
(feature extraction methodology) [11][9]. Here we focus on the feature
selection using mutual information. Hyperspectral images have three advantages
regarding the multispectral images [6],"
"The paper evaluates the error performance of three random finite set based
multi-object trackers in the context of pedestrian video tracking. The
evaluation is carried out using a publicly available video dataset of 4500
frames (town centre street) for which the ground truth is available. The input
to all pedestrian tracking algorithms is an identical set of head and body
detections, obtained using the Histogram of Oriented Gradients (HOG) detector.
The tracking error is measured using the recently proposed OSPA metric for
tracks, adopted as the only known mathematically rigorous metric for measuring
the distance between two sets of tracks. A comparative analysis is presented
under various conditions."
"The incidence of thyroid nodule is very high and generally increases with the
age. Thyroid nodule may presage the emergence of thyroid cancer. The thyroid
nodule can be completely cured if detected early. Fine needle aspiration
cytology is a recognized early diagnosis method of thyroid nodule. There are
still some limitations in the fine needle aspiration cytology, and the
ultrasound diagnosis of thyroid nodule has become the first choice for
auxiliary examination of thyroid nodular disease. If we could combine medical
imaging technology and fine needle aspiration cytology, the diagnostic rate of
thyroid nodule would be improved significantly. The properties of ultrasound
will degrade the image quality, which makes it difficult to recognize the edges
for physicians. Image segmentation technique based on graph theory has become a
research hotspot at present. Normalized cut (Ncut) is a representative one,
which is suitable for segmentation of feature parts of medical image. However,
how to solve the normalized cut has become a problem, which needs large memory
capacity and heavy calculation of weight matrix. It always generates over
segmentation or less segmentation which leads to inaccurate in the
segmentation. The speckle noise in B ultrasound image of thyroid tumor makes
the quality of the image deteriorate. In the light of this characteristic, we
combine the anisotropic diffusion model with the normalized cut in this paper.
After the enhancement of anisotropic diffusion model, it removes the noise in
the B ultrasound image while preserves the important edges and local details.
This reduces the amount of computation in constructing the weight matrix of the
improved normalized cut and improves the accuracy of the final segmentation
results. The feasibility of the method is proved by the experimental results."
"Remote sensing is a technology to acquire data for disatant substances,
necessary to construct a model knowledge for applications as classification.
Recently Hyperspectral Images (HSI) becomes a high technical tool that the main
goal is to classify the point of a region. The HIS is more than a hundred
bidirectional measures, called bands (or simply images), of the same region
called Ground Truth Map (GT). But some bands are not relevant because they are
affected by different atmospheric effects; others contain redundant
information; and high dimensionality of HSI features make the accuracy of
classification lower. All these bands can be important for some applications;
but for the classification a small subset of these is relevant. The problematic
related to HSI is the dimensionality reduction. Many studies use mutual
information (MI) to select the relevant bands. Others studies use the MI
normalized forms, like Symmetric Uncertainty, in medical imagery applications.
In this paper we introduce an algorithm based also on MI to select relevant
bands and it apply the Symmetric Uncertainty coefficient to control redundancy
and increase the accuracy of classification. This algorithm is feature
selection tool and a Filter strategy. We establish this study on HSI AVIRIS
92AV3C. This is an effectiveness, and fast scheme to control redundancy."
"Radon Transformation is generally used to construct optical image (like CT
image) from the projection data in biomedical imaging. In this paper, the
concept of Radon Transformation is implemented to reconstruct Electrical
Impedance Topographic Image (conductivity or resistivity distribution) of a
circular subject. A parallel resistance model of a subject is proposed for
Electrical Impedance Topography(EIT) or Magnetic Induction Tomography(MIT). A
circular subject with embedded circular objects is segmented into equal width
slices from different angles. For each angle, Conductance and Conductivity of
each slice is calculated and stored in an array. A back projection method is
used to generate a two-dimensional image from one-dimensional projections. As a
back projection method, Inverse Radon Transformation is applied on the
calculated conductance and conductivity to reconstruct two dimensional images.
These images are compared to the target image. In the time of image
reconstruction, different filters are used and these images are compared with
each other and target image."
"Motion capture is the process of recording the movement of objects or people.
It is used in military, entertainment, sports, and medical applications, and
for validation of computer vision[2] and robotics. In filmmaking and video game
development, it refers to recording actions of human actors, and using that
information to animate digital character models in 2D or 3D computer animation.
When it includes face and fingers or captures subtle"
"Image computing has become a real catchphrase over the past few years and the
interpretations of the meaning of the term vary greatly. The Imagecomputing
market is currently rapidly evolving with high growth prospects and almost
daily announcements of new devices and application platforms, which results in
an increasing diversification of devices, operating system and development
platforms. Compared to more traditional information technology markets like the
one of desktop computing, mobile computing is much less consolidated and
neither standards nor even industry standards have yet been established. There
are various platforms and interfaces which may be used to perform the desired
tasks through the device. We have tried to compare the various mobile operating
systems and their trade-offs."
"Non-Local Means (NLM) and variants have been proven to be effective and
robust in many image denoising tasks. In this letter, we study the parameter
selection problem of center pixel weights (CPW) in NLM. Our key contributions
are: 1) we give a novel formulation of the CPW problem from the statistical
shrinkage perspective; 2) we introduce the James-Stein type CPWs for NLM; and
3) we propose a new adaptive CPW that is locally tuned for each image pixel.
Our experimental results showed that compared to existing CPW solutions, the
new proposed CPWs are more robust and effective under various noise levels. In
particular, the NLM with the James-Stein type CPWs attain higher means with
smaller variances in terms of the peak signal and noise ratio, implying they
improve the NLM robustness and make it less sensitive to parameter selection."
"We pose 3D scene-understanding as a problem of parsing in a grammar. A
grammar helps us capture the compositional structure of real-word objects,
e.g., a chair is composed of a seat, a back-rest and some legs. Having multiple
rules for an object helps us capture structural variations in objects, e.g., a
chair can optionally also have arm-rests. Finally, having rules to capture
composition at different levels helps us formulate the entire scene-processing
pipeline as a single problem of finding most likely parse-tree---small segments
combine to form parts of objects, parts to objects and objects to a scene. We
attach a generative probability model to our grammar by having a
feature-dependent probability function for every rule. We evaluated it by
extracting labels for every segment and comparing the results with the
state-of-the-art segment-labeling algorithm. Our algorithm was outperformed by
the state-or-the-art method. But, Our model can be trained very efficiently
(within seconds), and it scales only linearly in with the number of rules in
the grammar. Also, we think that this is an important problem for the 3D vision
community. So, we are releasing our dataset and related code."
"This paper is practically interested in the unchangeable feature of Arabic
handwritten character. It presents results of comparative study achieved on
certain features extraction techniques of handwritten character, based on Hough
transform, Fourier transform, Wavelet transform and Gabor Filter. Obtained
results show that Hough Transform and Gabor filter are insensible to the
rotation and translation, Fourier Transform is sensible to the rotation but
insensible to the translation, in contrast to Hough Transform and Gabor filter,
Wavelets Transform is sensitive to the rotation as well as to the translation."
"We present an efficient and accurate algorithm for principal component
analysis (PCA) of a large set of two dimensional images, and, for each image,
the set of its uniform rotations in the plane and its reflection. The algorithm
starts by expanding each image, originally given on a Cartesian grid, in the
Fourier-Bessel basis for the disk. Because the images are bandlimited in the
Fourier domain, we use a sampling criterion to truncate the Fourier-Bessel
expansion such that the maximum amount of information is preserved without the
effect of aliasing. The constructed covariance matrix is invariant to rotation
and reflection and has a special block diagonal structure. PCA is efficiently
done for each block separately. This Fourier-Bessel based PCA detects more
meaningful eigenimages and has improved denoising capability compared to
traditional PCA for a finite number of noisy images."
"In this paper, we propose a novel architecture of wavelet network called
Multi-input Multi-output Wavelet Network MIMOWN as a generalization of the old
architecture of wavelet network. This newel prototype was applied to speech
recognition application especially to model acoustic unit of speech. The
originality of our work is the proposal of MIMOWN to model acoustic unit of
speech. This approach was proposed to overcome limitation of old wavelet
network model. The use of the multi-input multi-output architecture will allows
training wavelet network on various examples of acoustic units."
"Segmentation-based image coding methods provide high compression ratios when
compared with traditional image coding approaches like the transform and sub
band coding for low bit-rate compression applications. In this paper, a
segmentation-based image coding method, namely the Binary Space Partition
scheme, that divides the desired image using a recursive procedure for coding
is presented. The BSP approach partitions the desired image recursively by
using bisecting lines, selected from a collection of discrete optional lines,
in a hierarchical manner. This partitioning procedure generates a binary tree,
which is referred to as the BSP-tree representation of the desired image. The
algorithm is extremely complex in computation and has high execution time. The
time complexity of the BSP scheme is explored in this work."
"In this paper, we propose a novel technique to reconstruct 3D surface of an
underwater object using stereo images. Reconstructing the 3D surface of an
underwater object is really a challenging task due to degraded quality of
underwater images. There are various reason of quality degradation of
underwater images i.e., non-uniform illumination of light on the surface of
objects, scattering and absorption effects. Floating particles present in
underwater produces Gaussian noise on the captured underwater images which
degrades the quality of images. The degraded underwater images are preprocessed
by applying homomorphic, wavelet denoising and anisotropic filtering
sequentially. The uncalibrated rectification technique is applied to
preprocessed images to rectify the left and right images. The rectified left
and right image lies on a common plane. To find the correspondence points in a
left and right images, we have applied dense stereo matching technique i.e.,
graph cut method. Finally, we estimate the depth of images using triangulation
technique. The experimental result shows that the proposed method reconstruct
3D surface of underwater objects accurately using captured underwater stereo
images."
"This paper describes a method to localise all those areas which may
constitute the date field in an Indian handwritten document. Spatial patterns
of the date field are studied from various handwritten documents and an
algorithm is developed through statistical analysis to identify those sets of
connected components which may constitute the date. Common date patterns
followed in India are considered to classify the date formats in different
classes. Reported results demonstrate promising performance of the proposed
approach"
"In this paper we propose a robust approach for text extraction and
recognition from video clips which is called Neuro-Fuzzy system for Arabic
Video OCR. In Arabic video text recognition, a number of noise components
provide the text relatively more complicated to separate from the background.
Further, the characters can be moving or presented in a diversity of colors,
sizes and fonts that are not uniform. Added to this, is the fact that the
background is usually moving making text extraction a more intricate process.
Video include two kinds of text, scene text and artificial text. Scene text is
usually text that becomes part of the scene itself as it is recorded at the
time of filming the scene. But artificial text is produced separately and away
from the scene and is laid over it at a later stage or during the post
processing time. The emergence of artificial text is consequently vigilantly
directed. This type of text carries with it important information that helps in
video referencing, indexing and retrieval."
"Edge detection is one of the most critical tasks in automatic image analysis.
There exists no universal edge detection method which works well under all
conditions. This paper shows the new approach based on the one of the most
efficient techniques for edge detection, which is entropy-based thresholding.
The main advantages of the proposed method are its robustness and its
flexibility. We present experimental results for this method, and compare
results of the algorithm against several leading edge detection methods, such
as Canny, LOG, and Sobel. Experimental results demonstrate that the proposed
method achieves better result than some classic methods and the quality of the
edge detector of the output images is robust and decrease the computation time."
"Edge detection is an important field in image processing. Edges characterize
object boundaries and are therefore useful for segmentation, registration,
feature extraction, and identification of objects in a scene. In this paper, an
approach utilizing an improvement of Baljit and Amar method which uses Shannon
entropy other than the evaluation of derivatives of the image in detecting
edges in gray level images has been proposed. The proposed method can reduce
the CPU time required for the edge detection process and the quality of the
edge detector of the output images is robust. A standard test images, the
real-world and synthetic images are used to compare the results of the proposed
edge detector with the Baljit and Amar edge detector method. In order to
validate the results, the run time of the proposed method and the pervious
method are presented. It has been observed that the proposed edge detector
works effectively for different gray scale digital images. The performance
evaluation of the proposed technique in terms of the measured CPU time and the
quality of edge detector method are presented. Experimental results demonstrate
that the proposed method achieve better result than the relevant classic
method."
"Large high-dimensional datasets are becoming more and more popular in an
increasing number of research areas. Processing the high dimensional data
incurs a high computational cost and is inherently inefficient since many of
the values that describe a data object are redundant due to noise and inner
correlations. Consequently, the dimensionality, i.e. the number of values that
are used to describe a data object, needs to be reduced prior to any other
processing of the data. The dimensionality reduction removes, in most cases,
noise from the data and reduces substantially the computational cost of
algorithms that are applied to the data.
  In this thesis, a novel coherent integrated methodology is introduced
(theory, algorithm and applications) to reduce the dimensionality of
high-dimensional datasets. The method constructs a diffusion process among the
data coordinates via a random walk. The dimensionality reduction is obtained
based on the eigen-decomposition of the Markov matrix that is associated with
the random walk. The proposed method is utilized for: (a) segmentation and
detection of anomalies in hyper-spectral images; (b) segmentation of
multi-contrast MRI images; and (c) segmentation of video sequences.
  We also present algorithms for: (a) the characterization of materials using
their spectral signatures to enable their identification; (b) detection of
vehicles according to their acoustic signatures; and (c) classification of
vascular vessels recordings to detect hyper-tension and cardio-vascular
diseases.
  The proposed methodology and algorithms produce excellent results that
successfully compete with current state-of-the-art algorithms."
"We present a method for recognition of isolated Swedish Sign Language signs.
The method will be used in a game intended to help children training signing at
home, as a complement to training with a teacher. The target group is not
primarily deaf children, but children with language disorders. Using sign
language as a support in conversation has been shown to greatly stimulate the
speech development of such children. The signer is captured with an RGB-D
(Kinect) sensor, which has three advantages over a regular RGB camera. Firstly,
it allows complex backgrounds to be removed easily. We segment the hands and
face based on skin color and depth information. Secondly, it helps with the
resolution of hand over face occlusion. Thirdly, signs take place in 3D; some
aspects of the signs are defined by hand motion vertically to the image plane.
This motion can be estimated if the depth is observable. The 3D motion of the
hands relative to the torso are used as a cue together with the hand shape, and
HMMs trained with this input are used for classification. To obtain higher
robustness towards differences across signers, Fisher Linear Discriminant
Analysis is used to find the combinations of features that are most descriptive
for each sign, regardless of signer. Experiments show that the system can
distinguish signs from a challenging 94 word vocabulary with a precision of up
to 94% in the signer dependent case and up to 47% in the signer independent
case."
"It was recently demonstrated in [Chaudhury et al.,Non-Local Euclidean
Medians,2012] that the denoising performance of Non-Local Means (NLM) can be
improved at large noise levels by replacing the mean by the robust Euclidean
median. Numerical experiments on synthetic and natural images showed that the
latter consistently performed better than NLM beyond a certain noise level, and
significantly so for images with sharp edges. The Euclidean mean and median can
be put into a common regression (on the patch space) framework, in which the
l_2 norm of the residuals is considered in the former, while the l_1 norm is
considered in the latter. The natural question then is what happens if we
consider l_p (0<p<1) regression? We investigate this possibility in this paper."
"In this article, we address the issue of recovering latent transparent layers
from superimposition images. Here, we assume we have the estimated
transformations and extracted gradients of latent layers. To rapidly recover
high-quality image layers, we propose an Efficient Superimposition Recovering
Algorithm (ESRA) by extending the framework of accelerated gradient method. In
addition, a key building block (in each iteration) in our proposed method is
the proximal operator calculating. Here we propose to employ a dual approach
and present our Parallel Algorithm with Constrained Total Variation (PACTV)
method. Our recovering method not only reconstructs high-quality layers without
color-bias problem, but also theoretically guarantees good convergence
performance."
"Depth image based rendering techniques for multiview applications have been
recently introduced for efficient view generation at arbitrary camera
positions. Encoding rate control has thus to consider both texture and depth
data. Due to different structures of depth and texture images and their
different roles on the rendered views, distributing the available bit budget
between them however requires a careful analysis. Information loss due to
texture coding affects the value of pixels in synthesized views while errors in
depth information lead to shift in objects or unexpected patterns at their
boundaries. In this paper, we address the problem of efficient bit allocation
between textures and depth data of multiview video sequences. We adopt a
rate-distortion framework based on a simplified model of depth and texture
images. Our model preserves the main features of depth and texture images.
Unlike most recent solutions, our method permits to avoid rendering at encoding
time for distortion estimation so that the encoding complexity is not
augmented. In addition to this, our model is independent of the underlying
inpainting method that is used at decoder. Experiments confirm our theoretical
results and the efficiency of our rate allocation strategy."
"This paper addresses how to construct features for the problem of image
correspondence, in particular, the paper addresses how to construct features so
as to maintain the right level of invariance versus discriminability. We show
that without additional prior knowledge of the 3D scene, the right tradeoff
cannot be established in a pre-processing step of the images as is typically
done in most feature-based matching methods. However, given knowledge of the
second image to match, the tradeoff between invariance and discriminability of
features in the first image is less ambiguous. This suggests to setup the
problem of feature extraction and matching as a joint estimation problem. We
develop a possible mathematical framework, a possible computational algorithm,
and we give example demonstration on finding correspondence on images related
by a scene that undergoes large 3D deformation of non-planar objects and camera
viewpoint change."
"Cobb angle, which is a measure of spinal curvature is the standard method for
quantifying the magnitude of Scoliosis related to spinal deformity in
orthopedics. Determining the Cobb angle through manual process is subject to
human errors. In this work, we propose a methodology to measure the magnitude
of Cobb angle, which appreciably reduces the variability related to its
measurement compared to the related works. The proposed methodology is
facilitated by using a suitable new improved version of Non-Local Means for
image denoisation and Otsus automatic threshold selection for Canny edge
detection. We have selected NLM for preprocessing of the image as it is one of
the fine states of art for image denoisation and helps in retaining the image
quality. Trimmedmean, median are more robust to outliners than mean and
following this concept we observed that NLM denoising quality performance can
be enhanced by using Euclidean trimmed-mean replacing the mean. To prove the
better performance of the Non-Local Euclidean Trimmed-mean denoising filter, we
have provided some comparative study results of the proposed denoising
technique with traditional NLM and NonLocal Euclidean Medians. The experimental
results for Cobb angle measurement over intra observer and inter observer
experimental data reveals the better performance and superiority of the
proposed approach compared to the related works. MATLAB2009b image processing
toolbox was used for the purpose of simulation and verification of the proposed
methodology."
"The problem of finding elliptical shapes in an image will be considered. We
discuss the solution which uses cross-entropy clustering. The proposed method
allows the search for ellipses with predefined sizes and position in the space.
Moreover, it works well for search of ellipsoids in higher dimensions."
"Hyperspectral images contain mixed pixels due to low spatial resolution of
hyperspectral sensors. Mixed pixels are pixels containing more than one
distinct material called endmembers. The presence percentages of endmembers in
mixed pixels are called abundance fractions. Spectral unmixing problem refers
to decomposing these pixels into a set of endmembers and abundance fractions.
Due to nonnegativity constraint on abundance fractions, nonnegative matrix
factorization methods (NMF) have been widely used for solving spectral unmixing
problem. In this paper we have used graph regularized (GNMF) method with
sparseness constraint to unmix hyperspectral data. This method applied on
simulated data using AVIRIS Indian Pines dataset and USGS library and results
are quantified based on AAD and SAD measures. Results in comparison with other
methods show that the proposed method can unmix data more effectively."
"Image Segmentation is a technique of partitioning the original image into
some distinct classes. Many possible solutions may be available for segmenting
an image into a certain number of classes, each one having different quality of
segmentation. In our proposed method, multilevel thresholding technique has
been used for image segmentation. A new approach of Cuckoo Search (CS) is used
for selection of optimal threshold value. In other words, the algorithm is used
to achieve the best solution from the initial random threshold values or
solutions and to evaluate the quality of a solution correlation function is
used. Finally, MSE and PSNR are measured to understand the segmentation
quality."
"Compressed Sensing (CS) takes advantage of signal sparsity or compressibility
and allows superb signal reconstruction from relatively few measurements. Based
on CS theory, a suitable dictionary for sparse representation of the signal is
required. In diffusion MRI (dMRI), CS methods were proposed to reconstruct
diffusion-weighted signal and the Ensemble Average Propagator (EAP), and there
are two kinds of Dictionary Learning (DL) methods: 1) Discrete Representation
DL (DR-DL), and 2) Continuous Representation DL (CR-DL). DR-DL is susceptible
to numerical inaccuracy owing to interpolation and regridding errors in a
discretized q-space. In this paper, we propose a novel CR-DL approach, called
Dictionary Learning - Spherical Polar Fourier Imaging (DL-SPFI) for effective
compressed-sensing reconstruction of the q-space diffusion-weighted signal and
the EAP. In DL-SPFI, an dictionary that sparsifies the signal is learned from
the space of continuous Gaussian diffusion signals. The learned dictionary is
then adaptively applied to different voxels using a weighted LASSO framework
for robust signal reconstruction. The adaptive dictionary is proved to be
optimal. Compared with the start-of-the-art CR-DL and DR-DL methods proposed by
Merlet et al. and Bilgic et al., espectively, our work offers the following
advantages. First, the learned dictionary is proved to be optimal for Gaussian
diffusion signals. Second, to our knowledge, this is the first work to learn a
voxel-adaptive dictionary. The importance of the adaptive dictionary in EAP
reconstruction will be demonstrated theoretically and empirically. Third,
optimization in DL-SPFI is only performed in a small subspace resided by the
SPF coefficients, as opposed to the q-space approach utilized by Merlet et al.
The experiment results demonstrate the advantages of DL-SPFI over the original
SPF basis and Bilgic et al.'s method."
"Imaging has occupied a huge role in the management of patients, whether
hospitalized or not. Depending on the patients clinical problem, a variety of
imaging modalities were available for use. This gave birth of the annotation of
medical image process. The annotation is intended to image analysis and solve
the problem of semantic gap. The reason for image annotation is due to increase
in acquisition of images. Physicians and radiologists feel better while using
annotation techniques for faster remedy in surgery and medicine due to the
following reasons: giving details to the patients, searching the present and
past records from the larger databases, and giving solutions to them in a
faster and more accurate way. However, classical conceptual modeling does not
incorporate the specificity of medical domain specially the annotation of
medical image. The design phase is the most important activity in the
successful building of annotation process. For this reason, we focus in this
paper on presenting the conceptual modeling of the annotation of medical image
by defining a new profile using the StarUML extensibility mechanism."
"As an extension of projective homology, stereohomology is proposed via an
extension of Desargues theorem and the extended Desargues configuration.
Geometric transformations such as reflection, translation, central symmetry,
central projection, parallel projection, shearing, central dilation, scaling,
and so on are all included in stereohomology and represented as
Householder-Chen elementary matrices. Hence all these geometric transformations
are called elementary. This makes it possible to represent these elementary
geometric transformations in homogeneous square matrices independent of a
particular choice of coordinate system."
"A set label disagreement function is defined over the number of variables
that deviates from the dominant label. The dominant label is the value assumed
by the largest number of variables within a set of binary variables. The
submodularity of a certain family of set label disagreement function is
discussed in this manuscript. Such disagreement function could be utilized as a
cost function in combinatorial optimization approaches for problems defined
over hypergraphs."
"Illumination variation remains a central challenge in object detection and
recognition. Existing analyses of illumination variation typically pertain to
convex, Lambertian objects, and guarantee quality of approximation in an
average case sense. We show that it is possible to build V(vertex)-description
convex cone models with worst-case performance guarantees, for non-convex
Lambertian objects. Namely, a natural verification test based on the angle to
the constructed cone guarantees to accept any image which is sufficiently
well-approximated by an image of the object under some admissible lighting
condition, and guarantees to reject any image that does not have a sufficiently
good approximation. The cone models are generated by sampling point
illuminations with sufficient density, which follows from a new perturbation
bound for point images in the Lambertian model. As the number of point images
required for guaranteed verification may be large, we introduce a new
formulation for cone preserving dimensionality reduction, which leverages tools
from sparse and low-rank decomposition to reduce the complexity, while
controlling the approximation error with respect to the original cone."
"Remote sensing has proven to be a powerful tool for the monitoring of the
Earth surface to improve our perception of our surroundings has led to
unprecedented developments in sensor and information technologies. However,
technologies for effective use of the data and for extracting useful
information from the data of Remote sensing are still very limited since no
single sensor combines the optimal spectral, spatial and temporal resolution.
This paper briefly reviews the limitations of satellite remote sensing. Also,
reviews on the problems of image fusion techniques. The conclusion of this,
According to literature, the remote sensing is still the lack of software tools
for effective information extraction from remote sensing data. The trade-off in
spectral and spatial resolution will remain and new advanced data fusion
approaches are needed to make optimal use of remote sensors for extract the
most useful information."
"Several remote sensing software packages are used to the explicit purpose of
analyzing and visualizing remotely sensed data, with the developing of remote
sensing sensor technologies from last ten years. Accord-ing to literature, the
remote sensing is still the lack of software tools for effective information
extraction from remote sensing data. So, this paper provides a state-of-art of
multi-sensor image fusion technologies as well as review on the quality
evaluation of the single image or fused images in the commercial remote sensing
pack-ages. It also introduces program (ALwassaiProcess) developed for image
fusion and classification."
"The automatic segmentation of human knee cartilage from 3D MR images is a
useful yet challenging task due to the thin sheet structure of the cartilage
with diffuse boundaries and inhomogeneous intensities. In this paper, we
present an iterative multi-class learning method to segment the femoral, tibial
and patellar cartilage simultaneously, which effectively exploits the spatial
contextual constraints between bone and cartilage, and also between different
cartilages. First, based on the fact that the cartilage grows in only certain
area of the corresponding bone surface, we extract the distance features of not
only to the surface of the bone, but more informatively, to the densely
registered anatomical landmarks on the bone surface. Second, we introduce a set
of iterative discriminative classifiers that at each iteration, probability
comparison features are constructed from the class confidence maps derived by
previously learned classifiers. These features automatically embed the semantic
context information between different cartilages of interest. Validated on a
total of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, the
proposed approach demonstrates high robustness and accuracy of segmentation in
comparison with existing state-of-the-art MR cartilage segmentation methods."
"The Braille system has been used by the visually impaired for reading and
writing. Due to limited availability of the Braille text books an efficient
usage of the books becomes a necessity. This paper proposes a method to convert
a scanned Braille document to text which can be read out to many through the
computer. The Braille documents are pre processed to enhance the dots and
reduce the noise. The Braille cells are segmented and the dots from each cell
is extracted and converted in to a number sequence. These are mapped to the
appropriate alphabets of the language. The converted text is spoken out through
a speech synthesizer. The paper also provides a mechanism to type the Braille
characters through the number pad of the keyboard. The typed Braille character
is mapped to the alphabet and spoken out. The Braille cell has a standard
representation but the mapping differs for each language. In this paper mapping
of English, Hindi and Tamil are considered."
"Conditional Random Fields (CRF) are among the most popular techniques for
image labelling because of their flexibility in modelling dependencies between
the labels and the image features. This paper proposes a novel CRF-framework
for image labeling problems which is capable to classify partially occluded
objects. Our approach is evaluated on aerial near-vertical images as well as on
urban street-view images and compared with another methods."
"Histogram Equalization (HE) has been an essential addition to the Image
Enhancement world. Enhancement techniques like Classical Histogram Equalization
(CHE), Adaptive Histogram Equalization (ADHE), Bi-Histogram Equalization (BHE)
and Recursive Mean Separate Histogram Equalization (RMSHE) methods enhance
contrast, however, brightness is not well preserved with these methods, which
gives an unpleasant look to the final image obtained. Thus, we introduce a
novel technique Multi-Decomposition Histogram Equalization (MDHE) to eliminate
the drawbacks of the earlier methods. In MDHE, we have decomposed the input
sixty-four parts, applied CHE in each of the sub-images and then finally
interpolated them in correct order. The final image after MDHE results in
contrast enhanced and brightness preserved image compared to all other
techniques mentioned above. We have calculated the various parameters like
PSNR, SNR, RMSE, MSE, etc. for every technique. Our results are well supported
by bar graphs, histograms and the parameter calculations at the end."
"Fiber tracking based on diffusion weighted Magnetic Resonance Imaging (dMRI)
allows for noninvasive reconstruction of fiber bundles in the human brain. In
this chapter, we discuss sources of error and uncertainty in this technique,
and review strategies that afford a more reliable interpretation of the
results. This includes methods for computing and rendering probabilistic
tractograms, which estimate precision in the face of measurement noise and
artifacts. However, we also address aspects that have received less attention
so far, such as model selection, partial voluming, and the impact of
parameters, both in preprocessing and in fiber tracking itself. We conclude by
giving impulses for future research."
"This study is a part of design of an audio system for in-house object
detection system for visually impaired, low vision personnel by birth or by an
accident or due to old age. The input of the system will be scene and output as
audio. Alert facility is provided based on severity levels of the objects
(snake, broke glass etc) and also during difficulties. The study proposed
techniques to provide speedy detection of objects based on shapes and its
scale. Features are extraction to have minimum spaces using dynamic scaling.
From a scene, clusters of objects are formed based on the scale and shape.
Searching is performed among the clusters initially based on the shape, scale,
mean cluster value and index of object(s). The minimum operation to detect the
possible shape of the object is performed. In case the object does not have a
likely matching shape, scale etc, then the several operations required for an
object detection will not perform; instead, it will declared as a new object.
In such way, this study finds a speedy way of detecting objects."
"A non-iterative auto-calibration algorithm is presented. It deals with a
minimal set of six scene points in three views taken by a camera with fixed but
unknown intrinsic parameters. Calibration is based on the image correspondences
only. The algorithm is implemented and validated on synthetic image data."
"Image segmentation is a crucial step in a wide range of method image
processing systems. It is useful in visualization of the different objects
present in the image. In spite of the several methods available in the
literature, image segmentation still a challenging problem in most of image
processing applications. The challenge comes from the fuzziness of image
objects and the overlapping of the different regions. Detection of edges in an
image is a very important step towards understanding image features. There are
large numbers of edge detection operators available, each designed to be
sensitive to certain types of edges. The Quality of edge detection can be
measured from several criteria objectively. Some criteria are proposed in terms
of mathematical measurement, some of them are based on application and
implementation requirements. Since edges often occur at image locations
representing object boundaries, edge detection is extensively used in image
segmentation when images are divided into areas corresponding to different
objects. This can be used specifically for enhancing the tumor area in
mammographic images. Different methods are available for edge detection like
Roberts, Sobel, Prewitt, Canny, Log edge operators. In this paper a novel
algorithms for edge detection has been proposed for mammographic images. Breast
boundary, pectoral region and tumor location can be seen clearly by using this
method. For comparison purpose Roberts, Sobel, Prewitt, Canny, Log edge
operators are used and their results are displayed. Experimental results
demonstrate the effectiveness of the proposed approach."
"Feature means countenance, remote sensing scene objects with similar
characteristics, associated to interesting scene elements in the image
formation process. They are classified into three types in image processing,
that is low, middle and high. Low level features are color, texture and middle
level feature is shape and high level feature is semantic gap of objects. An
image retrieval system is a computer system for browsing, searching and
retrieving images from a large image database. Content Based Image Retrieval is
a technique which uses visual features of image such as color, shape, texture
to search user required image from large image database according to user
requests in the form of a query. MKNN is an enhancing method of KNN. The
proposed KNN classification is called MKNN. MKNN contains two parts for
processing, they are validity of the train samples and applying weighted KNN.
The validity of each point is computed according to its neighbors. In our
proposal, Modified K-Nearest Neighbor can be considered a kind of weighted KNN
so that the query label is approximated by weighting the neighbors of the
query."
"Text in video is useful and important in indexing and retrieving the video
documents efficiently and accurately. In this paper, we present a new method of
text detection using a combined dictionary consisting of wavelets and a
recently introduced transform called shearlets. Wavelets provide optimally
sparse expansion for point-like structures and shearlets provide optimally
sparse expansions for curve-like structures. By combining these two features we
have computed a high frequency sub-band to brighten the text part. Then K-means
clustering is used for obtaining text pixels from the Standard Deviation (SD)
of combined coefficient of wavelets and shearlets as well as the union of
wavelets and shearlets features. Text parts are obtained by grouping
neighboring regions based on geometric properties of the classified output
frame of unsupervised K-means classification. The proposed method tested on a
standard as well as newly collected database shows to be superior to some
existing methods."
"This work proposes an agnostic inference strategy for material diagnostics,
conceived within the context of laser-based non-destructive evaluation methods,
which extract information about structural anomalies from the analysis of
acoustic wavefields measured on the structure's surface by means of a scanning
laser interferometer. The proposed approach couples spatiotemporal windowing
with low rank plus outlier modeling, to identify a priori unknown deviations in
the propagating wavefields caused by material inhomogeneities or defects, using
virtually no knowledge of the structural and material properties of the medium.
This characteristic makes the approach particularly suitable for diagnostics
scenarios where the mechanical and material models are complex, unknown, or
unreliable. We demonstrate our approach in a simulated environment using
benchmark point and line defect localization problems based on propagating
flexural waves in a thin plate."
"Shape based classification is one of the most challenging tasks in the field
of computer vision. Shapes play a vital role in object recognition. The basic
shapes in an image can occur in varying scale, position and orientation. And
specially when detecting human, the task becomes more challenging owing to the
largely varying size, shape, posture and clothing of human. So, in our work we
detect human, based on the head-shoulder shape as it is the most unvarying part
of human body. Here, firstly a new and a novel equation named as the Omega
Equation that describes the shape of human head-shoulder is developed and based
on this equation, a classifier is designed particularly for detecting human
presence in a scene. The classifier detects human by analyzing some of the
discriminative features of the values of the parameters obtained from the Omega
equation. The proposed method has been tested on a variety of shape dataset
taking into consideration the complexities of human head-shoulder shape. In all
the experiments the proposed method demonstrated satisfactory results."
"Parameter tuning is a common issue for many tracking algorithms. In order to
solve this problem, this paper proposes an online parameter tuning to adapt a
tracking algorithm to various scene contexts. In an offline training phase,
this approach learns how to tune the tracker parameters to cope with different
contexts. In the online control phase, once the tracking quality is evaluated
as not good enough, the proposed approach computes the current context and
tunes the tracking parameters using the learned values. The experimental
results show that the proposed approach improves the performance of the
tracking algorithm and outperforms recent state of the art trackers. This paper
brings two contributions: (1) an online tracking evaluation, and (2) a method
to adapt online tracking parameters to scene contexts."
"Since the early 2000s, computational visual saliency has been a very active
research area. Each year, more and more new models are published in the main
computer vision conferences. Nowadays, one of the big challenges is to find a
way to fairly evaluate all of these models. In this paper, a new framework is
proposed to assess models of visual saliency. This evaluation is divided into
three experiments leading to the proposition of a new evaluation framework.
Each experiment is based on a basic question: 1) there are two ground truths
for saliency evaluation: what are the differences between eye fixations and
manually segmented salient regions?, 2) the properties of the salient regions:
for example, do large, medium and small salient regions present different
difficulties for saliency models? and 3) the metrics used to assess saliency
models: what advantages would there be to mix them with PCA? Statistical
analysis is used here to answer each of these three questions."
"In the last few decades, significant achievements have been attained in
predicting where humans look at images through different computational models.
However, how to determine contributions of different visual features to overall
saliency still remains an open problem. To overcome this issue, a recent class
of models formulates saliency estimation as a supervised learning problem and
accordingly apply machine learning techniques. In this paper, we also address
this challenging problem and propose to use multiple kernel learning (MKL) to
combine information coming from different feature dimensions and to perform
integration at an intermediate level. Besides, we suggest to use responses of a
recently proposed filterbank of object detectors, known as Object-Bank, as
additional semantic high-level features. Here we show that our MKL-based
framework together with the proposed object-specific features provide
state-of-the-art performance as compared to SVM or AdaBoost-based saliency
models."
"The human visual system employs a selective attention mechanism to understand
the visual world in an eficient manner. In this paper, we show how
computational models of this mechanism can be exploited for the computer vision
application of scene recognition. First, we consider saliency weighting and
saliency pruning, and provide a comparison of the performance of different
attention models in these approaches in terms of classification accuracy.
Pruning can achieve a high degree of computational savings without
significantly sacrificing classification accuracy. In saliency weighting,
however, we found that classification performance does not improve. In
addition, we present a new method to incorporate salient and non-salient
regions for improved classification accuracy. We treat the salient and
non-salient regions separately and combine them using Multiple Kernel Learning.
We evaluate our approach using the UIUC sports dataset and find that with a
small training size, our method improves upon the classification accuracy of
the baseline bag of features approach."
"Region-based artificial attention constitutes a framework for bio-inspired
attentional processes on an intermediate abstraction level for the use in
computer vision and mobile robotics. Segmentation algorithms produce regions of
coherently colored pixels. These serve as proto-objects on which the
attentional processes determine image portions of relevance. A single
region---which not necessarily represents a full object---constitutes the focus
of attention. For many post-attentional tasks, however, such as identifying or
tracking objects, single segments are not sufficient. Here, we present a
saliency-guided approach that groups regions that potentially belong to the
same object based on proximity and similarity of motion. We compare our results
to object selection by thresholding saliency maps and a further
attention-guided strategy."
"In video-surveillance, person re-identification is the task of recognising
whether an individual has already been observed over a network of cameras.
Typically, this is achieved by exploiting the clothing appearance, as classical
biometric traits like the face are impractical in real-world video surveillance
scenarios. Clothing appearance is represented by means of low-level
\textit{local} and/or \textit{global} features of the image, usually extracted
according to some part-based body model to treat different body parts (e.g.
torso and legs) independently. This paper provides a comprehensive review of
current approaches to build appearance descriptors for person
re-identification. The most relevant techniques are described in detail, and
categorised according to the body models and features used. The aim of this
work is to provide a structured body of knowledge and a starting point for
researchers willing to conduct novel investigations on this challenging topic."
"Efficient security management has become an important parameter in todays
world. As the problem is growing, there is an urgent need for the introduction
of advanced technology and equipment to improve the state-of art of
surveillance. In this paper we propose a model for real time background
subtraction using AGMM. The proposed model is robust and adaptable to dynamic
background, fast illumination changes, repetitive motion. Also we have
incorporated a method for detecting shadows using the Horpresert color model.
The proposed model can be employed for monitoring areas where movement or entry
is highly restricted. So on detection of any unexpected events in the scene an
alarm can be triggered and hence we can achieve real time surveillance even in
the absence of constant human monitoring."
"This volume contains the papers accepted at the 6th International Symposium
on Attention in Cognitive Systems (ISACS 2013), held in Beijing, August 5,
2013. The aim of this symposium is to highlight the central role of attention
on various kinds of performance in cognitive systems processing. It brings
together researchers and developers from both academia and industry, from
computer vision, robotics, perception psychology, psychophysics and
neuroscience, in order to provide an interdisciplinary forum to present and
communicate on computational models of attention, with the focus on
interdependencies with visual cognition. Furthermore, it intends to investigate
relevant objectives for performance comparison, to document and to investigate
promising application domains, and to discuss visual attention with reference
to other aspects of AI enabled systems."
"In object segmentation by active contours, the initial contour is often
required. Conventionally, the initial contour is provided by the user. This
paper extends the conventional active contour model by incorporating feature
matching in the formulation, which gives rise to a novel matching-constrained
active contour. The numerical solution to the new optimization model provides
an automated framework of object segmentation without user intervention. The
main idea is to incorporate feature point matching as a constraint in active
contour models. To this effect, we obtain a mathematical model of interior
points to boundary contour such that matching of interior feature points gives
contour alignment, and we formulate the matching score as a constraint to
active contour model such that the feature matching of maximum score that gives
the contour alignment provides the initial feasible solution to the constrained
optimization model of segmentation. The constraint also ensures that the
optimal contour does not deviate too much from the initial contour.
Projected-gradient descent equations are derived to solve the constrained
optimization. In the experiments, we show that our method is capable of
achieving the automatic object segmentation, and it outperforms the related
methods."
"Computer Aided Diagnosis (CAD) system has been developed for the early
detection of breast cancer, one of the most deadly cancer for women. The benign
of mammogram has different texture from malignant. There are fifty mammogram
images used in this work which are divided for training and testing. Therefore,
the selection of the right texture to determine the level of accuracy of CAD
system is important. The first and second order statistics are the texture
feature extraction methods which can be used on a mammogram. This work
classifies texture descriptor into nine groups where the extraction of features
is classified using backpropagation learning with two types of multi-layer
perceptron (MLP). The best texture descriptor as selected when the value of
regression 1 appears in both the MLP-1 and the MLP-2 with the number of epoches
less than 1000. The results of testing show that the best selected texture
descriptor is the second order (combination) using all direction (0, 45, 90 and
135) that have twenty four descriptors."
"Automatic analysis of the enormous sets of images is a critical task in life
sciences. This faces many challenges such as: algorithms are highly
parameterized, significant human input is intertwined, and lacking a standard
meta-visualization approach. This paper proposes an alternative iterative
approach for optimizing input parameters, saving time by minimizing the user
involvement, and allowing for understanding the workflow of algorithms and
discovering new ones. The main focus is on developing an interactive
visualization technique that enables users to analyze the relationships between
sampled input parameters and corresponding output. This technique is
implemented as a prototype called Veni Vidi Vici, or ""I came, I saw, I
conquered."" This strategy is inspired by the mathematical formulas of numbering
computable functions and is developed atop ImageJ, a scientific image
processing program. A case study is presented to investigate the proposed
framework. Finally, the paper explores some potential future issues in the
application of the proposed approach in parameter space analysis in
visualization."
"Currently Mammography is a most effective imaging modality used by
radiologists for the screening of breast cancer. Finding an accurate, robust
and efficient breast region segmentation technique still remains a challenging
problem in digital mammography. Extraction of the breast profile region and the
removal of pectoral muscle are essential pre-processing steps in Computer Aided
Diagnosis (CAD) system for the diagnosis of breast cancer. Primarily it allows
the search for abnormalities to be limited to the region of the breast tissue
without undue influence from the background of the mammogram. The presence of
pectoral muscle in mammograms biases detection procedures, which recommends
removing the pectoral muscle during mammogram image pre-processing. The
presence of pectoral muscle in mammograms may disturb or influence the
detection of breast cancer as the pectoral muscle and mammographic parenchymas
appear similar. The goal of breast region extraction is reducing the image size
without losing anatomic information, it improve the accuracy of the overall CAD
system. The main objective of this study is to propose an automated method to
identify the pectoral muscle in Medio-Lateral Oblique (MLO) view mammograms. In
this paper, we proposed histogram based 8-neighborhood connected component
labelling method for breast region extraction and removal of pectoral muscle.
The proposed method is evaluated by using the mean values of accuracy and
error. The comparative analysis shows that the proposed method identifies the
breast region more accurately."
"Energy minimization algorithms, such as graph cuts, enable the computation of
the MAP solution under certain probabilistic models such as Markov random
fields. However, for many computer vision problems, the MAP solution under the
model is not the ground truth solution. In many problem scenarios, the system
has access to certain statistics of the ground truth. For instance, in image
segmentation, the area and boundary length of the object may be known. In these
cases, we want to estimate the most probable solution that is consistent with
such statistics, i.e., satisfies certain equality or inequality constraints.
  The above constrained energy minimization problem is NP-hard in general, and
is usually solved using Linear Programming formulations, which relax the
integrality constraints. This paper proposes a novel method that finds the
discrete optimal solution of such problems by maximizing the corresponding
Lagrangian dual. This method can be applied to any constrained energy
minimization problem whose unconstrained version is polynomial time solvable,
and can handle multiple, equality or inequality, and linear or non-linear
constraints. We demonstrate the efficacy of our method on the
foreground/background image segmentation problem, and show that it produces
impressive segmentation results with less error, and runs more than 20 times
faster than the state-of-the-art LP relaxation based approaches."
"This work describes a computer vision system that enables pervasive mapping
and monitoring of human attention. The key contribution is that our methodology
enables full 3D recovery of the gaze pointer, human view frustum and associated
human centered measurements directly into an automatically computed 3D model in
real-time. We apply RGB-D SLAM and descriptor matching methodologies for the 3D
modeling, localization and fully automated annotation of ROIs (regions of
interest) within the acquired 3D model. This innovative methodology will open
new avenues for attention studies in real world environments, bringing new
potential into automated processing for human factors technologies."
"In this paper, we address a problem of managing tagged images with hybrid
summarization. We formulate this problem as finding a few image exemplars to
represent the image set semantically and visually, and solve it in a hybrid way
by exploiting both visual and textual information associated with images. We
propose a novel approach, called homogeneous and heterogeneous message
propagation ($\text{H}^\text{2}\text{MP}$). Similar to the affinity propagation
(AP) approach, $\text{H}^\text{2}\text{MP}$ reduce the conventional
\emph{vector} message propagation to \emph{scalar} message propagation to make
the algorithm more efficient. Beyond AP that can only handle homogeneous data,
$\text{H}^\text{2}\text{MP}$ generalizes it to exploit extra heterogeneous
relations and the generalization is non-trivial as the reduction to scalar
messages from vector messages is more challenging. The main advantages of our
approach lie in 1) that $\text{H}^\text{2}\text{MP}$ exploits visual similarity
and in addition the useful information from the associated tags, including the
associations relation between images and tags and the relations within tags,
and 2) that the summary is both visually and semantically satisfactory. In
addition, our approach can also present a textual summary to a tagged image
collection, which can be used to automatically generate a textual description.
The experimental results demonstrate the effectiveness and efficiency of the
roposed approach."
"Artificial visual attention systems aim to support technical systems in
visual tasks by applying the concepts of selective attention observed in humans
and other animals. Such systems are typically evaluated against ground truth
obtained from human gaze-data or manually annotated test images. When applied
to robotics, the systems are required to be adaptable to the target system.
Here, we describe a flexible environment based on a robotic middleware layer
allowing the development and testing of attention-guided vision systems. In
such a framework, the systems can be tested with input from various sources,
different attention algorithms at the core, and diverse subsequent tasks."
"In this paper, we consider the clustering problem on images where each image
contains patches in people and location domains. We exploit the correlation
between people and location domains, and proposed a semi-supervised
co-clustering algorithm to cluster images. Our algorithm updates the
correlation links at the runtime, and produces clustering in both domains
simultaneously. We conduct experiments in a manually collected dataset and a
Flickr dataset. The result shows that the such correlation improves the
clustering performance."
"In this work, a new constrained hybrid variational deblurring model is
developed by combining the non-convex first- and second-order total variation
regularizers. Moreover, a box constraint is imposed on the proposed model to
guarantee high deblurring performance. The developed constrained hybrid
variational model could achieve a good balance between preserving image details
and alleviating ringing artifacts. In what follows, we present the
corresponding numerical solution by employing an iteratively reweighted
algorithm based on alternating direction method of multipliers. The
experimental results demonstrate the superior performance of the proposed
method in terms of quantitative and qualitative image quality assessments."
"Blind image quality assessment (BIQA) aims to predict perceptual image
quality scores without access to reference images. State-of-the-art BIQA
methods typically require subjects to score a large number of images to train a
robust model. However, the acquisition of image quality scores has several
limitations: 1) scores are not precise, because subjects are usually uncertain
about which score most precisely represents the perceptual quality of a given
image; 2) subjective judgements of quality may be biased by image content; 3)
the quality scales between different distortion categories are inconsistent;
and 4) it is challenging to obtain a large scale database, or to extend
existing databases, because of the inconvenience of collecting sufficient
images, training the subjects, conducting subjective experiments, and
realigning human quality evaluations. To combat these limitations, this paper
explores and exploits preference image pairs such as ""the quality of image Ia
is better than that of image Ib"" for training a robust BIQA model. The
preference label, representing the relative quality of two images, is generally
precise and consistent, and is not sensitive to image content, distortion type,
or subject identity; such PIPs can be generated at very low cost. The proposed
BIQA method is one of learning to rank. We first formulate the problem of
learning the mapping from the image features to the preference label as one of
classification. In particular, we investigate the utilization of a multiple
kernel learning algorithm based on group lasso (MKLGL) to provide a solution. A
simple but effective strategy to estimate perceptual image quality scores is
then presented. Experiments show that the proposed BIQA method is highly
effective and achieves comparable performance to state-of-the-art BIQA
algorithms. Moreover, the proposed method can be easily extended to new
distortion categories."
"Our Multi-Column Deep Neural Networks achieve best known recognition rates on
Chinese characters from the ICDAR 2011 and 2013 offline handwriting
competitions, approaching human performance."
"Many efforts have been devoted to develop alternative methods to traditional
vector quantization in image domain such as sparse coding and soft-assignment.
These approaches can be split into a dictionary learning phase and a feature
encoding phase which are often closely connected. In this paper, we investigate
the effects of these phases by separating them for video-based action
classification. We compare several dictionary learning methods and feature
encoding schemes through extensive experiments on KTH and HMDB51 datasets.
Experimental results indicate that sparse coding performs consistently better
than the other encoding methods in large complex dataset (i.e., HMDB51), and it
is robust to different dictionaries. For small simple dataset (i.e., KTH) with
less variation, however, all the encoding strategies perform competitively. In
addition, we note that the strength of sophisticated encoding approaches comes
not from their corresponding dictionaries but the encoding mechanisms, and we
can just use randomly selected exemplars as dictionaries for video-based action
classification."
"This paper describes an efficient approach for human face recognition based
on blood perfusion data from infra-red face images. Blood perfusion data are
characterized by the regional blood flow in human tissue and therefore do not
depend entirely on surrounding temperature. These data bear a great potential
for deriving discriminating facial thermogram for better classification and
recognition of face images in comparison to optical image data. Blood perfusion
data are related to distribution of blood vessels under the face skin. A
distribution of blood vessels are unique for each person and as a set of
extracted minutiae points from a blood perfusion data of a human face should be
unique for that face. There may be several such minutiae point sets for a
single face but all of these correspond to that particular face only. Entire
face image is partitioned into equal blocks and the total number of minutiae
points from each block is computed to construct final vector. Therefore, the
size of the feature vectors is found to be same as total number of blocks
considered. For classification, a five layer feed-forward backpropagation
neural network has been used. A number of experiments were conducted to
evaluate the performance of the proposed face recognition system with varying
block sizes. Experiments have been performed on the database created at our own
laboratory. The maximum success of 91.47% recognition has been achieved with
block size 8X8."
"In this paper an efficient approach for human face recognition based on the
use of minutiae points in thermal face image is proposed. The thermogram of
human face is captured by thermal infra-red camera. Image processing methods
are used to pre-process the captured thermogram, from which different
physiological features based on blood perfusion data are extracted. Blood
perfusion data are related to distribution of blood vessels under the face
skin. In the present work, three different methods have been used to get the
blood perfusion image, namely bit-plane slicing and medial axis transform,
morphological erosion and medial axis transform, sobel edge operators.
Distribution of blood vessels is unique for each person and a set of extracted
minutiae points from a blood perfusion data of a human face should be unique
for that face. Two different methods are discussed for extracting minutiae
points from blood perfusion data. For extraction of features entire face image
is partitioned into equal size blocks and the total number of minutiae points
from each block is computed to construct final feature vector. Therefore, the
size of the feature vectors is found to be same as total number of blocks
considered. A five layer feed-forward back propagation neural network is used
as the classification tool. A number of experiments were conducted to evaluate
the performance of the proposed face recognition methodologies with varying
block size on the database created at our own laboratory. It has been found
that the first method supercedes the other two producing an accuracy of 97.62%
with block size 16X16 for bit-plane 4."
"Thermal infra-red (IR) images focus on changes of temperature distribution on
facial muscles and blood vessels. These temperature changes can be regarded as
texture features of images. A comparative study of face recognition methods
working in thermal spectrum is carried out in this paper. In these study two
local-matching methods based on Haar wavelet transform and Local Binary Pattern
(LBP) are analyzed. Wavelet transform is a good tool to analyze multi-scale,
multi-direction changes of texture. Local binary patterns (LBP) are a type of
feature used for classification in computer vision. Firstly, human thermal IR
face image is preprocessed and cropped the face region only from the entire
image. Secondly, two different approaches are used to extract the features from
the cropped face region. In the first approach, the training images and the
test images are processed with Haar wavelet transform and the LL band and the
average of LH/HL/HH bands sub-images are created for each face image. Then a
total confidence matrix is formed for each face image by taking a weighted sum
of the corresponding pixel values of the LL band and average band. For LBP
feature extraction, each of the face images in training and test datasets is
divided into 161 numbers of sub images, each of size 8X8 pixels. For each such
sub images, LBP features are extracted which are concatenated in row wise
manner. PCA is performed separately on the individual feature set for
dimensionality reeducation. Finally two different classifiers are used to
classify face images. One such classifier multi-layer feed forward neural
network and another classifier is minimum distance classifier. The Experiments
have been performed on the database created at our own laboratory and Terravic
Facial IR Database."
"The goal of object detection is to find objects in an image. An object
detector accepts an image and produces a list of locations as $(x,y)$ pairs.
Here we introduce a new concept: {\bf location-based boosting}. Location-based
boosting differs from previous boosting algorithms because it optimizes a new
spatial loss function to combine object detectors, each of which may have
marginal performance, into a single, more accurate object detector. A
structured representation of object locations as a list of $(x,y)$ pairs is a
more natural domain for object detection than the spatially unstructured
representation produced by classifiers. Furthermore, this formulation allows us
to take advantage of the intuition that large areas of the background are
uninteresting and it is not worth expending computational effort on them. This
results in a more scalable algorithm because it does not need to take measures
to prevent the background data from swamping the foreground data such as
subsampling or applying an ad-hoc weighting to the pixels. We first present the
theory of location-based boosting, and then motivate it with empirical results
on a challenging data set."
"In this paper, a thermal infra red face recognition system for human
identification and verification using blood perfusion data and back propagation
feed forward neural network is proposed. The system consists of three steps. At
the very first step face region is cropped from the colour 24-bit input images.
Secondly face features are extracted from the croped region, which will be
taken as the input of the back propagation feed forward neural network in the
third step and classification and recognition is carried out. The proposed
approaches are tested on a number of human thermal infra red face images
created at our own laboratory. Experimental results reveal the higher degree
performance"
"Thermal infrared (IR) images represent the heat patterns emitted from hot
object and they do not consider the energies reflected from an object. Objects
living or non-living emit different amounts of IR energy according to their
body temperature and characteristics. Humans are homoeothermic and hence
capable of maintaining constant temperature under different surrounding
temperature. Face recognition from thermal (IR) images should focus on changes
of temperature on facial blood vessels. These temperature changes can be
regarded as texture features of images and wavelet transform is a very good
tool to analyze multi-scale and multi-directional texture. Wavelet transform is
also used for image dimensionality reduction, by removing redundancies and
preserving original features of the image. The sizes of the facial images are
normally large. So, the wavelet transform is used before image similarity is
measured. Therefore this paper describes an efficient approach of human face
recognition based on wavelet transform from thermal IR images. The system
consists of three steps. At the very first step, human thermal IR face image is
preprocessed and the face region is only cropped from the entire image.
Secondly, Haar wavelet is used to extract low frequency band from the cropped
face region. Lastly, the image classification between the training images and
the test images is done, which is based on low-frequency components. The
proposed approach is tested on a number of human thermal infrared face images
created at our own laboratory and Terravic Facial IR Database. Experimental
results indicated that the thermal infra red face images can be recognized by
the proposed system effectively. The maximum success of 95% recognition has
been achieved."
"Low-rank matrix completion is a problem of immense practical importance.
Recent works on the subject often use nuclear norm as a convex surrogate of the
rank function. Despite its solid theoretical foundation, the convex version of
the problem often fails to work satisfactorily in real-life applications. Real
data often suffer from very few observations, with support not meeting the
random requirements, ubiquitous presence of noise and potentially gross
corruptions, sometimes with these simultaneously occurring.
  This paper proposes a Proximal Alternating Robust Subspace Minimization
(PARSuMi) method to tackle the three problems. The proximal alternating scheme
explicitly exploits the rank constraint on the completed matrix and uses the
$\ell_0$ pseudo-norm directly in the corruption recovery step. We show that the
proposed method for the non-convex and non-smooth model converges to a
stationary point. Although it is not guaranteed to find the global optimal
solution, in practice we find that our algorithm can typically arrive at a good
local minimizer when it is supplied with a reasonably good starting point based
on convex optimization. Extensive experiments with challenging synthetic and
real data demonstrate that our algorithm succeeds in a much larger range of
practical problems where convex optimization fails, and it also outperforms
various state-of-the-art algorithms."
"A topology preserving skeleton is a synthetic representation of an object
that retains its topology and many of its significant morphological properties.
The process of obtaining the skeleton, referred to as skeletonization or
thinning, is a very active research area. It plays a central role in reducing
the amount of information to be processed during image analysis and
visualization, computer-aided diagnosis or by pattern recognition algorithms.
  This paper introduces a novel topology preserving thinning algorithm which
removes \textit{simple cells}---a generalization of simple points---of a given
cell complex. The test for simple cells is based on \textit{acyclicity tables}
automatically produced in advance with homology computations. Using acyclicity
tables render the implementation of thinning algorithms straightforward.
Moreover, the fact that tables are automatically filled for all possible
configurations allows to rigorously prove the generality of the algorithm and
to obtain fool-proof implementations. The novel approach enables, for the first
time, according to our knowledge, to thin a general unstructured simplicial
complex. Acyclicity tables for cubical and simplicial complexes and an open
source implementation of the thinning algorithm are provided as additional
material to allow their immediate use in the vast number of practical
applications arising in medical imaging and beyond."
"Synthetic aperture radar (SAR) images are widely used in target recognition
tasks nowadays. In this letter, we propose an automatic approach for radar
shadow detection and extraction from SAR images utilizing geometric projections
along with the digital elevation model (DEM) which corresponds to the given
geo-referenced SAR image. First, the DEM is rotated into the radar geometry so
that each row would match that of a radar line of sight. Next, we extract the
shadow regions by processing row by row until the image is covered fully. We
test the proposed shadow detection approach on different DEMs and a simulated
1D signals and 2D hills and volleys modeled by various variance based Gaussian
functions. Experimental results indicate the proposed algorithm produces good
results in detecting shadows in SAR images with high resolution."
"Recently single image super resolution is very important research area to
generate high resolution image from given low resolution image. Algorithms of
single image resolution are mainly based on wavelet domain and spatial domain.
Filters support to model the regularity of natural images is exploited in
wavelet domain while edges of images get sharp during up sampling in spatial
domain. Here single image super resolution algorithm is presented which based
on both spatial and wavelet domain and take the advantage of both. Algorithm is
iterative and use back projection to minimize reconstruction error. Wavelet
based denoising method is also introduced to remove noise."
"In This paper we presented new approach for cursive Arabic text recognition
system. The objective is to propose methodology analytical offline recognition
of handwritten Arabic for rapid implementation. The first part in the writing
recognition system is the preprocessing phase is the preprocessing phase to
prepare the data was introduces and extracts a set of simple statistical
features by two methods : from a window which is sliding long that text line
the right to left and the approach VH2D (consists in projecting every character
on the abscissa, on the ordinate and the diagonals 45{\deg} and 135{\deg}) . It
then injects the resulting feature vectors to Hidden Markov Model (HMM) and
combined the two HMM by multi-stream approach."
"In this paper, we propose a re-weighted elastic net (REN) model for biometric
recognition. The new model is applied to data separated into geometric and
color spatial components. The geometric information is extracted using a fast
cartoon - texture decomposition model based on a dual formulation of the total
variation norm allowing us to carry information about the overall geometry of
images. Color components are defined using linear and nonlinear color spaces,
namely the red-green-blue (RGB), chromaticity-brightness (CB) and
hue-saturation-value (HSV). Next, according to a Bayesian fusion-scheme, sparse
representations for classification purposes are obtained. The scheme is
numerically solved using a gradient projection (GP) algorithm. In the empirical
validation of the proposed model, we have chosen the periocular region, which
is an emerging trait known for its robustness against low quality data. Our
results were obtained in the publicly available UBIRIS.v2 data set and show
consistent improvements in recognition effectiveness when compared to related
state-of-the-art techniques."
"This paper focuses on two main issues; first one is the impact of Similarity
Search to learning the training sample in metric space, and searching based on
supervised learning classi-fication. In particular, four metrics space
searching are based on spatial information that are introduced as the
following; Cheby-shev Distance (CD); Bray Curtis Distance (BCD); Manhattan
Distance (MD) and Euclidean Distance(ED) classifiers. The second issue
investigates the performance of combination of mul-ti-sensor images on the
supervised learning classification accura-cy. QuickBird multispectral data (MS)
and panchromatic data (PAN) have been used in this study to demonstrate the
enhance-ment and accuracy assessment of fused image over the original images.
The supervised classification results of fusion image generated better than the
MS did. QuickBird and the best results with ED classifier than the other did."
"In this paper we present a novel approach that takes as input a 3D image and
gives as output its pose i.e. it tells whether the face is oriented with
respect the X, Y or Z axes with angles of rotation up to 40 degree. All the
experiments have been performed on the FRAV3D Database. After applying the
proposed algorithm to the 3D facial surface we have obtained i.e. on 848 3D
face images our method detected the pose correctly for 566 face images,thus
giving an approximately 67 % of correct pose detection."
"In this paper we present a novel technique of registering 3D images across
pose. In this context, we have taken into account the images which are aligned
across X, Y and Z axes. We have first determined the angle across which the
image is rotated with respect to X, Y and Z axes and then translation is
performed on the images. After testing the proposed method on 472 images from
the FRAV3D database, the method correctly registers 358 images thus giving a
performance rate of 75.84%."
"Superpixel algorithms aim to over-segment the image by grouping pixels that
belong to the same object. Many state-of-the-art superpixel algorithms rely on
minimizing objective functions to enforce color ho- mogeneity. The optimization
is accomplished by sophis- ticated methods that progressively build the
superpix- els, typically by adding cuts or growing superpixels. As a result,
they are computationally too expensive for real-time applications. We introduce
a new approach based on a simple hill-climbing optimization. Starting from an
initial superpixel partitioning, it continuously refines the superpixels by
modifying the boundaries. We define a robust and fast to evaluate energy
function, based on enforcing color similarity between the bound- aries and the
superpixel color histogram. In a series of experiments, we show that we achieve
an excellent com- promise between accuracy and efficiency. We are able to
achieve a performance comparable to the state-of- the-art, but in real-time on
a single Intel i7 CPU at 2.8GHz."
"Analysis of microscopy images can provide insight into many biological
processes. One particularly challenging problem is cell nuclear segmentation in
highly anisotropic and noisy 3D image data. Manually localizing and segmenting
each and every cell nuclei is very time consuming, which remains a bottleneck
in large scale biological experiments. In this work we present a tool for
automated segmentation of cell nuclei from 3D fluorescent microscopic data. Our
tool is based on state-of-the-art image processing and machine learning
techniques and supports a friendly graphical user interface (GUI). We show that
our tool is as accurate as manual annotation but greatly reduces the time for
the registration."
"This paper is based on an application of smoothing of 3D face images followed
by feature detection i.e. detecting the nose tip. The present method uses a
weighted mesh median filtering technique for smoothing. In this present
smoothing technique we have built the neighborhood surrounding a particular
point in 3D face and replaced that with the weighted value of the surrounding
points in 3D face image. After applying the smoothing technique to the 3D face
images our experimental results show that we have obtained considerable
improvement as compared to the algorithm without smoothing. We have used here
the maximum intensity algorithm for detecting the nose-tip and this method
correctly detects the nose-tip in case of any pose i.e. along X, Y, and Z axes.
The present technique gave us worked successfully on 535 out of 542 3D face
images as compared to the method without smoothing which worked only on 521 3D
face images out of 542 face images. Thus we have obtained a 98.70% performance
rate over 96.12% performance rate of the algorithm without smoothing. All the
experiments have been performed on the FRAV3D database."
"In this paper, we propose a new approach that takes as input a 3D face image
across X, Y and Z axes as well as both Y and X axes and gives output as its
pose i.e. it tells whether the face is oriented with respect the X, Y or Z axes
or is it oriented across multiple axes with angles of rotation up to 42 degree.
All the experiments have been performed on the FRAV3D, GAVADB and Bosphorus
database which has two figures of each individual across multiple axes. After
applying the proposed algorithm to the 3D facial surface from FRAV3D on 848 3D
faces, 566 3D faces were correctly recognized for pose thus giving 67% of
correct identification rate. We had experimented on 420 images from the GAVADB
database, and only 336 images were detected for correct pose identification
rate i.e. 80% and from Bosphorus database on 560 images only 448 images were
detected for correct pose identification i.e. 80%.abstract goes here."
"In this paper we present a novel method that combines a HK curvature-based
approach for three-dimensional (3D) face detection in different poses (X-axis,
Y-axis and Z-axis). Salient face features, such as the eyes and nose, are
detected through an analysis of the curvature of the entire facial surface. All
the experiments have been performed on the FRAV3D Database. After applying the
proposed algorithm to the 3D facial surface we have obtained considerably good
results i.e. on 752 3D face images our method detected the eye corners for 543
face images, thus giving a 72.20% of eye corners detection and 743 face images
for nose-tip detection thus giving a 98.80% of good nose tip localization"
"In this paper, we present an algorithm for identifying a parametrically
described destructive unknown system based on a non-gaussianity measure. It is
known that under certain conditions the output of a linear system is more
gaussian than the input. Hence, an inverse filter is searched, such that its
output is minimally gaussian. We use the kurtosis as a measure of the
non-gaussianity of the signal. A maximum of the kurtosis as a function of the
deconvolving filter coefficients is searched. The search is done iteratively
using the gradient ascent algorithm, and the coefficients at the maximum point
correspond to the inverse filter coefficients. This filter may be applied to
the distorted signal to obtain the original undistorted signal. While a similar
approach has been used before, it was always directed at a particular kind of a
signal, commonly of impulsive characteristics. In this paper a successful
attempt has been made to apply the algorithm to a wider range of signals, such
as to process distorted audio signals and destructed images. This innovative
implementation required the revelation of a way to preprocess the distorted
signal at hand. The experimental results show very good performance in terms of
recovering audio signals and blurred images, both for an FIR and IIR distorting
filters."
"In handwritten character recognition, benchmark database plays an important
role in evaluating the performance of various algorithms and the results
obtained by various researchers. In Devnagari script, there is lack of such
official benchmark. This paper focuses on the generation of offline benchmark
database for Devnagari handwritten numerals and characters. The present work
generated 5137 and 20305 isolated samples for numeral and character database,
respectively, from 750 writers of all ages, sex, education, and profession. The
offline sample images are stored in TIFF image format as it occupies less
memory. Also, the data is presented in binary level so that memory requirement
is further reduced. It will facilitate research on handwriting recognition of
Devnagari script through free access to the researchers."
"The main finding of this work is that the standard image classification
pipeline, which consists of dictionary learning, feature encoding, spatial
pyramid pooling and linear classification, outperforms all state-of-the-art
face recognition methods on the tested benchmark datasets (we have tested on
AR, Extended Yale B, the challenging FERET, and LFW-a datasets). This
surprising and prominent result suggests that those advances in generic image
classification can be directly applied to improve face recognition systems. In
other words, face recognition may not need to be viewed as a separate object
classification problem.
  While recently a large body of residual based face recognition methods focus
on developing complex dictionary learning algorithms, in this work we show that
a dictionary of randomly extracted patches (even from non-face images) can
achieve very promising results using the image classification pipeline. That
means, the choice of dictionary learning methods may not be important. Instead,
we find that learning multiple dictionaries using different low-level image
features often improve the final classification accuracy. Our proposed face
recognition approach offers the best reported results on the widely-used face
recognition benchmark datasets. In particular, on the challenging FERET and
LFW-a datasets, we improve the best reported accuracies in the literature by
about 20% and 30% respectively."
"Compressed Sensing based Terahertz imaging (CS-THz) is a computational
imaging technique. It uses only one THz receiver to accumulate the random
modulated image measurements where the original THz image is reconstruct from
these measurements using compressed sensing solvers. The advantage of the
CS-THz is its reduced acquisition time compared with the raster scan mode.
However, when it applied to large-scale two-dimensional (2D) imaging, the
increased dimension resulted in both high computational complexity and
excessive memory usage. In this paper, we introduced a novel CS-based THz
imaging system that progressively compressed the THz image column by column.
Therefore, the CS-THz system could be simplified with a much smaller sized
modulator and reduced dimension. In order to utilize the block structure and
the correlation of adjacent columns of the THz image, a complex-valued block
sparse Bayesian learning algorithm was proposed. We conducted systematic
evaluation of state-of-the-art CS algorithms under the scan based CS-THz
architecture. The compression ratios and the choices of the sensing matrices
were analyzed in detail using both synthetic and real-life THz images.
Simulation results showed that both the scan based architecture and the
proposed recovery algorithm were superior and efficient for large scale CS-THz
applications."
"We propose a large deformation diffeomorphic metric mapping algorithm to
align multiple b-value diffusion weighted imaging (mDWI) data, specifically
acquired via hybrid diffusion imaging (HYDI), denoted as LDDMM-HYDI. We then
propose a Bayesian model for estimating the white matter atlas from HYDIs. We
adopt the work given in Hosseinbor et al. (2012) and represent the q-space
diffusion signal with the Bessel Fourier orientation reconstruction (BFOR)
signal basis. The BFOR framework provides the representation of mDWI in the
q-space and thus reduces memory requirement. In addition, since the BFOR signal
basis is orthonormal, the L2 norm that quantifies the differences in the
q-space signals of any two mDWI datasets can be easily computed as the sum of
the squared differences in the BFOR expansion coefficients. In this work, we
show that the reorientation of the $q$-space signal due to spatial
transformation can be easily defined on the BFOR signal basis. We incorporate
the BFOR signal basis into the LDDMM framework and derive the gradient descent
algorithm for LDDMM-HYDI with explicit orientation optimization. Additionally,
we extend the previous Bayesian atlas estimation framework for scalar-valued
images to HYDIs and derive the expectation-maximization algorithm for solving
the HYDI atlas estimation problem. Using real HYDI datasets, we show the
Bayesian model generates the white matter atlas with anatomical details.
Moreover, we show that it is important to consider the variation of mDWI
reorientation due to a small change in diffeomorphic transformation in the
LDDMM-HYDI optimization and to incorporate the full information of HYDI for
aligning mDWI."
"In this paper we are interested in analyzing behaviour in crowded public
places at the level of holistic motion. Our aim is to learn, without user
input, strong scene priors or labelled data, the scope of ""normal behaviour""
for a particular scene and thus alert to novelty in unseen footage. The first
contribution is a low-level motion model based on what we term tracklet
primitives, which are scene-specific elementary motions. We propose a
clustering-based algorithm for tracklet estimation from local approximations to
tracks of appearance features. This is followed by two methods for motion
novelty inference from tracklet primitives: (a) we describe an approach based
on a non-hierarchial ensemble of Markov chains as a means of capturing
behavioural characteristics at different scales, and (b) a more flexible
alternative which exhibits a higher generalizing power by accounting for
constraints introduced by intentionality and goal-oriented planning of human
motion in a particular scene. Evaluated on a 2h long video of a busy city
marketplace, both algorithms are shown to be successful at inferring unusual
behaviour, the latter model achieving better performance for novelties at a
larger spatial scale."
"This paper addresses the problem of tracking moving objects of variable
appearance in challenging scenes rich with features and texture. Reliable
tracking is of pivotal importance in surveillance applications. It is made
particularly difficult by the nature of objects encountered in such scenes:
these too change in appearance and scale, and are often articulated (e.g.
humans). We propose a method which uses fast motion detection and segmentation
as a constraint for both building appearance models and their robust
propagation (matching) in time. The appearance model is based on sets of local
appearances automatically clustered using spatio-kinetic similarity, and is
updated with each new appearance seen. This integration of all seen appearances
of a tracked object makes it extremely resilient to errors caused by occlusion
and the lack of permanence of due to low data quality, appearance change or
background clutter. These theoretical strengths of our algorithm are
empirically demonstrated on two hour long video footage of a busy city
marketplace."
"Text in an image provides vital information for interpreting its contents,
and text in a scene can aide with a variety of tasks from navigation, to
obstacle avoidance, and odometry. Despite its value, however, identifying
general text in images remains a challenging research problem. Motivated by the
need to consider the widely varying forms of natural text, we propose a
bottom-up approach to the problem which reflects the `characterness' of an
image region. In this sense our approach mirrors the move from saliency
detection methods to measures of `objectness'. In order to measure the
characterness we develop three novel cues that are tailored for character
detection, and a Bayesian method for their integration. Because text is made up
of sets of characters, we then design a Markov random field (MRF) model so as
to exploit the inherent dependencies between characters.
  We experimentally demonstrate the effectiveness of our characterness cues as
well as the advantage of Bayesian multi-cue integration. The proposed text
detector outperforms state-of-the-art methods on a few benchmark scene text
detection datasets. We also show that our measurement of `characterness' is
superior than state-of-the-art saliency detection models when applied to the
same task."
"We present a family of online algorithms for real-time factorization-based
structure from motion, leveraging a relationship between incremental singular
value decomposition and recently proposed methods for online matrix completion.
Our methods are orders of magnitude faster than previous state of the art, can
handle missing data and a variable number of feature points, and are robust to
noise and sparse outliers. We demonstrate our methods on both real and
synthetic sequences and show that they perform well in both online and batch
settings. We also provide an implementation which is able to produce 3D models
in real time using a laptop with a webcam."
"Human identification has always been a topic that interested researchers
around the world. Biometric methods are found to be more effective and much
easier for the users than the traditional identification methods like keys,
smart cards and passwords. Unlike with the traditional methods, with biometric
methods the data acquisition is most of the times passive, which means the
users do not take active part in data acquisition. Data acquisition can be
performed using cameras, scanners or sensors. Human physiological biometrics
such as face, eye and ear are good candidates for uniquely identifying an
individual. However, human ear scores over face and eye because of certain
advantages it has over face. The most challenging phase in human identification
based on ear biometric is the segmentation of the ear image from the captured
image which may contain many unwanted details. In this work, PDE based image
processing techniques are used to segment out the ear image. Level Set Theory
based image processing is employed to obtain the contour of the ear image. A
few Level set algorithms are compared for their efficiency in segmenting test
ear images."
"This paper proposes a new approach for face verification, where a pair of
images needs to be classified as belonging to the same person or not. This
problem is relatively new and not well-explored in the literature. Current
methods mostly adopt techniques borrowed from face recognition, and process
each of the images in the pair independently, which is counter intuitive. In
contrast, we propose to extract cross-image features, i.e. features across the
pair of images, which, as we demonstrate, is more discriminative to the
similarity and the dissimilarity of faces. Our features are derived from the
popular Haar-like features, however, extended to handle the face verification
problem instead of face detection. We collect a large bank of cross-image
features using filters of different sizes, locations, and orientations.
Consequently, we use AdaBoost to select and weight the most discriminative
features. We carried out extensive experiments on the proposed ideas using
three standard face verification datasets, and obtained promising results
outperforming state-of-the-art."
"In the past decade, SIFT descriptor has been witnessed as one of the most
robust local invariant feature descriptors and widely used in various vision
tasks. Most traditional image classification systems depend on the
luminance-based SIFT descriptors, which only analyze the gray level variations
of the images. Misclassification may happen since their color contents are
ignored. In this article, we concentrate on improving the performance of
existing image classification algorithms by adding color information. To
achieve this purpose, different kinds of colored SIFT descriptors are
introduced and implemented. Locality-constrained Linear Coding (LLC), a
state-of-the-art sparse coding technology, is employed to construct the image
classification system for the evaluation. The real experiments are carried out
on several benchmarks. With the enhancements of color SIFT, the proposed image
classification system obtains approximate 3% improvement of classification
accuracy on the Caltech-101 dataset and approximate 4% improvement of
classification accuracy on the Caltech-256 dataset."
"The effects of global climate change on Peruvian glaciers have brought about
several processes of deglaciation during the last few years. The immediate
effect is the change of size of lakes and rivers. Public institutions that
monitor water resources currently have only recent studies which make up less
than 10% of the total. The effects of climate change and the lack of updated
information intensify social-economic problems related to water resources in
Peru. The objective of this research is to develop a software application to
automate the Cadastral Registry of Water Bodies in Peru, using techniques of
digital image processing, which would provide tools for detection, record,
temporal analysis and visualization of water bodies. The images used are from
the satellite Landsat5, which undergo a pre-processing of calibration and
correction of the satellite. Detection results are archived into a file that
contains location vectors and images of the segmentated bodies of water."
"Image fusion is one of the recent trends in image registration which is an
essential field of image processing. The basic principle of this paper is to
fuse multi-focus images using simple statistical standard deviation. Firstly,
the simple standard deviation for the k-by-k window inside each of the
multi-focus images was computed. The contribution in this paper came from the
idea that the focused part inside an image had high details rather than the
unfocused part. Hence, the dispersion between pixels inside the focused part is
higher than the dispersion inside the unfocused part. Secondly, a simple
comparison between the standard deviation for each k-by-k window in the
multi-focus images could be computed. The highest standard deviation between
all the computed standard deviations for the multi-focus images could be
treated as the optimal that is to be placed in the fused image. The
experimental visual results show that the proposed method produces very
satisfactory results in spite of its simplicity."
"This work aims at generating a model of the ocean surface and its dynamics
from one or more video cameras. The idea is to model wave patterns from video
as a first step towards a larger system of photogrammetric monitoring of marine
conditions for use in offshore oil drilling platforms. The first part of the
proposed approach consists in reducing the dimensionality of sensor data made
up of the many pixels of each frame of the input video streams. This enables
finding a concise number of most relevant parameters to model the temporal
dataset, yielding an efficient data-driven model of the evolution of the
observed surface. The second part proposes stochastic modeling to better
capture the patterns embedded in the data. One can then draw samples from the
final model, which are expected to simulate the behavior of previously observed
flow, in order to determine conditions that match new observations. In this
paper we focus on proposing and discussing the overall approach and on
comparing two different techniques for dimensionality reduction in the first
stage: principal component analysis and diffusion maps. Work is underway on the
second stage of constructing better stochastic models of fluid surface dynamics
as proposed here."
"In identity management system, commonly used biometric recognition system
needs attention towards issue of biometric template protection as far as more
reliable solution is concerned. In view of this biometric template protection
algorithm should satisfy security, discriminability and cancelability. As no
single template protection method is capable of satisfying the basic
requirements, a novel technique for face template generation and protection is
proposed. The novel approach is proposed to provide security and accuracy in
new user enrollment as well as authentication process. This novel technique
takes advantage of both the hybrid approach and the binary discriminant
analysis algorithm. This algorithm is designed on the basis of random
projection, binary discriminant analysis and fuzzy commitment scheme. Three
publicly available benchmark face databases are used for evaluation. The
proposed novel technique enhances the discriminability and recognition accuracy
by 80% in terms of matching score of the face images and provides high
security."
"Hybrid approach has a special status among Face Recognition Systems as they
combine different recognition approaches in an either serial or parallel to
overcome the shortcomings of individual methods. This paper explores the area
of Hybrid Face Recognition using score based strategy as a combiner/fusion
process. In proposed approach, the recognition system operates in two modes:
training and classification. Training mode involves normalization of the face
images (training set), extracting appropriate features using Principle
Component Analysis (PCA) and Independent Component Analysis (ICA). The
extracted features are then trained in parallel using Back-propagation neural
networks (BPNNs) to partition the feature space in to different face classes.
In classification mode, the trained PCA BPNN and ICA BPNN are fed with new face
image(s). The score based strategy which works as a combiner is applied to the
results of both PCA BPNN and ICA BPNN to classify given new face image(s)
according to face classes obtained during the training mode. The proposed
approach has been tested on ORL and other face databases; the experimented
results show that the proposed system has higher accuracy than face recognition
systems using single feature extractor."
"In this work we propose a hybrid NN/HMM model for online Arabic handwriting
recognition. The proposed system is based on Hidden Markov Models (HMMs) and
Multi Layer Perceptron Neural Networks (MLPNNs). The input signal is segmented
to continuous strokes called segments based on the Beta-Elliptical strategy by
inspecting the extremum points of the curvilinear velocity profile. A neural
network trained with segment level contextual information is used to extract
class character probabilities. The output of this network is decoded by HMMs to
provide character level recognition. In evaluations on the ADAB database, we
achieved 96.4% character recognition accuracy that is statistically
significantly important in comparison with character recognition accuracies
obtained from state-of-the-art online Arabic systems.8"
"We provide two novel adaptive-rate compressive sensing (CS) strategies for
sparse, time-varying signals using side information. Our first method utilizes
extra cross-validation measurements, and the second one exploits extra
low-resolution measurements. Unlike the majority of current CS techniques, we
do not assume that we know an upper bound on the number of significant
coefficients that comprise the images in the video sequence. Instead, we use
the side information to predict the number of significant coefficients in the
signal at the next time instant. For each image in the video sequence, our
techniques specify a fixed number of spatially-multiplexed CS measurements to
acquire, and adjust this quantity from image to image. Our strategies are
developed in the specific context of background subtraction for surveillance
video, and we experimentally validate the proposed methods on real video
sequences."
"Automatic authentication of paper money has been targeted. Indian bank notes
are taken as reference to show how a system can be developed for discriminating
fake notes from genuine ones. Image processing and pattern recognition
techniques are used to design the overall approach. The ability of the embedded
security aspects is thoroughly analysed for detecting fake currencies. Real
forensic samples are involved in the experiment that shows a high precision
machine can be developed for authentication of paper money. The system
performance is reported in terms of both accuracy and processing speed.
Comparison with human subjects namely forensic experts and bank staffs clearly
shows its applicability for mass checking of currency notes in the real world.
The analysis of security features to protect counterfeiting highlights some
facts that should be taken care of in future designing of currency notes."
"Unusual events are important as being possible indicators of undesired
consequences. Moreover, unusualness in everyday life activities may also be
amusing to watch as proven by the popularity of such videos shared in social
media. Discovery of unusual events in videos is generally attacked as a problem
of finding usual patterns, and then separating the ones that do not resemble to
those. In this study, we address the problem from the other side, and try to
answer what type of patterns are shared among unusual videos that make them
resemble to each other regardless of the ongoing event. With this challenging
problem at hand, we propose a novel descriptor to encode the rapid motions in
videos utilizing densely extracted trajectories. The proposed descriptor, which
is referred to as trajectory snipped histograms, is used to distinguish unusual
videos from usual videos, and further exploited to discover snapshots in which
unusualness happen. Experiments on domain specific people falling videos and
unrestricted funny videos show the effectiveness of our method in capturing
unusualness."
"We introduce ConceptVision, a method that aims for high accuracy in
categorizing large number of scenes, while keeping the model relatively simpler
and efficient for scalability. The proposed method combines the advantages of
both low-level representations and high-level semantic categories, and
eliminates the distinctions between different levels through the definition of
concepts. The proposed framework encodes the perspectives brought through
different concepts by considering them in concept groups. Different
perspectives are ensembled for the final decision. Extensive experiments are
carried out on benchmark datasets to test the effects of different concepts,
and methods used to ensemble. Comparisons with state-of-the-art studies show
that we can achieve better results with incorporation of concepts in different
levels with different perspectives."
"Extraction and recognition of Bangla text from video frame images is
challenging due to complex color background, low-resolution etc. In this paper,
we propose an algorithm for extraction and recognition of Bangla text form such
video frames with complex background. Here, a two-step approach has been
proposed. First, the text line is segmented into words using information based
on line contours. First order gradient value of the text blocks are used to
find the word gap. Next, a local binarization technique is applied on each word
and text line is reconstructed using those words. Secondly, this binarized text
block is sent to OCR for recognition purpose."
"We describe a completely automated large scale visual recommendation system
for fashion. Our focus is to efficiently harness the availability of large
quantities of online fashion images and their rich meta-data. Specifically, we
propose four data driven models in the form of Complementary Nearest Neighbor
Consensus, Gaussian Mixture Models, Texture Agnostic Retrieval and Markov Chain
LDA for solving this problem. We analyze relative merits and pitfalls of these
algorithms through extensive experimentation on a large-scale data set and
baseline them against existing ideas from color science. We also illustrate key
fashion insights learned through these experiments and show how they can be
employed to design better recommendation systems. Finally, we also outline a
large-scale annotated data set of fashion images (Fashion-136K) that can be
exploited for future vision research."
"Due to the increasingly need for automatic traffic monitoring, vehicle
license plate detection is of high interest to perform automatic toll
collection, traffic law enforcement, parking lot access control, among others.
In this paper, a sliding window approach based on Histogram of Oriented
Gradients (HOG) features is used for Brazilian license plate detection. This
approach consists in scanning the whole image in a multiscale fashion such that
the license plate is located precisely. The main contribution of this work
consists in a deep study of the best setup for HOG descriptors on the detection
of Brazilian license plates, in which HOG have never been applied before. We
also demonstrate the reliability of this method ensured by a recall higher than
98% (with a precision higher than 78%) in a publicly available data set."
"This paper presents the maneuver of mouse pointer and performs various mouse
operations such as left click, right click, double click, drag etc using
gestures recognition technique. Recognizing gestures is a complex task which
involves many aspects such as motion modeling, motion analysis, pattern
recognition and machine learning. Keeping all the essential factors in mind a
system has been created which recognizes the movement of fingers and various
patterns formed by them. Color caps have been used for fingers to distinguish
it from the background color such as skin color. Thus recognizing the gestures
various mouse events have been performed. The application has been created on
MATLAB environment with operating system as windows 7."
"Here we compare the Boltzmann-Gibbs-Shannon (standard) with the Tsallis
entropy on the pattern recognition and segmentation of coloured images obtained
by satellites, via ""Google Earth"". By segmentation we mean split an image to
locate regions of interest. Here, we discriminate and define an image partition
classes according to a training basis. This training basis consists of three
pattern classes: aquatic, urban and vegetation regions. Our numerical
experiments demonstrate that the Tsallis entropy, used as a feature vector
composed of distinct entropic indexes $q$ outperforms the standard entropy.
There are several applications of our proposed methodology, once satellite
images can be used to monitor migration form rural to urban regions,
agricultural activities, oil spreading on the ocean etc."
"The computation of the geometric transformation between a reference and a
target image, known as registration or alignment, corresponds to the projection
of the target image onto the transformation manifold of the reference image
(the set of images generated by its geometric transformations). It, however,
often takes a nontrivial form such that the exact computation of projections on
the manifold is difficult. The tangent distance method is an effective
algorithm to solve this problem by exploiting a linear approximation of the
manifold. As theoretical studies about the tangent distance algorithm have been
largely overlooked, we present in this work a detailed performance analysis of
this useful algorithm, which can eventually help its implementation. We
consider a popular image registration setting using a multiscale pyramid of
lowpass filtered versions of the (possibly noisy) reference and target images,
which is particularly useful for recovering large transformations. We first
show that the alignment error has a nonmonotonic variation with the filter
size, due to the opposing effects of filtering on both manifold nonlinearity
and image noise. We then study the convergence of the multiscale tangent
distance method to the optimal solution. We finally examine the performance of
the tangent distance method in image classification applications. Our
theoretical findings are confirmed by experiments on image transformation
models involving translations, rotations and scalings. Our study is the first
detailed study of the tangent distance algorithm that leads to a better
understanding of its efficacy and to the proper selection of its design
parameters."
"In this paper, we present an algorithm to automatically detect meaningful
modes in a histogram. The proposed method is based on the behavior of local
minima in a scale-space representation. We show that the detection of such
meaningful modes is equivalent in a two classes clustering problem on the
length of minima scale-space curves. The algorithm is easy to implement, fast,
and does not require any parameters. We present several results on histogram
and spectrum segmentation, grayscale image segmentation and color image
reduction."
"This paper addresses a new learning algorithm for the recently introduced
co-sparse analysis model. First, we give new insights into the co-sparse
analysis model by establishing connections to filter-based MRF models, such as
the Field of Experts (FoE) model of Roth and Black. For training, we introduce
a technique called bi-level optimization to learn the analysis operators.
Compared to existing analysis operator learning approaches, our training
procedure has the advantage that it is unconstrained with respect to the
analysis operator. We investigate the effect of different aspects of the
co-sparse analysis model and show that the sparsity promoting function (also
called penalty function) is the most important factor in the model. In order to
demonstrate the effectiveness of our training approach, we apply our trained
models to various classical image restoration problems. Numerical experiments
show that our trained models clearly outperform existing analysis operator
learning approaches and are on par with state-of-the-art image denoising
algorithms. Our approach develops a framework that is intuitive to understand
and easy to implement."
"One of the main purposes of earth observation is to extract interested
information and knowledge from remote sensing (RS) images with high efficiency
and accuracy. However, with the development of RS technologies, RS system
provide images with higher spatial and temporal resolution and more spectral
channels than before, and it is inefficient and almost impossible to manually
interpret these images. Thus, it is of great interests to explore automatic and
intelligent algorithms to quickly process such massive RS data with high
accuracy. This thesis targets to develop some efficient information extraction
algorithms for RS images, by relying on the advanced technologies in machine
learning. More precisely, we adopt the manifold learning algorithms as the
mainline and unify the regularization theory, tensor-based method, sparse
learning and transfer learning into the same framework. The main contributions
of this thesis are as follows."
"In this paper the Modified Fractal Signature method is applied to real
Synthetic Aperture Radar images provided to our research group by SET 163
Working Group on SAR radar techniques. This method uses the blanket technique
to provide useful information for SAR image classification. It is based on the
calculation of the volume of a blanket, corresponding to the image to be
classified, and then on the calculation of the corresponding Fractal Area curve
and Fractal Dimension curve of the image. The main idea concerning this
proposed technique is the fact that different terrain types encountered in SAR
images yield different values of Fractal Area curves and Fractal Dimension
curves, upon which classification of different types of terrain is possible. As
a result, a classification technique for five different terrain types, i.e.
urban, suburban, rural, mountain and sea, is presented in this paper."
"One of important components in an image retrieval system is selecting a
distance measure to compute rank between two objects. In this paper, several
distance measures were researched to implement a foliage plant retrieval
system. Sixty kinds of foliage plants with various leaf color and shape were
used to test the performance of 7 different kinds of distance measures: city
block distance, Euclidean distance, Canberra distance, Bray-Curtis distance, x2
statistics, Jensen Shannon divergence and Kullback Leibler divergence. The
results show that city block and Euclidean distance measures gave the best
performance among the others."
"This paper proposes a new method for rigid body pose estimation based on
spectrahedral representations of the tautological orbitopes of $SE(2)$ and
$SE(3)$. The approach can use dense point cloud data from stereo vision or an
RGB-D sensor (such as the Microsoft Kinect), as well as visual appearance data.
The method is a convex relaxation of the classical pose estimation problem, and
is based on explicit linear matrix inequality (LMI) representations for the
convex hulls of $SE(2)$ and $SE(3)$. Given these representations, the relaxed
pose estimation problem can be framed as a robust least squares problem with
the optimization variable constrained to these convex sets. Although this
formulation is a relaxation of the original problem, numerical experiments
indicate that it is indeed exact - i.e. its solution is a member of $SE(2)$ or
$SE(3)$ - in many interesting settings. We additionally show that this method
is guaranteed to be exact for a large class of pose estimation problems."
"We consider the analysis operator and synthesis dictionary learning problems
based on the the $\ell_1$ regularized sparse representation model. We reveal
the internal relations between the $\ell_1$-based analysis model and synthesis
model. We then introduce an approach to learn both analysis operator and
synthesis dictionary simultaneously by using a unified framework of bi-level
optimization. Our aim is to learn a meaningful operator (dictionary) such that
the minimum energy solution of the analysis (synthesis)-prior based model is as
close as possible to the ground-truth. We solve the bi-level optimization
problem using the implicit differentiation technique. Moreover, we demonstrate
the effectiveness of our leaning approach by applying the learned analysis
operator (dictionary) to the image denoising task and comparing its performance
with state-of-the-art methods. Under this unified framework, we can compare the
performance of the two types of priors."
"It is now well known that Markov random fields (MRFs) are particularly
effective for modeling image priors in low-level vision. Recent years have seen
the emergence of two main approaches for learning the parameters in MRFs: (1)
probabilistic learning using sampling-based algorithms and (2) loss-specific
training based on MAP estimate. After investigating existing training
approaches, it turns out that the performance of the loss-specific training has
been significantly underestimated in existing work. In this paper, we revisit
this approach and use techniques from bi-level optimization to solve it. We
show that we can get a substantial gain in the final performance by solving the
lower-level problem in the bi-level framework with high accuracy using our
newly proposed algorithm. As a result, our trained model is on par with highly
specialized image denoising algorithms and clearly outperforms
probabilistically trained MRF models. Our findings suggest that for the
loss-specific training scheme, solving the lower-level problem with higher
accuracy is beneficial. Our trained model comes along with the additional
advantage, that inference is extremely efficient. Our GPU-based implementation
takes less than 1s to produce state-of-the-art performance."
"Inpainting based image compression approaches, especially linear and
non-linear diffusion models, are an active research topic for lossy image
compression. The major challenge in these compression models is to find a small
set of descriptive supporting points, which allow for an accurate
reconstruction of the original image. It turns out in practice that this is a
challenging problem even for the simplest Laplacian interpolation model. In
this paper, we revisit the Laplacian interpolation compression model and
introduce two fast algorithms, namely successive preconditioning primal dual
algorithm and the recently proposed iPiano algorithm, to solve this problem
efficiently. Furthermore, we extend the Laplacian interpolation based
compression model to a more general form, which is based on principles from
bi-level optimization. We investigate two different variants of the Laplacian
model, namely biharmonic interpolation and smoothed Total Variation
regularization. Our numerical results show that significant improvements can be
obtained from the biharmonic interpolation model, and it can recover an image
with very high quality from only 5% pixels."
"It remains a challenge to simultaneously remove geometric distortion and
space-time-varying blur in frames captured through a turbulent atmospheric
medium. To solve, or at least reduce these effects, we propose a new scheme to
recover a latent image from observed frames by integrating a new variational
model and distortion-driven spatial-temporal kernel regression. The proposed
scheme first constructs a high-quality reference image from the observed frames
using low-rank decomposition. Then, to generate an improved registered
sequence, the reference image is iteratively optimized using a variational
model containing a new spatial-temporal regularization. The proposed fast
algorithm efficiently solves this model without the use of partial differential
equations (PDEs). Next, to reduce blur variation, distortion-driven
spatial-temporal kernel regression is carried out to fuse the registered
sequence into one image by introducing the concept of the near-stationary
patch. Applying a blind deconvolution algorithm to the fused image produces the
final output. Extensive experimental testing shows, both qualitatively and
quantitatively, that the proposed method can effectively alleviate distortion
and blur and recover details of the original scene compared to state-of-the-art
methods."
"The problem of robust extraction of visual odometry from a sequence of images
obtained by an eye in hand camera configuration is addressed. A novel approach
toward solving planar template based tracking is proposed which performs a
non-linear image alignment for successful retrieval of camera transformations.
In order to obtain global optimum a bio-metaheuristic is used for optimization
of similarity among the planar regions. The proposed method is validated on
image sequences with real as well as synthetic transformations and found to be
resilient to intensity variations. A comparative analysis of the various
similarity measures as well as various state-of-art methods reveal that the
algorithm succeeds in tracking the planar regions robustly and has good
potential to be used in real applications."
"Thresholding is an important task in image processing. It is a main tool in
pattern recognition, image segmentation, edge detection and scene analysis. In
this paper, we present a new thresholding technique based on two-dimensional
Tsallis entropy. The two-dimensional Tsallis entropy was obtained from the
twodimensional histogram which was determined by using the gray value of the
pixels and the local average gray value of the pixels, the work it was applied
a generalized entropy formalism that represents a recent development in
statistical mechanics. The effectiveness of the proposed method is demonstrated
by using examples from the real-world and synthetic images. The performance
evaluation of the proposed technique in terms of the quality of the thresholded
images are presented. Experimental results demonstrate that the proposed method
achieve better result than the Shannon method."
"The randomness and uniqueness of human eye patterns is a major breakthrough
in the search for quicker, easier and highly reliable forms of automatic human
identification. It is being used extensively in security solutions. This
includes access control to physical facilities, security systems and
information databases, Suspect tracking, surveillance and intrusion detection
and by various Intelligence agencies through out the world. We use the
advantage of human eye uniqueness to identify people and approve its validity
as a biometric. . Eye detection involves first extracting the eye from a
digital face image, and then encoding the unique patterns of the eye in such a
way that they can be compared with pre-registered eye patterns. The eye
detection system consists of an automatic segmentation system that is based on
the wavelet transform, and then the Wavelet analysis is used as a pre-processor
for a back propagation neural network with conjugate gradient learning. The
inputs to the neural network are the wavelet maxima neighborhood coefficients
of face images at a particular scale. The output of the neural network is the
classification of the input into an eye or non-eye region. An accuracy of 90%
is observed for identifying test images under different conditions included in
training stage."
"In this work the method of masks, creating and using of inverted image masks,
together with binary operation of image data are used in edge detection of
binary images, monochrome images, which yields about 300 times faster than
ordinary methods. The method is divided into three stages: Mask construction,
Fundamental edge detection, and Edge Construction Comparison with an ordinary
method and a fuzzy based method is carried out."
"To perform unconstrained face recognition robust to variations in
illumination, pose and expression, this paper presents a new scheme to extract
""Multi-Directional Multi-Level Dual-Cross Patterns"" (MDML-DCPs) from face
images. Specifically, the MDMLDCPs scheme exploits the first derivative of
Gaussian operator to reduce the impact of differences in illumination and then
computes the DCP feature at both the holistic and component levels. DCP is a
novel face image descriptor inspired by the unique textural structure of human
faces. It is computationally efficient and only doubles the cost of computing
local binary patterns, yet is extremely robust to pose and expression
variations. MDML-DCPs comprehensively yet efficiently encodes the invariant
characteristics of a face image from multiple levels into patterns that are
highly discriminative of inter-personal differences but robust to
intra-personal variations. Experimental results on the FERET, CAS-PERL-R1, FRGC
2.0, and LFW databases indicate that DCP outperforms the state-of-the-art local
descriptors (e.g. LBP, LTP, LPQ, POEM, tLBP, and LGXP) for both face
identification and face verification tasks. More impressively, the best
performance is achieved on the challenging LFW and FRGC 2.0 databases by
deploying MDML-DCPs in a simple recognition scheme."
"Most vehicle license plate recognition use neural network techniques to
enhance its computing capability. The image of the vehicle license plate is
captured and processed to produce a textual output for further processing. This
paper reviews image processing and neural network techniques applied at
different stages which are preprocessing, filtering, feature extraction,
segmentation and recognition in such way to remove the noise of the image, to
enhance the image quality and to expedite the computing process by converting
the characters in the image into respective text. An exemplar experiment has
been done in MATLAB to show the basic process of the image processing
especially for license plate in Malaysia case study. An algorithm is adapted
into the solution for parking management system. The solution then is
implemented as proof of concept to the algorithm."
"In this paper we address the issues of using edge detection techniques on
facial images to produce cancellable biometric templates and a novel method for
template verification against tampering. With increasing use of biometrics,
there is a real threat for the conventional systems using face databases, which
store images of users in raw and unaltered form. If compromised not only it is
irrevocable, but can be misused for cross-matching across different databases.
So it is desirable to generate and store revocable templates for the same user
in different applications to prevent cross-matching and to enhance security,
while maintaining privacy and ethics. By comparing different edge detection
methods it has been observed that the edge detection based on the Roberts Cross
operator performs consistently well across multiple face datasets, in which the
face images have been taken under a variety of conditions. We have proposed a
novel scheme using hashing, for extra verification, in order to harden the
security of the stored biometric templates."
"In the paper a piecewise constant image approximations of sequential number
of pixel clusters or segments are treated. A majorizing of optimal
approximation sequence by hierarchical sequence of image approximations is
studied. Transition from pixel clustering to image segmentation by reducing of
segment numbers in clusters is provided. Algorithms are proved by elementary
formulas."
"Background modeling is a critical component for various vision-based
applications. Most traditional methods tend to be inefficient when solving
large-scale problems. In this paper, we introduce sparse representation into
the task of large scale stable background modeling, and reduce the video size
by exploring its 'discriminative' frames. A cyclic iteration process is then
proposed to extract the background from the discriminative frame set. The two
parts combine to form our Sparse Outlier Iterative Removal (SOIR) algorithm.
The algorithm operates in tensor space to obey the natural data structure of
videos. Experimental results show that a few discriminative frames determine
the performance of the background extraction. Further, SOIR can achieve high
accuracy and high speed simultaneously when dealing with real video sequences.
Thus, SOIR has an advantage in solving large-scale tasks."
"In the beginning stage, face verification is done using easy method of
geometric algorithm models, but the verification route has now developed into a
scientific progress of complicated geometric representation and matching
process. In modern time the skill have enhanced face detection system into the
vigorous focal point. Researchers currently undergoing strong research on
finding face recognition system for wider area information taken under
hysterical elucidation dissimilarity. The proposed face recognition system
consists of a narrative exposition indiscreet preprocessing method, a hybrid
Fourier-based facial feature extraction and a score fusion scheme. We take in
conventional the face detection in unlike cheer up circumstances and at unusual
setting. Image processing, Image detection, Feature removal and Face detection
are the methods used for Face Verification System . This paper focuses mainly
on the issue of toughness to lighting variations. The proposed system has
obtained an average of verification rate on Two-Dimensional images under
different lightening conditions."
"Character identification plays a vital role in the contemporary world of
Image processing. It can solve many composite problems and makes humans work
easier. An instance is Handwritten Character detection. Handwritten recognition
is not a novel expertise, but it has not gained community notice until Now. The
eventual aim of designing Handwritten Character recognition structure with an
accurateness rate of 100% is pretty illusionary. Tamil Handwritten Character
recognition system uses the Neural Networks to distinguish them. Neural Network
and structural characteristics are used to instruct and recognize written
characters. After training and testing the exactness rate reached 99%. This
correctness rate is extremely high. In this paper we are exploring image
processing through the Hilditch algorithm foundation and structural
characteristics of a character in the image. And we recognized some character
of the Tamil language, and we are trying to identify all the character of Tamil
In our future works."
"In this paper, a concept of multipurpose object detection system, recently
introduced in our previous work, is clarified. The business aspect of this
method is transformation of a classifier into an object detector/locator via an
image grid. This is a universal framework for locating objects of interest
through classification. The framework standardizes and simplifies
implementation of custom systems by doing only a custom analysis of the
classification results on the image grid."
"Advances in computing technology have allowed researchers across many fields
of endeavor to collect and maintain vast amounts of observational statistical
data such as clinical data, biological patient data, data regarding access of
web sites, financial data, and the like. This paper addresses some of the
challenging issues on brain magnetic resonance (MR) image tumor segmentation
caused by the weak correlation between magnetic resonance imaging (MRI)
intensity and anatomical meaning. With the objective of utilizing more
meaningful information to improve brain tumor segmentation, an approach which
employs bilateral symmetry information as an additional feature for
segmentation is proposed. This is motivated by potential performance
improvement in the general automatic brain tumor segmentation systems which are
important for many medical and scientific applications"
"The idea of combining multiple image modalities to provide a single, enhanced
image is well established different fusion methods have been proposed in
literature. This paper is based on image fusion using laplacian pyramid and
wavelet transform method. Images of same size are used for experimentation.
Images used for the experimentation are standard images and averaging filter is
used of equal weights in original images to burl. Performance of image fusion
technique is measured by mean square error, normalized absolute error and peak
signal to noise ratio. From the performance analysis it has been observed that
MSE is decreased in case of both the methods where as PSNR increased, NAE
decreased in case of laplacian pyramid where as constant for wavelet transform
method."
"The connectivity and structural integrity of the white matter of the brain is
nowadays known to be implicated into a wide range of brain-related disorders.
However, it was not before the advent of diffusion Magnetic Resonance Imaging
(dMRI) that researches have been able to examine the properties of white matter
in vivo. Presently, among a range of various methods of dMRI, high angular
resolution diffusion imaging (HARDI) is known to excel in its ability to
provide reliable information about the local orientations of neural fasciculi
(aka fibre tracts). Moreover, as opposed to the more traditional diffusion
tensor imaging (DTI), HARDI is capable of distinguishing the orientations of
multiple fibres passing through a given spatial voxel. Unfortunately, the
ability of HARDI to discriminate between neural fibres that cross each other at
acute angles is always limited, which is the main reason behind the development
of numerous post-processing tools, aiming at the improvement of the directional
resolution of HARDI. Among such tools is spherical deconvolution (SD). Due to
its ill-posed nature, however, SD standardly relies on a number of a priori
assumptions which are to render its results unique and stable. In this paper,
we propose a different approach to the problem of SD in HARDI, which accounts
for the spatial continuity of neural fibres as well as the presence of
isotropic diffusion. Subsequently, we demonstrate how the proposed solution can
be used to successfully overcome the effect of partial voluming, while
preserving the spatial coherency of cerebral diffusion at moderate-to-severe
noise levels. In a series of both in silico and in vivo experiments, the
performance of the proposed method is compared with that of several available
alternatives, with the comparative results clearly supporting the viability and
usefulness of our approach."
"It is convenient to calibrate time-of-flight cameras by established methods,
using images of a chequerboard pattern. The low resolution of the amplitude
image, however, makes it difficult to detect the board reliably. Heuristic
detection methods, based on connected image-components, perform very poorly on
this data. An alternative, geometrically-principled method is introduced here,
based on the Hough transform. The projection of a chequerboard is represented
by two pencils of lines, which are identified as oriented clusters in the
gradient-data of the image. A projective Hough transform is applied to each of
the two clusters, in axis-aligned coordinates. The range of each transform is
properly bounded, because the corresponding gradient vectors are approximately
parallel. Each of the two transforms contains a series of collinear peaks; one
for every line in the given pencil. This pattern is easily detected, by
sweeping a dual line through the transform. The proposed Hough-based method is
compared to the standard OpenCV detection routine, by application to several
hundred time-of-flight images. It is shown that the new method detects
significantly more calibration boards, over a greater variety of poses, without
any overall loss of accuracy. This conclusion is based on an analysis of both
geometric and photometric error."
"Pan-tilt-zoom (PTZ) cameras are powerful to support object identification and
recognition in far-field scenes. However, the effective use of PTZ cameras in
real contexts is complicated by the fact that a continuous on-line camera
calibration is needed and the absolute pan, tilt and zoom positional values
provided by the camera actuators cannot be used because are not synchronized
with the video stream. So, accurate calibration must be directly extracted from
the visual content of the frames. Moreover, the large and abrupt scale changes,
the scene background changes due to the camera operation and the need of camera
motion compensation make target tracking with these cameras extremely
challenging. In this paper, we present a solution that provides continuous
on-line calibration of PTZ cameras which is robust to rapid camera motion,
changes of the environment due to illumination or moving objects and scales
beyond thousands of landmarks. The method directly derives the relationship
between the position of a target in the 3D world plane and the corresponding
scale and position in the 2D image, and allows real-time tracking of multiple
targets with high and stable degree of accuracy even at far distances and any
zooming level."
"These days to gain classification system with high accuracy that can classify
complicated pattern are so useful in medicine and industry. In this article a
process for getting the best classifier for Lasik data is suggested. However at
first it's been tried to find the best line and curve by this classifier in
order to gain classifier fitting, and in the end by using the Markov method a
classifier for topographies is gained."
"Compact and discriminative visual codebooks are preferred in many visual
recognition tasks. In the literature, a number of works have taken the approach
of hierarchically merging visual words of an initial large-sized codebook, but
implemented this approach with different merging criteria. In this work, we
propose a single probabilistic framework to unify these merging criteria, by
identifying two key factors: the function used to model class-conditional
distribution and the method used to estimate the distribution parameters. More
importantly, by adopting new distribution functions and/or parameter estimation
methods, our framework can readily produce a spectrum of novel merging
criteria. Three of them are specifically focused in this work. In the first
criterion, we adopt the multinomial distribution with Bayesian method; In the
second criterion, we integrate Gaussian distribution with maximum likelihood
parameter estimation. In the third criterion, which shows the best merging
performance, we propose a max-margin-based parameter estimation method and
apply it with multinomial distribution. Extensive experimental study is
conducted to systematically analyse the performance of the above three criteria
and compare them with existing ones. As demonstrated, the best criterion
obtained in our framework achieves the overall best merging performance among
the comparable merging criteria developed in the literature."
"Remote sensing image classification can be performed in many different ways
to extract meaningful features. One common approach is to perform edge
detection. A second approach is to try and detect whole shapes, given the fact
that these shapes usually tend to have distinctive properties such as object
foreground or background. To get optimal results, these two approaches can be
combined. This paper adopts a combinatorial optimization method to adaptively
select threshold based features to improve remote sensing image. Feature
selection is an important combinatorial optimization problem in the remote
sensing image classification. The feature selection method has to achieve three
characteristics: first the performance issues by facilitating data collection
and reducing storage space and classification time, second to perform semantics
analysis helping to understand the problem, and third to improve prediction
accuracy by avoiding the curse of dimensionality. The goal of this thresholding
an image is to classify pixels as either dark or light and evaluation of
classification results. Interactive adaptive thresholding is a form of
thresholding that takes into account spatial variations in illumination of
remote sensing image. We present a technique for remote sensing based adaptive
thresholding using the interactive satellite image of the input. However, our
solution is more robust to illumination changes in the remote sensing image.
Additionally, our method is simple and easy to implement but it is effective
algorithm to classify the image pixels. This technique is suitable for
preprocessing the remote sensing image classification, making it a valuable
tool for interactive remote based applications such as augmented reality of the
classification procedure."
"Linear subspace representations of appearance variation are pervasive in
computer vision. This paper addresses the problem of robustly matching such
subspaces (computing the similarity between them) when they are used to
describe the scope of variations within sets of images of different (possibly
greatly so) scales. A naive solution of projecting the low-scale subspace into
the high-scale image space is described first and subsequently shown to be
inadequate, especially at large scale discrepancies. A successful approach is
proposed instead. It consists of (i) an interpolated projection of the
low-scale subspace into the high-scale space, which is followed by (ii) a
rotation of this initial estimate within the bounds of the imposed
``downsampling constraint''. The optimal rotation is found in the closed-form
which best aligns the high-scale reconstruction of the low-scale subspace with
the reference it is compared to. The method is evaluated on the problem of
matching sets of (i) face appearances under varying illumination and (ii)
object appearances under varying viewpoint, using two large data sets. In
comparison to the naive matching, the proposed algorithm is shown to greatly
increase the separation of between-class and within-class similarities, as well
as produce far more meaningful modes of common appearance on which the match
score is based."
"Automatic face recognition is an area with immense practical potential which
includes a wide range of commercial and law enforcement applications. Hence it
is unsurprising that it continues to be one of the most active research areas
of computer vision. Even after over three decades of intense research, the
state-of-the-art in face recognition continues to improve, benefitting from
advances in a range of different research fields such as image processing,
pattern recognition, computer graphics, and physiology. Systems based on
visible spectrum images, the most researched face recognition modality, have
reached a significant level of maturity with some practical success. However,
they continue to face challenges in the presence of illumination, pose and
expression changes, as well as facial disguises, all of which can significantly
decrease recognition accuracy. Amongst various approaches which have been
proposed in an attempt to overcome these limitations, the use of infrared (IR)
imaging has emerged as a particularly promising research direction. This paper
presents a comprehensive and timely review of the literature on this subject.
Our key contributions are: (i) a summary of the inherent properties of infrared
imaging which makes this modality promising in the context of face recognition,
(ii) a systematic review of the most influential approaches, with a focus on
emerging common trends as well as key differences between alternative
methodologies, (iii) a description of the main databases of infrared facial
images available to the researcher, and lastly (iv) a discussion of the most
promising avenues for future research."
"Super-resolution is an important but difficult problem in image/video
processing. If a video sequence or some training set other than the given
low-resolution image is available, this kind of extra information can greatly
aid in the reconstruction of the high-resolution image. The problem is
substantially more difficult with only a single low-resolution image on hand.
The image reconstruction methods designed primarily for denoising is
insufficient for super-resolution problem in the sense that it tends to
oversmooth images with essentially no noise. We propose a new adaptive linear
interpolation method based on variational method and inspired by local linear
embedding (LLE). The experimental result shows that our method avoids the
problem of oversmoothing and preserves image structures well."
"Kernel estimation techniques, such as mean shift, suffer from one major
drawback: the kernel bandwidth selection. The bandwidth can be fixed for all
the data set or can vary at each points. Automatic bandwidth selection becomes
a real challenge in case of multidimensional heterogeneous features. This paper
presents a solution to this problem. It is an extension of \cite{Comaniciu03a}
which was based on the fundamental property of normal distributions regarding
the bias of the normalized density gradient. The selection is done iteratively
for each type of features, by looking for the stability of local bandwidth
estimates across a predefined range of bandwidths. A pseudo balloon mean shift
filtering and partitioning are introduced. The validity of the method is
demonstrated in the context of color image segmentation based on a
5-dimensional space."
"High resolution satellite image sequences are multidimensional signals
composed of spatio-temporal patterns associated to numerous and various
phenomena. Bayesian methods have been previously proposed in (Heas and Datcu,
2005) to code the information contained in satellite image sequences in a graph
representation using Bayesian methods. Based on such a representation, this
paper further presents a supervised learning methodology of semantics
associated to spatio-temporal patterns occurring in satellite image sequences.
It enables the recognition and the probabilistic retrieval of similar events.
Indeed, graphs are attached to statistical models for spatio-temporal
processes, which at their turn describe physical changes in the observed scene.
Therefore, we adjust a parametric model evaluating similarity types between
graph patterns in order to represent user-specific semantics attached to
spatio-temporal phenomena. The learning step is performed by the incremental
definition of similarity types via user-provided spatio-temporal pattern
examples attached to positive or/and negative semantics. From these examples,
probabilities are inferred using a Bayesian network and a Dirichlet model. This
enables to links user interest to a specific similarity model between graph
patterns. According to the current state of learning, semantic posterior
probabilities are updated for all possible graph patterns so that similar
spatio-temporal phenomena can be recognized and retrieved from the image
sequence. Few experiments performed on a multi-spectral SPOT image sequence
illustrate the proposed spatio-temporal recognition method."
"In this paper, we design linear time algorithms to recognize and determine
topological invariants such as the genus and homology groups in 3D. These
properties can be used to identify patterns in 3D image recognition. This has
tremendous amount of applications in 3D medical image analysis. Our method is
based on cubical images with direct adjacency, also called (6,26)-connectivity
images in discrete geometry. According to the fact that there are only six
types of local surface points in 3D and a discrete version of the well-known
Gauss-Bonnett Theorem in differential geometry, we first determine the genus of
a closed 2D-connected component (a closed digital surface). Then, we use
Alexander duality to obtain the homology groups of a 3D object in 3D space."
"This paper proposes a novel algorithm for the problem of structural image
segmentation through an interactive model-based approach. Interaction is
expressed in the model creation, which is done according to user traces drawn
over a given input image. Both model and input are then represented by means of
attributed relational graphs derived on the fly. Appearance features are taken
into account as object attributes and structural properties are expressed as
relational attributes. To cope with possible topological differences between
both graphs, a new structure called the deformation graph is introduced. The
segmentation process corresponds to finding a labelling of the input graph that
minimizes the deformations introduced in the model when it is updated with
input information. This approach has shown to be faster than other segmentation
methods, with competitive output quality. Therefore, the method solves the
problem of multiple label segmentation in an efficient way. Encouraging results
on both natural and target-specific color images, as well as examples showing
the reusability of the model, are presented and discussed."
"By considering the features of the airport runway image filtering, an
improved bilateral filtering method was proposed which can remove noise with
edge preserving. Firstly the steerable filtering decomposition is used to
calculate the sub-band parameters of 4 orients, and the texture feature matrix
is then obtained from the sub-band local median energy. The texture similar,
the spatial closer and the color similar functions are used to filter the
image.The effect of the weighting function parameters is qualitatively analyzed
also. In contrast with the standard bilateral filter and the simulation results
for the real airport runway image show that the multilateral filtering is more
effective than the standard bilateral filtering."
"Methods of increasing linear optical dynamic range of commercial photocamera
for optical-digital imaging systems are described. Use of such methods allows
to use commercial photocameras for optical measurements. Experimental results
are reported."
"In this paper, we focus on statistical region-based active contour models
where image features (e.g. intensity) are random variables whose distribution
belongs to some parametric family (e.g. exponential) rather than confining
ourselves to the special Gaussian case. Using shape derivation tools, our
effort focuses on constructing a general expression for the derivative of the
energy (with respect to a domain) and derive the corresponding evolution speed.
A general result is stated within the framework of multi-parameter exponential
family. More particularly, when using Maximum Likelihood estimators, the
evolution speed has a closed-form expression that depends simply on the
probability density function, while complicating additive terms appear when
using other estimators, e.g. moments method. Experimental results on both
synthesized and real images demonstrate the applicability of our approach."
"In this paper, we propose to combine formally noise and shape priors in
region-based active contours. On the one hand, we use the general framework of
exponential family as a prior model for noise. On the other hand, translation
and scale invariant Legendre moments are considered to incorporate the shape
prior (e.g. fidelity to a reference shape). The combination of the two prior
terms in the active contour functional yields the final evolution equation
whose evolution speed is rigorously derived using shape derivative tools.
Experimental results on both synthetic images and real life cardiac echography
data clearly demonstrate the robustness to initialization and noise,
flexibility and large potential applicability of our segmentation algorithm."
"Feature selection is a pattern recognition approach to choose important
variables according to some criteria to distinguish or explain certain
phenomena. There are many genomic and proteomic applications which rely on
feature selection to answer questions such as: selecting signature genes which
are informative about some biological state, e.g. normal tissues and several
types of cancer; or defining a network of prediction or inference among
elements such as genes, proteins, external stimuli and other elements of
interest. In these applications, a recurrent problem is the lack of samples to
perform an adequate estimate of the joint probabilities between element states.
A myriad of feature selection algorithms and criterion functions are proposed,
although it is difficult to point the best solution in general. The intent of
this work is to provide an open-source multiplataform graphical environment to
apply, test and compare many feature selection approaches suitable to be used
in bioinformatics problems."
"In block-matching motion estimation (BMME), the search patterns have a
significant impact on the algorithm's performance, both the search speed and
the search quality. The search pattern should be designed to fit the motion
vector probability (MVP) distribution characteristics of the real-world
sequences. In this paper, we build a directional model of MVP distribution to
describe the directional-center-biased characteristic of the MVP distribution
and the directional characteristics of the conditional MVP distribution more
exactly based on the detailed statistical data of motion vectors of eighteen
popular sequences. Three directional search patterns are firstly designed by
utilizing the directional characteristics and they are the smallest search
patterns among the popular ones. A new algorithm is proposed using the
horizontal cross search pattern as the initial step and the horizontal/vertical
diamond search pattern as the subsequent step for the fast BMME, which is
called the directional cross diamond search (DCDS) algorithm. The DCDS
algorithm can obtain the motion vector with fewer search points than CDS, DS or
HEXBS while maintaining the similar or even better search quality. The gain on
speedup of DCDS over CDS or DS can be up to 54.9%. The simulation results show
that DCDS is efficient, effective and robust, and it can always give the faster
search speed on different sequences than other fast block-matching algorithm in
common use."
"We investigate a biologically motivated approach to fast visual
classification, directly inspired by the recent work of Serre et al.
Specifically, trading-off biological accuracy for computational efficiency, we
explore using wavelet and grouplet-like transforms to parallel the tuning of
visual cortex V1 and V2 cells, alternated with max operations to achieve scale
and translation invariance. A feature selection procedure is applied during
learning to accelerate recognition. We introduce a simple attention-like
feedback mechanism, significantly improving recognition and robustness in
multiple-object scenes. In experiments, the proposed algorithm achieves or
exceeds state-of-the-art success rate on object recognition, texture and
satellite image classification, language identification and sound
classification."
"We propose a robust classification algorithm for curves in 2D and 3D, under
the special and full groups of affine transformations. To each plane or spatial
curve we assign a plane signature curve. Curves, equivalent under an affine
transformation, have the same signature. The signatures introduced in this
paper are based on integral invariants, which behave much better on noisy
images than classically known differential invariants. The comparison with
other types of invariants is given in the introduction. Though the integral
invariants for planar curves were known before, the affine integral invariants
for spatial curves are proposed here for the first time. Using the inductive
variation of the moving frame method we compute affine invariants in terms of
Euclidean invariants. We present two types of signatures, the global signature
and the local signature. Both signatures are independent of parameterization
(curve sampling). The global signature depends on the choice of the initial
point and does not allow us to compare fragments of curves, and is therefore
sensitive to occlusions. The local signature, although is slightly more
sensitive to noise, is independent of the choice of the initial point and is
not sensitive to occlusions in an image. It helps establish local equivalence
of curves. The robustness of these invariants and signatures in their
application to the problem of classification of noisy spatial curves extracted
from a 3D object is analyzed."
"Adams and Bishop have proposed in 1994 a novel region growing algorithm
called seeded region growing by pixels aggregation (SRGPA). This paper
introduces a framework to implement an algorithm using SRGPA. This framework is
built around two concepts: localization and organization of applied action.
This conceptualization gives a quick implementation of algorithms, a direct
translation between the mathematical idea and the numerical implementation, and
an improvement of algorithms efficiency."
"In the previous paper, we have conceptualized the localization and the
organization of seeded region growing by pixels aggregation (SRGPA) but we do
not give the issue when there is a collision between two distinct regions
during the growing process. In this paper, we propose two implementations to
manage two classical growing processes: one without a boundary region region to
divide the other regions and another with. Unfortunately, as noticed by Mehnert
and Jakway (1997), this partition depends on the seeded region initialisation
order (SRIO). We propose a growing process, invariant about SRIO such as the
boundary region is the set of ambiguous pixels."
"In the two previous papers of this serie, we have created a library, called
Population, dedicated to seeded region growing by pixels aggregation and we
have proposed different growing processes to get a partition with or without a
boundary region to divide the other regions or to get a partition invariant
about the seeded region initialisation order. Using this work, we implement
some algorithms belonging to the field of SRGPA using this library and these
growing processes."
"This paper proposes a simple, generic and robust method to extract the grains
from experimental tridimensionnal images of granular materials obtained by
X-ray tomography. This extraction has two steps: segmentation and splitting.
For the segmentation step, if there is a sufficient contrast between the
different components, a classical threshold procedure followed by a succession
of morphological filters can be applied. If not, and if the boundary needs to
be localized precisely, a watershed transformation controlled by labels is
applied. The basement of this transformation is to localize a label included in
the component and another label in the component complementary. A ""soft""
threshold following by an opening is applied on the initial image to localize a
label in a component. For any segmentation procedure, the visualisation shows a
problem: some groups of two grains, close one to each other, become connected.
So if a classical cluster procedure is applied on the segmented binary image,
these numerical connected grains are considered as a single grain. To overcome
this problem, we applied a procedure introduced by L. Vincent in 1993. This
grains extraction is tested for various complexes porous media and granular
material, to predict various properties (diffusion, electrical conductivity,
deformation field) in a good agreement with experiment data."
"Dipole and higher moments are physical quantities used to describe a charge
distribution. In analogy with electromagnetism, it is possible to define the
dipole moments for a gray-scale image, according to the single aspect of a
gray-tone map. In this paper we define the color dipole moments for color
images. For color maps in fact, we have three aspects, the three primary
colors, to consider. Associating three color charges to each pixel, color
dipole moments can be easily defined and used for edge detection."
"We show the closed-form solution to the maximization of trace(A'R), where A
is given and R is unknown rotation matrix. This problem occurs in many computer
vision tasks involving optimal rotation matrix estimation. The solution has
been continuously reinvented in different fields as part of specific problems.
We summarize the historical evolution of the problem and present the general
proof of the solution. We contribute to the proof by considering the degenerate
cases of A and discuss the uniqueness of R."
"Point set registration is a key component in many computer vision tasks. The
goal of point set registration is to assign correspondences between two sets of
points and to recover the transformation that maps one point set to the other.
Multiple factors, including an unknown non-rigid spatial transformation, large
dimensionality of point set, noise and outliers, make the point set
registration a challenging problem. We introduce a probabilistic method, called
the Coherent Point Drift (CPD) algorithm, for both rigid and non-rigid point
set registration. We consider the alignment of two point sets as a probability
density estimation problem. We fit the GMM centroids (representing the first
point set) to the data (the second point set) by maximizing the likelihood. We
force the GMM centroids to move coherently as a group to preserve the
topological structure of the point sets. In the rigid case, we impose the
coherence constraint by re-parametrization of GMM centroid locations with rigid
parameters and derive a closed form solution of the maximization step of the EM
algorithm in arbitrary dimensions. In the non-rigid case, we impose the
coherence constraint by regularizing the displacement field and using the
variational calculus to derive the optimal transformation. We also introduce a
fast algorithm that reduces the method computation complexity to linear. We
test the CPD algorithm for both rigid and non-rigid transformations in the
presence of noise, outliers and missing points, where CPD shows accurate
results and outperforms current state-of-the-art methods."
"Natural images in the colour space YUV have been observed to have a
non-Gaussian, heavy tailed distribution (called 'sparse') when the filter
G(U)(r) = U(r) - sum_{s \in N(r)} w{(Y)_{rs}} U(s), is applied to the
chromacity channel U (and equivalently to V), where w is a weighting function
constructed from the intensity component Y [1]. In this paper we develop
Bayesian analysis of the colorization problem using the filter response as a
regularization term to arrive at a non-convex optimization problem. This
problem is convexified using L1 optimization which often gives the same results
for sparse signals [2]. It is observed that L1 optimization, in many cases,
over-performs the famous colorization algorithm by Levin et al [3]."
"A statistical learning/inference framework for color demosaicing is
presented. We start with simplistic assumptions about color constancy, and
recast color demosaicing as a blind linear inverse problem: color parameterizes
the unknown kernel, while brightness takes on the role of a latent variable. An
expectation-maximization algorithm naturally suggests itself for the estimation
of them both. Then, as we gradually broaden the family of hypothesis where
color is learned, we let our demosaicing behave adaptively, in a manner that
reflects our prior knowledge about the statistics of color images. We show that
we can incorporate realistic, learned priors without essentially changing the
complexity of the simple expectation-maximization algorithm we started with."
"This paper presents a new method to recover the relative pose between two
images, using three points and the vertical direction information. The vertical
direction can be determined in two ways: 1- using direct physical measurement
like IMU (inertial measurement unit), 2- using vertical vanishing point. This
knowledge of the vertical direction solves 2 unknowns among the 3 parameters of
the relative rotation, so that only 3 homologous points are requested to
position a couple of images. Rewriting the coplanarity equations leads to a
simpler solution. The remaining unknowns resolution is performed by an
algebraic method using Grobner bases. The elements necessary to build a
specific algebraic solver are given in this paper, allowing for a real-time
implementation. The results on real and synthetic data show the efficiency of
this method."
"In this paper, we use semi-definite programming and generalized principal
component analysis (GPCA) to distinguish between two or more different facial
expressions. In the first step, semi-definite programming is used to reduce the
dimension of the image data and ""unfold"" the manifold which the data points
(corresponding to facial expressions) reside on. Next, GPCA is used to fit a
series of subspaces to the data points and associate each data point with a
subspace. Data points that belong to the same subspace are claimed to belong to
the same facial expression category. An example is provided."
"This paper defines the basis of a new hierarchical framework for segmentation
algorithms based on energy minimization schemes. This new framework is based on
two formal tools. First, a combinatorial pyramid encode efficiently a hierarchy
of partitions. Secondly, discrete geometric estimators measure precisely some
important geometric parameters of the regions. These measures combined with
photometrical and topological features of the partition allows to design energy
terms based on discrete measures. Our segmentation framework exploits these
energies to build a pyramid of image partitions with a minimization scheme.
Some experiments illustrating our framework are shown and discussed."
"We present a parametric deformable model which recovers image components with
a complexity independent from the resolution of input images. The proposed
model also automatically changes its topology and remains fully compatible with
the general framework of deformable models. More precisely, the image space is
equipped with a metric that expands salient image details according to their
strength and their curvature. During the whole evolution of the model, the
sampling of the contour is kept regular with respect to this metric. By this
way, the vertex density is reduced along most parts of the curve while a high
quality of shape representation is preserved. The complexity of the deformable
model is thus improved and is no longer influenced by feature-preserving
changes in the resolution of input images. Building the metric requires a prior
estimation of contour curvature. It is obtained using a robust estimator which
investigates the local variations in the orientation of image gradient.
Experimental results on both computer generated and biomedical images are
presented to illustrate the advantages of our approach."
"We introduce an adaptive regularization approach. In contrast to conventional
Tikhonov regularization, which specifies a fixed regularization operator, we
estimate it simultaneously with parameters. From a Bayesian perspective we
estimate the prior distribution on parameters assuming that it is close to some
given model distribution. We constrain the prior distribution to be a
Gauss-Markov random field (GMRF), which allows us to solve for the prior
distribution analytically and provides a fast optimization algorithm. We apply
our approach to non-rigid image registration to estimate the spatial
transformation between two images. Our evaluation shows that the adaptive
regularization approach significantly outperforms standard variational methods."
"Quality control is an important issue in the ceramic tile industry. On the
other hand maintaining the rate of production with respect to time is also a
major issue in ceramic tile manufacturing. Again, price of ceramic tiles also
depends on purity of texture, accuracy of color, shape etc. Considering this
criteria, an automated defect detection and classification technique has been
proposed in this report that can have ensured the better quality of tiles in
manufacturing process as well as production rate. Our proposed method plays an
important role in ceramic tiles industries to detect the defects and to control
the quality of ceramic tiles. This automated classification method helps us to
acquire knowledge about the pattern of defect within a very short period of
time and also to decide about the recovery process so that the defected tiles
may not be mixed with the fresh tiles."
"Image segmentation techniques are predominately based on parameter-laden
optimization. The objective function typically involves weights for balancing
competing image fidelity and segmentation regularization cost terms. Setting
these weights suitably has been a painstaking, empirical process. Even if such
ideal weights are found for a novel image, most current approaches fix the
weight across the whole image domain, ignoring the spatially-varying properties
of object shape and image appearance. We propose a novel technique that
autonomously balances these terms in a spatially-adaptive manner through the
incorporation of image reliability in a graph-based segmentation framework. We
validate on synthetic data achieving a reduction in mean error of 47% (p-value
<< 0.05) when compared to the best fixed parameter segmentation. We also
present results on medical images (including segmentations of the corpus
callosum and brain tissue in MRI data) and on natural images."
"The selection of the optimal feature subset and the classification has become
an important issue in the field of iris recognition. In this paper we propose
several methods for iris feature subset selection and vector creation. The
deterministic feature sequence is extracted from the iris image by using the
contourlet transform technique. Contourlet transform captures the intrinsic
geometrical structures of iris image. It decomposes the iris image into a set
of directional sub-bands with texture details captured in different
orientations at various scales so for reducing the feature vector dimensions we
use the method for extract only significant bit and information from normalized
iris images. In this method we ignore fragile bits. And finally we use SVM
(Support Vector Machine) classifier for approximating the amount of people
identification in our proposed system. Experimental result show that most
proposed method reduces processing time and increase the classification
accuracy and also the iris feature vector length is much smaller versus the
other methods."
"We present in this paper a new approach for hand gesture analysis that allows
digit recognition. The analysis is based on extracting a set of features from a
hand image and then combining them by using an induction graph. The most
important features we extract from each image are the fingers locations, their
heights and the distance between each pair of fingers. Our approach consists of
three steps: (i) Hand detection and localization, (ii) fingers extraction and
(iii) features identification and combination to digit recognition. Each input
image is assumed to contain only one person, thus we apply a fuzzy classifier
to identify the skin pixels. In the finger extraction step, we attempt to
remove all the hand components except the fingers, this process is based on the
hand anatomy properties. The final step consists on representing histogram of
the detected fingers in order to extract features that will be used for digit
recognition. The approach is invariant to scale, rotation and translation of
the hand. Some experiments have been undertaken to show the effectiveness of
the proposed approach."
"We exam various geometric active contour methods for radar image
segmentation. Due to special properties of radar images, we propose our new
model based on modified Chan-Vese functional. Our method is efficient in
separating non-meteorological noises from meteorological images."
"A hierarchical interval subdivision is shown to lead to a $p$-adic encoding
of image data. This allows in the case of the relative pose problem in computer
vision and photogrammetry to derive equations having 2-adic numbers as
coefficients, and to use Hensel's lifting method to their solution. This method
is applied to the linear and non-linear equations coming from eight, seven or
five point correspondences. An inherent property of the method is its
robustness."
"Neural Networks are being used for character recognition from last many years
but most of the work was confined to English character recognition. Till date,
a very little work has been reported for Handwritten Farsi Character
recognition. In this paper, we have made an attempt to recognize handwritten
Farsi characters by using a multilayer perceptron with one hidden layer. The
error backpropagation algorithm has been used to train the MLP network. In
addition, an analysis has been carried out to determine the number of hidden
nodes to achieve high performance of backpropagation network in the recognition
of handwritten Farsi characters. The system has been trained using several
different forms of handwriting provided by both male and female participants of
different age groups. Finally, this rigorous training results an automatic HCR
system using MLP network. In this work, the experiments were carried out on two
hundred fifty samples of five writers. The results showed that the MLP networks
trained by the error backpropagation algorithm are superior in recognition
accuracy and memory usage. The result indicates that the backpropagation
network provides good recognition accuracy of more than 80% of handwritten
Farsi characters."
"The nearest neighbor (NN) technique is very simple, highly efficient and
effective in the field of pattern recognition, text categorization, object
recognition etc. Its simplicity is its main advantage, but the disadvantages
can't be ignored even. The memory requirement and computation complexity also
matter. Many techniques are developed to overcome these limitations. NN
techniques are broadly classified into structure less and structure based
techniques. In this paper, we present the survey of such techniques. Weighted
kNN, Model based kNN, Condensed NN, Reduced NN, Generalized NN are structure
less techniques whereas k-d tree, ball tree, Principal Axis Tree, Nearest
Feature Line, Tunable NN, Orthogonal Search Tree are structure based algorithms
developed on the basis of kNN. The structure less method overcome memory
limitation and structure based techniques reduce the computational complexity."
"This paper presents a method for improving any object tracking algorithm
based on machine learning. During the training phase, important trajectory
features are extracted which are then used to calculate a confidence value of
trajectory. The positions at which objects are usually lost and found are
clustered in order to construct the set of 'lost zones' and 'found zones' in
the scene. Using these zones, we construct a triplet set of zones i.e. three
zones: In/Out zone (zone where an object can enter or exit the scene), 'lost
zone' and 'found zone'. Thanks to these triplets, during the testing phase, we
can repair the erroneous trajectories according to which triplet they are most
likely to belong to. The advantage of our approach over the existing state of
the art approaches is that (i) this method does not depend on a predefined
contextual scene, (ii) we exploit the semantic of the scene and (iii) we have
proposed a method to filter out noisy trajectories based on their confidence
value."
"Many techniques have been proposed to speedup the performance of classic
Hough Transform. These techniques are primarily based on converting the voting
procedure to a hierarchy based voting method. These methods use approximate
decision-making process. In this paper, we propose a fast decision making
process that enhances the speed and reduces the space requirements.
Experimental results demonstrate that the proposed algorithm is much faster
than a similar Fast Hough Transform."
"This paper aims at determining the characteristics of a face image by
extracting its components. The FASY (FAce SYnthesis) System is a Face Database
Retrieval and new Face generation System that is under development. One of its
main features is the generation of the requested face when it is not found in
the existing database, which allows a continuous growing of the database also.
To generate the new face image, we need to store the face components in the
database. So we have designed a new technique to extract the face components by
a sophisticated method. After extraction of the facial feature points we have
analyzed the components to determine their characteristics. After extraction
and analysis we have stored the components along with their characteristics
into the face database for later use during the face construction."
"This paper investigates the multiresolution level-1 and level-2 Quotient
based Fusion of thermal and visual images. In the proposed system, the method-1
namely ""Decompose then Quotient Fuse Level-1"" and the method-2 namely
""Decompose-Reconstruct then Quotient Fuse Level-2"" both work on wavelet
transformations of the visual and thermal face images. The wavelet transform is
well-suited to manage different image resolution and allows the image
decomposition in different kinds of coefficients, while preserving the image
information without any loss. This approach is based on a definition of an
illumination invariant signature image which enables an analytic generation of
the image space with varying illumination. The quotient fused images are passed
through Principal Component Analysis (PCA) for dimension reduction and then
those images are classified using a multi-layer perceptron (MLP). The
performances of both the methods have been evaluated using OTCBVS and IRIS
databases. All the different classes have been tested separately, among them
the maximum recognition result is 100%."
"In this paper fusion of visual and thermal images in wavelet transformed
domain has been presented. Here, Daubechies wavelet transform, called as D2,
coefficients from visual and corresponding coefficients computed in the same
manner from thermal images are combined to get fused coefficients. After
decomposition up to fifth level (Level 5) fusion of coefficients is done.
Inverse Daubechies wavelet transform of those coefficients gives us fused face
images. The main advantage of using wavelet transform is that it is well-suited
to manage different image resolution and allows the image decomposition in
different kinds of coefficients, while preserving the image information. Fused
images thus found are passed through Principal Component Analysis (PCA) for
reduction of dimensions and then those reduced fused images are classified
using a multi-layer perceptron. For experiments IRIS Thermal/Visual Face
Database was used. Experimental results show that the performance of the
approach presented here achieves maximum success rate of 100% in many cases."
"In this paper we present a comparative study on fusion of visual and thermal
images using different wavelet transformations. Here, coefficients of discrete
wavelet transforms from both visual and thermal images are computed separately
and combined. Next, inverse discrete wavelet transformation is taken in order
to obtain fused face image. Both Haar and Daubechies (db2) wavelet transforms
have been used to compare recognition results. For experiments IRIS
Thermal/Visual Face Database was used. Experimental results using Haar and
Daubechies wavelets show that the performance of the approach presented here
achieves maximum success rate of 100% in many cases."
"Artificial neural networks have already shown their success in face
recognition and similar complex pattern recognition tasks. However, a major
disadvantage of the technique is that it is extremely slow during training for
larger classes and hence not suitable for real-time complex problems such as
pattern recognition. This is an attempt to develop a parallel framework for the
training algorithm of a perceptron. In this paper, two general architectures
for a Multilayer Perceptron (MLP) have been demonstrated. The first
architecture is All-Class-in-One-Network (ACON) where all the classes are
placed in a single network and the second one is One-Class-in-One-Network
(OCON) where an individual single network is responsible for each and every
class. Capabilities of these two architectures were compared and verified in
solving human face recognition, which is a complex pattern recognition task
where several factors affect the recognition performance like pose variations,
facial expression changes, occlusions, and most importantly illumination
changes. Both the structures were implemented and tested for face recognition
purpose and experimental results show that the OCON structure performs better
than the generally used ACON ones in term of training convergence speed of the
network. Unlike the conventional sequential approach of training the neural
networks, the OCON technique may be implemented by training all the classes of
the face images simultaneously."
"In this paper we present a technique for fusion of optical and thermal face
images based on image pixel fusion approach. Out of several factors, which
affect face recognition performance in case of visual images, illumination
changes are a significant factor that needs to be addressed. Thermal images are
better in handling illumination conditions but not very consistent in capturing
texture details of the faces. Other factors like sunglasses, beard, moustache
etc also play active role in adding complicacies to the recognition process.
Fusion of thermal and visual images is a solution to overcome the drawbacks
present in the individual thermal and visual face images. Here fused images are
projected into an eigenspace and the projected images are classified using a
radial basis function (RBF) neural network and also by a multi-layer perceptron
(MLP). In the experiments Object Tracking and Classification Beyond Visible
Spectrum (OTCBVS) database benchmark for thermal and visual face images have
been used. Comparison of experimental results show that the proposed approach
performs significantly well in recognizing face images with a success rate of
96% and 95.07% for RBF Neural Network and MLP respectively."
"Here an efficient fusion technique for automatic face recognition has been
presented. Fusion of visual and thermal images has been done to take the
advantages of thermal images as well as visual images. By employing fusion a
new image can be obtained, which provides the most detailed, reliable, and
discriminating information. In this method fused images are generated using
visual and thermal face images in the first step. In the second step, fused
images are projected into eigenspace and finally classified using a radial
basis function neural network. In the experiments Object Tracking and
Classification Beyond Visible Spectrum (OTCBVS) database benchmark for thermal
and visual face images have been used. Experimental results show that the
proposed approach performs well in recognizing unknown individuals with a
maximum success rate of 96%."
"This paper presents a concept of image pixel fusion of visual and thermal
faces, which can significantly improve the overall performance of a face
recognition system. Several factors affect face recognition performance
including pose variations, facial expression changes, occlusions, and most
importantly illumination changes. So, image pixel fusion of thermal and visual
images is a solution to overcome the drawbacks present in the individual
thermal and visual face images. Fused images are projected into eigenspace and
finally classified using a multi-layer perceptron. In the experiments we have
used Object Tracking and Classification Beyond Visible Spectrum (OTCBVS)
database benchmark thermal and visual face images. Experimental results show
that the proposed approach significantly improves the verification and
identification performance and the success rate is 95.07%. The main objective
of employing fusion is to produce a fused image that provides the most detailed
and reliable information. Fusion of multiple images together produces a more
efficient representation of the image."
"In this paper we present a simple novel approach to tackle the challenges of
scaling and rotation of face images in face recognition. The proposed approach
registers the training and testing visual face images by log-polar
transformation, which is capable to handle complicacies introduced by scaling
and rotation. Log-polar images are projected into eigenspace and finally
classified using an improved multi-layer perceptron. In the experiments we have
used ORL face database and Object Tracking and Classification Beyond Visible
Spectrum (OTCBVS) database for visual face images. Experimental results show
that the proposed approach significantly improves the recognition performances
from visual to log-polar-visual face images. In case of ORL face database,
recognition rate for visual face images is 89.5% and that is increased to 97.5%
for log-polar-visual face images whereas for OTCBVS face database recognition
rate for visual images is 87.84% and 96.36% for log-polar-visual face images."
"In this work we investigate a novel approach to handle the challenges of face
recognition, which includes rotation, scale, occlusion, illumination etc. Here,
we have used thermal face images as those are capable to minimize the affect of
illumination changes and occlusion due to moustache, beards, adornments etc.
The proposed approach registers the training and testing thermal face images in
polar coordinate, which is capable to handle complicacies introduced by scaling
and rotation. Line features are extracted from thermal polar images and feature
vectors are constructed using these line. Feature vectors thus obtained passes
through principal component analysis (PCA) for the dimensionality reduction of
feature vectors. Finally, the images projected into eigenspace are classified
using a multi-layer perceptron. In the experiments we have used Object Tracking
and Classification Beyond Visible Spectrum (OTCBVS) database. Experimental
results show that the proposed approach significantly improves the verification
and identification performance and the success rate is 99.25%."
"Nonlinear bilateral filters (BF) deliver a fine blend of computational
simplicity and blur-free denoising. However, little is known about their
nature, noise-suppressing properties, and optimal choices of filter parameters.
Our study is meant to fill this gap-explaining the underlying mechanism of
bilateral filtering and providing the methodology for optimal filter selection.
Practical application to CT image denoising is discussed to illustrate our
results."
"A lot of image registration techniques have been developed with great
significance for data analysis in medicine, astrophotography, satellite imaging
and few other areas. This work proposes a method for medical image registration
using Fast Walsh Hadamard transform. This algorithm registers images of the
same or different modalities. Each image bit is lengthened in terms of Fast
Walsh Hadamard basis functions. Each basis function is a notion of determining
various aspects of local structure, e.g., horizontal edge, corner, etc. These
coefficients are normalized and used as numerals in a chosen number system
which allows one to form a unique number for each type of local structure. The
experimental results show that Fast Walsh Hadamard transform accomplished
better results than the conventional Walsh transform in the time domain. Also
Fast Walsh Hadamard transform is more reliable in medical image registration
consuming less time."
"The nematode Caenorhabditis elegans is a well-known model organism used to
investigate fundamental questions in biology. Motility assays of this small
roundworm are designed to study the relationships between genes and behavior.
Commonly, motility analysis is used to classify nematode movements and
characterize them quantitatively. Over the past years, C. elegans' motility has
been studied across a wide range of environments, including crawling on
substrates, swimming in fluids, and locomoting through microfluidic substrates.
However, each environment often requires customized image processing tools
relying on heuristic parameter tuning. In the present study, we propose a novel
Multi-Environment Model Estimation (MEME) framework for automated image
segmentation that is versatile across various environments. The MEME platform
is constructed around the concept of Mixture of Gaussian (MOG) models, where
statistical models for both the background environment and the nematode
appearance are explicitly learned and used to accurately segment a target
nematode. Our method is designed to simplify the burden often imposed on users;
here, only a single image which includes a nematode in its environment must be
provided for model learning. In addition, our platform enables the extraction
of nematode `skeletons' for straightforward motility quantification. We test
our algorithm on various locomotive environments and compare performances with
an intensity-based thresholding method. Overall, MEME outperforms the
threshold-based approach for the overwhelming majority of cases examined.
Ultimately, MEME provides researchers with an attractive platform for C.
elegans' segmentation and `skeletonizing' across a wide range of motility
assays."
"RANSAC is a popular technique for estimating model parameters in the presence
of outliers. The best speed is achieved when the minimum possible number of
points is used to estimate hypotheses for the model. Many useful problems can
be represented using polynomial constraints (for instance, the determinant of a
fundamental matrix must be zero) and so have a number of solutions which are
consistent with a minimal set. A considerable amount of effort has been
expended on finding the constraints of such problems, and these often require
the solution of systems of polynomial equations. We show that better
performance can be achieved by using a simple optimization based approach on
minimal sets. For a given minimal set, the optimization approach is not
guaranteed to converge to the correct solution. However, when used within
RANSAC the greater speed and numerical stability results in better performance
overall, and much simpler algorithms. We also show that by selecting more than
the minimal number of points and using robust optimization can yield better
results for very noisy by reducing the number of trials required. The increased
speed of our method demonstrated with experiments on essential matrix
estimation."
"Template matching is one of the simplest methods used for eyes and mouth
detection. However, it can be modified and extended to become a powerful tool.
Since the patch itself plays a significant role in optimizing detection
performance, a study on the influence of patch size and shape is carried out.
The optimum patch size and shape is determined using the proposed method.
Usually, template matching is also combined with other methods in order to
improve detection accuracy. Thus, in this paper, the effectiveness of two image
processing methods i.e. grayscale and Haar wavelet transform, when used with
template matching are analyzed."
"We propose a new approach for constructing a 3D representation from a 2D
wireframe drawing. A drawing is simply a parallel projection of a 3D object
onto a 2D surface; humans are able to recreate mental 3D models from 2D
representations very easily, yet the process is very difficult to emulate
computationally. We hypothesize that our ability to perform this construction
relies on the angles in the 2D scene, among other geometric properties. Being
able to reproduce this reconstruction process automatically would allow for
efficient and robust 3D sketch interfaces. Our research focuses on the
relationship between 2D geometry observable in the sketch and 3D geometry
derived from a potential 3D construction. We present a fully automated system
that constructs 3D representations from 2D wireframes using a neural network in
conjunction with a genetic search algorithm."
"VERSA provides a general-purpose framework for defining and recognizing
events in live or recorded surveillance video streams. The approach for event
recognition in VERSA is using a declarative logic language to define the
spatial and temporal relationships that characterize a given event or activity.
Doing so requires the definition of certain fundamental spatial and temporal
relationships and a high-level syntax for specifying frame templates and query
parameters. Although the handling of uncertainty in the current VERSA
implementation is simplistic, the language and architecture is amenable to
extending using Fuzzy Logic or similar approaches. VERSA's high-level
architecture is designed to work in XML-based, services- oriented environments.
VERSA can be thought of as subscribing to the XML annotations streamed by a
lower-level video analytics service that provides basic entity detection,
labeling, and tracking. One or many VERSA Event Monitors could thus analyze
video streams and provide alerts when certain events are detected."
"This paper proposes a robust ear identification system which is developed by
fusing SIFT features of color segmented slice regions of an ear. The proposed
ear identification method makes use of Gaussian mixture model (GMM) to build
ear model with mixture of Gaussian using vector quantization algorithm and K-L
divergence is applied to the GMM framework for recording the color similarity
in the specified ranges by comparing color similarity between a pair of
reference ear and probe ear. SIFT features are then detected and extracted from
each color slice region as a part of invariant feature extraction. The
extracted keypoints are then fused separately by the two fusion approaches,
namely concatenation and the Dempster-Shafer theory. Finally, the fusion
approaches generate two independent augmented feature vectors which are used
for identification of individuals separately. The proposed identification
technique is tested on IIT Kanpur ear database of 400 individuals and is found
to achieve 98.25% accuracy for identification while top 5 matched criteria is
set for each subject."
"In this paper we present an efficient computer aided mass classification
method in digitized mammograms using Artificial Neural Network (ANN), which
performs benign-malignant classification on region of interest (ROI) that
contains mass. One of the major mammographic characteristics for mass
classification is texture. ANN exploits this important factor to classify the
mass into benign or malignant. The statistical textural features used in
characterizing the masses are mean, standard deviation, entropy, skewness,
kurtosis and uniformity. The main aim of the method is to increase the
effectiveness and efficiency of the classification process in an objective
manner to reduce the numbers of false-positive of malignancies. Three layers
artificial neural network (ANN) with seven features was proposed for
classifying the marked regions into benign and malignant and 90.91% sensitivity
and 83.87% specificity is achieved that is very much promising compare to the
radiologist's sensitivity 75%."
"The physiological and behavioral trait is employed to develop biometric
authentication systems. The proposed work deals with the authentication of iris
and signature based on minimum variance criteria. The iris patterns are
preprocessed based on area of the connected components. The segmented image
used for authentication consists of the region with large variations in the
gray level values. The image region is split into quadtree components. The
components with minimum variance are determined from the training samples. Hu
moments are applied on the components. The summation of moment values
corresponding to minimum variance components are provided as input vector to
k-means and fuzzy k-means classifiers. The best performance was obtained for
MMU database consisting of 45 subjects. The number of subjects with zero False
Rejection Rate [FRR] was 44 and number of subjects with zero False Acceptance
Rate [FAR] was 45. This paper addresses the computational load reduction in
off-line signature verification based on minimal features using k-means, fuzzy
k-means, k-nn, fuzzy k-nn and novel average-max approaches. FRR of 8.13% and
FAR of 10% was achieved using k-nn classifier. The signature is a biometric,
where variations in a genuine case, is a natural expectation. In the genuine
signature, certain parts of signature vary from one instance to another. The
system aims to provide simple, fast and robust system using less number of
features when compared to state of art works."
"Due to the rapid development of World Wide Web (WWW) and imaging technology,
more and more images are available in the Internet and stored in databases.
Searching the related images by the querying image is becoming tedious and
difficult. Most of the images on the web are compressed by methods based on
discrete cosine transform (DCT) including Joint Photographic Experts
Group(JPEG) and H.261. This paper presents an efficient content-based image
indexing technique for searching similar images using discrete cosine transform
features. Experimental results demonstrate its superiority with the existing
techniques."
"Cascade classifiers are widely used in real-time object detection. Different
from conventional classifiers that are designed for a low overall
classification error rate, a classifier in each node of the cascade is required
to achieve an extremely high detection rate and moderate false positive rate.
Although there are a few reported methods addressing this requirement in the
context of object detection, there is no a principled feature selection method
that explicitly takes into account this asymmetric node learning objective. We
provide such an algorithm here. We show a special case of the biased minimax
probability machine has the same formulation as the linear asymmetric
classifier (LAC) of \cite{wu2005linear}. We then design a new boosting
algorithm that directly optimizes the cost function of LAC. The resulting
totally-corrective boosting algorithm is implemented by the column generation
technique in convex optimization. Experimental results on object detection
verify the effectiveness of the proposed boosting algorithm as a node
classifier in cascade object detection, and show performance better than that
of the current state-of-the-art."
"Human ovarian reserve is defined by the population of nongrowing follicles
(NGFs) in the ovary. Direct estimation of ovarian reserve involves the
identification of NGFs in prepared ovarian tissue. Previous studies involving
human tissue have used hematoxylin and eosin (HE) stain, with NGF populations
estimated by human examination either of tissue under a microscope, or of
images taken of this tissue. In this study we replaced HE with proliferating
cell nuclear antigen (PCNA), and automated the identification and enumeration
of NGFs that appear in the resulting microscopic images. We compared the
automated estimates to those obtained by human experts, with the ""gold
standard"" taken to be the average of the conservative and liberal estimates by
three human experts. The automated estimates were within 10% of the ""gold
standard"", for images at both 100x and 200x magnifications. Automated analysis
took longer than human analysis for several hundred images, not allowing for
breaks from analysis needed by humans. Our results both replicate and improve
on those of previous studies involving rodent ovaries, and demonstrate the
viability of large-scale studies of human ovarian reserve using a combination
of immunohistochemistry and computational image analysis techniques."
"Object detection has been a focus of research in human-computer interaction.
Skin area detection has been a key to different recognitions like face
recognition, human motion detection, pornographic and nude image prediction,
etc. Most of the research done in the fields of skin detection has been trained
and tested on human images of African, Mongolian and Anglo-Saxon ethnic
origins. Although there are several intensity invariant approaches to skin
detection, the skin color of Indian sub-continentals have not been focused
separately. The approach of this research is to make a comparative study
between three image segmentation approaches using Indian sub-continental human
images, to optimize the detection criteria, and to find some efficient
parameters to detect the skin area from these images. The experiments observed
that HSV color model based approach to Indian sub-continental skin detection is
more suitable with considerable success rate of 91.1% true positives and 88.1%
true negatives."
"Calibration in a multi camera network has widely been studied for over
several years starting from the earlier days of photogrammetry. Many authors
have presented several calibration algorithms with their relative advantages
and disadvantages. In a stereovision system, multiple view reconstruction is a
challenging task. However, the total computational procedure in detail has not
been presented before. Here in this work, we are dealing with the problem that,
when a world coordinate point is fixed in space, image coordinates of that 3D
point vary for different camera positions and orientations. In computer vision
aspect, this situation is undesirable. That is, the system has to be designed
in such a way that image coordinate of the world coordinate point will be fixed
irrespective of the position & orientation of the cameras. We have done it in
an elegant fashion. Firstly, camera parameters are calculated in its local
coordinate system. Then, we use global coordinate data to transfer all local
coordinate data of stereo cameras into same global coordinate system, so that
we can register everything into this global coordinate system. After all the
transformations, when the image coordinate of the world coordinate point is
calculated, it gives same coordinate value for all camera positions &
orientations. That is, the whole system is calibrated."
"Background: Dermoscopy is one of the major imaging modalities used in the
diagnosis of melanoma and other pigmented skin lesions. Due to the difficulty
and subjectivity of human interpretation, computerized analysis of dermoscopy
images has become an important research area. One of the most important steps
in dermoscopy image analysis is the automated detection of lesion borders.
Methods: In this article, we present a systematic overview of the recent border
detection methods in the literature paying particular attention to
computational issues and evaluation aspects. Conclusion: Common problems with
the existing approaches include the acquisition, size, and diagnostic
distribution of the test image set, the evaluation of the results, and the
inadequate description of the employed methods. Border determination by
dermatologists appears to depend upon higher-level knowledge, therefore it is
likely that the incorporation of domain knowledge in automated methods will
enable them to perform better, especially in sets of images with a variety of
diagnoses."
"The problem of identifying the 3D pose of a known object from a given 2D
image has important applications in Computer Vision ranging from robotic vision
to image analysis. Our proposed method of registering a 3D model of a known
object on a given 2D photo of the object has numerous advantages over existing
methods: It does neither require prior training nor learning, nor knowledge of
the camera parameters, nor explicit point correspondences or matching features
between image and model. Unlike techniques that estimate a partial 3D pose (as
in an overhead view of traffic or machine parts on a conveyor belt), our method
estimates the complete 3D pose of the object, and works on a single static
image from a given view, and under varying and unknown lighting conditions. For
this purpose we derive a novel illumination-invariant distance measure between
2D photo and projected 3D model, which is then minimised to find the best pose
parameters. Results for vehicle pose detection are presented."
"In this paper, a new directionally adaptive, learning based, single image
super resolution method using multiple direction wavelet transform, called
Directionlets is presented. This method uses directionlets to effectively
capture directional features and to extract edge information along different
directions of a set of available high resolution images .This information is
used as the training set for super resolving a low resolution input image and
the Directionlet coefficients at finer scales of its high-resolution image are
learned locally from this training set and the inverse Directionlet transform
recovers the super-resolved high resolution image. The simulation results
showed that the proposed approach outperforms standard interpolation techniques
like Cubic spline interpolation as well as standard Wavelet-based learning,
both visually and in terms of the mean squared error (mse) values. This method
gives good result with aliased images also."
"Combining the properties of monovariate internal functions as proposed in
Kolmogorov superimposition theorem, in tandem with the bounds wielded by the
multivariate formulation of Chebyshev inequality, a hybrid model is presented,
that decomposes images into homogeneous probabilistically bounded multivariate
surfaces. Given an image, the model shows a novel way of working on reduced
image representation while processing and capturing the interaction among the
multidimensional information that describes the content of the same. Further,
it tackles the practical issues of preventing leakage by bounding the growth of
surface and reducing the problem sample size. The model if used, also sheds
light on how the Chebyshev parameter relates to the number of pixels and the
dimensionality of the feature space that associates with a pixel. Initial
segmentation results on the Berkeley image segmentation benchmark indicate the
effectiveness of the proposed decomposition algorithm."
"A scattering vector is a local descriptor including multiscale and
multi-direction co-occurrence information. It is computed with a cascade of
wavelet decompositions and complex modulus. This scattering representation is
locally translation invariant and linearizes deformations. A supervised
classification algorithm is computed with a PCA model selection on scattering
vectors. State of the art results are obtained for handwritten digit
recognition and texture classification."
"Classification is one of the most important tasks of machine learning.
Although the most well studied model is the two-class problem, in many
scenarios there is the opportunity to label critical items for manual revision,
instead of trying to automatically classify every item. In this paper we adapt
a paradigm initially proposed for the classification of ordinal data to address
the classification problem with reject option. The technique reduces the
problem of classifying with reject option to the standard two-class problem.
The introduced method is then mapped into support vector machines and neural
networks. Finally, the framework is extended to multiclass ordinal data with
reject option. An experimental study with synthetic and real data sets,
verifies the usefulness of the proposed approach."
"In this paper a fuzzy clustering model for fuzzy data with outliers is
proposed. The model is based on Wasserstein distance between interval valued
data which is generalized to fuzzy data. In addition, Keller's approach is used
to identify outliers and reduce their influences. We have also defined a
transformation to change our distance to the Euclidean distance. With the help
of this approach, the problem of fuzzy clustering of fuzzy data is reduced to
fuzzy clustering of crisp data. In order to show the performance of the
proposed clustering algorithm, two simulation experiments are discussed."
"In this paper we propose a new wavelet transform applicable to functions
defined on graphs, high dimensional data and networks. The proposed method
generalizes the Haar-like transform proposed in [1], and it is defined via a
hierarchical tree, which is assumed to capture the geometry and structure of
the input data. It is applied to the data using a modified version of the
common one-dimensional (1D) wavelet filtering and decimation scheme, which can
employ different wavelet filters. In each level of this wavelet decomposition
scheme, a permutation derived from the tree is applied to the approximation
coefficients, before they are filtered. We propose a tree construction method
that results in an efficient representation of the input function in the
transform domain. We show that the proposed transform is more efficient than
both the 1D and two-dimensional (2D) separable wavelet transforms in
representing images. We also explore the application of the proposed transform
to image denoising, and show that combined with a subimage averaging scheme, it
achieves denoising results which are similar to those obtained with the K-SVD
algorithm."
"The goal of this paper is the development of a novel approach for the problem
of Noise Removal, based on the theory of Reproducing Kernels Hilbert Spaces
(RKHS). The problem is cast as an optimization task in a RKHS, by taking
advantage of the celebrated semiparametric Representer Theorem. Examples verify
that in the presence of gaussian noise the proposed method performs relatively
well compared to wavelet based technics and outperforms them significantly in
the presence of impulse or mixed noise.
  A more detailed version of this work has been published in the IEEE Trans.
Im. Proc. : P. Bouboulis, K. Slavakis and S. Theodoridis, Adaptive Kernel-based
Image Denoising employing Semi-Parametric Regularization, IEEE Transactions on
Image Processing, vol 19(6), 2010, 1465 - 1479."
"This paper introduces a new method for learning and inferring sparse
representations of depth (disparity) maps. The proposed algorithm relaxes the
usual assumption of the stationary noise model in sparse coding. This enables
learning from data corrupted with spatially varying noise or uncertainty,
typically obtained by laser range scanners or structured light depth cameras.
Sparse representations are learned from the Middlebury database disparity maps
and then exploited in a two-layer graphical model for inferring depth from
stereo, by including a sparsity prior on the learned features. Since they
capture higher-order dependencies in the depth structure, these priors can
complement smoothness priors commonly used in depth inference based on Markov
Random Field (MRF) models. Inference on the proposed graph is achieved using an
alternating iterative optimization technique, where the first layer is solved
using an existing MRF-based stereo matching algorithm, then held fixed as the
second layer is solved using the proposed non-stationary sparse coding
algorithm. This leads to a general method for improving solutions of state of
the art MRF-based depth estimation algorithms. Our experimental results first
show that depth inference using learned representations leads to state of the
art denoising of depth maps obtained from laser range scanners and a time of
flight camera. Furthermore, we show that adding sparse priors improves the
results of two depth estimation methods: the classical graph cut algorithm by
Boykov et al. and the more recent algorithm of Woodford et al."
"This paper proposes the problem of modeling video sequences of dynamic swarms
(DS). We define DS as a large layout of stochastically repetitive spatial
configurations of dynamic objects (swarm elements) whose motions exhibit local
spatiotemporal interdependency and stationarity, i.e., the motions are similar
in any small spatiotemporal neighborhood. Examples of DS abound in nature,
e.g., herds of animals and flocks of birds. To capture the local spatiotemporal
properties of the DS, we present a probabilistic model that learns both the
spatial layout of swarm elements and their joint dynamics that are modeled as
linear transformations. To this end, a spatiotemporal neighborhood is
associated with each swarm element, in which local stationarity is enforced
both spatially and temporally. We assume that the prior on the swarm dynamics
is distributed according to an MRF in both space and time. Embedding this model
in a MAP framework, we iterate between learning the spatial layout of the swarm
and its dynamics. We learn the swarm transformations using ICM, which iterates
between estimating these transformations and updating their distribution in the
spatiotemporal neighborhoods. We demonstrate the validity of our method by
conducting experiments on real video sequences. Real sequences of birds, geese,
robot swarms, and pedestrians evaluate the applicability of our model to real
world data."
"There is an increasing use of some imperceivable and redundant local features
for face recognition. While only a relatively small fraction of them is
relevant to the final recognition task, the feature selection is a crucial and
necessary step to select the most discriminant ones to obtain a compact face
representation. In this paper, we investigate the sparsity-enforced
regularization-based feature selection methods and propose a multi-task feature
selection method for building person specific models for face verification. We
assume that the person specific models share a common subset of features and
novelly reformulated the common subset selection problem as a simultaneous
sparse approximation problem. To the best of our knowledge, it is the first
time to apply the sparsity-enforced regularization methods for person specific
face verification. The effectiveness of the proposed methods is verified with
the challenging LFW face databases."
"Feature-based approaches have recently become very popular in computer vision
and image analysis applications, and are becoming a promising direction in
shape retrieval. SHREC'11 robust feature detection and description benchmark
simulates the feature detection and description stages of feature-based shape
retrieval algorithms. The benchmark tests the performance of shape feature
detectors and descriptors under a wide variety of transformations. The
benchmark allows evaluating how algorithms cope with certain classes of
transformations and strength of the transformations that can be dealt with. The
present paper is a report of the SHREC'11 robust feature detection and
description benchmark results."
"Due to the factors like processing power limitations and channel capabilities
images are often down sampled and transmitted at low bit rates resulting in a
low resolution compressed image. High resolution images can be reconstructed
from several blurred, noisy and down sampled low resolution images using a
computational process know as super resolution reconstruction. Super-resolution
is the process of combining multiple aliased low-quality images to produce a
high resolution, high-quality image. The problem of recovering a high
resolution image progressively from a sequence of low resolution compressed
images is considered. In this paper we propose a novel DCT based progressive
image display algorithm by stressing on the encoding and decoding process. At
the encoder we consider a set of low resolution images which are corrupted by
additive white Gaussian noise and motion blur. The low resolution images are
compressed using 8 by 8 blocks DCT and noise is filtered using our proposed
novel zonal filter. Multiframe fusion is performed in order to obtain a single
noise free image. At the decoder the image is reconstructed progressively by
transmitting the coarser image first followed by the detail image. And finally
a super resolution image is reconstructed by applying our proposed novel
adaptive interpolation technique. We have performed both objective and
subjective analysis of the reconstructed image, and the resultant image has
better super resolution factor, and a higher ISNR and PSNR. A comparative study
done with Iterative Back Projection (IBP) and Projection on to Convex Sets
(POCS),Papoulis Grechberg, FFT based Super resolution Reconstruction shows that
our method has out performed the previous contributions."
"The applications using face biometric has proved its reliability in last
decade. In this paper, we propose DBC based Face Recognition using DWT (DBC-
FR) model. The Poly-U Near Infra Red (NIR) database images are scanned and
cropped to get only the face part in pre-processing. The face part is resized
to 100*100 and DWT is applied to derive LL, LH, HL and HH subbands. The LL
subband of size 50*50 is converted into 100 cells with 5*5 dimention of each
cell. The Directional Binary Code (DBC) is applied on each 5*5 cell to derive
100 features. The Euclidian distance measure is used to compare the features of
test image and database images. The proposed algorithm render better percentage
recognition rate compared to the existing algorithm."
"Karyotyping is a process in which chromosomes in a dividing cell are properly
stained, identified and displayed in a standard format, which helps geneticist
to study and diagnose genetic factors behind various genetic diseases and for
studying cancer. M-FISH (Multiplex Fluorescent In-Situ Hybridization) provides
color karyotyping. In this paper, an automated method for M-FISH chromosome
segmentation based on watershed transform followed by naive Bayes
classification of each region using the features, mean and standard deviation,
is presented. Also, a post processing step is added to re-classify the small
chromosome segments to the neighboring larger segment for reducing the chances
of misclassification. The approach provided improved accuracy when compared to
the pixel-by-pixel approach. The approach was tested on 40 images from the
dataset and achieved an accuracy of 84.21 %."
"India is a multilingual multi-script country. In every state of India there
are two languages one is state local language and the other is English. For
example in Andhra Pradesh, a state in India, the document may contain text
words in English and Telugu script. For Optical Character Recognition (OCR) of
such a bilingual document, it is necessary to identify the script before
feeding the text words to the OCRs of individual scripts. In this paper, we are
introducing a simple and efficient technique of script identification for
Kannada, English and Hindi text words of a printed document. The proposed
approach is based on the horizontal and vertical projection profile for the
discrimination of the three scripts. The feature extraction is done based on
the horizontal projection profile of each text words. We analysed 700 different
words of Kannada, English and Hindi in order to extract the discrimination
features and for the development of knowledge base. We use the horizontal
projection profile of each text word and based on the horizontal projection
profile we extract the appropriate features. The proposed system is tested on
100 different document images containing more than 1000 text words of each
script and a classification rate of 98.25%, 99.25% and 98.87% is achieved for
Kannada, English and Hindi respectively."
"Mid-level features based on visual dictionaries are today a cornerstone of
systems for classification and retrieval of images. Those state-of-the-art
representations depend crucially on the choice of a codebook (visual
dictionary), which is usually derived from the dataset. In general-purpose,
dynamic image collections (e.g., the Web), one cannot have the entire
collection in order to extract a representative dictionary. However, based on
the hypothesis that the dictionary reflects only the diversity of low-level
appearances and does not capture semantics, we argue that a dictionary based on
a small subset of the data, or even on an entirely different dataset, is able
to produce a good representation, provided that the chosen images span a
diverse enough portion of the low-level feature space. Our experiments confirm
that hypothesis, opening the opportunity to greatly alleviate the burden in
generating the codebook, and confirming the feasibility of employing visual
dictionaries in large-scale dynamic environments."
"Sampling from distributions of implicitly defined shapes enables analysis of
various energy functionals used for image segmentation. Recent work describes a
computationally efficient Metropolis-Hastings method for accomplishing this
task. Here, we extend that framework so that samples are accepted at every
iteration of the sampler, achieving an order of magnitude speed up in
convergence. Additionally, we show how to incorporate topological constraints."
"According to the character of Gaussian, we modify the Rank-Ordered Absolute
Differences (ROAD) to Rank-Ordered Absolute Differences of mixture of Gaussian
and impulse noises (ROADG). It will be more effective to detect impulse noise
when the impulse is mixed with Gaussian noise. Combining rightly the ROADG with
Optimal Weights Filter (OWF), we obtain a new method to deal with the mixed
noise, called Optimal Weights Mixed Filter (OWMF). The simulation results show
that the method is effective to remove the mixed noise."
"Breast cancer is the second leading cause for death among women and it is
diagnosed with the help of mammograms. Oncologists are miserably failed in
identifying the micro calcification at the early stage with the help of the
mammogram visually. In order to improve the performance of the breast cancer
screening, most of the researchers have proposed Computer Aided Diagnosis using
image processing. In this study mammograms are preprocessed and features are
extracted, then the abnormality is identified through the classification. If
all the extracted features are used, most of the cases are misidentified. Hence
feature selection procedure is sought. In this paper, Fuzzy-Rough feature
selection with {\pi} membership function is proposed. The selected features are
used to classify the abnormalities with help of Ant-Miner and Weka tools. The
experimental analysis shows that the proposed method improves the mammograms
classification accuracy."
"Spectral graph theory is well known and widely used in computer vision. In
this paper, we analyze image segmentation algorithms that are based on spectral
graph theory, e.g., normalized cut, and show that there is a natural connection
between spectural graph theory based image segmentationand and edge preserving
filtering. Based on this connection we show that the normalized cut algorithm
is equivalent to repeated iterations of bilateral filtering. Then, using this
equivalence we present and implement a fast normalized cut algorithm for image
segmentation. Experiments show that our implementation can solve the original
optimization problem in the normalized cut algorithm 10 to 100 times faster.
Furthermore, we present a new algorithm called conditioned normalized cut for
image segmentation that can easily incorporate color image patches and
demonstrate how this segmentation problem can be solved with edge preserving
filtering."
"Gray Level Co-occurrence Matrices (GLCM) are one of the earliest techniques
used for image texture analysis. In this paper we defined a new feature called
trace extracted from the GLCM and its implications in texture analysis are
discussed in the context of Content Based Image Retrieval (CBIR). The
theoretical extension of GLCM to n-dimensional gray scale images are also
discussed. The results indicate that trace features outperform Haralick
features when applied to CBIR."
"Driving support systems, such as car navigation systems are becoming common
and they support driver in several aspects. Non-intrusive method of detecting
Fatigue and drowsiness based on eye-blink count and eye directed instruction
controlhelps the driver to prevent from collision caused by drowsy driving. Eye
detection and tracking under various conditions such as illumination,
background, face alignment and facial expression makes the problem
complex.Neural Network based algorithm is proposed in this paper to detect the
eyes efficiently. In the proposed algorithm, first the neural Network is
trained to reject the non-eye regionbased on images with features of eyes and
the images with features of non-eye using Gabor filter and Support Vector
Machines to reduce the dimension and classify efficiently. In the algorithm,
first the face is segmented using L*a*btransform color space, then eyes are
detected using HSV and Neural Network approach. The algorithm is tested on
nearly 100 images of different persons under different conditions and the
results are satisfactory with success rate of 98%.The Neural Network is trained
with 50 non-eye images and 50 eye images with different angles using Gabor
filter. This paper is a part of research work on ""Development of Non-Intrusive
system for real-time Monitoring and Prediction of Driver Fatigue and
drowsiness"" project sponsored by Department of Science & Technology, Govt. of
India, New Delhi at Vignan Institute of Technology and Sciences, Vignan Hills,
Hyderabad."
"Transform Invariant Low-rank Textures (TILT) is a novel and powerful tool
that can effectively rectify a rich class of low-rank textures in 3D scenes
from 2D images despite significant deformation and corruption. The existing
algorithm for solving TILT is based on the alternating direction method (ADM).
It suffers from high computational cost and is not theoretically guaranteed to
converge to a correct solution. In this paper, we propose a novel algorithm to
speed up solving TILT, with guaranteed convergence. Our method is based on the
recently proposed linearized alternating direction method with adaptive penalty
(LADMAP). To further reduce computation, warm starts are also introduced to
initialize the variables better and cut the cost on singular value
decomposition. Extensive experimental results on both synthetic and real data
demonstrate that this new algorithm works much more efficiently and robustly
than the existing algorithm. It could be at least five times faster than the
previous method."
"Image registration is an important tool for medical image analysis and is
used to bring images into the same reference frame by warping the coordinate
field of one image, such that some similarity measure is minimized. We study
similarity in image registration in the context of Locally Orderless Images
(LOI), which is the natural way to study density estimates and reveals the 3
fundamental scales: the measurement scale, the intensity scale, and the
integration scale.
  This paper has three main contributions: Firstly, we rephrase a large set of
popular similarity measures into a common framework, which we refer to as
Locally Orderless Registration, and which makes full use of the features of
local histograms. Secondly, we extend the theoretical understanding of the
local histograms. Thirdly, we use our framework to compare two state-of-the-art
intensity density estimators for image registration: The Parzen Window (PW) and
the Generalized Partial Volume (GPV), and we demonstrate their differences on a
popular similarity measure, Normalized Mutual Information (NMI).
  We conclude, that complicated similarity measures such as NMI may be
evaluated almost as fast as simple measures such as Sum of Squared Distances
(SSD) regardless of the choice of PW and GPV. Also, GPV is an asymmetric
measure, and PW is our preferred choice."
"This paper addresses the problem of approximate MAP-MRF inference in general
graphical models. Following [36], we consider a family of linear programming
relaxations of the problem where each relaxation is specified by a set of
nested pairs of factors for which the marginalization constraint needs to be
enforced. We develop a generalization of the TRW-S algorithm [9] for this
problem, where we use a decomposition into junction chains, monotonic w.r.t.
some ordering on the nodes. This generalizes the monotonic chains in [9] in a
natural way. We also show how to deal with nested factors in an efficient way.
Experiments show an improvement over min-sum diffusion, MPLP and subgradient
ascent algorithms on a number of computer vision and natural language
processing problems."
"This note presents some representative methods which are based on dictionary
learning (DL) for classification. We do not review the sophisticated methods or
frameworks that involve DL for classification, such as online DL and spatial
pyramid matching (SPM), but rather, we concentrate on the direct DL-based
classification methods. Here, the ""so-called direct DL-based method"" is the
approach directly deals with DL framework by adding some meaningful penalty
terms. By listing some representative methods, we can roughly divide them into
two categories, i.e. (1) directly making the dictionary discriminative and (2)
forcing the sparse coefficients discriminative to push the discrimination power
of the dictionary. From this taxonomy, we can expect some extensions of them as
future researches."
"This paper proposes a Genetic Algorithm based segmentation method that can
automatically segment gray-scale images. The proposed method mainly consists of
spatial unsupervised grayscale image segmentation that divides an image into
regions. The aim of this algorithm is to produce precise segmentation of images
using intensity information along with neighborhood relationships. In this
paper, Fuzzy Hopfield Neural Network (FHNN) clustering helps in generating the
population of Genetic algorithm which there by automatically segments the
image. This technique is a powerful method for image segmentation and works for
both single and multiple-feature data with spatial information. Validity index
has been utilized for introducing a robust technique for finding the optimum
number of components in an image. Experimental results shown that the algorithm
generates good quality segmented image."
"We present a scale-invariant, template-based segmentation paradigm that sets
up a graph and performs a graph cut to separate an object from the background.
Typically graph-based schemes distribute the nodes of the graph uniformly and
equidistantly on the image, and use a regularizer to bias the cut towards a
particular shape. The strategy of uniform and equidistant nodes does not allow
the cut to prefer more complex structures, especially when areas of the object
are indistinguishable from the background. We propose a solution by introducing
the concept of a ""template shape"" of the target object in which the nodes are
sampled non-uniformly and non-equidistantly on the image. We evaluate it on
2D-images where the object's textures and backgrounds are similar, and large
areas of the object have the same gray level appearance as the background. We
also evaluate it in 3D on 60 brain tumor datasets for neurosurgical planning
purposes."
"A novel method of gender Classification from fingerprint is proposed based on
discrete wavelet transform (DWT) and singular value decomposition (SVD). The
classification is achieved by extracting the energy computed from all the
sub-bands of DWT combined with the spatial features of non-zero singular values
obtained from the SVD of fingerprint images. K nearest neighbor (KNN) used as a
classifier. This method is experimented with the internal database of 3570
fingerprints finger prints in which 1980 were male fingerprints and 1590 were
female fingerprints. Finger-wise gender classification is achieved which is
94.32% for the left hand little fingers of female persons and 95.46% for the
left hand index finger of male persons. Gender classification for any finger of
male persons tested is attained as 91.67% and 84.69% for female persons
respectively. Overall classification rate is 88.28% has been achieved."
"This paper presents a novel approach towards identification of human beings
from the statistical analysis of their lip prints. Lip features are extracted
by studying the spatial orientations of the grooves present in lip prints of
individuals using standard edge detection techniques. Horizontal, vertical and
diagonal groove features are analysed using connected-component analysis to
generate the region-specific edge datasets. Comparison between test and
reference sample datasets against a threshold value to define a match yield
satisfactory results. FAR, FRR and ROC metrics have been used to gauge the
performance of the algorithm for real-world deployment in unimodal and
multimodal biometric verification systems."
"Subject of this paper is the theoretical analysis of structure-adaptive
median filter algorithms that approximate curvature-based PDEs for image
filtering and segmentation. These so-called morphological amoeba filters are
based on a concept introduced by Lerallut et al. They achieve similar results
as the well-known geodesic active contour and self-snakes PDEs. In the present
work, the PDE approximated by amoeba active contours is derived for a general
geometric situation and general amoeba metric. This PDE is structurally similar
but not identical to the geodesic active contour equation. It reproduces the
previous PDE approximation results for amoeba median filters as special cases.
Furthermore, modifications of the basic amoeba active contour algorithm are
analysed that are related to the morphological force terms frequently used with
geodesic active contours. Experiments demonstrate the basic behaviour of amoeba
active contours and its similarity to geodesic active contours."
"We propose a new framework for object detection based on a generalization of
the keypoint correspondence framework. This framework is based on replacing
keypoints by keygraphs, i.e. isomorph directed graphs whose vertices are
keypoints, in order to explore relative and structural information. Unlike
similar works in the literature, we deal directly with graphs in the entire
pipeline: we search for graph correspondences instead of searching for
individual point correspondences and then building graph correspondences from
them afterwards. We also estimate the pose from graph correspondences instead
of falling back to point correspondences through a voting table. The
contributions of this paper are the proposed framework and an implementation
that properly handles its inherent issues of loss of locality and combinatorial
explosion, showing its viability for real-time applications. In particular, we
introduce the novel concept of keytuples to solve a running time issue. The
accuracy of the implementation is shown by results of over 800 experiments with
a well-known database of images. The speed is illustrated by real-time tracking
with two different cameras in ordinary hardware."
"Surface registration is a technique that is used in various areas such as
object recognition and 3D model reconstruction. Problem of surface registration
can be analyzed as an optimization problem of seeking a rigid motion between
two different views. Genetic algorithms can be used for solving this
optimization problem, both for obtaining the robust parameter estimation and
for its fine-tuning. The main drawback of genetic algorithms is that they are
time consuming which makes them unsuitable for online applications. Modern
acquisition systems enable the implementation of the solutions that would
immediately give the information on the rotational angles between the different
views, thus reducing the dimension of the optimization problem. The paper gives
an analysis of the genetic algorithm implemented in the conditions when the
rotation matrix is known and a comparison of these results with results when
this information is not available."
"Breast tissue segmentation into dense and fat tissue is important for
determining the breast density in mammograms. Knowing the breast density is
important both in diagnostic and computer-aided detection applications. There
are many different ways to express the density of a breast and good quality
segmentation should provide the possibility to perform accurate classification
no matter which classification rule is being used. Knowing the right breast
density and having the knowledge of changes in the breast density could give a
hint of a process which started to happen within a patient. Mammograms
generally suffer from a problem of different tissue overlapping which results
in the possibility of inaccurate detection of tissue types. Fibroglandular
tissue presents rather high attenuation of X-rays and is visible as brighter in
the resulting image but overlapping fibrous tissue and blood vessels could
easily be replaced with fibroglandular tissue in automatic segmentation
algorithms. Small blood vessels and microcalcifications are also shown as
bright objects with similar intensities as dense tissue but do have some
properties which makes possible to suppress them from the final results. In
this paper we try to divide dense and fat tissue by suppressing the scattered
structures which do not represent glandular or dense tissue in order to divide
mammograms more accurately in the two major tissue types. For suppressing blood
vessels and microcalcifications we have used Gabor filters of different size
and orientation and a combination of morphological operations on filtered image
with enhanced contrast."
"Most visual quality inspections in discrete manufacturing are composed of
length, surface, angle or intensity measurements. Those are implemented as
end-user configurable inspection tools that should not require an image
processing expert to set up. Currently available software solutions providing
such capability use a flowchart based programming environment, but do not fully
address an inspection flowchart robustness and can require a redefinition of
the flowchart if a small variation is introduced. In this paper we propose an
acquire-register-analyze image processing pattern designed for discrete
manufacturing that aims to increase the robustness of the inspection flowchart
by consistently addressing variations in product position, orientation and
size. A proposed pattern is transparent to the end-user and simplifies the
flowchart. We describe a developed software solution that is a practical
implementation of the proposed pattern. We give an example of its real-life use
in industrial production of electric components."
"In this paper the use of Random Sprays Retinex (RSR) algorithm for global
illumination estimation is proposed and its feasibility tested. Like other
algorithms based on the Retinex model, RSR also provides local illumination
estimation and brightness adjustment for each pixel and it is faster than other
path-wise Retinex algorithms. As the assumption of the uniform illumination
holds in many cases, it should be possible to use the mean of local
illumination estimations of RSR as a global illumination estimation for images
with (assumed) uniform illumination allowing also the accuracy to be easily
measured. Therefore we propose a method for estimating global illumination
estimation based on local RSR results. To our best knowledge this is the first
time that RSR algorithm is used to obtain global illumination estimation. For
our tests we use a publicly available color constancy image database for
testing. The results are presented and discussed and it turns out that the
proposed method outperforms many existing unsupervised color constancy
algorithms. The source code is available at
http://www.fer.unizg.hr/ipg/resources/color_constancy/."
"This paper proposes combining spatio-temporal appearance (STA) descriptors
with optical flow for human action recognition. The STA descriptors are local
histogram-based descriptors of space-time, suitable for building a partial
representation of arbitrary spatio-temporal phenomena. Because of the
possibility of iterative refinement, they are interesting in the context of
online human action recognition. We investigate the use of dense optical flow
as the image function of the STA descriptor for human action recognition, using
two different algorithms for computing the flow: the Farneb\""ack algorithm and
the TVL1 algorithm. We provide a detailed analysis of the influencing optical
flow algorithm parameters on the produced optical flow fields. An extensive
experimental validation of optical flow-based STA descriptors in human action
recognition is performed on the KTH human action dataset. The encouraging
experimental results suggest the potential of our approach in online human
action recognition."
"In this work, we present a novel dataset for assessing the accuracy of stereo
visual odometry. The dataset has been acquired by a small-baseline stereo rig
mounted on the top of a moving car. The groundtruth is supplied by a consumer
grade GPS device without IMU. Synchronization and alignment between GPS
readings and stereo frames are recovered after the acquisition. We show that
the attained groundtruth accuracy allows to draw useful conclusions in
practice. The presented experiments address influence of camera calibration,
baseline distance and zero-disparity features to the achieved reconstruction
performance."
"We consider the problem of multiclass road sign detection using a
classification function with multiplicative kernel comprised from two kernels.
We show that problems of detection and within-foreground classification can be
jointly solved by using one kernel to measure object-background differences and
another one to account for within-class variations. The main idea behind this
approach is that road signs from different foreground variations can share
features that discriminate them from backgrounds. The classification function
training is accomplished using SVM, thus feature sharing is obtained through
support vector sharing. Training yields a family of linear detectors, where
each detector corresponds to a specific foreground training sample. The
redundancy among detectors is alleviated using k-medoids clustering. Finally,
we report detection and classification results on a set of road sign images
obtained from a camera on a moving vehicle."
"Global localization of a mobile robot using planar surface segments extracted
from depth images is considered. The robot's environment is represented by a
topological map consisting of local models, each representing a particular
location modeled by a set of planar surface segments. The discussed
localization approach segments a depth image acquired by a 3D camera into
planar surface segments which are then matched to model surface segments. The
robot pose is estimated by the Extended Kalman Filter using surface segment
pairs as measurements. The reliability and accuracy of the considered approach
are experimentally evaluated using a mobile robot equipped by a Microsoft
Kinect sensor."
"The number of road vehicles significantly increased in recent decades. This
trend accompanied a build-up of road infrastructure and development of various
control systems to increase road traffic safety, road capacity and travel
comfort. In traffic safety significant development has been made and today's
systems more and more include cameras and computer vision methods. Cameras are
used as part of the road infrastructure or in vehicles. In this paper a review
on computer vision systems in vehicles from the stand point of traffic
engineering is given. Safety problems of road vehicles are presented, current
state of the art in-vehicle vision systems is described and open problems with
future research directions are discussed."
"This paper investigates classification of traffic scenes in a very low
bandwidth scenario, where an image should be coded by a small number of
features. We introduce a novel dataset, called the FM1 dataset, consisting of
5615 images of eight different traffic scenes: open highway, open road,
settlement, tunnel, tunnel exit, toll booth, heavy traffic and the overpass. We
evaluate the suitability of the GIST descriptor as a representation of these
images, first by exploring the descriptor space using PCA and k-means
clustering, and then by using an SVM classifier and recording its 10-fold
cross-validation performance on the introduced FM1 dataset. The obtained
recognition rates are very encouraging, indicating that the use of the GIST
descriptor alone could be sufficiently descriptive even when very high
performance is required."
"In this work various methods and algorithms for face and eyes detection are
examined in order to decide which of them are applicable for use in a driver
fatigue monitoring system. In the case of face detection the standard
Viola-Jones face detector has shown best results, while the method of finding
the eye centers by means of gradients has proven to be most appropriate in the
case of eyes detection. The later method has also a potential for retrieving
behavioral parameters needed for estimation of the level of driver fatigue.
This possibility will be examined in future work."
"Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013,
http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb,
Croatia. Workshop was organized by the Center of Excellence for Computer Vision
of the University of Zagreb."
"It is proposed a complex valued channel encoding for multidimensional data.
The basic approach contains overlapping of complex nonlinear mappings. Its
development leads to sparse representation of multi-channel data, increasing
their dimensions and the distance between the images."
"Second layer scattering descriptors are known to provide good classification
performance on natural quasi-stationary processes such as visual textures due
to their sensitivity to higher order moments and continuity with respect to
small deformations. In a functional Magnetic Resonance Imaging (fMRI)
experiment we present visual textures to subjects and evaluate the predictive
power of these descriptors with respect to the predictive power of simple
contour energy - the first scattering layer. We are able to conclude not only
that invariant second layer scattering coefficients better encode voxel
activity, but also that well predicted voxels need not necessarily lie in known
retinotopic regions."
"We evaluate whether features extracted from the activation of a deep
convolutional network trained in a fully supervised fashion on a large, fixed
set of object recognition tasks can be re-purposed to novel generic tasks. Our
generic tasks may differ significantly from the originally trained tasks and
there may be insufficient labeled or unlabeled data to conventionally train or
adapt a deep architecture to the new tasks. We investigate and visualize the
semantic clustering of deep convolutional features with respect to a variety of
such tasks, including scene recognition, domain adaptation, and fine-grained
recognition challenges. We compare the efficacy of relying on various network
levels to define a fixed feature, and report novel results that significantly
outperform the state-of-the-art on several important vision challenges. We are
releasing DeCAF, an open-source implementation of these deep convolutional
activation features, along with all associated network parameters to enable
vision researchers to be able to conduct experimentation with deep
representations across a range of visual concept learning paradigms."
"Feature encoding with respect to an over-complete dictionary learned by
unsupervised methods, followed by spatial pyramid pooling, and linear
classification, has exhibited powerful strength in various vision applications.
Here we propose to use the feature learning pipeline for visual tracking.
Tracking is implemented using tracking-by-detection and the resulted framework
is very simple yet effective. First, online dictionary learning is used to
build a dictionary, which captures the appearance changes of the tracking
target as well as the background changes. Given a test image window, we extract
local image patches from it and each local patch is encoded with respect to the
dictionary. The encoded features are then pooled over a spatial pyramid to form
an aggregated feature vector. Finally, a simple linear classifier is trained on
these features.
  Our experiments show that the proposed powerful---albeit simple---tracker,
outperforms all the state-of-the-art tracking methods that we have tested.
Moreover, we evaluate the performance of different dictionary learning and
feature encoding methods in the proposed tracking framework, and analyse the
impact of each component in the tracking scenario. We also demonstrate the
flexibility of feature learning by plugging it into Hare et al.'s tracking
method. The outcome is, to our knowledge, the best tracker ever reported, which
facilitates the advantages of both feature learning and structured output
prediction."
"The problem of minimizing the Potts energy function frequently occurs in
computer vision applications. One way to tackle this NP-hard problem was
proposed by Kovtun [19,20]. It identifies a part of an optimal solution by
running $k$ maxflow computations, where $k$ is the number of labels. The number
of ""labeled"" pixels can be significant in some applications, e.g. 50-93% in our
tests for stereo. We show how to reduce the runtime to $O(\log k)$ maxflow
computations (or one {\em parametric maxflow} computation). Furthermore, the
output of our algorithm allows to speed-up the subsequent alpha expansion for
the unlabeled part, or can be used as it is for time-critical applications.
  To derive our technique, we generalize the algorithm of Felzenszwalb et al.
[7] for {\em Tree Metrics}. We also show a connection to {\em $k$-submodular
functions} from combinatorial optimization, and discuss {\em $k$-submodular
relaxations} for general energy functions."
"The problem of detecting and recognizing text in natural scenes has proved to
be more challenging than its counterpart in documents, with most of the
previous work focusing on a single part of the problem. In this work, we
propose new solutions to the character and word recognition problems and then
show how to combine these solutions in an end-to-end text-recognition system.
We do so by leveraging the recently introduced Maxout networks along with
hybrid HMM models that have proven useful for voice recognition. Using these
elements, we build a tunable and highly accurate recognition system that beats
state-of-the-art results on all the sub-problems for both the ICDAR 2003 and
SVT benchmark datasets."
"That the Microsoft Kinect, an RGB-D sensor, transformed the gaming and end
consumer sector has been anticipated by the developers. That it also impacted
in rigorous computer vision research has probably been a surprise to the whole
community. Shortly before the commercial deployment of its successor, Kinect
One, the research literature fills with resumees and state-of-the art papers to
summarize the development over the past 3 years. This particular report
describes significant research projects which have built on sensoring setups
that include two or more RGB-D sensors in one scene."
"The advent of the Microsoft Kinect three years ago stimulated not only the
computer vision community for new algorithms and setups to tackle well-known
problems in the community but also sparked the launch of several new benchmark
datasets to which future algorithms can be compared 019 to. This review of the
literature and industry developments concludes that the current RGB-D benchmark
datasets can be useful to determine the accuracy of a variety of applications
of a single or multiple RGB-D sensors."
"In this paper, an iterative method for robust deconvolution with positivity
constraints is discussed. It is based on the known variational interpretation
of the Richardson-Lucy iterative deconvolution as fixed-point iteration for the
minimisation of an information divergence functional under a multiplicative
perturbation model. The asymmetric penaliser function involved in this
functional is then modified into a robust penaliser, and complemented with a
regulariser. The resulting functional gives rise to a fixed point iteration
that we call robust and regularised Richardson-Lucy deconvolution. It achieves
an image restoration quality comparable to state-of-the-art robust variational
deconvolution with a computational efficiency similar to that of the original
Richardson-Lucy method. Experiments on synthetic and real-world image data
demonstrate the performance of the proposed method."
"The skeleton is an essential shape characteristic providing a compact
representation of the studied shape. Its computation on the image grid raises
many issues. Due to the effects of discretization, the required properties of
the skeleton - thinness, homotopy to the shape, reversibility, connectivity -
may become incompatible. However, as regards practical use, the choice of a
specific skeletonization algorithm depends on the application. This allows to
classify the desired properties by order of importance, and tend towards the
most critical ones. Our goal is to make a skeleton dedicated to shape matching
for recognition. So, the discrete skeleton has to be thin - so that it can be
represented by a graph -, robust to noise, reversible - so that the initial
shape can be fully reconstructed - and homotopic to the shape. We propose a
linear-time skeletonization algorithm based on the squared Euclidean distance
map from which we extract the maximal balls and ridges. After a thinning and
pruning process, we obtain the skeleton. The proposed method is finally
compared to fairly recent methods."
"We develop a framework for extracting a concise representation of the shape
information available from diffuse shading in a small image patch. This
produces a mid-level scene descriptor, comprised of local shape distributions
that are inferred separately at every image patch across multiple scales. The
framework is based on a quadratic representation of local shape that, in the
absence of noise, has guarantees on recovering accurate local shape and
lighting. And when noise is present, the inferred local shape distributions
provide useful shape information without over-committing to any particular
image explanation. These local shape distributions naturally encode the fact
that some smooth diffuse regions are more informative than others, and they
enable efficient and robust reconstruction of object-scale shape. Experimental
results show that this approach to surface reconstruction compares well against
the state-of-art on both synthetic images and captured photographs."
"We present a Bayesian probabilistic model to estimate the brain white matter
atlas from high angular resolution diffusion imaging (HARDI) data. This model
incorporates a shape prior of the white matter anatomy and the likelihood of
individual observed HARDI datasets. We first assume that the atlas is generated
from a known hyperatlas through a flow of diffeomorphisms and its shape prior
can be constructed based on the framework of large deformation diffeomorphic
metric mapping (LDDMM). LDDMM characterizes a nonlinear diffeomorphic shape
space in a linear space of initial momentum uniquely determining diffeomorphic
geodesic flows from the hyperatlas. Therefore, the shape prior of the HARDI
atlas can be modeled using a centered Gaussian random field (GRF) model of the
initial momentum. In order to construct the likelihood of observed HARDI
datasets, it is necessary to study the diffeomorphic transformation of
individual observations relative to the atlas and the probabilistic
distribution of orientation distribution functions (ODFs). To this end, we
construct the likelihood related to the transformation using the same
construction as discussed for the shape prior of the atlas. The probabilistic
distribution of ODFs is then constructed based on the ODF Riemannian manifold.
We assume that the observed ODFs are generated by an exponential map of random
tangent vectors at the deformed atlas ODF. Hence, the likelihood of the ODFs
can be modeled using a GRF of their tangent vectors in the ODF Riemannian
manifold. We solve for the maximum a posteriori using the
Expectation-Maximization algorithm and derive the corresponding update
equations. Finally, we illustrate the HARDI atlas constructed based on a
Chinese aging cohort of 94 adults and compare it with that generated by
averaging the coefficients of spherical harmonics of the ODF across subjects."
"Prostate cancer is the most abundant cancer in men, with over 200,000
expected new cases and around 28,000 deaths in 2012 in the US alone. In this
study, the segmentation results for the prostate central gland (PCG) in MR
scans are presented. The aim of this research study is to apply a graph-based
algorithm to automated segmentation (i.e. delineation) of organ limits for the
prostate central gland. The ultimate goal is to apply automated segmentation
approach to facilitate efficient MR-guided biopsy and radiation treatment
planning. The automated segmentation algorithm used is graph-driven based on a
spherical template. Therefore, rays are sent through the surface points of a
polyhedron to sample the graph's nodes. After graph construction - which only
requires the center of the polyhedron defined by the user and located inside
the prostate center gland - the minimal cost closed set on the graph is
computed via a polynomial time s-t-cut, which results in the segmentation of
the prostate center gland's boundaries and volume. The algorithm has been
realized as a C++ modul within the medical research platform MeVisLab and the
ground truth of the central gland boundaries were manually extracted by
clinical experts (interventional radiologists) with several years of experience
in prostate treatment. For evaluation the automated segmentations of the
proposed scheme have been compared with the manual segmentations, yielding an
average Dice Similarity Coefficient (DSC) of 78.94 +/- 10.85%."
"In this manuscript we present the technique to detect and analyze the DNA
rich structure in Haemotoxylin & Eosin (H&E) image of a tissue treated with
anti CD4 green antigen. The detection of DNA rich structure can be considered
as a detection of blue nuclei present through the biomedical signal/image
processing technique performed on the image of the tissue obtained by the
Scanning Electron Microscope(SEM). Earlier the tissue treated with the anti CD4
green antigen, is stained with the H&E staining solution."
"We propose a new model, together with advanced optimization, to separate a
thick scattering media layer from a single natural image. It is able to handle
challenging underwater scenes and images taken in fog and sandstorm, both of
which are with significantly reduced visibility. Our method addresses the
critical issue -- this is, originally unnoticeable impurities will be greatly
magnified after removing the scattering media layer -- with transmission-aware
optimization. We introduce non-local structure-aware regularization to properly
constrain transmission estimation without introducing the halo artifacts. A
selective-neighbor criterion is presented to convert the unconventional
constrained optimization problem to an unconstrained one where the latter can
be efficiently solved."
"Misfire in an IC Engine continues to be a problem leading to reduced fuel
efficiency, increased power loss and emissions containing heavy concentration
of hydrocarbons. Misfiring creates a unique vibration pattern attributed to a
particular cylinder. Useful features can be extracted from these patterns and
can be analyzed to detect misfire. Statistical features from these vibration
signals were extracted. Out of these, useful features were identified using the
J48 decision tree algorithm and selected features were used for classification
using the Kstar algorithm. In this paper performance analysis of Kstar
algorithm is presented."
"The goal of compressive sensing is efficient reconstruction of data from few
measurements, sometimes leading to a categorical decision. If only
classification is required, reconstruction can be circumvented and the
measurements needed are orders-of-magnitude sparser still. We define enhanced
sparsity as the reduction in number of measurements required for classification
over reconstruction. In this work, we exploit enhanced sparsity and learn
spatial sensor locations that optimally inform a categorical decision. The
algorithm solves an l1-minimization to find the fewest entries of the full
measurement vector that exactly reconstruct the discriminant vector in feature
space. Once the sensor locations have been identified from the training data,
subsequent test samples are classified with remarkable efficiency, achieving
performance comparable to that obtained by discrimination using the full image.
Sensor locations may be learned from full images, or from a random subsample of
pixels. For classification between more than two categories, we introduce a
coupling parameter whose value tunes the number of sensors selected, trading
accuracy for economy. We demonstrate the algorithm on example datasets from
image recognition using PCA for feature extraction and LDA for discrimination;
however, the method can be broadly applied to non-image data and adapted to
work with other methods for feature extraction and discrimination."
"In this paper, we tackle the problem of visual categorization of dog breeds,
which is a surprisingly challenging task due to simultaneously present low
interclass distances and high intra-class variances. Our approach combines
several techniques well known in our community but often not utilized for
fine-grained recognition:
  (1) automatic segmentation, (2) efficient part detection, and (3) combination
of multiple features. In particular, we demonstrate that a simple head detector
embedded in an off-the-shelf recognition pipeline can improve recognition
accuracy quite significantly, highlighting the importance of part features for
fine-grained recognition tasks. Using our approach, we achieved a 24.59% mean
average precision performance on the Stanford dog dataset."
"This paper introduces principal motion components (PMC), a new method for
one-shot gesture recognition. In the considered scenario a single
training-video is available for each gesture to be recognized, which limits the
application of traditional techniques (e.g., HMMs). In PMC, a 2D map of motion
energy is obtained per each pair of consecutive frames in a video. Motion maps
associated to a video are processed to obtain a PCA model, which is used for
recognition under a reconstruction-error approach. The main benefits of the
proposed approach are its simplicity, easiness of implementation, competitive
performance and efficiency. We report experimental results in one-shot gesture
recognition using the ChaLearn Gesture Dataset; a benchmark comprising more
than 50,000 gestures, recorded as both RGB and depth video with a Kinect
camera. Results obtained with PMC are competitive with alternative methods
proposed for the same data set."
"Recent advances in computer vision and machine learning suggest that a wide
range of problems can be addressed more appropriately by considering
non-Euclidean geometry. In this paper we explore sparse dictionary learning
over the space of linear subspaces, which form Riemannian structures known as
Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into
the space of symmetric matrices by an isometric mapping, which enables us to
devise a closed-form solution for updating a Grassmann dictionary, atom by
atom. Furthermore, to handle non-linearity in data, we propose a kernelised
version of the dictionary learning algorithm. Experiments on several
classification tasks (face recognition, action recognition, dynamic texture
classification) show that the proposed approach achieves considerable
improvements in discrimination accuracy, in comparison to state-of-the-art
methods such as kernelised Affine Hull Method and graph-embedding Grassmann
discriminant analysis."
"Hyperspectral images show similar statistical properties to natural grayscale
or color photographic images. However, the classification of hyperspectral
images is more challenging because of the very high dimensionality of the
pixels and the small number of labeled examples typically available for
learning. These peculiarities lead to particular signal processing problems,
mainly characterized by indetermination and complex manifolds. The framework of
statistical learning has gained popularity in the last decade. New methods have
been presented to account for the spatial homogeneity of images, to include
user's interaction via active learning, to take advantage of the manifold
structure with semisupervised learning, to extract and encode invariances, or
to adapt classifiers and image representations to unseen yet similar scenes.
This tutuorial reviews the main advances for hyperspectral remote sensing image
classification through illustrative examples."
"There have been intensive research interests in ship detection and
segmentation due to high demands on a wide range of civil applications in the
last two decades. However, existing approaches, which are mainly based on
statistical properties of images, fail to detect smaller ships and boats.
Specifically, known techniques are not robust enough in view of inevitable
small geometric and photometric changes in images consisting of ships. In this
paper a novel approach for ship detection is proposed based on correlation of
maritime images. The idea comes from the observation that a fine pattern of the
sea surface changes considerably from time to time whereas the ship appearance
basically keeps unchanged. We want to examine whether the images have a common
unaltered part, a ship in this case. To this end, we developed a method -
Focused Correlation (FC) to achieve robustness to geometric distortions of the
image content. Various experiments have been conducted to evaluate the
effectiveness of the proposed approach."
"This paper presents a Devnagari Numerical recognition method based on
statistical discriminant functions. 17 geometric features based on pixel
connectivity, lines, line directions, holes, image area, perimeter,
eccentricity, solidity, orientation etc. are used for representing the
numerals. Five discriminant functions viz. Linear, Quadratic, Diaglinear,
Diagquadratic and Mahalanobis distance are used for classification. 1500
handwritten numerals are used for training. Another 1500 handwritten numerals
are used for testing. Experimental results show that Linear, Quadratic and
Mahalanobis discriminant functions provide better results. Results of these
three Discriminants are fed to a majority voting type Combination classifier.
It is found that Combination classifier offers better results over individual
classifiers."
"Salient object detection aims to locate objects that capture human attention
within images. Previous approaches often pose this as a problem of image
contrast analysis. In this work, we model an image as a hypergraph that
utilizes a set of hyperedges to capture the contextual properties of image
pixels or regions. As a result, the problem of salient object detection becomes
one of finding salient vertices and hyperedges in the hypergraph. The main
advantage of hypergraph modeling is that it takes into account each pixel's (or
region's) affinity with its neighborhood as well as its separation from image
background. Furthermore, we propose an alternative approach based on
center-versus-surround contextual contrast analysis, which performs salient
object detection by optimizing a cost-sensitive support vector machine (SVM)
objective function. Experimental results on four challenging datasets
demonstrate the effectiveness of the proposed approaches against the
state-of-the-art approaches to salient object detection."
"Hyperspectral imaging, due to providing high spectral resolution images, is
one of the most important tools in the remote sensing field. Because of
technological restrictions hyperspectral sensors has a limited spatial
resolution. On the other hand panchromatic image has a better spatial
resolution. Combining this information together can provide a better
understanding of the target scene. Spectral unmixing of mixed pixels in
hyperspectral images results in spectral signature and abundance fractions of
endmembers but gives no information about their location in a mixed pixel. In
this paper we have used spectral unmixing results of hyperspectral images and
segmentation results of panchromatic image for data fusion. The proposed method
has been applied on simulated data using AVRIS Indian Pines datasets. Results
show that this method can effectively combine information in hyperspectral and
panchromatic images."
"Diabetic Retinopathy is a medical condition where the retina is damaged
because fluid leaks from blood vessels into the retina. The presence of
hemorrhages in the retina is the earliest symptom of diabetic retinopathy. The
number and shape of hemorrhages is used to indicate the severity of the
disease. Early automated hemorrhage detection can help reduce the incidence of
blindness. This paper introduced new method depending on the hemorrhage shape
to detect the dot hemorrhage (DH), its number, and size at early stage, this
can be achieved by reducing the retinal image details. Detection and recognize
the DH by following three sequential steps, removing the fovea, removing the
vasculature and recognize DH by determining the circularity for all the objects
in the image, finally determine the shape factor which is related to DH
recognition, this stage strengthens the recognition process. The proposed
method recognizes and separates all the DH."
"There is a large collection of Handwritten English paper documents of
Historical and Scientific importance. But paper documents are not recognized
directly by computer. Hence the closest way of indexing these documents is by
storing their document digital image. Hence a large database of document images
can replace the paper documents. But the document and data corresponding to
each image cannot be directly recognized by the computer.
  This paper applies the technique of word spotting using Modified Character
Shape Code to Handwritten English document images for quick and efficient query
search of words on a database of document images. It is different from other
Word Spotting techniques as it implements two level of selection for word
segments to match search query. First based on word size and then based on
character shape code of query. It makes the process faster and more efficient
and reduces the need of multiple pre-processing."
"This paper is aimed at developing and combining different algorithms for face
detection and face recognition to generate an efficient mechanism that can
detect and recognize the facial regions of input image. For the detection of
face from complex region, skin segmentation isolates the face-like regions in a
complex image and following operations of morphology and template matching
rejects false matches to extract facial region. For the recognition of the
face, the image database is now converted into a database of facial segments.
Hence, implementing the technique of Elastic Bunch Graph matching (EBGM) after
skin segmentation generates Face Bunch Graphs that acutely represents the
features of an individual face enhances the quality of the training set. This
increases the matching probability significantly."
"Diffusion Tensor Imaging (DTI) is a non-invasive imaging technique that
allows estimation of the location of white matter tracts in-vivo, based on the
measurement of water diffusion properties. For each voxel, a second-order
tensor can be calculated by using diffusion-weighted sequences (DWI) that are
sensitive to the random motion of water molecules. Given at least 6
diffusion-weighted images with different gradients and one unweighted image,
the coefficients of the symmetric diffusion tensor matrix can be calculated.
Deriving the eigensystem of the tensor, the eigenvectors and eigenvalues can be
calculated to describe the three main directions of diffusion and its
magnitude. Using DTI data, fiber bundles can be determined, to gain information
about eloquent brain structures. Especially in neurosurgery, information about
location and dimension of eloquent structures like the corticospinal tract or
the visual pathways is of major interest. Therefore, the fiber bundle boundary
has to be determined. In this paper, a novel ray-based approach for boundary
estimation of tubular structures is presented."
"In Biometrics, facial uniqueness is commonly inferred from impostor
similarity scores. In this paper, we show that such uniqueness measures are
highly unstable in the presence of image quality variations like pose, noise
and blur. We also experimentally demonstrate the instability of a recently
introduced impostor-based uniqueness measure of [Klare and Jain 2013] when
subject to poor quality facial images."
"In recent years, Printed Circuit Boards (PCB) have become the backbone of a
large number of consumer electronic devices leading to a surge in their
production. This has made it imperative to employ automatic inspection systems
to identify manufacturing defects in PCB before they are installed in the
respective systems. An important task in this regard is the classification of
defects as either true or pseudo defects, which decides if the PCB is to be
re-manufactured or not. This work proposes a novel approach to detect most
common defects in the PCBs. The problem has been approached by employing highly
discriminative features based on multi-scale wavelet transform, which are
further boosted by using a kernalized version of the support vector machines
(SVM). A real world printed circuit board dataset has been used for
quantitative analysis. Experimental results demonstrated the efficacy of the
proposed method."
"Automated detection of visually salient regions is an active area of research
in computer vision. Salient regions can serve as inputs for object detectors as
well as inputs for region based registration algorithms. In this paper we
consider the problem of speeding up computationally intensive bottom-up salient
region detection in 3D medical volumes.The method uses the Kadir Brady
formulation of saliency. We show that in the vicinity of a salient region,
entropy is a monotonically increasing function of the degree of overlap of a
candidate window with the salient region. This allows us to initialize a sparse
seed-point grid as the set of tentative salient region centers and iteratively
converge to the local entropy maxima, thereby reducing the computation
complexity compared to the Kadir Brady approach of performing this computation
at every point in the image. We propose two different approaches for achieving
this. The first approach involves evaluating entropy in the four quadrants
around the seed point and iteratively moving in the direction that increases
entropy. The second approach we propose makes use of mean shift tracking
framework to affect entropy maximizing moves. Specifically, we propose the use
of uniform pmf as the target distribution to seek high entropy regions. We
demonstrate the use of our algorithm on medical volumes for left ventricle
detection in PET images and tumor localization in brain MR sequences."
"A novel methodology for gender classification is presented in this paper. It
extracts feature from local region of a face using gray color intensity
difference. The facial area is divided into sub-regions and GDP histogram
extracted from those regions are concatenated into a single vector to represent
the face. The classification accuracy obtained by using support vector machine
has outperformed all traditional feature descriptors for gender classification.
It is evaluated on the images collected from FERET database and obtained very
high accuracy."
"We consider the problem of clustering data that reside on discrete, low
dimensional lattices. Canonical examples for this setting are found in image
segmentation and key point extraction. Our solution is based on a recent
approach to information theoretic clustering where clusters result from an
iterative procedure that minimizes a divergence measure. We replace costly
processing steps in the original algorithm by means of convolutions. These
allow for highly efficient implementations and thus significantly reduce
runtime. This paper therefore bridges a gap between machine learning and signal
processing."
"The first contribution of this paper is architecture of a multipurpose
system, which delegates a range of object detection tasks to a classifier,
applied in special grid positions of the tested image. The second contribution
is Gray Level-Radius Co-occurrence Matrix, which describes local image texture
and topology and, unlike common second order statistics methods, is robust to
image resolution. The third contribution is a parametrically controlled
automatic synthesis of unlimited number of numerical features for
classification. The fourth contribution is a method of optimizing parameters C
and gamma in LibSVM-based classifier, which is 20-100 times faster than the
commonly applied method. The work is essentially experimental, with
demonstration of various methods for definition of objects of interest in
images and video sequences."
"This paper gives the definition of Transparent Neural Network ""TNN"" for the
simulation of the globallocal vision and its application to the segmentation of
administrative document image. We have developed and have adapted a recognition
method which models the contextual effects reported from studies in
experimental psychology. Then, we evaluated and tested the TNN and the
multi-layer perceptron ""MLP"", which showed its effectiveness in the field of
the recognition, in order to show that the TNN is clearer for the user and more
powerful on the level of the recognition. Indeed, the TNN is the only system
which makes it possible to recognize the document and its structure."
"A new method for removing impulse noise from speech in the wavelet transform
domain is proposed. The method utilizes the multiresolution property of the
wavelet transform, which provides finer time resolution at the higher
frequencies than the short-time Fourier transform (STFT), to effectively
identify and remove impulse noise. It uses two features of speech to
discriminate speech from impulse noise: one is the slow time-varying nature of
speech and the other is the Lipschitz regularity of the speech components. On
the basis of these features, an algorithm has been developed to identify and
suppress wavelet coefficients that correspond to impulse noise. Experiment
results show that the new method is able to significantly reduce impulse noise
without degrading the quality of the speech signal or introducing any audible
artifacts."
"In application of tomography imaging, limited-angle problem is a quite
practical and important issue. In this paper, an iterative
reprojection-reconstruction (IRR) algorithm using a modified Papoulis-Gerchberg
(PG) iterative scheme is developed for reconstruction from limited-angle
projections which contain noise. The proposed algorithm has two iterative
update processes, one is the extrapolation of unknown data, and the other is
the modification of the known noisy observation data. And the algorithm
introduces scaling factors to control the two processes, respectively. The
convergence of the algorithm is guaranteed, and the method of choosing the
scaling factors is given with energy constraints. The simulation result
demonstrates our conclusions and indicates that the algorithm proposed in this
paper can obviously improve the reconstruction quality."
"This paper presents an intelligent traffic monitoring system using wireless
vision sensor network that captures and processes the real-time video image to
obtain the traffic flow rate and vehicle speeds along different urban roadways.
This system will display the traffic states on the front roadways that can
guide the drivers to select the right way and avoid potential traffic
congestions. On the other hand, it will also monitor the vehicle speeds and
store the vehicle details, for those breaking the roadway speed limits, in its
database. The real-time traffic data is processed by the Personal Computer (PC)
at the sub roadway station and the traffic flow rate data is transmitted to the
main roadway station Arduino 3G via email, where the data is extracted and
traffic flow rate displayed."
"Multi-region segmentation algorithms often have the onus of incorporating
complex anatomical knowledge representing spatial or geometric relationships
between objects, and general-purpose methods of addressing this knowledge in an
optimization-based manner have thus been lacking. This paper presents
Generalized Hierarchical Max-Flow (GHMF) segmentation, which captures simple
anatomical part-whole relationships in the form of an unconstrained hierarchy.
Regularization can then be applied to both parts and wholes independently,
allowing for spatial grouping and clustering of labels in a globally optimal
convex optimization framework. For the purposes of ready integration into a
variety of segmentation tasks, the hierarchies can be presented in run-time,
allowing for the segmentation problem to be readily specified and alternatives
explored without undue programming effort or recompilation."
"Szeliski et al. published an influential study in 2006 on energy minimization
methods for Markov Random Fields (MRF). This study provided valuable insights
in choosing the best optimization technique for certain classes of problems.
While these insights remain generally useful today, the phenomenal success of
random field models means that the kinds of inference problems that have to be
solved changed significantly. Specifically, the models today often include
higher order interactions, flexible connectivity structures, large
la\-bel-spaces of different cardinalities, or learned energy tables. To reflect
these changes, we provide a modernized and enlarged study. We present an
empirical comparison of 32 state-of-the-art optimization techniques on a corpus
of 2,453 energy minimization instances from diverse applications in computer
vision. To ensure reproducibility, we evaluate all methods in the OpenGM 2
framework and report extensive results regarding runtime and solution quality.
Key insights from our study agree with the results of Szeliski et al. for the
types of models they studied. However, on new and challenging types of models
our findings disagree and suggest that polyhedral methods and integer
programming solvers are competitive in terms of runtime and solution quality
over a large range of model types."
"We deal with the Fourier-like analysis of functions on discrete grids in
two-dimensional simplexes using $C-$ and $E-$ Weyl group orbit functions. For
these cases we present the convolution theorem. We provide an example of
application of image processing using the $C-$ functions and the convolutions
for spatial filtering of the treated image."
"We present MBIS (Multivariate Bayesian Image Segmentation tool), a clustering
tool based on the mixture of multivariate normal distributions model. MBIS
supports multi-channel bias field correction based on a B-spline model. A
second methodological novelty is the inclusion of graph-cuts optimization for
the stationary anisotropic hidden Markov random field model. Along with MBIS,
we release an evaluation framework that contains three different experiments on
multi-site data. We first validate the accuracy of segmentation and the
estimated bias field for each channel. MBIS outperforms a widely used
segmentation tool in a cross-comparison evaluation. The second experiment
demonstrates the robustness of results on atlas-free segmentation of two image
sets from scan-rescan protocols on 21 healthy subjects. Multivariate
segmentation is more replicable than the monospectral counterpart on
T1-weighted images. Finally, we provide a third experiment to illustrate how
MBIS can be used in a large-scale study of tissue volume change with increasing
age in 584 healthy subjects. This last result is meaningful as multivariate
segmentation performs robustly without the need for prior knowledge"
"Document Image Analysis, like any Digital Image Analysis requires
identification and extraction of proper features, which are generally extracted
from uncompressed images, though in reality images are made available in
compressed form for the reasons such as transmission and storage efficiency.
However, this implies that the compressed image should be decompressed, which
indents additional computing resources. This limitation induces the motivation
to research in extracting features directly from the compressed image. In this
research, we propose to extract essential features such as projection profile,
run-histogram and entropy for text document analysis directly from run-length
compressed text-documents. The experimentation illustrates that features are
extracted directly from the compressed image without going through the stage of
decompression, because of which the computing time is reduced. The feature
values so extracted are exactly identical to those extracted from uncompressed
images."
"There are a large number of methods for solving under-determined linear
inverse problem. Many of them have very high time complexity for large
datasets. We propose a new method called Two-Stage Sparse Representation (TSSR)
to tackle this problem. We decompose the representing space of signals into two
parts, the measurement dictionary and the sparsifying basis. The dictionary is
designed to approximate a sub-Gaussian distribution to exploit its
concentration property. We apply sparse coding to the signals on the dictionary
in the first stage, and obtain the training and testing coefficients
respectively. Then we design the basis to approach an identity matrix in the
second stage, to acquire the Restricted Isometry Property (RIP) and
universality property. The testing coefficients are encoded over the basis and
the final representing coefficients are obtained. We verify that the projection
of testing coefficients onto the basis is a good approximation of the signal
onto the representing space. Since the projection is conducted on a much
sparser space, the runtime is greatly reduced. For concrete realization, we
provide an instance for the proposed TSSR. Experiments on four biometrics
databases show that TSSR is effective and efficient, comparing with several
classical methods for solving linear inverse problem."
"Handwritten automatic character recognition has attracted many researchers
all over the world to contribute automatic character recognition domain. Shape
identification and feature extraction is very important part of any character
recognition system and success of method is highly dependent on selection of
features. However feature extraction is the most important step in defining the
shape of the character as precisely and as uniquely as possible. This is indeed
the most important step and complex task as well and achieved success by using
invariance property, irrespective of position and orientation. Zernike moments
describes shape, identify rotation invariant due to its Orthogonality property.
MODI is an ancient script of India had cursive and complex representation of
characters. The work described in this paper presents efficiency of Zernike
moments over Hus moment for automatic recognition of handwritten MODI numerals."
"The capability to exploit multiple sources of information is of fundamental
importance in a battlefield scenario. Information obtained from different
sources, and separated in space and time, provide the opportunity to exploit
diversities in order to mitigate uncertainty. For the specific challenge of
Automatic Target Recognition (ATR) from radar platforms, both channel (e.g.
polarization) and spatial diversity can provide useful information for such a
specific and critical task. In this paper the use of pseudo-Zernike moments
applied to multi-channel multi-pass data is presented exploiting diversities
and invariant properties leading to high confidence ATR, small computational
complexity and data transfer requirements. The effectiveness of the proposed
approach, in different configurations and data source availability is
demonstrated using real data."
"It has been shown that the activations invoked by an image within the top
layers of a large convolutional neural network provide a high-level descriptor
of the visual content of the image. In this paper, we investigate the use of
such descriptors (neural codes) within the image retrieval application. In the
experiments with several standard retrieval benchmarks, we establish that
neural codes perform competitively even when the convolutional neural network
has been trained for an unrelated classification task (e.g.\ Image-Net). We
also evaluate the improvement in the retrieval performance of neural codes,
when the network is retrained on a dataset of images that are similar to images
encountered at test time.
  We further evaluate the performance of the compressed neural codes and show
that a simple PCA compression provides very good short codes that give
state-of-the-art accuracy on a number of datasets. In general, neural codes
turn out to be much more resilient to such compression in comparison other
state-of-the-art descriptors. Finally, we show that discriminative
dimensionality reduction trained on a dataset of pairs of matched photographs
improves the performance of PCA-compressed neural codes even further. Overall,
our quantitative experiments demonstrate the promise of neural codes as visual
descriptors for image retrieval."
"The top-performing systems for billion-scale high-dimensional approximate
nearest neighbor (ANN) search are all based on two-layer architectures that
include an indexing structure and a compressed datapoints layer. An indexing
structure is crucial as it allows to avoid exhaustive search, while the lossy
data compression is needed to fit the dataset into RAM. Several of the most
successful systems use product quantization (PQ) for both the indexing and the
dataset compression layers. These systems are however limited in the way they
exploit the interaction of product quantization processes that happen at
different stages of these systems.
  Here we introduce and evaluate two approximate nearest neighbor search
systems that both exploit the synergy of product quantization processes in a
more efficient way. The first system, called Fast Bilayer Product Quantization
(FBPQ), speeds up the runtime of the baseline system (Multi-D-ADC) by several
times, while achieving the same accuracy. The second system, Hierarchical
Bilayer Product Quantization (HBPQ) provides a significantly better recall for
the same runtime at a cost of small memory footprint increase. For the BIGANN
dataset of billion SIFT descriptors, the 10% increase in Recall@1 and the 17%
increase in Recall@10 is observed."
"Convolutional Neural Networks (CNNs) can provide accurate object
classification. They can be extended to perform object detection by iterating
over dense or selected proposed object regions. However, the runtime of such
detectors scales as the total number and/or area of regions to examine per
image, and training such detectors may be prohibitively slow. However, for some
CNN classifier topologies, it is possible to share significant work among
overlapping regions to be classified. This paper presents DenseNet, an open
source system that computes dense, multiscale features from the convolutional
layers of a CNN based object classifier. Future work will involve training
efficient object detectors with DenseNet feature descriptors."
"The tracking algorithm performance depends on video content. This paper
presents a new multi-object tracking approach which is able to cope with video
content variations. First the object detection is improved using Kanade-
Lucas-Tomasi (KLT) feature tracking. Second, for each mobile object, an
appropriate tracker is selected among a KLT-based tracker and a discriminative
appearance-based tracker. This selection is supported by an online tracking
evaluation. The approach has been experimented on three public video datasets.
The experimental results show a better performance of the proposed approach
compared to recent state of the art trackers."
"Compression of documents, images, audios and videos have been traditionally
practiced to increase the efficiency of data storage and transfer. However, in
order to process or carry out any analytical computations, decompression has
become an unavoidable pre-requisite. In this research work, we have attempted
to compute the entropy, which is an important document analytic directly from
the compressed documents. We use Conventional Entropy Quantifier (CEQ) and
Spatial Entropy Quantifiers (SEQ) for entropy computations [1]. The entropies
obtained are useful in applications like establishing equivalence, word
spotting and document retrieval. Experiments have been performed with all the
data sets of [1], at character, word and line levels taking compressed
documents in run-length compressed domain. The algorithms developed are
computational and space efficient, and results obtained match 100% with the
results reported in [1]."
"Conditional random fields (CRFs) are popular discriminative models for
computer vision and have been successfully applied in the domain of image
restoration, especially to image denoising. For image deblurring, however,
discriminative approaches have been mostly lacking. We posit two reasons for
this: First, the blur kernel is often only known at test time, requiring any
discriminative approach to cope with considerable variability. Second, given
this variability it is quite difficult to construct suitable features for
discriminative prediction. To address these challenges we first show a
connection between common half-quadratic inference for generative image priors
and Gaussian CRFs. Based on this analysis, we then propose a cascade model for
image restoration that consists of a Gaussian CRF at each stage. Each stage of
our cascade is semi-parametric, i.e. it depends on the instance-specific
parameters of the restoration problem, such as the blur kernel. We train our
model by loss minimization with synthetically generated training data. Our
experiments show that when applied to non-blind image deblurring, the proposed
approach is efficient and yields state-of-the-art restoration quality on images
corrupted with synthetic and real blur. Moreover, we demonstrate its
suitability for image denoising, where we achieve competitive results for
grayscale and color images."
"We propose a novel compact linear programming (LP) relaxation for binary
sub-modular MRF in the context of object segmentation. Our model is obtained by
linearizing an $l_1^+$-norm derived from the quadratic programming (QP) form of
the MRF energy. The resultant LP model contains significantly fewer variables
and constraints compared to the conventional LP relaxation of the MRF energy.
In addition, unlike QP which can produce ambiguous labels, our model can be
viewed as a quasi-total-variation minimization problem, and it can therefore
preserve the discontinuities in the labels. We further establish a relaxation
bound between our LP model and the conventional LP model. In the experiments,
we demonstrate our method for the task of interactive object segmentation. Our
LP model outperforms QP when converting the continuous labels to binary labels
using different threshold values on the entire Oxford interactive segmentation
dataset. The computational complexity of our LP is of the same order as that of
the QP, and it is significantly lower than the conventional LP relaxation."
"Optimization techniques have been widely used in deformable registration,
allowing for the incorporation of similarity metrics with regularization
mechanisms. These regularization mechanisms are designed to mitigate the
effects of trivial solutions to ill-posed registration problems and to
otherwise ensure the resulting deformation fields are well-behaved. This paper
introduces a novel deformable registration algorithm, RANCOR, which uses
iterative convexification to address deformable registration problems under
total-variation regularization. Initial comparative results against four
state-of-the-art registration algorithms are presented using the Internet Brain
Segmentation Repository (IBSR) database."
"A number of psychological and physiological evidences suggest that early
visual attention works in a coarse-to-fine way, which lays a basis for the
reverse hierarchy theory (RHT). This theory states that attention propagates
from the top level of the visual hierarchy that processes gist and abstract
information of input, to the bottom level that processes local details.
Inspired by the theory, we develop a computational model for saliency detection
in images. First, the original image is downsampled to different scales to
constitute a pyramid. Then, saliency on each layer is obtained by image
super-resolution reconstruction from the layer above, which is defined as
unpredictability from this coarse-to-fine reconstruction. Finally, saliency on
each layer of the pyramid is fused into stochastic fixations through a
probabilistic model, where attention initiates from the top layer and
propagates downward through the pyramid. Extensive experiments on two standard
eye-tracking datasets show that the proposed method can achieve competitive
results with state-of-the-art models."
"In this paper, we propose a novel action recognition framework. The method
uses pictorial structures and shrinkage optimized directed information
assessment (SODA) coupled with Markov Random Fields called SODA+MRF to model
the directional temporal dependency and bidirectional spatial dependency. As a
variant of mutual information, directional information captures the directional
information flow and temporal structure of video sequences across frames.
Meanwhile, within each frame, Markov random fields are utilized to model the
spatial relations among different parts of a human body and the body parts of
different people. The proposed SODA+MRF model is robust to view point
transformations and detect complex interactions accurately. We compare the
proposed method against several baseline methods to highlight the effectiveness
of the SODA+MRF model. We demonstrate that our algorithm has superior action
recognition performance on the UCF action recognition dataset, the Olympic
sports dataset and the collective activity dataset over several
state-of-the-art methods."
"Effective and accurate diagnosis of Alzheimer's disease (AD) or mild
cognitive impairment (MCI) can be critical for early treatment and thus has
attracted more and more attention nowadays. Since first introduced, machine
learning methods have been gaining increasing popularity for AD related
research. Among the various identified biomarkers, magnetic resonance imaging
(MRI) are widely used for the prediction of AD or MCI. However, before a
machine learning algorithm can be applied, image features need to be extracted
to represent the MRI images. While good representations can be pivotal to the
classification performance, almost all the previous studies typically rely on
human labelling to find the regions of interest (ROI) which may be correlated
to AD, such as hippocampus, amygdala, precuneus, etc. This procedure requires
domain knowledge and is costly and tedious.
  Instead of relying on extraction of ROI features, it is more promising to
remove manual ROI labelling from the pipeline and directly work on the raw MRI
images. In other words, we can let the machine learning methods to figure out
these informative and discriminative image structures for AD classification. In
this work, we propose to learn deep convolutional image features using
unsupervised and supervised learning. Deep learning has emerged as a powerful
tool in the machine learning community and has been successfully applied to
various tasks. We thus propose to exploit deep features of MRI images based on
a pre-trained large convolutional neural network (CNN) for AD and MCI
classification, which spares the effort of manual ROI annotation process."
"The 38th Annual Workshop of the Austrian Association for Pattern Recognition
(\""OAGM) will be held at IST Austria, on May 22-23, 2014. The workshop provides
a platform for researchers and industry to discuss traditional and new areas of
computer vision. This year the main topic is: Pattern Recognition:
interdisciplinary challenges and opportunities."
"Face images in the wild undergo large intra-personal variations, such as
poses, illuminations, occlusions, and low resolutions, which cause great
challenges to face-related applications. This paper addresses this challenge by
proposing a new deep learning framework that can recover the canonical view of
face images. It dramatically reduces the intra-person variances, while
maintaining the inter-person discriminativeness. Unlike the existing face
reconstruction methods that were either evaluated in controlled 2D environment
or employed 3D information, our approach directly learns the transformation
from the face images with a complex set of variations to their canonical views.
At the training stage, to avoid the costly process of labeling canonical-view
images from the training set by hand, we have devised a new measurement to
automatically select or synthesize a canonical-view image for each identity. As
an application, this face recovery approach is used for face verification.
Facial features are learned from the recovered canonical-view face images by
using a facial component-based convolutional neural network. Our approach
achieves the state-of-the-art performance on the LFW dataset."
"This paper presents a part-based face detection approach where the spatial
relationship between the face parts is represented by a hidden 3D model with
six parameters. The computational complexity of the search in the six
dimensional pose space is addressed by proposing meaningful 3D pose candidates
by image-based regression from detected face keypoint locations. The 3D pose
candidates are evaluated using a parameter sensitive classifier based on
difference features relative to the 3D pose. A compatible subset of candidates
is then obtained by non-maximal suppression. Experiments on two standard face
detection datasets show that the proposed 3D model based approach obtains
results comparable to or better than state of the art."
"Natural image matting, which separates foreground from background, is a very
important intermediate step in recent computer vision algorithms. However, it
is severely underconstrained and difficult to solve. State-of-the-art
approaches include matting by graph Laplacian, which significantly improves the
underconstrained nature by reducing the solution space. However, matting by
graph Laplacian is still very difficult to solve and gets much harder as the
image size grows: current iterative methods slow down as $\mathcal{O}\left(n^2
\right)$ in the resolution $n$. This creates uncomfortable practical limits on
the resolution of images that we can matte. Current literature mitigates the
problem, but they all remain super-linear in complexity. We expose properties
of the problem that remain heretofore unexploited, demonstrating that an
optimization technique originally intended to solve PDEs can be adapted to take
advantage of this knowledge to solve the matting problem, not heuristically,
but exactly and with sub-linear complexity. This makes ours the most efficient
matting solver currently known by a very wide margin and allows matting finally
to be practical and scalable in the future as consumer photos exceed many
dozens of megapixels, and also relieves matting from being a bottleneck for
vision algorithms that depend on it."
"This is the first report on Working Paper WP-RFM-14-01. The potential and
capability of sparse representations is well-known. However, their
(multivariate variable) vectorial form, which is completely fine in many fields
and disciplines, results in removal and filtering of important ""spatial""
relations that are implicitly carried by two-dimensional [or multi-dimensional]
objects, such as images. In this paper, a new approach, called spiralet sparse
representation, is proposed in order to develop an augmented representation and
therefore a modified sparse representation and theory, which is capable to
preserve the data associated to the spatial relations."
"This paper addresses the challenge of establishing a bridge between deep
convolutional neural networks and conventional object detection frameworks for
accurate and efficient generic object detection. We introduce Dense Neural
Patterns, short for DNPs, which are dense local features derived from
discriminatively trained deep convolutional neural networks. DNPs can be easily
plugged into conventional detection frameworks in the same way as other dense
local features(like HOG or LBP). The effectiveness of the proposed approach is
demonstrated with the Regionlets object detection framework. It achieved 46.1%
mean average precision on the PASCAL VOC 2007 dataset, and 44.1% on the PASCAL
VOC 2010 dataset, which dramatically improves the original Regionlets approach
without DNPs."
"In this article, we present a graph-based method using a cubic template for
volumetric segmentation of vertebrae in magnetic resonance imaging (MRI)
acquisitions. The user can define the degree of deviation from a regular cube
via a smoothness value Delta. The Cube-Cut algorithm generates a directed graph
with two terminal nodes (s-t-network), where the nodes of the graph correspond
to a cubic-shaped subset of the image's voxels. The weightings of the graph's
terminal edges, which connect every node with a virtual source s or a virtual
sink t, represent the affinity of a voxel to the vertebra (source) and to the
background (sink). Furthermore, a set of infinite weighted and non-terminal
edges implements the smoothness term. After graph construction, a minimal
s-t-cut is calculated within polynomial computation time, which splits the
nodes into two disjoint units. Subsequently, the segmentation result is
determined out of the source-set. A quantitative evaluation of a C++
implementation of the algorithm resulted in an average Dice Similarity
Coefficient (DSC) of 81.33% and a running time of less than a minute."
"Learning fine-grained image similarity is a challenging task. It needs to
capture between-class and within-class image differences. This paper proposes a
deep ranking model that employs deep learning techniques to learn similarity
metric directly from images.It has higher learning capability than models based
on hand-crafted features. A novel multiscale network structure has been
developed to describe the images effectively. An efficient triplet sampling
algorithm is proposed to learn the model with distributed asynchronized
stochastic gradient. Extensive experiments show that the proposed algorithm
outperforms models based on hand-crafted visual features and deep
classification models."
"Online feature selection with dynamic features has become an active research
area in recent years. However, in some real-world applications such as image
analysis and email spam filtering, features may arrive by groups. Existing
online feature selection methods evaluate features individually, while existing
group feature selection methods cannot handle online processing. Motivated by
this, we formulate the online group feature selection problem, and propose a
novel selection approach for this problem. Our proposed approach consists of
two stages: online intra-group selection and online inter-group selection. In
the intra-group selection, we use spectral analysis to select discriminative
features in each group when it arrives. In the inter-group selection, we use
Lasso to select a globally optimal subset of features. This 2-stage procedure
continues until there are no more features to come or some predefined stopping
conditions are met. Extensive experiments conducted on benchmark and real-world
data sets demonstrate that our proposed approach outperforms other
state-of-the-art online feature selection methods."
"Sparse Representation (or coding) based Classification (SRC) has gained great
success in face recognition in recent years. However, SRC emphasizes the
sparsity too much and overlooks the correlation information which has been
demonstrated to be critical in real-world face recognition problems. Besides,
some work considers the correlation but overlooks the discriminative ability of
sparsity. Different from these existing techniques, in this paper, we propose a
framework called Adaptive Sparse Representation based Classification (ASRC) in
which sparsity and correlation are jointly considered. Specifically, when the
samples are of low correlation, ASRC selects the most discriminative samples
for representation, like SRC; when the training samples are highly correlated,
ASRC selects most of the correlated and discriminative samples for
representation, rather than choosing some related samples randomly. In general,
the representation model is adaptive to the correlation structure, which
benefits from both $\ell_1$-norm and $\ell_2$-norm.
  Extensive experiments conducted on publicly available data sets verify the
effectiveness and robustness of the proposed algorithm by comparing it with
state-of-the-art methods."
"In this paper, we present a new pipeline which automatically identifies and
annotates axoplasmic reticula, which are small subcellular structures present
only in axons. We run our algorithm on the Kasthuri11 dataset, which was color
corrected using gradient-domain techniques to adjust contrast. We use a
bilateral filter to smooth out the noise in this data while preserving edges,
which highlights axoplasmic reticula. These axoplasmic reticula are then
annotated using a morphological region growing algorithm. Additionally, we
perform Laplacian sharpening on the bilaterally filtered data to enhance edges,
and repeat the morphological region growing algorithm to annotate more
axoplasmic reticula. We track our annotations through the slices to improve
precision, and to create long objects to aid in segment merging. This method
annotates axoplasmic reticula with high precision. Our algorithm can easily be
adapted to annotate axoplasmic reticula in different sets of brain data by
changing a few thresholds. The contribution of this work is the introduction of
a straightforward and robust pipeline which annotates axoplasmic reticula with
high precision, contributing towards advancements in automatic feature
annotations in neural EM data."
"In this paper, we utilize structured learning to simultaneously address two
intertwined problems: human pose estimation (HPE) and garment attribute
classification (GAC), which are valuable for a variety of computer vision and
multimedia applications. Unlike previous works that usually handle the two
problems separately, our approach aims to produce a jointly optimal estimation
for both HPE and GAC via a unified inference procedure. To this end, we adopt a
preprocessing step to detect potential human parts from each image (i.e., a set
of ""candidates"") that allows us to have a manageable input space. In this way,
the simultaneous inference of HPE and GAC is converted to a structured learning
problem, where the inputs are the collections of candidate ensembles, the
outputs are the joint labels of human parts and garment attributes, and the
joint feature representation involves various cues such as pose-specific
features, garment-specific features, and cross-task features that encode
correlations between human parts and garment attributes. Furthermore, we
explore the ""strong edge"" evidence around the potential human parts so as to
derive more powerful representations for oriented human parts. Such evidences
can be seamlessly integrated into our structured learning model as a kind of
energy function, and the learning process could be performed by standard
structured Support Vector Machines (SVM) algorithm. However, the joint
structure of the two problems is a cyclic graph, which hinders efficient
inference. To resolve this issue, we compute instead approximate optima by
using an iterative procedure, where in each iteration the variables of one
problem are fixed. In this way, satisfactory solutions can be efficiently
computed by dynamic programming. Experimental results on two benchmark datasets
show the state-of-the-art performance of our approach."
"Creating geometric abstracted models from image-based scene reconstructions
is difficult due to noise and irregularities in the reconstructed model. In
this paper, we present a geometric modeling method for noisy reconstructions
dominated by planar horizontal and orthogonal vertical structures. We partition
the scene into horizontal slices and create an inside/outside labeling
represented by a floor plan for each slice by solving an energy minimization
problem. Consecutively, we create an irregular discretization of the volume
according to the individual floor plans and again label each cell as
inside/outside by minimizing an energy function. By adjusting the smoothness
parameter, we introduce different levels of detail. In our experiments, we show
results with varying regularization levels using synthetically generated and
real-world data."
"The Fields of Experts (FoE) image prior model, a filter-based higher-order
Markov Random Fields (MRF) model, has been shown to be effective for many image
restoration problems. Motivated by the successes of FoE-based approaches, in
this letter, we propose a novel variational model for multiplicative noise
reduction based on the FoE image prior model. The resulted model corresponds to
a non-convex minimization problem, which can be solved by a recently published
non-convex optimization algorithm. Experimental results based on synthetic
speckle noise and real synthetic aperture radar (SAR) images suggest that the
performance of our proposed method is on par with the best published
despeckling algorithm. Besides, our proposed model comes along with an
additional advantage, that the inference is extremely efficient. {Our GPU based
implementation takes less than 1s to produce state-of-the-art despeckling
performance.}"
"We identify a novel instance of the background subtraction problem that
focuses on extracting near-field foreground objects captured using handheld
cameras. Given two user-generated videos of a scene, one with and the other
without the foreground object(s), our goal is to efficiently generate an output
video with only the foreground object(s) present in it. We cast this challenge
as a spatio-temporal frame matching problem, and propose an efficient solution
for it that exploits the temporal smoothness of the video sequences. We present
theoretical analyses for the error bounds of our approach, and validate our
findings using a detailed set of simulation experiments. Finally, we present
the results of our approach tested on multiple real videos captured using
handheld cameras, and compare them to several alternate foreground extraction
approaches."
"In this paper, we propose a novel image set representation and classification
method by maximizing the margin of image sets. The margin of an image set is
defined as the difference of the distance to its nearest image set from
different classes and the distance to its nearest image set of the same class.
By modeling the image sets by using both their image samples and their affine
hull models, and maximizing the margins of the images sets, the image set
representation parameter learning problem is formulated as an minimization
problem, which is further optimized by an expectation -maximization (EM)
strategy with accelerated proximal gradient (APG) optimization in an iterative
algorithm. To classify a given test image set, we assign it to the class which
could provide the largest margin. Experiments on two applications of
video-sequence-based face recognition demonstrate that the proposed method
significantly outperforms state-of-the-art image set classification methods in
terms of both effectiveness and efficiency."
"Correlation Filters (CFs) are a class of classifiers which are designed for
accurate pattern localization. Traditionally CFs have been used with scalar
features only, which limits their ability to be used with vector feature
representations like Gabor filter banks, SIFT, HOG, etc. In this paper we
present a new CF named Maximum Margin Vector Correlation Filter (MMVCF) which
extends the traditional CF designs to vector features. MMVCF further combines
the generalization capability of large margin based classifiers like Support
Vector Machines (SVMs) and the localization properties of CFs for better
robustness to outliers. We demonstrate the efficacy of MMVCF for object
detection and landmark localization on a variety of databases and demonstrate
that MMVCF consistently shows improved pattern localization capability in
comparison to SVMs."
"We present algebraic projective geometry definitions of 3D rotations so as to
bridge a small gap between the applications and the definitions of 3D rotations
in homogeneous matrix form. A general homogeneous matrix formulation to 3D
rotation geometric transformations is proposed which suits for the cases when
the rotation axis is unnecessarily through the coordinate system origin given
their rotation axes and rotation angles."
"In air traffic management (ATM) all necessary operations (tactical planing,
sector configuration, required staffing, runway configuration, routing of
approaching aircrafts) rely on accurate measurements and predictions of the
current weather situation. An essential basis of information is delivered by
weather radar images (WXR), which, unfortunately, exhibit a vast amount of
disturbances. Thus, the improvement of these datasets is the key factor for
more accurate predictions of weather phenomena and weather conditions. Image
processing methods based on texture analysis and geometric operators allow to
identify regions including artefacts as well as zones of missing information.
Correction of these zones is implemented by exploiting multi-spectral satellite
data (Meteosat Second Generation). Results prove that the proposed system for
artefact detection and data correction significantly improves the quality of
WXR data and, thus, enables more reliable weather now- and forecast leading to
increased ATM safety."
"Activity recognition in sport is an attractive field for computer vision
research. Game, player and team analysis are of great interest and research
topics within this field emerge with the goal of automated analysis. The very
specific underlying rules of sports can be used as prior knowledge for the
recognition task and present a constrained environment for evaluation. This
paper describes recognition of single player activities in sport with special
emphasis on volleyball. Starting from a per-frame player-centered activity
recognition, we incorporate geometry and contextual information via an activity
context descriptor that collects information about all player's activities over
a certain timespan relative to the investigated player. The benefit of this
context information on single player activity recognition is evaluated on our
new real-life dataset presenting a total amount of almost 36k annotated frames
containing 7 activity classes within 6 videos of professional volleyball games.
Our incorporation of the contextual information improves the average
player-centered classification performance of 77.56% by up to 18.35% on
specific classes, proving that spatio-temporal context is an important clue for
activity recognition."
"This paper studies the subspace segmentation problem which aims to segment
data drawn from a union of multiple linear subspaces. Recent works by using
sparse representation, low rank representation and their extensions attract
much attention. If the subspaces from which the data drawn are independent or
orthogonal, they are able to obtain a block diagonal affinity matrix, which
usually leads to a correct segmentation. The main differences among them are
their objective functions. We theoretically show that if the objective function
satisfies some conditions, and the data are sufficiently drawn from independent
subspaces, the obtained affinity matrix is always block diagonal. Furthermore,
the data sampling can be insufficient if the subspaces are orthogonal. Some
existing methods are all special cases. Then we present the Least Squares
Regression (LSR) method for subspace segmentation. It takes advantage of data
correlation, which is common in real data. LSR encourages a grouping effect
which tends to group highly correlated data together. Experimental results on
the Hopkins 155 database and Extended Yale Database B show that our method
significantly outperforms state-of-the-art methods. Beyond segmentation
accuracy, all experiments demonstrate that LSR is much more efficient."
"We propose an algorithm for recovering depth using less than two images.
Instead of having both cameras send their entire image to the host computer,
the left camera sends its image to the host while the right camera sends only a
fraction $\epsilon$ of its image. The key aspect is that the cameras send the
information without communicating at all. Hence, the required communication
bandwidth is significantly reduced.
  While standard image compression techniques can reduce the communication
bandwidth, this requires additional computational resources on the part of the
encoder (camera). We aim at designing a light weight encoder that only touches
a fraction of the pixels. The burden of decoding is placed on the decoder
(host).
  We show that it is enough for the encoder to transmit a sparse set of pixels.
Using only $1+\epsilon$ images, with $\epsilon$ as little as 2% of the image,
the decoder can compute a depth map. The depth map's accuracy is comparable to
traditional stereo matching algorithms that require both images as input. Using
the depth map and the left image, the right image can be synthesized. No
computations are required at the encoder, and the decoder's runtime is linear
in the images' size."
"The ability to recognize the liquid surface and the liquid level in
transparent containers is perhaps the most commonly used evaluation method when
dealing with fluids. Such recognition is essential in determining the liquid
volume, fill level, phase boundaries and phase separation in various fluid
systems. The recognition of liquid surfaces is particularly important in
solution chemistry, where it is essential to many laboratory techniques (e.g.,
extraction, distillation, titration). A general method for the recognition of
interfaces between liquid and air or between phase-separating liquids could
have a wide range of applications and contribute to the understanding of the
visual properties of such interfaces. This work examines a computer vision
method for the recognition of liquid surfaces and liquid levels in various
transparent containers. The method can be applied to recognition of both
liquid-air and liquid-liquid surfaces. No prior knowledge of the number of
phases is required. The method receives the image of the liquid container and
the boundaries of the container in the image and scans all possible curves that
could correspond to the outlines of liquid surfaces in the image. The method
then compares each curve to the image to rate its correspondence with the
outline of the real liquid surface by examining various image properties in the
area surrounding each point of the curve. The image properties that were found
to give the best indication of the liquid surface are the relative intensity
change, the edge density change and the gradient direction relative to the
curve normal."
"A novel coding strategy for block-based compressive sens-ing named spatially
directional predictive coding (SDPC) is proposed, which efficiently utilizes
the intrinsic spatial cor-relation of natural images. At the encoder, for each
block of compressive sensing (CS) measurements, the optimal pre-diction is
selected from a set of prediction candidates that are generated by four
designed directional predictive modes. Then, the resulting residual is
processed by scalar quantiza-tion (SQ). At the decoder, the same prediction is
added onto the de-quantized residuals to produce the quantized CS measurements,
which is exploited for CS reconstruction. Experimental results substantiate
significant improvements achieved by SDPC-plus-SQ in rate distortion
performance as compared with SQ alone and DPCM-plus-SQ."
"Compressive Sensing (CS) theory shows that a signal can be decoded from many
fewer measurements than suggested by the Nyquist sampling theory, when the
signal is sparse in some domain. Most of conventional CS recovery approaches,
however, exploited a set of fixed bases (e.g. DCT, wavelet, contourlet and
gradient domain) for the entirety of a signal, which are irrespective of the
nonstationarity of natural signals and cannot achieve high enough degree of
sparsity, thus resulting in poor rate-distortion performance. In this paper, we
propose a new framework for image compressive sensing recovery via structural
group sparse representation (SGSR) modeling, which enforces image sparsity and
self-similarity simultaneously under a unified framework in an adaptive group
domain, thus greatly confining the CS solution space. In addition, an efficient
iterative shrinkage/thresholding algorithm based technique is developed to
solve the above optimization problem. Experimental results demonstrate that the
novel CS recovery strategy achieves significant performance improvements over
the current state-of-the-art schemes and exhibits nice convergence."
"From many fewer acquired measurements than suggested by the Nyquist sampling
theory, compressive sensing (CS) theory demonstrates that, a signal can be
reconstructed with high probability when it exhibits sparsity in some domain.
Most of the conventional CS recovery approaches, however, exploited a set of
fixed bases (e.g. DCT, wavelet and gradient domain) for the entirety of a
signal, which are irrespective of the non-stationarity of natural signals and
cannot achieve high enough degree of sparsity, thus resulting in poor CS
recovery performance. In this paper, we propose a new framework for image
compressive sensing recovery using adaptively learned sparsifying basis via L0
minimization. The intrinsic sparsity of natural images is enforced
substantially by sparsely representing overlapped image patches using the
adaptively learned sparsifying basis in the form of L0 norm, greatly reducing
blocking artifacts and confining the CS solution space. To make our proposed
scheme tractable and robust, a split Bregman iteration based technique is
developed to solve the non-convex L0 minimization problem efficiently.
Experimental results on a wide range of natural images for CS recovery have
shown that our proposed algorithm achieves significant performance improvements
over many current state-of-the-art schemes and exhibits good convergence
property."
"The core component of most modern trackers is a discriminative classifier,
tasked with distinguishing between the target and the surrounding environment.
To cope with natural image changes, this classifier is typically trained with
translated and scaled sample patches. Such sets of samples are riddled with
redundancies -- any overlapping pixels are constrained to be the same. Based on
this simple observation, we propose an analytic model for datasets of thousands
of translated patches. By showing that the resulting data matrix is circulant,
we can diagonalize it with the Discrete Fourier Transform, reducing both
storage and computation by several orders of magnitude. Interestingly, for
linear regression our formulation is equivalent to a correlation filter, used
by some of the fastest competitive trackers. For kernel regression, however, we
derive a new Kernelized Correlation Filter (KCF), that unlike other kernel
algorithms has the exact same complexity as its linear counterpart. Building on
it, we also propose a fast multi-channel extension of linear correlation
filters, via a linear kernel, which we call Dual Correlation Filter (DCF). Both
KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50
videos benchmark, despite running at hundreds of frames-per-second, and being
implemented in a few lines of code (Algorithm 1). To encourage further
developments, our tracking framework was made open-source."
"This paper introduces the method of dynamic mode decomposition (DMD) for
robustly separating video frames into background (low-rank) and foreground
(sparse) components in real-time. The method is a novel application of a
technique used for characterizing nonlinear dynamical systems in an
equation-free manner by decomposing the state of the system into low-rank terms
whose Fourier components in time are known. DMD terms with Fourier frequencies
near the origin (zero-modes) are interpreted as background (low-rank) portions
of the given video frames, and the terms with Fourier frequencies bounded away
from the origin are their sparse counterparts. An approximate low-rank/sparse
separation is achieved at the computational cost of just one singular value
decomposition and one linear equation solve, thus producing results orders of
magnitude faster than a leading separation method, namely robust principal
component analysis (RPCA). The DMD method that is developed here is
demonstrated to work robustly in real-time with personal laptop-class computing
power and without any parameter tuning, which is a transformative improvement
in performance that is ideal for video surveillance and recognition
applications."
"Finding the best set of gestures to use for a given computer recognition
problem is an essential part of optimizing the recognition performance while
being mindful to those who may articulate the gestures. An objective function,
called the ellipsoidal distance ratio metric (EDRM), for determining the best
gestures from a larger lexicon library is presented, along with a numerical
method for incorporating subjective preferences. In particular, we demonstrate
an efficient algorithm that chooses the best $n$ gestures from a lexicon of $m$
gestures where typically $n \ll m$ using a weighting of both subjective and
objective measures."
"This survey paper aims at providing a ""literary"" anthology of mathematical
morphology on graphs. It describes in the English language many ideas stemming
from a large number of different papers, hence providing a unified view of an
active and diverse field of research."
"This paper introduces sparse coding and dictionary learning for Symmetric
Positive Definite (SPD) matrices, which are often used in machine learning,
computer vision and related areas. Unlike traditional sparse coding schemes
that work in vector spaces, in this paper we discuss how SPD matrices can be
described by sparse combination of dictionary atoms, where the atoms are also
SPD matrices. We propose to seek sparse coding by embedding the space of SPD
matrices into Hilbert spaces through two types of Bregman matrix divergences.
This not only leads to an efficient way of performing sparse coding, but also
an online and iterative scheme for dictionary learning. We apply the proposed
methods to several computer vision tasks where images are represented by region
covariance matrices. Our proposed algorithms outperform state-of-the-art
methods on a wide range of classification tasks, including face recognition,
action recognition, material classification and texture categorization."
"Representing images by compact codes has proven beneficial for many visual
recognition tasks. Most existing techniques, however, perform this coding step
directly in image feature space, where the distributions of the different
classes are typically entangled. In contrast, here, we study the problem of
performing coding in a high-dimensional Hilbert space, where the classes are
expected to be more easily separable. To this end, we introduce a general
coding formulation that englobes the most popular techniques, such as bag of
words, sparse coding and locality-based coding, and show how this formulation
and its special cases can be kernelized. Importantly, we address several
aspects of learning in our general formulation, such as kernel learning,
dictionary learning and supervised kernel coding. Our experimental evaluation
on several visual recognition tasks demonstrates the benefits of performing
coding in Hilbert space, and in particular of jointly learning the kernel, the
dictionary and the classifier."
"Many tensor-based data completion methods aim to solve image and video
in-painting problems. But, all methods were only developed for a single
dataset. In most of real applications, we can usually obtain more than one
dataset to reflect one phenomenon, and all the datasets are mutually related in
some sense. Thus one question raised whether such the relationship can improve
the performance of data completion or not? In the paper, we proposed a novel
and efficient method by exploiting the relationship among datasets for
multi-video data completion. Numerical results show that the proposed method
significantly improve the performance of video in-painting, particularly in the
case of very high missing percentage."
"The ImageNet Large Scale Visual Recognition Challenge is a benchmark in
object category classification and detection on hundreds of object categories
and millions of images. The challenge has been run annually from 2010 to
present, attracting participation from more than fifty institutions.
  This paper describes the creation of this benchmark dataset and the advances
in object recognition that have been possible as a result. We discuss the
challenges of collecting large-scale ground truth annotation, highlight key
breakthroughs in categorical object recognition, provide a detailed analysis of
the current state of the field of large-scale image classification and object
detection, and compare the state-of-the-art computer vision accuracy with human
accuracy. We conclude with lessons learned in the five years of the challenge,
and propose future directions and improvements."
"Dataset bias is a well known problem in object recognition domain. This
issue, nonetheless, is rarely explored in face alignment research. In this
study, we show that dataset plays an integral part of face alignment
performance. Specifically, owing to face alignment dataset bias, training on
one database and testing on another or unseen domain would lead to poor
performance. Creating an unbiased dataset through combining various existing
databases, however, is non-trivial as one has to exhaustively re-label the
landmarks for standardisation. In this work, we propose a simple and yet
effective method to bridge the disparate annotation spaces between databases,
making datasets fusion possible. We show extensive results on combining various
popular databases (LFW, AFLW, LFPW, HELEN) for improved cross-dataset and
unseen data alignment."
"Hyperspectral unmixing (HU) plays a fundamental role in a wide range of
hyperspectral applications. It is still challenging due to the common presence
of outlier channels and the large solution space. To address the above two
issues, we propose a novel model by emphasizing both robust representation and
learning-based sparsity. Specifically, we apply the $\ell_{2,1}$-norm to
measure the representation error, preventing outlier channels from dominating
our objective. In this way, the side effects of outlier channels are greatly
relieved. Besides, we observe that the mixed level of each pixel varies over
image grids. Based on this observation, we exploit a learning-based sparsity
method to simultaneously learn the HU results and a sparse guidance map. Via
this guidance map, the sparsity constraint in the $\ell_{p}\!\left(\!0\!<\!
p\!\leq\!1\right)$-norm is adaptively imposed according to the learnt mixed
level of each pixel. Compared with state-of-the-art methods, our model is
better suited to the real situation, thus expected to achieve better HU
results. The resulted objective is highly non-convex and non-smooth, and so it
is hard to optimize. As a profound theoretical contribution, we propose an
efficient algorithm to solve it. Meanwhile, the convergence proof and the
computational complexity analysis are systematically provided. Extensive
evaluations verify that our method is highly promising for the HU task---it
achieves very accurate guidance maps and much better HU results compared with
state-of-the-art methods."
"In this paper, we describe a simple strategy for mitigating variability in
temporal data series by shifting focus onto long-term, frequency domain
features that are less susceptible to variability. We apply this method to the
human action recognition task and demonstrate how working in the frequency
domain can yield good recognition features for commonly used optical flow and
articulated pose features, which are highly sensitive to small differences in
motion, viewpoint, dynamic backgrounds, occlusion and other sources of
variability. We show how these frequency-based features can be used in
combination with a simple forest classifier to achieve good and robust results
on the popular KTH Actions dataset."
"Lip reading is used to understand or interpret speech without hearing it, a
technique especially mastered by people with hearing difficulties. The ability
to lip read enables a person with a hearing impairment to communicate with
others and to engage in social activities, which otherwise would be difficult.
Recent advances in the fields of computer vision, pattern recognition, and
signal processing has led to a growing interest in automating this challenging
task of lip reading. Indeed, automating the human ability to lip read, a
process referred to as visual speech recognition (VSR) (or sometimes speech
reading), could open the door for other novel related applications. VSR has
received a great deal of attention in the last decade for its potential use in
applications such as human-computer interaction (HCI), audio-visual speech
recognition (AVSR), speaker recognition, talking heads, sign language
recognition and video surveillance. Its main aim is to recognise spoken word(s)
by using only the visual signal that is produced during speech. Hence, VSR
deals with the visual domain of speech and involves image processing,
artificial intelligence, object detection, pattern recognition, statistical
modelling, etc."
"The emergence of new wearable technologies such as action cameras and
smart-glasses has increased the interest of computer vision scientists in the
First Person perspective. Nowadays, this field is attracting attention and
investments of companies aiming to develop commercial devices with First Person
Vision recording capabilities. Due to this interest, an increasing demand of
methods to process these videos, possibly in real-time, is expected. Current
approaches present a particular combinations of different image features and
quantitative methods to accomplish specific objectives like object detection,
activity recognition, user machine interaction and so on. This paper summarizes
the evolution of the state of the art in First Person Vision video analysis
between 1997 and 2014, highlighting, among others, most commonly used features,
methods, challenges and opportunities within the field."
"In this work we investigate the effect of the convolutional network depth on
its accuracy in the large-scale image recognition setting. Our main
contribution is a thorough evaluation of networks of increasing depth using an
architecture with very small (3x3) convolution filters, which shows that a
significant improvement on the prior-art configurations can be achieved by
pushing the depth to 16-19 weight layers. These findings were the basis of our
ImageNet Challenge 2014 submission, where our team secured the first and the
second places in the localisation and classification tracks respectively. We
also show that our representations generalise well to other datasets, where
they achieve state-of-the-art results. We have made our two best-performing
ConvNet models publicly available to facilitate further research on the use of
deep visual representations in computer vision."
"In this work, we propose a learning framework for identifying synapses using
a deep and wide multi-scale recursive (DAWMR) network, previously considered in
image segmentation applications. We apply this approach on electron microscopy
data from invertebrate fly brain tissue. By learning features directly from the
data, we are able to achieve considerable improvements over existing techniques
that rely on a small set of hand-designed features. We show that this system
can reduce the amount of manual annotation required, in both acquisition of
training data as well as verification of inferred detections."
"We present the development and evaluation of a hand tracking algorithm based
on single depth images captured from an overhead perspective for use in the
COACH prompting system. We train a random decision forest body part classifier
using approximately 5,000 manually labeled, unbalanced, partially labeled
training images. The classifier represents a random subset of pixels in each
depth image with a learned probability density function across all trained body
parts. A local mode-find approach is used to search for clusters present in the
underlying feature space sampled by the classified pixels. In each frame, body
part positions are chosen as the mode with the highest confidence. User hand
positions are translated into hand washing task actions based on proximity to
environmental objects. We validate the performance of the classifier and task
action proposals on a large set of approximately 24,000 manually labeled
images."
"We propose a new 2D shape decomposition method based on the short-cut rule.
The short-cut rule originates from cognition research, and states that the
human visual system prefers to partition an object into parts using the
shortest possible cuts. We propose and implement a computational model for the
short-cut rule and apply it to the problem of shape decomposition. The model we
proposed generates a set of cut hypotheses passing through the points on the
silhouette which represent the negative minima of curvature. We then show that
most part-cut hypotheses can be eliminated by analysis of local properties of
each. Finally, the remaining hypotheses are evaluated in ascending length
order, which guarantees that of any pair of conflicting cuts only the shortest
will be accepted. We demonstrate that, compared with state-of-the-art shape
decomposition methods, the proposed approach achieves decomposition results
which better correspond to human intuition as revealed in psychological
experiments."
"Gabor filters can extract multi-orientation and multiscale features from face
images. Researchers have designed different ways to use the magnitude of the
filtered results for face recognition: Gabor Fisher classifier exploited only
the magnitude information of Gabor magnitude pictures (GMPs); Local Gabor
Binary Pattern uses only the gradient information. In this paper, we regard
GMPs as smooth surfaces. By completely describing the shape of GMPs, we get a
face representation method called Gabor Surface Feature (GSF). First, we
compute the magnitude, 1st and 2nd derivatives of GMPs, then binarize them and
transform them into decimal values. Finally we construct joint histograms and
use subspace methods for classification. Experiments on FERET, ORL and FRGC
1.0.4 database show the effectiveness of GSF."
"Most computer vision application rely on algorithms finding local
correspondences between different images. These algorithms detect and compare
stable local invariant descriptors centered at scale-invariant keypoints.
Because of the importance of the problem, new keypoint detectors and
descriptors are constantly being proposed, each one claiming to perform better
(or to be complementary) to the preceding ones. This raises the question of a
fair comparison between very diverse methods. This evaluation has been mainly
based on a repeatability criterion of the keypoints under a series of image
perturbations (blur, illumination, noise, rotations, homotheties, homographies,
etc). In this paper, we argue that the classic repeatability criterion is
biased towards algorithms producing redundant overlapped detections. To
compensate this bias, we propose a variant of the repeatability rate taking
into account the descriptors overlap. We apply this variant to revisit the
popular benchmark by Mikolajczyk et al., on classic and new feature detectors.
Experimental evidence shows that the hierarchy of these feature detectors is
severely disrupted by the amended comparator."
"Detection of groups of interacting people is a very interesting and useful
task in many modern technologies, with application fields spanning from
video-surveillance to social robotics. In this paper we first furnish a
rigorous definition of group considering the background of the social sciences:
this allows us to specify many kinds of group, so far neglected in the Computer
Vision literature. On top of this taxonomy, we present a detailed state of the
art on the group detection algorithms. Then, as a main contribution, we present
a brand new method for the automatic detection of groups in still images, which
is based on a graph-cuts framework for clustering individuals; in particular we
are able to codify in a computational sense the sociological definition of
F-formation, that is very useful to encode a group having only proxemic
information: position and orientation of people. We call the proposed method
Graph-Cuts for F-formation (GCFF). We show how GCFF definitely outperforms all
the state of the art methods in terms of different accuracy measures (some of
them are brand new), demonstrating also a strong robustness to noise and
versatility in recognizing groups of various cardinality."
"This study formulates the IR target detection as a binary classification
problem of each pixel. Each pixel is associated with a label which indicates
whether it is a target or background pixel. The optimal label set for all the
pixels of an image maximizes aposteriori distribution of label configuration
given the pixel intensities. The posterior probability is factored into (or
proportional to) a conditional likelihood of the intensity values and a prior
probability of label configuration. Each of these two probabilities are
computed assuming a Markov Random Field (MRF) on both pixel intensities and
their labels. In particular, this study enforces neighborhood dependency on
both intensity values, by a Simultaneous Auto Regressive (SAR) model, and on
labels, by an Auto-Logistic model. The parameters of these MRF models are
learned from labeled examples. During testing, an MRF inference technique,
namely Iterated Conditional Mode (ICM), produces the optimal label for each
pixel. The detection performance is further improved by incorporating temporal
information through background subtraction. High performances on benchmark
datasets demonstrate effectiveness of this method for IR target detection."
"A quantum edge detector for image segmentation in optical environments is
presented in this work. A Boolean version of the same detector is presented
too. The quantum version of the new edge detector works with computational
basis states, exclusively. This way, we can easily avoid the problem of quantum
measurement retrieving the result of applying the new detector on the image.
Besides, a new criterion and logic based on projections onto vertical axis of
Bloch's Sphere exclusively are presented too. This approach will allow us: 1) a
simpler development of logic quantum operations, where they will closer to
those used in the classical logic operations, 2) building simple and robust
classical-to-quantum and quantum-to-classical interfaces. Said so far is
extended to quantum algorithms outside image processing too. In a special
section on metric and simulations, a new metric based on the comparison between
the classical and quantum versions algorithms for edge detection of images is
presented. Notable differences between the results of classical and quantum
versions of such algorithms (outside and inside of quantum computer,
respectively) show the existence of implementation problems involved in the
experiment, and that they have not been properly modeled for optical
environments. However, although they are different, the quantum results are
equally valid. The latter is clearly seen in the computer simulations"
"Template matching is a basic method in image analysis to extract useful
information from images. In this paper, we suggest a new method for pattern
matching. Our method transform the template image from two dimensional image
into one dimensional vector. Also all sub-windows (same size of template) in
the reference image will transform into one dimensional vectors. The three
similarity measures SAD, SSD, and Euclidean are used to compute the likeness
between template and all sub-windows in the reference image to find the best
match. The experimental results show the superior performance of the proposed
method over the conventional methods on various template of different sizes."
"In this paper, we propose multi-stage and deformable deep convolutional
neural networks for object detection. This new deep learning object detection
diagram has innovations in multiple aspects. In the proposed new deep
architecture, a new deformation constrained pooling (def-pooling) layer models
the deformation of object parts with geometric constraint and penalty. With the
proposed multi-stage training strategy, multiple classifiers are jointly
optimized to process samples at different difficulty levels. A new pre-training
strategy is proposed to learn feature representations more suitable for the
object detection task and with good generalization capability. By changing the
net structures, training strategies, adding and removing some key components in
the detection pipeline, a set of models with large diversity are obtained,
which significantly improves the effectiveness of modeling averaging. The
proposed approach ranked \#2 in ILSVRC 2014. It improves the mean averaged
precision obtained by RCNN, which is the state-of-the-art of object detection,
from $31\%$ to $45\%$. Detailed component-wise analysis is also provided
through extensive experimental evaluation."
"With the explosive growth of web-based cameras and mobile devices, billions
of photographs are uploaded to the internet. We can trivially collect a huge
number of photo streams for various goals, such as image clustering, 3D scene
reconstruction, and other big data applications. However, such tasks are not
easy due to the fact the retrieved photos can have large variations in their
view perspectives, resolutions, lighting, noises, and distortions.
Fur-thermore, with the occlusion of unexpected objects like people, vehicles,
it is even more challenging to find feature correspondences and reconstruct
re-alistic scenes. In this paper, we propose a structure-based image completion
algorithm for object removal that produces visually plausible content with
consistent structure and scene texture. We use an edge matching technique to
infer the potential structure of the unknown region. Driven by the estimated
structure, texture synthesis is performed automatically along the estimated
curves. We evaluate the proposed method on different types of images: from
highly structured indoor environment to natural scenes. Our experimental
results demonstrate satisfactory performance that can be potentially used for
subsequent big data processing, such as image localization, object retrieval,
and scene reconstruction. Our experiments show that this approach achieves
favorable results that outperform existing state-of-the-art techniques."
"In object tracking, outlier is one of primary factors which degrade
performance of image-based tracking algorithms. In this respect, therefore,
most of the existing methods simply discard detected outliers and pay little or
no attention to employing them as an important source of information for motion
estimation. We consider outliers as important as inliers for object tracking
and propose a motion estimation algorithm based on concurrent tracking of
inliers and outliers. Our tracker makes use of pyramidal implementation of the
Lucas-Kanade tracker to estimate motion flows of inliers and outliers and final
target motion is estimated robustly based on both of these information.
Experimental results from challenging benchmark video sequences confirm
enhanced tracking performance, showing highly stable target tracking under
severe occlusion compared with state-of-the-art algorithms. The proposed
algorithm runs at more than 100 frames per second even without using a hardware
accelerator, which makes the proposed method more practical and portable."
"This paper introduces self-taught object localization, a novel approach that
leverages deep convolutional networks trained for whole-image recognition to
localize objects in images without additional human supervision, i.e., without
using any ground-truth bounding boxes for training. The key idea is to analyze
the change in the recognition scores when artificially masking out different
regions of the image. The masking out of a region that includes the object
typically causes a significant drop in recognition score. This idea is embedded
into an agglomerative clustering technique that generates self-taught
localization hypotheses. Our object localization scheme outperforms existing
proposal methods in both precision and recall for small number of subwindow
proposals (e.g., on ILSVRC-2012 it produces a relative gain of 23.4% over the
state-of-the-art for top-1 hypothesis). Furthermore, our experiments show that
the annotations automatically-generated by our method can be used to train
object detectors yielding recognition results remarkably close to those
obtained by training on manually-annotated bounding boxes."
"Recently, mid-level features have shown promising performance in computer
vision. Mid-level features learned by incorporating class-level information are
potentially more discriminative than traditional low-level local features. In
this paper, an effective method is proposed to extract mid-level features from
Kinect skeletons for 3D human action recognition. Firstly, the orientations of
limbs connected by two skeleton joints are computed and each orientation is
encoded into one of the 27 states indicating the spatial relationship of the
joints. Secondly, limbs are combined into parts and the limb's states are
mapped into part states. Finally, frequent pattern mining is employed to mine
the most frequent and relevant (discriminative, representative and
non-redundant) states of parts in continuous several frames. These parts are
referred to as Frequent Local Parts or FLPs. The FLPs allow us to build
powerful bag-of-FLP-based action representation. This new representation yields
state-of-the-art results on MSR DailyActivity3D and MSR ActionPairs3D."
"A new approach for tuning the parameters of MultiScale Retinex (MSR) based
color image enhancement algorithm using a popular optimization method, namely,
Particle Swarm Optimization (PSO) is presented in this paper. The image
enhancement using MSR scheme heavily depends on parameters such as Gaussian
surround space constant, number of scales, gain and offset etc. Selection of
these parameters, empirically and its application to MSR scheme to produce
inevitable results are the major blemishes. The method presented here results
in huge savings of computation time as well as improvement in the visual
quality of an image, since the PSO exploited maximizes the MSR parameters. The
objective of PSO is to validate the visual quality of the enhanced image
iteratively using an effective objective criterion based on entropy and edge
information of an image. The PSO method of parameter optimization of MSR scheme
achieves a very good quality of reconstructed images, far better than that
possible with the other existing methods. Finally, the quality of the enhanced
color images obtained by the proposed method are evaluated using novel metric,
namely, Wavelet Energy (WE). The experimental results presented show that color
images enhanced using the proposed scheme are clearer, more vivid and
efficient."
"We introduce a method based on the deflectometry principle for the
reconstruction of specular objects exhibiting significant size and geometric
complexity. A key feature of our approach is the deployment of an Automatic
Virtual Environment (CAVE) as pattern generator. To unfold the full power of
this extraordinary experimental setup, an optical encoding scheme is developed
which accounts for the distinctive topology of the CAVE. Furthermore, we devise
an algorithm for detecting the object of interest in raw deflectometric images.
The segmented foreground is used for single-view reconstruction, the background
for estimation of the camera pose, necessary for calibrating the sensor system.
Experiments suggest a significant gain of coverage in single measurements
compared to previous methods. To facilitate research on specular surface
reconstruction, we will make our data set publicly available."
"We propose a general and versatile framework that significantly speeds-up
graphical model optimization while maintaining an excellent solution accuracy.
The proposed approach relies on a multi-scale pruning scheme that is able to
progressively reduce the solution space by use of a novel strategy based on a
coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with
classic computer vision related MRF problems, where our framework constantly
yields a significant time speed-up (with respect to the most efficient
inference methods) and obtains a more accurate solution than directly
optimizing the MRF."
"A proof of the optimality of the eigenfunctions of the Laplace-Beltrami
operator (LBO) in representing smooth functions on surfaces is provided and
adapted to the field of applied shape and data analysis. It is based on the
Courant-Fischer min-max principle adapted to our case. % The theorem we present
supports the new trend in geometry processing of treating geometric structures
by using their projection onto the leading eigenfunctions of the decomposition
of the LBO. Utilisation of this result can be used for constructing numerically
efficient algorithms to process shapes in their spectrum. We review a couple of
applications as possible practical usage cases of the proposed optimality
criteria. % We refer to a scale invariant metric, which is also invariant to
bending of the manifold. This novel pseudo-metric allows constructing an LBO by
which a scale invariant eigenspace on the surface is defined. We demonstrate
the efficiency of an intermediate metric, defined as an interpolation between
the scale invariant and the regular one, in representing geometric structures
while capturing both coarse and fine details. Next, we review a numerical
acceleration technique for classical scaling, a member of a family of
flattening methods known as multidimensional scaling (MDS). There, the
optimality is exploited to efficiently approximate all geodesic distances
between pairs of points on a given surface, and thereby match and compare
between almost isometric surfaces. Finally, we revisit the classical principal
component analysis (PCA) definition by coupling its variational form with a
Dirichlet energy on the data manifold. By pairing the PCA with the LBO we can
handle cases that go beyond the scope defined by the observation set that is
handled by regular PCA."
"We present a novel, real-time algorithm to track the trajectory of each
pedestrian in moderately dense crowded scenes. Our formulation is based on an
adaptive particle-filtering scheme that uses a combination of various
multi-agent heterogeneous pedestrian simulation models. We automatically
compute the optimal parameters for each of these different models based on
prior tracked data and use the best model as motion prior for our
particle-filter based tracking algorithm. We also use our ""mixture of motion
models"" for adaptive particle selection and accelerate the performance of the
online tracking algorithm. The motion model parameter estimation is formulated
as an optimization problem, and we use an approach that solves this
combinatorial optimization problem in a model independent manner and hence
scalable to any multi-agent pedestrian motion model. We evaluate the
performance of our approach on different crowd video datasets and highlight the
improvement in accuracy over homogeneous motion models and a baseline
mean-shift based tracker. In practice, our formulation can compute trajectories
of tens of pedestrians on a multi-core desktop CPU in in real time and offer
higher accuracy as compared to prior real time pedestrian tracking algorithms."
"Fractal analysis has been shown to be useful in image processing for
characterizing shape and gray-scale complexity. The fractal feature is a
compact descriptor used to give a numerical measure of the degree of
irregularity of the medical images. This descriptor property does not give
ownership of the local image structure. In this paper, we present a combination
of this parameter based on Box Counting with GLCM Features. This powerful
combination has proved good results especially in classification of medical
texture from MRI and CT Scan images of trabecular bone. This method has the
potential to improve clinical diagnostics tests for osteoporosis pathologies."
"We propose a deep convolutional neural network architecture codenamed
""Inception"", which was responsible for setting the new state of the art for
classification and detection in the ImageNet Large-Scale Visual Recognition
Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the
improved utilization of the computing resources inside the network. This was
achieved by a carefully crafted design that allows for increasing the depth and
width of the network while keeping the computational budget constant. To
optimize quality, the architectural decisions were based on the Hebbian
principle and the intuition of multi-scale processing. One particular
incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22
layers deep network, the quality of which is assessed in the context of
classification and detection."
"Not all tags are relevant to an image, and the number of relevant tags is
image-dependent. Although many methods have been proposed for image
auto-annotation, the question of how to determine the number of tags to be
selected per image remains open. The main challenge is that for a large tag
vocabulary, there is often a lack of ground truth data for acquiring optimal
cutoff thresholds per tag. In contrast to previous works that pre-specify the
number of tags to be selected, we propose in this paper adaptive tag selection.
The key insight is to divide the vocabulary into two disjoint subsets, namely a
seen set consisting of tags having ground truth available for optimizing their
thresholds and a novel set consisting of tags without any ground truth. Such a
division allows us to estimate how many tags shall be selected from the novel
set according to the tags that have been selected from the seen set. The
effectiveness of the proposed method is justified by our participation in the
ImageCLEF 2014 image annotation task. On a set of 2,065 test images with ground
truth available for 207 tags, the benchmark evaluation shows that compared to
the popular top-$k$ strategy which obtains an F-score of 0.122, adaptive tag
selection achieves a higher F-score of 0.223. Moreover, by treating the
underlying image annotation system as a black box, the new method can be used
as an easy plug-in to boost the performance of existing systems."
"Heterogeneous face recognition (HFR) refers to matching face imagery across
different domains. It has received much interest from the research community as
a result of its profound implications in law enforcement. A wide variety of new
invariant features, cross-modality matching models and heterogeneous datasets
being established in recent years. This survey provides a comprehensive review
of established techniques and recent developments in HFR. Moreover, we offer a
detailed account of datasets and benchmarks commonly used for evaluation. We
finish by assessing the state of the field and discussing promising directions
for future research."
"Fingerprint classification is an effective technique for reducing the
candidate numbers of fingerprints in the stage of matching in automatic
fingerprint identification system (AFIS). In recent years, deep learning is an
emerging technology which has achieved great success in many fields, such as
image processing, natural language processing and so on. In this paper, we only
choose the orientation field as the input feature and adopt a new method
(stacked sparse autoencoders) based on depth neural network for fingerprint
classification. For the four-class problem, we achieve a classification of 93.1
percent using the depth network structure which has three hidden layers (with
1.8% rejection) in the NIST-DB4 database. And then we propose a novel method
using two classification probabilities for fuzzy classification which can
effectively enhance the accuracy of classification. By only adjusting the
probability threshold, we get the accuracy of classification is 96.1% (setting
threshold is 0.85), 97.2% (setting threshold is 0.90) and 98.0% (setting
threshold is 0.95). Using the fuzzy method, we obtain higher accuracy than
other methods."
"In this paper, we present a deep regression approach for face alignment. The
deep architecture consists of a global layer and multi-stage local layers. We
apply the back-propagation algorithm with the dropout strategy to jointly
optimize the regression parameters. We show that the resulting deep regressor
gradually and evenly approaches the true facial landmarks stage by stage,
avoiding the tendency to yield over-strong early stage regressors while
over-weak later stage regressors. Experimental results show that our approach
achieves the state-of-the-art"
"In this paper, we introduce a new domain adaptation (DA) algorithm where the
source and target domains are represented by subspaces spanned by eigenvectors.
Our method seeks a domain invariant feature space by learning a mapping
function which aligns the source subspace with the target one. We show that the
solution of the corresponding optimization problem can be obtained in a simple
closed form, leading to an extremely fast algorithm. We present two approaches
to determine the only hyper-parameter in our method corresponding to the size
of the subspaces. In the first approach we tune the size of subspaces using a
theoretical bound on the stability of the obtained result. In the second
approach, we use maximum likelihood estimation to determine the subspace size,
which is particularly useful for high dimensional data. Apart from PCA, we
propose a subspace creation method that outperform partial least squares (PLS)
and linear discriminant analysis (LDA) in domain adaptation. We test our method
on various datasets and show that, despite its intrinsic simplicity, it
outperforms state of the art DA methods."
"The task of a visual landmark recognition system is to identify photographed
buildings or objects in query photos and to provide the user with relevant
information on them. With their increasing coverage of the world's landmark
buildings and objects, Internet photo collections are now being used as a
source for building such systems in a fully automatic fashion. This process
typically consists of three steps: clustering large amounts of images by the
objects they depict; determining object names from user-provided tags; and
building a robust, compact, and efficient recognition index. To this date,
however, there is little empirical information on how well current approaches
for those steps perform in a large-scale open-set mining and recognition task.
Furthermore, there is little empirical information on how recognition
performance varies for different types of landmark objects and where there is
still potential for improvement. With this paper, we intend to fill these gaps.
Using a dataset of 500k images from Paris, we analyze each component of the
landmark recognition pipeline in order to answer the following questions: How
many and what kinds of objects can be discovered automatically? How can we best
use the resulting image clusters to recognize the object in a query? How can
the object be efficiently represented in memory for recognition? How reliably
can semantic information be extracted? And finally: What are the limiting
factors in the resulting pipeline from query to semantics? We evaluate how
different choices of methods and parameters for the individual pipeline steps
affect overall system performance and examine their effects for different query
categories such as buildings, paintings or sculptures."
"Deformable part models (DPMs) and convolutional neural networks (CNNs) are
two widely used tools for visual recognition. They are typically viewed as
distinct approaches: DPMs are graphical models (Markov random fields), while
CNNs are ""black-box"" non-linear classifiers. In this paper, we show that a DPM
can be formulated as a CNN, thus providing a novel synthesis of the two ideas.
Our construction involves unrolling the DPM inference algorithm and mapping
each step to an equivalent (and at times novel) CNN layer. From this
perspective, it becomes natural to replace the standard image features used in
DPM with a learned feature extractor. We call the resulting model DeepPyramid
DPM and experimentally validate it on PASCAL VOC. DeepPyramid DPM significantly
outperforms DPMs based on histograms of oriented gradients features (HOG) and
slightly outperforms a comparable version of the recently introduced R-CNN
detection system, while running an order of magnitude faster."
"This paper presents a variational based approach to fusing hyperspectral and
multispectral images. The fusion process is formulated as an inverse problem
whose solution is the target image assumed to live in a much lower dimensional
subspace. A sparse regularization term is carefully designed, relying on a
decomposition of the scene on a set of dictionaries. The dictionary atoms and
the corresponding supports of active coding coefficients are learned from the
observed images. Then, conditionally on these dictionaries and supports, the
fusion problem is solved via alternating optimization with respect to the
target image (using the alternating direction method of multipliers) and the
coding coefficients. Simulation results demonstrate the efficiency of the
proposed algorithm when compared with the state-of-the-art fusion methods."
"Sparse representation, which uses dictionary atoms to reconstruct input
vectors, has been studied intensively in recent years. A proper dictionary is a
key for the success of sparse representation. In this paper, an active
dictionary learning (ADL) method is introduced, in which classification error
and reconstruction error are considered as the active learning criteria in
selection of the atoms for dictionary construction. The learned dictionaries
are caculated in sparse representation based classification (SRC). The
classification accuracy and reconstruction error are used to evaluate the
proposed dictionary learning method. The performance of the proposed dictionary
learning method is compared with other methods, including unsupervised
dictionary learning and whole-training-data dictionary. The experimental
results based on the UCI data sets and face data set demonstrate the efficiency
of the proposed method."
"Spatial Pyramid Matching (SPM) and its variants have achieved a lot of
success in image classification. The main difference among them is their
encoding schemes. For example, ScSPM incorporates Sparse Code (SC) instead of
Vector Quantization (VQ) into the framework of SPM. Although the methods
achieve a higher recognition rate than the traditional SPM, they consume more
time to encode the local descriptors extracted from the image. In this paper,
we propose using Low Rank Representation (LRR) to encode the descriptors under
the framework of SPM. Different from SC, LRR considers the group effect among
data points instead of sparsity. Benefiting from this property, the proposed
method (i.e., LrrSPM) can offer a better performance. To further improve the
generalizability and robustness, we reformulate the rank-minimization problem
as a truncated projection problem. Extensive experimental studies show that
LrrSPM is more efficient than its counterparts (e.g., ScSPM) while achieving
competitive recognition rates on nine image data sets."
"We consider apictorial edge-matching puzzles, in which the goal is to arrange
a collection of puzzle pieces with colored edges so that the colors match along
the edges of adjacent pieces. We devise an algebraic representation for this
problem and provide conditions under which it exactly characterizes a puzzle.
Using the new representation, we recast the combinatorial, discrete problem of
solving puzzles as a global, polynomial system of equations with continuous
variables. We further propose new algorithms for generating approximate
solutions to the continuous problem by solving a sequence of convex
relaxations."
"A video can be represented as a sequence of tracklets, each spanning 10-20
frames, and associated with one entity (eg. a person). The task of \emph{Entity
Discovery} in videos can be naturally posed as tracklet clustering. We approach
this task by leveraging \emph{Temporal Coherence}(TC): the fundamental property
of videos that each tracklet is likely to be associated with the same entity as
its temporal neighbors. Our major contributions are the first Bayesian
nonparametric models for TC at tracklet-level. We extend Chinese Restaurant
Process (CRP) to propose TC-CRP, and further to Temporally Coherent Chinese
Restaurant Franchise (TC-CRF) to jointly model short temporal segments. On the
task of discovering persons in TV serial videos without meta-data like scripts,
these methods show considerable improvement in cluster purity and person
coverage compared to state-of-the-art approaches to tracklet clustering. We
represent entities with mixture components, and tracklets with vectors of very
generic features, which can work for any type of entity (not necessarily
person). The proposed methods can perform online tracklet clustering on
streaming videos with little performance deterioration unlike existing
approaches, and can automatically reject tracklets resulting from false
detections. Finally we discuss entity-driven video summarization- where some
temporal segments of the video are selected automatically based on the
discovered entities."
"The Imagenet Large Scale Visual Recognition Challenge (ILSVRC) is the one of
the most important big data challenges to date. We participated in the object
detection track of ILSVRC 2014 and received the fourth place among the 38
teams. We introduce in our object detection system a number of novel techniques
in localization and recognition. For localization, initial candidate proposals
are generated using selective search, and a novel bounding boxes regression
method is used for better object localization. For recognition, to represent a
candidate proposal, we adopt three features, namely, RCNN feature, IFV feature,
and DPM feature. Given these features, category-specific combination functions
are learned to improve the object recognition rate. In addition, object context
in the form of background priors and object interaction priors are learned and
applied in our system. Our ILSVRC 2014 results are reported alongside with the
results of other participating teams."
"Although the human visual system is surprisingly robust to extreme distortion
when recognizing objects, most evaluations of computer object detection methods
focus only on robustness to natural form deformations such as people's pose
changes. To determine whether algorithms truly mirror the flexibility of human
vision, they must be compared against human vision at its limits. For example,
in Cubist abstract art, painted objects are distorted by object fragmentation
and part-reorganization, to the point that human vision often fails to
recognize them. In this paper, we evaluate existing object detection methods on
these abstract renditions of objects, comparing human annotators to four
state-of-the-art object detectors on a corpus of Picasso paintings. Our results
demonstrate that while human perception significantly outperforms current
methods, human perception and part-based models exhibit a similarly graceful
degradation in object detection performance as the objects become increasingly
abstract and fragmented, corroborating the theory of part-based object
representation in the brain."
"Lip reading is used to understand or interpret speech without hearing it, a
technique especially mastered by people with hearing difficulties. The ability
to lip read enables a person with a hearing impairment to communicate with
others and to engage in social activities, which otherwise would be difficult.
Recent advances in the fields of computer vision, pattern recognition, and
signal processing has led to a growing interest in automating this challenging
task of lip reading. Indeed, automating the human ability to lip read, a
process referred to as visual speech recognition, could open the door for other
novel applications. This thesis investigates various issues faced by an
automated lip-reading system and proposes a novel ""visual words"" based approach
to automatic lip reading. The proposed approach includes a novel automatic face
localisation scheme and a lip localisation method."
"This paper presents a computational model of concept learning using Bayesian
inference for a grammatically structured hypothesis space, and test the model
on multisensory (visual and haptics) recognition of 3D objects. The study is
performed on a set of artificially generated 3D objects known as fribbles,
which are complex, multipart objects with categorical structures. The goal of
this work is to develop a working multisensory representational model that
integrates major themes on concepts and concepts learning from the cognitive
science literature. The model combines the representational power of a
probabilistic generative grammar with the inferential power of Bayesian
induction."
"Existing techniques for 3D action recognition are sensitive to viewpoint
variations because they extract features from depth images which are viewpoint
dependent. In contrast, we directly process pointclouds for cross-view action
recognition from unknown and unseen views. We propose the Histogram of Oriented
Principal Components (HOPC) descriptor that is robust to noise, viewpoint,
scale and action speed variations. At a 3D point, HOPC is computed by
projecting the three scaled eigenvectors of the pointcloud within its local
spatio-temporal support volume onto the vertices of a regular dodecahedron.
HOPC is also used for the detection of Spatio-Temporal Keypoints (STK) in 3D
pointcloud sequences so that view-invariant STK descriptors (or Local HOPC
descriptors) at these key locations only are used for action recognition. We
also propose a global descriptor computed from the normalized spatio-temporal
distribution of STKs in 4-D, which we refer to as STK-D. We have evaluated the
performance of our proposed descriptors against nine existing techniques on two
cross-view and three single-view human action recognition datasets. The
Experimental results show that our techniques provide significant improvement
over state-of-the-art methods."
"This paper comprehensively reviews the recent development of image
deblurring, including non-blind/blind, spatially invariant/variant deblurring
techniques. Indeed, these techniques share the same objective of inferring a
latent sharp image from one or several corresponding blurry images, while the
blind deblurring techniques are also required to derive an accurate blur
kernel. Considering the critical role of image restoration in modern imaging
systems to provide high-quality images under complex environments such as
motion, undesirable lighting conditions, and imperfect system components, image
deblurring has attracted growing attention in recent years. From the viewpoint
of how to handle the ill-posedness which is a crucial issue in deblurring
tasks, existing methods can be grouped into five categories: Bayesian inference
framework, variational methods, sparse representation-based methods,
homography-based modeling, and region-based methods. In spite of achieving a
certain level of development, image deblurring, especially the blind case, is
limited in its success by complex application conditions which make the blur
kernel hard to obtain and be spatially variant. We provide a holistic
understanding and deep insight into image deblurring in this review. An
analysis of the empirical evidence for representative methods, practical
issues, as well as a discussion of promising future directions are also
presented."
"Deep Convolutional Neural Networks (CNNs) have gained great success in image
classification and object detection. In these fields, the outputs of all layers
of CNNs are usually considered as a high dimensional feature vector extracted
from an input image and the correspondence between finer level feature vectors
and concepts that the input image contains is all-important. However, fewer
studies focus on this deserving issue. On considering the correspondence, we
propose a novel approach which generates an edited version for each original
CNN feature vector by applying the maximum entropy principle to abandon
particular vectors. These selected vectors correspond to the unfriendly
concepts in each image category. The classifier trained from merged feature
sets can significantly improve model generalization of individual categories
when training data is limited. The experimental results for
classification-based object detection on canonical datasets including VOC 2007
(60.1%), 2010 (56.4%) and 2012 (56.3%) show obvious improvement in mean average
precision (mAP) with simple linear support vector machines."
"We study the problem of how to build a deep learning representation for 3D
shape. Deep learning has shown to be very effective in variety of visual
applications, such as image classification and object detection. However, it
has not been successfully applied to 3D shape recognition. This is because 3D
shape has complex structure in 3D space and there are limited number of 3D
shapes for feature learning. To address these problems, we project 3D shapes
into 2D space and use autoencoder for feature learning on the 2D images. High
accuracy 3D shape retrieval performance is obtained by aggregating the features
learned on 2D images. In addition, we show the proposed deep learning feature
is complementary to conventional local image descriptors. By combing the global
deep learning representation and the local descriptor representation, our
method can obtain the state-of-the-art performance on 3D shape retrieval
benchmarks."
"To simplify the parameter of the deep learning network, a cascaded
compressive sensing model ""CSNet"" is implemented for image classification.
Firstly, we use cascaded compressive sensing network to learn feature from the
data. Secondly, CSNet generates the feature by binary hashing and block-wise
histograms. Finally, a linear SVM classifier is used to classify these
features. The experiments on the MNIST dataset indicate that higher
classification accuracy can be obtained by this algorithm."
"In this paper, we propose a new deep learning network ""GENet"", it combines
the multi-layer network architec- ture and graph embedding framework. Firstly,
we use simplest unsupervised learning PCA/LDA as first layer to generate the
low- level feature. Secondly, many cascaded dimensionality reduction layers
based on graph embedding framework are applied to GENet. Finally, a linear SVM
classifier is used to classify dimension-reduced features. The experiments
indicate that higher classification accuracy can be obtained by this algorithm
on the CMU-PIE, ORL, Extended Yale B dataset."
"Object extraction from remote sensing images has long been an intensive
research topic in the field of surveying and mapping. Most existing methods are
devoted to handling just one type of object and little attention has been paid
to improving the computational efficiency. In recent years, level set evolution
(LSE) has been shown to be very promising for object extraction in the
community of image processing and computer vision because it can handle
topological changes automatically while achieving high accuracy. However, the
application of state-of-the-art LSEs is compromised by laborious parameter
tuning and expensive computation. In this paper, we proposed two fast LSEs for
man-made object extraction from high spatial resolution remote sensing images.
The traditional mean curvature-based regularization term is replaced by a
Gaussian kernel and it is mathematically sound to do that. Thus a larger time
step can be used in the numerical scheme to expedite the proposed LSEs. In
contrast to existing methods, the proposed LSEs are significantly faster. Most
importantly, they involve much fewer parameters while achieving better
performance. The advantages of the proposed LSEs over other state-of-the-art
approaches have been verified by a range of experiments."
"Would it be possible to automatically associate ancient pictures to modern
ones and create fancy cultural heritage city maps? We introduce here the task
of recognizing the location depicted in an old photo given modern annotated
images collected from the Internet. We present an extensive analysis on
different features, looking for the most discriminative and most robust to the
image variability induced by large time lags. Moreover, we show that the
described task benefits from domain adaptation."
"Multiple Object Tracking (MOT) is an important computer vision problem which
has gained increasing attention due to its academic and commercial potential.
Although different kinds of approaches have been proposed to tackle this
problem, it still remains challenging due to factors like abrupt appearance
changes and severe object occlusions. In this work, we contribute the first
comprehensive and most recent review on this problem. We inspect the recent
advances in various aspects and propose some interesting directions for future
research. To the best of our knowledge, there has not been any extensive review
on this topic in the community. We endeavor to provide a thorough review on the
development of this problem in recent decades. The main contributions of this
review are fourfold: 1) Key aspects in a multiple object tracking system,
including formulation, categorization, key principles, evaluation of an MOT are
discussed. 2) Instead of enumerating individual works, we discuss existing
approaches according to various aspects, in each of which methods are divided
into different groups and each group is discussed in detail for the principles,
advances and drawbacks. 3) We examine experiments of existing publications and
summarize results on popular datasets to provide quantitative comparisons. We
also point to some interesting discoveries by analyzing these results. 4) We
provide a discussion about issues of MOT research, as well as some interesting
directions which could possibly become potential research effort in the future."
"Biometric-based identification has drawn a lot of attention in the recent
years. Among all biometrics, palmprint is known to possess a rich set of
features. In this paper we have proposed to use DCT-based features in parallel
with wavelet-based ones for palmprint identification. PCA is applied to the
features to reduce their dimensionality and the majority voting algorithm is
used to perform classification. The features introduced here result in a
near-perfectly accurate identification. This method is tested on a well-known
multispectral palmprint database and an accuracy rate of 99.97-100\% is
achieved, outperforming all previous methods in similar conditions."
"Image denoising algorithms are evaluated using images corrupted by artificial
noise, which may lead to incorrect conclusions about their performances on real
noise. In this paper we introduce a dataset of color images corrupted by
natural noise due to low-light conditions, together with spatially and
intensity-aligned low noise images of the same scenes. We also introduce a
method for estimating the true noise level in our images, since even the low
noise images contain small amounts of noise. We evaluate the accuracy of our
noise estimation method on real and artificial noise, and investigate the
Poisson-Gaussian noise model. Finally, we use our dataset to evaluate six
denoising algorithms: Active Random Field, BM3D, Bilevel-MRF, Multi-Layer
Perceptron, and two versions of NL-means. We show that while the Multi-Layer
Perceptron, Bilevel-MRF, and NL-means with soft threshold outperform BM3D on
gray images with synthetic noise, they lag behind on our dataset."
"Image classification has advanced significantly in recent years with the
availability of large-scale image sets. However, fine-grained classification
remains a major challenge due to the annotation cost of large numbers of
fine-grained categories. This project shows that compelling classification
performance can be achieved on such categories even without labeled training
data. Given image and class embeddings, we learn a compatibility function such
that matching embeddings are assigned a higher score than mismatching ones;
zero-shot classification of an image proceeds by finding the label yielding the
highest joint compatibility score. We use state-of-the-art image features and
focus on different supervised attributes and unsupervised output embeddings
either derived from hierarchies or learned from unlabeled text corpora. We
establish a substantially improved state-of-the-art on the Animals with
Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate
that purely unsupervised output embeddings (learned from Wikipedia and improved
with fine-grained text) achieve compelling results, even outperforming the
previous supervised state-of-the-art. By combining different output embeddings,
we further improve results."
"I describe an approach to similarity motivated by Bayesian methods. This
yields a similarity function that is learnable using a standard Bayesian
methods. The relationship of the approach to variable kernel and variable
metric methods is discussed. The approach is related to variable kernel
Experimental results on character recognition and 3D object recognition are
presented.."
"Learning object models from views in 3D visual object recognition is usually
formulated either as a function approximation problem of a function describing
the view-manifold of an object, or as that of learning a class-conditional
density. This paper describes an alternative framework for learning in visual
object recognition, that of learning the view-generalization function. Using
the view-generalization function, an observer can perform Bayes-optimal 3D
object recognition given one or more 2D training views directly, without the
need for a separate model acquisition step. The paper shows that view
generalization functions can be computationally practical by restating two
widely-used methods, the eigenspace and linear combination of views approaches,
in a view generalization framework. The paper relates the approach to recent
methods for object recognition based on non-uniform blurring. The paper
presents results both on simulated 3D ``paperclip'' objects and real-world
images from the COIL-100 database showing that useful view-generalization
functions can be realistically be learned from a comparatively small number of
training examples."
"This paper proves that visual object recognition systems using only 2D
Euclidean similarity measurements to compare object views against previously
seen views can achieve the same recognition performance as observers having
access to all coordinate information and able of using arbitrary 3D models
internally. Furthermore, it demonstrates that such systems do not require more
training views than Bayes-optimal 3D model-based systems. For building computer
vision systems, these results imply that using view-based or appearance-based
techniques with carefully constructed combination of evidence mechanisms may
not be at a disadvantage relative to 3D model-based systems. For computational
approaches to human vision, they show that it is impossible to distinguish
view-based and 3D model-based techniques for 3D object recognition solely by
comparing the performance achievable by human and 3D model-based systems.}"
"Segmentation algorithms based on an energy minimisation framework often
depend on a scale parameter which balances a fit to data and a regularising
term. Irregular pyramids are defined as a stack of graphs successively reduced.
Within this framework, the scale is often defined implicitly as the height in
the pyramid. However, each level of an irregular pyramid can not usually be
readily associated to the global optimum of an energy or a global criterion on
the base level graph. This last drawback is addressed by the scale set
framework designed by Guigues. The methods designed by this author allow to
build a hierarchy and to design cuts within this hierarchy which globally
minimise an energy. This paper studies the influence of the construction scheme
of the initial hierarchy on the resulting optimal cuts. We propose one
sequential and one parallel method with two variations within both. Our
sequential methods provide partitions near the global optima while parallel
methods require less execution times than the sequential method of Guigues even
on sequential machines."
"The LULU operators for sequences are extended to multi-dimensional arrays via
the morphological concept of connection in a way which preserves their
essential properties, e.g. they are separators and form a four element fully
ordered semi-group. The power of the operators is demonstrated by deriving a
total variation preserving discrete pulse decomposition of images."
"This paper proposes a novel method for segmentation of images by hierarchical
multilevel thresholding. The method is global, agglomerative in nature and
disregards pixel locations. It involves the optimization of the ratio of the
unbiased estimators of within class to between class variances. We obtain a
recursive relation at each step for the variances which expedites the process.
The efficacy of the method is shown in a comparison with some well-known
methods."
"The compound models of clutter statistics are found suitable to describe the
nonstationary nature of radar backscattering from high-resolution observations.
In this letter, we show that the properties of Mellin transform can be utilized
to generate higher order moments of simple and compound models of clutter
statistics in a compact manner."
"In the paper, region based stereo matching algorithms are developed for
extraction depth information from two color stereo image pair. A filter
eliminating unreliable disparity estimation was used for increasing reliability
of the disparity map. Obtained results by algorithms were represented and
compared."
"In this paper, we propose a new method for impulse noise removal from images.
It uses the sparsity of images in the Discrete Cosine Transform (DCT) domain.
The zeros in this domain give us the exact mathematical equation to reconstruct
the pixels that are corrupted by random-value impulse noises. The proposed
method can also detect and correct the corrupted pixels. Moreover, in a simpler
case that salt and pepper noise is the brightest and darkest pixels in the
image, we propose a simpler version of our method. In addition to the proposed
method, we suggest a combination of the traditional median filter method with
our method to yield better results when the percentage of the corrupted samples
is high."
This paper has been withdrawn by the author ali pourmohammad.
"This paper advocates an improved solution for real-time error detection of
texture errors that occurs in the production process in textile industry. The
research is focused on the mono-color products with 3D texture model (Jaquard
fabrics). This is a more difficult task than, for example, 2D multicolor
textures."
"Image processing can be used for digital restoration of ancient papyri, that
is, for a restoration performed on their digital images. The digital
manipulation allows reducing the background signals and enhancing the
readability of texts. In the case of very old and damaged documents, this is
fundamental for identification of the patterns of letters. Some examples of
restoration, obtained with an image processing which uses edges detection and
Fourier filtering, are shown. One of them concerns 7Q5 fragment of the Dead Sea
Scrolls."
"There are many applications of graph cuts in computer vision, e.g.
segmentation. We present a novel method to reformulate the NP-hard, k-way graph
partitioning problem as an approximate minimal s-t graph cut problem, for which
a globally optimal solution is found in polynomial time. Each non-terminal
vertex in the original graph is replaced by a set of ceil(log_2(k)) new
vertices. The original graph edges are replaced by new edges connecting the new
vertices to each other and to only two, source s and sink t, terminal nodes.
The weights of the new edges are obtained using a novel least squares solution
approximating the constraints of the initial k-way setup. The minimal s-t cut
labels each new vertex with a binary (s vs t) ""Gray"" encoding, which is then
decoded into a decimal label number that assigns each of the original vertices
to one of k classes. We analyze the properties of the approximation and present
quantitative as well as qualitative segmentation results."
"We describe an algorithm to enhance and binarize a fingerprint image. The
algorithm is based on accurate determination of orientation flow of the ridges
of the fingerprint image by computing variance of the neighborhood pixels
around a pixel in different directions. We show that an iterative algorithm
which captures the mutual interdependence of orientation flow computation,
enhancement and binarization gives very good results on poor quality images."
"We consider models for which it is important, early in processing, to
estimate some variables with high precision, but perhaps at relatively low
rates of recall. If some variables can be identified with near certainty, then
they can be conditioned upon, allowing further inference to be done
efficiently. Specifically, we consider optical character recognition (OCR)
systems that can be bootstrapped by identifying a subset of correctly
translated document words with very high precision. This ""clean set"" is
subsequently used as document-specific training data. While many current OCR
systems produce measures of confidence for the identity of each letter or word,
thresholding these confidence values, even at very high values, still produces
some errors.
  We introduce a novel technique for identifying a set of correct words with
very high precision. Rather than estimating posterior probabilities, we bound
the probability that any given word is incorrect under very general
assumptions, using an approximate worst case analysis. As a result, the
parameters of the model are nearly irrelevant, and we are able to identify a
subset of words, even in noisy documents, of which we are highly confident. On
our set of 10 documents, we are able to identify about 6% of the words on
average without making a single error. This ability to produce word lists with
very high precision allows us to use a family of models which depends upon such
clean word lists."
"The ray-based 4D light field representation cannot be directly used to
analyze diffractive or phase--sensitive optical elements. In this paper, we
exploit tools from wave optics and extend the light field representation via a
novel ""light field transform"". We introduce a key modification to the
ray--based model to support the transform. We insert a ""virtual light source"",
with potentially negative valued radiance for certain emitted rays. We create a
look-up table of light field transformers of canonical optical elements. The
two key conclusions are that (i) in free space, the 4D light field completely
represents wavefront propagation via rays with real (positive as well as
negative) valued radiance and (ii) at occluders, a light field composed of
light field transformers plus insertion of (ray--based) virtual light sources
represents resultant phase and amplitude of wavefronts. For free--space
propagation, we analyze different wavefronts and coherence possibilities. For
occluders, we show that the light field transform is simply based on a
convolution followed by a multiplication operation. This formulation brings
powerful concepts from wave optics to computer vision and graphics. We show
applications in cubic-phase plate imaging and holographic displays."
"Medical image registration is a difficult problem. Not only a registration
algorithm needs to capture both large and small scale image deformations, it
also has to deal with global and local image intensity variations. In this
paper we describe a new multiresolution elastic image registration method that
challenges these difficulties in image registration. To capture large and small
scale image deformations, we use both global and local affine transformation
algorithms. To address global and local image intensity variations, we apply an
image intensity standardization algorithm to correct image intensity
variations. This transforms image intensities into a standard intensity scale,
which allows highly accurate registration of medical images."
"In this paper, we propose three novel and important methods for the
registration of histological images for 3D reconstruction. First, possible
intensity variations and nonstandardness in images are corrected by an
intensity standardization process which maps the image scale into a standard
scale where the similar intensities correspond to similar tissues meaning.
Second, 2D histological images are mapped into a feature space where continuous
variables are used as high confidence image features for accurate registration.
Third, we propose an automatic best reference slice selection algorithm that
improves reconstruction quality based on both image entropy and mean square
error of the registration process. We demonstrate that the choice of reference
slice has a significant impact on registration error, standardization, feature
space and entropy information. After 2D histological slices are registered
through an affine transformation with respect to an automatically chosen
reference, the 3D volume is reconstructed by co-registering 2D slices
elastically."
"In this paper, we propose a computational framework for 3D volume
reconstruction from 2D histological slices using registration algorithms in
feature space. To improve the quality of reconstructed 3D volume, first,
intensity variations in images are corrected by an intensity standardization
process which maps image intensity scale to a standard scale where similar
intensities correspond to similar tissues. Second, a subvolume approach is
proposed for 3D reconstruction by dividing standardized slices into groups.
Third, in order to improve the quality of the reconstruction process, an
automatic best reference slice selection algorithm is developed based on an
iterative assessment of image entropy and mean square error of the registration
process. Finally, we demonstrate that the choice of the reference slice has a
significant impact on registration quality and subsequent 3D reconstruction."
"In this paper, the problem of automatic Gabor wavelet selection for face
recognition is tackled by introducing an automatic algorithm based on Parallel
AdaBoosting method. Incorporating mutual information into the algorithm leads
to the selection procedure not only based on classification accuracy but also
on efficiency. Effective image features are selected by using properly chosen
Gabor wavelets optimised with Parallel AdaBoost method and mutual information
to get high recognition rates with low computational cost. Experiments are
conducted using the well-known FERET face database. In proposed framework,
memory and computation costs are reduced significantly and high classification
accuracy is obtained."
"We present BEAMER: a new spatially exploitative approach to learning object
detectors which shows excellent results when applied to the task of detecting
objects in greyscale aerial imagery in the presence of ambiguous and noisy
data. There are four main contributions used to produce these results. First,
we introduce a grammar-guided feature extraction system, enabling the
exploration of a richer feature space while constraining the features to a
useful subset. This is specified with a rule-based generative grammar crafted
by a human expert. Second, we learn a classifier on this data using a newly
proposed variant of AdaBoost which takes into account the spatially correlated
nature of the data. Third, we perform another round of training to optimize the
method of converting the pixel classifications generated by boosting into a
high quality set of (x, y) locations. Lastly, we carefully define three common
problems in object detection and define two evaluation criteria that are
tightly matched to these problems. Major strengths of this approach are: (1) a
way of randomly searching a broad feature space, (2) its performance when
evaluated on well-matched evaluation criteria, and (3) its use of the location
prediction domain to learn object detectors as well as to generate detections
that perform well on several tasks: object counting, tracking, and target
detection. We demonstrate the efficacy of BEAMER with a comprehensive
experimental evaluation on a challenging data set."
"We present in this paper a biometric system of face detection and recognition
in color images. The face detection technique is based on skin color
information and fuzzy classification. A new algorithm is proposed in order to
detect automatically face features (eyes, mouth and nose) and extract their
correspondent geometrical points. These fiducial points are described by sets
of wavelet components which are used for recognition. To achieve the face
recognition, we use neural networks and we study its performances for different
inputs. We compare the two types of features used for recognition: geometric
distances and Gabor coefficients which can be used either independently or
jointly. This comparison shows that Gabor coefficients are more powerful than
geometric distances. We show with experimental results how the importance
recognition ratio makes our system an effective tool for automatic face
detection and recognition."
"A robust classification method is developed on the basis of sparse subspace
decomposition. This method tries to decompose a mixture of subspaces of
unlabeled data (queries) into class subspaces as few as possible. Each query is
classified into the class whose subspace significantly contributes to the
decomposed subspace. Multiple queries from different classes can be
simultaneously classified into their respective classes. A practical greedy
algorithm of the sparse subspace decomposition is designed for the
classification. The present method achieves high recognition rate and robust
performance exploiting joint sparsity."
"We present a new modular traffic signs recognition system, successfully
applied to both American and European speed limit signs. Our sign detection
step is based only on shape-detection (rectangles or circles). This enables it
to work on grayscale images, contrary to most European competitors, which eases
robustness to illumination conditions (notably night operation). Speed sign
candidates are classified (or rejected) by segmenting potential digits inside
them (which is rather original and has several advantages), and then applying a
neural digit recognition. The global detection rate is ~90% for both (standard)
U.S. and E.U. speed signs, with a misclassification rate <1%, and no validated
false alarm in >150 minutes of video. The system processes in real-time ~20
frames/s on a standard high-end laptop."
"Radiofrequency (RF) catheter ablation has transformed treatment for
tachyarrhythmias and has become first-line therapy for some tachycardias. The
precise localization of the arrhythmogenic site and the positioning of the RF
catheter over that site are problematic: they can impair the efficiency of the
procedure and are time consuming (several hours). Electroanatomic mapping
technologies are available that enable the display of the cardiac chambers and
the relative position of ablation lesions. However, these are expensive and use
custom-made catheters. The proposed methodology makes use of standard catheters
and inexpensive technology in order to create a 3D volume of the heart chamber
affected by the arrhythmia. Further, we propose a novel method that uses a
priori 3D information of the mapping catheter in order to estimate the 3D
locations of multiple electrodes across single view C-arm images. The monoplane
algorithm is tested for feasibility on computer simulations and initial canine
data."
"With the advancement in image capturing device, the image data been generated
at high volume. If images are analyzed properly, they can reveal useful
information to the human users. Content based image retrieval address the
problem of retrieving images relevant to the user needs from image databases on
the basis of low-level visual features that can be derived from the images.
Grouping images into meaningful categories to reveal useful information is a
challenging and important problem. Clustering is a data mining technique to
group a set of unsupervised data based on the conceptual clustering principal:
maximizing the intraclass similarity and minimizing the interclass similarity.
Proposed framework focuses on color as feature. Color Moment and Block
Truncation Coding (BTC) are used to extract features for image dataset.
Experimental study using K-Means clustering algorithm is conducted to group the
image dataset into various clusters."
"There are many resources useful for processing images, most of them freely
available and quite friendly to use. In spite of this abundance of tools, a
study of the processing methods is still worthy of efforts. Here, we want to
discuss the possibilities arising from the use of fractional differential
calculus. This calculus evolved in the research field of pure mathematics until
1920, when applied science started to use it. Only recently, fractional
calculus was involved in image processing methods. As we shall see, the
fractional calculation is able to enhance the quality of images, with
interesting possibilities in edge detection and image restoration. We suggest
also the fractional differentiation as a tool to reveal faint objects in
astronomical images."
"Background subtraction has been a driving engine for many computer vision and
video analytics tasks. Although its many variants exist, they all share the
underlying assumption that photometric scene properties are either static or
exhibit temporal stationarity. While this works in some applications, the model
fails when one is interested in discovering {\it changes in scene dynamics}
rather than those in a static background; detection of unusual pedestrian and
motor traffic patterns is but one example. We propose a new model and
computational framework that address this failure by considering stationary
scene dynamics as a ``background'' with which observed scene dynamics are
compared. Central to our approach is the concept of an {\it event}, that we
define as short-term scene dynamics captured over a time window at a specific
spatial location in the camera field of view. We compute events by
time-aggregating motion labels, obtained by background subtraction, as well as
object descriptors (e.g., object size). Subsequently, we characterize events
probabilistically, but use a low-memory, low-complexity surrogates in practical
implementation. Using these surrogates amounts to {\it behavior subtraction}, a
new algorithm with some surprising properties. As demonstrated here, behavior
subtraction is an effective tool in anomaly detection and localization. It is
resilient to spurious background motion, such as one due to camera jitter, and
is content-blind, i.e., it works equally well on humans, cars, animals, and
other objects in both uncluttered and highly-cluttered scenes. Clearly,
treating video as a collection of events rather than colored pixels opens new
possibilities for video analytics."
"A $p$-adic variation of the Ran(dom) Sa(mple) C(onsensus) method for solving
the relative pose problem in stereo vision is developped. From two 2-adically
encoded images a random sample of five pairs of corresponding points is taken,
and the equations for the essential matrix are solved by lifting solutions
modulo 2 to the 2-adic integers. A recently devised $p$-adic hierarchical
classification algorithm imitating the known LBG quantisation method classifies
the solutions for all the samples after having determined the number of
clusters using the known intra-inter validity of clusterings. In the successful
case, a cluster ranking will determine the cluster containing a 2-adic
approximation to the ""true"" solution of the problem."
"The problem of restoration of digital images from their degraded measurements
plays a central role in a multitude of practically important applications. A
particularly challenging instance of this problem occurs in the case when the
degradation phenomenon is modeled by an ill-conditioned operator. In such a
case, the presence of noise makes it impossible to recover a valuable
approximation of the image of interest without using some a priori information
about its properties. Such a priori information is essential for image
restoration, rendering it stable and robust to noise. Particularly, if the
original image is known to be a piecewise smooth function, one of the standard
priors used in this case is defined by the Rudin-Osher-Fatemi model, which
results in total variation (TV) based image restoration. The current arsenal of
algorithms for TV-based image restoration is vast. In the present paper, a
different approach to the solution of the problem is proposed based on the
method of iterative shrinkage (aka iterated thresholding). In the proposed
method, the TV-based image restoration is performed through a recursive
application of two simple procedures, viz. linear filtering and soft
thresholding. Therefore, the method can be identified as belonging to the group
of first-order algorithms which are efficient in dealing with images of
relatively large sizes. Another valuable feature of the proposed method
consists in its working directly with the TV functional, rather then with its
smoothed versions. Moreover, the method provides a single solution for both
isotropic and anisotropic definitions of the TV functional, thereby
establishing a useful connection between the two formulae."
"Vector quantization(VQ) is a lossy data compression technique from signal
processing for which simple competitive learning is one standard method to
quantize patterns from the input space. Extending competitive learning VQ to
the domain of graphs results in competitive learning for quantizing input
graphs. In this contribution, we propose an accelerated version of competitive
learning graph quantization (GQ) without trading computational time against
solution quality. For this, we lift graphs locally to vectors in order to avoid
unnecessary calculations of intractable graph distances. In doing so, the
accelerated version of competitive learning GQ gradually turns locally into a
competitive learning VQ with increasing number of iterations. Empirical results
show a significant speedup by maintaining a comparable solution quality."
"The k-nearest neighbors (k-NN) classification rule has proven extremely
successful in countless many computer vision applications. For example, image
categorization often relies on uniform voting among the nearest prototypes in
the space of descriptors. In spite of its good properties, the classic k-NN
rule suffers from high variance when dealing with sparse prototype datasets in
high dimensions. A few techniques have been proposed to improve k-NN
classification, which rely on either deforming the nearest neighborhood
relationship or modifying the input space. In this paper, we propose a novel
boosting algorithm, called UNN (Universal Nearest Neighbors), which induces
leveraged k-NN, thus generalizing the classic k-NN rule. We redefine the voting
rule as a strong classifier that linearly combines predictions from the k
closest prototypes. Weak classifiers are learned by UNN so as to minimize a
surrogate risk. A major feature of UNN is the ability to learn which prototypes
are the most relevant for a given class, thus allowing one for effective data
reduction. Experimental results on the synthetic two-class dataset of Ripley
show that such a filtering strategy is able to reject ""noisy"" prototypes. We
carried out image categorization experiments on a database containing eight
classes of natural scenes. We show that our method outperforms significantly
the classic k-NN classification, while enabling significant reduction of the
computational cost by means of data filtering."
"The need of sign language is increasing radically especially to hearing
impaired community. Only few research groups try to automatically recognize
sign language from video, colored gloves and etc. Their approach requires a
valid segmentation of the data that is used for training and of the data that
is used to be recognized. Recognition of a sign language image sequence is
challenging because of the variety of hand shapes and hand motions. Here, this
paper proposes to apply a combination of image segmentation with restoration
using topological derivatives for achieving high recognition accuracy. Image
quality measures are conceded here to differentiate the methods both
subjectively as well as objectively. Experiments show that the additional use
of the restoration before segmenting the postures significantly improves the
correct rate of hand detection, and that the discrete derivatives yields a high
rate of discrimination between different static hand postures as well as
between hand postures and the scene background. Eventually, the research is to
contribute to the implementation of automated sign language recognition system
mainly established for the welfare purpose."
"As the Internet help us cross cultural border by providing different
information, plagiarism issue is bound to arise. As a result, plagiarism
detection becomes more demanding in overcoming this issue. Different plagiarism
detection tools have been developed based on various detection techniques.
Nowadays, fingerprint matching technique plays an important role in those
detection tools. However, in handling some large content articles, there are
some weaknesses in fingerprint matching technique especially in space and time
consumption issue. In this paper, we propose a new approach to detect
plagiarism which integrates the use of fingerprint matching technique with four
key features to assist in the detection process. These proposed features are
capable to choose the main point or key sentence in the articles to be
compared. Those selected sentence will be undergo the fingerprint matching
process in order to detect the similarity between the sentences. Hence, time
and space usage for the comparison process is reduced without affecting the
effectiveness of the plagiarism detection."
"Biometrics has become a ""hot"" area. Governments are funding research programs
focused on biometrics. In this paper the problem of person recognition and
verification based on a different biometric application has been addressed. The
system is based on the 3DSkull recognition using 3D matching technique, in fact
this paper present several bio-metric approaches in order of assign the weak
point in term of used the biometric from the authorize person and insure the
person who access the data is the real person. The feature of the simulate
system shows the capability of using 3D matching system as an efficient way to
identify the person through his or her skull by match it with database, this
technique grantee fast processing with optimizing the false positive and
negative as well ."
"The main focus of image mining in the proposed method is concerned with the
classification of brain tumor in the CT scan brain images. The major steps
involved in the system are: pre-processing, feature extraction, association
rule mining and hybrid classifier. The pre-processing step has been done using
the median filtering process and edge features have been extracted using canny
edge detection technique. The two image mining approaches with a hybrid manner
have been proposed in this paper. The frequent patterns from the CT scan images
are generated by frequent pattern tree (FP-Tree) algorithm that mines the
association rules. The decision tree method has been used to classify the
medical images for diagnosis. This system enhances the classification process
to be more accurate. The hybrid method improves the efficiency of the proposed
method than the traditional image mining methods. The experimental result on
prediagnosed database of brain images showed 97% sensitivity and 95% accuracy
respectively. The physicians can make use of this accurate decision tree
classification phase for classifying the brain images into normal, benign and
malignant for effective medical diagnosis."
"Segmentation of medical images using seeded region growing technique is
increasingly becoming a popular method because of its ability to involve
high-level knowledge of anatomical structures in seed selection process. Region
based segmentation of medical images are widely used in varied clinical
applications like visualization, bone detection, tumor detection and
unsupervised image retrieval in clinical databases. As medical images are
mostly fuzzy in nature, segmenting regions based intensity is the most
challenging task. In this paper, we discuss about popular seeded region grow
methodology used for segmenting anatomical structures in CT Angiography images.
We have proposed a gradient based homogeneity criteria to control the region
grow process while segmenting CTA images."
"Segmenting a MRI images into homogeneous texture regions representing
disparate tissue types is often a useful preprocessing step in the
computer-assisted detection of breast cancer. That is why we proposed new
algorithm to detect cancer in mammogram breast cancer images. In this paper we
proposed segmentation using vector quantization technique. Here we used Linde
Buzo-Gray algorithm (LBG) for segmentation of MRI images. Initially a codebook
of size 128 was generated for MRI images. These code vectors were further
clustered in 8 clusters using same LBG algorithm. These 8 images were displayed
as a result. This approach does not leads to over segmentation or under
segmentation. For the comparison purpose we displayed results of watershed
segmentation and Entropy using Gray Level Co-occurrence Matrix along with this
method."
"Automated tracking of animal movement allows analyses that would not
otherwise be possible by providing great quantities of data. The additional
capability of tracking in realtime - with minimal latency - opens up the
experimental possibility of manipulating sensory feedback, thus allowing
detailed explorations of the neural basis for control of behavior. Here we
describe a new system capable of tracking the position and body orientation of
animals such as flies and birds. The system operates with less than 40 msec
latency and can track multiple animals simultaneously. To achieve these
results, a multi target tracking algorithm was developed based on the Extended
Kalman Filter and the Nearest Neighbor Standard Filter data association
algorithm. In one implementation, an eleven camera system is capable of
tracking three flies simultaneously at 60 frames per second using a gigabit
network of nine standard Intel Pentium 4 and Core 2 Duo computers. This
manuscript presents the rationale and details of the algorithms employed and
shows three implementations of the system. An experiment was performed using
the tracking system to measure the effect of visual contrast on the flight
speed of Drosophila melanogaster. At low contrasts, speed is more variable and
faster on average than at high contrasts. Thus, the system is already a useful
tool to study the neurobiology and behavior of freely flying animals. If
combined with other techniques, such as `virtual reality'-type computer
graphics or genetic manipulation, the tracking system would offer a powerful
new way to investigate the biology of flying animals."
"Intensive research has been done on optical character recognition ocr and a
large number of articles have been published on this topic during the last few
decades. Many commercial OCR systems are now available in the market, but most
of these systems work for Roman, Chinese, Japanese and Arabic characters. There
are no sufficient number of works on Indian language character recognition
especially Kannada script among 12 major scripts in India. This paper presents
a review of existing work on printed Kannada script and their results. The
characteristics of Kannada script and Kannada Character Recognition System kcr
are discussed in detail. Finally fusion at the classifier level is proposed to
increase the recognition accuracy."
"One of the important evidence in a crime scene that is normally overlooked
but very important evidence is shoe print as the criminal is normally unaware
of the mask for this. In this paper we use image processing technique to
process reference shoe images to make it index-able for a search from the
database the shoe print impressions available in the commercial market. This is
achieved first by converting the commercially available image through the
process of converting them to gray scale then apply image enhancement and
restoration techniques and finally do image segmentation to store the segmented
parameter as index in the database storage. We use histogram method for image
enhancement, inverse filtering for image restoration and threshold method for
indexing. We use global threshold as index of the shoe print. The paper
describes this method and simulation results are included to validate the
method."
"In this paper an accurate real-time sequence-based system for representation,
recognition, interpretation, and analysis of the facial action units (AUs) and
expressions is presented. Our system has the following characteristics: 1)
employing adaptive-network-based fuzzy inference systems (ANFIS) and temporal
information, we developed a classification scheme based on neuro-fuzzy modeling
of the AU intensity, which is robust to intensity variations, 2) using both
geometric and appearance-based features, and applying efficient dimension
reduction techniques, our system is robust to illumination changes and it can
represent the subtle changes as well as temporal information involved in
formation of the facial expressions, and 3) by continuous values of intensity
and employing top-down hierarchical rule-based classifiers, we can develop
accurate human-interpretable AU-to-expression converters. Extensive experiments
on Cohn-Kanade database show the superiority of the proposed method, in
comparison with support vector machines, hidden Markov models, and neural
network classifiers. Keywords: biased discriminant analysis (BDA), classifier
design and evaluation, facial action units (AUs), hybrid learning, neuro-fuzzy
modeling."
"Restoration of digital images from their degraded measurements has always
been a problem of great theoretical and practical importance in numerous
applications of imaging sciences. A specific solution to the problem of image
restoration is generally determined by the nature of degradation phenomenon as
well as by the statistical properties of measurement noises. The present study
is concerned with the case in which the images of interest are corrupted by
convolutional blurs and Poisson noises. To deal with such problems, there
exists a range of solution methods which are based on the principles
originating from the fixed-point algorithm of Richardson and Lucy (RL). In this
paper, we provide conceptual and experimental proof that such methods tend to
converge to sparse solutions, which makes them applicable only to those images
which can be represented by a relatively small number of non-zero samples in
the spatial domain. Unfortunately, the set of such images is relatively small,
which restricts the applicability of RL-type methods. On the other hand,
virtually all practical images admit sparse representations in the domain of a
properly designed linear transform. To take advantage of this fact, it is
therefore tempting to modify the RL algorithm so as to make it recover
representation coefficients, rather than the values of their associated image.
Such modification is introduced in this paper. Apart from the generality of its
assumptions, the proposed method is also superior to many established
reconstruction approaches in terms of estimation accuracy and computational
complexity. This and other conclusions of this study are validated through a
series of numerical experiments."
"This paper present a novel off-line signature recognition method based on
multi scale Fourier Descriptor and wavelet transform . The main steps of
constructing a signature recognition system are discussed and experiments on
real data sets show that the average error rate can reach 1%. Finally we
compare 8 distance measures between feature vectors with respect to the
recognition performance.
  Key words: signature recognition; Fourier Descriptor; Wavelet transform;
personal verification"
"Medical image segmentation demands an efficient and robust segmentation
algorithm against noise. The conventional fuzzy c-means algorithm is an
efficient clustering algorithm that is used in medical image segmentation. But
FCM is highly vulnerable to noise since it uses only intensity values for
clustering the images. This paper aims to develop a novel and efficient fuzzy
spatial c-means clustering algorithm which is robust to noise. The proposed
clustering algorithm uses fuzzy spatial information to calculate membership
value. The input image is clustered using proposed ISFCM algorithm. A
comparative study has been made between the conventional FCM and proposed
ISFCM. The proposed approach is found to be outperforming the conventional FCM."
"Image segmentation is a vital part of image processing. Segmentation has its
application widespread in the field of medical images in order to diagnose
curious diseases. The same medical images can be segmented manually. But the
accuracy of image segmentation using the segmentation algorithms is more when
compared with the manual segmentation. In the field of medical diagnosis an
extensive diversity of imaging techniques is presently available, such as
radiography, computed tomography (CT) and magnetic resonance imaging (MRI).
Medical image segmentation is an essential step for most consequent image
analysis tasks. Although the original FCM algorithm yields good results for
segmenting noise free images, it fails to segment images corrupted by noise,
outliers and other imaging artifact. This paper presents an image segmentation
approach using Modified Fuzzy C-Means (FCM) algorithm and Fuzzy Possibilistic
c-means algorithm (FPCM). This approach is a generalized version of standard
Fuzzy CMeans Clustering (FCM) algorithm. The limitation of the conventional FCM
technique is eliminated in modifying the standard technique. The Modified FCM
algorithm is formulated by modifying the distance measurement of the standard
FCM algorithm to permit the labeling of a pixel to be influenced by other
pixels and to restrain the noise effect during segmentation. Instead of having
one term in the objective function, a second term is included, forcing the
membership to be as high as possible without a maximum limit constraint of one.
Experiments are conducted on real images to investigate the performance of the
proposed modified FCM technique in segmenting the medical images. Standard FCM,
Modified FCM, Fuzzy Possibilistic CMeans algorithm (FPCM) are compared to
explore the accuracy of our proposed approach."
"This paper presents a feature level fusion approach which uses the improved
K-medoids clustering algorithm and isomorphic graph for face and palmprint
biometrics. Partitioning around medoids (PAM) algorithm is used to partition
the set of n invariant feature points of the face and palmprint images into k
clusters. By partitioning the face and palmprint images with scale invariant
features SIFT points, a number of clusters is formed on both the images. Then
on each cluster, an isomorphic graph is drawn. In the next step, the most
probable pair of graphs is searched using iterative relaxation algorithm from
all possible isomorphic graphs for a pair of corresponding face and palmprint
images. Finally, graphs are fused by pairing the isomorphic graphs into
augmented groups in terms of addition of invariant SIFT points and in terms of
combining pair of keypoint descriptors by concatenation rule. Experimental
results obtained from the extensive evaluation show that the proposed feature
level fusion with the improved K-medoids partitioning algorithm increases the
performance of the system with utmost level of accuracy."
"This paper presents a robust and dynamic face recognition technique based on
the extraction and matching of devised probabilistic graphs drawn on SIFT
features related to independent face areas. The face matching strategy is based
on matching individual salient facial graph characterized by SIFT features as
connected to facial landmarks such as the eyes and the mouth. In order to
reduce the face matching errors, the Dempster-Shafer decision theory is applied
to fuse the individual matching scores obtained from each pair of salient
facial features. The proposed algorithm is evaluated with the ORL and the IITK
face databases. The experimental results demonstrate the effectiveness and
potential of the proposed face recognition technique also in case of partially
occluded faces."
"Handwriting Recognition enables a person to scribble something on a piece of
paper and then convert it into text. If we look into the practical reality
there are enumerable styles in which a character may be written. These styles
can be self combined to generate more styles. Even if a small child knows the
basic styles a character can be written, he would be able to recognize
characters written in styles intermediate between them or formed by their
mixture. This motivates the use of Genetic Algorithms for the problem. In order
to prove this, we made a pool of images of characters. We converted them to
graphs. The graph of every character was intermixed to generate styles
intermediate between the styles of parent character. Character recognition
involved the matching of the graph generated from the unknown character image
with the graphs generated by mixing. Using this method we received an accuracy
of 98.44%."
"In Image Compression, the researchers' aim is to reduce the number of bits
required to represent an image by removing the spatial and spectral
redundancies. Recently discrete wavelet transform and wavelet packet has
emerged as popular techniques for image compression. The wavelet transform is
one of the major processing components of image compression. The result of the
compression changes as per the basis and tap of the wavelet used. It is
proposed that proper selection of mother wavelet on the basis of nature of
images, improve the quality as well as compression ratio remarkably. We suggest
the novel technique, which is based on wavelet packet best tree based on
Threshold Entropy with enhanced run-length encoding. This method reduces the
time complexity of wavelet packets decomposition as complete tree is not
decomposed. Our algorithm selects the sub-bands, which include significant
information based on threshold entropy. The enhanced run length encoding
technique is suggested provides better results than RLE. The result when
compared with JPEG-2000 proves to be better."
"A new approach for signature region of interest pre-processing was presented.
It used new auto cropping preparation on the basis of the image content, where
the intensity value of pixel is the source of cropping. This approach provides
both the possibility of improving the performance of security systems based on
signature images, and also the ability to use only the region of interest of
the used image to suit layout design of biometric systems. Underlying the
approach is a novel segmentation method which identifies the exact region of
foreground of signature for feature extraction usage. Evaluation results of
this approach shows encouraging prospects by eliminating the need for false
region isolating, reduces the time cost associated with signature false points
detection, and addresses enhancement issues. A further contribution of this
paper is an automated cropping stage in bio-secure based systems."
"We numerically investigate a mean-field Bayesian approach with the assistance
of the Markov chain Monte Carlo method to estimate motion velocity fields and
probabilistic models simultaneously in consecutive digital images described by
spatio-temporal Markov random fields. Preliminary to construction of our
procedure, we find that mean-field variables in the iteration diverge due to
improper normalization factor of regularization terms appearing in the
posterior. To avoid this difficulty, we rescale the regularization term by
introducing a scaling factor and optimizing it by means of minimization of the
mean-square error. We confirm that the optimal scaling factor stabilizes the
mean-field iterative process of the motion velocity estimation. We next attempt
to estimate the optimal values of hyper-parameters including the regularization
term, which define our probabilistic model macroscopically, by using the
Boltzmann-machine type learning algorithm based on gradient descent of marginal
likelihood (type-II likelihood) with respect to the hyper-parameters. In our
framework, one can estimate both the probabilistic model (hyper-parameters) and
motion velocity fields simultaneously. We find that our motion estimation is
much better than the result obtained by Zhang and Hanouer (1995) in which the
hyper-parameters are set to some ad-hoc values without any theoretical
justification."
"In this paper we present a Bayesian image zooming/super-resolution algorithm
based on a patch based representation. We work on a patch based model with
overlap and employ a Locally Linear Embedding (LLE) based approach as our data
fidelity term in the Bayesian inference. The image prior imposes continuity
constraints across the overlapping patches. We apply an error back-projection
technique, with an approximate cross bilateral filter. The problem of nearest
neighbor search is handled by a variant of the locality sensitive hashing (LSH)
scheme. The novelty of our work lies in the speed up achieved by the hashing
scheme and the robustness and inherent modularity and parallel structure
achieved by the LLE setup. The ill-posedness of the image reconstruction
problem is handled by the introduction of regularization priors which encode
the knowledge present in vast collections of natural images. We present
comparative results for both run-time as well as visual image quality based
measurements."
"We propose a direct reconstruction algorithm for Computed Tomography, based
on a local fusion of a few preliminary image estimates by means of a non-linear
fusion rule. One such rule is based on a signal denoising technique which is
spatially adaptive to the unknown local smoothness. Another, more powerful
fusion rule, is based on a neural network trained off-line with a high-quality
training set of images. Two types of linear reconstruction algorithms for the
preliminary images are employed for two different reconstruction tasks. For an
entire image reconstruction from full projection data, the proposed scheme uses
a sequence of Filtered Back-Projection algorithms with a gradually growing
cut-off frequency. To recover a Region Of Interest only from local projections,
statistically-trained linear reconstruction algorithms are employed. Numerical
experiments display the improvement in reconstruction quality when compared to
linear reconstruction algorithms."
"This paper attempts to undertake the study of Restored Gaussian Blurred
Images. by using four types of techniques of deblurring image as Wiener filter,
Regularized filter, Lucy Richardson deconvlutin algorithm and Blind
deconvlution algorithm with an information of the Point Spread Function (PSF)
corrupted blurred image with Different values of Size and Alfa and then
corrupted by Gaussian noise. The same is applied to the remote sensing image
and they are compared with one another, So as to choose the base technique for
restored or deblurring image.This paper also attempts to undertake the study of
restored Gaussian blurred image with no any information about the Point Spread
Function (PSF) by using same four techniques after execute the guess of the
PSF, the number of iterations and the weight threshold of it. To choose the
base guesses for restored or deblurring image of this techniques."
"Capacity, Robustness, & Perceptual quality of watermark data are very
important issues to be considered. A lot of research is going on to increase
these parameters for watermarking of the digital images, as there is always a
tradeoff among them. . In this paper an efficient watermarking algorithm to
improve payload and robustness without affecting perceptual quality of image
data based on DWT is discussed. The aim of the paper is to employ the nested
watermarks in wavelet domain which increases the capacity and ultimately the
robustness against attacks and selection of different scaling factor values for
LL & HH bands and during embedding not to create the visible artifacts in the
original image and therefore the original and watermarked image is similar."
"A logical approach to object recognition on image is proposed. The main idea
of the approach is to perform the object recognition as a logical inference on
a set of rules describing an object shape."
"We present a method for segmenting an arbitrary number of moving objects in
image sequences using the geometry of 6 points in 2D to infer motion
consistency. The method has been evaluated on the Hopkins 155 database and
surpasses current state-of-the-art methods such as SSC, both in terms of
overall performance on two and three motions but also in terms of maximum
errors. The method works by finding initial clusters in the spatial domain, and
then classifying each remaining point as belonging to the cluster that
minimizes a motion consistency score. In contrast to most other motion
segmentation methods that are based on an affine camera model, the proposed
method is fully projective."
"We revisit the problem of model-based object recognition for intensity images
and attempt to address some of the shortcomings of existing Bayesian methods,
such as unsuitable priors and the treatment of residuals with a non-robust
error norm. We do so by using a refor- mulation of the Huber metric and
carefully chosen prior distributions. Our proposed method is invariant to
2-dimensional affine transforma- tions and, because it is relatively easy to
train and use, it is suited for general object matching problems."
"The $\ell_1$ tracker obtains robustness by seeking a sparse representation of
the tracking object via $\ell_1$ norm minimization \cite{Xue_ICCV_09_Track}.
However, the high computational complexity involved in the $ \ell_1 $ tracker
restricts its further applications in real time processing scenario. Hence we
propose a Real Time Compressed Sensing Tracking (RTCST) by exploiting the
signal recovery power of Compressed Sensing (CS). Dimensionality reduction and
a customized Orthogonal Matching Pursuit (OMP) algorithm are adopted to
accelerate the CS tracking. As a result, our algorithm achieves a real-time
speed that is up to $6,000$ times faster than that of the $\ell_1$ tracker.
Meanwhile, RTCST still produces competitive (sometimes even superior) tracking
accuracy comparing to the existing $\ell_1$ tracker. Furthermore, for a
stationary camera, a further refined tracker is designed by integrating a
CS-based background model (CSBM). This CSBM-equipped tracker coined as RTCST-B,
outperforms most state-of-the-arts with respect to both accuracy and
robustness. Finally, our experimental results on various video sequences, which
are verified by a new metric---Tracking Success Probability (TSP), show the
excellence of the proposed algorithms."
"In this paper, we show how to efficiently and effectively extract a class of
""low-rank textures"" in a 3D scene from 2D images despite significant
corruptions and warping. The low-rank textures capture geometrically meaningful
structures in an image, which encompass conventional local features such as
edges and corners as well as all kinds of regular, symmetric patterns
ubiquitous in urban environments and man-made objects. Our approach to finding
these low-rank textures leverages the recent breakthroughs in convex
optimization that enable robust recovery of a high-dimensional low-rank matrix
despite gross sparse errors. In the case of planar regions with significant
affine or projective deformation, our method can accurately recover both the
intrinsic low-rank texture and the precise domain transformation, and hence the
3D geometry and appearance of the planar regions. Extensive experimental
results demonstrate that this new technique works effectively for many regular
and near-regular patterns or objects that are approximately low-rank, such as
symmetrical patterns, building facades, printed texts, and human faces."
"This chapter presents a framework for detecting fake regions by using various
methods including watermarking technique and blind approaches. In particular,
we describe current categories on blind approaches which can be divided into
five: pixel-based techniques, format-based techniques, camera-based techniques,
physically-based techniques and geometric-based techniques. Then we take a
second look on the geometric-based techniques and further categorize them in
detail. In the following section, the state-of-the-art methods involved in the
geometric technique are elaborated."
"Maximally stable component detection is a very popular method for feature
analysis in images, mainly due to its low computation cost and high
repeatability. With the recent advance of feature-based methods in geometric
shape analysis, there is significant interest in finding analogous approaches
in the 3D world. In this paper, we formulate a diffusion-geometric framework
for stable component detection in non-rigid 3D shapes, which can be used for
geometric feature detection and description. A quantitative evaluation of our
method on the SHREC'10 feature detection benchmark shows its potential as a
source of high-quality features."
"We introduce an (equi-)affine invariant diffusion geometry by which surfaces
that go through squeeze and shear transformations can still be properly
analyzed. The definition of an affine invariant metric enables us to construct
an invariant Laplacian from which local and global geometric structures are
extracted. Applications of the proposed framework demonstrate its power in
generalizing and enriching the existing set of tools for shape analysis."
"Natural objects can be subject to various transformations yet still preserve
properties that we refer to as invariants. Here, we use definitions of affine
invariant arclength for surfaces in R^3 in order to extend the set of existing
non-rigid shape analysis tools. In fact, we show that by re-defining the
surface metric as its equi-affine version, the surface with its modified metric
tensor can be treated as a canonical Euclidean object on which most classical
Euclidean processing and analysis tools can be applied. The new definition of a
metric is used to extend the fast marching method technique for computing
geodesic distances on surfaces, where now, the distances are defined with
respect to an affine invariant arclength. Applications of the proposed
framework demonstrate its invariance, efficiency, and accuracy in shape
analysis."
"In this paper we present a novel approach for automatic recognition of ring
worm skin disease based on LBP (Local Binary Pattern) feature extracted from
the affected skin images. The proposed method is evaluated by extensive
experiments on the skin images collected from internet. The dataset is tested
using three different classifiers i.e. Bayesian, MLP and SVM. Experimental
results show that the proposed methodology efficiently discriminates between a
ring worm skin and a normal skin. It is a low cost technique and does not
require any special imaging devices."
"Diffusion Tensor Imaging (DTI) allows estimating the position, orientation
and dimension of bundles of nerve pathways. This non-invasive imaging technique
takes advantage of the diffusion of water molecules and determines the
diffusion coefficients for every voxel of the data set. The identification of
the diffusion coefficients and the derivation of information about fiber
bundles is of major interest for planning and performing neurosurgical
interventions. To minimize the risk of neural deficits during brain surgery as
tumor resection (e.g. glioma), the segmentation and integration of the results
in the operating room is of prime importance. In this contribution, a robust
and efficient graph-based approach for segmentating tubular fiber bundles in
the human brain is presented. To define a cost function, the fractional
anisotropy (FA) is used, derived from the DTI data, but this value may differ
from patient to patient. Besides manually definining seed regions describing
the structure of interest, additionally a manual definition of the cost
function by the user is necessary. To improve the approach the contribution
introduces a solution for automatically determining the cost function by using
different 3D masks for each individual data set."
"This short article presents a class of projection-based solution algorithms
to the problem considered in the pioneering work on compressed sensing -
perfect reconstruction of a phantom image from 22 radial lines in the frequency
domain. Under the framework of projection-based image reconstruction, we will
show experimentally that several old and new tools of nonlinear filtering
(including Perona-Malik diffusion, nonlinear diffusion, Translation-Invariant
thresholding and SA-DCT thresholding) all lead to perfect reconstruction of the
phantom image."
"Diffusion Tensor Imaging (DTI) provides the possibility of estimating the
location and course of eloquent structures in the human brain. Knowledge about
this is of high importance for preoperative planning of neurosurgical
interventions and for intraoperative guidance by neuronavigation in order to
minimize postoperative neurological deficits. Therefore, the segmentation of
these structures as closed, three-dimensional object is necessary. In this
contribution, two methods for fiber bundle segmentation between two defined
regions are compared using software phantoms (abstract model and anatomical
phantom modeling the right corticospinal tract). One method uses evaluation
points from sampled rays as candidates for boundary points, the other method
sets up a directed and weighted (depending on a scalar measure) graph and
performs a min-cut for optimal segmentation results. Comparison is done by
using the Dice Similarity Coefficient (DSC), a measure for spatial overlap of
different segmentation results."
"In this paper, a new method for handwritten signature identification based on
rotated complex wavelet filters is proposed. We have proposed to use the
rotated complex wavelet filters (RCWF) and dual tree complex wavelet
transform(DTCWT) together to derive signature feature extraction, which
captures information in twelve different directions. In identification phase,
Canberra distance measure is used. The proposed method is compared with
discrete wavelet transform (DWT). From experimental results it is found that
signature identification rate of proposed method is superior over DWT"
"In the 21st century, Aerial and satellite images are information rich. They
are also complex to analyze. For GIS systems, many features require fast and
reliable extraction of open space area from high resolution satellite imagery.
In this paper we will study efficient and reliable automatic extraction
algorithm to find out the open space area from the high resolution urban
satellite imagery. This automatic extraction algorithm uses some filters and
segmentations and grouping is applying on satellite images. And the result
images may use to calculate the total available open space area and the built
up area. It may also use to compare the difference between present and past
open space area using historical urban satellite images of that same projection"
"In this paper, we study efficient and reliable automatic extraction algorithm
to find out the open space area from the high resolution urban satellite
imagery, and to detect changes from the extracted open space area during the
period 2003, 2006 and 2008. This automatic extraction and change detection
algorithm uses some filters, segmentation and grouping that are applied on
satellite images. The resultant images may be used to calculate the total
available open space area and the built up area. It may also be used to compare
the difference between present and past open space area using historical urban
satellite images of that same projection, which is an important geo spatial
data management application."
"Historical documents such as old books and manuscripts have a high aesthetic
value and highly appreciated. Unfortunately, there are some documents cannot be
read due to quality problems like faded paper, ink expand, uneven colour tone,
torn paper and other elements disruption such as the existence of small spots.
The study aims to produce a copy of manuscript that shows clear wordings so
they can easily be read and the copy can also be displayed for visitors. 16
samples of Jawi historical manuscript with different quality problems were
obtained from The Royal Museum of Pahang, Malaysia. We applied three
binarization techniques; Otsu's method represents global threshold technique;
Sauvola and Niblack method which are categorized as local threshold techniques.
We compared the binarized images with the original manuscript to be visually
inspected by the museum's curator. The unclear features were marked and
analyzed. Most of the examined images show that with optimal parameters and
effective pre processing technique, local thresholding methods are work well
compare with the other one. Niblack's and Sauvola's techniques seem to be the
suitable approaches for these types of images. Most of binarized images with
these two methods show improvement for readability and character recognition.
For this research, even the differences of image result were hard to be
distinguished by human capabilities, after comparing the time cost and overall
achievement rate of recognized symbols, Niblack's method is performing better
than Sauvola's. We could improve the post processing step by adding edge
detection techniques and further enhanced by an innovative image refinement
technique and a formulation of a class proper method."
"Discontinuity preserving smoothing is a fundamentally important procedure
that is useful in a wide variety of image processing contexts. It is directly
useful for noise reduction, and frequently used as an intermediate step in
higher level algorithms. For example, it can be particularly useful in edge
detection and segmentation. Three well known algorithms for discontinuity
preserving smoothing are nonlinear anisotropic diffusion, bilateral filtering,
and mean shift filtering. Although slight differences make them each better
suited to different tasks, all are designed to preserve discontinuities while
smoothing. However, none of them satisfy this goal perfectly: they each have
exception cases in which smoothing may occur across hard edges. The principal
contribution of this paper is the identification of a property we call edge
awareness that should be satisfied by any discontinuity preserving smoothing
algorithm. This constraint can be incorporated into existing algorithms to
improve quality, and usually has negligible changes in runtime performance
and/or complexity. We present modifications necessary to augment diffusion and
mean shift, as well as a new formulation of the bilateral filter that unifies
the spatial and range spaces to achieve edge awareness."
"The fundamental matrix and trifocal tensor are convenient algebraic
representations of the epipolar geometry of two and three view configurations,
respectively. The estimation of these entities is central to most
reconstruction algorithms, and a solid understanding of their properties and
constraints is therefore very important. The fundamental matrix has 1 internal
constraint which is well understood, whereas the trifocal tensor has 8
independent algebraic constraints. The internal tensor constraints can be
represented in many ways, although there is only one minimal and sufficient set
of 8 constraints known. In this paper, we derive a second set of minimal and
sufficient constraints that is simpler. We also show how this can be used in a
new parameterization of the trifocal tensor. We hope that this increased
understanding of the internal constraints may lead to improved algorithms for
estimating the trifocal tensor, although the primary contribution is an
improved theoretical understanding."
"Preoperative templating in Total Hip Replacement (THR) is a method to
estimate the optimal size and position of the implant. Today, observational
(manual) size recognition techniques are still used to find a suitable implant
for the patient. Therefore, a digital and automated technique should be
developed so that the implant size recognition process can be effectively
implemented. For this purpose, we have introduced the new technique for
acetabular implant size recognition in THR preoperative planning based on the
diameter of acetabulum size. This technique enables the surgeon to recognise a
digital acetabular implant size automatically. Ten randomly selected X-rays of
unidentified patients were used to test the accuracy and utility of an
automated implant size recognition technique. Based on the testing result, the
new technique yielded very close results to those obtained by the observational
method in nine studies (90%)."
"This paper presents some experiments regarding applications development on
high performance media processors included in Philips Nexperia Family. The
PNX1302 dedicated DVB-T kit used has some limitations. Our work has succeeded
to overcome these limitations and to make possible a general-purpose use of
this kit. For exemplification two typical applications, important both for
multimedia and DVB, are analyzed: MPEG2 video stream decoding and MP3 audio
decoding. These original implementations are compared (in speed, memory
requirements and costs) with Philips Nexperia Library."
"Person re-identification consists in recognizing an individual that has
already been observed over a network of cameras. It is a novel and challenging
research topic in computer vision, for which no reference framework exists yet.
Despite this, previous works share similar representations of human body based
on part decomposition and the implicit concept of multiple instances. Building
on these similarities, we propose a Multiple Component Matching (MCM) framework
for the person re-identification problem, which is inspired by Multiple
Component Learning, a framework recently proposed for object detection. We show
that previous techniques for person re-identification can be considered
particular implementations of our MCM framework. We then present a novel person
re-identification technique as a direct, simple implementation of our
framework, focused in particular on robustness to varying lighting conditions,
and show that it can attain state of the art performances."
"In this paper, we investigate the use of 3D surface geometry for face
recognition and compare it to one based on color map information. The 3D
surface and color map data are from the CAESAR anthropometric database. We find
that the recognition performance is not very different between 3D surface and
color map information using a principal component analysis algorithm. We also
discuss the different techniques for the combination of the 3D surface and
color map information for multi-modal recognition by using different fusion
approaches and show that there is significant improvement in results. The
effectiveness of various techniques is compared and evaluated on a dataset with
200 subjects in two different positions."
"Structural pattern recognition describes and classifies data based on the
relationships of features and parts. Topological invariants, like the Euler
number, characterize the structure of objects of any dimension. Cohomology can
provide more refined algebraic invariants to a topological space than does
homology. It assigns `quantities' to the chains used in homology to
characterize holes of any dimension. Graph pyramids can be used to describe
subdivisions of the same object at multiple levels of detail. This paper
presents cohomology in the context of structural pattern recognition and
introduces an algorithm to efficiently compute representative cocycles (the
basic elements of cohomology) in 2D using a graph pyramid. An extension to
obtain scanning and rotation invariant cocycles is given."
"We give a new algorithmic solution to the well-known five-point relative pose
problem. Our approach does not deal with the famous cubic constraint on an
essential matrix. Instead, we use the Cayley representation of rotations in
order to obtain a polynomial system from epipolar constraints. Solving that
system, we directly get relative rotation and translation parameters of the
cameras in terms of roots of a 10th degree polynomial."
"This article describes JECT-OMR, a system that analyzes digital images
representing scans of multiple-choice tests compiled by students. The system
performs a structural analysis of the document in order to get the chosen
answer for each question, and it also contains a bar-code decoder, used for the
identification of additional information encoded in the document. JECT-OMR was
implemented using the Python programming language, and leverages the power of
the Gamera framework in order to accomplish its task. The system exhibits an
accuracy of over 99% in the recognition of marked and non-marked squares
representing answers, thus making it suitable for real world applications"
"Cohomology and cohomology ring of three-dimensional (3D) objects are
topological invariants that characterize holes and their relations. Cohomology
ring has been traditionally computed on simplicial complexes. Nevertheless,
cubical complexes deal directly with the voxels in 3D images, no additional
triangulation is necessary, facilitating efficient algorithms for the
computation of topological invariants in the image context. In this paper, we
present formulas to directly compute the cohomology ring of 3D cubical
complexes without making use of any additional triangulation. Starting from a
cubical complex $Q$ that represents a 3D binary-valued digital picture whose
foreground has one connected component, we compute first the cohomological
information on the boundary of the object, $\partial Q$ by an incremental
technique; then, using a face reduction algorithm, we compute it on the whole
object; finally, applying the mentioned formulas, the cohomology ring is
computed from such information."
"Uterine Cervical Cancer is one of the most common forms of cancer in women
worldwide. Most cases of cervical cancer can be prevented through screening
programs aimed at detecting precancerous lesions. During Digital Colposcopy,
colposcopic images or cervigrams are acquired in raw form. They contain
specular reflections which appear as bright spots heavily saturated with white
light and occur due to the presence of moisture on the uneven cervix surface
and. The cervix region occupies about half of the raw cervigram image. Other
parts of the image contain irrelevant information, such as equipment, frames,
text and non-cervix tissues. This irrelevant information can confuse automatic
identification of the tissues within the cervix. Therefore we focus on the
cervical borders, so that we have a geometric boundary on the relevant image
area. Our novel technique eliminates the SR, identifies the region of interest
and makes the cervigram ready for segmentation algorithms."
"We propose a method for computing the cohomology ring of three--dimensional
(3D) digital binary-valued pictures. We obtain the cohomology ring of a 3D
digital binary--valued picture $I$, via a simplicial complex K(I)topologically
representing (up to isomorphisms of pictures) the picture I. The usefulness of
a simplicial description of the ""digital"" cohomology ring of 3D digital
binary-valued pictures is tested by means of a small program visualizing the
different steps of the method. Some examples concerning topological thinning,
the visualization of representative (co)cycles of (co)homology generators and
the computation of the cup product on the cohomology of simple pictures are
showed."
"In this paper, we formalize the notion of lambda-AT-model (where $\lambda$ is
a non-null integer) for a given chain complex, which allows the computation of
homological information in the integer domain avoiding using the Smith Normal
Form of the boundary matrices. We present an algorithm for computing such a
model, obtaining Betti numbers, the prime numbers p involved in the invariant
factors of the torsion subgroup of homology, the amount of invariant factors
that are a power of p and a set of representative cycles of generators of
homology mod p, for each p. Moreover, we establish the minimum valid lambda for
such a construction, what cuts down the computational costs related to the
torsion subgroup. The tools described here are useful to determine topological
information of nD structured objects such as simplicial, cubical or simploidal
complexes and are applicable to extract such an information from digital
pictures."
"Image splicing is a common form of image forgery. Such alterations may leave
no visual clues of tampering. In recent works camera characteristics
consistency across the image has been used to establish the authenticity and
integrity of digital images. Such constant camera characteristic properties are
inherent from camera manufacturing processes and are unique. The majority of
digital cameras are equipped with spherical lens and this introduces radial
distortions on images. This aberration is often disturbed and fails to be
consistent across the image, when an image is spliced. This paper describes the
detection of splicing operation on images by estimating radial distortion from
different portions of the image using line-based calibration. For the first
time, the detection of image splicing through the verification of consistency
of lens radial distortion has been explored in this paper. The conducted
experiments demonstrate the efficacy of our proposed approach for the detection
of image splicing on both synthetic and real images."
"It is argued that for the computer to be able to interact with humans, it
needs to have the communication skills of humans. One of these skills is the
ability to understand the emotional state of the person. This thesis describes
a neural network-based approach for emotion classification. We learn a
classifier that can recognize six basic emotions with an average accuracy of
77% over the Cohn-Kanade database. The novelty of this work is that instead of
empirically selecting the parameters of the neural network, i.e. the learning
rate, activation function parameter, momentum number, the number of nodes in
one layer, etc. we developed a strategy that can automatically select
comparatively better combination of these parameters. We also introduce another
way to perform back propagation. Instead of using the partial differential of
the error function, we use optimal algorithm; namely Powell's direction set to
minimize the error function. We were also interested in construction an
authentic emotion databases. This is a very important task because nowadays
there is no such database available. Finally, we perform several experiments
and show that our neural network approach can be successfully used for emotion
recognition."
"This work discusses preliminary work aimed at simulating and visualizing the
growth process of a tiny structure inside the cell---the microtubule.
Difficulty of recording the process lies in the fact that the tissue
preparation method for electronic microscopes is highly destructive to live
cells. Here in this paper, our approach is to take pictures of microtubules at
different time slots and then appropriately combine these images into a
coherent video. Experimental results are given on real data."
"Random hypothesis sampling lies at the core of many popular robust fitting
techniques such as RANSAC. In this paper, we propose a novel hypothesis
sampling scheme based on incremental computation of distances between partial
rankings (top-$k$ lists) derived from residual sorting information. Our method
simultaneously (1) guides the sampling such that hypotheses corresponding to
all true structures can be quickly retrieved and (2) filters the hypotheses
such that only a small but very promising subset remain. This permits the usage
of simple agglomerative clustering on the surviving hypotheses for accurate
model selection. The outcome is a highly efficient multi-structure robust
estimation technique. Experiments on synthetic and real data show the superior
performance of our approach over previous methods."
"Using a vehicle toy (in next future called vehicle) as a moving object an
automatic road lighting system (ARLS) model is constructed. A digital video
camera with 25 fps is used to capture the vehicle motion as it moves in the
test segment of the road. Captured images are then processed to calculate
vehicle speed. This information of the speed together with position of vehicle
is then used to control the lighting system along the path that passes by the
vehicle. Length of the road test segment is 1 m, the video camera is positioned
about 1.1 m above the test segment, and the vehicle toy dimension is 13 cm
\times 9.3 cm. In this model, the maximum speed that ARLS can handle is about
1.32 m/s, and the highest performance is obtained about 91% at speed 0.93 m/s."
"We propose a traffic congestion estimation system based on unsupervised
on-line learning algorithm. The system does not rely on background extraction
or motion detection. It extracts local features inside detection regions of
variable size which are drawn on lanes in advance. The extracted features are
then clustered into two classes using K-means and Gaussian Mixture Models(GMM).
A Bayes classifier is used to detect vehicles according to the previous cluster
information which keeps updated whenever system is running by on-line EM
algorithm. Experimental result shows that our system can be adapted to various
traffic scenes for estimating traffic status."
"This paper presents multi-font/multi-size Kannada numerals and vowels
recognition based on spatial features. Directional spatial features viz stroke
density, stroke length and the number of stokes in an image are employed as
potential features to characterize the printed Kannada numerals and vowels.
Based on these features 1100 numerals and 1400 vowels are classified with
Multi-class Support Vector Machines (SVM). The proposed system achieves the
recognition accuracy as 98.45% and 90.64% for numerals and vowels respectively."
"We analyze and improve low rank representation (LRR), the state-of-the-art
algorithm for subspace segmentation of data. We prove that for the noiseless
case, the optimization model of LRR has a unique solution, which is the shape
interaction matrix (SIM) of the data matrix. So in essence LRR is equivalent to
factorization methods. We also prove that the minimum value of the optimization
model of LRR is equal to the rank of the data matrix. For the noisy case, we
show that LRR can be approximated as a factorization method that combines noise
removal by column sparse robust PCA. We further propose an improved version of
LRR, called Robust Shape Interaction (RSI), which uses the corrected data as
the dictionary instead of the noisy data. RSI is more robust than LRR when the
corruption in data is heavy. Experiments on both synthetic and real data
testify to the improved robustness of RSI."
"This paper reviews Kunchenko's polynomials using as template matching method
to recognize template in one-dimensional input signal. Kunchenko's polynomials
method is compared with classical methods - cross-correlation and sum of
squared differences according to numerical statistical example."
"The box counting method for fractal dimension estimation had not been applied
to large or colour images thus far due to the processing time required. In this
letter we present a fast, easy to implement and very easily expandable to any
number of dimensions variation, the box merging method. It is applied here in
RGB images which are considered as sets in 5-D space."
"This study poses the feature correspondence problem as a hypergraph node
labeling problem. Candidate feature matches and their subsets (usually of size
larger than two) are considered to be the nodes and hyperedges of a hypergraph.
A hypergraph labeling algorithm, which models the subset-wise interaction by an
undirected graphical model, is applied to label the nodes (feature
correspondences) as correct or incorrect. We describe a method to learn the
cost function of this labeling algorithm from labeled examples using a
graphical model training algorithm. The proposed feature matching algorithm is
different from the most of the existing learning point matching methods in
terms of the form of the objective function, the cost function to be learned
and the optimization method applied to minimize it. The results on standard
datasets demonstrate how learning over a hypergraph improves the matching
performance over existing algorithms, notably one that also uses higher order
information without learning."
"This paper shows that the k-means quantization of a signal can be interpreted
both as a crisp indicator function and as a fuzzy membership assignment
describing fuzzy clusters and fuzzy boundaries. Combined crisp and fuzzy
indicator functions are defined here as natural generalizations of the ordinary
crisp and fuzzy indicator functions, respectively. An application to iris
segmentation is presented together with a demo program."
"A new approach in iris recognition based on Circular Fuzzy Iris Segmentation
(CFIS) and Gabor Analytic Iris Texture Binary Encoder (GAITBE) is proposed and
tested here. CFIS procedure is designed to guarantee that similar iris segments
will be obtained for similar eye images, despite the fact that the degree of
occlusion may vary from one image to another. Its result is a circular iris
ring (concentric with the pupil) which approximates the actual iris. GAITBE
proves better encoding of statistical independence between the iris codes
extracted from different irides using Hilbert Transform. Irides from University
of Bath Iris Database are binary encoded on two different lengths (768 / 192
bytes) and tested in both single-enrollment and multi-enrollment identification
scenarios. All cases illustrate the capacity of the newly proposed methodology
to narrow down the distribution of inter-class matching scores, and
consequently, to guarantee a steeper descent of the False Accept Rate."
"Feature selection and extraction plays an important role in different
classification based problems such as face recognition, signature verification,
optical character recognition (OCR) etc. The performance of OCR highly depends
on the proper selection and extraction of feature set. In this paper, we
present novel features based on the topography of a character as visible from
different viewing directions on a 2D plane. By topography of a character we
mean the structural features of the strokes and their spatial relations. In
this work we develop topographic features of strokes visible with respect to
views from different directions (e.g. North, South, East, and West). We
consider three types of topographic features: closed region, convexity of
strokes, and straight line strokes. These features are represented as a
shape-based graph which acts as an invariant feature set for discriminating
very similar type characters efficiently. We have tested the proposed method on
printed and handwritten Bengali and Hindi character images. Initial results
demonstrate the efficacy of our approach."
"Face recognition has been studied extensively for more than 20 years now.
Since the beginning of 90s the subject has became a major issue. This
technology is used in many important real-world applications, such as video
surveillance, smart cards, database security, internet and intranet access.
This report reviews recent two algorithms for face recognition which take
advantage of a relatively new multiscale geometric analysis tool - Curvelet
transform, for facial processing and feature extraction. This transform proves
to be efficient especially due to its good ability to detect curves and lines,
which characterize the human's face. An algorithm which is based on the two
algorithms mentioned above is proposed, and its performance is evaluated on
three data bases of faces: AT&T (ORL), Essex Grimace and Georgia-Tech.
k-nearest neighbour (k-NN) and Support vector machine (SVM) classifiers are
used, along with Principal Component Analysis (PCA) for dimensionality
reduction. This algorithm shows good results, and it even outperforms other
algorithms in some cases."
"Fingerprint recognition is one of most popular and accuracy Biometric
technologies. Nowadays, it is used in many real applications. However,
recognizing fingerprints in poor quality images is still a very complex
problem. In recent years, many algorithms, models...are given to improve the
accuracy of recognition system. This paper discusses on the standardized
fingerprint model which is used to synthesize the template of fingerprints. In
this model, after pre-processing step, we find the transformation between
templates, adjust parameters, synthesize fingerprint, and reduce noises. Then,
we use the final fingerprint to match with others in FVC2004 fingerprint
database (DB4) to show the capability of the model."
"Facial expression classification is a kind of image classification and it has
received much attention, in recent years. There are many approaches to solve
these problems with aiming to increase efficient classification. One of famous
suggestions is described as first step, project image to different spaces;
second step, in each of these spaces, images are classified into responsive
class and the last step, combine the above classified results into the final
result. The advantages of this approach are to reflect fulfill and multiform of
image classified. In this paper, we use 2D-PCA and its variants to project the
pattern or image into different spaces with different grouping strategies. Then
we develop a model which combines many Neural Networks applied for the last
step. This model evaluates the reliability of each space and gives the final
classification conclusion. Our model links many Neural Networks together, so we
call it Multi Artificial Neural Network (MANN). We apply our proposal model for
6 basic facial expressions on JAFFE database consisting 213 images posed by 10
Japanese female models."
"In remote sensing, image fusion technique is a useful tool used to fuse high
spatial resolution panchromatic images (PAN) with lower spatial resolution
multispectral images (MS) to create a high spatial resolution multispectral of
image fusion (F) while preserving the spectral information in the multispectral
image (MS).There are many PAN sharpening techniques or Pixel-Based image fusion
techniques that have been developed to try to enhance the spatial resolution
and the spectral property preservation of the MS. This paper attempts to
undertake the study of image fusion, by using two types of pixel-based image
fusion techniques i.e. Arithmetic Combination and Frequency Filtering Methods
of Pixel-Based Image Fusion Techniques. The first type includes Brovey
Transform (BT), Color Normalized Transformation (CN) and Multiplicative Method
(MLT). The second type include High-Pass Filter Additive Method (HPFA),
High-Frequency-Addition Method (HFA) High Frequency Modulation Method (HFM) and
The Wavelet transform-based fusion method (WT). This paper also devotes to
concentrate on the analytical techniques for evaluating the quality of image
fusion (F) by using various methods including Standard Deviation (SD),
Entropy(En), Correlation Coefficient (CC), Signal-to Noise Ratio (SNR),
Normalization Root Mean Square Error (NRMSE) and Deviation Index (DI) to
estimate the quality and degree of information improvement of a fused image
quantitatively."
"The IHS sharpening technique is one of the most commonly used techniques for
sharpening. Different transformations have been developed to transfer a color
image from the RGB space to the IHS space. Through literature, it appears that,
various scientists proposed alternative IHS transformations and many papers
have reported good results whereas others show bad ones as will as not those
obtained which the formula of IHS transformation were used. In addition to
that, many papers show different formulas of transformation matrix such as IHS
transformation. This leads to confusion what is the exact formula of the IHS
transformation?. Therefore, the main purpose of this work is to explore
different IHS transformation techniques and experiment it as IHS based image
fusion. The image fusion performance was evaluated, in this study, using
various methods to estimate the quality and degree of information improvement
of a fused image quantitatively."
"This paper addresses the problem of correlation estimation in sets of
compressed images. We consider a framework where images are represented under
the form of linear measurements due to low complexity sensing or security
requirements. We assume that the images are correlated through the displacement
of visual objects due to motion or viewpoint change and the correlation is
effectively represented by optical flow or motion field models. The correlation
is estimated in the compressed domain by jointly processing the linear
measurements. We first show that the correlated images can be efficiently
related using a linear operator. Using this linear relationship we then
describe the dependencies between images in the compressed domain. We further
cast a regularized optimization problem where the correlation is estimated in
order to satisfy both data consistency and motion smoothness objectives with a
Graph Cut algorithm. We analyze in detail the correlation estimation
performance and quantify the penalty due to image compression. Extensive
experiments in stereo and video imaging applications show that our novel
solution stays competitive with methods that implement complex image
reconstruction steps prior to correlation estimation. We finally use the
estimated correlation in a novel joint image reconstruction scheme that is
based on an optimization problem with sparsity priors on the reconstructed
images. Additional experiments show that our correlation estimation algorithm
leads to an effective reconstruction of pairs of images in distributed image
coding schemes that outperform independent reconstruction algorithms by 2 to 4
dB."
"In this paper, we propose a novel large deformation diffeomorphic
registration algorithm to align high angular resolution diffusion images
(HARDI) characterized by orientation distribution functions (ODFs). Our
proposed algorithm seeks an optimal diffeomorphism of large deformation between
two ODF fields in a spatial volume domain and at the same time, locally
reorients an ODF in a manner such that it remains consistent with the
surrounding anatomical structure. To this end, we first review the Riemannian
manifold of ODFs. We then define the reorientation of an ODF when an affine
transformation is applied and subsequently, define the diffeomorphic group
action to be applied on the ODF based on this reorientation. We incorporate the
Riemannian metric of ODFs for quantifying the similarity of two HARDI images
into a variational problem defined under the large deformation diffeomorphic
metric mapping (LDDMM) framework. We finally derive the gradient of the cost
function in both Riemannian spaces of diffeomorphisms and the ODFs, and present
its numerical implementation. Both synthetic and real brain HARDI data are used
to illustrate the performance of our registration algorithm."
"This paper presents a simple and efficient method to convolve an image with a
Gaussian kernel. The computation is performed in a constant number of
operations per pixel using running sums along the image rows and columns. We
investigate the error function used for kernel approximation and its relation
to the properties of the input signal. Based on natural image statistics we
propose a quadratic form kernel error function so that the output image l2
error is minimized. We apply the proposed approach to approximate the Gaussian
kernel by linear combination of constant functions. This results in very
efficient Gaussian filtering method. Our experiments show that the proposed
technique is faster than state of the art methods while preserving a similar
accuracy."
"We introduce a novel tracking technique which uses dynamic confidence-based
fusion of two different information sources for robust and efficient tracking
of visual objects. Mean-shift tracking is a popular and well known method used
in object tracking problems. Originally, the algorithm uses a similarity
measure which is optimized by shifting a search area to the center of a
generated weight image to track objects. Recent improvements on the original
mean-shift algorithm involves using a classifier that differentiates the object
from its surroundings. We adopt this classifier-based approach and propose an
application of a classifier fusion technique within this classifier-based
context in this work. We use two different classifiers, where one comes from a
background modeling method, to generate the weight image and we calculate
contributions of the classifiers dynamically using their confidences to
generate a final weight image to be used in tracking. The contributions of the
classifiers are calculated by using correlations between histograms of their
weight images and histogram of a defined ideal weight image in the previous
frame. We show with experiments that our dynamic combination scheme selects
good contributions for classifiers for different cases and improves tracking
accuracy significantly."
"Spectral unmixing is an important tool in hyperspectral data analysis for
estimating endmembers and abundance fractions in a mixed pixel. This paper
examines the applicability of a recently developed algorithm called graph
regularized nonnegative matrix factorization (GNMF) for this aim. The proposed
approach exploits the intrinsic geometrical structure of the data besides
considering positivity and full additivity constraints. Simulated data based on
the measured spectral signatures, is used for evaluating the proposed
algorithm. Results in terms of abundance angle distance (AAD) and spectral
angle distance (SAD) show that this method can effectively unmix hyperspectral
data."
"This report concerns the use of techniques for sparse signal representation
and sparse error correction for automatic face recognition. Much of the recent
interest in these techniques comes from the paper ""Robust Face Recognition via
Sparse Representation"" by Wright et al. (2009), which showed how, under certain
technical conditions, one could cast the face recognition problem as one of
seeking a sparse representation of a given input face image in terms of a
""dictionary"" of training images and images of individual pixels. In this
report, we have attempted to clarify some frequently encountered questions
about this work and particularly, on the validity of using sparse
representation techniques for face recognition."
"Extending the Liouville-Caputo definition of a fractional derivative to a
nonlocal covariant generalization of arbitrary bound operators acting on
multidimensional Riemannian spaces an appropriate approach for the 3D shape
recovery of aperture afflicted 2D slide sequences is proposed. We demonstrate,
that the step from a local to a nonlocal algorithm yields an order of magnitude
in accuracy and by using the specific fractional approach an additional factor
2 in accuracy of the derived results."
"Many applications require comparing multimodal data with different structure
and dimensionality that cannot be compared directly. Recently, there has been
increasing interest in methods for learning and efficiently representing such
multimodal similarity. In this paper, we present a simple algorithm for
multimodal similarity-preserving hashing, trying to map multimodal data into
the Hamming space while preserving the intra- and inter-modal similarities. We
show that our method significantly outperforms the state-of-the-art method in
the field."
"Iris recognition is considered as one of the best biometric methods used for
human identification and verification, this is because of its unique features
that differ from one person to another, and its importance in the security
field. This paper proposes an algorithm for iris recognition and classification
using a system based on Local Binary Pattern and histogram properties as a
statistical approaches for feature extraction, and Combined Learning Vector
Quantization Classifier as Neural Network approach for classification, in order
to build a hybrid model depends on both features. The localization and
segmentation techniques are presented using both Canny edge detection and Hough
Circular Transform in order to isolate an iris from the whole eye image and for
noise detection .Feature vectors results from LBP is applied to a Combined LVQ
classifier with different classes to determine the minimum acceptable
performance, and the result is based on majority voting among several LVQ
classifier. Different iris datasets CASIA, MMU1, MMU2, and LEI with different
extensions and size are presented. Since LBP is working on a grayscale level so
colored iris images should be transformed into a grayscale level. The proposed
system gives a high recognition rate 99.87 % on different iris datasets
compared with other methods."
"Object detection and classification using video is necessary for intelligent
planning and navigation on a mobile robot. However, current methods can be too
slow or not sufficient for distinguishing multiple classes. Techniques that
rely on binary (foreground/background) labels incorrectly identify areas with
multiple overlapping objects as single segment. We propose two Hierarchical
Markov Random Field models in efforts to distinguish connected objects using
tiered, binary label sets. Near-realtime performance has been achieved using
efficient optimization methods which runs up to 11 frames per second on a dual
core 2.2 Ghz processor. Evaluation of both models is done using footage taken
from a robot obstacle course at the 2010 Intelligent Ground Vehicle
Competition."
"The recent technological progress in acquisition, modeling and processing of
3D data leads to the proliferation of a large number of 3D objects databases.
Consequently, the techniques used for content based 3D retrieval has become
necessary. In this paper, we introduce a new method for 3D objects recognition
and retrieval by using a set of binary images CLI (Characteristic level
images). We propose a 3D indexing and search approach based on the similarity
between characteristic level images using Hu moments for it indexing. To
measure the similarity between 3D objects we compute the Hausdorff distance
between a vectors descriptor. The performance of this new approach is evaluated
at set of 3D object of well known database, is NTU (National Taiwan University)
database."
"A key recent advance in face recognition models a test face image as a sparse
linear combination of a set of training face images. The resulting sparse
representations have been shown to possess robustness against a variety of
distortions like random pixel corruption, occlusion and disguise. This approach
however makes the restrictive (in many scenarios) assumption that test faces
must be perfectly aligned (or registered) to the training data prior to
classification. In this paper, we propose a simple yet robust local block-based
sparsity model, using adaptively-constructed dictionaries from local features
in the training data, to overcome this misalignment problem. Our approach is
inspired by human perception: we analyze a series of local discriminative
features and combine them to arrive at the final classification decision. We
propose a probabilistic graphical model framework to explicitly mine the
conditional dependencies between these distinct sparse local features. In
particular, we learn discriminative graphs on sparse representations obtained
from distinct local slices of a face. Conditional correlations between these
sparse features are first discovered (in the training phase), and subsequently
exploited to bring about significant improvements in recognition rates.
Experimental results obtained on benchmark face databases demonstrate the
effectiveness of the proposed algorithms in the presence of multiple
registration errors (such as translation, rotation, and scaling) as well as
under variations of pose and illumination."
"Texture is an important spatial feature which plays a vital role in content
based image retrieval. The enormous growth of the internet and the wide use of
digital data have increased the need for both efficient image database creation
and retrieval procedure. This paper describes a new approach for texture
classification by combining statistical texture features of Local Binary
Pattern and Texture spectrum. Since most significant information of a texture
often appears in the high frequency channels, the features are extracted by the
computation of LBP and Texture Spectrum and Legendre Moments. Euclidean
distance is used for similarity measurement. The experimental result shows that
97.77% classification accuracy is obtained by the proposed method."
"In this text we show, that the notion of a ""good pair"" that was introduced in
the paper ""Digital Manifolds and the Theorem of Jordan-Brouwer"" has actually
known models. We will show, how to choose cubical adjacencies, the
generalizations of the well known 4- and 8-neighborhood to arbitrary
dimensions, in order to find good pairs. Furthermore, we give another proof for
the well known fact that the Khalimsky-topology implies good pairs. The outcome
is consistent with the known theory as presented by T.Y. Kong, A. Rosenfeld,
G.T. Herman and M. Khachan et.al and gives new insights in higher dimensions."
"Facial Expression Classification is an interesting research problem in recent
years. There are a lot of methods to solve this problem. In this research, we
propose a novel approach using Canny, Principal Component Analysis (PCA) and
Artificial Neural Network. Firstly, in preprocessing phase, we use Canny for
local region detection of facial images. Then each of local region's features
will be presented based on Principal Component Analysis (PCA). Finally, using
Artificial Neural Network (ANN)applies for Facial Expression Classification. We
apply our proposal method (Canny_PCA_ANN) for recognition of six basic facial
expressions on JAFFE database consisting 213 images posed by 10 Japanese female
models. The experimental result shows the feasibility of our proposal method."
"In this paper a novel approach is proposed based on single Euler number
feature which is free from thinning and size normalization for multi-font and
multi-size Kannada numeral recognition system. A nearest neighbor
classification is used for classification of Kannada numerals by considering
the Euclidian distance. A total 1500 numeral images with different font sizes
between (10..84) are tested for algorithm efficiency and the overall the
classification accuracy is found to be 99.00% .The said method is thinning
free, fast, and showed encouraging results on varying font styles and sizes of
Kannada numerals."
"In this paper a fast and novel method is proposed for multi-font multi-size
Kannada numeral recognition which is thinning free and without size
normalization approach. The different structural feature are used for numeral
recognition namely, directional density of pixels in four directions, water
reservoirs, maximum profile distances, and fill hole density are used for the
recognition of Kannada numerals. A Euclidian minimum distance criterion is used
to find minimum distances and K-nearest neighbor classifier is used to classify
the Kannada numerals by varying the size of numeral image from 16 to 50 font
sizes for the 20 different font styles from NUDI and BARAHA popular word
processing Kannada software. The total 1150 numeral images are tested and the
overall accuracy of classification is found to be 100%. The average time taken
by this method is 0.1476 seconds."
"In this paper, we propose a new redundant wavelet transform applicable to
scalar functions defined on high dimensional coordinates, weighted graphs and
networks. The proposed transform utilizes the distances between the given data
points. We modify the filter-bank decomposition scheme of the redundant wavelet
transform by adding in each decomposition level linear operators that reorder
the approximation coefficients. These reordering operators are derived by
organizing the tree-node features so as to shorten the path that passes through
these points. We explore the use of the proposed transform to image denoising,
and show that it achieves denoising results that are close to those obtained
with the BM3D algorithm."
"One of the most famous drawings by Leonardo da Vinci is a self-portrait in
red chalk, where he looks quite old. In fact, there is a sketch in one of his
notebooks, partially covered by written notes, that can be a self-portrait of
the artist when he was young. The use of image processing, to remove the
handwritten text and improve the image, allows a comparison of the two
portraits."
"This report is about facial asymmetry, its connection to emotional
expression, and methods of measuring facial asymmetry in videos of faces. The
research was motivated by two factors: firstly, there was a real opportunity to
develop a novel measure of asymmetry that required minimal human involvement
and that improved on earlier measures in the literature; and secondly, the
study of the relationship between facial asymmetry and emotional expression is
both interesting in its own right, and important because it can inform
neuropsychological theory and answer open questions concerning emotional
processing in the brain. The two aims of the research were: first, to develop
an automatic frame-by-frame measure of facial asymmetry in videos of faces that
improved on previous measures; and second, to use the measure to analyse the
relationship between facial asymmetry and emotional expression, and connect our
findings with previous research of the relationship."
"An image articulation manifold (IAM) is the collection of images formed when
an object is articulated in front of a camera. IAMs arise in a variety of image
processing and computer vision applications, where they provide a natural
low-dimensional embedding of the collection of high-dimensional images. To date
IAMs have been studied as embedded submanifolds of Euclidean spaces.
Unfortunately, their promise has not been realized in practice, because real
world imagery typically contains sharp edges that render an IAM
non-differentiable and hence non-isometric to the low-dimensional parameter
space under the Euclidean metric. As a result, the standard tools from
differential geometry, in particular using linear tangent spaces to transport
along the IAM, have limited utility. In this paper, we explore a nonlinear
transport operator for IAMs based on the optical flow between images and
develop new analytical tools reminiscent of those from differential geometry
using the idea of optical flow manifolds (OFMs). We define a new metric for
IAMs that satisfies certain local isometry conditions, and we show how to use
this metric to develop a new tools such as flow fields on IAMs, parallel flow
fields, parallel transport, as well as a intuitive notion of curvature. The
space of optical flow fields along a path of constant curvature has a natural
multi-scale structure via a monoid structure on the space of all flow fields
along a path. We also develop lower bounds on approximation errors while
approximating non-parallel flow fields by parallel flow fields."
"In one of his paintings, the School of Athens, Raphael is depicting Leonardo
da Vinci as the philosopher Plato. Some image processing tools can help us in
comparing this portrait with two Leonardo's portraits, considered as
self-portraits."
"In many image processing applications, such as segmentation and
classification, the selection of robust features descriptors is crucial to
improve the discrimination capabilities in real world scenarios. In particular,
it is well known that image textures constitute power visual cues for feature
extraction and classification. In the past few years the local binary pattern
(LBP) approach, a texture descriptor method proposed by Ojala et al., has
gained increased acceptance due to its computational simplicity and more
importantly for encoding a powerful signature for describing textures. However,
the original algorithm presents some limitations such as noise sensitivity and
its lack of rotational invariance which have led to many proposals or
extensions in order to overcome such limitations. In this paper we performed a
quantitative study of the Ojala's original LBP proposal together with other
recently proposed LBP extensions in the presence of rotational, illumination
and noisy changes. In the experiments we have considered two different
databases: Brodatz and CUReT for different sizes of LBP masks. Experimental
results demonstrated the effectiveness and robustness of the described texture
descriptors for images that are subjected to geometric or radiometric changes."
"The watershed is one of the most used tools in image segmentation. We present
how its concept is born and developed over time. Its implementation as an
algorithm or a hardwired device evolved together with the technology which
allowed it. We present also how it is used in practice, first together with
markers, and later introduced in a multiscale framework, in order to produce
not a unique partition but a complete hierarchy."
"Speeded Up Robust Features (SURF) has emerged as one of the more popular
feature descriptors and detectors in recent years. Performance and algorithmic
details vary widely between implementations due to SURF's complexity and
ambiguities found in its description. To resolve these ambiguities, a set of
general techniques for feature stability is defined based on the smoothness
rule. Additional improvements to SURF are proposed for speed and stability. To
illustrate the importance of these implementation details, a performance study
of popular SURF implementations is done. By utilizing all the suggested
improvements, it is possible to create a SURF implementation that is several
times faster and more stable."
"In this paper, we compare various image background subtraction algorithms
with the ground truth of cars counted. We have given a sample of thousand
images, which are the snap shots of current traffic as records at various
intersections and highways. We have also counted an approximate number of cars
that are visible in these images. In order to ascertain the accuracy of
algorithms to be used for the processing of million images, we compare them on
many metrics that includes (i) Scalability (ii) Accuracy (iii) Processing time."
"In this paper, the inverse problem of reconstructing reflectivity function of
a medium is examined within a blind deconvolution framework. The ultrasound
pulse is estimated using higher-order statistics, and Wiener filter is used to
obtain the ultrasonic reflectivity function through wavelet-based models. A new
approach to the parameter estimation of the inverse filtering step is proposed
in the nondestructive evaluation field, which is based on the theory of
Fourier-Wavelet regularized deconvolution (ForWaRD). This new approach can be
viewed as a solution to the open problem of adaptation of the ForWaRD framework
to perform the convolution kernel estimation and deconvolution
interdependently. The results indicate stable solutions of the estimated pulse
and an improvement in the radio-frequency (RF) signal taking into account its
signal-to-noise ratio (SNR) and axial resolution. Simulations and experiments
showed that the proposed approach can provide robust and optimal estimates of
the reflectivity function."
"Determining optimal number of clusters in a dataset is a challenging task.
Though some methods are available, there is no algorithm that produces unique
clustering solution. The paper proposes an Automatic Merging for Single Optimal
Solution (AMSOS) which aims to generate unique and nearly optimal clusters for
the given datasets automatically. The AMSOS is iteratively merges the closest
clusters automatically by validating with cluster validity measure to find
single and nearly optimal clusters for the given data set. Experiments on both
synthetic and real data have proved that the proposed algorithm finds single
and nearly optimal clustering structure in terms of number of clusters,
compactness and separation."
"This chapter shows that combining Haar-Hilbert and Log-Gabor improves iris
recognition performance leading to a less ambiguous biometric decision
landscape in which the overlap between the experimental intra- and interclass
score distributions diminishes or even vanishes. Haar-Hilbert, Log-Gabor and
combined Haar-Hilbert and Log-Gabor encoders are tested here both for single
and dual iris approach. The experimental results confirm that the best
performance is obtained for the dual iris approach when the iris code is
generated using the combined Haar-Hilbert and Log-Gabor encoder, and when the
matching score fuses the information from both Haar-Hilbert and Log-Gabor
channels of the combined encoder."
"The problem of segmenting a given image into coherent regions is important in
Computer Vision and many industrial applications require segmenting a known
object into its components. Examples include identifying individual parts of a
component for process control work in a manufacturing plant and identifying
parts of a car from a photo for automatic damage detection. Unfortunately most
of an object's parts of interest in such applications share the same pixel
characteristics, having similar colour and texture. This makes segmenting the
object into its components a non-trivial task for conventional image
segmentation algorithms. In this paper, we propose a ""Model Assisted
Segmentation"" method to tackle this problem. A 3D model of the object is
registered over the given image by optimising a novel gradient based loss
function. This registration obtains the full 3D pose from an image of the
object. The image can have an arbitrary view of the object and is not limited
to a particular set of views. The segmentation is subsequently performed using
a level-set based method, using the projected contours of the registered 3D
model as initialisation curves. The method is fully automatic and requires no
user interaction. Also, the system does not require any prior training. We
present our results on photographs of a real car."
"Context-dependence in human cognition process is a well-established fact.
Following this, we introduced the image segmentation method that can use
context to classify a pixel on the basis of its membership to a particular
object-class of the concerned image. In the broad methodological steps, each
pixel was defined by its context window (CW) surrounding it the size of which
was fixed heuristically. CW texture defined by the intensities of its pixels
was convoluted with weights optimized through a non-parametric function
supported by a backpropagation network. Result of convolution was used to
classify them. The training data points (i.e., pixels) were carefully chosen to
include all variety of contexts of types, i) points within the object, ii)
points near the edge but inside the objects, iii) points at the border of the
objects, iv) points near the edge but outside the objects, v) points near or at
the edge of the image frame. Moreover the training data points were selected
from all the images within image-dataset. CW texture information for 1000
pixels from face area and background area of images were captured, out of which
700 CWs were used as training input data, and remaining 300 for testing. Our
work gives the first time foundation of quantitative enumeration of efficiency
of image-segmentation which is extendable to segment out more than 2 objects
within an image."
"A method is developed to distinguish between cars and trucks present in a
video feed of a highway. The method builds upon previously done work using
covariance matrices as an accurate descriptor for regions. Background
subtraction and other similar proven image processing techniques are used to
identify the regions where the vehicles are most likely to be, and a distance
metric comparing the vehicle inside the region to a fixed library of vehicles
is used to determine the class of vehicle."
"An innovative way of calculating the von Mises distribution (VMD) of image
entropy is introduced in this paper. The VMD's concentration parameter and some
fitness parameter that will be later defined, have been analyzed in the
experimental part for determining their suitability as a image quality
assessment measure in some particular distortions such as Gaussian blur or
additive Gaussian noise. To achieve such measure, the local R\'{e}nyi entropy
is calculated in four equally spaced orientations and used to determine the
parameters of the von Mises distribution of the image entropy. Considering
contextual images, experimental results after applying this model show that the
best-in-focus noise-free images are associated with the highest values for the
von Mises distribution concentration parameter and the highest approximation of
image data to the von Mises distribution model. Our defined von Misses fitness
parameter experimentally appears also as a suitable no-reference image quality
assessment indicator for no-contextual images."
"Boundary detection is essential for a variety of computer vision tasks such
as segmentation and recognition. In this paper we propose a unified formulation
and a novel algorithm that are applicable to the detection of different types
of boundaries, such as intensity edges, occlusion boundaries or object category
specific boundaries. Our formulation leads to a simple method with
state-of-the-art performance and significantly lower computational cost than
existing methods. We evaluate our algorithm on different types of boundaries,
from low-level boundaries extracted in natural images, to occlusion boundaries
obtained using motion cues and RGB-D cameras, to boundaries from
soft-segmentation. We also propose a novel method for figure/ground
soft-segmentation that can be used in conjunction with our boundary detection
method and improve its accuracy at almost no extra computational cost."
"This paper describes a geometry based technique for feature extraction
applicable to segmentation-based word recognition systems. The proposed system
extracts the geometric features of the character contour. This features are
based on the basic line types that forms the character skeleton. The system
gives a feature vector as its output. The feature vectors so generated from a
training set, were then used to train a pattern recognition engine based on
Neural Networks so that the system can be benchmarked."
"At least two software packages---DARWIN, Eckerd College, and FinScan, Texas
A&M---exist to facilitate the identification of cetaceans---whales, dolphins,
porpoises---based upon the naturally occurring features along the edges of
their dorsal fins. Such identification is useful for biological studies of
population, social interaction, migration, etc. The process whereby fin
outlines are extracted in current fin-recognition software packages is manually
intensive and represents a major user input bottleneck: it is both time
consuming and visually fatiguing. This research aims to develop automated
methods (employing unsupervised thresholding and morphological processing
techniques) to extract cetacean dorsal fin outlines from digital photographs
thereby reducing manual user input. Ideally, automatic outline generation will
improve the overall user experience and improve the ability of the software to
correctly identify cetaceans. Various transformations from color to gray space
were examined to determine which produced a grayscale image in which a suitable
threshold could be easily identified. To assist with unsupervised thresholding,
a new metric was developed to evaluate the jaggedness of figures (""pixelarity"")
in an image after thresholding. The metric indicates how cleanly a threshold
segments background and foreground elements and hence provides a good measure
of the quality of a given threshold. This research results in successful
extractions in roughly 93% of images, and significantly reduces user-input
time."
"Recently the sparse representation based classification (SRC) has been
proposed for robust face recognition (FR). In SRC, the testing image is coded
as a sparse linear combination of the training samples, and the representation
fidelity is measured by the l2-norm or l1-norm of the coding residual. Such a
sparse coding model assumes that the coding residual follows Gaussian or
Laplacian distribution, which may not be effective enough to describe the
coding residual in practical FR systems. Meanwhile, the sparsity constraint on
the coding coefficients makes SRC's computational cost very high. In this
paper, we propose a new face coding model, namely regularized robust coding
(RRC), which could robustly regress a given signal with regularized regression
coefficients. By assuming that the coding residual and the coding coefficient
are respectively independent and identically distributed, the RRC seeks for a
maximum a posterior solution of the coding problem. An iteratively reweighted
regularized robust coding (IR3C) algorithm is proposed to solve the RRC model
efficiently. Extensive experiments on representative face databases demonstrate
that the RRC is much more effective and efficient than state-of-the-art sparse
representation based methods in dealing with face occlusion, corruption,
lighting and expression changes, etc."
"Color image segmentation is an important topic in the image processing field.
MRF-MAP is often adopted in the unsupervised segmentation methods, but their
performance are far behind recent interactive segmentation tools supervised by
user inputs. Furthermore, the existing related unsupervised methods also suffer
from the low efficiency, and high risk of being trapped in the local optima,
because MRF-MAP is currently solved by iterative frameworks with inaccurate
initial color distribution models. To address these problems, the letter
designs an efficient method to calculate the energy functions approximately in
the non-iteration style, and proposes a new binary segmentation algorithm based
on the slightly tuned Lanczos eigensolver. The experiments demonstrate that the
new algorithm achieves competitive performance compared with two state-of-art
segmentation methods."
"In this work we review the basic principles of stochastic logic and propose
its application to probabilistic-based pattern-recognition analysis. The
proposed technique is intrinsically a parallel comparison of input data to
various pre-stored categories using Bayesian techniques. We design smart
pulse-based stochastic-logic blocks to provide an efficient pattern recognition
analysis. The proposed rchitecture is applied to a specific navigation problem.
The resulting system is orders of magnitude faster than processor-based
solutions."
"We describe a method for fast approximation of sparse coding. The input space
is subdivided by a binary decision tree, and we simultaneously learn a
dictionary and assignment of allowed dictionary elements for each leaf of the
tree. We store a lookup table with the assignments and the pseudoinverses for
each node, allowing for very fast inference. We give an algorithm for learning
the tree, the dictionary and the dictionary element assignment, and In the
process of describing this algorithm, we discuss the more general problem of
learning the groups in group structured sparse modelling. We show that our
method creates good sparse representations by using it in the object
recognition framework of \cite{lazebnik06,yang-cvpr-09}. Implementing our own
fast version of the SIFT descriptor the whole system runs at 20 frames per
second on $321 \times 481$ sized images on a laptop with a quad-core cpu, while
sacrificing very little accuracy on the Caltech 101 and 15 scenes benchmarks."
"3D motion tracking is a critical task in many computer vision applications.
Unsupervised markerless 3D motion tracking systems determine the most relevant
object in the screen and then track it by continuously estimating its
projection features (center and area) from the edge image and a point inside
the relevant object projection (namely, inner point), until the tracking fails.
Existing object projection feature estimation techniques are based on
ray-casting from the inner point. These techniques present three main
drawbacks: when the inner point is surrounded by edges, rays may not reach
other relevant areas; as a consequence of that issue, the estimated features
may greatly vary depending on the position of the inner point relative to the
object projection; and finally, increasing the number of rays being casted and
the ray-casting iterations (which would make the results more accurate and
stable) increases the processing time to the point the tracking cannot be
performed on the fly. In this paper, we analyze an intuitive filling-based
object projection feature estimation technique that solves the aforementioned
problems but is too sensitive to edge miscalculations. Then, we propose a less
computing-intensive modification to that technique that would not be affected
by the existing techniques issues and would be no more sensitive to edge
miscalculations than ray-casting-based techniques."
"Segmentation of medical images is a challenging task owing to their
complexity. A standard segmentation problem within Magnetic Resonance Imaging
(MRI) is the task of labeling voxels according to their tissue type. Image
segmentation provides volumetric quantification of liver area and thus helps in
the diagnosis of disorders, such as Hepatitis, Cirrhosis, Jaundice,
Hemochromatosis etc.This work deals with comparison of segmentation by applying
Level Set Method,Fuzzy Level Information C-Means Clustering Algorithm and
Gradient Vector Flow Snake Algorithm.The results are compared using the
parameters such as Number of pixels correctly classified, and percentage of
area segmented."
"We propose a fast approximate algorithm for large graph matching. A new
projected fixed-point method is defined and a new doubly stochastic projection
is adopted to derive the algorithm. Previous graph matching algorithms suffer
from high computational complexity and therefore do not have good scalability
with respect to graph size. For matching two weighted graphs of $n$ nodes, our
algorithm has time complexity only $O(n^3)$ per iteration and space complexity
$O(n^2)$. In addition to its scalability, our algorithm is easy to implement,
robust, and able to match undirected weighted attributed graphs of different
sizes. While the convergence rate of previous iterative graph matching
algorithms is unknown, our algorithm is theoretically guaranteed to converge at
a linear rate. Extensive experiments on large synthetic and real graphs (more
than 1,000 nodes) were conducted to evaluate the performance of various
algorithms. Results show that in most cases our proposed algorithm achieves
better performance than previous state-of-the-art algorithms in terms of both
speed and accuracy in large graph matching. In particular, with high accuracy,
our algorithm takes only a few seconds (in a PC) to match two graphs of 1,000
nodes."
"From The late 90th, ""Skin Detection"" becomes one of the major problems in
image processing. If ""Skin Detection"" will be done in high accuracy, it can be
used in many cases as face recognition, Human Tracking and etc. Until now so
many methods were presented for solving this problem. In most of these methods,
color space was used to extract feature vector for classifying pixels, but the
most of them have not good accuracy in detecting types of skin. The proposed
approach in this paper is based on ""Color based image retrieval"" (CBIR)
technique. In this method, first by means of CBIR method and image tiling and
considering the relation between pixel and its neighbors, a feature vector
would be defined and then with using a training step, detecting the skin in the
test stage. The result shows that the presenting approach, in addition to its
high accuracy in detecting type of skin, has no sensitivity to illumination
intensity and moving face orientation."
"In recent years there has been considerable interest in human action
recognition. Several approaches have been developed in order to enhance the
automatic video analysis. Although some developments have been achieved by the
computer vision community, the properly classification of human motion is still
a hard and challenging task. The objective of this study is to investigate the
use of 3D multi-scale fractal dimension to recognize motion patterns in videos.
In order to develop a robust strategy for human motion classification, we
proposed a method where the Fourier transform is used to calculate the
derivative in which all data points are deemed. Our results shown that
different accuracy rates can be found for different databases. We believe that
in specific applications our results are the first step to develop an automatic
monitoring system, which can be applied in security systems, traffic
monitoring, biology, physical therapy, cardiovascular disease among many
others."
"The Quality of image fusion is an essential determinant of the value of
processing images fusion for many applications. Spatial and spectral qualities
are the two important indexes that used to evaluate the quality of any fused
image. However, the jury is still out of fused image's benefits if it compared
with its original images. In addition, there is a lack of measures for
assessing the objective quality of the spatial resolution for the fusion
methods. Therefore, an objective quality of the spatial resolution assessment
for fusion images is required. Most important details of the image are in edges
regions, but most standards of image estimation do not depend upon specifying
the edges in the image and measuring their edges. However, they depend upon the
general estimation or estimating the uniform region, so this study deals with
new method proposed to estimate the spatial resolution by Contrast Statistical
Analysis (CSA) depending upon calculating the contrast of the edge, non edge
regions and the rate for the edges regions. Specifying the edges in the image
is made by using Soble operator with different threshold values. In addition,
estimating the color distortion added by image fusion based on Histogram
Analysis of the edge brightness values of all RGB-color bands and Lcomponent."
"Let $I=(\mathbb{Z}^3,26,6,B)$ be a 3D digital image, let $Q(I)$ be the
associated cubical complex and let $\partial Q(I)$ be the subcomplex of $Q(I)$
whose maximal cells are the quadrangles of $Q(I)$ shared by a voxel of $B$ in
the foreground -- the object under study -- and by a voxel of
$\mathbb{Z}^3\smallsetminus B$ in the background -- the ambient space. We show
how to simplify the combinatorial structure of $\partial Q(I)$ and obtain a 3D
polyhedral complex $P(I)$ homeomorphic to $\partial Q(I)$ but with fewer cells.
We introduce an algorithm that computes cup products on
$H^*(P(I);\mathbb{Z}_2)$ directly from the combinatorics. The computational
method introduced here can be effectively applied to any polyhedral complex
embedded in $\mathbb{R}^3$."
"In a vision system, every task needs that the operators to apply should be
{\guillemotleft} well chosen {\guillemotright} and their parameters should be
also {\guillemotleft} well adjusted {\guillemotright}. The diversity of
operators and the multitude of their parameters constitute a big challenge for
users. As it is very difficult to make the {\guillemotleft} right
{\guillemotright} choice, lack of a specific rule, many disadvantages appear
and affect the computation time and especially the quality of results. In this
paper we present a multi-agent architecture to learn the best operators to
apply and their best parameters for a class of images. Our architecture
consists of three types of agents: User Agent, Operator Agent and Parameter
Agent. The User Agent determines the phases of treatment, a library of
operators and the possible values of their parameters. The Operator Agent
constructs all possible combinations of operators and the Parameter Agent, the
core of the architecture, adjusts the parameters of each combination by
treating a large number of images. Through the reinforcement learning
mechanism, our architecture does not consider only the system opportunities but
also the user preferences."
"Human face recognition is, indeed, a challenging task, especially under the
illumination and pose variations. We examine in the present paper effectiveness
of two simple algorithms using coiflet packet and Radon transforms to recognize
human faces from some databases of still gray level images, under the
environment of illumination and pose variations. Both the algorithms convert
2-D gray level training face images into their respective depth maps or
physical shape which are subsequently transformed by Coiflet packet and Radon
transforms to compute energy for feature extraction. Experiments show that such
transformed shape features are robust to illumination and pose variations. With
the features extracted, training classes are optimally separated through linear
discriminant analysis (LDA), while classification for test face images is made
through a k-NN classifier, based on L1 norm and Mahalanobis distance measures.
Proposed algorithms are then tested on face images that differ in
illumination,expression or pose separately, obtained from three
databases,namely, ORL, Yale and Essex-Grimace databases. Results, so obtained,
are compared with two different existing algorithms.Performance using
Daubechies wavelets is also examined. It is seen that the proposed Coiflet
packet and Radon transform based algorithms have significant performance,
especially under different illumination conditions and pose variation.
Comparison shows the proposed algorithms are superior."
"The traditional color-based mean-shift tracking algorithm is popular among
tracking methods due to its simple and efficient procedure, however, the lack
of dynamism in its target model makes it unsuitable for tracking objects which
have changes in their sizes and shapes. In this paper, we propose a fast novel
threephase colored object tracker algorithm based on mean shift idea while
utilizing adaptive model. The proposed method can improve the mentioned
weaknesses of the original mean-shift algorithm. The experimental results show
that the new method is feasible, robust and has acceptable speed in comparison
with other algorithms.15 page,"
"Previous research showed that camera specific noise patterns, so-called
PRNU-patterns, are extracted from images and related images could be found. In
this particular research the focus is on grouping images from a database, based
on a shared noise pattern as an identification method for cameras. Using the
method as described in this article, groups of images, created using the same
camera, could be linked from a large database of images. Using MATLAB
programming, relevant image noise patterns are extracted from images much
quicker than common methods by the use of faster noise extraction filters and
improvements to reduce the calculation costs. Relating noise patterns, with a
correlation above a certain threshold value, can quickly be matched. Hereby,
from a database of images, groups of relating images could be linked and the
method could be used to scan a large number of images for suspect noise
patterns."
"Matching cells over time has long been the most difficult step in cell
tracking. In this paper, we approach this problem by recasting it as a
classification problem. We construct a feature set for each cell, and compute a
feature difference vector between a cell in the current frame and a cell in a
previous frame. Then we determine whether the two cells represent the same cell
over time by training decision trees as our binary classifiers. With the output
of decision trees, we are able to formulate an assignment problem for our cell
association task and solve it using a modified version of the Hungarian
algorithm."
"Computational color constancy is a very important topic in computer vision
and has attracted many researchers' attention. Recently, lots of research has
shown the effects of high level visual content information for illumination
estimation. However, all of these existing methods are essentially
combinational strategies in which image's content analysis is only used to
guide the combination or selection from a variety of individual illumination
estimation methods. In this paper, we propose a novel bilayer sparse coding
model for illumination estimation that considers image similarity in terms of
both low level color distribution and high level image scene content
simultaneously. For the purpose, the image's scene content information is
integrated with its color distribution to obtain optimal illumination
estimation model. The experimental results on two real-world image sets show
that our algorithm is superior to other prevailing illumination estimation
methods, even better than combinational methods."
"Vibro-acoustography (VA) is a medical imaging method based on the
difference-frequency generation produced by the mixture of two focused
ultrasound beams. VA has been applied to different problems in medical imaging
such as imaging bones, microcalcifications in the breast, mass lesions, and
calcified arteries. The obtained images may have a resolution of 0.7--0.8 mm.
Current VA systems based on confocal or linear array transducers generate
C-scan images at the beam focal plane. Images on the axial plane are also
possible, however the system resolution along depth worsens when compared to
the lateral one. Typical axial resolution is about 1.0 cm. Furthermore, the
elevation resolution of linear array systems is larger than that in lateral
direction. This asymmetry degrades C-scan images obtained using linear arrays.
The purpose of this article is to study VA image restoration based on a 3D
point spread function (PSF) using classical deconvolution algorithms: Wiener,
constrained least-squares (CLSs), and geometric mean filters. To assess the
filters' performance, we use an image quality index that accounts for
correlation loss, luminance and contrast distortion. Results for simulated VA
images show that the quality index achieved with the Wiener filter is 0.9 (1
indicates perfect restoration). This filter yielded the best result in
comparison with the other ones. Moreover, the deconvolution algorithms were
applied to an experimental VA image of a phantom composed of three stretched
0.5 mm wires. Experiments were performed using transducer driven at two
frequencies, 3075 kHz and 3125 kHz, which resulted in the difference-frequency
of 50 kHz. Restorations with the theoretical line spread function (LSF) did not
recover sufficient information to identify the wires in the images. However,
using an estimated LSF the obtained results displayed enough information to
spot the wires in the images."
"In this project, we study the hidden Markov random field (HMRF) model and its
expectation-maximization (EM) algorithm. We implement a MATLAB toolbox named
HMRF-EM-image for 2D image segmentation using the HMRF-EM framework. This
toolbox also implements edge-prior-preserving image segmentation, and can be
easily reconfigured for other problems, such as 3D image segmentation."
"Principal component analysis (PCA) is a popular tool for linear
dimensionality reduction and feature extraction. Kernel PCA is the nonlinear
form of PCA, which better exploits the complicated spatial structure of
high-dimensional features. In this paper, we first review the basic ideas of
PCA and kernel PCA. Then we focus on the reconstruction of pre-images for
kernel PCA. We also give an introduction on how PCA is used in active shape
models (ASMs), and discuss how kernel PCA can be applied to improve traditional
ASMs. Then we show some experimental results to compare the performance of
kernel PCA and standard PCA for classification problems. We also implement the
kernel PCA-based ASMs, and use it to construct human face models."
"The art of recovering an image from damage in an undetectable form is known
as inpainting. The manual work of inpainting is most often a very time
consuming process. Due to digitalization of this technique, it is automatic and
faster. In this paper, after the user selects the regions to be reconstructed,
the algorithm automatically reconstruct the lost regions with the help of the
information surrounding them. The existing methods perform very well when the
region to be reconstructed is very small, but fails in proper reconstruction as
the area increases. This paper describes a Hierarchical method by which the
area to be inpainted is reduced in multiple levels and Total Variation(TV)
method is used to inpaint in each level. This algorithm gives better
performance when compared with other existing algorithms such as nearest
neighbor interpolation, Inpainting through Blurring and Sobolev Inpainting."
"We address the problem of unsupervised learning of complex articulated object
models from 3D range data. We describe an algorithm whose input is a set of
meshes corresponding to different configurations of an articulated object. The
algorithm automatically recovers a decomposition of the object into
approximately rigid parts, the location of the parts in the different object
instances, and the articulated object skeleton linking the parts. Our algorithm
first registers allthe meshes using an unsupervised non-rigid technique
described in a companion paper. It then segments the meshes using a graphical
model that captures the spatial contiguity of parts. The segmentation is done
using the EM algorithm, iterating between finding a decomposition of the object
into rigid parts, and finding the location of the parts in the object
instances. Although the graphical model is densely connected, the object
decomposition step can be performed optimally and efficiently, allowing us to
identify a large number of object parts while avoiding local maxima. We
demonstrate the algorithm on real world datasets, recovering a 15-part
articulated model of a human puppet from just 7 different puppet
configurations, as well as a 4 part model of a fiexing arm where significant
non-rigid deformation was present."
"One of the major problems in modeling natural signals is that signals with
very similar structure may locally have completely different measurements,
e.g., images taken under different illumination conditions, or the speech
signal captured in different environments. While there have been many
successful attempts to address these problems in application-specific settings,
we believe that underlying a large set of problems in signal representation is
a representational deficiency of intensity-derived local measurements that are
the basis of most efficient models. We argue that interesting structure in
signals is better captured when the signal is de- fined as a matrix whose
entries are discrete indices to a separate palette of possible measurements. In
order to model the variability in signal structure, we define a signal class
not by a single index map, but by a probability distribution over the index
maps, which can be estimated from the data, and which we call probabilistic
index maps. The existing algorithm can be adapted to work with this
representation. Furthermore, the probabilistic index map representation leads
to algorithms with computational costs proportional to either the size of the
palette or the log of the size of the palette, making the cost of significantly
increased invariance to non-structural changes quite bearable. We illustrate
the benefits of the probabilistic index map representation in several
applications in computer vision and speech processing."
"Stack filters are a special case of non-linear filters. They have a good
performance for filtering images with different types of noise while preserving
edges and details. A stack filter decomposes an input image into several binary
images according to a set of thresholds. Each binary image is then filtered by
a Boolean function, which characterizes the filter. Adaptive stack filters can
be designed to be optimal; they are computed from a pair of images consisting
of an ideal noiseless image and its noisy version. In this work we study the
performance of adaptive stack filters when they are applied to Synthetic
Aperture Radar (SAR) images. This is done by evaluating the quality of the
filtered images through the use of suitable image quality indexes and by
measuring the classification accuracy of the resulting images."
"In this paper, we carry out a comparative study of the efficacy of wavelets
belonging to Daubechies and Coiflet family in achieving image segmentation
through a fast statistical algorithm.The fact that wavelets belonging to
Daubechies family optimally capture the polynomial trends and those of Coiflet
family satisfy mini-max condition, makes this comparison interesting. In the
context of the present algorithm, it is found that the performance of Coiflet
wavelets is better, as compared to Daubechies wavelet."
"Various and different methods can be used to produce high-resolution
multispectral images from high-resolution panchromatic image (PAN) and
low-resolution multispectral images (MS), mostly on the pixel level. The
Quality of image fusion is an essential determinant of the value of processing
images fusion for many applications. Spatial and spectral qualities are the two
important indexes that used to evaluate the quality of any fused image.
However, the jury is still out of fused image's benefits if it compared with
its original images. In addition, there is a lack of measures for assessing the
objective quality of the spatial resolution for the fusion methods. So, an
objective quality of the spatial resolution assessment for fusion images is
required. Therefore, this paper describes a new approach proposed to estimate
the spatial resolution improve by High Past Division Index (HPDI) upon
calculating the spatial-frequency of the edge regions of the image and it deals
with a comparison of various analytical techniques for evaluating the Spatial
quality, and estimating the colour distortion added by image fusion including:
MG, SG, FCC, SD, En, SNR, CC and NRMSE. In addition, this paper devotes to
concentrate on the comparison of various image fusion techniques based on pixel
and feature fusion technique."
"In this paper, we propose a unified energy minimization model for the
segmentation of non-smooth image structures. The energy of piecewise linear
patch reconstruction is considered as an objective measure of the quality of
the segmentation of non-smooth structures. The segmentation is achieved by
minimizing the single energy without any separate process of feature
extraction. We also prove that the error of segmentation is bounded by the
proposed energy functional, meaning that minimizing the proposed energy leads
to reducing the error of segmentation. As a by-product, our method produces a
dictionary of optimized orthonormal descriptors for each segmented region. The
unique feature of our method is that it achieves the simultaneous segmentation
and description for non-smooth image structures under the same optimization
framework. The experiments validate our theoretical claims and show the clear
superior performance of our methods over other related methods for segmentation
of various image textures. We show that our model can be coupled with the
piecewise smooth model to handle both smooth and non-smooth structures, and we
demonstrate that the proposed model is capable of coping with multiple
different regions through the one-against-all strategy."
"This paper presents a review of human activity recognition and behaviour
understanding in video sequence. The key objective of this paper is to provide
a general review on the overall process of a surveillance system used in the
current trend. Visual surveillance system is directed on automatic
identification of events of interest, especially on tracking and classification
of moving objects. The processing step of the video surveillance system
includes the following stages: Surrounding model, object representation, object
tracking, activity recognition and behaviour understanding. It describes
techniques that use to define a general set of activities that are applicable
to a wide range of scenes and environments in video sequence."
"In this technical report, we review related works and recent trends in visual
vocabulary based web image search, object recognition, mobile visual search,
and 3D object retrieval. Especial focuses would be also given for the recent
trends in supervised/unsupervised vocabulary optimization, compact descriptor
for visual search, as well as in multi-view based 3D object representation."
"Segmentation of blood vessels in retinal images provides early diagnosis of
diseases like glaucoma, diabetic retinopathy and macular degeneration. Among
these diseases occurrence of Glaucoma is most frequent and has serious ocular
consequences that can even lead to blindness, if it is not detected early. The
clinical criteria for the diagnosis of glaucoma include intraocular pressure
measurement, optic nerve head evaluation, retinal nerve fiber layer and visual
field defects. This form of blood vessel segmentation helps in early detection
for ophthalmic diseases, and potentially reduces the risk of blindness. The
low-contrast images at the retina owing to narrow blood vessels of the retina
are difficult to extract. These low contrast images are, however useful in
revealing certain systemic diseases. Motivated by the goals of improving
detection of such vessels, this present work proposes an algorithm for
segmentation of blood vessels and compares the results between expert
ophthalmologist hand-drawn ground-truths and segmented image(i.e. the output of
the present work).Sensitivity, specificity, positive predictive value (PPV),
positive likelihood ratio (PLR) and accuracy are used to evaluate overall
performance.It is found that this work segments blood vessels successfully with
sensitivity, specificity, PPV, PLR and accuracy of 99.62%, 54.66%, 95.08%,
219.72 and 95.03%, respectively."
"The present work proposes a computer-aided normal and abnormal heart sound
identification based on Discrete Wavelet Transform (DWT), it being useful for
tele-diagnosis of heart diseases. Due to the presence of Cumulative Frequency
components in the spectrogram, DWT is applied on the spectro-gram up to n level
to extract the features from the individual approximation components. One
dimensional feature vector is obtained by evaluating the Row Mean of the
approximation components of these spectrograms. For this present approach, the
set of spectrograms has been considered as the database, rather than raw sound
samples. Minimum Euclidean distance is computed between feature vector of the
test sample and the feature vectors of the stored samples to identify the heart
sound. By applying this algorithm, almost 82% of accuracy was achieved."
"In this paper a comparative study between Moravec and Harris Corner Detection
has been done for obtaining features required to track and recognize objects
within a noisy image. Corner detection of noisy images is a challenging task in
image processing. Natural images often get corrupted by noise during
acquisition and transmission. As Corner detection of these noisy images does
not provide desired results, hence de-noising is required. Adaptive wavelet
thresholding approach is applied for the same."
"The Electrocardiogram (ECG) is a sensitive diagnostic tool that is used to
detect various cardiovascular diseases by measuring and recording the
electrical activity of the heart in exquisite detail. A wide range of heart
condition is determined by thorough examination of the features of the ECG
report. Automatic extraction of time plane features is important for
identification of vital cardiac diseases. This paper presents a
multi-resolution wavelet transform based system for detection 'P', 'Q', 'R',
'S', 'T' peaks complex from original ECG signal. 'R-R' time lapse is an
important minutia of the ECG signal that corresponds to the heartbeat of the
concerned person. Abrupt increase in height of the 'R' wave or changes in the
measurement of the 'R-R' denote various anomalies of human heart. Similarly
'P-P', 'Q-Q', 'S-S', 'T-T' also corresponds to different anomalies of heart and
their peak amplitude also envisages other cardiac diseases. In this proposed
method the 'PQRST' peaks are marked and stored over the entire signal and the
time interval between two consecutive 'R' peaks and other peaks interval are
measured to detect anomalies in behavior of heart, if any. The peaks are
achieved by the composition of Daubeheissub bands wavelet of original ECG
signal. The accuracy of the 'PQRST' complex detection and interval measurement
is achieved up to 100% with high exactitude by processing and thresholding the
original ECG signal."
"A novel multi-scale operator for unorganized 3D point clouds is introduced.
The Difference of Normals (DoN) provides a computationally efficient,
multi-scale approach to processing large unorganized 3D point clouds. The
application of DoN in the multi-scale filtering of two different real-world
outdoor urban LIDAR scene datasets is quantitatively and qualitatively
demonstrated. In both datasets the DoN operator is shown to segment large 3D
point clouds into scale-salient clusters, such as cars, people, and lamp posts
towards applications in semi-automatic annotation, and as a pre-processing step
in automatic object recognition. The application of the operator to
segmentation is evaluated on a large public dataset of outdoor LIDAR scenes
with ground truth annotations."
"This paper presents two new MAP (Maximum a Posteriori) filters for speckle
noise reduction and a Monte Carlo procedure for the assessment of their
performance. In order to quantitatively evaluate the results obtained using
these new filters, with respect to classical ones, a Monte Carlo extension of
Lee's protocol is proposed. This extension of the protocol shows that its
original version leads to inconsistencies that hamper its use as a general
procedure for filter assessment. Some solutions for these inconsistencies are
proposed, and a consistent comparison of speckle-reducing filters is provided."
"In this paper, we study the problem of recovering a sharp version of a given
blurry image when the blur kernel is unknown. Previous methods often introduce
an image-independent regularizer (such as Gaussian or sparse priors) on the
desired blur kernel. We shall show that the blurry image itself encodes rich
information about the blur kernel. Such information can be found through
analyzing and comparing how the spectrum of an image as a convolution operator
changes before and after blurring. Our analysis leads to an effective convex
regularizer on the blur kernel which depends only on the given blurry image. We
show that the minimizer of this regularizer guarantees to give good
approximation to the blur kernel if the original image is sharp enough. By
combining this powerful regularizer with conventional image deblurring
techniques, we show how we could significantly improve the deblurring results
through simulations and experiments on real images. In addition, our analysis
and experiments help explaining a widely accepted doctrine; that is, the edges
are good features for deblurring."
"A variety of new and powerful algorithms have been developed for image
compression over the years. Among them the wavelet-based image compression
schemes have gained much popularity due to their overlapping nature which
reduces the blocking artifacts that are common phenomena in JPEG compression
and multiresolution character which leads to superior energy compaction with
high quality reconstructed images. This paper provides a detailed survey on
some of the popular wavelet coding techniques such as the Embedded Zerotree
Wavelet (EZW) coding, Set Partitioning in Hierarchical Tree (SPIHT) coding, the
Set Partitioned Embedded Block (SPECK) Coder, and the Embedded Block Coding
with Optimized Truncation (EBCOT) algorithm. Other wavelet-based coding
techniques like the Wavelet Difference Reduction (WDR) and the Adaptive Scanned
Wavelet Difference Reduction (ASWDR) algorithms, the Space Frequency
Quantization (SFQ) algorithm, the Embedded Predictive Wavelet Image Coder
(EPWIC), Compression with Reversible Embedded Wavelet (CREW), the Stack-Run
(SR) coding and the recent Geometric Wavelet (GW) coding are also discussed.
Based on the review, recommendations and discussions are presented for
algorithm development and implementation."
"Inpainting is the technique of reconstructing unknown or damaged portions of
an image in a visually plausible way. Inpainting algorithm automatically fills
the damaged region in an image using the information available in undamaged
region. Propagation of structure and texture information becomes a challenge as
the size of damaged area increases. In this paper, a hierarchical inpainting
algorithm using wavelets is proposed. The hierarchical method tries to keep the
mask size smaller while wavelets help in handling the high pass structure
information and low pass texture information separately. The performance of the
proposed algorithm is tested using different factors. The results of our
algorithm are compared with existing methods such as interpolation, diffusion
and exemplar techniques."
"In this paper we propose a method of corner detection for obtaining features
which is required to track and recognize objects within a noisy image. Corner
detection of noisy images is a challenging task in image processing. Natural
images often get corrupted by noise during acquisition and transmission. Though
Corner detection of these noisy images does not provide desired results, hence
de-noising is required. Adaptive wavelet thresholding approach is applied for
the same."
"In this paper, the identification and classification of Viewer Age Range
Smart Signs, designed by the Radio and Television Supreme Council of Turkey, to
give age range information for the TV viewers, are realized. Therefore, the
automatic detection at the broadcast will be possible, enabling the
manufacturing of TV receivers which are sensible to these signs. The most
important step at this process is the pattern recognition. Since the symbols
that must be identified are circular, various circle detection techniques can
be employed. In our study, first, two different circle segmentation methods for
still images are analyzed, their advantages and drawbacks are discussed. A
popular neural network structure called Multilayer Perceptron is employed for
the classification. Afterwards, the same procedures are carried out for
streaming video. All of the steps depicted above are realized on a standard PC."
"Image super-resolution (SR) is one of the long-standing and active topics in
image processing community. A large body of works for image super resolution
formulate the problem with Bayesian modeling techniques and then obtain its
Maximum-A-Posteriori (MAP) solution, which actually boils down to a regularized
regression task over separable regularization term. Although straightforward,
this approach cannot exploit the full potential offered by the probabilistic
modeling, as only the posterior mode is sought. Also, the separable property of
the regularization term can not capture any correlations between the sparse
coefficients, which sacrifices much on its modeling accuracy. We propose a
Bayesian image SR algorithm via sparse modeling of natural images. The sparsity
property of the latent high resolution image is exploited by introducing latent
variables into the high-order Markov Random Field (MRF) which capture the
content adaptive variance by pixel-wise adaptation. The high-resolution image
is estimated via Empirical Bayesian estimation scheme, which is substantially
faster than our previous approach based on Markov Chain Monte Carlo sampling
[1]. It is shown that the actual cost function for the proposed approach
actually incorporates a non-factorial regularization term over the sparse
coefficients. Experimental results indicate that the proposed method can
generate competitive or better results than \emph{state-of-the-art} SR
algorithms."
"Automatic head frontal-view identification is challenging due to appearance
variations caused by pose changes, especially without any training samples. In
this paper, we present an unsupervised algorithm for identifying frontal view
among multiple facial images under various yaw poses (derived from the same
person). Our approach is based on Locally Linear Embedding (LLE), with the
assumption that with yaw pose being the only variable, the facial images should
lie in a smooth and low dimensional manifold. We horizontally flip the facial
images and present two K-nearest neighbor protocols for the original images and
the flipped images, respectively. In the proposed extended LLE, for any facial
image (original or flipped one), we search (1) the Ko nearest neighbors among
the original facial images and (2) the Kf nearest neighbors among the flipped
facial images to construct the same neighborhood graph. The extended LLE
eliminates the differences (because of background, face position and scale in
the whole image and some asymmetry of left-right face) between the original
facial image and the flipped facial image at the same yaw pose so that the
flipped facial images can be used effectively. Our approach does not need any
training samples as prior information. The experimental results show that the
frontal view of head can be identified reliably around the lowest point of the
pose manifold for multiple facial images, especially the cropped facial images
(little background and centered face)."
"A novel and uniform framework for face verification is presented in this
paper. First of all, a 2-directional 2-dimensional feature extraction method is
adopted to extract client-specific template - 2D discrimant projection matrix.
Then the face skin color information is utilized as an additive feature to
enhance decision making strategy that makes use of not only 2D grey feature but
also 2D skin color feature. A fusion decision of both is applied to experiment
the performance on the XM2VTS database according to Lausanne protocol.
Experimental results show that the framework achieves high verification
accuracy and verification speed."
"The main problem in colour management in prepress department is lack of
availability of literature on colour management and knowledge gap between
prepress department and press department. So a digital test from has been
created by Adobe Photoshop to analyse the ICC profile and to create a new
profile and this analysed data is used to study about various grey scale of RGB
and CMYK images. That helps in conversion of image from RGB to CMYK in prepress
department."
"By taking into account the properties and limitations of the human visual
system, images can be more efficiently compressed, colors more accurately
reproduced, prints better rendered. To show all these advantages in this paper
new adapted color charts have been created based on technical and visual image
category analysis. A number of tests have been carried out using extreme images
with their key information strictly in dark and light areas. It was shown that
the image categorization using the adapted color charts improves the analysis
of relevant image information with regard to both the image gradation and the
detail reproduction. The images with key information in hi-key areas were also
test printed using the adapted color charts."
"The production of a printed product involves three stages: prepress, the
printing process (press) itself, and finishing (post press). There are various
types of equipments (printers, scanners) and various qualities image are
present in the market. These give different color rendering each time during
reproduction. So, a color key tool has been developed keeping Color Management
Scheme (CMS) in mind so that during reproduction no color rendering takes place
irrespective of use of any device and resolution level has also been improved."
"In this paper an algorithm for recognizing speech has been proposed. The
recognized speech is used to execute related commands which use the MFCC and
two kind of classifiers, first one uses MLP and second one uses fuzzy inference
system as a classifier. The experimental results demonstrate the high gain and
efficiency of the proposed algorithm. We have implemented this system based on
graphical design and tested on a fix point digital signal processor (DSP) of
600 MHz, with reference DM6437-EVM of Texas instrument."
"The focus of this paper is to review approaches for segmentation of breast
regions in mammograms according to breast density. Studies based on density
have been undertaken because of the relationship between breast cancer and
density. Breast cancer usually occurs in the fibroglandular area of breast
tissue, which appears bright on mammograms and is described as breast density.
Most of the studies are focused on the classification methods for glandular
tissue detection. Others highlighted on the segmentation methods for
fibroglandular tissue, while few researchers performed segmentation of the
breast anatomical regions based on density. There have also been works on the
segmentation of other specific parts of breast regions such as either detection
of nipple position, skin-air interface or pectoral muscles. The problems on the
evaluation performance of the segmentation results in relation to ground truth
are also discussed in this paper."
"This paper presents novel approaches for efficient feature extraction using
environmental sound magnitude spectrogram. We propose approach based on the
visual domain. This approach included three methods. The first method is based
on extraction for each spectrogram a single log-Gabor filter followed by mutual
information procedure. In the second method, the spectrogram is passed by the
same steps of the first method but with an averaged bank of 12 log-Gabor
filter. The third method consists of spectrogram segmentation into three
patches, and after that for each spectrogram patch we applied the second
method. The classification results prove that the second method is the most
efficient in our environmental sound classification system."
"With the advent of digital images the problem of keeping picture
visualization uniformity arises because each printing or scanning device has
its own color chart. So, universal color profiles are made by ICC to bring
uniformity in various types of devices. Keeping that color profile in mind
various new color charts are created and calibrated with the help of standard
IT8 test charts available in the market. The main objective to color
reproduction is to produce the identical picture at device output. For that
principles for gamut mapping has been designed"
"The Active Shape Model (ASM) is one of the most popular local texture models
for face alignment. It applies in many fields such as locating facial features
in the image, face synthesis, etc. However, the experimental results show that
the accuracy of the classical ASM for some applications is not high. This paper
suggests some improvements on the classical ASM to increase the performance of
the model in the application: face alignment. Four of our major improvements
include: i) building a model combining Sobel filter and the 2-D profile in
searching face in image; ii) applying Canny algorithm for the enhancement edge
on image; iii) Support Vector Machine (SVM) is used to classify landmarks on
face, in order to determine exactly location of these landmarks support for
ASM; iv)automatically adjust 2-D profile in the multi-level model based on the
size of the input image. The experimental results on Caltech face database and
Technical University of Denmark database (imm_face) show that our proposed
improvement leads to far better performance."
"This paper proves that in iris recognition, the concepts of sheep, goats,
lambs and wolves - as proposed by Doddington and Yager in the so-called
Biometric Menagerie, are at most fuzzy and at least not quite well defined.
They depend not only on the users or on their biometric templates, but also on
the parameters that calibrate the iris recognition system. This paper shows
that, in the case of iris recognition, the extensions of these concepts have
very unsharp and unstable (non-stationary) boundaries. The membership of a user
to these categories is more often expressed as a degree (as a fuzzy value)
rather than as a crisp value. Moreover, they are defined by fuzzy Sugeno rules
instead of classical (crisp) definitions. For these reasons, we said that the
Biometric Menagerie proposed by Doddington and Yager could be at most a fuzzy
concept of biometry, but even this status is conditioned by improving its
definition. All of these facts are confirmed experimentally in a series of 12
exhaustive iris recognition tests undertaken for University of Bath Iris Image
Database while using three different iris code dimensions (256x16, 128x8 and
64x4), two different iris texture encoders (Log-Gabor and Haar-Hilbert) and two
different types of safety models."
"This paper analyses the set of iris codes stored or used in an iris
recognition system as an f-granular space. The f-granulation is given by
identifying in the iris code space the extensions of the fuzzy concepts wolves,
goats, lambs and sheep (previously introduced by Doddington as 'animals' of the
biometric menagerie) - which together form a partitioning of the iris code
space. The main question here is how objective (stable / stationary) this
partitioning is when the iris segments are subject to noisy acquisition. In
order to prove that the f-granulation of iris code space with respect to the
fuzzy concepts that define the biometric menagerie is unstable in noisy
conditions (is sensitive to noise), three types of noise (localvar, motion
blur, salt and pepper) have been alternatively added to the iris segments
extracted from University of Bath Iris Image Database. The results of 180
exhaustive (all-to-all) iris recognition tests are presented and commented
here."