summary
"Recent manifestations of apparently faster-than-light effects confirmed our
predictions that the group velocity in transparent optical media can exceed c.
Special relativity is not violated by these phenomena. Moreover, in the
electronic domain, the causality principle does not forbid negative group
delays of analytic signals in electronic circuits, in which the peak of an
output pulse leaves the exit port of a circuit before the peak of the input
pulse enters the input port. Furthermore, pulse distortion for these
superluminal analytic signals can be negligible in both the optical and
electronic domains. Here we suggest an extension of these ideas to the
microelectronic domain. The underlying principle is that negative feedback can
be used to produce negative group delays. Such negative group delays can be
used to cancel out the positive group delays due to transistor latency (e.g.,
the finite RC rise time of MOSFETS caused by their intrinsic gate capacitance),
as well as the propagation delays due to the interconnects between transistors.
Using this principle, it is possible to speed up computer systems."
"The Common Object Request Broker Architecture (CORBA) is successfully used in
many control systems (CS) for data transfer and device modeling. Communication
rates below 1 millisecond, high reliability, scalability, language independence
and other features make it very attractive. For common types of applications
like error logging, alarm messaging or slow monitoring, one can benefit from
standard CORBA services that are implemented by third parties and save
tremendous amount of developing time. We have started using few CORBA services
on our previous CORBA-based control system for the light source ANKA [1] and
use now several CORBA services for the ALMA Common Software (ACS) [2], the core
of the control system of the Atacama Large Millimeter Array. Our experiences
with the interface repository (IFR), the implementation repository, the naming
service, the property service, telecom log service and the notify service from
different vendors are presented. Performance and scalability benchmarks have
been performed."
"A number of known techniques for improving cache performance in scientific
computations involve the reordering of the iteration space. Some of these
reorderings can be considered coverings of the iteration space with sets having
small surface-to-volume ratios. Use of such sets may reduce the number of cache
misses in computations of local operators having the iteration space as their
domain. First, we derive lower bounds on cache misses that any algorithm must
suffer while computing a local operator on a grid. Then, we explore coverings
of iteration spaces of structured and unstructured discretization grid
operators which allow us to approach these lower bounds. For structured grids
we introduce a covering by successive minima tiles based on the interference
lattice of the grid. We show that the covering has a small surface-to-volume
ratio and present a computer experiment showing actual reduction of the cache
misses achieved by using these tiles. For planar unstructured grids we show
existence of a covering which reduces the number of cache misses to the level
of that of structured grids. Next, we introduce a class of multidimensional
grids, called starry grids in this paper. These grids represent an abstraction
of unstructured grids used in, for example, molecular simulations and the
solution of partial differential equations. We show that starry grids can be
covered by sets having a low surface-to-volume ratio and, hence have the same
cache efficiency as structured grids. Finally, we present a triangulation of a
three-dimensional cube that has the property that any local operator on the
corresponding grid must incur a significantly larger number of cache misses
than a similar operator on a structured grid of the same size."
"It has been suggested in voice over IP that an appropriate choice of the
distribution used in modeling the delay jitters, can improve the play-out
algorithm. In this paper, we propose a tool using which, one can determine, at
a given instance, which distribution model best explains the jitter
distribution. This is done using Expectation Maximization, to choose amongst
possible distribution models which include, the i.i.d exponential distribution,
the gamma distribution etc."
"Monitoring and information services form a key component of a distributed
system, or Grid. A quantitative study of such services can aid in understanding
the performance limitations, advise in the deployment of the systems, and help
evaluate future development work. To this end, we study the performance of
three monitoring and information services for distributed systems: the Globus
Toolkit's Monitoring and Discovery Service (MDS), the European Data Grid
Relational Grid Monitoring Architecture (R-GMA), and Hawkeye, part of the
Condor project. We perform experiments to test their scalability with respect
to number of users, number of resources, and amount of data collected. Our
study shows that each approach has different behaviors, often due to their
different design goals. In the four sets of experiments we conducted to
evaluate the performance of the service components under different
circumstances, we found a strong advantage to caching or prefetching the data,
as well as the need to have primary components at well connected sites due to
high load seen by all systems."
"Monitoring large clusters is a challenging problem. It is necessary to
observe a large quantity of devices with a reasonably short delay between
consecutive observations. The set of monitored devices may include PCs, network
switches, tape libraries and other equipments. The monitoring activity should
not impact the performances of the system. In this paper we present PerfMC, a
monitoring system for large clusters. PerfMC is driven by an XML configuration
file, and uses the Simple Network Management Protocol (SNMP) for data
collection. SNMP is a standard protocol implemented by many networked
equipments, so the tool can be used to monitor a wide range of devices. System
administrators can display informations on the status of each device by
connecting to a WEB server embedded in PerfMC. The WEB server can produce
graphs showing the value of different monitored quantities as a function of
time; it can also produce arbitrary XML pages by applying XSL Transformations
to an internal XML representation of the cluster's status. XSL Transformations
may be used to produce HTML pages which can be displayed by ordinary WEB
browsers. PerfMC aims at being relatively easy to configure and operate, and
highly efficient. It is currently being used to monitor the Italian
Reprocessing farm for the BaBar experiment, which is made of about 200 dual-CPU
Linux machines."
"We report on our investigations on some technologies that can be used to
build disk servers and networks of disk servers using commodity hardware and
software solutions. It focuses on the performance that can be achieved by these
systems and gives measured figures for different configurations.
  It is divided into two parts : iSCSI and other technologies and hardware and
software RAID solutions.
  The first part studies different technologies that can be used by clients to
access disk servers using a gigabit ethernet network. It covers block access
technologies (iSCSI, hyperSCSI, ENBD). Experimental figures are given for
different numbers of clients and servers.
  The second part compares a system based on 3ware hardware RAID controllers, a
system using linux software RAID and IDE cards and a system mixing both
hardware RAID and software RAID. Performance measurements for reading and
writing are given for different RAID levels."
"We present some measurements and ideas for response time statistics in ERP
systems. It is shown that the response time distribution of a given transaction
in a given system is generically a log-normal distribution or, in some
situations, a sum of two or more log-normal distributions. We present some
arguments for this form of the distribution based on heuristic rules for
response times, and we show data from performance measurements in actual
systems to support the log-normal form. Deviations of the log-normal form can
often be traced back to performance problems in the system. Consequences for
the interpretation of response time data and for service level agreements are
discussed."
"The application of decentralized reputation systems is a promising approach
to ensure cooperation and fairness, as well as to address random failures and
malicious attacks in Mobile Ad-Hoc Networks. However, they are potentially
vulnerable to liars. With our work, we provide a first step to analyzing
robustness of a reputation system based on a deviation test. Using a mean-field
approach to our stochastic process model, we show that liars have no impact
unless their number exceeds a certain threshold (phase transition). We give
precise formulae for the critical values and thus provide guidelines for an
optimal choice of parameters."
"Decentralized search aims to find the target node in a large network by using
only local information. The applications of it include peer-to-peer file
sharing, web search and anything else that requires locating a specific target
in a complex system. In this paper, we examine the degree-based decentralized
search method. Specifically, we evaluate the efficiency of the method in
different cases with different amounts of available local information. In
addition, we propose a simple refinement algorithm for significantly shortening
the length of the route that has been found. Some insights useful for the
future developments of efficient decentralized search schemes have been
achieved."
"E-commerce Web-servers often face overload conditions during which
revenue-generating requests may be dropped or abandoned due to an increase in
the browsing requests. In this paper we present a simple, yet effective,
mechanism for overload control of E-commerce Web-servers. We develop an
E-commerce workload model that separates the browsing requests from
revenue-generating transaction requests. During overload, we apply LIFO
discipline in the browsing queues and use a dynamic priority model to service
them. The transaction queues are given absolute priority over the browsing
queues. This is called the LIFO-Pri scheduling discipline. Experimental results
show that LIFO-Pri dramatically improves the overall Web-server throughput
while also increasing the completion rate of revenue-generating requests. The
Web-server was able to operate at nearly 60% of its maximum capacity even when
offered load was 1.5 times its capacity. Further, when compared to a single
queue FIFO system, there was a seven-fold increase in the number of completed
revenue-generating requests during overload."
"This paper shows how the steady-state availability and failure frequency can
be calculated in a single pass for very large systems, when the availability is
expressed as a product of matrices. We apply the general procedure to
$k$-out-of-$n$:G and linear consecutive $k$-out-of-$n$:F systems, and to a
simple ladder network in which each edge and node may fail. We also give the
associated generating functions when the components have identical
availabilities and failure rates. For large systems, the failure rate of the
whole system is asymptotically proportional to its size. This paves the way to
ready-to-use formulae for various architectures, as well as proof that the
differential operator approach to failure frequency calculations is very useful
and straightforward."
"The exact calculation of network reliability in a probabilistic context has
been a long-standing issue of practical importance, but a difficult one, even
for planar graphs, with perfect nodes and with edges of identical reliability
p. Many approaches (determination of bounds, sums of disjoint products
algorithms, Monte Carlo evaluations, studies of the reliability polynomials,
etc.) can only provide approximations when the network's size increases. We
consider here a ladder graph of arbitrary size corresponding to real-life
network configurations, and give the exact, analytical solutions for the all-
and two-terminal reliabilities. These solutions use transfer matrices, in which
individual reliabilities of edges and nodes are taken into account. The special
case of identical edge and node reliabilities -- p and rho, respectively -- is
solved. We show that the zeros of the two-terminal reliability polynomial
exhibit structures which differ substantially for seemingly similar networks,
and we compare the sensitivity of various edges. We discuss how the present
work may be further extended to lead to a catalog of exactly solvable networks
in terms of reliability, which could be useful as elementary bricks for a new
and improved set of bounds or benchmarks in the general case."
"The two- and all-terminal reliabilities of the Brecht-Colbourn ladder and the
generalized fan have been calculated exactly for arbitrary size as well as
arbitrary individual edge and node reliabilities, using transfer matrices of
dimension four at most. While the all-terminal reliabilities of these graphs
are identical, the special case of identical edge ($p$) and node ($\rho$)
reliabilities shows that their two-terminal reliabilities are quite distinct,
as demonstrated by their generating functions and the locations of the zeros of
the reliability polynomials, which undergo structural transitions at $\rho =
\displaystyle {1/2}$."
"We present the results of gravitational direct $N$-body simulations using the
commercial graphics processing units (GPU) NVIDIA Quadro FX1400 and GeForce
8800GTX, and compare the results with GRAPE-6Af special purpose hardware. The
force evaluation of the $N$-body problem was implemented in Cg using the GPU
directly to speed-up the calculations. The integration of the equations of
motions were, running on the host computer, implemented in C using the 4th
order predictor-corrector Hermite integrator with block time steps. We find
that for a large number of particles ($N \apgt 10^4$) modern graphics
processing units offer an attractive low cost alternative to GRAPE special
purpose hardware. A modern GPU continues to give a relatively flat scaling with
the number of particles, comparable to that of the GRAPE. Using the same time
step criterion the total energy of the $N$-body system was conserved better
than to one in $10^6$ on the GPU, which is only about an order of magnitude
worse than obtained with GRAPE. For $N\apgt 10^6$ the GeForce 8800GTX was about
20 times faster than the host computer. Though still about an order of
magnitude slower than GRAPE, modern GPU's outperform GRAPE in their low cost,
long mean time between failure and the much larger onboard memory; the
GRAPE-6Af holds at most 256k particles whereas the GeForce 8800GTF can hold 9
million particles in memory."
"Grids include heterogeneous resources, which are based on different hardware
and software architectures or components. In correspondence with this diversity
of the infrastructure, the execution time of any single job, as well as the
total grid performance can both be affected substantially, which can be
demonstrated by measurements. Running a simple benchmarking suite can show this
heterogeneity and give us results about the differences over the grid sites."
"This paper presents a measurement-based availability assessment study using
field data collected during a 4-year period from 373 SunOS/Solaris Unix
workstations and servers interconnected through a local area network. We focus
on the estimation of machine uptimes, downtimes and availability based on the
identification of failures that caused total service loss. Data corresponds to
syslogd event logs that contain a large amount of information about the normal
activity of the studied systems as well as their behavior in the presence of
failures. It is widely recognized that the information contained in such event
logs might be incomplete or imperfect. The solution investigated in this paper
to address this problem is based on the use of auxiliary sources of data
obtained from wtmpx files maintained by the SunOS/Solaris Unix operating
system. The results obtained suggest that the combined use of wtmpx and syslogd
log files provides more complete information on the state of the target systems
that is useful to provide availability estimations that better reflect reality."
"We present a hierarchical simulation approach for the dependability analysis
and evaluation of a highly available commercial cache-based RAID storage
system. The archi-tecture is complex and includes several layers of
overlap-ping error detection and recovery mechanisms. Three ab-straction levels
have been developed to model the cache architecture, cache operations, and
error detection and recovery mechanism. The impact of faults and errors
oc-curring in the cache and in the disks is analyzed at each level of the
hierarchy. A simulation submodel is associated with each abstraction level. The
models have been devel-oped using DEPEND, a simulation-based environment for
system-level dependability analysis, which provides facili-ties to inject
faults into a functional behavior model, to simulate error detection and
recovery mechanisms, and to evaluate quantitative measures. Several fault
models are defined for each submodel to simulate cache component failures, disk
failures, transmission errors, and data errors in the cache memory and in the
disks. Some of the parame-ters characterizing fault injection in a given
submodel cor-respond to probabilities evaluated from the simulation of the
lower-level submodel. Based on the proposed method-ology, we evaluate and
analyze 1) the system behavior un-der a real workload and high error rate
(focusing on error bursts), 2) the coverage of the error detection mechanisms
implemented in the system and the error latency distribu-tions, and 3) the
accumulation of errors in the cache and in the disks."
"Grids include heterogeneous resources, which are based on different hardware
and software architectures or components. In correspondence with this diversity
of the infrastructure, the execution time of any single job, as well as the
total grid performance can both be affected substantially, which can be
demonstrated by measurements. Running a simple benchmarking suite can show this
heterogeneity and give us results about the differences over the grid sites."
"New nanotechnology based devices are replacing CMOS devices to overcome CMOS
technology's scaling limitations. However, many such devices exhibit
non-monotonic I-V characteristics and uncertain properties which lead to the
negative differential resistance (NDR) problem and the chaotic performance.
This paper proposes a new circuit simulation approach that can effectively
simulate nanotechnology devices with uncertain input sources and negative
differential resistance (NDR) problem. The experimental results show a 20-30
times speedup comparing with existing simulators."
"This paper presents a scheme for efficient channel usage between simulator
and accelerator where the accelerator models some RTL sub-blocks in the
accelerator-based hardware/software co-simulation while the simulator runs
transaction-level model of the remaining part of the whole chip being verified.
With conventional simulation accelerator, evaluations of simulator and
accelerator alternate at every valid simulation time, which results in poor
simulation performance due to startup overhead of simulator-accelerator channel
access. The startup overhead can be reduced by merging multiple transactions on
the channel into a single burst traffic. We propose a predictive packetizing
scheme for reducing channel traffic by merging as many transactions into a
burst traffic as possible based on 'prediction and rollback.' Under ideal
condition with 100% prediction accuracy, the proposed method shows a
performance gain of 1500% compared to the conventional one."
"This paper reports a novel simulation methodology for analysis and prediction
of substrate noise impact on analog / RF circuits taking into account the role
of the parasitic resistance of the on-chip interconnect in the impact
mechanism. This methodology allows investigation of the role of the separate
devices (also parasitic devices) in the analog / RF circuit in the overall
impact. This way is revealed which devices have to be taken care of (shielding,
topology change) to protect the circuit against substrate noise. The developed
methodology is used to analyze impact of substrate noise on a 3 GHz LC-tank
Voltage Controlled Oscillator (VCO) designed in a high-ohmic 0.18 $\mu$m 1PM6
CMOS technology. For this VCO (in the investigated frequency range from DC to
15 MHz) impact is mainly caused by resistive coupling of noise from the
substrate to the non-ideal on-chip ground interconnect, resulting in analog
ground bounce and frequency modulation. Hence, the presented test-case reveals
the important role of the on-chip interconnect in the phenomenon of substrate
noise impact."
"This paper deals with asymptotic expressions of the Mean Time To Failure
(MTTF) and higher moments for large, recursive, and non-repairable systems in
the context of two-terminal reliability. Our aim is to extend the well-known
results of the series and parallel cases. We first consider several exactly
solvable configurations of identical components with exponential failure-time
distribution functions to illustrate different (logarithmic or power-law)
behaviors as the size of the system, indexed by an integer n, increases. The
general case is then addressed: it provides a simple interpretation of the
origin of the power-law exponent and an efficient asymptotic expression for the
total reliability of large, recursive systems. Finally, we assess the influence
of the non-exponential character of the component reliability on the
n-dependence of the MTTF."
"The calculation of network reliability in a probabilistic context has long
been an issue of practical and academic importance. Conventional approaches
(determination of bounds, sums of disjoint products algorithms, Monte Carlo
evaluations, studies of the reliability polynomials, etc.) only provide
approximations when the network's size increases, even when nodes do not fail
and all edges have the same reliability p. We consider here a directed, generic
graph of arbitrary size mimicking real-life long-haul communication networks,
and give the exact, analytical solution for the two-terminal reliability. This
solution involves a product of transfer matrices, in which individual
reliabilities of edges and nodes are taken into account. The special case of
identical edge and node reliabilities (p and rho, respectively) is addressed.
We consider a case study based on a commonly-used configuration, and assess the
influence of the edges being directed (or not) on various measures of network
performance. While the two-terminal reliability, the failure frequency and the
failure rate of the connection are quite similar, the locations of complex
zeros of the two-terminal reliability polynomials exhibit strong differences,
and various structure transitions at specific values of rho. The present work
could be extended to provide a catalog of exactly solvable networks in terms of
reliability, which could be useful as building blocks for new and improved
bounds, as well as benchmarks, in the general case."
"With the advent of increasingly complex hardware in real-time embedded
systems (processors with performance enhancing features such as pipelines,
cache hierarchy, multiple cores), many processors now have a set-associative L2
cache. Thus, there is a need for considering cache hierarchies when validating
the temporal behavior of real-time systems, in particular when estimating
tasks' worst-case execution times (WCETs). To the best of our knowledge, there
is only one approach for WCET estimation for systems with cache hierarchies
[Mueller, 1997], which turns out to be unsafe for set-associative caches. In
this paper, we highlight the conditions under which the approach described in
[Mueller, 1997] is unsafe. A safe static instruction cache analysis method is
then presented. Contrary to [Mueller, 1997] our method supports set-associative
and fully associative caches. The proposed method is experimented on
medium-size and large programs. We show that the method is most of the time
tight. We further show that in all cases WCET estimations are much tighter when
considering the cache hierarchy than when considering only the L1 cache. An
evaluation of the analysis time is conducted, demonstrating that analysing the
cache hierarchy has a reasonable computation time."
"In this paper, we present a probabilistic adaptation of an Assume/Guarantee
contract formalism. For the sake of generality, we assume that the extended
state machines used in the contracts and implementations define sets of runs on
a given set of variables, that compose by intersection over the common
variables. In order to enable probabilistic reasoning, we consider that the
contracts dictate how certain input variables will behave, being either
non-deterministic, or probabilistic; the introduction of probabilistic
variables leading us to tune the notions of implementation, refinement and
composition. As shown in the report, this probabilistic adaptation of the
Assume/Guarantee contract theory preserves compositionality and therefore
allows modular reliability analysis, either with a top-down or a bottom-up
approach."
"In order to achieve both fast and coordinated data transfer to collaborative
sites as well as to create a distribution of data over multiple sites,
efficient data movement is one of the most essential aspects in distributed
environment. With such capabilities at hand, truly distributed task scheduling
with minimal latencies would be reachable by internationally distributed
collaborations (such as ones in HENP) seeking for scavenging or maximizing on
geographically spread computational resources. But it is often not all clear
(a) how to move data when available from multiple sources or (b) how to move
data to multiple compute resources to achieve an optimal usage of available
resources. We present a method of creating a Constraint Programming (CP) model
consisting of sites, links and their attributes such as bandwidth for grid
network data transfer also considering user tasks as part of the objective
function for an optimal solution. We will explore and explain trade-off between
schedule generation time and divergence from the optimal solution and show how
to improve and render viable the solution's finding time by using search tree
time limit, approximations, restrictions such as symmetry breaking or grouping
similar tasks together, or generating sequence of optimal schedules by
splitting the input problem. Results of data transfer simulation for each case
will also include a well known Peer-2-Peer model, and time taken to generate a
schedule as well as time needed for a schedule execution will be compared to a
CP optimal solution. We will additionally present a possible implementation
aimed to bring a distributed datasets (multiple sources) to a given site in a
minimal time."
"The community of program optimisation and analysis, code performance
evaluation, parallelisation and optimising compilation has published since many
decades hundreds of research and engineering articles in major conferences and
journals. These articles study efficient algorithms, strategies and techniques
to accelerate programs execution times, or optimise other performance metrics
(MIPS, code size, energy/power, MFLOPS, etc.). Many speedups are published, but
nobody is able to reproduce them exactly. The non-reproducibility of our
research results is a dark point of the art, and we cannot be qualified as {\it
computer scientists} if we do not provide rigorous experimental methodology.
This article provides a first effort towards a correct statistical protocol for
analysing and measuring speedups. As we will see, some common mistakes are done
by the community inside published articles, explaining part of the
non-reproducibility of the results. Our current article is not sufficient by
its own to deliver a complete experimental methodology, further efforts must be
done by the community to decide about a common protocol for our future
experiences. Anyway, our community should take care about the aspect of
reproducibility of the results in the future."
"We propose a new exact solution algorithm for closed multiclass product-form
queueing networks that is several orders of magnitude faster and less memory
consuming than established methods for multiclass models, such as the Mean
Value Analysis (MVA) algorithm. The technique is an important generalization of
the recently proposed Method of Moments (MoM) which, differently from MVA,
recursively computes higher-order moments of queue-lengths instead of mean
values.
  The main contribution of this paper is to prove that the information used in
the MoM recursion can be increased by considering multiple recursive branches
that evaluate models with different number of queues. This reformulation allows
to formulate a simpler matrix difference equation which leads to large
computational savings with respect to the original MoM recursion. Computational
analysis shows several cases where the proposed algorithm is between 1,000 and
10,000 times faster and less memory consuming than the original MoM, thus
extending the range of multiclass models where exact solutions are feasible."
"Does the advent of flash devices constitute a radical change for secondary
storage? How should database systems adapt to this new form of secondary
storage? Before we can answer these questions, we need to fully understand the
performance characteristics of flash devices. More specifically, we want to
establish what kind of IOs should be favored (or avoided) when designing
algorithms and architectures for flash-based systems. In this paper, we focus
on flash IO patterns, that capture relevant distribution of IOs in time and
space, and our goal is to quantify their performance. We define uFLIP, a
benchmark for measuring the response time of flash IO patterns. We also present
a benchmarking methodology which takes into account the particular
characteristics of flash devices. Finally, we present the results obtained by
measuring eleven flash devices, and derive a set of design hints that should
drive the development of flash-based systems on current devices."
"Multiprocessor task scheduling is an important and computationally difficult
problem. This paper proposes a comparison study of genetic algorithm and list
scheduling algorithm. Both algorithms are naturally parallelizable but have
heavy data dependencies. Based on experimental results, this paper presents a
detailed analysis of the scalability, advantages and disadvantages of each
algorithm. Multiprocessors have emerged as a powerful computing means for
running real-time applications, especially where a uni-processor system would
not be sufficient enough to execute all the tasks. The high performance and
reliability of multiprocessors have made them a powerful computing resource.
Such computing environment requires an efficient algorithm to determine when
and on which processor a given task should execute. In multiprocessor systems,
an efficient scheduling of a parallel program onto the processors that
minimizes the entire execution time is vital for achieving a high performance.
This scheduling problem is known to be NP- Hard. In multiprocessor scheduling
problem, a given program is to be scheduled in a given multiprocessor system
such that the program's execution time is minimized. The last job must be
completed as early as possible. Genetic algorithm (GA) is one of the widely
used techniques for constrained optimization."
"The complexity of multimedia applications in terms of intensity of
computation and heterogeneity of treated data led the designers to embark them
on multiprocessor systems on chip. The complexity of these systems on one hand
and the expectations of the consumers on the other hand complicate the
designers job to conceive and supply strong and successful systems in the
shortest deadlines. They have to explore the different solutions of the design
space and estimate their performances in order to deduce the solution that
respects their design constraints. In this context, we propose the modeling of
one of the design space possible solutions: the software to hardware task
migration. This modeling exploits the synchronous dataflow graphs to take into
account the different migration impacts and estimate their performances in
terms of throughput."
"The cache replacement algorithm plays an important role in the overall
performance of Proxy-Server system. In this paper we have proposed VoD cache
memory replacement algorithm for a multimedia server system. We propose a Rank
based cache replacement policy to manage the cache space in individual proxy
server cache. Proposed replacement strategy incorporates in a simple way the
most important characteristics of the video and its accesses such as its size,
access frequency, recentness of the last access and the cost incurred while
transferring the requested video from the server to the proxy. We compare our
algorithm with some popular cache replacement algorithm using simulation. The
video objects are ranked based on the access trend by considering the factors
such as size, frequency and cost. Many studies have demonstrated that
Zipf's-like law can govern many features of the VoD and is used to describe the
popularity of the video. In this paper, we have designed a model, which ranks
the video on the basis of its popularity using the Zipf-like law. The video
with higher ranking is named ""hot"", while the video with lower ranking is named
""cold"". The result show that the proposed rank based algorithm improves cache
hit ratio, cache byte ratio and average request latencies compared to other
algorithms. Our experimental results indicate that Rank based cache replacement
algorithm outperforms LRU, LFU and Greedy Dual."
"Parallel computing plays a major role in almost all the fields from research
to major concern problem solving purposes. Many researches are till now
focusing towards the area of parallel processing. Nowadays it extends its usage
towards the end user application such as GPU as well as multi-core processor
development. The bandwidth measurement is essential for resource management and
for studying the various performance factors of the existing super computer
systems which will be helpful for better system utilization since super
computers are very few and their resources should be properly utilized. In this
paper the real workload trace of one of the super computers LANL is taken and
shown how the bandwidth is estimated with the given parameters."
"We propose a new graph-theoretic benchmark in this paper. The benchmark is
developed to address shortcomings of an existing widely-used graph benchmark.
We thoroughly studied a large number of traditional and contemporary graph
algorithms reported in the literature to have clear understanding of their
algorithmic and run-time characteristics. Based on this study, we designed a
suite of kernels, each of which represents a specific class of graph
algorithms. The kernels are designed to capture the typical run-time behavior
of target algorithms accurately, while limiting computational and spatial
overhead to ensure its computation finishes in reasonable time. We expect that
the developed benchmark will serve as a much needed tool for evaluating
different architectures and programming models to run graph algorithms."
"Congestion is an important issue which researchers focus on in the
Transmission Control Protocol (TCP) network environment. To keep the stability
of the whole network, congestion control algorithms have been extensively
studied. Queue management method employed by the routers is one of the
important issues in the congestion control study. Active queue management (AQM)
has been proposed as a router-based mechanism for early detection of congestion
inside the network. In this paper we analyzed several active queue management
algorithms with respect to their abilities of maintaining high resource
utilization, identifying and restricting disproportionate bandwidth usage, and
their deployment complexity. We compare the performance of FRED, BLUE, SFB, and
CHOKe based on simulation results, using RED and Drop Tail as the evaluation
baseline. The characteristics of different algorithms are also discussed and
compared. Simulation is done by using Network Simulator(NS2) and the graphs are
drawn using X- graph."
"The problem of optimal allocation of monitoring resources for tracking
transactions progressing through a distributed system, modeled as a queueing
network, is considered. Two forms of monitoring information are considered,
viz., locally unique transaction identifiers, and arrival and departure
timestamps of transactions at each processing queue. The timestamps are assumed
available at all the queues but in the absence of identifiers, only enable
imprecise tracking since parallel processing can result in out-of-order
departures. On the other hand, identifiers enable precise tracking but are not
available without proper instrumentation. Given an instrumentation budget, only
a subset of queues can be selected for production of identifiers, while the
remaining queues have to resort to imprecise tracking using timestamps. The
goal is then to optimally allocate the instrumentation budget to maximize the
overall tracking accuracy. The challenge is that the optimal allocation
strategy depends on accuracies of timestamp-based tracking at different queues,
which has complex dependencies on the arrival and service processes, and the
queueing discipline. We propose two simple heuristics for allocation by
predicting the order of timestamp-based tracking accuracies of different
queues. We derive sufficient conditions for these heuristics to achieve
optimality through the notion of stochastic comparison of queues. Simulations
show that our heuristics are close to optimality, even when the parameters
deviate from these conditions."
"In this paper, we consider a two-hop relay-assisted cognitive downlink OFDMA
system (named as secondary system) dynamically accessing a spectrum licensed to
a primary network, thereby improving the efficiency of spectrum usage. A
cluster-based relay-assisted architecture is proposed for the secondary system,
where relay stations are employed for minimizing the interference to the users
in the primary network and achieving fairness for cell-edge users. Based on
this architecture, an asymptotically optimal solution is derived for jointly
controlling data rates, transmission power, and subchannel allocation to
optimize the average weighted sum goodput where the proportional fair
scheduling (PFS) is included as a special case. This solution supports
decentralized implementation, requires small communication overhead, and is
robust against imperfect channel state information at the transmitter (CSIT)
and sensing measurement. The proposed solution achieves significant throughput
gains and better user-fairness compared with the existing designs. Finally, we
derived a simple and asymptotically optimal scheduling solution as well as the
associated closed-form performance under the proportional fair scheduling for a
large number of users. The system throughput is shown to be
$\mathcal{O}\left(N(1-q_p)(1-q_p^N)\ln\ln K_c\right)$, where $K_c$ is the
number of users in one cluster, $N$ is the number of subchannels and $q_p$ is
the active probability of primary users."
"Engaging mobility with file sharing is considered very promising in today's
run Anywhere, Anytime, Anything (3As) environments. The Bittorrent file sharing
protocol can be rarely combined with the mobility scenario framework since
resources are not available due to the dynamically changing topology network.
As a result, mobility in P2P-oriented file sharing platforms, degrades the
end-to-end efficiency and the system's performance. This work proposes a new
hybridized model, which takes into account the mobility characteristics of the
combined Bittorrent protocol in a centralized manner enabling partial mobility
characteristics, where the clients of the network use a distinct technique to
differentiate between mobile and static nodes. Many parameters were taken into
consideration like the round trip delays, the diffusion process, and the
seeding techniques, targeting the maximization of the average throughput in the
clustered swarms containing mobile peers. Partial mobility characteristics are
set in a peer-tracker and peer-peer communication enhancement schema with
partial mobility, allowing an optimistic approach to attain high availability
and throughput response as simulation results show."
"The demand for Internet services that require frequent updates through small
messages, such as microblogging, has tremendously grown in the past few years.
Although the use of such applications by domestic users is usually free, their
access from mobile devices is subject to fees and consumes energy from limited
batteries. If a user activates his mobile device and is in range of a service
provider, a content update is received at the expense of monetary and energy
costs. Thus, users face a tradeoff between such costs and their messages aging.
The goal of this paper is to show how to cope with such a tradeoff, by devising
\emph{aging control policies}. An aging control policy consists of deciding,
based on the current utility of the last message received, whether to activate
the mobile device, and if so, which technology to use (WiFi or 3G). We present
a model that yields the optimal aging control policy. Our model is based on a
Markov Decision Process in which states correspond to message ages. Using our
model, we show the existence of an optimal strategy in the class of threshold
strategies, wherein users activate their mobile devices if the age of their
messages surpasses a given threshold and remain inactive otherwise. We then
consider strategic content providers (publishers) that offer \emph{bonus
packages} to users, so as to incent them to download updates of advertisement
campaigns. We provide simple algorithms for publishers to determine optimal
bonus levels, leveraging the fact that users adopt their optimal aging control
strategies. The accuracy of our model is validated against traces from the
UMass DieselNet bus network."
"This paper presents results of the performance benchmarks of the Open Source
hypervisor Xen. The study focuses on the network related performance as well as
on the application related performance of multiple virtual machines that were
running on the same Xen hypervisor. The comparison was carried out using a
self-developed benchmark suite that consists of easily available Open Source
tools. The goal is to measure the performance of the hypervisor in typical
real-world application scenarios when used for ""mass virtual hosting"", such as
hosting solutions of so called virtual private servers for small-to-medium
sized businesses environments. The results of the benchmarks show, that the
tested Xen setup offers good performance with respect to network traffic stress
tests, but only 75% of the performance of the non-virtualized reference
environment. This application performance score decreases as more virtual
machines are running simultaneously."
"In recent years, Reversible Logic is becoming more and more prominent
technology having its applications in Low Power CMOS, Quantum Computing,
Nanotechnology, and Optical Computing. Reversibility plays an important role
when energy efficient computations are considered. In this paper, Reversible
eight-bit Parallel Binary Adder/Subtractor with Design I, Design II and Design
III are proposed. In all the three design approaches, the full Adder and
Subtractors are realized in a single unit as compared to only full Subtractor
in the existing design. The performance analysis is verified using number
reversible gates, Garbage input/outputs and Quantum Cost. It is observed that
Reversible eight-bit Parallel Binary Adder/Subtractor with Design III is
efficient compared to Design I, Design II and existing design."
"We evaluate optimized parallel sparse matrix-vector operations for two
representative application areas on widespread multicore-based cluster
configurations. First the single-socket baseline performance is analyzed and
modeled with respect to basic architectural properties of standard multicore
chips. Going beyond the single node, parallel sparse matrix-vector operations
often suffer from an unfavorable communication to computation ratio. Starting
from the observation that nonblocking MPI is not able to hide communication
cost using standard MPI implementations, we demonstrate that explicit overlap
of communication and computation can be achieved by using a dedicated
communication thread, which may run on a virtual core. We compare our approach
to pure MPI and the widely used ""vector-like"" hybrid programming strategy."
"In this paper, we investigate an opportunistic relaying scheme where the
selected relay assists the source-destination (direct) communication. In our
study, we consider a regenerative opportunistic relaying scheme in which the
direct path can be considered unusable, and takes into account the effect of
the possible erroneously detected and transmitted data at the best relay. We
first derive statistics based on exact probability density function (PDF) of
each hop. Then, the PDFs are used to determine accurate closed form expressions
for end-to-end bit-error rate (BER) of binary phase-shift keying (BPSK)
modulation. Furthermore, we evaluate the asymptotical performance analysis and
the diversity order is deduced. Finally, we validate our analysis by showing
that performance simulation results coincide with our analytical results over
different network architectures."
"We investigate the scheduling of a common resource between several concurrent
users when the feasible transmission rate of each user varies randomly over
time. Time is slotted and users arrive and depart upon service completion. This
may model for example the flow-level behavior of end-users in a narrowband HDR
wireless channel (CDMA 1xEV-DO). As performance criteria we consider the
stability of the system and the mean delay experienced by the users. Given the
complexity of the problem we investigate the fluid-scaled system, which allows
to obtain important results and insights for the original system: (1) We
characterize for a large class of scheduling policies the stability conditions
and identify a set of maximum stable policies, giving in each time slot
preference to users being in their best possible channel condition. We find in
particular that many opportunistic scheduling policies like Score-Based,
Proportionally Best or Potential Improvement are stable under the maximum
stability conditions, whereas the opportunistic scheduler Relative-Best or the
cmu-rule are not. (2) We show that choosing the right tie-breaking rule is
crucial for the performance (e.g. average delay) as perceived by a user. We
prove that a policy is asymptotically optimal if it is maximum stable and the
tie-breaking rule gives priority to the user with the highest departure
probability. We will refer to such tie-breaking rule as myopic. (3) We derive
the growth rates of the number of users in the system in overload settings
under various policies, which give additional insights on the performance. (4)
We conclude that simple priority-index policies with the myopic tie-breaking
rule, are stable and asymptotically optimal. All our findings are validated
with extensive numerical experiments."
"This paper has been withdrawn by the author due to a crucial problem in Lemma
3. This equation must be changed."
"Performance curves of queuing systems can be analyzed by separating them into
three regions: the flat region, the knee region, and the exponential region.
Practical considerations, usually locate the knee region between 70-90% of the
theoretical maximum utilization. However, there is not a clear agreement about
where the boundaries between regions are, and where exactly the utilization
knee is located. An open debate about knees in performance curves was
undertaken at least 20 years ago. This historical debate is mainly divided
between those who claim that a knee in the curve is not a well-defined term in
mathematics, or it is a subjective and not really meaningful concept, and those
who define knees mathematically and consider their relevance and application.
In this paper, we present a mathematical model and analysis for identifying the
three mentioned regions on performance curves for M/M/1 systems; specifically,
we found the knees, or optimal utilization percentiles, at the vertices of the
hyperbolas that relate response time as a function of utilization. Using these
results, we argue that an adaptive and optimal queuing system could be deployed
by keeping load and throughput within the knee region."
"Agarwal et al. gave an closed-form expression for write amplification in NAND
flash memory by finding the probability of a page being valid over the whole
flash memory. This paper gives an improved analytic expression for write
amplification in NAND flash memory by finding the probability of a page being
invalid over the block selected for garbage collection. The improved expression
uses Lambert W function. Through asymptotic analysis, write amplification is
shown to depend on overprovisioning factor only, consistent with the previous
work. Comparison with numerical simulations shows that the improved expression
achieves a more accurate prediction of write amplification. For example, when
the overprovisioning factor is 0.3, the expression proposed by this paper gives
a write amplification of 2.36 whereas that of the previous work gives 2.17,
when the actual value is 2.35."
"In this tutorial paper, a comprehensive survey is given on several major
systematic approaches in dealing with delay-aware control problems, namely the
equivalent rate constraint approach, the Lyapunov stability drift approach and
the approximate Markov Decision Process (MDP) approach using stochastic
learning. These approaches essentially embrace most of the existing literature
regarding delay-aware resource control in wireless systems. They have their
relative pros and cons in terms of performance, complexity and implementation
issues. For each of the approaches, the problem setup, the general solution and
the design methodology are discussed. Applications of these approaches to
delay-aware resource allocation are illustrated with examples in single-hop
wireless networks. Furthermore, recent results regarding delay-aware multi-hop
routing designs in general multi-hop networks are elaborated. Finally, the
delay performance of the various approaches are compared through simulations
using an example of the uplink OFDMA systems."
"We develop a generalized loss network framework for capacity planning of a
perinatal network in the UK. Decomposing the network by hospitals, each unit is
analyzed with a GI/G/c/0 overflow loss network model. A two-moment
approximation is performed to obtain the steady state solution of the GI/G/c/0
loss systems, and expressions for rejection probability and overflow
probability have been derived. Using the model framework, the number of
required cots can be estimated based on the rejection probability at each level
of care of the neonatal units in a network. The generalization ensures that the
model can be applied to any perinatal network for renewal arrival and discharge
processes."
"Energy consumption represents a significant cost in data center operation. A
large fraction of the energy, however, is used to power idle servers when the
workload is low. Dynamic provisioning techniques aim at saving this portion of
the energy, by turning off unnecessary servers. In this paper, we explore how
much performance gain can knowing future workload information brings to dynamic
provisioning. In particular, we study the dynamic provisioning problem under
the cost model that a running server consumes a fixed amount energy per unit
time, and develop online solutions with and without future workload information
available. We first reveal an elegant structure of the off-line dynamic
provisioning problem, which allows us to characterize and achieve the optimal
solution in a {}""divide-and-conquer"" manner. We then exploit this insight to
design three online algorithms with competitive ratios $2-\alpha$,
$(e-\alpha)/(e-1)\approx1.58-\alpha/(e-1)$ and $e/(e-1+\alpha)$, respectively,
where $0\leq\alpha\leq1$ is the fraction of a critical window in which future
workload information is available. A fundamental observation is that
\emph{future workload information beyond the critical window will not}
\emph{improve dynamic provisioning performance}. Our algorithms are
decentralized and are simple to implement. We demonstrate their effectiveness
in simulations using real-world traces. We also compare their performance with
state-of-the-art solutions."
"GPUs offer several times the floating point performance and memory bandwidth
of current standard two socket CPU servers, e.g. NVIDIA C2070 vs. Intel Xeon
Westmere X5650. The lattice Boltzmann method has been established as a flow
solver in recent years and was one of the first flow solvers to be successfully
ported and that performs well on GPUs. We demonstrate advanced optimization
strategies for a D3Q19 lattice Boltzmann based incompressible flow solver for
GPGPUs and CPUs based on NVIDIA CUDA and OpenCL. Since the implemented
algorithm is limited by memory bandwidth, we concentrate on improving memory
access. Basic data layout issues for optimal data access are explained and
discussed. Furthermore, the algorithmic steps are rearranged to improve
scattered access of the GPU memory. The importance of occupancy is discussed as
well as optimization strategies to improve overall concurrency. We arrive at a
well-optimized GPU kernel, which is integrated into a larger framework that can
handle single phase fluid flow simulations as well as particle-laden flows. Our
3D LBM GPU implementation reaches up to 650 MLUPS in single precision and 290
MLUPS in double precision on an NVIDIA Tesla C2070."
"Queueing networks are gaining attraction for the performance analysis of
parallel computer systems. A Jackson network is a set of interconnected
servers, where the completion of a job at server i may result in the creation
of a new job for server j. We propose to extend Jackson networks by ""branching""
and by ""control"" features. Both extensions are new and substantially expand the
modelling power of Jackson networks. On the other hand, the extensions raise
computational questions, particularly concerning the stability of the networks,
i.e, the ergodicity of the underlying Markov chain. We show for our extended
model that it is decidable in polynomial time if there exists a controller that
achieves stability. Moreover, if such a controller exists, one can efficiently
compute a static randomized controller which stabilizes the network in a very
strong sense; in particular, all moments of the queue sizes are finite."
"Stochastic network calculus is a newly developed theory for stochastic
service guarantee analysis of computer networks. In the current stochastic
network calculus literature, its fundamental models are based on the cumulative
amount of traffic or cumulative amount of service. However, there are network
scenarios where direct application of such models is difficult. This paper
presents a temporal approach to stochastic network calculus. The key idea is to
develop models and derive results from the time perspective. Particularly, we
define traffic models and service models based on the cumulative packet
inter-arrival time and the cumulative packet service time, respectively.
Relations among these models as well as with the existing models in the
literature are established. In addition, we prove the basic properties of the
proposed models, such as delay bound and backlog bound, output
characterization, concatenation property and superposition property. These
results form a temporal stochastic network calculus and compliment the existing
results."
"Web Service is an interface which implements business logic. Performance is
an important quality aspect of Web services because of their distributed
nature. Predicting the performance of web services during early stages of
software development is significant. In this paper we model web service using
Unified Modeling Language, Use Case Diagram, Sequence Diagram, Deployment
Diagram. We obtain the Performance metrics by simulating the web services model
using a simulation tool Simulation of Multi-Tier Queuing Architecture. We have
identified the bottle neck resources."
"Several emerging petascale architectures use energy-efficient processors with
vectorized computational units and in-order thread processing. On these
architectures the sustained performance of streaming numerical kernels,
ubiquitous in the solution of partial differential equations, represents a
challenge despite the regularity of memory access. Sophisticated optimization
techniques are required to fully utilize the Central Processing Unit (CPU).
  We propose a new method for constructing streaming numerical kernels using a
high-level assembly synthesis and optimization framework. We describe an
implementation of this method in Python targeting the IBM Blue Gene/P
supercomputer's PowerPC 450 core. This paper details the high-level design,
construction, simulation, verification, and analysis of these kernels utilizing
a subset of the CPU's instruction set.
  We demonstrate the effectiveness of our approach by implementing several
three-dimensional stencil kernels over a variety of cached memory scenarios and
analyzing the mechanically scheduled variants, including a 27-point stencil
achieving a 1.7x speedup over the best previously published results."
"We consider a system of parallel queues where tasks are assigned (dispatched)
to one of the available servers upon arrival. The dispatching decision is based
on the full state information, i.e., on the sizes of the new and existing jobs.
We are interested in minimizing the so-called mean slowdown criterion
corresponding to the mean of the sojourn time divided by the processing time.
Assuming no new jobs arrive, the shortest-processing-time-product (SPTP)
schedule is known to minimize the slowdown of the existing jobs. The main
contribution of this paper is three-fold: 1) To show the optimality of SPTP
with respect to slowdown in a single server queue under Poisson arrivals; 2) to
derive the so-called size-aware value functions for
M/G/1-FIFO/LIFO/SPTP/SPT/SRPT with general holding costs of which the slowdown
criterion is a special case; and 3) to utilize the value functions to derive
efficient dispatching policies so as to minimize the mean slowdown in a
heterogeneous server system. The derived policies offer a significantly better
performance than e.g., the size-aware-task-assignment with equal load (SITA-E)
and least-work-left (LWL) policies."
"Perceived responsiveness of a web page is one of the most important and least
understood metrics of web page design, and is critical for attracting and
maintaining a large audience. Web pages can be designed to meet performance
SLAs early in the product lifecycle if there is a way to predict the apparent
responsiveness of a particular page layout. Response time of a web page is
largely influenced by page layout and various network characteristics. Since
the network characteristics vary widely from country to country, accurately
modeling and predicting the perceived responsiveness of a web page from the end
user's perspective has traditionally proven very difficult. We propose a model
for predicting end user web page response time based on web page, network,
browser download and browser rendering characteristics. We start by
understanding the key parameters that affect perceived response time. We then
model each of these parameters individually using experimental tests and
statistical techniques. Finally, we demonstrate the effectiveness of this model
by conducting an experimental study with Yahoo! web pages in two countries and
compare it with 3rd party measurement application."
"Due to the inherent aleatory uncertainties in renewable generators, the
reliability/adequacy assessments of distributed generation (DG) systems have
been particularly focused on the probabilistic modeling of random behaviors,
given sufficient informative data. However, another type of uncertainty
(epistemic uncertainty) must be accounted for in the modeling, due to
incomplete knowledge of the phenomena and imprecise evaluation of the related
characteristic parameters. In circumstances of few informative data, this type
of uncertainty calls for alternative methods of representation, propagation,
analysis and interpretation. In this study, we make a first attempt to
identify, model, and jointly propagate aleatory and epistemic uncertainties in
the context of DG systems modeling for adequacy assessment. Probability and
possibility distributions are used to model the aleatory and epistemic
uncertainties, respectively. Evidence theory is used to incorporate the two
uncertainties under a single framework. Based on the plausibility and belief
functions of evidence theory, the hybrid propagation approach is introduced. A
demonstration is given on a DG system adapted from the IEEE 34 nodes
distribution test feeder. Compared to the pure probabilistic approach, it is
shown that the hybrid propagation is capable of explicitly expressing the
imprecision in the knowledge on the DG parameters into the final adequacy
values assessed. It also effectively captures the growth of uncertainties with
higher DG penetration levels."
"The problem addressed in this paper is the analysis of a distributed
consensus algorithm for arbitrary networks, proposed by B\'en\'ezit et al.. In
the initial setting, each node in the network has one of two possible states
(""yes"" or ""no""). Nodes can update their states by communicating with their
neighbors via a 2-bit message in an asynchronous clock setting. Eventually, all
nodes reach consensus on the majority states. We use the theory of electric
networks, random walks, and couplings of Markov chains to derive an O(N4 logN)
upper bound for the expected convergence time on an arbitrary graph of size N."
"The paper refers to CRUTIAL, CRitical UTility InfrastructurAL Resilience, a
European project within the research area of Critical Information
Infrastructure Protection, with a specific focus on the infrastructures
operated by power utilities, widely recognized as fundamental to national and
international economy, security and quality of life. Such infrastructures faced
with the recent market deregulations and the multiple interdependencies with
other infrastructures are becoming more and more vulnerable to various threats,
including accidental failures and deliberate sabotage and malicious attacks.
The subject of CRUTIAL research are small scale networked ICT systems used to
control and manage the electric power grid, in which artifacts controlling the
physical process of electricity transportation need to be connected with
corporate and societal applications performing management and maintenance
functionality. The peculiarity of such ICT-supported systems is that they are
related to the power system dynamics and its emergency conditions. Specific
effort need to be devoted by the Electric Power community and by the
Information Technology community to influence the technological progress in
order to allow commercial intelligent electronic devices to be effectively
deployed for the protection of citizens against cyber threats to electric power
management and control systems. A well-founded know-how needs to be built
inside the industrial power sector to allow all the involved stakeholders to
achieve their service objectives without compromising the resilience properties
of the logical and physical assets that support the electric power provision."
"This paper summarizes the state of knowledge and ongoing research on methods
and techniques for resilience evaluation, taking into account the
resilience-scaling challenges and properties related to the ubiquitous
computerized systems. We mainly focus on quantitative evaluation approaches
and, in particular, on model-based evaluation techniques that are commonly used
to evaluate and compare, from the dependability point of view, different
architecture alternatives at the design stage. We outline some of the main
modeling techniques aiming at mastering the largeness of analytical
dependability models at the construction level. Actually, addressing the model
largeness problem is important with respect to the investigation of the
scalability of current techniques to meet the complexity challenges of
ubiquitous systems. Finally we present two case studies in which some of the
presented techniques are applied for modeling web services and General Packet
Radio Service (GPRS) mobile telephone networks, as prominent examples of large
and evolving systems."
"This paper presents a set of models dedicated to describe a flash storage
subsystem structure, functions, performance and power consumption behaviors.
These models cover a large range of today's NAND flash memory applications.
They are designed to be implemented in simulation tools allowing to estimate
and compare performance and power consumption of I/O requests on flash memory
based storage systems. Such tools can also help in designing and validating new
flash storage systems and management mechanisms. This work is integrated in a
global project aiming to build a framework simulating complex flash storage
hierarchies for performance and power consumption analysis. This tool will be
highly configurable and modular with various levels of usage complexity
according to the required aim: from a software user point of view for
simulating storage systems, to a developer point of view for designing, testing
and validating new flash storage management systems."
"This paper investigates the throughput capacity of a flow crossing a
multi-hop wireless network, whose geometry is characterized by general
randomness laws including Uniform, Poisson, Heavy-Tailed distributions for both
the nodes' densities and the number of hops. The key contribution is to
demonstrate \textit{how} the \textit{per-flow throughput} depends on the
distribution of 1) the number of nodes $N_j$ inside hops' interference sets, 2)
the number of hops $K$, and 3) the degree of spatial correlations. The
randomness in both $N_j$'s and $K$ is advantageous, i.e., it can yield larger
scalings (as large as $\Theta(n)$) than in non-random settings. An interesting
consequence is that the per-flow capacity can exhibit the opposite behavior to
the network capacity, which was shown to suffer from a logarithmic decrease in
the presence of randomness. In turn, spatial correlations along the end-to-end
path are detrimental by a logarithmic term."
"Now we live in an era of big data, and big data applications are becoming
more and more pervasive. How to benchmark data center computer systems running
big data applications (in short big data systems) is a hot topic. In this
paper, we focus on measuring the performance impacts of diverse applications
and scalable volumes of data sets on big data systems. For four typical data
analysis applications---an important class of big data applications, we find
two major results through experiments: first, the data scale has a significant
impact on the performance of big data systems, so we must provide scalable
volumes of data sets in big data benchmarks. Second, for the four applications,
even all of them use the simple algorithms, the performance trends are
different with increasing data scales, and hence we must consider not only
variety of data sets but also variety of applications in benchmarking big data
systems."
"As the amount of data explodes rapidly, more and more corporations are using
data centers to make effective decisions and gain a competitive edge. Data
analysis applications play a significant role in data centers, and hence it has
became increasingly important to understand their behaviors in order to further
improve the performance of data center computer systems. In this paper, after
investigating three most important application domains in terms of page views
and daily visitors, we choose eleven representative data analysis workloads and
characterize their micro-architectural characteristics by using hardware
performance counters, in order to understand the impacts and implications of
data analysis workloads on the systems equipped with modern superscalar
out-of-order processors. Our study on the workloads reveals that data analysis
applications share many inherent characteristics, which place them in a
different class from desktop (SPEC CPU2006), HPC (HPCC), and service workloads,
including traditional server workloads (SPECweb2005) and scale-out service
workloads (four among six benchmarks in CloudSuite), and accordingly we give
several recommendations for architecture and system optimizations. On the basis
of our workload characterization work, we released a benchmark suite named
DCBench for typical datacenter workloads, including data analysis and service
workloads, with an open-source license on our project home page on
http://prof.ict.ac.cn/DCBench. We hope that DCBench is helpful for performing
architecture and small-to-medium scale system researches for datacenter
computing."
"Queueing systems with a single server in which customers wait to be served at
a finite number of distinct locations (buffers/queues) are called discrete
polling systems. Polling systems in which arrivals of users occur anywhere in a
continuum are called continuous polling systems. Often one encounters a
combination of the two systems: the users can either arrive in a continuum or
wait in a finite set (i.e. wait at a finite number of queues). We call these
systems mixed polling systems. Also, in some applications, customers are
rerouted to a new location (for another service) after their service is
completed. In this work, we study mixed polling systems with rerouting. We
obtain their steady state performance by discretization using the known pseudo
conservation laws of discrete polling systems. Their stationary expected
workload is obtained as a limit of the stationary expected workload of a
discrete system. The main tools for our analysis are: a) the fixed point
analysis of infinite dimensional operators and; b) the convergence of Riemann
sums to an integral.
  We analyze two applications using our results on mixed polling systems and
discuss the optimal system design. We consider a local area network, in which a
moving ferry facilitates communication (data transfer) using a wireless link.
We also consider a distributed waste collection system and derive the optimal
collection point. In both examples, the service requests can arrive anywhere in
a subset of the two dimensional plane. Namely, some users arrive in a
continuous set while others wait for their service in a finite set. The only
polling systems that can model these applications are mixed systems with
rerouting as introduced in this manuscript."
"Performance analysis based on modelling consists of two major steps: model
construction and model analysis. Formal modelling techniques significantly aid
model construction but can exacerbate model analysis. In particular, here we
consider the analysis of large-scale systems which consist of one or more
entities replicated many times to form large populations. The replication of
entities in such models can cause their state spaces to grow exponentially to
the extent that their exact stochastic analysis becomes computationally
expensive or even infeasible.
  In this paper, we propose a new approximate aggregation algorithm for a class
of large-scale PEPA models. For a given model, the method quickly checks if it
satisfies a syntactic condition, indicating that the model may be solved
approximately with high accuracy. If so, an aggregated CTMC is generated
directly from the model description. This CTMC can be used for efficient
derivation of an approximate marginal probability distribution over some of the
model's populations. In the context of a large-scale client-server system, we
demonstrate the usefulness of our method."
"Scientific software applications are increasingly developed by large
interdiscplinary teams operating on functional modules organized around a
common software framework, which is capable of integrating new functional
capabilities without modifying the core of the framework. In such environment,
software correctness and modularity take precedence at the expense of code
performance, which is an important concern during execution on supercomputing
facilities, where the allocation of core-hours is a valuable resource. To
alleviate the performance problems, we propose automated performance tuning
(autotuning) of software to extract the maximum performance on a given hardware
platform and to enable performance portability across heterogeneous hardware
platforms. The resulting code remains generic without committing to a
particular software stack and yet is compile-time specializable for maximal
sustained performance."
"This paper treats power-aware throughput maximization in a multi-user file
downloading system. Each user can receive a new file only after its previous
file is finished. The file state processes for each user act as coupled Markov
chains that form a generalized restless bandit system. First, an optimal
algorithm is derived for the case of one user. The algorithm maximizes
throughput subject to an average power constraint. Next, the one-user algorithm
is extended to a low complexity heuristic for the multi-user problem. The
heuristic uses a simple online index policy and its effectiveness is shown via
simulation. For simple 3-user cases where the optimal solution can be computed
offline, the heuristic is shown to be near-optimal for a wide range of
parameters."
"We propose a set of benchmarks that specifically targets a major cause of
performance degradation in high performance computing platforms: irregular
access patterns. These benchmarks are meant to be used to asses the performance
of optimizing compilers on codes with a varying degree of irregular access. The
irregularity caused by the use of pointers and indirection arrays are a major
challenge for optimizing compilers. Codes containing such patterns are
notoriously hard to optimize but they have a huge impact on the performance of
modern architectures, which are under-utilized when encountering irregular
memory accesses. In this paper, a set of benchmarks is described that
explicitly measures the performance of kernels containing a variety of
different access patterns found in real world applications. By offering a
varying degree of complexity, we provide a platform for measuring the
effectiveness of transformations. The difference in complexity stems from a
difference in traversal patterns, the use of multiple indirections and control
flow statements. The kernels used cover a variety of different access patterns,
namely pointer traversals, indirection arrays, dynamic loop bounds and run-time
dependent if-conditions. The kernels are small enough to be fully understood
which makes this benchmark set very suitable for the evaluation of
restructuring transformations."
"Maintenance plays now a critical role in manufacturing for achieving
important cost savings and competitive advantage while preserving product
conditions. It suggests moving from conventional maintenance practices to
predictive strategy. Indeed the maintenance action has to be done at the right
time based on the system performance and component Remaining Useful Life (RUL)
assessed by a prognostic process. In that way, this paper proposes a
methodology in order to evaluate the performance loss of the system according
to the degradation of component and the deviations of system input flows. This
methodology is supported by the neuro-fuzzy tool ANFIS (Adaptive Neuro-Fuzzy
Inference Systems) that allows to integrate knowledge from two different
sources: expertise and real data. The feasibility and added value of such
methodology is then highlighted through an application case extracted from the
TELMA platform used for education and research."
"In the present paper, the transmission performance analysis of digital wire
and wireless optical links in local and wide areas optical networks have been
modeled and parametrically investigated over wide range of the affecting
parameters. Moreover, we have analyzed the basic equations of the comparative
study of the performance of digital fiber optic links with wire and wireless
optical links. The development of optical wireless communication systems is
accelerating as a high cost effective to wire fiber optic links. The optical
wireless technology is used mostly in wide bandwidth data transmission
applications. Finally, we have investigated the maximum transmission distance
and data transmission bit rates that can be achieved within digital wire and
wireless optical links for local and wide areas optical network applications."
"In IEEE 802.11, load balancing algorithms (LBA) consider only the associated
stations to balance the load of the available access points (APs). However,
although the APs are balanced, it causes a bad situation if the AP has a lower
signal length (SNR) less than the neighbor APs. So, balance the load and
associate one mobile station to an access point without care about the signal
to noise ratio (SNR) of the AP cause possibly an unforeseen QoS, such as the
bit rate, the end to end delay, the packet loss. In this way, we study an
improvement load balancing algorithm with SNR integration at the selection
policy."
"Typical constraints on embedded systems include code size limits, upper
bounds on energy consumption and hard or soft deadlines. To meet these
requirements, it may be necessary to improve the software by applying various
kinds of transformations like compiler optimizations, specific mapping of code
and data in the available memories, code compression, etc. However, a
transformation that aims at improving the software with respect to a given
criterion might engender side effects on other criteria and these effects must
be carefully analyzed. For this purpose, we have developed a common framework
that makes it possible to experiment various code transfor-mations and to
evaluate their impact of various criteria. This work has been carried out
within the French ANR MORE project."
"In this paper, we design an analytically and experimentally better online
energy and job scheduling algorithm with the objective of maximizing net profit
for a service provider in green data centers. We first study the previously
known algorithms and conclude that these online algorithms have provable poor
performance against their worst-case scenarios. To guarantee an online
algorithm's performance in hindsight, we design a randomized algorithm to
schedule energy and jobs in the data centers and prove the algorithm's expected
competitive ratio in various settings. Our algorithm is theoretical-sound and
it outperforms the previously known algorithms in many settings using both real
traces and simulated data. An optimal offline algorithm is also implemented as
an empirical benchmark."
"This is the Proceedings of the 1st OMNeT++ Community Summit, which was held
in Hamburg, Germany, September 2, 2014."
"Big data benchmark suites must include a diversity of data and workloads to
be useful in fairly evaluating big data systems and architectures. However,
using truly comprehensive benchmarks poses great challenges for the
architecture community. First, we need to thoroughly understand the behaviors
of a variety of workloads. Second, our usual simulation-based research methods
become prohibitively expensive for big data. As big data is an emerging field,
more and more software stacks are being proposed to facilitate the development
of big data applications, which aggravates hese challenges. In this paper, we
first use Principle Component Analysis (PCA) to identify the most important
characteristics from 45 metrics to characterize big data workloads from
BigDataBench, a comprehensive big data benchmark suite. Second, we apply a
clustering technique to the principle components obtained from the PCA to
investigate the similarity among big data workloads, and we verify the
importance of including different software stacks for big data benchmarking.
Third, we select seven representative big data workloads by removing redundant
ones and release the BigDataBench simulation version, which is publicly
available from http://prof.ict.ac.cn/BigDataBench/simulatorversion/."
"In 2013 Intel introduced the Xeon Phi, a new parallel co-processor board. The
Xeon Phi is a cache-coherent many-core shared memory architecture claiming
CPU-like versatility, programmability, high performance, and power efficiency.
The first published micro-benchmark studies indicate that many of Intel's
claims appear to be true. The current paper is the first study on the Phi of a
complex artificial intelligence application. It contains an open source MCTS
application for playing tournament quality Go (an oriental board game). We
report the first speedup figures for up to 240 parallel threads on a real
machine, allowing a direct comparison to previous simulation studies. After a
substantial amount of work, we observed that performance scales well up to 32
threads, largely confirming previous simulation results of this Go program,
although the performance surprisingly deteriorates between 32 and 240 threads.
Furthermore, we report (1) unexpected performance anomalies between the Xeon
Phi and Xeon CPU for small problem sizes and small numbers of threads, and (2)
that performance is sensitive to scheduling choices. Achieving good performance
on the Xeon Phi for complex programs is not straightforward; it requires a deep
understanding of (1) search patterns, (2) of scheduling, and (3) of the
architecture and its many cores and caches. In practice, the Xeon Phi is less
straightforward to program for than originally envisioned by Intel."
"Countless applications cast their computational core in terms of dense linear
algebra operations. These operations can usually be implemented by combining
the routines offered by standard linear algebra libraries such as BLAS and
LAPACK, and typically each operation can be obtained in many alternative ways.
Interestingly, identifying the fastest implementation -- without executing it
-- is a challenging task even for experts. An equally challenging task is that
of tuning each routine to performance-optimal configurations. Indeed, the
problem is so difficult that even the default values provided by the libraries
are often considerably suboptimal; as a solution, normally one has to resort to
executing and timing the routines, driven by some form of parameter search. In
this paper, we discuss a methodology to solve both problems: identifying the
best performing algorithm within a family of alternatives, and tuning
algorithmic parameters for maximum performance; in both cases, we do not
execute the algorithms themselves. Instead, our methodology relies on timing
and modeling the computational kernels underlying the algorithms, and on a
technique for tracking the contents of the CPU cache. In general, our
performance predictions allow us to tune dense linear algebra algorithms within
few percents from the best attainable results, thus allowing computational
scientists and code developers alike to efficiently optimize their linear
algebra routines and codes."
"New information technologies provide a lot of prospects for performance
improvement. One of them is ""Dynamic Source Code Generation and Compilation"".
This article shows how this way provides high performance for engineering
problems."
"In this letter, we present a closed-form approximation of the outage
probability for the multi-hop amplify-and-forward (AF) relaying systems with
fixed gain in Rayleigh fading channel. The approximation is derived from the
outage event for each hop. The simulation results show the tightness of the
proposed approximation in low and high signal-to-noise ratio (SNR) region."
"The efficiency and the performance of anagement systems is becoming a hot
research topic within the networks and services management community. This
concern is due to the new challenges of large scale managed systems, where the
management plane is integrated within the functional plane and where management
activities have to carry accurate and up-to-date information. We defined a set
of primary and secondary metrics to measure the performance of a management
approach. Secondary metrics are derived from the primary ones and quantifies
mainly the efficiency, the scalability and the impact of management activities.
To validate our proposals, we have designed and developed a benchmarking
platform dedicated to the measurement of the performance of a JMX manager-agent
based management system. The second part of our work deals with the collection
of measurement data sets from our JMX benchmarking platform. We mainly studied
the effect of both load and the number of agents on the scalability, the impact
of management activities on the user perceived performance of a managed server
and the delays of JMX operations when carrying variables values. Our findings
show that most of these delays follow a Weibull statistical distribution. We
used this statistical model to study the behavior of a monitoring algorithm
proposed in the literature, under heavy tail delays distribution. In this case,
the view of the managed system on the manager side becomes noisy and out of
date."
"Wimax (Worldwide Interoperability for Microwave Access) is a promising
technology which can offer high speed voice, video and data service up to the
customer end. The aim of this paper is the performance evaluation of an Wimax
system under different combinations of digital modulation (BPSK, QPSK, 4 QAM
and 16 QAM) and different communication channels AWGN and fading channels
(Rayleigh and Rician). And the Wimax system incorporates Reed Solomon (RS)
encoder with Convolutional encoder with half and two third rated codes in FEC
channel coding. The simulation results of estimated Bit Error Rate (BER)
displays that the implementation of interleaved RS code (255, 239, 8) with two
third rated Convolutional code under BPSK modulation technique is highly
effective to combat in the Wimax communication system. To complete this
performance analysis in Wimax based systems, a segment of audio signal is used
for analysis. The transmitted audio message is found to have retrieved
effectively under noisy situation."
"The increasing importance of multicore processors calls for a reevaluation of
established numerical algorithms in view of their ability to profit from this
new hardware concept. In order to optimize the existent algorithms, a detailed
knowledge of the different performance-limiting factors is mandatory. In this
contribution we investigate sparse matrix-vector multiplication, which is the
dominant operation in many sparse eigenvalue solvers. Two conceptually
different storage schemes and computational kernels have been conceived in the
past to target cache-based and vector architectures, respectively. Starting
from a series of microbenchmarks we apply the gained insight on optimized
sparse MVM implementations, whose serial and OpenMP-parallel performance we
review on state-of-the-art multicore systems."
"This article reports on first results of the KONWIHR-II project OMI4papps at
the Leibniz Supercomputing Centre (LRZ). The first part describes Apex-MAP, a
tunable synthetic benchmark designed to simulate the performance of typical
scientific applications. Apex-MAP mimics common memory access patterns and
different computational intensity of scientific codes. An approach for
modelling LRZ's application mix is given whichh makes use of performance
counter measurements of real applications running on ""HLRB II"", an SGI Altix
system based on 9728 Intel Montecito dual-cores.
  The second part will show how the Apex-MAP benchmark could be used to
simulate the performance of two mathematical kernels frequently used in
scientific applications: a dense matrix-matrix multiplication and a sparse
matrix-vector multiplication. The performance of both kernels has been
intensively studied on x86 cores and hardware accelerators. We will compare the
predicted performance with measured data to validate our Apex-MAP approach."
"Real time systems are systems in which there is a commitment for timely
response by the computer to external stimuli. Real time applications have to
function correctly even in presence of faults. Fault tolerance can be achieved
by either hardware or software or time redundancy. Safety-critical applications
have strict time and cost constraints, which means that not only faults have to
be tolerated but also the constraints should be satisfied. Deadline scheduling
means that the taskwith the earliest required response time is processed. The
most common scheduling algorithms are :Rate Monotonic(RM) and Earliest deadline
first(EDF).This paper deals with the interaction between the fault tolerant
strategy and the EDF real time scheduling strategy."
"To analyze complex and heterogeneous real-time embedded systems, recent works
have proposed interface techniques between real-time calculus (RTC) and timed
automata (TA), in order to take advantage of the strengths of each technique
for analyzing various components. But the time to analyze a state-based
component modeled by TA may be prohibitively high, due to the state space
explosion problem. In this paper, we propose a framework of granularity-based
interfacing to speed up the analysis of a TA modeled component. First, we
abstract fine models to work with event streams at coarse granularity. We
perform analysis of the component at multiple coarse granularities and then
based on RTC theory, we derive lower and upper bounds on arrival patterns of
the fine output streams using the causality closure algorithm. Our framework
can help to achieve tradeoffs between precision and analysis time."
"Stochastic network calculus provides an elegant way to characterize traffic
and service processes. However, little effort has been made on applying it to
multi-access communication systems such as 802.11. In this paper, we take the
first step to apply it to the backlog and delay analysis of an 802.11 wireless
local network. In particular, we address the following questions: In applying
stochastic network calculus, under what situations can we derive stable backlog
and delay bounds? How to derive the backlog and delay bounds of an 802.11
wireless node? And how tight are these bounds when compared with simulations?
To answer these questions, we first derive the general stability condition of a
wireless node (not restricted to 802.11). From this, we give the specific
stability condition of an 802.11 wireless node. Then we derive the backlog and
delay bounds of an 802.11 node based on an existing model of 802.11. We observe
that the derived bounds are loose when compared with ns-2 simulations,
indicating that improvements are needed in the current version of stochastic
network calculus."
"The AMTHA (Automatic Mapping Task on Heterogeneous Architectures) algorithm
for task-to-processors assignment and the MPAHA (Model of Parallel Algorithms
on Heterogeneous Architectures) model are presented. The use of AMTHA is
analyzed for multicore processor-based architectures, considering the
communication model among processes in use. The results obtained in the tests
carried out are presented, comparing the real execution times on multicores of
a set of synthetic applications with the predictions obtained with AMTHA.
Finally current lines of research are presented, focusing on clusters of
multicores and hybrid programming paradigms."
"In this paper, we compare two analytical models for evaluation of cache
coherence overhead of a shared bus multiprocessor with private caches. The
models are based on a closed queuing network with different service
disciplines. We find that the priority discipline can be used as a lower-level
bound. Some numerical results are shown graphically."
"We study the problem of scheduling tasks for execution by a processor when
the tasks can stochastically generate new tasks. Tasks can be of different
types, and each type has a fixed, known probability of generating other tasks.
We present results on the random variable S^sigma modeling the maximal space
needed by the processor to store the currently active tasks when acting under
the scheduler sigma. We obtain tail bounds for the distribution of S^sigma for
both offline and online schedulers, and investigate the expected value of
S^sigma."
"Modern high-end machines feature multiple processor packages, each of which
contains multiple independent cores and integrated memory controllers connected
directly to dedicated physical RAM. These packages are connected via a shared
bus, creating a system with a heterogeneous memory hierarchy. Since this shared
bus has less bandwidth than the sum of the links to memory, aggregate memory
bandwidth is higher when parallel threads all access memory local to their
processor package than when they access memory attached to a remote package.
  But, the impact of this heterogeneous memory architecture is not easily
understood from vendor benchmarks. Even where these measurements are available,
they provide only best-case memory throughput. This work presents a series of
modifications to the well-known STREAM benchmark to measure the effects of NUMA
on both a 48-core AMD Opteron machine and a 32-core Intel Xeon machine."
"In this paper we propose a stochastic broadcast PI-calculus which can be used
to model server-client based systems where synchronization is always governed
by only one participant. Therefore, there is no need to determine the joint
synchronization rates. We also take immediate transitions into account which is
useful to model behaviors with no impact on the temporal properties of a
system. Since immediate transitions may introduce non-determinism, we will show
how these non-determinism can be resolved, and as result a valid CTMC will be
obtained finally. Also some practical examples are given to show the
application of this calculus."
"In this paper, we study the sensitivity and robustness of Space Shift Keying
(SSK) modulation to imperfect channel knowledge at the receiver. Unlike the
common widespread belief, we show that SSK modulation is more robust to
imperfect channel knowledge than other state-of-the-art transmission
technologies, and only few training pilots are needed to get reliable enough
channel estimates for data detection. More precisely, we focus our attention on
the so-called Time-Orthogonal-Signal-Design (TOSD-) SSK modulation scheme,
which is an improved version of SSK modulation offering transmit-diversity
gains, and provide the following contributions: i) we develop a closed-form
analytical framework to compute the Average Bit Error Probability (ABEP) of a
mismatched detector for TOSD-SSK modulation, which can be used for arbitrary
transmit-antenna, receive-antenna, channel fading, and training pilots; ii) we
perform a comparative study of the performance of TOSD-SSK modulation and the
Alamouti code under the same imperfect channel knowledge, and show that
TOSD-SSK modulation is more robust to channel estimation errors; iii) we point
out that only few pilot pulses are required to get performance very close to
the perfect channel knowledge lower-bound; and iv) we verify that transmit- and
receive-diversity gains of TOSD-SSK modulation are preserved even for a
mismatched receiver."
"A large class of dense linear algebra operations, such as LU decomposition or
inversion of a triangular matrix, are usually performed by blocked algorithms.
For one such operation, typically, not only one but many algorithmic variants
exist; depending on computing architecture, libraries and problem size, each
variant attains a different performances. We propose methods and tools to rank
the algorithmic variants according to their performance for a given scenario
without executing them.
  For this purpose, we identify the routines upon which the algorithms are
built. A first tool - the Sampler - measures the performance of these routines.
Using the Sampler, a second tool models their performance. The generated models
are then used to predict the performance of the considered algorithms. For a
given scenario, these predictions allow us to correctly rank the algorithms
according to their performance without executing them. With the help of the
same tools, algorithmic parameters such as block-size can be optimally tuned."
"Energy consumption imposes a significant cost for data centers; yet much of
that energy is used to maintain excess service capacity during periods of
predictably low load. Resultantly, there has recently been interest in
developing designs that allow the service capacity to be dynamically resized to
match the current workload. However, there is still much debate about the value
of such approaches in real settings. In this paper, we show that the value of
dynamic resizing is highly dependent on statistics of the workload process. In
particular, both slow time-scale non-stationarities of the workload (e.g., the
peak-to-mean ratio) and the fast time-scale stochasticity (e.g., the burstiness
of arrivals) play key roles. To illustrate the impact of these factors, we
combine optimization-based modeling of the slow time-scale with stochastic
modeling of the fast time scale. Within this framework, we provide both
analytic and numerical results characterizing when dynamic resizing does (and
does not) provide benefits."
"A workload analysis technique is presented that processes data from operation
type traces and creates a Hidden Markov Model (HMM) to represent the workload
that generated those traces. The HMM can be used to create representative
traces for performance models, such as simulators, avoiding the need to
repeatedly acquire suitable traces. It can also be used to estimate directly
the transition probabilities and rates of a Markov modulated arrival process,
for use as input to an analytical performance model of Flash memory. The HMMs
obtained from industrial workloads are validated by comparing their
autocorrelation functions and other statistics with those of the corresponding
monitored time series. Further, the performance model applications are
illustrated by numerical examples."
"This paper presents a model of NAND flash SSD utilization and write
amplification when the ATA/ATAPI SSD Trim command is incorporated into
object-based storage under a variety of user workloads, including a uniform
random workload with objects of fixed size and a uniform random workload with
objects of varying sizes. We first summarize the existing models for write
amplification in SSDs for workloads with and without the Trim command, then
propose an alteration of the models that utilizes a framework of object-based
storage. The utilization of objects and pages in the SSD is derived, with the
analytic results compared to simulation. Finally, the effect of objects on
write amplification and its computation is discussed along with a potential
application to optimization of SSD usage through object storage metadata
servers that allocate object classes of distinct object size."
"Now a days many algorithms are invented or being inventing to find the
solution for Euclidean Minimum Spanning Tree, EMST, problem, as its
applicability is increasing in much wide range of fields containing spatial or
spatio temporal data viz. astronomy which consists of millions of spatial data.
To solve this problem, we are presenting a technique by adopting the dual tree
algorithm for finding efficient EMST and experimented on a variety of real time
and synthetic datasets. This paper presents the observed experimental
observations and the efficiency of the dual tree framework, in the context of
kdtree and ball tree on spatial datasets of different dimensions."
"This paper presents observations about power consumption of a latest
smartphone. Modern smartphones are powerful devices with different choices of
data connections and other functional modes. This paper provides analysis of
power utilization for these different operation modes. Also, we present power
consumption by vital operating system (OS) components."
"In this work we investigate the bounds on the estimation accuracy of Primary
User (PU) traffic parameters with exponentially distributed busy and idle
times. We derive closed-form expressions for the Cramer-Rao bounds on the mean
squared estimation error for the blind joint estimation of the PU traffic
parameters, specifically, the duty cycle, and the mean arrival and departure
rates. Moreover, we present the corresponding maximum-likelihood estimators for
the traffic parameters. In addition, we derive a modified likelihood function
for the joint estimation of traffic parameters when spectrum sensing errors are
considered, and we present the impact of spectrum sensing errors on the
estimation error via simulations. Finally, we consider a duty cycle estimator,
common in traffic estimation literature, that is based on averaging the traffic
samples. We derive, in closed-form, the mean squared estimation error of the
considered estimator under spectrum sensing errors."
"The practicality of the stochastic network calculus (SNC) is often questioned
on grounds of potential looseness of its performance bounds. In this paper it
is uncovered that for bursty arrival processes (specifically Markov-Modulated
On-Off (MMOO)), whose amenability to \textit{per-flow} analysis is typically
proclaimed as a highlight of SNC, the bounds can unfortunately indeed be very
loose (e.g., by several orders of magnitude off). In response to this uncovered
weakness of SNC, the (Standard) per-flow bounds are herein improved by deriving
a general sample-path bound, using martingale based techniques, which
accommodates FIFO, SP, EDF, and GPS scheduling. The obtained (Martingale)
bounds gain an exponential decay factor of ${\mathcal{O}}(e^{-\alpha n})$ in
the number of flows $n$. Moreover, numerical comparisons against simulations
show that the Martingale bounds are remarkably accurate for FIFO, SP, and EDF
scheduling; for GPS scheduling, although the Martingale bounds substantially
improve the Standard bounds, they are numerically loose, demanding for
improvements in the core SNC analysis of GPS."
"Solid state drives (SSDs) have seen wide deployment in mobiles, desktops, and
data centers due to their high I/O performance and low energy consumption. As
SSDs write data out-of-place, garbage collection (GC) is required to erase and
reclaim space with invalid data. However, GC poses additional writes that
hinder the I/O performance, while SSD blocks can only endure a finite number of
erasures. Thus, there is a performance-durability tradeoff on the design space
of GC. To characterize the optimal tradeoff, this paper formulates an
analytical model that explores the full optimal design space of any GC
algorithm. We first present a stochastic Markov chain model that captures the
I/O dynamics of large-scale SSDs, and adapt the mean-field approach to derive
the asymptotic steady-state performance. We further prove the model convergence
and generalize the model for all types of workload. Inspired by this model, we
propose a randomized greedy algorithm (RGA) that can operate along the optimal
tradeoff curve with a tunable parameter. Using trace-driven simulation on
DiskSim with SSD add-ons, we demonstrate how RGA can be parameterized to
realize the performance-durability tradeoff."
"This paper presents an analysis of the energy consumption of an extensive
number of the optimisations a modern compiler can perform. Using GCC as a test
case, we evaluate a set of ten carefully selected benchmarks for five different
embedded platforms.
  A fractional factorial design is used to systematically explore the large
optimisation space (2^82 possible combinations), whilst still accurately
determining the effects of optimisations and optimisation combinations.
Hardware power measurements on each platform are taken to ensure all
architectural effects on the energy consumption are captured.
  We show that fractional factorial design can find more optimal combinations
than relying on built in compiler settings. We explore the relationship between
run-time and energy consumption, and identify scenarios where they are and are
not correlated.
  A further conclusion of this study is the structure of the benchmark has a
larger effect than the hardware architecture on whether the optimisation will
be effective, and that no single optimisation is universally beneficial for
execution time or energy consumption."
"This paper presents a systematic approach to the complex problem of high
confidence performance assurance of high performance architectures based on
methods used over several generations of industrial microprocessors. A taxonomy
is presented for performance assurance through three key stages of a product
life cycle-high level performance, RTL performance, and silicon performance.
The proposed taxonomy includes two components-independent performance assurance
space for each stage and a correlation performance assurance space between
stages. It provides a detailed insight into the performance assurance space in
terms of coverage provided taking into account capabilities and limitations of
tools and methodologies used at each stage. An application of the taxonomy to
cases described in the literature and to high performance Intel architectures
is shown. The proposed work should be of interest to manufacturers of high
performance microprocessor/chipset architectures and has not been discussed in
the literature."
"We analyze the so-called Shortest Queue First (SQF) queueing discipline
whereby a unique server addresses queues in parallel by serving at any time
that queue with the smallest workload. Considering a stationary system composed
of two parallel queues and assuming Poisson arrivals and general service time
distributions, we first establish the functional equations satisfied by the
Laplace transforms of the workloads in each queue. We further specialize these
equations to the so-called ""symmetric case"", with same arrival rates and
identical exponential service time distributions at each queue; we then obtain
a functional equation $$ M(z) = q(z) \cdot M \circ h(z) + L(z) $$ for unknown
function $M$, where given functions $q$, $L$ and $h$ are related to one branch
of a cubic polynomial equation. We study the analyticity domain of function $M$
and express it by a series expansion involving all iterates of function $h$.
This allows us to determine empty queue probabilities along with the tail of
the workload distribution in each queue. This tail appears to be identical to
that of the Head-of-Line preemptive priority system, which is the key feature
desired for the SQF discipline."
"As a follow-up to a recent paper considering two symmetric queues, the
\textit{Shortest Queue First} service discipline is presently analysed for two
general asymmetric queues. Using the results previously established and
assuming exponentially distributed service times, the bivariate Laplace
transform of workloads in each queue is shown to depend on the solution
$\mathbf{M}$ to a two-dimensional functional equation $$ \mathbf{M} = Q_1 \cdot
\mathbf{M}\circ h_1 + Q_2 \cdot \mathbf{M}\circ h_2 + \mathbf{L} $$ with given
matrices $Q_1$, $Q_2$ and vector $\mathbf{L}$ and where functions $h_1$ and
$h_2$ are defined each on some rational curve; solution $\mathbf{M}$ can then
represented by a series expansion involving the semi-group $< h_1, h_2 >$
generated by these two functions. The empty queue probabilities along with the
tail behaviour of the workload distribution at each queue are characterised."
"We consider in this paper a non work-conserving Generalized Processor Sharing
(GPS) system composed of two queues with Poisson arrivals and exponential
service times. Using general results due to Fayolle \emph{et al}, we first
establish the stability condition for this system. We then determine the
functional equation satisfied by the generating function of the numbers of jobs
in both queues and the associated Riemann-Hilbert problem. We prove the
existence and the uniqueness of the solution. This allows us to completely
characterize the system, in particular to compute the empty queue probability.
We finally derive the tail asymptotics of the number of jobs in one queue."
"Multiclass FIFO is used in communication networks such as in input-queueing
routers/switches and in wireless networks. For the concern of providing service
guarantees in such networks, it is crucial to have analytical results, e.g.
bounds, on the performance of multi-class FIFO. Surprisingly, there are few
such results in the literature. This paper is devoted to filling the gap.
Specifically, a single hop deterministic case is studied, for which, delay and
backlog bounds are derived, in addition to guaranteed rate and service curve
characterizations that may be exploited to extend the analysis to network
cases."
"This paper presents and justifies an open benchmark suite named BEEBS,
targeted at evaluating the energy consumption of embedded processors.
  We explore the possible sources of energy consumption, then select individual
benchmarks from contemporary suites to cover these areas. Version one of BEEBS
is presented here and contains 10 benchmarks that cover a wide range of typical
embedded applications. The benchmark suite is portable across diverse
architectures and is freely available.
  The benchmark suite is extensively evaluated, and the properties of its
constituent programs are analysed. Using real hardware platforms we show case
examples which illustrate the difference in power dissipation between three
processor architectures and their related ISAs. We observe significant
differences in the average instruction dissipation between the architectures of
4.4x, specifically 170uW/MHz (ARM Cortex-M0), 65uW/MHz (Adapteva Epiphany) and
88uW/MHz (XMOS XS1-L1)."
"To cope with the complex embedded system design, early design space
exploration (DSE) is used to make design decisions early in the design phase.
For early DSE it is crucial that the running time of the exploration is as
small as possible. In this paper, we describe both the porting of our
scenario-based DSE to the SPARC T3-4 server and the analysis of its performance
behavior."
"Embedded Systems combine one or more processor cores with dedicated logic
running on an ASIC or FPGA to meet design goals at reasonable cost. It is
achieved by profiling the application with variety of aspects like performance,
memory usage, cache hit versus cache miss, energy consumption, etc. Out of
these, performance estimation is more important than others. With ever
increasing system complexities, it becomes quite necessary to carry out
performance estimation of embedded software implemented in a particular
processor for fast design space exploration. Such profiled data also guides the
designer how to partition the system for Hardware (HW) and Software (SW)
environments. In this paper, we propose a classification for currently
available Embedded Software Profiling Tools, and we present different academic
and industrial approaches in this context. Based on these observations, it will
be easy to identify such common principles and needs which are required for a
true Software Profiling Tool for a particular application."
"Offloading of cellular traffic through a wireless local area network (WLAN)
is theoretically evaluated. First, empirical data sets of the locations of WLAN
internet access points are analyzed and an inhomogeneous Poisson process
consisting of high, normal, and low density regions is proposed as a spatial
point process model for these configurations. Second, performance metrics, such
as mean available bandwidth for a user and the number of vertical handovers,
are evaluated for the proposed model through geometric analysis. Explicit
formulas are derived for the metrics, although they depend on many parameters
such as the number of WLAN access points, the shape of each WLAN coverage
region, the location of each WLAN access point, the available bandwidth (bps)
of the WLAN, and the shape and available bandwidth (bps) of each subregion
identified by the channel quality indicator in a cell of the cellular network.
Explicit formulas strongly suggest that the bandwidth a user experiences does
not depend on the user mobility. This is because the bandwidth available by a
user who does not move and that available by a user who moves are the same or
approximately the same as a probabilistic distribution. Numerical examples show
that parameters, such as the size of regions where placement of WLAN access
points is not allowed and the mean density of WLANs in high density regions,
have a large impact on performance metrics. In particular, a homogeneous
Poisson process model as the WLAN access point location model largely
overestimates the mean available bandwidth for a user and the number of
vertical handovers. The overestimated mean available bandwidth is, for example,
about 50% in a certain condition."
"This paper considers a retrial queueing model for a base station in cellular
networks where fresh calls and handover calls are available. Fresh calls are
initiated from the cell of the base station. On the other hand, a handover call
has been connecting to a base station and moves to another one. In order to
keep the continuation of the communication, it is desired that an available
channel in the new base station is immediately assigned to the handover call.
To this end, a channel is reserved as the guard channel for handover calls in
base stations. Blocked fresh and handover calls join a virtual orbit and repeat
their attempts in a later time. We assume that a base station can recognize
retrial calls and give them the same priority as that of handover calls. We
model a base station by a multiserver retrial queue with priority customers for
which a level-dependent QBD process is formulated. We obtain Taylor series
expansion for the nonzero elements of the rate matrices of the level-dependent
QBD. Using the expansion results, we obtain an asymptotic upper bound for the
joint stationary distribution of the number of busy channels and that of
customers in the orbit. Furthermore, we derive an efficient numerical algorithm
to calculate the joint stationary distribution."
"In this paper we examine the key elements determining the best performance of
computing by increasing the frequency of a single chip and to get the minimum
latency during execution of the programs to achieve best possible output. It is
not enough to provide concurrent improvements in the hardware as Software also
have to introduce concurrency in order to exploit the parallelism. The software
parallelism is defined by the control and data dependency of programs whereas
Hardware refers to the type of parallelism defined by the machine architecture
and hardware multiplicity."
"Supermarket models are a class of parallel queueing networks with an adaptive
control scheme that play a key role in the study of resource management of,
such as, computer networks, manufacturing systems and transportation networks.
When the arrival processes are non-Poisson and the service times are
non-exponential, analysis of such a supermarket model is always limited,
interesting, and challenging.
  This paper describes a supermarket model with non-Poisson inputs: Markovian
Arrival Processes (MAPs) and with non-exponential service times: Phase-type
(PH) distributions, and provides a generalized matrix-analytic method which is
first combined with the operator semigroup and the mean-field limit. When
discussing such a more general supermarket model, this paper makes some new
results and advances as follows: (1) Providing a detailed probability analysis
for setting up an infinite-dimensional system of differential vector equations
satisfied by the expected fraction vector, where ""the invariance of environment
factors"" is given as an important result. (2) Introducing the phase-type
structure to the operator semigroup and to the mean-field limit, and a
Lipschitz condition can be obtained by means of a unified matrix-differential
algorithm. (3) The matrix-analytic method is used to compute the fixed point
which leads to performance computation of this system. Finally, we use some
numerical examples to illustrate how the performance measures of this
supermarket model depend on the non-Poisson inputs and on the non-exponential
service times. Thus the results of this paper give new highlight on
understanding influence of non-Poisson inputs and of non-exponential service
times on performance measures of more general supermarket models."
"Individual-based hybrid modelling of spatially distributed systems is usually
expensive. Here, we consider a hybrid system in which mobile agents spread over
the space and interact with each other when in close proximity. An
individual-based model for this system needs to capture the spatial attributes
of every agent and monitor the interaction between each pair of them. As a
result, the cost of simulating this model grows exponentially as the number of
agents increases. For this reason, a patch-based model with more abstraction
but better scalability is advantageous. In a patch-based model, instead of
representing each agent separately, we model the agents in a patch as an
aggregation. This property significantly enhances the scalability of the model.
In this paper, we convert an individual-based model for a spatially distributed
network system for wild-life monitoring, ZebraNet, to a patch-based stochastic
HYPE model with accurate performance evaluation. We show the ease and
expressiveness of stochastic HYPE for patch-based modelling of hybrid systems.
Moreover, a mean-field analytical model is proposed as the fluid flow
approximation of the stochastic HYPE model, which can be used to investigate
the average behaviour of the modelled system over an infinite number of
simulation runs of the stochastic HYPE model."
"Recently multiserver queues with setup times have been extensively studied
because they have applications in power-saving data centers. The most
challenging model is the M/M/$c$/Setup queue where a server is turned off when
it is idle and is turned on if there are some waiting jobs. Recently, Gandhi et
al.~(SIGMETRICS 2013, QUESTA 2014) present the recursive renewal reward
approach as a new mathematical tool to analyze the model. In this paper, we
derive exact solutions for the same model using two alternative methodologies:
generating function approach and matrix analytic method. The former yields
several theoretical insights into the systems while the latter provides an
exact recursive algorithm to calculate the joint stationary distribution and
then some performance measures so as to give new application insights."
"Every probability distribution can be approximated up to a given precision by
a phase-type distribution, i.e. a distribution encoded by a continuous time
Markov chain (CTMC). However, an excessive number of states in the
corresponding CTMC is needed for some standard distributions, in particular
most distributions with regions of zero density such as uniform or shifted
distributions. Addressing this class of distributions, we suggest an
alternative representation by CTMC extended with discrete-time transitions.
Using discrete-time transitions we split the density function into multiple
intervals. Within each interval, we then approximate the density with standard
phase-type fitting. We provide an experimental evidence that our method
requires only a moderate number of states to approximate such distributions
with regions of zero density. Furthermore, the usage of CTMC with discrete-time
transitions is supported by a number of techniques for their analysis. Thus,
our results promise an efficient approach to the transient analysis of a class
of non-Markovian models."
"We consider parametric version of fixed-delay continuous-time Markov chains
(or equivalently deterministic and stochastic Petri nets, DSPN) where
fixed-delay transitions are specified by parameters, rather than concrete
values. Our goal is to synthesize values of these parameters that, for a given
cost function, minimise expected total cost incurred before reaching a given
set of target states. We show that under mild assumptions, optimal values of
parameters can be effectively approximated using translation to a Markov
decision process (MDP) whose actions correspond to discretized values of these
parameters."
"The use of photovoltaic (PV) sources is becoming very popular in smart grid
for their ecological benefits, with higher scalability and utilization for
local generation and delivery. PV can also potentially avoid the energy losses
that are normally associated with long-range grid distribution. The increased
penetration of solar panels, however, has introduced a need for solar energy
models that are capable of producing realistic synthetic data with small error
margins. Such models, for instance, can be used to design the appropriate size
of energy storage devices or to determine the maximum charging rate of a
PV-powered electric vehicle (EV) charging station. In this regard, this paper
proposes a stochastic model for solar generation using a Markov chain approach.
Based on real data, it is first shown that the solar states are
inter-dependent, and thus suitable for modeling using a Markov model. Then, the
probabilities of transition between states are shown to be heterogeneous over
different time segments. A model is proposed that captures the inter temporal
dependency of solar irradiance through segmentation of the Markov chain across
different times of the day. In the studied model, different state transition
matrices are constructed for different time segments, which the proposed
algorithm then uses to generate the solar states for different times of the
day. Numerical examples are provided to show the effectiveness of the proposed
synthetic generator."
"Accurate simulations of various physical processes on digital computers
requires huge computing performance, therefore accelerating these scientific
and engineering applications has a great importance. Density of programmable
logic devices doubles in every 18 months according to Moore's Law. On the
recent devices around one hundred double precision floating-point adders and
multipliers can be implemented. In the paper an FPGA based framework is
described to efficiently utilize this huge computing power to accelerate
simulation of complex physical spatiotemporal phenomena. Simulating complicated
geometries requires unstructured spatial discretization which results in
irregular memory access patterns severely limiting computing performance. Data
locality is improved by mesh node renumbering technique which results in
predictable memory access pattern. Additionally storing a small window of node
data in the on-chip memory of the FPGA can increase data reuse and decrease
memory bandwidth requirements. Generation of the floating-point data path and
control structure of the arithmetic unit containing dozens of operators is a
very challenging task when the goal is high operating frequency. Long and high
fanout control lines and improper placement can severely affect computing
performance. In the paper an automatic data path generation and partitioning
algorithm is presented to eliminate long delays and aid placement of the
circuit. Efficiency and use of the framework is described by a case study
solving the Euler equations on an unstructured mesh using finite volume
technique. On the currently available largest FPGA the generated architecture
contains three processing elements working in parallel providing 90 times
speedup compared to a high performance microprocessor core."
"We study the impact of tunable parameters on computational intensity (i.e.,
inverse code balance) and energy consumption of multicore-optimized wavefront
diamond temporal blocking (MWD) applied to different stencil-based update
schemes. MWD combines the concepts of diamond tiling and multicore-aware
wavefront blocking in order to achieve lower cache size requirements than
standard single-core wavefront temporal blocking. We analyze the impact of the
cache block size on the theoretical and observed code balance, introduce loop
tiling in the leading dimension to widen the range of applicable diamond sizes,
and show performance results on a contemporary Intel CPU. The impact of code
balance on power dissipation on the CPU and in the DRAM is investigated and
shows that DRAM power is a decisive factor for energy consumption, which is
strongly influenced by the code balance. Furthermore we show that highest
performance does not necessarily lead to lowest energy even if the clock speed
is fixed."
"In this paper, we consider a network of rate proportional processor sharing
servers in which sessions with long-tailed duration arrive as Poisson
processes. In particular, we assume that a session of type $n$ transmits at a
rate $r_n$ bits per unit time and lasts for a random time $\tau_n$ with a
generalized Pareto distribution given by $P \{\tau_n > x\} \sim \alpha_n
x^{-(1+\beta_n)}$ for large $x$, where $\alpha_n, \beta_n > 0$. The weights are
taken to be the rates of the flows. The network is assumed to be loop-free with
respect to source-destination routes. We characterize the order $O-$asymptotics
of the complementary buffer occupancy distribution at each node in terms of the
input characteristics of the sessions. In particular, we show that the
distributions obey a power law whose exponent can be calculated via solving a
fixed point and deterministic knapsack problem. The paper concludes with some
canonical examples."
"In this paper we analyze Least Recently Used (LRU) caches operating under the
Shot Noise requests Model (SNM). The SNM was recently proposed to better
capture the main characteristics of today Video on Demand (VoD) traffic. We
investigate the validity of Che's approximation through an asymptotic analysis
of the cache eviction time. In particular, we provide a large deviation
principle, a law of large numbers and a central limit theorem for the cache
eviction time, as the cache size grows large. Finally, we derive upper and
lower bounds for the ""hit"" probability in tandem networks of caches under Che's
approximation."
"In this paper we develop a novel technique to analyze both isolated and
interconnected caches operating under different caching strategies and
realistic traffic conditions. The main strength of our approach is the ability
to consider dynamic contents which are constantly added into the system
catalogue, and whose popularity evolves over time according to desired
profiles. We do so while preserving the simplicity and computational efficiency
of models developed under stationary popularity conditions, which are needed to
analyze several caching strategies. Our main achievement is to show that the
impact of content popularity dynamics on cache performance can be effectively
captured into an analytical model based on a fixed content catalogue (i.e., a
catalogue whose size and objects' popularity do not change over time)."
"This is the proceedings of the 5th International Workshop on Adaptive
Self-tuning Computing Systems 2015 (ADAPT'15)."
"Correlation Power Analysis (CPA) is a type of power analysis based side
channel attack that can be used to derive the secret key of encryption
algorithms including DES (Data Encryption Standard) and AES (Advanced
Encryption Standard). A typical CPA attack on unprotected AES is performed by
analysing a few thousand power traces that requires about an hour of
computational time on a general purpose CPU. Due to the severity of this
situation, a large number of researchers work on countermeasures to such
attacks. Verifying that a proposed countermeasure works well requires
performing the CPA attack on about 1.5 million power traces. Such processing,
even for a single attempt of verification on commodity hardware would run for
several days making the verification process infeasible. Modern Graphics
Processing Units (GPUs) have support for thousands of light weight threads,
making them ideal for parallelizable algorithms like CPA. While the cost of a
GPU being lesser than a high performance multicore server, still the GPU
performance for this algorithm is many folds better than that of a multicore
server. We present an algorithm and its implementation on GPU for CPA on
128-bit AES that is capable of executing 1300x faster than that on a single
threaded CPU and more than 60x faster than that on a 32 threaded multicore
server. We show that an attack that would take hours on the multicore server
would take even less than a minute on a much cost effective GPU."
"The Roofline Model and its derivatives provide an intuitive representation of
the best achievable performance on a given architecture. The Roofline Toolkit
project is a collaboration among researchers at Argonne National Laboratory,
Lawrence Berkeley National Laboratory, and the University of Oregon and
consists of three main parts: hardware characterization, software
characterization, and data manipulation and visualization interface. These
components address the different aspects of performance data acquisition and
manipulation required for performance analysis, modeling and optimization of
codes on existing and emerging architectures. In this paper we introduce an
initial implementation of the third component, a system for visualizing
roofline charts and managing roofline performance analysis data. We discuss the
implementation and rationale for the integration of the roofline visualization
system into the Eclipse IDE. An overview of our continuing efforts and goals in
the development of this project is provided."
"A queue is required when a service provider is not able to handle jobs
arriving over the time. In a highly flexible and dynamic environment, some jobs
might demand for faster execution at run-time especially when the resources are
limited and the jobs are competing for acquiring resources. A user might demand
for speed up (reduced wait time) for some of the jobs present in the queue at
run time. In such cases, it is required to accelerate (directly sending the job
to the server) urgent jobs (requesting for speed up) ahead of other jobs
present in the queue for an earlier completion of urgent jobs. Under the
assumption of no additional resources, such acceleration of jobs would result
in slowing down of other jobs present in the queue. In this paper, we formulate
the problem of Speed Up Scheduling without acquiring any additional resources
for the scheduling of on-line speed up requests posed by a user at run-time and
present algorithms for the same. We apply the idea of Speed Up Scheduling to
two different domains -Web Scheduling and CPU Scheduling. We demonstrate our
results with a simulation based model using trace driven workload and synthetic
datasets to show the usefulness of Speed Up scheduling. Speed Up provides a new
way of addressing urgent jobs, provides a different evaluation criteria for
comparing scheduling algorithms and has practical applications."
"In this thesis, we study the optimal tradeoff of average delay, average
service cost, and average utility for single server queueing models, with and
without admission control. The continuous time and discrete time queueing
models that we consider are motivated by cross-layer models for noisy
point-to-point links, with random packet arrivals. We study the above tradeoff
problem for a class of admissible policies, which are monotone and stationary
and obtain an asymptotic characterization of the minimum average delay as a
function of the average service cost and average utility constraints."
"Wireless sensor networks are usually composed of a large number of nodes, and
with the increasing processing power and power consumption efficiency they are
expected to run more complex protocols in the future. These pose problems in
the field of verification and performance evaluation of wireless networks. In
this paper, we tailor the mean-field theory as a modeling technique to analyze
their behavior. We apply this method to the slotted ALOHA protocol, and
establish results on the long term trends of the protocol within a very large
network, specially regarding the stability of ALOHA-type protocols."
"Continuous Time Markov Chains (CTMC) have been used extensively to model
reliability of storage systems. While the exponentially distributed sojourn
time of Markov models is widely known to be unrealistic (and it is necessary to
consider Weibull-type models for components such as disks), recent work has
also highlighted some additional infirmities with the CTMC model, such as the
ability to handle repair times. Due to the memoryless property of these models,
any failure or repair of one component resets the ""clock"" to zero with any
partial repair or aging in some other subsystem forgotten. It has therefore
been argued that simulation is the only accurate technique available for
modelling the reliability of a storage system with multiple components.
  We show how both the above problematic aspects can be handled when we
consider a careful set of approximations in a detailed model of the system. A
detailed model has many states, and the transitions between them and the
current state captures the ""memory"" of the various components. We model a
non-exponential distribution using a sum of exponential distributions, along
with the use of a CTMC solver in a probabilistic model checking tool that has
support for reducing large state spaces. Furthermore, it is possible to get
results close to what is obtained through simulation and at much lower cost."
"Stream processing is a compute paradigm that promises safe and efficient
parallelism. Modern big-data problems are often well suited for stream
processing's throughput-oriented nature. Realization of efficient stream
processing requires monitoring and optimization of multiple communications
links. Most techniques to optimize these links use queueing network models or
network flow models, which require some idea of the actual execution rate of
each independent compute kernel within the system. What we want to know is how
fast can each kernel process data independent of other communicating kernels.
This is known as the ""service rate"" of the kernel within the queueing
literature. Current approaches to divining service rates are static. Modern
workloads, however, are often dynamic. Shared cloud systems also present
applications with highly dynamic execution environments (multiple users,
hardware migration, etc.). It is therefore desirable to continuously re-tune an
application during run time (online) in response to changing conditions. Our
approach enables online service rate monitoring under most conditions,
obviating the need for reliance on steady state predictions for what are
probably non-steady state phenomena. First, some of the difficulties associated
with online service rate determination are examined. Second, the algorithm to
approximate the online non-blocking service rate is described. Lastly, the
algorithm is implemented within the open source RaftLib framework for
validation using a simple microbenchmark as well as two full streaming
applications."
"Energy efficiency is an essential requirement for all contemporary computing
systems. We thus need tools to measure the energy consumption of computing
systems and to understand how workloads affect it. Significant recent research
effort has targeted direct power measurements on production computing systems
using on-board sensors or external instruments. These direct methods have in
turn guided studies of software techniques to reduce energy consumption via
workload allocation and scaling. Unfortunately, direct energy measurements are
hampered by the low power sampling frequency of power sensors. The coarse
granularity of power sensing limits our understanding of how power is allocated
in systems and our ability to optimize energy efficiency via workload
allocation.
  We present ALEA, a tool to measure power and energy consumption at the
granularity of basic blocks, using a probabilistic approach. ALEA provides
fine-grained energy profiling via statistical sampling, which overcomes the
limitations of power sensing instruments. Compared to state-of-the-art energy
measurement tools, ALEA provides finer granularity without sacrificing
accuracy. ALEA achieves low overhead energy measurements with mean error rates
between 1.4% and 3.5% in 14 sequential and parallel benchmarks tested on both
Intel and ARM platforms. The sampling method caps execution time overhead at
approximately 1%. ALEA is thus suitable for online energy monitoring and
optimization. Finally, ALEA is a user-space tool with a portable,
machine-independent sampling method. We demonstrate two use cases of ALEA,
where we reduce the energy consumption of a k-means computational kernel by 37%
and an ocean modelling code by 33%, compared to high-performance execution
baselines, by varying the power optimization strategy between basic blocks."
"This paper treats power-aware throughput maxi-mization in a multi-user file
downloading system. Each user can receive a new file only after its previous
file is finished. The file state processes for each user act as coupled Markov
chains that form a generalized restless bandit system. First, an optimal
algorithm is derived for the case of one user. The algorithm maximizes
throughput subject to an average power constraint. Next, the one-user algorithm
is extended to a low complexity heuristic for the multi-user problem. The
heuristic uses a simple online index policy. In a special case with no
power-constraint, the multi-user heuristic is shown to be throughput optimal.
Simulations are used to demonstrate effectiveness of the heuristic in the
general case. For simple cases where the optimal solution can be computed
offline, the heuristic is shown to be near-optimal for a wide range of
parameters."
"This paper considers a nonstationary multiserver queuing model with
abandonment and balking for inbound call centers. We present a continuous time
Markov chain (CTMC) model which captures the important characteristics of an
inbound call center and obtain a numerical solution for its transient state
probabilities using uniformization method with steady-state detection.
Keywords: call center, transient, Markov processes, numerical methods,
uniformization, abandonment, balking"
"Safely meeting Worst Case Energy Consumption (WCEC) criteria requires
accurate energy modeling of software. We investigate the impact of instruction
operand values upon energy consumption in cacheless embedded processors.
Existing instruction-level energy models typically use measurements from random
input data, providing estimates unsuitable for safe WCEC analysis.
  We examine probabilistic energy distributions of instructions and propose a
model for composing instruction sequences using distributions, enabling WCEC
analysis on program basic blocks. The worst case is predicted with statistical
analysis. Further, we verify that the energy of embedded benchmarks can be
characterised as a distribution, and compare our proposed technique with other
methods of estimating energy consumption."
"This paper describes NS - Network Simulator, the computer networks simulation
tool. We offer an overview NS, and also analyze its characteristics and
functions. Finally, we present in detail all steps for preparing a simulation
of a simple model in NS."
"The great prosperity of big data systems such as Hadoop in recent years makes
the benchmarking of these systems become crucial for both research and industry
communities. The complexity, diversity, and rapid evolution of big data systems
gives rise to various new challenges about how we design generators to produce
data with the 4V properties (i.e. volume, velocity, variety and veracity), as
well as implement application-specific but still comprehensive workloads.
However, most of the existing big data benchmarks can be described as attempts
to solve specific problems in benchmarking systems. This article investigates
the state-of-the-art in benchmarking big data systems along with the future
challenges to be addressed to realize a successful and efficient benchmark."
"Simple floating point operations like addition or multiplication on
normalized floating point values can be computed by current AMD and Intel
processors in three to five cycles. This is different for denormalized numbers,
which appear when an underflow occurs and the value can no longer be
represented as a normalized floating-point value. Here the costs are about two
magnitudes higher."
"Reliability modelling of RAID storage systems with its various components
such as RAID controllers, enclosures, expanders, interconnects and disks is
important from a storage system designer's point of view. A model that can
express all the failure characteristics of the whole RAID storage system can be
used to evaluate design choices, perform cost reliability trade-offs and
conduct sensitivity analyses. However, including such details makes the
computational models of reliability quickly infeasible.
  We present a CTMC reliability model for RAID storage systems that scales to
much larger systems than heretofore reported and we try to model all the
components as accurately as possible. We use several state-space reduction
techniques at the user level, such as aggregating all in-series components and
hierarchical decomposition, to reduce the size of our model. To automate
computation of reliability, we use the PRISM model checker as a CTMC solver
where appropriate. Our modelling techniques using PRISM are more practical (in
both time and effort) compared to previously reported Monte-Carlo simulation
techniques.
  Our model for RAID storage systems (that includes, for example, disks,
expanders, enclosures) uses Weibull distributions for disks and, where
appropriate, correlated failure modes for disks, while we use exponential
distributions with independent failure modes for all other components. To use
the CTMC solver, we approximate the Weibull distribution for a disk using sum
of exponentials and we confirm that this model gives results that are in
reasonably good agreement with those from the sequential Monte Carlo simulation
methods for RAID disk subsystems reported in literature earlier. Using a
combination of scalable techniques, we are able to model and compute
reliability for fairly large configurations with upto 600 disks using this
model."
"Analytic performance models are essential for understanding the performance
characteristics of loop kernels, which consume a major part of CPU cycles in
computational science. Starting from a validated performance model one can
infer the relevant hardware bottlenecks and promising optimization
opportunities. Unfortunately, analytic performance modeling is often tedious
even for experienced developers since it requires in-depth knowledge about the
hardware and how it interacts with the software. We present the ""Kerncraft""
tool, which eases the construction of analytic performance models for streaming
kernels and stencil loop nests. Starting from the loop source code, the problem
size, and a description of the underlying hardware, Kerncraft can ideally
predict the single-core performance and scaling behavior of loops on multicore
processors using the Roofline or the Execution-Cache-Memory (ECM) model. We
describe the operating principles of Kerncraft with its capabilities and
limitations, and we show how it may be used to quickly gain insights by
accelerated analytic modeling."
"We describe our experience with modeling a video tracking system used to
detect and follow moving targets from an airplane. We provide a formal model
that takes into account the real-time properties of the system and use it to
compute the worst and best-case end to end latency. We also compute a lower
bound on the delay between the loss of two frames. Our approach is based on the
model-checking tool Tina, that provides state-space generation and
model-checking algorithms for an extension of Time Petri Nets with data and
priorities. We propose several models divided in two main categories: first
Time Petri Net models, which are used to study the behavior of the system in
the most basic way; then models based on the Fiacre specification language,
where we take benefit of richer data structures to directly model the buffering
of video information and the use of an unbounded number of frame identifiers."
"In this paper, we propose an optimization selection methodology for the
ubiquitous sparse matrix-vector multiplication (SpMV) kernel. We propose two
models that attempt to identify the major performance bottleneck of the kernel
for every instance of the problem and then select an appropriate optimization
to tackle it. Our first model requires online profiling of the input matrix in
order to detect its most prevailing performance issue, while our second model
only uses comprehensive structural features of the sparse matrix. Our method
delivers high performance stability for SpMV across different platforms and
sparse matrices, due to its application and architecture awareness. Our
experimental results demonstrate that a) our approach is able to distinguish
and appropriately optimize special matrices in multicore platforms that fall
out of the standard class of memory bandwidth bound matrices, and b) lead to a
significant performance gain of 29% in a manycore platform compared to an
architecture-centric optimization, as a result of the successful selection of
the appropriate optimization for the great majority of the matrices. With a
runtime overhead equivalent to a couple dozen SpMV operations, our approach is
practical for use in iterative numerical solvers of real-life applications."
"Traditional compiler optimization theory distinguishes three separate classes
of cache miss -- Cold, Conflict and Capacity. Tiling for cache is typically
guided by capacity miss counts. Models of cache function have not been
effectively used to guide cache tiling optimizations due to model error and
expense. Instead, heuristic or empirical approaches are used to select tilings.
We argue that conflict misses, traditionally neglected or seen as a small
constant effect, are the only fundamentally important cache miss category, that
they form a solid basis by which caches can become modellable, and that models
leaning on cache associatvity analysis can be used to generate cache performant
tilings. We develop a mathematical framework that expresses potential and
actual cache misses in associative caches using Associativity Lattices. We show
these lattices to possess two theoretical advantages over rectangular tiles --
volume maximization and miss regularity. We also show that to generate such
lattice tiles requires, unlike rectangular tiling, no explicit, expensive
lattice point counting. We also describe an implementation of our lattice
tiling approach, show that it can be used to give speedups of over 10x versus
unoptimized code, and despite currently only tiling for one level of cache, can
already be competitive with the aggressive compiler optimizations used in
general purposes compares such as GCC and Intel's ICC. We also show that the
tiling approach can lead to reasonable automatic parallelism when compared to
existing auto-threading compilers."
"Reducing energy consumption is a challenge that is faced on a daily basis by
teams from the High-Performance Computing as well as the Embedded domain. This
issue is mostly attacked from an hardware perspective, by devising
architectures that put energy efficiency as a primary target, often at the cost
of processing power. Lately, computing platforms have become more and more
heterogeneous, but the exploitation of these additional capabilities is so
complex from the application developer's perspective that they are left unused
most of the time, resulting therefore in a supplemental waste of energy rather
than in faster processing times.
  In this paper we present a transparent, on-the-fly optimization scheme that
allows a generic application to automatically exploit the available computing
units to partition its computational load. We have called our approach
Heterogeneous Platform Accelerator (HPA). The idea is to use profiling to
automatically select a computing-intensive candidate for acceleration, and then
distribute the computations to the different units by off-loading blocks of
code to them.
  Using an NVIDIA Jetson TK1 board, we demonstrate that not only HPA results in
faster processing speed, but also in a considerable reduction in the total
energy absorbed."
"In this paper we consider the problem of maximum throughput for tandem
queueing system. We modeled this system as a Quasi-Birth-Death process. In
order to do this we named level the number of customers waiting in the first
buffer (including the customer in service) and we called phase the state of the
remining servers. Using this model we studied the problem of maximum throughput
of the system: the maximum arrival rate that a given system could support
before becoming saturated, or unstable. We considered different particular
cases of such systems, which were obtained by modifying the capacity of the
intermediary buffers, the arrival rate and the service rates. The results of
the simulations are presented in our paper and can be summed up in the
following conclusions: 1. The analytic formula for the maximum throughput of
the system tends to become rather complicated when the number of servers
increase 2. The maximum throughput of the system converges as the number of
servers increases 3. The homogeneous case reveals an interesting
characteristic: if we reverse the order of the servers, maximum thruoughput of
the system remains unchanged The QBD process used for the case of Poisson
arrivals can be extended to model more general arrival processes."
"Task replication has recently been advocated as a practical solution to
reduce latencies in parallel systems. In addition to several convincing
empirical studies, some others provide analytical results, yet under some
strong assumptions such as Poisson arrivals, exponential service times, or
independent service times of the replicas themselves, which may lend themselves
to some contrasting and perhaps contriving behavior. For instance, under the
second assumption, an overloaded system can be stabilized by a replication
factor, but can be sent back in overload through further replication. In turn,
under the third assumption, strictly larger stability regions of replication
systems do not necessarily imply smaller delays.
  Motivated by the need to dispense with such common and restricting
assumptions, which may additionally cause unexpected behavior, we develop a
unified and general theoretical framework to compute tight bounds on the
distribution of response times in general replication systems. These results
immediately lend themselves to the optimal number of replicas minimizing
response time quantiles, depending on the parameters of the system (e.g., the
degree of correlation amongst replicas). As a concrete application of our
framework, we design a novel replication policy which can improve the stability
region of classical fork-join queueing systems by $\mathcal{O}(\ln K)$, in the
number of servers $K$."
"Future servers will incorporate many active lowpower modes for different
system components, such as cores and memory. Though these modes provide
flexibility for power management via Dynamic Voltage and Frequency Scaling
(DVFS), they must be operated in a coordinated manner. Such coordinated control
creates a combinatorial space of possible power mode configurations. Given the
rapid growth of the number of cores, it is becoming increasingly challenging to
quickly select the configuration that maximizes the performance under a given
power budget. Prior power capping techniques do not scale well to large numbers
of cores, and none of those works has considered memory DVFS. In this paper, we
present FastCap, our optimization approach for system-wide power capping, using
both CPU and memory DVFS. Based on a queuing model, FastCap formulates power
capping as a non-linear optimization problem where we seek to maximize the
system performance under a power budget, while promoting fairness across
applications. Our FastCap algorithm solves the optimization online and
efficiently (low complexity on the number of cores), using a small set of
performance counters as input. To evaluate FastCap, we simulate it for a
many-core server running different types of workloads. Our results show that
FastCap caps power draw accurately, while producing better application
performance and fairness than many existing CPU power capping methods (even
after they are extended to use of memory DVFS as well)."
"This paper assesses the performance of mobile messaging and VoIP connections.
We investigate the CPU usage of WhatsApp and IMO under different scenarios.
This analysis also enabled a comparison of the performance of these
applications on two Android operating system (OS) versions: KitKat or Lollipop.
Two models of smartphones were considered, viz. Galaxy Note 4 and Galaxy S4.
The applications behavior was statistically investigated for both sending and
receiving VoIP calls. Connections have been examined over 3G and WiFi. The
handset model plays a decisive role in CPU usage of the application. t-tests
showed that IMO has a better performance that WhatsApp whatever be the Android
at a significance level 1%, on Galaxy Note 4. In contrast, WhatsApp requires
less CPU than IMO on Galaxy S4 whatever be the OS and access (3G/WiFi). Galaxy
Note 4 using WiFi always outperformed S4 in terms of processing efficiency."
"We represent a computer cluster as a multi-server queue with some arbitrary
bipartite graph of compatibilities between jobs and servers. Each server
processes its jobs sequentially in FCFS order. The service rate of a job at any
given time is the sum of the service rates of all servers processing this job.
We show that the corresponding queue is quasi-reversible and use this property
to design a scheduling algorithm achieving balanced fair sharing of the service
capacity."
"We consider the peak age-of-information (PAoI) in an M/M/1 queueing system
with packet delivery error, i.e., update packets can get lost during
transmissions to their destination. We focus on two types of policies, one is
to adopt Last-Come-First-Served (LCFS) scheduling, and the other is to utilize
retransmissions, i.e., keep transmitting the most recent packet. Both policies
can effectively avoid the queueing delay of a busy channel and ensure a small
PAoI. Exact PAoI expressions under both policies with different error
probabilities are derived, including First-Come-First-Served (FCFS), LCFS with
preemptive priority, LCFS with non-preemptive priority, Retransmission with
preemptive priority, and Retransmission with non-preemptive priority. Numerical
results obtained from analysis and simulation are presented to validate our
results."
"We address the problem of giving robust performance bounds based on the study
of the asymptotic behavior of the insensitive load balancing schemes when the
number of servers and the load scales jointly. These schemes have the desirable
property that the stationary distribution of the resulting stochastic network
depends on the distribution of job sizes only through its mean. It was shown
that they give good estimates of performance indicators for systems with finite
buffers, generalizing henceforth Erlang's formula whereas optimal policies are
already theoretically and computationally out of reach for networks of moderate
size. We study a single class of traffic acting on a symmetric set of processor
sharing queues with finite buffers and we consider the case where the load
scales with the number of servers. We characterize central limit theorems and
large deviations, the response of symmetric systems under those schemes at
different scales and show that three amplitudes of deviations can be
identified. A central limit scaling takes place for a sub-critical load; for
$\rho=1$, the number of free servers scales like $n^{ {\theta \over \theta+1}}$
($\theta$ being the buffer depth and $n$ being the number of servers) and is of
order 1 for super-critical loads. This further implies the existence of
different phases for the blocking probability, Before a (refined) critical load
$\rho_c(n)=1-a n^{- {\theta \over \theta+1}}$, the blocking is exponentially
small and becomes of order $ n^{- {\theta \over \theta+1}}$ at $\rho_c(n)$.
This generalizes the well-known Quality and Efficiency Driven (QED) regime or
Halfin-Whitt regime for a one-dimensional queue, and leads to a generalized
staffing rule for a given target blocking probability."
"Recent development of peer-to-peer (P2P) services (e.g. streaming, file
sharing, and storage) systems introduces a new type of queue systems that
receive little attention before, where both job and server arrive and depart
randomly. Current study on these models focuses on the stability condition,
under exponential workload assumption. This paper extends existing result in
two aspects. In the first part of the paper we relax the exponential workload
assumption, and study the stability of systems with general workload
distribution. The second part of the paper focuses on the job sojourn time. An
upper bound and a lower bound for job sojourn time are investigated. We
evaluate tightness of the bounds by numerical analysis."
"FIFO is perhaps the simplest scheduling discipline. For single-class FIFO,
its delay guarantee performance has been extensively studied: The well-known
results include a stochastic delay bound for $GI/GI/1$ by Kingman and a
deterministic delay bound for $D/D/1$ by Cruz. However, for multiclass FIFO,
few such results are available. To fill the gap, we prove delay bounds for
multiclass FIFO in this work, considering both deterministic and stochastic
cases. Specifically, delay bounds are presented for $D/D/1$, $G/D/1$, $GI/D/1$,
and $GI/GI/1$, all under multiclass FIFO."
"The performance analysis of peer-to-peer (P2P) networks calls for a new kind
of queueing model, in which jobs and service stations arrive randomly. Except
in some simple special cases, in general, the queueing model with varying
service rate is mathematically intractable. Motivated by the P-K formula for
M/G/1 queue, we developed a limiting analysis approach based on the connection
between the fluctuation of service rate and the mean queue length. Considering
the two extreme service rates, we proved the conjecture on the lower bound and
upper bound of mean queue length previously postulated. Furthermore, an
approximate P-K formula to estimate the mean queue length is derived from the
convex combination of these two bounds and the conditional mean queue length
under the overload condition. We confirmed the accuracy of our approximation by
extensive simulation studies with different system parameters. We also verified
that all limiting cases of the system behavior are consistent with the
predictions of our formula."
"In simulation-based optimization of queuing systems, the presence of
discrete-valued parameters (such as buffer sizes and the number of servers)
makes the optimization difficult. We propose a novel technique for embedding
such discrete parameters into a continuous space, so that optimization can be
performed efficiently using continuous-space methods. Unlike spatial
interpolation, our embedding technique is based on a randomization of the
simulation model. The interpolated value can be computed using a single
simulation of this randomized model, irrespective of the number of parameters.
We first study the theoretical properties of such a randomization scheme
applied to M/M/1 and Geom/Geom/1 queues. We prove that the randomization
produces valid interpolations of the steady-state performance functions with
respect to an integer service-time parameter. We then show that such an
embedding is possible for more complex queueing networks whose parameters can
include deterministic service times, queue capacities, and the number of
servers, and empirically demonstrate that the interpolation is well behaved. To
demonstrate the utility of the embedding technique, we solve a 6-parameter
queuing network optimization problem by embedding the discrete parameters into
a continuous space. The technique produces smooth interpolations of the
objective function, and a continuous optimizer applied directly over this
embedding converges rapidly, producing good solutions."
"In recent years, there is an increasing demand of big memory systems so to
perform large scale data analytics. Since DRAM memories are expensive, some
researchers are suggesting to use other memory systems such as non-volatile
memory (NVM) technology to build large-memory computing systems. However,
whether the NVM technology can be a viable alternative (either economically and
technically) to DRAM remains an open question. To answer this question, it is
important to consider how to design a memory system from a ""system
perspective"", that is, incorporating different performance characteristics and
price ratios from hybrid memory devices.
  This paper presents an analytical model of a ""hybrid page cache system"" so to
understand the diverse design space and performance impact of a hybrid cache
system. We consider (1) various architectural choices, (2) design strategies,
and (3) configuration of different memory devices. Using this model, we provide
guidelines on how to design hybrid page cache to reach a good trade-off between
high system throughput (in I/O per sec or IOPS) and fast cache reactivity which
is defined by the time to fill the cache. We also show how one can configure
the DRAM capacity and NVM capacity under a fixed budget. We pick PCM as an
example for NVM and conduct numerical analysis. Our analysis indicates that
incorporating PCM in a page cache system significantly improves the system
performance, and it also shows larger benefit to allocate more PCM in page
cache in some cases. Besides, for the common setting of performance-price ratio
of PCM, ""flat architecture"" offers as a better choice, but ""layered
architecture"" outperforms if PCM write performance can be significantly
improved in the future."
"Parallel programming is emerging fast and intensive applications need more
resources, so there is a huge demand for on-chip multiprocessors. Accessing L1
caches beside the cores are the fastest after registers but the size of private
caches cannot increase because of design, cost and technology limits. Then
split I-cache and D-cache are used with shared LLC (last level cache). For a
unified shared LLC, bus interface is not scalable, and it seems that
distributed shared LLC (DSLLC) is a better choice. Most of papers assume a
distributed shared LLC beside each core in on-chip network. Many works assume
that DSLLCs are placed in all cores; however, we will show that this design
ignores the effect of traffic congestion in on-chip network. In fact, our work
focuses on optimal placement of cores, DSLLCs and even memory controllers to
minimize the expected latency based on traffic load in a mesh on-chip network
with fixed number of cores and total cache capacity. We try to do some
analytical modeling deriving intended cost function and then optimize the mean
delay of the on-chip network communication. This work is supposed to be
verified using some traffic patterns that are run on CSIM simulator."
"Improving performance is a central concern for software developers. To locate
optimization opportunities, developers rely on software profilers. However,
these profilers only report where programs spent their time: optimizing that
code may have no impact on performance. Past profilers thus both waste
developer time and make it difficult for them to uncover significant
optimization opportunities.
  This paper introduces causal profiling. Unlike past profiling approaches,
causal profiling indicates exactly where programmers should focus their
optimization efforts, and quantifies their potential impact. Causal profiling
works by running performance experiments during program execution. Each
experiment calculates the impact of any potential optimization by virtually
speeding up code: inserting pauses that slow down all other code running
concurrently. The key insight is that this slowdown has the same relative
effect as running that line faster, thus ""virtually"" speeding it up.
  We present Coz, a causal profiler, which we evaluate on a range of
highly-tuned applications: Memcached, SQLite, and the PARSEC benchmark suite.
Coz identifies previously unknown optimization opportunities that are both
significant and targeted. Guided by Coz, we improve the performance of
Memcached by 9%, SQLite by 25%, and accelerate six PARSEC applications by as
much as 68%; in most cases, these optimizations involve modifying under 10
lines of code."
"The life-cycle of a partial differential equation (PDE) solver is often
characterized by three development phases: the development of a stable
numerical discretization, development of a correct (verified) implementation,
and the optimization of the implementation for different computer
architectures. Often it is only after significant time and effort has been
invested that the performance bottlenecks of a PDE solver are fully understood,
and the precise details varies between different computer architectures. One
way to mitigate this issue is to establish a reliable performance model that
allows a numerical analyst to make reliable predictions of how well a numerical
method would perform on a given computer architecture, before embarking upon
potentially long and expensive implementation and optimization phases. The
availability of a reliable performance model also saves developer effort as it
both informs the developer on what kind of optimisations are beneficial, and
when the maximum expected performance has been reached and optimisation work
should stop. We show how discretization of a wave equation can be theoretically
studied to understand the performance limitations of the method on modern
computer architectures. We focus on the roofline model, now broadly used in the
high-performance computing community, which considers the achievable
performance in terms of the peak memory bandwidth and peak floating point
performance of a computer with respect to algorithmic choices. A first
principles analysis of operational intensity for key time-stepping
finite-difference algorithms is presented. With this information available at
the time of algorithm design, the expected performance on target computer
systems can be used as a driver for algorithm design."
"We propose a benchmarking strategy that is robust in the presence of timer
error, OS jitter and other environmental fluctuations, and is insensitive to
the highly nonideal statistics produced by timing measurements. We construct a
model that explains how these strongly nonideal statistics can arise from
environmental fluctuations, and also justifies our proposed strategy. We
implement this strategy in the BenchmarkTools Julia package, where it is used
in production continuous integration (CI) pipelines for developing the Julia
language and its ecosystem."
"We investigate an approach that uses low-level analysis and the
execution-cache-memory (ECM) performance model in combination with tuning of
hardware parameters to lower energy requirements of memory-bound applications.
The ECM model is extended appropriately to deal with software optimizations
such as non-temporal stores. Using incremental steps and the ECM model, we
analytically quantify the impact of various single-core optimizations and
pinpoint microarchitectural improvements that are relevant to energy
consumption. Using a 2D Jacobi solver as example that can serve as a blueprint
for other memory-bound applications, we evaluate our approach on the four most
recent Intel Xeon E5 processors (Sandy Bridge-EP, Ivy Bridge-EP, Haswell-EP,
and Broadwell-EP). We find that chip energy consumption can be reduced in the
range of 2.0-2.4$\times$ on the examined processors."
"We consider a system of processor-sharing queues with state-dependent service
rates. These are allocated according to balanced fairness within a polymatroid
capacity set. Balanced fairness is known to be both insensitive and
Pareto-efficient in such systems, which ensures that the performance metrics,
when computable, will provide robust insights into the real performance of the
system considered. We first show that these performance metrics can be
evaluated with a complexity that is polynomial in the system size if the system
is partitioned into a finite number of parts, so that queues are exchangeable
within each part and asymmetric across different parts. This in turn allows us
to derive stochastic bounds for a larger class of systems which satisfy less
restrictive symmetry assumptions. These results are applied to practical
examples of tree data networks, such as backhaul networks of Internet service
providers, and computer clusters."
"Despite computation becomes much complex on data with an unprecedented scale,
we argue computers or smart devices should and will consistently provide
information and knowledge to human being in the order of a few tens
milliseconds. We coin a new term 10-millisecond computing to call attention to
this class of workloads. 10-millisecond computing raises many challenges for
both software and hardware stacks. In this paper, using a typical
workload-memcached on a 40-core server (a main-stream server in near future),
we quantitatively measure 10-ms computing's challenges to conventional
operating systems. For better communication, we propose a simple metric-outlier
proportion to measure quality of service: for N completed requests or jobs, if
M jobs or requests' latencies exceed the outlier threshold t, the outlier
proportion is M/N . For a 1K-scale system running Linux (version 2.6.32), LXC
(version 0.7.5) or XEN (version 4.0.0), respectively, we surprisingly find that
so as to reduce the service outlier proportion to 10% (10% users will feel QoS
degradation), the outlier proportion of a single server has to be reduced by
871X, 2372X, 2372X accordingly. Also, we discuss the possible design spaces of
10-ms computing systems from perspectives of datacenter architectures,
networking, OS and scheduling, and benchmarking."
"A breakdown of a benchmark score is how much each aspect of the system
performance affects the score. Existing methods require internal analysis on
the benchmarking program and then involve the following problems: (1) require a
certain amount of labor for code analysis, profiling, simulation, and so on and
(2) require the benchmarking program itself. In this paper, we present a method
for breaking down a benchmark score without internal analysis of the
benchmarking program. The method utilizes regression analysis of benchmark
scores on a number of systems. Experimental results with 3 benchmarks on 15
Android smartphones showed that our method could break down those benchmark
scores even though there is room for improvement in accuracy."
"The term ""performance portability"" has been informally used in computing to
refer to a variety of notions which generally include: 1) the ability to run
one application across multiple hardware platforms; and 2) achieving some
notional level of performance on these platforms. However, there has been a
noticeable lack of consensus on the precise meaning of the term, and authors'
conclusions regarding their success (or failure) to achieve performance
portability have thus been subjective. Comparing one approach to performance
portability with another has generally been marked with vague claims and
verbose, qualitative explanation of the comparison. This paper presents a
concise definition for performance portability, along with a simple metric that
accurately captures the performance and portability of an application across
different platforms. The utility of this metric is then demonstrated with a
retroactive application to previous work."
"""Geographic Load Balancing"" is a strategy for reducing the energy cost of
data centers spreading across different terrestrial locations. In this paper,
we focus on load balancing among micro-datacenters powered by renewable energy
sources. We model via a Markov Chain the problem of scheduling jobs by
prioritizing datacenters where renewable energy is currently available. Not
finding a convenient closed form solution for the resulting chain, we use mean
field techniques to derive an asymptotic approximate model which instead is
shown to have an extremely simple and intuitive steady state solution. After
proving, using both theoretical and discrete event simulation results, that the
system performance converges to the asymptotic model for an increasing number
of datacenters, we exploit the simple closed form model's solution to
investigate relationships and trade-offs among the various system parameters."
"Fork-Join (FJ) queueing models capture the dynamics of system parallelization
under synchronization constraints, for example, for applications such as
MapReduce, multipath transmission and RAID systems. Arriving jobs are first
split into tasks and mapped to servers for execution, such that a job can only
leave the system when all of its tasks are executed.
  In this paper, we provide computable stochastic bounds for the waiting and
response time distributions for heterogeneous FJ systems under general
parallelization benefit. Our main contribution is a generalized mathematical
framework for probabilistic server scheduling strategies that are essentially
characterized by a probability distribution over the number of utilized
servers, and the optimization thereof. We highlight the trade-off between the
scaling benefit due to parallelization and the FJ inherent synchronization
penalty. Further, we provide optimal scheduling strategies for arbitrary
scaling regimes that map to different levels of parallelization benefit. One
notable insight obtained from our results is that different applications with
varying parallelization benefits result in different optimal strategies.
Finally, we complement our analytical results by applying them to various
applications showing the optimality of the proposed scheduling strategies."
"Frameworks, such as MapReduce and Hadoop are abundant nowadays. They seek to
reap benefits of parallelization, albeit subject to a synchronization
constraint at the output. Fork-Join (FJ) queuing models are used to analyze
such systems. Arriving jobs are split into tasks each of which is mapped to
exactly one server. A job leaves the system when all of its tasks are executed.
  As a metric of performance, we consider waiting times for both
work-conserving and non-work conserving server systems under a mathematical
set-up general enough to take into account possible phase-type behavior of the
servers, and as suggested by recent evidences, bursty arrivals.
  To this end, we present a Markov-additive process framework for an FJ system
and provide computable bounds on tail probabilities of steady-state waiting
times, for both types of servers separately. We apply our results to three
scenarios, namely, non-renewal (Markov-modulated) arrivals, servers showing
phase-type behavior, and Markov-modulated arrivals and services. We compare our
bounds against estimates obtained through simulations and also provide a
theoretical conceptualization of provisions in FJ systems. Finally, we
calibrate our model with real data traces, and illustrate how our bounds can be
used to devise provisions."
"In this paper, we present a stochastic queuing model for the road traffic,
which captures the stationary density-flow relationships in both uncongested
and congestion conditions. The proposed model is based on the $M/g/c/c$ state
dependent queuing model of Jain and Smith, and is inspired from the
deterministic Godunov scheme for the road traffic simulation. We first propose
a reformulation of the $M/g/c/c$ state dependent model that works with
density-flow fundamental diagrams rather than density-speed relationships. We
then extend this model in order to consider upstream traffic demand as well as
downstream traffic supply. Finally, we calculate the speed and travel time
distributions for the $M/g/c/c$ state dependent queuing model and for the
proposed model, and derive stationary performance measures (expected number of
cars, blocking probability, expected travel time, and throughput). A comparison
with results predicted by the $M/g/c/c$ state dependent queuing model shows
that the proposed model correctly represents the dynamics of traffic and gives
good performances measures. The results illustrate the good accuracy of the
proposed model."
"Graphics Processing Units (GPUs) support dynamic voltage and frequency
scaling (DVFS) in order to balance computational performance and energy
consumption. However, there still lacks simple and accurate performance
estimation of a given GPU kernel under different frequency settings on real
hardware, which is important to decide best frequency configuration for energy
saving. This paper reveals a fine-grained model to estimate the execution time
of GPU kernels with both core and memory frequency scaling. Over a 2.5x range
of both core and memory frequencies among 12 GPU kernels, our model achieves
accurate results (within 3.5\%) on real hardware. Compared with the cycle-level
simulators, our model only needs some simple micro-benchmark to extract a set
of hardware parameters and performance counters of the kernels to produce this
high accuracy."
"We consider a multi-server queueing system under the power-of-two policy with
Poisson job arrivals, heterogeneous servers and a general job requirement
distribution; each server operates under the first-come first-serve policy and
there are no buffer constraints. We analyze the performance of this system in
light traffic by evaluating the first two light traffic derivatives of the
average job response time. These expressions point to several interesting
structural features associated with server heterogeneity in light traffic: For
unequal capacities, the average job response time is seen to decrease for small
values of the arrival rate, and the more diverse the server speeds, the greater
the gain in performance. These theoretical findings are assessed through
limited simulations."
"Continuous Time Markov Chain (CMTC) is widely used to describe and analyze
systems in several knowledge areas. Steady state availability is one important
analysis that can be made through Markov chain formalism that allows
researchers generate equations for several purposes, such as channel capacity
estimation in wireless networks as well as system performance estimations. The
problem with this kind of analysis is the complex process to generating these
equations. In this letter, we have developed general equations for decision and
sequential processes of CMTC Models, aiming to help researchers to develop
steady state availability equations. We also have developed the general
equation here termed as Closed Decision Process."
"Achieving optimal program performance requires deep insight into the
interaction between hardware and software. For software developers without an
in-depth background in computer architecture, understanding and fully utilizing
modern architectures is close to impossible. Analytic loop performance modeling
is a useful way to understand the relevant bottlenecks of code execution based
on simple machine models. The Roofline Model and the Execution-Cache-Memory
(ECM) model are proven approaches to performance modeling of loop nests. In
comparison to the Roofline model, the ECM model can also describes the
single-core performance and saturation behavior on a multicore chip. We give an
introduction to the Roofline and ECM models, and to stencil performance
modeling using layer conditions (LC). We then present Kerncraft, a tool that
can automatically construct Roofline and ECM models for loop nests by
performing the required code, data transfer, and LC analysis. The layer
condition analysis allows to predict optimal spatial blocking factors for loop
nests. Together with the models it enables an ab-initio estimate of the
potential benefits of loop blocking optimizations and of useful block sizes. In
cases where LC analysis is not easily possible, Kerncraft supports a cache
simulator as a fallback option. Using a 25-point long-range stencil we
demonstrate the usefulness and predictive power of the Kerncraft tool."
"The aim of this study is the characterization of the computing resources used
by researchers at the ""Instituto de Astrof\'isica de Canarias"" (IAC). Since
there is a huge demand of computing time and we use tools such as HTCondor to
implement High Throughput Computing (HTC) across all available PCs, it is
essential for us to assess in a quantitative way, using objective parameters,
the performances of our computing nodes. In order to achieve that, we have run
a set of benchmark tests on a number of different desktop and laptop PC models
among those used in our institution. In particular, we run the ""Polyhedron
Fortran Benchmarks"" suite, using three different compilers: GNU Fortran
Compiler, Intel Fortran Compiler and the PGI Fortran Compiler; execution times
are then normalized to the reference values published by Polyhedron. The same
tests were run multiple times on a same PCs, and on 3 to 5 PCs of the same
model (whenever possible) to check for repeatability and consistency of the
results. We found that in general execution times, for a given PC model, are
consistent within an uncertainty of about 10%, and show a gain in CPU speed of
a factor of about 3 between the oldest PCs used at the IAC (7-8 years old) and
the newest ones."
"This paper presents a survey of architectural features among four generations
of Intel server processors (Sandy Bridge, Ivy Bridge, Haswell, and Broad- well)
with a focus on performance with floating point workloads. Starting on the core
level and going down the memory hierarchy we cover instruction throughput for
floating-point instructions, L1 cache, address generation capabilities, core
clock speed and its limitations, L2 and L3 cache bandwidth and latency, the
impact of Cluster on Die (CoD) and cache snoop modes, and the Uncore clock
speed. Using microbenchmarks we study the influence of these factors on code
performance. This insight can then serve as input for analytic performance
models. We show that the energy efficiency of the LINPACK and HPCG benchmarks
can be improved considerably by tuning the Uncore clock speed without
sacrificing performance, and that the Graph500 benchmark performance may profit
from a suitable choice of cache snoop mode settings."
"Supermarket models are a class of interesting parallel queueing networks with
dynamic randomized load balancing and real-time resource management. When the
parallel servers are subject to breakdowns and repairs, analysis of such a
supermarket model becomes more difficult and challenging. In this paper, we
apply the mean-field theory to studying four interrelated supermarket models
with repairable servers, and numerically indicate impact of the different
repairman groups on performance of the systems. First, we set up the systems of
mean-field equations for the supermarket models with repairable servers. Then
we prove the asymptotic independence of the supermarket models through the
operator semi-group and the mean-field limit. Furthermore, we show that the
fixed points of the supermarket models satisfy the systems of nonlinear
equations. Finally, we use the fixed points to give numerical computation for
performer analysis, and provide valuable observations on model improvement.
Therefore, this paper provides a new and effective method in the study of
complex supermarket models."
"Many different caching mechanisms have been previously proposed, exploring
different insertion and eviction policies and their performance individually
and as part of caching networks. We obtain a novel closed-form stationary
invariant distribution for a generalization of LRU and MRU caching nodes under
a reference Markov model. Numerical comparisons are made with an ""Incremental
Rank Progress"" (IRP a.k.a. CLIMB) and random eviction (a.k.a. random
replacement) methods under a steady-state Zipf popularity distribution. The
range of cache hit probabilities is smaller under MRU and larger under IRP
compared to LRU. We conclude with the invariant distribution for a special case
of a random-eviction caching tree-network and associated discussion."
"Asymptotic properties of Markov Processes, such as steady state probabilities
or hazard rate for absorbing states can be efficiently calculated by means of
linear algebra even for large-scale problems. This paper discusses the methods
for adjusting parameters of the Markov models to account for non-constant
transition rates. In particular, transitions with fixed delays are considered
along with the transitions that follow Weibull and lognormal distributions.
Procedures for both steady-state solutions in the absence of an absorbing
state, and for hazard rates to an absorbing state are provided and demonstrated
on several examples."
"We consider the problem of scheduling a group of heterogeneous, distributed
processes, with different relative priorities and service preferences, to a
group of heterogeneous virtual machines. Assuming linearly elastic IT resource
needs, we extend prior results on proportional fair and max-min fair scheduling
to a constrained multiresource case for a fam- ily of fairness criteria
(including our recently proposed Per- Server Dominant-Share Fairness).
Performance comparison is made by illustrative numerical example. We conclude
with a discussion of scheduling problems for a public cloud with heterogeneous
instances and servers."
"The performance model of an application can pro- vide understanding about its
runtime behavior on particular hardware. Such information can be analyzed by
developers for performance tuning. However, model building and analyzing is
frequently ignored during software development until perfor- mance problems
arise because they require significant expertise and can involve many
time-consuming application runs. In this paper, we propose a fast, accurate,
flexible and user-friendly tool, Mira, for generating performance models by
applying static program analysis, targeting scientific applications running on
supercomputers. We parse both the source code and binary to estimate
performance attributes with better accuracy than considering just source or
just binary code. Because our analysis is static, the target program does not
need to be executed on the target architecture, which enables users to perform
analysis on available machines instead of conducting expensive exper- iments on
potentially expensive resources. Moreover, statically generated models enable
performance prediction on non-existent or unavailable architectures. In
addition to flexibility, because model generation time is significantly reduced
compared to dynamic analysis approaches, our method is suitable for rapid
application performance analysis and improvement. We present several scientific
application validation results to demonstrate the current capabilities of our
approach on small benchmarks and a mini application."
"The notion of computer capacity was proposed in 2012, and this quantity has
been estimated for computers of different kinds.
  In this paper we show that, when designing new processors, the manufacturers
change the parameters that affect the computer capacity. This allows us to
predict the values of parameters of future processors. As the main example we
use Intel processors, due to the accessibility of detailed description of all
their technical characteristics."
"We investigate fusing several unreliable computational units that perform the
same task. We model an unreliable computational outcome as an additive
perturbation to its error-free result in terms of its fidelity and cost. We
analyze performance of repetition-based strategies that distribute cost across
several unreliable units and fuse their outcomes. When the cost is a convex
function of fidelity, the optimal repetition-based strategy in terms of
incurred cost while achieving a target mean-square error (MSE) performance may
fuse several computational units. For concave and linear costs, a single more
reliable unit incurs lower cost compared to fusion of several lower cost and
less reliable units while achieving the same MSE performance. We show how our
results give insight into problems from theoretical neuroscience, circuits, and
crowdsourcing."
"The classical secretary problem has been generalized over the years into
several directions. In this paper we confine our interest to those
generalizations which have to do with the more general problem of stopping on a
last observation of a specific kind. We follow Dendievel, (where a bibliography
can be found) who studies several types of such problems, mainly initiated by
Bruss and Weber. Whether in discrete time or continuous time, whether all
parameters are known or must be sequentially estimated, we shall call such
problems simply ""Bruss-Weber problems"". Our contribution in the present paper
is a refined analysis of several problems in this class and a study of the
asymptotic behaviour of solutions.
  The problems we consider center around the following model. Let
$X_1,X_2,\ldots,X_n$ be a sequence of independent random variables which can
take three values: $\{+1,-1,0\}.$ Let $p:=\P(X_i=1), p':=\P(X_i=-1),
\qt:=\P(X_i=0), p\geq p'$, where $p+p'+\qt=1$. The goal is to maximize the
probability of stopping on a value $+1$ or $-1$ appearing for the last time in
the sequence. Following a suggestion by Bruss, we have also analyzed an
x-strategy with incomplete information: the cases $p$ known, $n$ unknown, then
$n$ known, $p$ unknown and finally $n,p$ unknown are considered. We also
present simulations of the corresponding complete selection algorithm."
"Resource usage data, collected using tools such as TACC Stats, capture the
resource utilization by nodes within a high performance computing system. We
present methods to analyze the resource usage data to understand the system
performance and identify performance anomalies. The core idea is to model the
data as a three-way tensor corresponding to the compute nodes, usage metrics,
and time. Using the reconstruction error between the original tensor and the
tensor reconstructed from a low rank tensor decomposition, as a scalar
performance metric, enables us to monitor the performance of the system in an
online fashion. This error statistic is then used for anomaly detection that
relies on the assumption that the normal/routine behavior of the system can be
captured using a low rank approx- imation of the original tensor. We evaluate
the performance of the algorithm using information gathered from system logs
and show that the performance anomalies identified by the proposed method
correlates with critical errors reported in the system logs. Results are shown
for data collected for 2013 from the Lonestar4 system at the Texas Advanced
Computing Center (TACC)"
"This dissertation introduces measurement-based performance modeling and
prediction techniques for dense linear algebra algorithms. As a core principle,
these techniques avoid executions of such algorithms entirely, and instead
predict their performance through runtime estimates for the underlying compute
kernels. For a variety of operations, these predictions allow to quickly select
the fastest algorithm configurations from available alternatives. We consider
two scenarios that cover a wide range of computations:
  To predict the performance of blocked algorithms, we design
algorithm-independent performance models for kernel operations that are
generated automatically once per platform. For various matrix operations,
instantaneous predictions based on such models both accurately identify the
fastest algorithm, and select a near-optimal block size.
  For performance predictions of BLAS-based tensor contractions, we propose
cache-aware micro-benchmarks that take advantage of the highly regular
structure inherent to contraction algorithms. At merely a fraction of a
contraction's runtime, predictions based on such micro-benchmarks identify the
fastest combination of tensor traversal and compute kernel."
"A key function of cloud infrastructure is to store and deliver diverse files,
e.g., scientific datasets, social network information, videos, etc. In such
systems, for the purpose of fast and reliable delivery, files are divided into
chunks, replicated or erasure-coded, and disseminated across servers. It is
neither known in general how delays scale with the size of a request nor how
delays compare under different policies for coding, data dissemination, and
delivery.
  Motivated by these questions, we develop and explore a set of evolution
equations as a unified model which captures the above features. These equations
allow for both efficient simulation and mathematical analysis of several
delivery policies under general statistical assumptions. In particular, we
quantify in what sense a workload aware delivery policy performs better than a
workload agnostic policy. Under a dynamic or stochastic setting, the sample
path comparison of these policies does not hold in general. The comparison is
shown to hold under the weaker increasing convex stochastic ordering, still
stronger than the comparison of averages.
  This result further allows us to obtain insightful computable performance
bounds. For example, we show that in a system where files are divided into
chunks of equal size, replicated or erasure-coded, and disseminated across
servers at random, the job delays increase sub-logarithmically in the request
size for small and medium-sized files but linearly for large files."
"In this paper, we theoretically analyze the performance of vehicle-to-vehicle
(V2V) broadcast communications at an intersection and provide tractable
formulae of performance metrics to optimize them."
"We propose an online auto-tuning approach for computing kernels. Differently
from existing online auto-tuners, which regenerate code with long compilation
chains from the source to the binary code, our approach consists on deploying
auto-tuning directly at the level of machine code generation. This allows
auto-tuning to pay off in very short-running applications. As a proof of
concept, our approach is demonstrated in two benchmarks, which execute during
hundreds of milliseconds to a few seconds only. In a CPU-bound kernel, the
average speedups achieved are 1.10 to 1.58 depending on the target
micro-architecture, up to 2.53 in the most favourable conditions (all run-time
overheads included). In a memory-bound kernel, less favourable to our runtime
auto-tuning optimizations, the average speedups are 1.04 to 1.10, up to 1.30 in
the best configuration. Despite the short execution times of our benchmarks,
the overhead of our runtime auto-tuning is between 0.2 and 4.2% only of the
total application execution times. By simulating the CPU-bound application in
11 different CPUs, we showed that, despite the clear hardware disadvantage of
In-Order (io) cores vs. Out-of-Order (ooo) equivalent cores, online auto-tuning
in io CPUs obtained an average speedup of 1.03 and an energy efficiency
improvement of 39~\% over the SIMD reference in ooo CPUs."
"Mobile applications are benefiting significantly from the advancement in deep
learning, e.g. providing new features. Given a trained deep learning model,
applications usually need to perform a series of matrix operations based on the
input data, in order to infer possible output values. Because of model
computation complexity and increased model sizes, those trained models are
usually hosted in the cloud. When mobile apps need to utilize those models,
they will have to send input data over the network. While cloud-based deep
learning can provide reasonable response time for mobile apps, it also
restricts the use case scenarios, e.g. mobile apps need to have access to
network. With mobile specific deep learning optimizations, it is now possible
to employ device-based inference. However, because mobile hardware, e.g. GPU
and memory size, can be very different and limited when compared to desktop
counterpart, it is important to understand the feasibility of this new
device-based deep learning inference architecture. In this paper, we
empirically evaluate the inference efficiency of three Convolutional Neural
Networks using a benchmark Android application we developed. Based on our
application-driven analysis, we have identified several performance bottlenecks
for mobile applications powered by on-device deep learning inference."
"To keep pace with Moore's law, chip designers have focused on increasing the
number of cores per chip rather than single core performance. In turn, modern
jobs are often designed to run on any number of cores. However, to effectively
leverage these multi-core chips, one must address the question of how many
cores to assign to each job. Given that jobs receive sublinear speedups from
additional cores, there is an obvious tradeoff: allocating more cores to an
individual job reduces the job's runtime, but in turn decreases the efficiency
of the overall system. We ask how the system should schedule jobs across cores
so as to minimize the mean response time over a stream of incoming jobs.
  To answer this question, we develop an analytical model of jobs running on a
multi-core machine. We prove that EQUI, a policy which continuously divides
cores evenly across jobs, is optimal when all jobs follow a single speedup
curve and have exponentially distributed sizes. EQUI requires jobs to change
their level of parallelization while they run. Since this is not possible for
all workloads, we consider a class of ""fixed-width"" policies, which choose a
single level of parallelization, k, to use for all jobs. We prove that,
surprisingly, it is possible to achieve EQUI's performance without requiring
jobs to change their levels of parallelization by using the optimal fixed level
of parallelization, k*. We also show how to analytically derive the optimal k*
as a function of the system load, the speedup curve, and the job size
distribution.
  In the case where jobs may follow different speedup curves, finding a good
scheduling policy is even more challenging. We find that policies like EQUI
which performed well in the case of a single speedup function now perform
poorly. We propose a very simple policy, GREEDY*, which performs near-optimally
when compared to the numerically-derived optimal policy."
"In modern data centers, energy usage represents one of the major factors
affecting operational costs. Power capping is a technique that limits the power
consumption of individual systems, which allows reducing the overall power
demand at both cluster and data center levels. However, literature power
capping approaches do not fit well the nature of important applications based
on first-class multi-thread technology. For these applications performance may
not grow linearly as a function of the thread-level parallelism because of the
need for thread synchronization while accessing shared resources, such as
shared data. In this paper we consider the problem of maximizing the
application performance under a power cap by dynamically tuning the
thread-level parallelism and the power state of the CPU-cores. Based on
experimental observations, we design an adaptive technique that aims at setting
the best combination of thread-level parallelism and CPU-core power state
depending on the workload profile of the multi-threaded application. We
evaluate our proposal by relying on different benchmarks, configured to use
different thread synchronization methods. This makes the set of tested
configurations wide and increases the representativeness of the experimental
outcomes."
"Traditional UNIX time-share schedulers attempt to be fair to all users by
employing a round-robin style algorithm for allocating CPU time. Unfortunately,
a loophole exists whereby the scheduler can be biased in favor of a greedy user
running many short CPU-time processes. This loophole is not a defect but an
intrinsic property of the round-robin scheduler that ensures responsiveness to
the short CPU demands associated with multiple interactive users. A new
generation of UNIX system resource management software constrains the scheduler
to be equitable to all users regardless of the number of processes each may be
running. This ""fair-share"" scheduling draws on the concept of pro rating
resource ""shares"" across users and groups and then dynamically adjusting CPU
usage to meet those share proportions. The simple notion of statically
allocating these shares, however, belies the potential consequences for
performance as measured by user response time and service level targets. We
demonstrate this point by modeling several simple share allocation scenarios
and analyzing the corresponding performance effects. A brief comparison of
commercial system resource management implementations from HP, IBM, and SUN is
also given."
"The latest implementations of commercial UNIX to offer mainframe style
capacity management on enterprise servers include: AIX Workload Manager (WLM),
HP-UX Process Resource Manager (PRM), Solaris Resource Manager (SRM), as well
as SGI and Compaq. The ability to manage server capacity is achieved by making
significant modifications to the standard UNIX operating system so that
processes are inherently tied to specific users. Those users, in turn, are
granted only a certain fraction of system resources. Resource usage is
monitored and compared with each users grant to ensure that the assigned
entitlement constraints are met. In this paper, we begin by clearing up some of
the confusion that has surrounded the motivation and the terminology behind the
new technology. The common theme across each of the commercial implementations
is the introduction of the fair-share scheduler. After reviewing some potential
performance pitfalls, we present capacity planning guidelines for migrating to
automated UNIX resource management."
"Many scientific applications use the X11 window environment; an open source
windows GUI standard employing a client/server architecture. X11 promotes:
distributed computing, thin-client functionality, cheap desktop displays,
compatibility with heterogeneous servers, remote services and administration,
and greater maturity than newer web technologies. This paper details the
author's investigations into close encounters with alien performance in
X11-based seismic applications running on a 200-node cluster, backed by 2 TB of
mass storage. End-users cited two significant UFOs (Unidentified Faulty
Operations) i) long application launch times and ii) poor interactive response
times. The paper is divided into three major sections describing Close
Encounters of the 1st Kind: citings of UFO experiences, the 2nd Kind: recording
evidence of a UFO, and the 3rd Kind: contact and analysis. UFOs do exist and
this investigation presents a real case study for evaluating workload analysis
and other diagnostic tools."
"We derive tight bounds on cache misses for evaluation of explicit stencil
operators on structured grids. Our lower bound is based on the isoperimetrical
property of the discrete octahedron. Our upper bound is based on good surface
to volume ratio of a parallelepiped spanned by a reduced basis of the inter-
ference lattice of a grid. Measurements show that our algorithm typically
reduces the number of cache misses by factor of three relative to a compiler
optimized code. We show that stencil calculations on grids whose interference
lattice have a short vector feature abnormally high numbers of cache misses. We
call such grids unfavorable and suggest to avoid these in computations by
appropriate padding. By direct measurements on MIPS R10000 we show a good
correlation of abnormally high cache misses and unfavorable three-dimensional
grids."
"Constraint propagation algorithms form an important part of most of the
constraint programming systems. We provide here a simple, yet very general
framework that allows us to explain several constraint propagation algorithms
in a systematic way. In this framework we proceed in two steps. First, we
introduce a generic iteration algorithm on partial orderings and prove its
correctness in an abstract setting. Then we instantiate this algorithm with
specific partial orderings and functions to obtain specific constraint
propagation algorithms.
  In particular, using the notions commutativity and semi-commutativity, we
show that the {\tt AC-3}, {\tt PC-2}, {\tt DAC} and {\tt DPC} algorithms for
achieving (directional) arc consistency and (directional) path consistency are
instances of a single generic algorithm. The work reported here extends and
simplifies that of Apt \citeyear{Apt99b}."
"A 100 Mbps Ethernet link between a college campus and the outside world was
monitored with a dedicated PC and the measured data analysed for its
statistical properties. Similar measurements were taken at an internal node of
the network. The networks in both cases are a full-duplex switched Ethernet.
Inter-event interval histograms and power spectra of the throughput aggregated
for 10ms bins were used to analyse the measured traffic. For most investigated
cases both methods reveal that the traffic behaves according to a power law.
The results will be used in later studies to parameterise models for network
traffic."
"Most popular, modern network simulators, such as ns, are targeted towards
simulating low-level protocol details. These existing simulators are not
intended for simulating large distributed applications with many hosts and many
concurrent connections over long periods of simulated time. We introduce a new
simulator, Narses, targeted towards large distributed applications. The goal of
Narses is to simulate and validate large applications efficiently using network
models of varying levels of detail. We introduce several simplifying
assumptions that allow our simulator to scale to the needs of large distributed
applications while maintaining a reasonable degree of accuracy. Initial results
show up to a 45 times speedup while consuming 28% of the memory used by ns.
Narses maintains a reasonable degree of accuracy -- within 8% on average."
"The Grid Datafarm architecture is designed for global petascale
data-intensive computing. It provides a global parallel filesystem with online
petascale storage, scalable I/O bandwidth, and scalable parallel processing,
and it can exploit local I/O in a grid of clusters with tens of thousands of
nodes. One of features is that it manages file replicas in filesystem metadata
for fault tolerance and load balancing.
  This paper discusses and evaluates several techniques to support
long-distance fast file replication. The Grid Datafarm manages a ranked group
of files as a Gfarm file, each file, called a Gfarm file fragment, being stored
on a filesystem node, or replicated on several filesystem nodes. Each Gfarm
file fragment is replicated independently and in parallel using rate-controlled
HighSpeed TCP with network striping. On a US-Japan testbed with 10,000 km
distance, we achieve 419 Mbps using 2 nodes on each side, and 741 Mbps using 4
nodes out of 893 Mbps with two transpacific networks."
"Performance profiling consists of tracing a software system during execution
and then analyzing the obtained traces. However, traces themselves affect the
performance of the system distorting its execution. Therefore, there is a need
to minimize the effect of the tracing on the underlying system's performance.
To achieve this, the trace set needs to be optimized according to the
performance profiling problem being solved. Our position is that such
minimization can be achieved only by adding the software trace design and
implementation to the overall software development process. In such a process,
the performance analyst supplies the knowledge of performance measurement
requirements, while the software developer supplies the knowledge of the
software. Both of these are needed for an optimal trace placement."
"Evolvable hardware combines the powerful search capability of evolutionary
algorithms with the flexibility of reprogrammable devices, thereby providing a
natural framework for reconfiguration. This framework has generated an interest
in using evolvable hardware for fault-tolerant systems because reconfiguration
can effectively deal with hardware faults whenever it is impossible to provide
spares. But systems cannot tolerate faults indefinitely, which means
reconfiguration does have a deadline. The focus of previous evolvable hardware
research relating to fault-tolerance has been primarily restricted to restoring
functionality, with no real consideration of time constraints. In this paper we
are concerned with evolvable hardware performing reconfiguration under deadline
constraints. In particular, we investigate reconfigurable hardware that
undergoes intrinsic evolution. We show that fault recovery done by intrinsic
reconfiguration has some restrictions, which designers cannot ignore."
"Benchmarking; by which I mean any computer system that is driven by a
controlled workload, is the ultimate in performance testing and simulation.
Aside from being a form of institutionalized cheating, it also offer countless
opportunities for systematic mistakes in the way the workloads are applied and
the resulting measurements interpreted. Right test, wrong conclusion is a
ubiquitous mistake that happens because test engineers tend to treat data as
divine. Such reverence is not only misplaced, it's also a sure ticket to
production hell when the application finally goes live. I demonstrate how such
mistakes can be avoided by means of two war stories that are real WOPRs. (a)
How to resolve benchmark flaws over the psychic hotline and (b) How benchmarks
can go flat with too much Java juice. In each case I present simple performance
models and show how they can be applied to correctly assess benchmark data."
"We present DiPerF, a distributed performance testing framework, aimed at
simplifying and automating service performance evaluation. DiPerF coordinates a
pool of machines that test a target service, collects and aggregates
performance metrics, and generates performance statistics. The aggregate data
collected provide information on service throughput, on service ""fairness"" when
serving multiple clients concurrently, and on the impact of network latency on
service performance. Furthermore, using this data, it is possible to build
predictive models that estimate a service performance given the service load.
We have tested DiPerF on 100+ machines on two testbeds, Grid3 and PlanetLab,
and explored the performance of job submission services (pre WS GRAM and WS
GRAM) included with Globus Toolkit 3.2."
"Arrival times of requests to print in a student laboratory were analyzed.
Inter-arrival times between subsequent requests follow a universal scaling law
relating time intervals and the size of the request, indicating a scale
invariant dynamics with respect to the size. The cumulative distribution of
file sizes is well-described by a modified power law often seen in
non-equilibrium critical systems. For each user, waiting times between their
individual requests show long range dependence and are broadly distributed from
seconds to weeks. All results are incompatible with Poisson models, and may
provide evidence of critical dynamics associated with voluntary thought
processes in the brain."
"Programming patterns for sequential file access in the .NET Framework are
described and the performance is measured. The default behavior provides
excellent performance on a single disk - 50 MBps both reading and writing.
Using large request sizes and doing file pre-allocation when possible have
quantifiable benefits. When one considers disk arrays, .NET unbuffered IO
delivers 800 MBps on a 16-disk array, but buffered IO delivers about 12% of
that performance. Consequently, high-performance file and database utilities
are still forced to use unbuffered IO for maximum sequential performance. The
report is accompanied by downloadable source code that demonstrates the
concepts and code that was used to obtain these measurements."
"The key to speeding up applications is often understanding where the elapsed
time is spent, and why. This document reviews in depth the full array of
performance analysis tools and techniques available on Linux for this task,
from the traditional tools like gcov and gprof, to the more advanced tools
still under development like oprofile and the Linux Trace Toolkit. The focus is
more on the underlying data collection and processing algorithms, and their
overhead and precision, than on the cosmetic details of the graphical user
interface frontends."
"Block devices in computer operating systems typically correspond to disks or
disk partitions, and are used to store files in a filesystem. Disks are not the
only real or virtual device which adhere to the block accessible stream of
bytes block device model. Files, remote devices, or even RAM may be used as a
virtual disks. This article examines several common combinations of block
device layers used as virtual disks in the Linux operating system: disk
partitions, loopback files, software RAID, Logical Volume Manager, and Network
Block Devices. It measures their relative performance using different
filesystems: Ext2, Ext3, ReiserFS, JFS, XFS,NFS."
"Compression algorithms reduce the redundancy in data representation to
decrease the storage required for that data. Data compression offers an
attractive approach to reducing communication costs by using available
bandwidth effectively. Over the last decade there has been an unprecedented
explosion in the amount of digital data transmitted via the Internet,
representing text, images, video, sound, computer programs, etc. With this
trend expected to continue, it makes sense to pursue research on developing
algorithms that can most effectively use available network bandwidth by
maximally compressing data. It is also important to consider the security
aspects of the data being transmitted while compressing it, as most of the text
data transmitted over the Internet is very much vulnerable to a multitude of
attacks. This paper is focused on addressing this problem of lossless
compression of text files with an added security."
"Constraint Programming is roughly a new software technology introduced by
Jaffar and Lassez in 1987 for description and effective solving of large,
particularly combinatorial, problems especially in areas of planning and
scheduling. In the following we define three problems for constraint solving
from the domain of electrical networks; based on them we define 43 related
problems. For the defined set of problems we benchmarked five systems: ILOG
OPL, AMPL, GAMS, Mathematica and UniCalc. As expected some of the systems
performed very well for some problems while others performed very well on
others."
"Graph-based algorithms for point-to-point link scheduling in Spatial reuse
Time Division Multiple Access (STDMA) wireless ad hoc networks often result in
a significant number of transmissions having low Signal to Interference and
Noise density Ratio (SINR) at intended receivers, leading to low throughput. To
overcome this problem, we propose a new algorithm for STDMA link scheduling
based on a graph model of the network as well as SINR computations. The
performance of our algorithm is evaluated in terms of spatial reuse and
computational complexity. Simulation results demonstrate that our algorithm
achieves better performance than existing algorithms."
"The high volume of packets and packet rates of traffic on some router links
makes it exceedingly difficult for routers to examine every packet in order to
keep detailed statistics about the traffic which is traversing the router.
Sampling is commonly applied on routers in order to limit the load incurred by
the collection of information that the router has to undertake when evaluating
flow information for monitoring purposes. The sampling process in nearly all
cases is a deterministic process of choosing 1 in every N packets on a
per-interface basis, and then forming the flow statistics based on the
collected sampled statistics. Even though this sampling may not be significant
for some statistics, such as packet rate, others can be severely distorted.
However, it is important to consider the sampling techniques and their relative
accuracy when applied to different traffic patterns. The main disadvantage of
sampling is the loss of accuracy in the collected trace when compared to the
original traffic stream. To date there has not been a detailed analysis of the
impact of sampling at a router in various traffic profiles and flow criteria.
In this paper, we assess the performance of the sampling process as used in
NetFlow in detail, and we discuss some techniques for the compensation of loss
of monitoring detail."
"Honeypots are more and more used to collect data on malicious activities on
the Internet and to better understand the strategies and techniques used by
attackers to compromise target systems. Analysis and modeling methodologies are
needed to support the characterization of attack processes based on the data
collected from the honeypots. This paper presents some empirical analyses based
on the data collected from the Leurr{\'e}.com honeypot platforms deployed on
the Internet and presents some preliminary modeling studies aimed at fulfilling
such objectives."
"For efficiency reasons, the software system designers' will is to use an
integrated set of methods and tools to describe specifications and designs, and
also to perform analyses such as dependability, schedulability and performance.
AADL (Architecture Analysis and Design Language) has proved to be efficient for
software architecture modeling. In addition, AADL was designed to accommodate
several types of analyses. This paper presents an iterative dependency-driven
approach for dependability modeling using AADL. It is illustrated on a small
example. This approach is part of a complete framework that allows the
generation of dependability analysis and evaluation models from AADL models to
support the analysis of software and system architectures, in critical
application domains."
"Real-time access to accurate and reliable timing information is necessary to
profile scientific applications, and crucial as simulations become increasingly
complex, adaptive, and large-scale. The Cactus Framework provides flexible and
extensible capabilities for timing information through a well designed
infrastructure and timing API. Applications built with Cactus automatically
gain access to built-in timers, such as gettimeofday and getrusage,
system-specific hardware clocks, and high-level interfaces such as PAPI. We
describe the Cactus timer interface, its motivation, and its implementation. We
then demonstrate how this timing information can be used by an example
scientific application to profile itself, and to dynamically adapt itself to a
changing environment at run time."
"Thanks to its simplicity and cost efficiency, wireless local area network
(WLAN) enjoys unique advantages in providing high-speed and low-cost wireless
services in hot spots and indoor environments. Traditional WLAN
medium-access-control (MAC) protocols assume that only one station can transmit
at a time: simultaneous transmissions of more than one station causes the
destruction of all packets involved. By exploiting recent advances in PHY-layer
multiuser detection (MUD) techniques, it is possible for a receiver to receive
multiple packets simultaneously. This paper argues that such multipacket
reception (MPR) capability can greatly enhance the capacity of future WLANs. In
addition, it provides the MAC-layer and PHY-layer designs needed to achieve the
improved capacity. First, to demonstrate MUD/MPR as a powerful
capacity-enhancement technique, we prove a ""super-linearity"" result, which
states that the system throughput per unit cost increases as the MPR capability
increases. Second, we show that the commonly deployed binary exponential
backoff (BEB) algorithm in today's WLAN MAC may not be optimal in an MPR
system, and that the optimal backoff factor increases with the MPR capability:
the number of packets that can be received simultaneously. Third, based on the
above insights, we design a joint MAC-PHY layer protocol for an IEEE
802.11-like WLAN that incorporates advanced PHY-layer blind detection and MUD
techniques to implement MPR"
"ENUM is a DNS-based protocol standard for mapping E.164 telephone numbers to
Internet Uniform Resource Identifiers (URIs). It places unique requirements on
the existing DNS infrastructure, such as data scalability, query throughput,
response time, and database update rates. This paper measures and evaluates the
performance of existing name server implementation as ENUM servers. We compared
PowerDNS (PDNS), BIND and Navitas. Results show that BIND is not suitable for
ENUM due to its poor scaling property. Both PDNS and Navitas can serve ENUM.
However, Navitas turns out to be highly optimized and clearly outperforms PDNS
in all aspects we have tested. We also instrumented the PDNS server to identify
its performance bottleneck and investigated ways to improve it."
"In this paper, we analyze asymptotic delay-throughput trade-offs in mobile
ad-hoc networks comprising heterogeneous nodes with restricted mobility. We
show that node spatial heterogeneity has the ability to drastically improve
upon existing scaling laws established under the assumption that nodes are
identical and uniformly visit the entire network area. In particular, we
consider the situation in which each node moves around its own home-point
according to a restricted mobility process which results into a spatial
stationary distribution that decays as a power law of exponent delta with the
distance from the home-point. For such restricted mobility model, we propose a
novel class of scheduling and routing schemes, which significantly outperforms
all delay-throughput results previously obtained in the case of identical
nodes. In particular, for delta = 2 it is possible to achieve almost constant
delay and almost constant per-node throughput (except for a poly-logarithmic
factor) as the number of nodes increases, even without resorting to
sophisticated coding or signal processing techniques."
"Most generic performance tools display only system-level performance data
using 2-dimensional plots or diagrams and this limits the informational detail
that can be displayed. Moreover, a modern relational database system, like
Oracle, can concurrently serve thousands of client processes with different
workload characteristics, so that generic performance-data displays inevitably
hide important information. Drawing on our previous work, this paper
demonstrates the application of Barry007 multidimensional visualization to the
analysis of Oracle end-user, session-level, performance data, showing both
collective trends and individual performance anomalies."
"The universal scalability law (USL) is an analytic model used to quantify
application scaling. It is universal because it subsumes Amdahl's law and
Gustafson linearized scaling as special cases. Using simulation, we show: (i)
that the USL is equivalent to synchronous queueing in a load-dependent machine
repairman model and (ii) how USL, Amdahl's law, and Gustafson scaling can be
regarded as boundaries defining three scalability zones. Typical throughput
measurements lie across all three zones. Simulation scenarios provide deeper
insight into queueing effects and thus provide a clearer indication of which
application features should be tuned to get into the optimal performance zone."
"We analyze opportunistic schemes for transmission scheduling from one of $n$
homogeneous queues whose channel states fluctuate independently. Considered
schemes consist of the LCQ policy, which transmits from a longest connected
queue in the entire system, and its low-complexity variants that transmit from
a longest queue within a randomly chosen subset of connected queues. A
Markovian model is studied where mean packet transmission time is $n^{-1}$ and
packet arrival rate is $\lambda<1$ per queue. Transient and equilibrium
distributions of queue occupancies are obtained in the limit as the system size
$n$ tends to infinity."
"This paper investigates mobility management strategies from the point of view
of their need of signalling and processing resources on the backbone network
and load on the air interface. A method is proposed to model the serving
network and mobile node mobility in order to be able to compare the different
types of mobility management algorithms. To obtain a good description of the
network we calculate descriptive parameters from given topologies. Most
mobility approaches derived from existing protocols are analyzed and their
performances are numerically compared in various network and mobility
scenarios. We developed a mobility management framework that is able to give
general designing guidelines for the next generation mobility managements on
given network, technology and mobility properties. With our model an operator
can design the network and tune the parameters to obtain the optimal
implementation of course revising existing systems is also possible. We present
a vertical handover decision method as a special application of our model
framework."
"Information-driven networks include a large category of networking systems,
where network nodes are aware of information delivered and thus can not only
forward data packets but may also perform information processing. In many
situations, the quality of service (QoS) in information-driven networks is
provisioned with the redundancy in information. Traditional performance models
generally adopt evaluation measures suitable for packet-oriented service
guarantee, such as packet delay, throughput, and packet loss rate. These
performance measures, however, do not align well with the actual need of
information-driven networks. New performance measures and models for
information-driven networks, despite their importance, have been mainly blank,
largely because information processing is clearly application dependent and
cannot be easily captured within a generic framework. To fill the vacancy, we
present a new performance evaluation framework particularly tailored for
information-driven networks, based on the recent development of stochastic
network calculus. We analyze the QoS with respect to information delivery and
study the scheduling problem with the new performance metrics. Our analytical
framework can be used to calculate the network capacity in information delivery
and in the meantime to help transmission scheduling for a large body of systems
where QoS is stochastically guaranteed with the redundancy in information."
"This paper addresses the following foundational question: what is the maximum
theoretical delay performance achievable by an overlay peer-to-peer streaming
system where the streamed content is subdivided into chunks? As shown in this
paper, when posed for chunk-based systems, and as a consequence of the
store-and-forward way in which chunks are delivered across the network, this
question has a fundamentally different answer with respect to the case of
systems where the streamed content is distributed through one or more flows
(sub-streams). To circumvent the complexity emerging when directly dealing with
delay, we express performance in term of a convenient metric, called ""stream
diffusion metric"". We show that it is directly related to the end-to-end
minimum delay achievable in a P2P streaming network. In a homogeneous scenario,
we derive a performance bound for such metric, and we show how this bound
relates to two fundamental parameters: the upload bandwidth available at each
node, and the number of neighbors a node may deliver chunks to. In this bound,
k-step Fibonacci sequences do emerge, and appear to set the fundamental laws
that characterize the optimal operation of chunk-based systems."
"Task parallelism as employed by the OpenMP task construct, although ideal for
tackling irregular problems or typical producer/consumer schemes, bears some
potential for performance bottlenecks if locality of data access is important,
which is typically the case for memory-bound code on ccNUMA systems. We present
a programming technique which ameliorates adverse effects of dynamic task
distribution by sorting tasks into locality queues, each of which is preferably
processed by threads that belong to the same locality domain. Dynamic
scheduling is fully preserved inside each domain, and is preferred over
possible load imbalance even if non-local access is required. The effectiveness
of the approach is demonstrated using a blocked six-point stencil solver as a
toy model."
"New algorithms and optimization techniques are needed to balance the
accelerating trend towards bandwidth-starved multicore chips. It is well known
that the performance of stencil codes can be improved by temporal blocking,
lessening the pressure on the memory interface. We introduce a new pipelined
approach that makes explicit use of shared caches in multicore environments and
minimizes synchronization and boundary overhead. For clusters of shared-memory
nodes we demonstrate how temporal blocking can be employed successfully in a
hybrid shared/distributed-memory environment."
"This paper is devoted to the theoretical analysis of a problem derived from
interaction between two Iplanet products: Web Proxy Server and the Directory
Server. In particular, a probabilistic and stochastic-approximation model is
proposed to minimize the occurrence of LDAP connection failures in Iplanet Web
Proxy 3.6 Server. The proposed model serves not only to provide a
parameterization of the aforementioned phenomena, but also to provide
meaningful insights illustrating and supporting these theoretical results. In
addition, we shall also address practical considerations when estimating the
parameters of the proposed model from experimental data. Finally, we shall
provide some interesting results from real-world data collected from our
customers."
"Under Windows operating system, existing I/O benchmarking tools does not
allow a developer to efficiently define a file access strategy according to the
applications' constraints. This is essentially due to the fact that the
existing tools do allow only a restricted set of I/O workloads that does not
generally correspond to the target applications. To cope with this problem, we
designed and implemented a precise I/O simulator allowing to simulate whatever
real I/O trace on a given defined architecture, and in which most of file and
disk cache strategies, their interactions and the detailed storage system
architecture are implemented. Simulation results on different workloads and
architectures show a very high degree of precision. In fact, the mean error
rate as compared to real measures is of about 6% with a maximum of 10% on
global throughput."
"To analyze complex and heterogeneous real-time embedded systems, recent works
have proposed interface techniques between real-time calculus (RTC) and timed
automata (TA), in order to take advantage of the strengths of each technique
for analyzing various components. But the time to analyze a state-based
component modeled by TA may be prohibitively high, due to the state space
explosion problem. In this paper, we propose a framework of granularity-based
interfacing to speed up the analysis of a TA modeled component. First, we
abstract fine models to work with event streams at coarse granularity. We
perform analysis of the component at multiple coarse granularities and then
based on RTC theory, we derive lower and upper bounds on arrival patterns of
the fine output streams using the causality closure algorithm. Our framework
can help to achieve tradeoffs between precision and analysis time."
"In the last decade, Expression Templates (ET) have gained a reputation as an
efficient performance optimization tool for C++ codes. This reputation builds
on several ET-based linear algebra frameworks focused on combining both elegant
and high-performance C++ code. However, on closer examination the assumption
that ETs are a performance optimization technique cannot be maintained. In this
paper we demonstrate and explain the inability of current ET-based frameworks
to deliver high performance for dense and sparse linear algebra operations, and
introduce a new ""smart"" ET implementation that truly allows the combination of
high performance code with the elegance and maintainability of a
domain-specific language."
"Volume reconstruction by backprojection is the computational bottleneck in
many interventional clinical computed tomography (CT) applications. Today
vendors in this field replace special purpose hardware accelerators by standard
hardware like multicore chips and GPGPUs. Medical imaging algorithms are on the
verge of employing High Performance Computing (HPC) technology, and are
therefore an interesting new candidate for optimization. This paper presents
low-level optimizations for the backprojection algorithm, guided by a thorough
performance analysis on four generations of Intel multicore processors
(Harpertown, Westmere, Westmere EX, and Sandy Bridge).
  We choose the RabbitCT benchmark, a standardized testcase well supported in
industry, to ensure transparent and comparable results. Our aim is to provide
not only the fastest possible implementation but also compare to performance
models and hardware counter data in order to fully understand the results. We
separate the influence of algorithmic optimizations, parallelization, SIMD
vectorization, and microarchitectural issues and pinpoint problems with current
SIMD instruction set extensions on standard CPUs (SSE, AVX). The use of
assembly language is mandatory for best performance. Finally we compare our
results to the best GPGPU implementations available for this open competition
benchmark."
"We consider the performance modeling and evaluation of network systems
powered with renewable energy sources such as solar and wind energy. Such
energy sources largely depend on environmental conditions, which are hard to
predict accurately. As such, it may only make sense to require the network
systems to support a soft quality of service (QoS) guarantee, i.e., to
guarantee a service requirement with a certain high probability. In this paper,
we intend to build a solid mathematical foundation to help better understand
the stochastic energy constraint and the inherent correlation between QoS and
the uncertain energy supply. We utilize a calculus approach to model the
cumulative amount of charged energy and the cumulative amount of consumed
energy. We derive upper and lower bounds on the remaining energy level based on
a stochastic energy charging rate and a stochastic energy discharging rate. By
building the bridge between energy consumption and task execution (i.e.,
service), we study the QoS guarantee under the constraint of uncertain energy
sources. We further show how performance bounds can be improved if some strong
assumptions can be made."
"Peer-to-peer (P2P) computing is currently attracting enormous attention. In
P2P systems a very large number of autonomous computing nodes (the peers) pool
together their resources and rely on each other for data and services.
Peer-to-peer (P2P) Data-sharing systems now generate a significant portion of
Internet traffic. Examples include P2P systems for network storage, web
caching, searching and indexing of relevant documents and distributed
network-threat analysis. Requirements for widely distributed information
systems supporting virtual organizations have given rise to a new category of
P2P systems called schema-based. In such systems each peer exposes its own
schema and the main objective is the efficient search across the P2P network by
processing each incoming query without overly consuming bandwidth. The
usability of these systems depends on effective techniques to find and retrieve
data; however, efficient and effective routing of content-based queries is a
challenging problem in P2P networks. This work was attended as an attempt to
motivate the use of mining algorithms and hypergraphs context to develop two
different methods that improve significantly the efficiency of P2P
communications. The proposed query routing methods direct the query to a set of
relevant peers in such way as to avoid network traffic and bandwidth
consumption. We compare the performance of the two proposed methods with the
baseline one and our experimental results prove that our proposed methods
generate impressive levels of performance and scalability."
"This paper focuses on the stationary portion of file download in an
unstructured peer-to-peer network, which typically follows for many hours after
a flash crowd initiation. The model includes the case that peers can have some
pieces at the time of arrival. The contribution of the paper is to identify how
much help is needed from the seeds, either fixed seeds or peer seeds (which are
peers remaining in the system after obtaining a complete collection) to
stabilize the system. The dominant cause for instability is the missing piece
syndrome, whereby one piece becomes very rare in the network. It is shown that
stability can be achieved with only a small amount of help from peer
seeds--even with very little help from a fixed seed, peers need dwell as peer
seeds on average only long enough to upload one additional piece. The region of
stability is insensitive to the piece selection policy. Network coding can
substantially increase the region of stability in case a portion of the new
peers arrive with randomly coded pieces."
"Much of the current focus in high performance computing (HPC) for
computational fluid dynamics (CFD) deals with grid based methods. However,
parallel implementations for new meshfree particle methods such as Smoothed
Particle Hydrodynamics (SPH) are less studied. In this work, we present
optimizations for both central processing unit (CPU) and graphics processing
unit (GPU) of a SPH method. These optimization strategies can be further
applied to many other meshfree methods. The obtained performance for each
architecture and a comparison between the most efficient implementations for
CPU and GPU are shown."
"Reengineering multi tiered enterprise business applications for performance
enhancement and reciprocal or rectangular hyperbolic relation of variation of
data transportation time with row pre-fetch size of relational database drivers"
"Cloud computing is emerging as an important platform for business, personal
and mobile computing applications. In this paper, we study a stochastic model
of cloud computing, where jobs arrive according to a stochastic process and
request resources like CPU, memory and storage space. We consider a model where
the resource allocation problem can be separated into a routing or load
balancing problem and a scheduling problem. We study the
join-the-shortest-queue routing and power-of-two-choices routing algorithms
with MaxWeight scheduling algorithm. It was known that these algorithms are
throughput optimal. In this paper, we show that these algorithms are queue
length optimal in the heavy traffic limit."
"Many tools and libraries employ hardware performance monitoring (HPM) on
modern processors, and using this data for performance assessment and as a
starting point for code optimizations is very popular. However, such data is
only useful if it is interpreted with care, and if the right metrics are chosen
for the right purpose. We demonstrate the sensible use of hardware performance
counters in the context of a structured performance engineering approach for
applications in computational science. Typical performance patterns and their
respective metric signatures are defined, and some of them are illustrated
using case studies. Although these generic concepts do not depend on specific
tools or environments, we restrict ourselves to modern x86-based multicore
processors and use the likwid-perfctr tool under the Linux OS."
"We study delay tolerant networking (DTN) and in particular, its capacity to
store, carry and forward messages so that the messages eventually reach their
final destinations. We approach this broad question in the framework of
percolation theory. To this end, we assume an elementary mobility model, where
nodes arrive to an infinite plane according to a Poisson point process, move a
certain distance L, and then depart. In this setting, we characterize the mean
density of nodes required to support DTN style networking. In particular, under
the given assumptions, we show that DTN is feasible when the mean node degree
is greater than 4 e(g), where parameter g=L/d is the ratio of the distance L to
the transmission range d, and e(g) is the critical reduced number density of
tilted cylinders in a directed continuum percolation model. By means of Monte
Carlo simulations, we give numerical values for e(g). The asymptotic behavior
of e(g) when g tends to infinity is also derived from a fluid flow analysis."
"Modern multicore chips show complex behavior with respect to performance and
power. Starting with the Intel Sandy Bridge processor, it has become possible
to directly measure the power dissipation of a CPU chip and correlate this data
with the performance properties of the running code. Going beyond a simple
bottleneck analysis, we employ the recently published Execution-Cache-Memory
(ECM) model to describe the single- and multi-core performance of streaming
kernels. The model refines the well-known roofline model, since it can predict
the scaling and the saturation behavior of bandwidth-limited loop kernels on a
multicore chip. The saturation point is especially relevant for considerations
of energy consumption. From power dissipation measurements of benchmark
programs with vastly different requirements to the hardware, we derive a
simple, phenomenological power model for the Sandy Bridge processor. Together
with the ECM model, we are able to explain many peculiarities in the
performance and power behavior of multicore processors, and derive guidelines
for energy-efficient execution of parallel programs. Finally, we show that the
ECM and power models can be successfully used to describe the scaling and power
behavior of a lattice-Boltzmann flow solver code."
"In this paper, we propose and analyze a spectrum sensing method based on
cyclostationarity specifically targeted for receivers with multiple antennas.
This detection method is used for determining the presence or absence of
primary users in cognitive radio networks based on the eigenvalues of the
cyclic covariance matrix of received signals. In particular, the cyclic
correlation significance test is used to detect a specific signal-of-interest
by exploiting knowledge of its cyclic frequencies. Analytical expressions for
the probability of detection and probability of false-alarm under both
spatially uncorrelated or spatially correlated noise are derived and verified
by simulation. The detection performance in a Rayleigh flat-fading environment
is found and verified through simulations. One of the advantages of the
proposed method is that the detection threshold is shown to be independent of
both the number of samples and the noise covariance, effectively eliminating
the dependence on accurate noise estimation. The proposed method is also shown
to provide higher detection probability and better robustness to noise
uncertainty than existing multiple-antenna cyclostationary-based spectrum
sensing algorithms under both AWGN as well as a quasi-static Rayleigh fading
channel."
"We formulate the loop-free, binary superoptimization task as a stochastic
search problem. The competing constraints of transformation correctness and
performance improvement are encoded as terms in a cost function, and a Markov
Chain Monte Carlo sampler is used to rapidly explore the space of all possible
programs to find one that is an optimization of a given target program.
Although our method sacrifices com- pleteness, the scope of programs we are
able to reason about, and the quality of the programs we produce, far exceed
those of existing superoptimizers. Beginning from binaries com- piled by llvm
-O0 for 64-bit X86, our prototype implemen- tation, STOKE, is able to produce
programs which either match or outperform the code sequences produced by gcc
with full optimizations enabled, and, in some cases, expert handwritten
assembly."
"Intel Array Building Blocks is a high-level data-parallel programming
environment designed to produce scalable and portable results on existing and
upcoming multi- and many-core platforms.
  We have chosen several mathematical kernels - a dense matrix-matrix
multiplication, a sparse matrix-vector multiplication, a 1-D complex FFT and a
conjugate gradients solver - as synthetic benchmarks and representatives of
scientific codes and ported them to ArBB. This whitepaper describes the ArBB
ports and presents performance and scaling measurements on the Westmere-EX
based system SuperMIG at LRZ in comparison with OpenMP and MKL."
"The IT industry needs systems management models that leverage available
application information to detect quality of service, scalability and health of
service. Ideally this technique would be common for varying application types
with different n-tier architectures under normal production conditions of
varying load, user session traffic, transaction type, transaction mix, and
hosting environment.
  This paper shows that a whole of service measurement paradigm utilizing a
black box M/M/1 queuing model and auto regression curve fitting of the
associated CDF are an accurate model to characterize system performance
signatures. This modeling method is also used to detect application slow down
events. The technique was shown to work for a diverse range of workloads
ranging from 76 Tx/ 5min to 19,025 Tx/ 5min. The method did not rely on
customizations specific to the n-tier architecture of the systems being
analyzed and so the performance anomaly detection technique was shown to be
platform and configuration agnostic."
"In asymptotic regimes, both in time and space (network size), the derivation
of network capacity results is grossly simplified by brushing aside queueing
behavior in non-Jackson networks. This simplifying double-limit model, however,
lends itself to conservative numerical results in finite regimes. To properly
account for queueing behavior beyond a simple calculus based on average rates,
we advocate a system theoretic methodology for the capacity problem in finite
time and space regimes. This methodology also accounts for spatial correlations
arising in networks with CSMA/CA scheduling and it delivers rigorous
closed-form capacity results in terms of probability distributions. Unlike
numerous existing asymptotic results, subject to anecdotal practical concerns,
our transient one can be used in practical settings: for example, to compute
the time scales at which multi-hop routing is more advantageous than single-hop
routing."
"Distributed Software Systems are used these days by many people in the real
time operations and modern enterprise applications. One of the most important
and essential attributes of measurements for the quality of service of
distributed software is performance. Performance models can be employed at
early stages of the software development cycle to characterize the quantitative
behavior of software systems. In this research, performance models based on
fuzzy logic approach, queuing network approach and Petri net approach have been
reviewed briefly. One of the most common ways in performance analysis of
distributed software systems is translating the UML diagrams to mathematical
modeling languages for the description of distributed systems such as queuing
networks or Petri nets. In this paper, some of these approaches are reviewed
briefly. Attributes which are used for performance modeling in the literature
are mostly machine based. On the other hand, end users and client parameters
for performance evaluation are not covered extensively. In this way, future
research could be based on developing hybrid models to capture user decision
variables which make system performance evaluation more user driven."
"We study the conditional sojourn time distributions of processor sharing
(PS), foreground background processor sharing (FBPS) and shortest remaining
processing time first (SRPT) scheduling disciplines on an event where the job
size of a customer arriving in stationarity is smaller than exactly k>=0 out of
the preceding m>=k arrivals. Then, conditioning on the preceding event, the
sojourn time distribution of this newly arriving customer behaves
asymptotically the same as if the customer were served in isolation with a
server of rate (1-\rho)/(k+1) for PS/FBPS, and (1-\rho) for SRPT, respectively,
where \rho is the traffic intensity. Hence, the introduced notion of
conditional limits allows us to distinguish the asymptotic performance of the
studied schedulers by showing that SRPT exhibits considerably better asymptotic
behavior for relatively smaller jobs than PS/FBPS.
  Inspired by the preceding results, we propose an approximation to the SRPT
discipline based on a novel adaptive job grouping mechanism that uses relative
size comparison of a newly arriving job to the preceding m arrivals.
Specifically, if the newly arriving job is smaller than k and larger than m-k
of the previous m jobs, it is routed into class k. Then, the classes of smaller
jobs are served with higher priorities using the static priority scheduling.
The good performance of this mechanism, even for a small number of classes m+1,
is demonstrated using the asymptotic queueing analysis under the heavy-tailed
job requirements. We also discuss refinements of the comparison grouping
mechanism that improve the accuracy of job classification at the expense of a
small additional complexity."
"The use of advanced sequential paging algorithms has been suggested as a
means to reduce the signaling cost in future mobile cellular networks. In a
proposed algorithm (Koukoutsidis and Theologou, 2003), the system can use the
additional information of the last interaction cell combined with a mobility
model to predict the short-term location probabilities at the time of an
incoming call arrival. The short-term location probabilities reduce the
uncertainty in mobile user position and thus greatly improve the search. In
this paper, an analytical model is derived that allows for a general
distribution of cell residence times. By considering a Gamma distribution, we
study the effect of the variance of cell residence times and derive useful
results on the performance of the algorithm."
"We present a performance model for bandwidth limited loop kernels which is
founded on the analysis of modern cache based microarchitectures. This model
allows an accurate performance prediction and evaluation for existing
instruction codes. It provides an in-depth understanding of how performance for
different memory hierarchy levels is made up. The performance of raw memory
load, store and copy operations and a stream vector triad are analyzed and
benchmarked on three modern x86-type quad-core architectures in order to
demonstrate the capabilities of the model."
"This paper proposes a new scheduler applying the concept of non-uniform
laxity to Earliest deadline first (EDF) approach for aperiodic tasks. This
scheduler improves task utilisation (Execution time / deadline) and also
increases the number of tasks that are being scheduled. Laxity is a measure of
the spare time permitted for the task before it misses its deadline, and is
computed using the expression (deadline - (current time + execution time)).
Weight decides the priority of the task and is defined by the expression
(quantum slice time / allocated time)*total core time for the task. Quantum
slice time is the time actually used, allocated time is the time allocated by
the scheduler, and total core time is the time actually reserved by the core
for execution of one quantum of the task. Non-uniform laxity enables scheduling
of tasks that have higher priority before the normal execution of other tasks
and is computed by multiplying the weight of the task with its laxity. The
algorithm presented in the paper has been simulated on Cheddar, a real time
scheduling tool and also on SESC, an architectural simulator for multicore
platforms, for upto 5000 random task sets, and upto 5000 cores. This scheduler
improves task utilisation by 35% and the number of tasks being scheduled by
36%, compared to conventional EDF."
"The Intel Core i7 processor code named Nehalem provides a feature named Turbo
Boost which opportunistically varies the frequencies of the processor's cores.
The frequency of a core is determined by core temperature, the number of active
cores, the estimated power consumption, the estimated current consumption, and
operating system frequency scaling requests. For a chip multi-processor(CMP)
that has a small number of physical cores and a small set of performance
states, deciding the Turbo Boost frequency to use on a given core might not be
difficult. However, we do not know the complexity of this decision making
process in the context of a large number of cores, scaling to the 100s, as
predicted by researchers in the field."
"A server farm is examined, where a number of servers are used to offer a
service to impatient customers. Every completed request generates a certain
amount of profit, running servers consume electricity for power and cooling,
while waiting customers might leave the system before receiving service if they
experience excessive delays. A dynamic allocation policy aiming at satisfying
the conflicting goals of maximizing the quality of users' experience while
minimizing the cost for the provider is introduced and evaluated. The results
of several experiments are described, showing that the proposed scheme performs
well under different traffic conditions."
"A service provisioning system is examined, where a number of servers are used
to offer different types of services to paying customers. A customer is charged
for the execution of a stream of jobs; the number of jobs in the stream and the
rate of their submission is specified. On the other hand, the provider promises
a certain quality of service (QoS), measured by the average waiting time of the
jobs in the stream. A penalty is paid if the agreed QoS requirement is not met.
The objective is to maximize the total average revenue per unit time. Dynamic
policies for making server allocation and stream admission decisions are
introduced and evaluated. The results of several simulations are described."
"In VANET high speed is the real characteristics which leads frequent
breakdown, interference etc. In this paper we studied various Ad hoc routing
protocols, Reactive, Proactive & Hybrid, taking into consideration various
VANET parameters like speed, altitude etc in real traffic scenario and
evaluated them for various battery models for energy conservation.. The AODV
and DYMO (Reactive), OLSR (Proactive) and ZRP (hybrid) protocols are compared
for battery models Duracell AA(MX- 1500),Duracell AAA(MN-2400),Duracell
AAA(MX-2400), Duracell C-MN(MN-1400),Panasonic AA standard using Qualnet as a
Simulation tool. Since Energy conservation is main focus area nowadays. Hence
performance of the protocols with various battery models counts and helps to
make a right selection. Varying parameters of VANET shows that in the real
traffic scenarios proactive protocol performs more efficiently for energy
conservation."
"In all measurement campaigns, one needs to assert that the instrumentation
tools do not significantly impact the system being monitored. This is critical
to future claims based on the collected data and is sometimes overseen in
experimental studies. We propose a method to evaluate the potential ""observer
effect"" of an instrumentation system, and apply it to the OMF Measurement
Library (OML). OML allows the instrumentation of almost any software to collect
any type of measurements. As it is increasingly being used in networking
research, it is important to characterise possible biases it may introduce in
the collected metrics. Thus, we study its effect on multiple types of reports
from various applications commonly used in wireless research. To this end, we
designed experiments comparing OML-instrumented software with their original
flavours. Our analyses of the results from these experiments show that, with an
appropriate reporting setup, OML has no significant impact on the instrumented
applications, and may even improve some of their performances in specifics
cases. We discuss our methodology and the implication of using OML, and provide
guidelines on instrumenting off-the-shelf software."
"It is well known, mainly because of the work of Kurtz, that density dependent
Markov chains can be approximated by sets of ordinary differential equations
(ODEs) when their indexing parameter grows very large. This approximation
cannot capture the stochastic nature of the process and, consequently, it can
provide an erroneous view of the behavior of the Markov chain if the indexing
parameter is not sufficiently high. Important phenomena that cannot be revealed
include non-negligible variance and bi-modal population distributions. A
less-known approximation proposed by Kurtz applies stochastic differential
equations (SDEs) and provides information about the stochastic nature of the
process. In this paper we apply and extend this diffusion approximation to
study stochastic Petri nets. We identify a class of nets whose underlying
stochastic process is a density dependent Markov chain whose indexing parameter
is a multiplicative constant which identifies the population level expressed by
the initial marking and we provide means to automatically construct the
associated set of SDEs. Since the diffusion approximation of Kurtz considers
the process only up to the time when it first exits an open interval, we extend
the approximation by a machinery that mimics the behavior of the Markov chain
at the boundary and allows thus to apply the approach to a wider set of
problems. The resulting process is of the jump-diffusion type. We illustrate by
examples that the jump-diffusion approximation which extends to bounded domains
can be much more informative than that based on ODEs as it can provide accurate
quantity distributions even when they are multi-modal and even for relatively
small population levels. Moreover, we show that the method is faster than
simulating the original Markov chain."
"Queues with setup time are extensively studied because they have application
in performance evaluation of power-saving data centers. In a data center, there
are a huge number of servers which consume a large amount of energy. In the
current technology, an idle server still consumes about 60\% of its peak
processing a job. Thus, the only way to save energy is to turn off servers
which are not processing a job. However, when there are some waiting jobs, we
have to turn on the OFF servers. A server needs some setup time to be active
during which it consumes energy but cannot process a job. Therefore, there
exists a trade-off between power consumption and delay performance. Gandhi et
al. \cite{Gandhi10a,Gandhi10} analyze this trade-off using an M/M/$c$ queue
with staggered setup (one server in setup at a time). In this paper, using an
alternative approach, we obtain generating functions for the joint stationary
distribution of the number of active servers and that of jobs in the system for
a more general model with batch arrivals and state-dependent setup time. We
further obtain moments for the queue size. Numerical results reveal that
keeping the same traffic intensity, the mean power consumption decreases with
the mean batch size for the case of fixed batch size. One of the main
theoretical contribution is a new conditional decomposition formula showing
that the number of waiting customers under the condition that all servers are
busy can be decomposed to the sum of two independent random variables where the
first is the same quantity in the corresponding model without setup time while
the second is the number of waiting customers before an arbitrary customer."
"Power consumption in data centers has been growing significantly in recent
years. To reduce power, servers are being equipped with increasingly
sophisticated power management mechanisms. Different mechanisms offer
dramatically different trade-offs between power savings and performance
penalties. Considering the complexity, variety, and temporally varying nature
of the applications hosted in a typical data center, intelligently determining
which power management policy to use and when is a complicated task.
  In this paper we analyze a system model featuring both performance scaling
and low-power states. We reveal the interplay between performance scaling and
low-power states via intensive simulation and analytic verification. Based on
the observations, we present SleepScale, a runtime power management tool
designed to efficiently exploit existing power control mechanisms. At run time,
SleepScale characterizes power consumption and quality-of-service (QoS) for
each low-power state and frequency setting, and selects the best policy for a
given QoS constraint. We evaluate SleepScale using workload traces from data
centers and achieve significant power savings relative to conventional power
management strategies."
"In this note, we present preliminary results on the use of ""network calculus""
for parallel processing systems, specifically MapReduce."
"This paper focuses on analytical studies of the primary user (PU) traffic
classification problem. Observing that the gamma distribution can represent
positively skewed data and exponential distribution (popular in communication
networks performance analysis literature) it is considered here as the PU
traffic descriptor. We investigate two PU traffic classifiers utilizing
perfectly measured PU activity (busy) and inactivity (idle) periods: (i)
maximum likelihood classifier (MLC) and (ii) multi-hypothesis sequential
probability ratio test classifier (MSPRTC). Then, relaxing the assumption on
perfect period measurement, we consider a PU traffic observation through
channel sampling. For a special case of negligible probability of PU state
change in between two samplings, we propose a minimum variance PU busy/idle
period length estimator. Later, relaxing the assumption of the complete
knowledge of the parameters of the PU period length distribution, we propose
two PU traffic classification schemes: (i) estimate-then-classify (ETC), and
(ii) average likelihood function (ALF) classifiers considering time domain
fluctuation of the PU traffic parameters. Numerical results show that both MLC
and MSPRTC are sensitive to the periods measurement errors when the distance
among distribution hypotheses is small, and to the distribution parameter
estimation errors when the distance among hypotheses is large. For PU traffic
parameters with a partial prior knowledge of the distribution, the ETC
outperforms ALF when the distance among hypotheses is small, while the opposite
holds when the distance is large."
"Retransmissions represent a primary failure recovery mechanism on all layers
of communication network architecture. Similarly, fair sharing, e.g. processor
sharing (PS), is a widely accepted approach to resource allocation among
multiple users. Recent work has shown that retransmissions in failure-prone,
e.g. wireless ad hoc, networks can cause heavy tails and long delays. In this
paper, we discover a new phenomenon showing that PS-based scheduling induces
complete instability with zero throughput in the presence of retransmissions,
regardless of how low the traffic load may be. This phenomenon occurs even when
the job sizes are bounded/fragmented, e.g. deterministic. Our analytical
results are further validated via simulation experiments. Moreover, our work
demonstrates that scheduling one job at a time, such as first-come-first-serve,
achieves stability and should be preferred in these systems."
"This paper presents a queueing network approach to the analysis and control
of mobility-on-demand (MoD) systems for urban personal transportation. A MoD
system consists of a fleet of vehicles providing one-way car sharing service
and a team of drivers to rebalance such vehicles. The drivers then rebalance
themselves by driving select customers similar to a taxi service. We model the
MoD system as two coupled closed Jackson networks with passenger loss. We show
that the system can be approximately balanced by solving two decoupled linear
programs and exactly balanced through nonlinear optimization. The rebalancing
techniques are applied to a system sizing example using taxi data in three
neighborhoods of Manhattan, which suggests that the optimal vehicle-to-driver
ratio in a MoD system is between 3 and 5. Lastly, we formulate a real-time
closed-loop rebalancing policy for drivers and demonstrate its stability (in
terms of customer wait times) for typical system loads."
"Data compression has been widely applied in many data processing areas.
Compression methods use variable-size codes with the shorter codes assigned to
symbols or groups of symbols that appear in the data frequently. Fibonacci
coding, as a representative of these codes, is used for compressing small
numbers. Time consumption of a decompression algorithm is not usually as
important as the time of a compression algorithm. However, efficiency of the
decompression may be a critical issue in some cases. For example, a real-time
compression of tree data structures follows this issue. Tree's pages are
decompressed during every reading from a secondary storage into the main
memory. In this case, the efficiency of a decompression algorithm is extremely
important. We have developed a Fast Fibonacci decompression for this purpose.
Our approach is up to $3.5\times$ faster than the original implementation."
"The universal scalability law of computational capacity is a rational
function C_p = P(p)/Q(p) with P(p) a linear polynomial and Q(p) a second-degree
polynomial in the number of physical processors p, that has been long used for
statistical modeling and prediction of computer system performance. We prove
that C_p is equivalent to the synchronous throughput bound for a
machine-repairman with state-dependent service rate. Simpler rational
functions, such as Amdahl's law and Gustafson speedup, are corollaries of this
queue-theoretic bound. C_p is further shown to be both necessary and sufficient
for modeling all practical characteristics of computational scalability."
"We propose a concise approximate description, and a method for efficiently
obtaining this description, via adaptive random sampling of the performance
(running time, memory consumption, or any other profileable numerical quantity)
of a given algorithm on some low-dimensional rectangular grid of inputs. The
formal correctness is proven under reasonable assumptions on the algorithm
under consideration; and the approach's practical benefit is demonstrated by
predicting for which observer positions and viewing directions an occlusion
culling algorithm yields a net performance benefit or loss compared to a simple
brute force renderer."
"It is well known that the static caching algorithm that keeps the most
frequently requested documents in the cache is optimal in case when documents
are of the same size and requests are independent and equally distributed.
However, it is hard to develop explicit and provably optimal caching algorithms
when requests are statistically correlated. In this paper, we show that keeping
the most frequently requested documents in the cache is still optimal for large
cache sizes even if the requests are strongly correlated."
"The balance metric is a simple approach to estimate the performance of
bandwidth-limited loop kernels. However, applying the method to in-cache
situations and modern multi-core architectures yields unsatisfactory results.
This paper analyzes the in uence of cache hierarchy design on performance
predictions for bandwidth-limited loop kernels on current mainstream
processors. We present a diagnostic model with improved predictive power,
correcting the limitations of the simple balance metric. The importance of code
execution overhead even in bandwidth-bound situations is emphasized. Finally we
analyze the impact of synchronization overhead on multi-threaded performance
with a special emphasis on the in uence of cache topology."
"Recently, hybrid architectures using accelerators like GPGPUs or the Cell
processor have gained much interest in the HPC community. The RapidMind
Multi-Core Development Platform is a programming environment that allows
generating code which is able to seamlessly run on hardware accelerators like
GPUs or the Cell processor and multicore CPUs both from AMD and Intel. This
paper describes the ports of three mathematical kernels to RapidMind which are
chosen as synthetic benchmarks and representatives of scientific codes.
Performance of these kernels has been measured on various RapidMind backends
(cuda, cell and x86) and compared to other hardware-specific implementations
(using CUDA, Cell SDK and Intel MKL). The results give an insight in the degree
of portability of RapidMind code and code performance across different
architectures."
"Stochastic network calculus requires special care in the search of proper
stochastic traffic arrival models and stochastic service models. Tradeoff must
be considered between the feasibility for the analysis of performance bounds,
the usefulness of performance bounds, and the ease of their numerical
calculation. In theory, transform between different traffic arrival models and
transform between different service models are possible. Nevertheless, the
impact of the model transform on performance bounds has not been thoroughly
investigated. This paper is to investigate the effect of the model transform
and to provide practical guidance in the model selection in stochastic network
calculus."
"Stencil computations consume a major part of runtime in many scientific
simulation codes. As prototypes for this class of algorithms we consider the
iterative Jacobi and Gauss-Seidel smoothers and aim at highly efficient
parallel implementations for cache-based multicore architectures. Temporal
cache blocking is a known advanced optimization technique, which can reduce the
pressure on the memory bus significantly. We apply and refine this optimization
for a recently presented temporal blocking strategy designed to explicitly
utilize multicore characteristics. Especially for the case of Gauss-Seidel
smoothers we show that simultaneous multi-threading (SMT) can yield substantial
performance improvements for our optimized algorithm."
"Due to the huge difference in performance between the computer memory and
processor, the virtual memory management plays a vital role in system
performance. A Cache memory is the fast memory which is used to compensate the
speed difference between the memory and processor. This paper gives an adaptive
replacement policy over the traditional policy which has low overhead, better
performance and is easy to implement. Simulations show that our algorithm
performs better than Least-Recently-Used (LRU), First-In-First-Out (FIFO) and
Clock with Adaptive Replacement (CAR)."
"The performance of the emerging petaflops-scale supercomputers of the nearest
future (hypercomputers) will be governed not only by the clock frequency of the
processing nodes or by the width of the system bus, but also by such factors as
the overall power consumption and the geometric size. In this paper, we study
the influence of such parameters on one of the most important characteristics
of a general purpose computer - on the degree of multithreading that must be
present in an application to make the use of the hypercomputer justifiable. Our
major finding is that for the class of applications with purely random memory
access patterns ""super-fast computing"" and ""high-performance computing"" are
essentially synonyms for ""massively-parallel computing."""
"Powerful spectrum decision schemes enable cognitive radios (CRs) to find
transmission opportunities in spectral resources allocated exclusively to the
primary users. One of the key effecting factor on the CR network throughput is
the spectrum sensing sequence used by each secondary user. In this paper,
secondary users' throughput maximization through finding an appropriate sensing
matrix (SM) is investigated. To this end, first the average throughput of the
CR network is evaluated for a given SM. Then, an optimization problem based on
the maximization of the network throughput is formulated in order to find the
optimal SM. As the optimum solution is very complicated, to avoid its major
challenges, three novel sub optimum solutions for finding an appropriate SM are
proposed for various cases including perfect and non-perfect sensing. Despite
of having less computational complexities as well as lower consumed energies,
the proposed solutions perform quite well compared to the optimum solution (the
optimum SM). The structure and performance of the proposed SM setting schemes
are discussed in detail and a set of illustrative simulation results is
presented to validate their efficiencies."
"In this paper we address issues of reliability of RAID systems. We focus on
""big data"" systems with a large number of drives and advanced error correction
schemes beyond \RAID{6}. Our RAID paradigm is based on Reed-Solomon codes, and
thus we assume that the RAID consists of $N$ data drives and $M$ check drives.
The RAID fails only if the combined number of failed drives and sector errors
exceeds $M$, a property of Reed-Solomon codes.
  We review a number of models considered in the literature and build upon them
to construct models usable for a large number of data and check drives. We
attempt to account for a significant number of factors that affect RAID
reliability, such as drive replacement or lack thereof, mistakes during service
such as replacing the wrong drive, delayed repair, and the finite duration of
RAID reconstruction. We evaluate the impact of sector failures that do not
result in drive replacement.
  The reader who needs to consider large $M$ and $N$ will find applicable
mathematical techniques concisely summarized here, and should be able to apply
them to similar problems. Most methods are based on the theory of continuous
time Markov chains, but we move beyond this framework when we consider the
fixed time to rebuild broken hard drives, which we model using systems of delay
and partial differential equations.
  One universal statement is applicable across various models: increasing the
number of check drives in all cases increases the reliability of the system,
and is vastly superior to other approaches of ensuring reliability such as
mirroring."
"The overall performance of content distribution networks as well as recently
proposed information-centric networks rely on both memory and bandwidth
capacities. In this framework, the hit ratio is the key performance indicator
which captures the bandwidth / memory tradeoff for a given global
performance.This paper focuses on the estimation of the hit ratio in a network
of caches that employ the Random replacement policy. Assuming that requests are
independent and identically distributed, general expressions of miss
probabilities for a single Random cache are provided as well as exact results
for specific popularity distributions. Moreover, for any Zipf popularity
distribution with exponent $\alpha$ > 1, we obtain asymptotic equivalents for
the miss probability in the case of large cache size. We extend the analysis to
networks of Random caches, when the topology is either a line or a homogeneous
tree. In that case, approximations for miss probabilities across the network
are derived by assuming that miss events at any node occur independently in
time; the obtained results are compared to the same network using the
Least-Recently-Used discipline, already addressed in the literature. We further
analyze the case of a mixed tandem cache network where the two nodes employ
either Random or Least-Recently-Used policies. In all scenarios, asymptotic
formulas and approximations are extensively compared to simulations and shown
to perform very well. Finally, our results enable us to propose recommendations
for cache replacement disciplines in a network dedicated to content
distribution. These results also hold for a cache using the First-In-First-Out
policy."
"Conventional Web archives are created by periodically crawling a web site and
archiving the responses from the Web server. Although easy to implement and
common deployed, this form of archiving typically misses updates and may not be
suitable for all preservation scenarios, for example a site that is required
(perhaps for records compliance) to keep a copy of all pages it has served. In
contrast, transactional archives work in conjunction with a Web server to
record all pages that have been served. Los Alamos National Laboratory has
developed SiteSory, an open-source transactional archive written in Java
solution that runs on Apache Web servers, provides a Memento compatible access
interface, and WARC file export features. We used the ApacheBench utility on a
pre-release version of to measure response time and content delivery time in
different environments and on different machines. The performance tests were
designed to determine the feasibility of SiteStory as a production-level
solution for high fidelity automatic Web archiving. We found that SiteStory
does not significantly affect content server performance when it is performing
transactional archiving. Content server performance slows from 0.076 seconds to
0.086 seconds per Web page access when the content server is under load, and
from 0.15 seconds to 0.21 seconds when the resource has many embedded and
changing resources."
"In this paper we present a generic framework for the asymptotic performance
analysis of subspace-based parameter estimation schemes. It is based on earlier
results on an explicit first-order expansion of the estimation error in the
signal subspace obtained via an SVD of the noisy observation matrix. We extend
these results in a number of aspects. Firstly, we derive an explicit
first-order expansion of the Higher- Order SVD (HOSVD)-based subspace estimate.
Secondly, we show how to obtain explicit first-order expansions of the
estimation error of ESPRIT-type algorithms and provide the expressions for
matrix-based and tensor-based Standard ESPRIT and Unitary ESPRIT. Thirdly, we
derive closed-form expressions for the mean square error (MSE) and show that
they only depend on the second-order moments of the noise. Hence, we only need
the noise to be zero mean and possess finite second order moments. Fourthly, we
investigate the effect of using Structured Least Squares (SLS) to solve the
overdetermined shift invariance equations in ESPRIT and provide an explicit
first-order expansion as well as a closed-form MSE expression. Finally, we
simplify the MSE for the special case of a single source and compute the
asymptotic efficiency of the investigated ESPRIT-type algorithms in compact
closed-form expressions which only depend on the array size and the effective
SNR. Our results are more general than existing results on the performance
analysis of ESPRIT-type algorithms since (a) we do not need any assumptions
about the noise except for the mean to be zero and the second-order moments to
be finite (in contrast to earlier results that require Gaussianity or
second-order circular symmetry); (b) our results are asymptotic in the
effective SNR, i.e., we do not require the number of samples to be large; (c)
we present a framework that incorporates various ESPRIT-type algorithms in one
unified manner."
"Retransmission-based failure recovery represents a primary approach in
existing communication networks that guarantees data delivery in the presence
of channel failures. Recent work has shown that, when data sizes have infinite
support, retransmissions can cause long (-tailed) delays even if all traffic
and network characteristics are light-tailed. In this paper we investigate the
practically important case of bounded data units 0 <= L_b <= b under the
condition that the hazard functions of the distributions of data sizes and
channel statistics are proportional. To this end, we provide an explicit and
uniform characterization of the entire body of the retransmission distribution
Pr[N_b > n] in both n and b. Our main discovery is that this distribution can
be represented as the product of a power law and Gamma distribution. This
rigorous approximation clearly demonstrates the coupling of a power law
distribution, dominating the main body, and the Gamma distribution, determining
the exponential tail. Our results are validated via simulation experiments and
can be useful for designing retransmission-based systems with the required
performance characteristics. From a broader perspective, this study applies to
any other system, e.g., computing, where restart mechanisms are employed after
a job processing failure."
"In this paper we present product-form solutions from the point of view of
stochastic process algebra. In previous work we have shown how to derive
product-form solutions for a formalism called Labelled Markov Automata (LMA).
LMA are very useful as their relation with the Continuous Time Markov Chains is
very direct. The disadvantage of using LMA is that the proofs of properties are
cumbersome. In fact, in LMA it is not possible to use the inductive structure
of the language in a proof. In this paper we consider a simple stochastic
process algebra that has the great advantage of simplifying the proofs. This
simple language has been inspired by PEPA, however, detailed analysis of the
semantics of cooperation will show the differences between the two formalisms.
It will also be shown that the semantics of the cooperation in process algebra
influences the correctness of the derivation of the product-form solutions."
"Intel Xeon Phi is a recently released high-performance coprocessor which
features 61 cores each supporting 4 hardware threads with 512-bit wide SIMD
registers achieving a peak theoretical performance of 1Tflop/s in double
precision. Many scientific applications involve operations on large sparse
matrices such as linear solvers, eigensolver, and graph mining algorithms. The
core of most of these applications involves the multiplication of a large,
sparse matrix with a dense vector (SpMV). In this paper, we investigate the
performance of the Xeon Phi coprocessor for SpMV. We first provide a
comprehensive introduction to this new architecture and analyze its peak
performance with a number of micro benchmarks. Although the design of a Xeon
Phi core is not much different than those of the cores in modern processors,
its large number of cores and hyperthreading capability allow many application
to saturate the available memory bandwidth, which is not the case for many
cutting-edge processors. Yet, our performance studies show that it is the
memory latency not the bandwidth which creates a bottleneck for SpMV on this
architecture. Finally, our experiments show that Xeon Phi's sparse kernel
performance is very promising and even better than that of cutting-edge general
purpose processors and GPUs."
"Several approaches have been introduced in the last few years to tackle the
problem of interpreting model-based performance analysis results and
translating them into architectural feedback. Typically the interpretation can
take place by browsing either the software model or the performance model. In
this paper, we compare two approaches that we have recently introduced for this
goal: one based on the detection and solution of performance antipatterns, and
another one based on bidirectional model transformations between software and
performance models. We apply both approaches to the same example in order to
illustrate the differences in the obtained performance results. Thereafter, we
raise the level of abstraction and we discuss the pros and cons of working on
the software side and on the performance side."
"In this paper we study the power-performance relationship of power-efficient
computing from a queuing theoretic perspective. We investigate the interplay of
several system operations including processing speed, system on/off decisions,
and server farm size. We identify that there are oftentimes ""sweet spots"" in
power-efficient operations: there exist optimal combinations of processing
speed and system settings that maximize power efficiency. For the single server
case, a widely deployed threshold mechanism is studied. We show that there
exist optimal processing speed and threshold value pairs that minimize the
power consumption. This holds for the threshold mechanism with job batching.
For the multi-server case, it is shown that there exist best processing speed
and server farm size combinations."
"Achieving high efficiency with numerical kernels for sparse matrices is of
utmost importance, since they are part of many simulation codes and tend to use
most of the available compute time and resources. In addition, especially in
large scale simulation frameworks the readability and ease of use of
mathematical expressions are essential components for the continuous
maintenance, modification, and extension of software. In this context, the
sparse matrix-matrix multiplication is of special interest. In this paper we
thoroughly analyze the single-core performance of sparse matrix-matrix
multiplication kernels in the Blaze Smart Expression Template (SET) framework.
We develop simple models for estimating the achievable maximum performance, and
use them to assess the efficiency of our implementations. Additionally, we
compare these kernels with several commonly used SET-based C++ libraries,
which, just as Blaze, aim at combining the requirements of high performance
with an elegant user interface. For the different sparse matrix structures
considered here, we show that our implementations are competitive or faster
than those of the other SET libraries for most problem sizes on a current Intel
multicore processor."
"A packet-switched network node with constant capacity (in bps) is considered,
where packets within each flow are served in the first in first out (FIFO)
manner. While this single node system is perhaps the simplest computer
communication system, its stochastic service curve characterization and
independent case analysis in the context of stochastic network calculus
(snetcal) are still basic and many crucial questions surprisingly remain open.
Specifically, when the input is a single flow, what stochastic service curve
and delay bound does the node provide? When the considered flow shares the node
with another flow, what stochastic service curve and delay bound does the node
provide to the considered flow, and if the two flows are independent, can this
independence be made use of and how? The aim of this paper is to provide
answers to these fundamental questions."
"FASTEST-3D is an MPI-parallel finite-volume flow solver based on
block-structured meshes that has been developed at the University of
Erlangen-Nuremberg since the early 1990s. It can be used to solve the laminar
or turbulent incompressible Navier-Stokes equations. Up to now its scalability
was strongly limited by a rather rigid communication infrastructure, which led
to a dominance of MPI time already at small process counts.
  This paper describes several optimizations to increase the performance,
scalability, and flexibility of FASTEST-3D. First, a node-level performance
analysis is carried out in order to pinpoint the main bottlenecks and identify
sweet spots for energy-efficient execution. In addition, a single-precision
version of the solver for the linear equation system arising from the
discretization of the governing equations is devised, which significantly
increases the single-core performance. Then the communication mechanisms in
FASTEST-3D are analyzed and a new communication strategy based on non-blocking
calls is implemented. Performance results with the revised version show
significantly increased single-node performance and considerably improved
communication patterns along with much better parallel scalability. In this
context we discuss the concept of ""acceptable parallel efficiency"" and how it
influences the real gain of the optimizations. Scaling measurements are carried
out on a modern petascale system. The obtained improvements are of major
importance for the use of FASTEST-3D on current high-performance computer
clusters and will help to perform simulations with much higher spatial and
temporal resolution to tackle turbulent flow in technical applications."
"Solid-state drives (SSDs) have been widely deployed in desktops and data
centers. However, SSDs suffer from bit errors, and the bit error rate is time
dependent since it increases as an SSD wears down. Traditional storage systems
mainly use parity-based RAID to provide reliability guarantees by striping
redundancy across multiple devices, but the effectiveness of RAID in SSDs
remains debatable as parity updates aggravate the wearing and bit error rates
of SSDs. In particular, an open problem is that how different parity
distributions over multiple devices, such as the even distribution suggested by
conventional wisdom, or uneven distributions proposed in recent RAID schemes
for SSDs, may influence the reliability of an SSD RAID array. To address this
fundamental problem, we propose the first analytical model to quantify the
reliability dynamics of an SSD RAID array. Specifically, we develop a
""non-homogeneous"" continuous time Markov chain model, and derive the transient
reliability solution. We validate our model via trace-driven simulations and
conduct numerical analysis to provide insights into the reliability dynamics of
SSD RAID arrays under different parity distributions and subject to different
bit error rates and array configurations. Designers can use our model to decide
the appropriate parity distribution based on their reliability requirements."
"This report considers a fairly general model of constrained queuing networks
that allows us to represent both MMBP (Markov Modulated Bernoulli Processes)
arrivals and time-varying service constraints. We derive a set of sufficient
conditions for throughput optimality of scheduling policies that encompass and
generalize all the previously obtained results in the field. This leads to the
definition of new classes of (non diagonal) throughput optimal scheduling
policies. We prove the stability of queues by extending the traditional
Lyapunov drift criteria methodology."
"Input-sensitive profiling is a recent performance analysis technique that
makes it possible to estimate the empirical cost function of individual
routines of a program, helping developers understand how performance scales to
larger inputs and pinpoint asymptotic bottlenecks in the code. A current
limitation of input-sensitive profilers is that they specifically target
sequential computations, ignoring any communication between threads. In this
paper we show how to overcome this limitation, extending the range of
applicability of the original approach to multithreaded applications and to
applications that operate on I/O streams. We develop new metrics for
automatically estimating the size of the input given to each routine
activation, addressing input produced by non-deterministic memory stores
performed by other threads as well as by the OS kernel (e.g., in response to
I/O or network operations). We provide real case studies, showing that our
extension allows it to characterize the behavior of complex applications more
precisely than previous approaches. An extensive experimental investigation on
a variety of benchmark suites (including the SPEC OMP2012 and the PARSEC
benchmarks) shows that our Valgrind-based input-sensitive profiler incurs an
overhead comparable to other prominent heavyweight analysis tools, while
collecting significantly more performance points from each profiling session
and correctly characterizing both thread-induced and external input."
"Memory-bound algorithms show complex performance and energy consumption
behavior on multicore processors. We choose the lattice-Boltzmann method (LBM)
on an Intel Sandy Bridge cluster as a prototype scenario to investigate if and
how single-chip performance and power characteristics can be generalized to the
highly parallel case. First we perform an analysis of a sparse-lattice LBM
implementation for complex geometries. Using a single-core performance model,
we predict the intra-chip saturation characteristics and the optimal operating
point in terms of energy to solution as a function of implementation details,
clock frequency, vectorization, and number of active cores per chip. We show
that high single-core performance and a correct choice of the number of active
cores per chip are the essential optimizations for lowest energy to solution at
minimal performance degradation. Then we extrapolate to the MPI-parallel level
and quantify the energy-saving potential of various optimizations and execution
modes, where we find these guidelines to be even more important, especially
when communication overhead is non-negligible. In our setup we could achieve
energy savings of 35% in this case, compared to a naive approach. We also
demonstrate that a simple non-reflective reduction of the clock speed leaves
most of the energy saving potential unused."
"We consider an M/M/N/K/FCFS system (N>0, K>=N), where the servers operate at
(possibly) heterogeneous service rates. In this situation, the steady state
behavior depends on the routing policy that is used to select which idle server
serves the next job in queue. We define a class of idle-time-order-based
policies (including, for example, Longest Idle Server First (LISF)) and show
that all policies in this class result in the same steady state behavior. In
particular, they are all equivalent to the naive Random routing policy."
"With the rapidly growing demand for computing power new accelerator based
architectures have entered the world of high performance computing since around
5 years. In particular GPGPUs have recently become very popular, however
programming GPGPUs using programming languages like CUDA or OpenCL is
cumbersome and error-prone. Trying to overcome these difficulties, Intel
developed their own Many Integrated Core (MIC) architecture which can be
programmed using standard parallel programming techniques like OpenMP and MPI.
In the beginning of 2013, the first production-level cards named Intel Xeon Phi
came on the market. LRZ has been considered by Intel as a leading research
centre for evaluating coprocessors based on the MIC architecture since 2010
under strict NDA. Since the Intel Xeon Phi is now generally available, we can
share our experience on programming Intel's new MIC architecture."
"Predicting performance-related behavior of the underlying network structure
becomes more and more indispensable in terms of the aspired application outcome
quality. However, the reliable forecast of QoS metrics like packet transfer
delay in wireless network systems is still a challenging task. Even though
existing approaches are technically capable of determining such network
properties under certain assumptions, they mostly abstract away from primal
aspects that inherently have an essential impact on temporal network
performance dynamics. Also, they usually require auxiliary resources to be
implemented and deployed along with the actual network components. In the
course of developing a lightweight measurement-based alternative for the
self-inspection and prediction of volatile performance characteristics in
environments of any kind, we selectively investigate the duration of message
delivery and packet loss rate against various parameters peculiar to common
radio network technologies like Wireless Sensor Networks (WSNs). Our hands-on
experiments reveal the relations between the oftentimes underestimated medium
access delay and a variety of main influencing factors including packet size,
backoff period, and number of neighbor nodes contending for the communication
medium. A closed formulation of selected weighted drivers facilitates the
average-case prediction of inter-node packet transfer delays for arbitrary
configurations of given network parameters even on resource-scarce WSN devices.
We validate our prediction method against basic multi-hop networking scenarios.
Yield field test results proof the basic feasibility and high precision of our
approach to network property estimation in virtue of self-governed local
measurements and regression-based calculations paving the way for a prospective
self-management of network properties based upon autonomous distributed
coordination."
"Embedded system software is highly constrained from performance, memory
footprint, energy consumption and implementing cost view point. It is always
desirable to obtain better Instructions per Cycle. Instruction cache has major
contribution in improving IPC. Cache memories are realized on the same chip
where the processor is running. This considerably increases the system cost as
well. Hence, it is required to maintain a trade off between cache sizes and
performance improvement offered. Determining the number of cache lines and size
of cache line are important parameters for cache designing. The design space
for cache is quite large. It is time taking to execute the given application
with different cache sizes on an instruction set simulator to figure out the
optimal cache size. In this paper, a technique is proposed to identify a number
of cache lines and cache line size for the L1 instruction cache that will offer
best or nearly best IPC. Cache size is derived, at a higher abstraction level,
from basic block analysis in the Low Level Virtual Machine environment. The
cache size estimated is cross validated by simulating the set of benchmark
applications with different cache sizes in simple scalar simulator. The
proposed method seems to be superior in terms of estimation accuracy and
estimation time as compared to the existing methods for estimation of optimal
cache size parameters like cache line size, number of cache lines."
"With the ever increasing demands of cloud computing services, planning and
management of cloud resources has become a more and more important issue which
directed affects the resource utilization and SLA and customer satisfaction.
But before any management strategy is made, a good understanding of
applications' workload in virtualized environment is the basic fact and
principle to the resource management methods. Unfortunately, little work has
been focused on this area. Lack of raw data could be one reason; another reason
is that people still use the traditional models or methods shared under
non-virtualized environment. The study of applications' workload in virtualized
environment should take on some of its peculiar features comparing to the
non-virtualized environment. In this paper, we are open to analyze the workload
demands that reflect applications' behavior and the impact of virtualization.
The results are obtained from an experimental cloud testbed running web
applications, specifically the RUBiS benchmark application. We profile the
workload dynamics on both virtualized and non-virtualized environments and
compare the findings. The experimental results are valuable for us to estimate
the performance of applications on computer architectures, to predict SLA
compliance or violation based on the projected application workload and to
guide the decision making to support applications with the right hardware."
"Big data systems address the challenges of capturing, storing, managing,
analyzing, and visualizing big data. Within this context, developing benchmarks
to evaluate and compare big data systems has become an active topic for both
research and industry communities. To date, most of the state-of-the-art big
data benchmarks are designed for specific types of systems. Based on our
experience, however, we argue that considering the complexity, diversity, and
rapid evolution of big data systems, for the sake of fairness, big data
benchmarks must include diversity of data and workloads. Given this motivation,
in this paper, we first propose the key requirements and challenges in
developing big data benchmarks from the perspectives of generating data with 4V
properties (i.e. volume, velocity, variety and veracity) of big data, as well
as generating tests with comprehensive workloads for big data systems. We then
present the methodology on big data benchmarking designed to address these
challenges. Next, the state-of-the-art are summarized and compared, following
by our vision for future research directions."
"TTL caching models have recently regained significant research interest,
largely due to their ability to fit popular caching policies such as LRU. This
paper advances the state-of-the-art analysis of TTL-based cache networks by
developing two exact methods with orthogonal generality and computational
complexity. The first method generalizes existing results for line networks
under renewal requests to the broad class of caching policies whereby evictions
are driven by stopping times. The obtained results are further generalized,
using the second method, to feedforward networks with Markov arrival processes
(MAP) requests. MAPs are particularly suitable for non-line networks because
they are closed not only under superposition and splitting, as known, but also
under input-output caching operations as proven herein for phase-type TTL
distributions. The crucial benefit of the two closure properties is that they
jointly enable the first exact analysis of feedforward networks of TTL caches
in great generality."
"Apache Hadoop and Spark are gaining prominence in Big Data processing and
analytics. Both of them are widely deployed on Internet companies. On the other
hand, high-performance data analysis requirements are causing academical and
industrial communities to adopt state-of-the-art technologies in HPC to solve
Big Data problems. Recently, we have proposed a key-value pair based
communication library, DataMPI, which is extending MPI to support
Hadoop/Spark-like Big Data Computing jobs. In this paper, we use BigDataBench,
a Big Data benchmark suite, to do comprehensive studies on performance and
resource utilization characterizations of Hadoop, Spark and DataMPI. From our
experiments, we observe that the job execution time of DataMPI has up to 55%
and 39% speedups compared with those of Hadoop and Spark, respectively. Most of
the benefits come from the high-efficiency communication mechanisms in DataMPI.
We also notice that the resource (CPU, memory, disk and network I/O)
utilizations of DataMPI are also more efficient than those of the other two
frameworks."
"Exploration of task mappings plays a crucial role in achieving high
performance in heterogeneous multi-processor system-on-chip (MPSoC) platforms.
The problem of optimally mapping a set of tasks onto a set of given
heterogeneous processors for maximal throughput has been known, in general, to
be NP-complete. The problem is further exacerbated when multiple applications
(i.e., bigger task sets) and the communication between tasks are also
considered. Previous research has shown that Genetic Algorithms (GA) typically
are a good choice to solve this problem when the solution space is relatively
small. However, when the size of the problem space increases, classic genetic
algorithms still suffer from the problem of long evolution times. To address
this problem, this paper proposes a novel bias-elitist genetic algorithm that
is guided by domain-specific heuristics to speed up the evolution process.
Experimental results reveal that our proposed algorithm is able to handle large
scale task mapping problems and produces high-quality mapping solutions in only
a short time period."
"Tandem queues with finite buffer capacity commonly exist in practical
applications. By viewing a tandem queue as an integrated system, an innovative
approach has been developed to analyze its performance through the insight from
reduction method. In our approach, the starvation at the bottleneck caused by
service time randomness is modeled and captured by interruptions. Fundamental
properties of tandem queues with finite buffer capacity are examined. We show
that in general system service rate of a dual tandem queue with finite buffer
capacity is equal or smaller than its bottleneck service rate, and virtual
interruptions, which are the extra idle period at the bottleneck caused by the
non-bottlenecks, depend on arrival rates. Hence, system service rate is a
function of arrival rate when the buffer capacity of a tandem queue is finite.
Approximation for the mean queue time of a dual tandem queue has been developed
through the concept of virtual interruptions."
"This work presents an effort to bridge the gap between abstract high level
programming and OpenCL by extending an existing high level Java programming
framework (APARAPI), based on OpenCL, so that it can be used to program FPGAs
at a high level of abstraction and increased ease of programmability. We run
several real world algorithms to assess the performance of the framework on
both a low end and a high end system. On the low end and high end systems
respectively we observed up to 78-80 percent power reduction and 4.8X-5.3X
speed increase running NBody simulation, as well as up to 65-80 percent power
reduction and 6.2X-7X speed increase for a KMeans, MapReduce algorithm running
on top of the Hadoop framework and APARAPI."
"We analyze the performance of a linear-equality-constrained least-squares
(CLS) algorithm and its relaxed version, called rCLS, that is obtained via the
method of weighting. The rCLS algorithm solves an unconstrained least-squares
problem that is augmented by incorporating a weighted form of the linear
constraints. As a result, unlike the CLS algorithm, the rCLS algorithm is
amenable to our approach to performance analysis presented here, which is akin
to the energy-conservation-based methodology. Therefore, we initially inspect
the convergence properties and evaluate the precision of estimation as well as
satisfaction of the constraints for the rCLS algorithm in both mean and
mean-square senses. Afterwards, we examine the performance of the CLS algorithm
by evaluating the limiting performance of the rCLS algorithm as the relaxation
parameter (weight) approaches infinity. Numerical examples verify the accuracy
of the theoretical findings."
"A new approach to the steady state detection in the uniformization method of
solving continuous time Markov chains is introduced. The method is particularly
useful in solving inhomogenous CTMC's in multiple steps, where the desired
error bound of the whole solution can be distributed not proportionally to the
lengths of the respective intervals, but rather in a way, that maximizes the
chances of detecting a steady state. Additionally, the convergence properties
of the underlying DTMC are used to further enhance the computational savings
due to the steady state detection. The method is applied to the problem of
modeling a Call Center using inhomogenous CTMC model of a M(t)/M(t)/s(t)
queuing system."
"Remote data access for data analysis in high performance computing is
commonly done with specialized data access protocols and storage systems. These
protocols are highly optimized for high throughput on very large datasets,
multi-streams, high availability, low latency and efficient parallel I/O. The
purpose of this paper is to describe how we have adapted a generic protocol,
the Hyper Text Transport Protocol (HTTP) to make it a competitive alternative
for high performance I/O and data analysis applications in a global computing
grid: the Worldwide LHC Computing Grid. In this work, we first analyze the
design differences between the HTTP protocol and the most common high
performance I/O protocols, pointing out the main performance weaknesses of
HTTP. Then, we describe in detail how we solved these issues. Our solutions
have been implemented in a toolkit called davix, available through several
recent Linux distributions. Finally, we describe the results of our benchmarks
where we compare the performance of davix against a HPC specific protocol for a
data analysis use case."
"Stencil algorithms on regular lattices appear in many fields of computational
science, and much effort has been put into optimized implementations. Such
activities are usually not guided by performance models that provide estimates
of expected speedup. Understanding the performance properties and bottlenecks
by performance modeling enables a clear view on promising optimization
opportunities. In this work we refine the recently developed
Execution-Cache-Memory (ECM) model and use it to quantify the performance
bottlenecks of stencil algorithms on a contemporary Intel processor. This
includes applying the model to arrive at single-core performance and
scalability predictions for typical corner case stencil loop kernels. Guided by
the ECM model we accurately quantify the significance of ""layer conditions,""
which are required to estimate the data traffic through the memory hierarchy,
and study the impact of typical optimization approaches such as spatial
blocking, strength reduction, and temporal blocking for their expected
benefits. We also compare the ECM model to the widely known Roofline model."
"Performance modeling typically relies on two antithetic methodologies: white
box models, which exploit knowledge on system's internals and capture its
dynamics using analytical approaches, and black box techniques, which infer
relations among the input and output variables of a system based on the
evidences gathered during an initial training phase. In this paper we
investigate a technique, which we name Bootstrapping, which aims at reconciling
these two methodologies and at compensating the cons of the one with the pros
of the other. We thoroughly analyze the design space of this gray box modeling
technique, and identify a number of algorithmic and parametric trade-offs which
we evaluate via two realistic case studies, a Key-Value Store and a Total Order
Broadcast service."
"The amount of software running on mobile devices is constantly growing as
consumers and industry purchase more battery powered devices. On the other
hand, tools that provide developers with feed- back on how their software
changes affect battery life are not widely available. This work employs Green
Mining, the study of the rela- tionship between energy consumption and software
changesets, and n-gram language models to evaluate if source code changeset
perplex- ity correlates with change in energy consumption. A correlation be-
tween perplexity and change in energy consumption would permit the development
of a tool that predicts the impact a code changeset may have on a software
applications energy consumption. The case study results show that there is weak
to no correlation between cross en- tropy and change in energy consumption.
Therefore, future areas of investigation are proposed."
"OpenFlow is one of the most commonly used protocols for communication between
the controller and the forwarding element in a software defined network (SDN).
A model based on M/M/1 queues is proposed in [1] to capture the communication
between the forwarding element and the controller. Albeit the model provides
useful insight, it is accurate only for the case when the probability of
expecting a new flow is small. Secondly, it is not straight forward to extend
the model in [1] to more than one forwarding element in the data plane. In this
work we propose a model which addresses both these challenges. The model is
based on Jackson assumption but with corrections tailored to the OpenFlow based
SDN network. Performance analysis using the proposed model indicates that the
model is accurate even for the case when the probability of new flow is quite
large. Further we show by a toy example that the model can be extended to more
than one node in the data plane."
"In-memory (transactional) data stores are recognized as a first-class data
management technology for cloud platforms, thanks to their ability to match the
elasticity requirements imposed by the pay-as-you-go cost model. On the other
hand, defining the well-suited amount of cache servers to be deployed, and the
degree of in-memory replication of slices of data, in order to optimize
reliability/availability and performance tradeoffs, is far from being a trivial
task. Yet, it is an essential aspect of the provisioning process of cloud
platforms, given that it has an impact on how well cloud resources are actually
exploited. To cope with the issue of determining optimized configurations of
cloud in-memory data stores, in this article we present a flexible simulation
framework offering skeleton simulation models that can be easily specialized in
order to capture the dynamics of diverse data grid systems, such as those
related to the specific protocol used to provide data consistency and/or
transactional guarantees. Besides its flexibility, another peculiar aspect of
the framework lies in that it integrates simulation and machine-learning
(black-box) techniques, the latter being essentially used to capture the
dynamics of the data-exchange layer (e.g. the message passing layer) across the
cache servers. This is a relevant aspect when considering that the actual
data-transport/networking infrastructure on top of which the data grid is
deployed might be unknown, hence being not feasible to be modeled via white-box
(namely purely simulative) approaches. We also provide an extended experimental
study aimed at validating instances of simulation models supported by our
framework against execution dynamics of real data grid systems deployed on top
of either private or public cloud infrastructures."
"We consider a multi-class G/G/1 queue with a finite shared buffer. There is
task admission and server scheduling control which aims to minimize the cost
which consists of holding and rejection components. We construct a policy that
is asymptotically optimal in the heavy traffic limit. The policy stems from
solution to Harrison-Taksar (HT) free boundary problem and is expressed by a
single free boundary point. We show that the HT problem solution translated
into the queuelength processes follows a specific {\it triangular} form. This
form implies the queuelength control policy which is different from the known
$c\mu$ priority rule and has a novel structure.
  We exemplify that the probabilistic methods we exploit can be successfully
applied to solving scheduling and admission problems in cloud computing."
"Supermarket models with different servers become a key in modeling resource
management of stochastic networks, such as, computer networks, manufacturing
systems and transportation networks. While these different servers always make
analysis of such a supermarket model more interesting, difficult and
challenging. This paper provides a new novel method for analyzing the
supermarket model with different servers through a multi-dimensional
continuous-time Markov reward processes. Firstly, the utility functions are
constructed for expressing a routine selection mechanism that depends on queue
lengths, on service rates, and on some probabilities of individual preference.
Then applying the continuous-time Markov reward processes, some segmented
stochastic integrals of the random reward function are established by means of
an event-driven technique. Based on this, the mean of the random reward
function in a finite time period is effectively computed by means of the state
jump points of the Markov reward process, and also the mean of the discounted
random reward function in an infinite time period can be calculated through the
same event-driven technique. Finally, some simulation experiments are given to
indicate how the expected queue length of each server depends on the main
parameters of this supermarket model."
"We investigate the performance characteristics of a numerically enhanced
scalar product (dot) kernel loop that uses the Kahan algorithm to compensate
for numerical errors, and describe efficient SIMD-vectorized implementations on
recent Intel processors. Using low-level instruction analysis and the
execution-cache-memory (ECM) performance model we pinpoint the relevant
performance bottlenecks for single-core and thread-parallel execution, and
predict performance and saturation behavior. We show that the Kahan-enhanced
scalar product comes at almost no additional cost compared to the naive
(non-Kahan) scalar product if appropriate low-level optimizations, notably SIMD
vectorization and unrolling, are applied. We also investigate the impact of
architectural changes across four generations of Intel Xeon processors."
"The purpose of this paper is to analyze the so-called back-off technique of
the IEEE 802.11 protocol in broadcast mode with waiting queues. In contrast to
existing models, packets arriving when a station (or node) is in back-off state
are not discarded, but are stored in a buffer of infinite capacity. As in
previous studies, the key point of our analysis hinges on the assumption that
the time on the channel is viewed as a random succession of transmission slots
(whose duration corresponds to the length of a packet) and mini-slots during
which the back-o? of the station is decremented. These events occur
independently, with given probabilities. The state of a node is represented by
a two-dimensional Markov chain in discrete-time, formed by the back-off counter
and the number of packets at the station. Two models are proposed both of which
are shown to cope reasonably well with the physical principles of the protocol.
The stabillity (ergodicity) conditions are obtained and interpreted in terms of
maximum throughput. Several approximations related to these models are also
discussed."
"We present an extension of the window flow control analysis by R. Agrawal
et.al. (Reference [1]), C.-S. Chang (Reference [6]), and C.-S. Chang et. al.
(Reference [8]) to a system with random service time and fixed feedback delay.
We consider two network service models. In the first model, the network service
process itself has no time correlations. The second model addresses a two-state
Markov-modulated service."
"This is the Proceedings of the 2nd OMNeT++ Community Summit, which was held
at IBM Research - Zurich, Switzerland on September 3-4, 2015."
"Content Delivery Networks (CDNs) are becoming an integral part of the future
generation Internet. Traditionally, these networks have been designed with the
goals of traffic offload and the improvement of users' quality of experience
(QoE), but the energy consumption is also becoming an indispensable design
factor for CDNs to be a sustainable solution. To study and improve the CDN
architectures using this new design metric, we are planning to develop a
generic and flexible simulation package in OMNet++. This package is aimed to
render a holistic view about the CDN energy consumption behaviour by
incorporating the state-of-the-art energy consumption models proposed for the
individual elements of CDNs (e.g. servers, routers, wired and wireless links,
wireless devices, etc.) and for the various Internet contents (web pages,
files, streaming video, etc.)."
"In this work, a VANET (Vehicular Ad-hoc NETwork) is considered to operate on
a simple lane, without infrastructure. The arrivals of vehicles are assumed to
be general with any traffic and speed assumptions. The vehicles communicate
through the shortest path. In this paper, we study the probability distribution
of the number of hops on the maximal shortest path in a connected component of
vehicles. The general formulation is given for any assumption of road traffic.
Then, it is applied to calculate the z-transform of this distribution for
medium and dense networks in the Poisson case. Our model is validated with the
Madrid road traces of the Universitat Polit\`ecnica de Catalunya. These results
may be useful for example when evaluating diffusion protocols through the
shortest path in a VANET, where not only the mean but also the other moments
are needed to derive accurate results."
This paper has been withdrawn by the author
"Heterogeneous parallel systems are widely spread nowadays. Despite their
availability, their usage and adoption are still limited, and even more rarely
they are used to full power. Indeed, compelling new technologies are constantly
developed and keep changing the technological landscape, but each of them
targets a limited sub-set of supported devices, and nearly all of them require
new programming paradigms and specific toolsets. Software, however, can hardly
keep the pace with the growing number of computational capabilities, and
developers are less and less motivated in learning skills that could quickly
become obsolete. In this paper we present our effort in the direction of a
transparent system optimization based on automatic code profiling and
Just-In-Time compilation, that resulted in a fully-working embedded prototype
capable of dynamically detect computing-intensive code blocks and automatically
dispatch them to different computation units. Experimental results show that
our system allows gains up to 32x in performance --- after an initial warm-up
phase --- without requiring any human intervention."
"Parallel systems have received increasing attention with numerous recent
applications such as fork-join systems, load-balancing, and l-out-of-k
redundancy. Common to these systems is a join or resequencing stage, where
tasks that have finished service may have to wait for the completion of other
tasks so that they leave the system in a predefined order. These
synchronization constraints make the analysis of parallel systems challenging
and few explicit results are known. In this work, we model parallel systems
using a max-plus approach that enables us to derive statistical bounds of
waiting and sojourn times. Taking advantage of max-plus system theory, we also
show end-to-end delay bounds for multi-stage fork-join networks. We contribute
solutions for basic G|G|1 fork-join systems, parallel systems with
load-balancing, as well as general (k,l) fork-join systems with redundancy. Our
results provide insights into the respective advantages of l-out-of-k
redundancy vs. load-balancing."
"This article introduces a novel family of decentralised caching policies,
applicable to wireless networks with finite storage at the edge-nodes
(stations). These policies are based on the Least-Recently-Used replacement
principle, and are, here, referred to as spatial multi-LRU. Based on these,
cache inventories are updated in a way that provides content diversity to users
who are covered by, and thus have access to, more than one station. Two
variations are proposed, namely the multi-LRU-One and -All, which differ in the
number of replicas inserted in the involved caches. By introducing spatial
approximations, we propose a Che-like method to predict the hit probability,
which gives very accurate results under the Independent Reference Model (IRM).
It is shown that the performance of multi-LRU increases the more the
multi-coverage areas increase, and it approaches the performance of other
proposed centralised policies, when multi-coverage is sufficient. For IRM
traffic multi-LRU-One outperforms multi-LRU-All, whereas when the traffic
exhibits temporal locality the -All variation can perform better."
"We investigate the performance characteristics of a numerically enhanced
scalar product (dot) kernel loop that uses the Kahan algorithm to compensate
for numerical errors, and describe efficient SIMD-vectorized implementations on
recent multi- and manycore processors. Using low-level instruction analysis and
the execution-cache-memory (ECM) performance model we pinpoint the relevant
performance bottlenecks for single-core and thread-parallel execution, and
predict performance and saturation behavior. We show that the Kahan-enhanced
scalar product comes at almost no additional cost compared to the naive
(non-Kahan) scalar product if appropriate low-level optimizations, notably SIMD
vectorization and unrolling, are applied. The ECM model is extended
appropriately to accommodate not only modern Intel multicore chips but also the
Intel Xeon Phi ""Knights Corner"" coprocessor and an IBM POWER8 CPU. This allows
us to discuss the impact of processor features on the performance across four
modern architectures that are relevant for high performance computing."
"Cumulative advantage (CA) refers to the notion that accumulated resources
foster the accumulation of further resources in competitions, a phenomenon that
has been empirically observed in various contexts. The oldest and arguably
simplest mathematical model that embodies this general principle is the P\'olya
urn process, which finds applications in a myriad of problems. The original
model captures the dynamics of competitions between two equally fit agents
under linear CA effects, which can be readily generalized to incorporate
different fitnesses and nonlinear CA effects. We study two statistics of
competitions under the generalized model, namely duration (i.e., time of the
last tie) and intensity (i.e., number of ties). We give rigorous mathematical
characterizations of the tail distributions of both duration and intensity
under the various regimes for fitness and nonlinearity, which reveal very
interesting behaviors. For example, fitness superiority induces much shorter
competitions in the sublinear regime while much longer competitions in the
superlinear regime. Our findings can shed light on the application of P\'olya
urn processes in more general contexts where fitness and nonlinearity may be
present."
"It remains a challenging problem to tightly estimate the worst case response
time of an application in a distributed embedded system, especially when there
are dependencies between tasks. We discovered that the state-of-the art
techniques considering task dependencies either fail to obtain a conservative
bound or produce a loose upper bound. We propose a novel conservative
performance analysis, called hybrid performance analysis, combining the
response time analysis technique and the scheduling time bound analysis
technique to compute a tighter bound fast. Through extensive experiments with
randomly generated graphs, superior performance of our proposed approach
compared with previous methods is confirmed."
"We present a mechanism to symbolically gather performance-relevant operation
counts from numerically-oriented subprograms (`kernels') expressed in the Loopy
programming system, and apply these counts in a simple, linear model of kernel
run time. We use a series of `performance-instructive' kernels to fit the
parameters of a unified model to the performance characteristics of GPU
hardware from multiple hardware generations and vendors. We evaluate the
predictive power of the model on a broad array of computational kernels
relevant to scientific computing. In terms of the geometric mean, our simple,
vendor- and GPU-type-independent model achieves relative accuracy comparable to
that of previously published work using hardware specific models."
"We show a methodology for the computation of the probability of deadline miss
for a periodic real-time task scheduled by a resource reservation algorithm. We
propose a modelling technique for the system that reduces the computation of
such a probability to that of the steady state probability of an infinite state
Discrete Time Markov Chain with a periodic structure. This structure is
exploited to develop an efficient numeric solution where different
accuracy/computation time trade-offs can be obtained by operating on the
granularity of the model. More importantly we offer a closed form conservative
bound for the probability of a deadline miss. Our experiments reveal that the
bound remains reasonably close to the experimental probability in one real-time
application of practical interest. When this bound is used for the optimisation
of the overall Quality of Service for a set of tasks sharing the CPU, it
produces a good sub-optimal solution in a small amount of time."
"The recent research effort towards defining new communication solutions for
cyber-physical systems (CPS), to guarantee high availability level with limited
cabling costs and complexity, has renewed the interest in ring-based networks.
This topology has been recently used for various networked cyber-physical
systems (Net-CPS), e.g., avionics and automotive, with the implementation of
many Real Time Ethernet (RTE) profiles. A relevant issue for such networks is
to prove timing predictability, a key requirement for safety-critical systems.
We are interested in this paper in event-triggered ring-based networks, which
guarantee high resource utilization efficiency and (re)configuration
flexibility, at the cost of increasing the timing analysis complexity. The
implementation of such a communication scheme on top of a ring topology
actually induces cyclic dependencies, in comparison to time-triggered
solutions. To cope with this arising issue of cyclic dependencies, only few
techniques have been proposed in the literature, mainly based on Network
Calculus framework, and consist in analyzing locally the delay upper bound in
each crossed node, resulting in pessimistic end-to-end delay bounds. Hence, the
main contribution in this paper is enhancing the delay bounds tightness of such
networks, through an innovative global analysis based on Network Calculus,
accounting the flow serialization phenomena along the flow path. An extensive
analysis of such a proposal is conducted herein regarding the accuracy of delay
bounds and its impact on the system performance, i.e., scalability and
resource-efficiency; and the results highlight its outperformance, in
comparison to conventional methods."
"Diagnosing and fixing performance problems on multicore machines with deep
memory hierarchies is extremely challenging. Certain problems are best
addressed when we can analyze the entire trace of program execution, e.g.,
every memory access. Unfortunately such detailed execution logs are very large
and cannot be analyzed by direct inspection. We present DINAMITE: a toolkit for
Dynamic INstrumentation and Analysis for MassIve Trace Exploration. DINAMITE is
a collection of tools for end-to-end performance analysis: from the LLVM
compiler pass that instruments the program to plug-and-play tools that use a
modern data analytics engine Spark Streaming for trace introspection. Using
DINAMITE we found opportunities to improve data layout in several applications
that resulted in 15-20% performance improvements and found a shared-variable
bottleneck in a popular key-value store, whose elimination improved performance
by 20x."
"With the spread of multi- and many-core processors more and more typical task
is to re-implement some source code written originally for a single processor
to run on more than one cores. Since it is a serious investment, it is
important to decide how much efforts pays off, and whether the resulting
implementation has as good performability as it could be. The Amdahl's law
provides some theoretical upper limits for the performance gain reachable
through parallelizing the code, but it needs the detailed architectural
knowledge of the program code, does not consider the housekeeping activity
needed for parallelization and cannot tell how the actual stage of
parallelization implementation performs. The present paper suggests a
quantitative measure for that goal. This figure of merit is derived
experimentally, from measured running time, and number of threads/cores. It can
be used to quantify the used parallelization technology, the connection between
the computing units, the acceleration technology under the given conditions,
communication method within SoC, or the performance of the software
team/compiler."
"The DNS relies on caching to ensure high scalability and good performance. In
optimizing caching, TTL adjustment provides a means of balancing between query
load and TTL-dependent performances such as data consistency, load balancing,
migration time, etc. To gain the desired balance, TTL adjustment depends on
predictions of query loads under alternative TTLs. This paper proposes a model
of DNS server load, which employs the uniform aggregate caching model to
simplify the complexity of modeling clients' requests and their caching. A
method of predicting DNS server load is developed using that model. The
prediction method is solely based on the unilateral measurements or
observations at authoritative servers. Without reliance on lots of multi-point
measurements nor distributed measuring facilities, the method is best suited
for DNS authoritative operators. The proposed model and prediction method are
validated through extensive simulations. Finally, global sensibility analysis
is conducted to evaluate the impacts of measurement uncertainties or errors on
the predictions."
"We consider the fixed-delay synthesis problem for continuous-time Markov
chains extended with fixed-delay transitions (fdCTMC). The goal is to
synthesize concrete values of the fixed-delays (timeouts) that minimize the
expected total cost incurred before reaching a given set of target states. The
same problem has been considered and solved in previous works by computing an
optimal policy in a certain discrete-time Markov decision process (MDP) with a
huge number of actions that correspond to suitably discretized values of the
timeouts.
  In this paper, we design a symbolic fixed-delay synthesis algorithm which
avoids the explicit construction of large action spaces. Instead, the algorithm
computes a small sets of ""promising"" candidate actions on demand. The candidate
actions are selected by minimizing a certain objective function by computing
its symbolic derivative and extracting a univariate polynomial whose roots are
precisely the points where the derivative takes zero value. Since roots of high
degree univariate polynomials can be isolated very efficiently using modern
mathematical software, we achieve not only drastic memory savings but also
speedup by three orders of magnitude compared to the previous methods."
"Conventional load-testing tools are based on a fifty-year old time-share
computer paradigm where a finite number of users submit requests and respond in
a synchronized fashion. Conversely, modern web traffic is essentially
asynchronous and driven by an unknown number of users. This difference presents
a conundrum for testing the performance of modern web applications. Even when
the difference is recognized, performance engineers often introduce
modifications to their test scripts based on folklore or hearsay published in
various Internet fora, much of which can lead to wrong results. We present a
coherent methodology, based on two fundamental principles, for emulating web
traffic using a standard load-test environment."
"In this paper we provide a performance analysis framework for wireless
industrial networks by deriving a service curve and a bound on the delay
violation probability. For this purpose we use the (min,x) stochastic network
calculus as well as a recently presented recursive formula for an end-to-end
delay bound of wireless heterogeneous networks. The derived results are mapped
to WirelessHART networks used in process automation and were validated via
simulations. In addition to WirelessHART, our results can be applied to any
wireless network whose physical layer conforms the IEEE 802.15.4 standard,
while its MAC protocol incorporates TDMA and channel hopping, like e.g.
ISA100.11a or TSCH-based networks. The provided delay analysis is especially
useful during the network design phase, offering further research potential
towards optimal routing and power management in QoS-constrained wireless
industrial networks."
"To meet the ever-increasing demands on higher throughput and better network
delay performance, 60 GHZ networking is proposed as a promising solution for
the next generation of wireless communications. To successfully deploy such
networks, its important to understand their performance first. However, due to
the unique fading characteristic of the 60 GHz channel, the characterization of
the corresponding service process, offered by the channel, using the
conventional methodologies may not be tractable. In this work, we provide an
alternative approach to derive a closed-form expression that characterizes the
cumulative service process of the 60 GHz channel in terms of the moment
generating function (MGF) of its instantaneous channel capacity. We then use
this expression to derive probabilistic upper bounds on the backlog and delay
that are experienced by a flow traversing this network, using results from the
MGF-based network calculus. The computed bounds are validated using simulation.
We provide numerical results for different networking scenarios and for
different traffic and channel parameters and we show that the 60 GHz wireless
network is capable of satisfying stringent quality-of-Service (QoS)
requirements, in terms of network delay and reliability. With this analysis
approach at hand, a larger scale 60 GHz network design and optimization is
possible."
"High Speed Downlink Packet Access (HSDPA) was introduced to UMTS radio access
segment to provide higher capacity for new packet switched services. As a
result, packet switched sessions with multiple diverse traffic flows such as
concurrentvoice and data, or video and data being transmitted to the same user
are a likely commonplace cellular packet data scenario. In HSDPA, Radio Access
Network (RAN) buffer management schemes are essential to support the end-to-end
QoS of such sessions. Hence in this paper we present the end-to-end performance
study of a proposed RAN buffer management scheme for multi-flow sessions via
dynamic system-level HSDPA simulations. The scheme is an enhancement of a
Time-Space Priority (TSP)queuing strategy applied to the Node B MAC-hs buffer
allocated to an end user with concurrent real-time (RT) and non-real-time (NRT)
flows during a multi-flow session. The experimental multiflow scenario is a
packet voice call with concurrent TCP-based file download to the same user.
Results show that with the proposed enhancements to the TSP-based RAN buffer
management, end-to-end QoS performance gains accrue to the NRT flow without
compromising RT flow QoS of the same end user session."
"The increased deployment of wireless networks for battery-limited industrial
applications in recent years highlights the need for tractable performance
analysis and efficient QoS-aware transmit power management schemes. Modern
industrial solutions deploy multi-hop topologies in order to bridge larger
distances without necessarily shortening nodes' battery lifetime. This poses a
significant challenge, as multi-hop analysis for heterogeneous wireless
networks does not exist prior to our work. We overcome this challenge by
extending a newly developed methodology based on (min,x) network calculus and
provide a closed-form expression for the end-to-end delay violation probability
over a cascade of heterogeneous buffered wireless fading channels. We further
design model-based algorithms for power-minimization and network lifetime
maximization which compute the optimal transmit power per node, along a
QoS-constrained path. Our numerical study shows an overall transmit power
savings of up to 95% when compared to a fixed power allocation. We also apply
our algorithm to a realistic WirelessHART network setup and observe that link
heterogeneity can significantly influence network lifetime when no efficient
power management is applied. This work is especially useful for battery-powered
wireless sensor nodes in QoS-constrained applications and offers a solid
framework for network design and performance analysis of heterogeneous
multi-hop wireless industrial networks."
"Developments in the context of Open, Big, and Linked Data have led to an
enormous growth of structured data on the Web. To keep up with the pace of
efficient consumption and management of the data at this rate, many data
Management solutions have been developed for specific tasks and applications.
We present LITMUS, a framework for benchmarking data management solutions.
LITMUS goes beyond classical storage benchmarking frameworks by allowing for
analysing the performance of frameworks across query languages. In this
position paper we present the conceptual architecture of LITMUS as well as the
considerations that led to this architecture."
"Simulation is widely adopted in the study of modern computer networks. In
this context, OMNeT++ provides a set of very effective tools that span from the
definition of the network, to the automation of simulation execution and quick
result representation. However, as network models become more and more complex
to cope with the evolution of network systems, the amount of simulation
factors, the number of simulated nodes and the size of results grow
consequently, leading to simulations with larger scale. In this work, we
perform a critical analysis of the tools provided by OMNeT++ in case of such
large-scale simulations. We then propose a unified and flexible software
architecture to support simulation automation."
"Motivated by timeouts in Internet services, we consider networks of infinite
server queues in which routing decisions are based on deadlines. Specifically,
at each node in the network, the total service time equals the minimum of
several independent service times (e.g. the minimum of the amount of time
required to complete a transaction and a deadline). Furthermore, routing
decisions depend on which of the independent service times achieves the minimum
(e.g. exceeding a deadline will require the customer to be routed so they can
re-attempt the transaction). Because current routing decisions are dependent on
past service times, much of the existing theory on product-form queueing
networks does not apply. In spite of this, we are able to show that such
networks have product-form equilibrium distributions. We verify our analytic
characterization with a simulation of a simple network. We also discuss
extensions of this work to more general settings."
"In order to boost the performance of data-intensive computing on HPC systems,
in-memory computing frameworks, such as Apache Spark and Flink, use local DRAM
for data storage. Optimizing the memory allocation to data storage is critical
to delivering performance to traditional HPC compute jobs and throughput to
data-intensive applications sharing the HPC resources. Current practices that
statically configure in-memory storage may leave inadequate space for compute
jobs or lose the opportunity to utilize more available space for data-intensive
applications. In this paper, we explore techniques to dynamically adjust
in-memory storage and make the right amount of space for compute jobs. We have
developed a dynamic memory controller, DynIMS, which infers memory demands of
compute tasks online and employs a feedback-based control model to adapt the
capacity of in-memory storage. We test DynIMS using mixed HPCC and Spark
workloads on a HPC cluster. Experimental results show that DynIMS can achieve
up to 5X performance improvement compared to systems with static memory
allocations."
"This paper proposes a data-unit-size distribution model to represent the
retransmitted packet size preservation (RPSP) property in a scenario where
independently lost packets are retransmitted by a stop-and-wait protocol. RPSP
means that retransmitted packets with the same sequence number are equal in
size to the packet of the original transmission, which is identical to the
packet generated from a message through the segmentation function, namely,
generated packet. Furthermore, we derive goodput formula using an approach to
derive the data-unit-size distribution. We investigate the effect of RPSP on
frame size distributions and goodput in a simple case when no collision happens
over the bit-error prone wireless network equipped with IEEE 802.11 Distributed
Coordination Function, which is a typical example of the stop-and-wait
protocol. Numerical results show that the effect gets stronger as bit error
rate increases and the maximum size of the generated packets is larger than the
mean size for large enough packet retry limits because longer packets will be
repeatedly corrupted and retransmitted more times as a result of RPSP."
"Multi-server systems have received increasing attention with important
implementations such as Google MapReduce, Hadoop, and Spark. Common to these
systems are a fork operation, where jobs are first divided into tasks that are
processed in parallel, and a later join operation, where completed tasks wait
until the results of all tasks of a job can be combined and the job leaves the
system. The synchronization constraint of the join operation makes the analysis
of fork-join systems challenging and few explicit results are known. In this
work, we model fork-join systems using a max-plus server model that enables us
to derive statistical bounds on waiting and sojourn times for general arrival
and service time processes. We contribute end-to-end delay bounds for
multi-stage fork-join networks that grow in $\mathcal{O}(h \ln k)$ for $h$
fork-join stages, each with $k$ parallel servers. We perform a detailed
comparison of different multi-server configurations and highlight their pros
and cons. We also include an analysis of single-queue fork-join systems that
are non-idling and achieve a fundamental performance gain, and compare these
results to both simulation and a live Spark system."
"Nowadays, more and more increasingly hard computations are performed in
challenging fields like weather forecasting, oil and gas exploration, and
cryptanalysis. Many of such computations can be implemented using a computer
cluster with a large number of servers. Incoming computation requests are then,
via a so-called load balancing policy, distributed over the servers to ensure
optimal performance. Additionally, being able to switch-off some servers during
low period of workload, gives potential to reduced energy consumption.
Therefore, load balancing forms, albeit indirectly, a trade-off between
performance and energy consumption. In this paper, we introduce a syntax for
load-balancing policies to dynamically select a server for each request based
on relevant criteria, including the number of jobs queued in servers, power
states of servers, and transition delays between power states of servers. To
evaluate many policies, we implement two load balancers in: (i) iDSL, a
language and tool-chain for evaluating service-oriented systems, and (ii) a
simulation framework in AnyLogic. Both implementations are successfully
validated by comparison of the results."
"There exist multitudes of cloud performance metrics, including workload
performance, application placement, software/hardware optimization,
scalability, capacity, reliability, agility and so on. In this paper, we
consider jointly optimizing the performance of the software applications in the
cloud. The challenges lie in bringing a diversity of raw data into tidy data
format, unifying performance data from multiple systems based on timestamps,
and assessing the quality of the processed performance data. Even after
verifying the quality of cloud performance data, additional challenges block
optimizing cloud computing. In this paper, we identify the challenges of cloud
computing from the perspectives of computing environment, data collection,
performance analytics and production environment."
"The rapid advances in sensors and ultra-low power wireless communication has
enabled a new generation of wireless sensor networks: Wireless Body Area
Networks (WBAN). To the best of our knowledge the current paper is the first to
address broadcast in WBAN. We first analyze several broadcast strategies
inspired from the area of Delay Tolerant Networks (DTN). The proposed
strategies are evaluated via the OMNET++ simulator that we enriched with
realistic human body mobility models and channel models issued from the recent
research on biomedical and health informatics. Contrary to the common
expectation, our results show that existing research in DTN cannot be
transposed without significant modifications in WBANs area. That is, existing
broadcast strategies for DTNs do not perform well with human body mobility.
However, our extensive simulations give valuable insights and directions for
designing efficient broadcast in WBAN. Furthermore, we propose a novel
broadcast strategy that outperforms the existing ones in terms of end-to-end
delay, network coverage and energy consumption. Additionally, we performed
investigations of independent interest related to the ability of all the
studied strategies to ensure the total order delivery property when stressed
with various packet rates. These investigations open new and challenging
research directions."
"We present efficient realization of Householder Transform (HT) based QR
factorization through algorithm-architecture co-design where we achieve
performance improvement of 3-90x in-terms of Gflops/watt over state-of-the-art
multicore, General Purpose Graphics Processing Units (GPGPUs), Field
Programmable Gate Arrays (FPGAs), and ClearSpeed CSX700. Theoretical and
experimental analysis of classical HT is performed for opportunities to exhibit
higher degree of parallelism where parallelism is quantified as a number of
parallel operations per level in the Directed Acyclic Graph (DAG) of the
transform. Based on theoretical analysis of classical HT, an opportunity
re-arrange computations in the classical HT is identified that results in
Modified HT (MHT) where it is shown that MHT exhibits 1.33x times higher
parallelism than classical HT. Experiments in off-the-shelf multicore and
General Purpose Graphics Processing Units (GPGPUs) for HT and MHT suggest that
MHT is capable of achieving slightly better or equal performance compared to
classical HT based QR factorization realizations in the optimized software
packages for Dense Linear Algebra (DLA). We implement MHT on a customized
platform for Dense Linear Algebra (DLA) and show that MHT achieves 1.3x better
performance than native implementation of classical HT on the same accelerator.
For custom realization of HT and MHT based QR factorization, we also identify
macro operations in the DAGs of HT and MHT that are realized on a
Reconfigurable Data-path (RDP). We also observe that due to re-arrangement in
the computations in MHT, custom realization of MHT is capable of achieving 12%
better performance improvement over multicore and GPGPUs than the performance
improvement reported by General Matrix Multiplication (GEMM) over highly tuned
DLA software packages for multicore and GPGPUs which is counter-intuitive."
"We consider a Markovian many server queueing system in which customers are
preemptively scheduled according to exogenously assigned priority levels. The
priority levels are randomly assigned from a continuous probability measure
rather than a discrete one and hence, the queue is modeled by an infinite
dimensional stochastic process. We analyze the equilibrium behavior of the
system and provide several results. We derive the Radon-Nikodym derivative
(with respect to Lebesgue measure) of the measure that describes the average
distribution of customer priority levels in the system; we provide a formula
for the expected sojourn time of a customer as a function of his priority
level; and we provide a formula for the expected waiting time of a customer as
a function of his priority level. We verify our theoretical analysis with
discrete-event simulations. We discuss how each of our results generalizes
previous work on infinite dimensional models for single server priority queues."
"We study the scheduling polices for asymptotically optimal delay in queueing
systems with switching overhead. Such systems consist of a single server that
serves multiple queues, and some capacity is lost whenever the server switches
to serve a different set of queues. The capacity loss due to this switching
overhead can be significant in many emerging applications, and needs to be
explicitly addressed in the design of scheduling policies. For example, in
60GHz wireless networks with directional antennas, base stations need to train
and reconfigure their beam patterns whenever they switch from one client to
another. Considerable switching overhead can also be observed in many other
queueing systems such as transportation networks and manufacturing systems.
While the celebrated Max-Weight policy achieves asymptotically optimal average
delay for systems without switching overhead, it fails to preserve
throughput-optimality, let alone delay-optimality, when switching overhead is
taken into account. We propose a class of Biased Max-Weight scheduling policies
that explicitly takes switching overhead into account. The Biased Max-Weight
policy can use either queue length or head-of-line waiting time as an indicator
of the system status. We prove that our policies not only are
throughput-optimal, but also can be made arbitrarily close to the asymptotic
lower bound on average delay. To validate the performance of the proposed
policies, we provide extensive simulation with various system topologies and
different traffic patterns. We show that the proposed policies indeed achieve
much better delay performance than that of the state-of-the-art policy."
"Fast Fourier Transforms (FFTs) are exploited in a wide variety of fields
ranging from computer science to natural sciences and engineering. With the
rising data production bandwidths of modern FFT applications, judging best
which algorithmic tool to apply, can be vital to any scientific endeavor. As
tailored FFT implementations exist for an ever increasing variety of high
performance computer hardware, choosing the best performing FFT implementation
has strong implications for future hardware purchase decisions, for resources
FFTs consume and for possibly decisive financial and time savings ahead of the
competition. This paper therefor presents gearshifft, which is an open-source
and vendor agnostic benchmark suite to process a wide variety of problem sizes
and types with state-of-the-art FFT implementations (fftw, clfft and cufft).
gearshifft provides a reproducible, unbiased and fair comparison on a wide
variety of hardware to explore which FFT variant is best for a given problem
size."
"We present a comparative analysis of the maximum performance achieved by the
Linpack benchmark on compute intensive hardware publicly available from
multiple cloud providers. We study both performance within a single compute
node, and speedup for distributed memory calculations with up to 32 nodes or at
least 512 computing cores. We distinguish between hyper-threaded and
non-hyper-threaded scenarios and estimate the performance per single computing
core. We also compare results with a traditional supercomputing system for
reference. Our findings provide a way to rank the cloud providers and
demonstrate the viability of the cloud for high performance computing
applications."
"Consider a single server queue serving a multiclass population. Some popular
scheduling policies for such a system (and of interest in this paper) are the
discriminatory processor sharing (DPS), discriminatory random order service
(DROS), generalized processor sharing (GPS) and weighted fair queueing (WFQ).
The aim of this paper is to show a certain equivalence between these scheduling
policies for the special case when the multiclass population have identical and
exponential service requirements. In fact, we show the equivalence between two
broader classes of policies that generalize the above mentioned four policies.
We specifically show that the sojourn time distribution for a customer of a
particular class in a system with the DPS (GPS) scheduling policy is a constant
multiple of the waiting time distribution of a customer of the same class in a
system with the DROS (respectively WFQ) policy."
"The rapidly growing number of large network analysis problems has led to the
emergence of many parallel and distributed graph processing systems---one
survey in 2014 identified over 80. Since then, the landscape has evolved; some
packages have become inactive while more are being developed. Determining the
best approach for a given problem is infeasible for most developers. To enable
easy, rigorous, and repeatable comparison of the capabilities of such systems,
we present an approach and associated software for analyzing the performance
and scalability of parallel, open-source graph libraries. We demonstrate our
approach on five graph processing packages: GraphMat, the Graph500, the Graph
Algorithm Platform Benchmark Suite, GraphBIG, and PowerGraph using synthetic
and real-world datasets. We examine previously overlooked aspects of parallel
graph processing performance such as phases of execution and energy usage for
three algorithms: breadth first search, single source shortest paths, and
PageRank and compare our results to Graphalytics."
"It has been shown that it is impossible to achieve both stringent end-to-end
deadline and reliability guarantees in a large network without having complete
information of all future packet arrivals. In order to maintain desirable
performance in the presence of uncertainty of future packet arrivals, common
practice is to add redundancy by increasing link capacities. This paper studies
the amount of capacity needed to provide stringent performance guarantees. We
propose a low-complexity online algorithm and prove that it only requires a
small amount of redundancy to guarantee both end-to-end deadline and
reliability. Further, we show that in large networks with very high reliability
requirements, the redundancy needed by our policy is at most twice as large as
a theoretical lower bound. Also, for practical implementation, we propose a
fully distributed protocol based on the previous centralized policy. Without
adding redundancy, we further propose a low-complexity order-optimal online
policy for the network. Simulation results also show that our policy achieves
much better performance than other state-of-the-art policies."
"The main goal for this article is to compare performance penalties when using
KVM virtualization and Docker containers for creating isolated environments for
HPC applications. The article provides both data obtained using commonly
accepted synthetic tests (High Performance Linpack) and real life applications
(OpenFOAM). The article highlights the influence on resulting application
performance of major infrastructure configuration options: CPU type presented
to VM, networking connection type used."
"Integrating a product of linear forms over the unit simplex can be done in
polynomial time if the number of variables n is fixed (V. Baldoni et al.,
2011). In this note, we highlight that this problem is equivalent to obtaining
the normalizing constant of state probabilities for a popular class of Markov
processes used in queueing network theory. In light of this equivalence, we
survey existing computational algorithms developed in queueing theory that can
be used for exact integration. For example, under some regularity conditions,
queueing theory algorithms can exactly integrate a product of linear forms of
total degree N by solving N systems of linear equations."
"Building on the 1977 pioneering work of R. Fagin, we give a closed-form
expression for the approximated Miss Rate (MR) of LRU Caches assuming a
power-law popularity. Asymptotic behavior of this expression is an already
known result when power-law parameter is above 1. It is extended to any value
of the parameter. In addition, we bring a new analysis of the conditions (cache
relative size, popularity parameter) under which the ratio of LRU MR to Static
MR is worst-case."
"We implement and benchmark parallel I/O methods for the fully-manycore driven
particle-in-cell code PIConGPU. Identifying throughput and overall I/O size as
a major challenge for applications on today's and future HPC systems, we
present a scaling law characterizing performance bottlenecks in
state-of-the-art approaches for data reduction. Consequently, we propose,
implement and verify multi-threaded data-transformations for the I/O library
ADIOS as a feasible way to trade underutilized host-side compute potential on
heterogeneous systems for reduced I/O latency."
"Continuous-time Markov chains with alarms (ACTMCs) allow for alarm events
that can be non-exponentially distributed. Within parametric ACTMCs, the
parameters of alarm-event distributions are not given explicitly and can be
subject of parameter synthesis. An algorithm solving the $\varepsilon$-optimal
parameter synthesis problem for parametric ACTMCs with long-run average
optimization objectives is presented. Our approach is based on reduction of the
problem to finding long-run average optimal strategies in semi-Markov decision
processes (semi-MDPs) and sufficient discretization of parameter (i.e., action)
space. Since the set of actions in the discretized semi-MDP can be very large,
a straightforward approach based on explicit action-space construction fails to
solve even simple instances of the problem. The presented algorithm uses an
enhanced policy iteration on symbolic representations of the action space. The
soundness of the algorithm is established for parametric ACTMCs with
alarm-event distributions satisfying four mild assumptions that are shown to
hold for uniform, Dirac and Weibull distributions in particular, but are
satisfied for many other distributions as well. An experimental implementation
shows that the symbolic technique substantially improves the efficiency of the
synthesis algorithm and allows to solve instances of realistic size."
"Vectors of data are at the heart of machine learning and data mining.
Recently, vector quantization methods have shown great promise in reducing both
the time and space costs of operating on vectors. We introduce a vector
quantization algorithm that can compress vectors over 12x faster than existing
techniques while also accelerating approximate vector operations such as
distance and dot product computations by up to 10x. Because it can encode over
2GB of vectors per second, it makes vector quantization cheap enough to employ
in many more circumstances. For example, using our technique to compute
approximate dot products in a nested loop can multiply matrices faster than a
state-of-the-art BLAS implementation, even when our algorithm must first
compress the matrices.
  In addition to showing the above speedups, we demonstrate that our approach
can accelerate nearest neighbor search and maximum inner product search by over
100x compared to floating point operations and up to 10x compared to other
vector quantization methods. Our approximate Euclidean distance and dot product
computations are not only faster than those of related algorithms with slower
encodings, but also faster than Hamming distance computations, which have
direct hardware support on the tested platforms. We also assess the errors of
our algorithm's approximate distances and dot products, and find that it is
competitive with existing, slower vector quantization algorithms."
"Markov decision processes (MDPs) are standard models for probabilistic
systems with non-deterministic behaviours. Mean payoff (or long-run average
reward) provides a mathematically elegant formalism to express performance
related properties. Strategy iteration is one of the solution techniques
applicable in this context. While in many other contexts it is the technique of
choice due to advantages over e.g. value iteration, such as precision or
possibility of domain-knowledge-aware initialization, it is rarely used for
MDPs, since there it scales worse than value iteration. We provide several
techniques that speed up strategy iteration by orders of magnitude for many
MDPs, eliminating the performance disadvantage while preserving all its
advantages."
"As parallelism becomes critically important in the semiconductor technology,
high-performance computing, and cloud applications, parallel network systems
will increasingly follow suit. Today, parallelism is an essential architectural
feature of 40/100/400 Gigabit Ethernet standards, whereby high speed Ethernet
systems are equipped with multiple parallel network interfaces. This creates
new network topology abstractions and new technology requirements: instead of a
single high capacity network link, multiple Ethernet end-points and interfaces
need to be considered together with multiple links in form of discrete parallel
paths. This new paradigm is enabling implementations of various new features to
improve overall system performance. In this paper, we analyze the performance
of parallel network systems with network coding. In particular, by using random
LNC (RLNC), - a code without the need for decoding, we can make use of the fact
that we have codes that are both distributed (removing the need for
coordination or optimization of resources) and composable (without the need to
exchange code information), leading to a fully stateless operation. We propose
a novel theoretical modeling framework, including derivation of the upper and
lower bounds as well as an expected value of the differential delay of parallel
paths, and the resulting queue size at the receiver. The results show a great
promise of network system parallelism in combination with RLNC: with a proper
set of design parameters, the differential delay and the buffer size at the
Ethernet receiver can be reduced significantly, while the cross-layer design
and routing can be greatly simplified."
"HTTP/2 (h2) is a new standard for Web communications that already delivers a
large share of Web traffic. Unlike HTTP/1, h2 uses only one underlying TCP
connection. In a cellular network with high loss and sudden spikes in latency,
which the TCP stack might interpret as loss, using a single TCP connection can
negatively impact Web performance. In this paper, we perform an extensive
analysis of real world cellular network traffic and design a testbed to emulate
loss characteristics in cellular networks. We use the emulated cellular network
to measure h2 performance in comparison to HTTP/1.1, for webpages synthesized
from HTTP Archive repository data.
  Our results show that, in lossy conditions, h2 achieves faster page load
times (PLTs) for webpages with small objects. For webpages with large objects,
h2 degrades the PLT. We devise a new domain-sharding technique that isolates
large and small object downloads on separate connections. Using sharding, we
show that under lossy cellular conditions, h2 over multiple connections
improves the PLT compared to h2 with one connection and HTTP/1.1 with six
connections. Finally, we recommend content providers and content delivery
networks to apply h2-aware domain-sharding on webpages currently served over h2
for improved mobile Web performance."
"In this paper we focus on the LRU cache where requests for distinct contents
are described by independent stationary and ergodic processes. We extend a
TTL-based approximation of the cache hit probability first proposed by Fagin
for the independence reference model to this more general workload model. We
show that under very general conditions this approximation is exact as the
cache size and the number of contents go to infinity. Moreover, we establish
this not only for the aggregate cache hit probability but also for every
individual content. Last, we obtain the rate of convergence."
"A battery swapping and charging station (BSCS) is an energy refueling
station, where i) electric vehicles (EVs) with depleted batteries (DBs) can
swap their DBs for fully-charged ones, and ii) the swapped DBs are then charged
until they are fully-charged. Successful deployment of a BSCS system
necessitates a careful planning of swapping- and charging-related
infrastructures, and thus a comprehensive performance evaluation of the BSCS is
becoming crucial. This paper studies such a performance evaluation problem with
a novel mixed queueing network (MQN) model and validates this model with
extensive numerical simulation. We adopt the EVs' blocking probability as our
quality-of-service measure and focus on studying the impact of the key
parameters of the BSCS (e.g., the numbers of parking spaces, swapping islands,
chargers, and batteries) on the blocking probability. We prove a necessary and
sufficient condition for showing the ergodicity of the MQN when the number of
batteries approaches infinity, and further prove that the blocking probability
has two different types of asymptotic behaviors. Meanwhile, for each type of
asymptotic behavior, we analytically derive the asymptotic lower bound of the
blocking probability."
"The performance of successful Web-based e-commerce services has all the
allure of a roller-coaster ride: accelerated fiscal growth combined with the
ever-present danger of running out of server capacity. This chapter presents a
case study based on the author's own capacity planning engagement with one of
the hottest e-commerce Web sites in the world. Several spreadsheet techniques
are presented for forecasting both short-term and long-term trends in the
consumption of server capacity. Two new performance metrics are introduced for
site planning and procurement: the effective demand, and the doubling period."
"Criticism of Gnutella network scalability has rested on the bandwidth
attributes of the original interconnection topology: a Cayley tree. Trees, in
general, are known to have lower aggregate bandwidth than higher dimensional
topologies e.g., hypercubes, meshes and tori. Gnutella was intended to support
thousands to millions of peers. Studies of interconnection topologies in the
literature, however, have focused on hardware implementations which are limited
by cost to a few thousand nodes. Since the Gnutella network is virtual,
hyper-topologies are relatively unfettered by such constraints. We present
performance models for several plausible hyper-topologies and compare their
query throughput up to millions of peers. The virtual hypercube and the virtual
hypertorus are shown to offer near linear scalability subject to the number of
peer TCP/IP connections that can be simultaneously kept open."
"A basic calculus is presented for stochastic service guarantee analysis in
communication networks. Central to the calculus are two definitions,
maximum-(virtual)-backlog-centric (m.b.c) stochastic arrival curve and
stochastic service curve, which respectively generalize arrival curve and
service curve in the deterministic network calculus framework. With m.b.c
stochastic arrival curve and stochastic service curve, various basic results
are derived under the (min, +) algebra for the general case analysis, which are
crucial to the development of stochastic network calculus. These results
include (i) superposition of flows, (ii) concatenation of servers, (iii) output
characterization, (iv) per-flow service under aggregation, and (v) stochastic
backlog and delay guarantees. In addition, to perform independent case
analysis, stochastic strict server is defined, which uses an ideal service
process and an impairment process to characterize a server. The concept of
stochastic strict server not only allows us to improve the basic results (i) --
(v) under the independent case, but also provides a convenient way to find the
stochastic service curve of a serve. Moreover, an approach is introduced to
find the m.b.c stochastic arrival curve of a flow and the stochastic service
curve of a server."
"Algorithms for efficiently finding optimal alphabetic decision trees -- such
as the Hu-Tucker algorithm -- are well established and commonly used. However,
such algorithms generally assume that the cost per decision is uniform and thus
independent of the outcome of the decision. The few algorithms without this
assumption instead use one cost if the decision outcome is ``less than'' and
another cost otherwise. In practice, neither assumption is accurate for
software optimized for today's microprocessors. Such software generally has one
cost for the more likely decision outcome and a greater cost -- often far
greater -- for the less likely decision outcome. This problem and
generalizations thereof are thus applicable to hard coding static decision tree
instances in software, e.g., for optimizing program bottlenecks or for
compiling switch statements. An O(n^3)-time O(n^2)-space dynamic programming
algorithm can solve this optimal binary decision tree problem, and this
approach has many generalizations that optimize for the behavior of processors
with predictive branch capabilities, both static and dynamic. Solutions to this
formulation are often faster in practice than ``optimal'' decision trees as
formulated in the literature. Different search paradigms can sometimes yield
even better performance."
"The decision tree is one of the most fundamental programming abstractions. A
commonly used type of decision tree is the alphabetic binary tree, which uses
(without loss of generality) ``less than'' versus ''greater than or equal to''
tests in order to determine one of $n$ outcome events. The process of finding
an optimal alphabetic binary tree for a known probability distribution on
outcome events usually has the underlying assumption that the cost (time) per
decision is uniform and thus independent of the outcome of the decision. This
assumption, however, is incorrect in the case of software to be optimized for a
given microprocessor, e.g., in compiling switch statements or in fine-tuning
program bottlenecks. The operation of the microprocessor generally means that
the cost for the more likely decision outcome can or will be less -- often far
less -- than the less likely decision outcome. Here we formulate a variety of
$O(n^3)$-time $O(n^2)$-space dynamic programming algorithms to solve such
optimal binary decision tree problems, optimizing for the behavior of
processors with predictive branch capabilities, both static and dynamic. In the
static case, we use existing results to arrive at entropy-based performance
bounds. Solutions to this formulation are often faster in practice than
``optimal'' decision trees as formulated in the literature, and, for small
problems, are easily worth the extra complexity in finding the better solution.
This can be applied in fast implementation of decoding Huffman codes."
"This is Part II of a two-part paper series that studies the use of the
proportional fairness (PF) utility function as the basis for capacity
allocation and scheduling in multi-channel multi-rate wireless networks. The
contributions of Part II are twofold. (i) First, we extend the problem
formulation, theoretical results, and algorithms to the case of time-varying
channels, where opportunistic capacity allocation and scheduling can be
exploited to improve system performance. We lay down the theoretical foundation
for optimization that ""couples"" the time-varying characteristic of channels
with the requirements of the underlying applications into one consideration. In
particular, the extent to which opportunistic optimization is possible is not
just a function of how fast the channel characteristics vary, but also a
function of the elasticity of the underlying applications for delayed capacity
allocation. (ii) Second, building upon our theoretical framework and results,
we study subcarrier allocation and scheduling in orthogonal frequency division
multiplexing (OFDM) cellular wireless networks. We introduce the concept of a
W-normalized Doppler frequency to capture the extent to which opportunistic
scheduling can be exploited to achieve throughput-fairness performance gain. We
show that a ""look-back PF"" scheduling can strike a good balance between system
throughput and fairness while taking the underlying application requirements
into account."
"ICOOOLPS'2006 was the first edition of ECOOP-ICOOOLPS workshop. It intended
to bring researchers and practitioners both from academia and industry
together, with a spirit of openness, to try and identify and begin to address
the numerous and very varied issues of optimization. This succeeded, as can be
seen from the papers, the attendance and the liveliness of the discussions that
took place during and after the workshop, not to mention a few new cooperations
or postdoctoral contracts. The 22 talented people from different groups who
participated were unanimous to appreciate this first edition and recommend that
ICOOOLPS be continued next year. A community is thus beginning to form, and
should be reinforced by a second edition next year, with all the improvements
this first edition made emerge."
This paper has been withdrawn by the author.
"We report the design and implementation of a call-graph profiler for GNU
Octave, a numerical computing platform. GNU Octave simplifies matrix
computation for use in modeling or simulation. Our work provides a call-graph
profiler, which is an improvement on the flat profiler. We elaborate design
constraints of building a profiler for numerical computation, and benchmark the
profiler by comparing it to the rudimentary timer start-stop (tic-toc)
measurements, for a similar set of programs. The profiler code provides clean
interfaces to internals of GNU Octave, for other (newer) profiling tools on GNU
Octave."
"ALOHA is one of the most basic Medium Access Control (MAC) protocols and
represents a foundation for other more sophisticated distributed and
asynchronous MAC protocols, e.g., CSMA. In this paper, unlike in the
traditional work that focused on mean value analysis, we study the
distributional properties of packet transmission delays over an ALOHA channel.
We discover a new phenomenon showing that a basic finite population ALOHA model
with variable size (exponential) packets is characterized by power law
transmission delays, possibly even resulting in zero throughput. These results
are in contrast to the classical work that shows exponential delays and
positive throughput for finite population ALOHA with fixed packets.
Furthermore, we characterize a new stability condition that is entirely derived
from the tail behavior of the packet and backoff distributions that may not be
determined by mean values. The power law effects and the possible instability
might be diminished, or perhaps eliminated, by reducing the variability of
packets. However, we show that even a slotted (synchronized) ALOHA with packets
of constant size can exhibit power law delays when the number of active users
is random. From an engineering perspective, our results imply that the
variability of packet sizes and number of active users need to be taken into
consideration when designing robust MAC protocols, especially for ad-hoc/sensor
networks where other factors, such as link failures and mobility, might further
compound the problem."
"Scheduling policies for real-time systems exhibit threshold behavior that is
related to the utilization of the task set they schedule, and in some cases
this threshold is sharp. For the rate monotonic scheduling policy, we show that
periodic workload with utilization less than a threshold $U_{RM}^{*}$ can be
scheduled almost surely and that all workload with utilization greater than
$U_{RM}^{*}$ is almost surely not schedulable. We study such sharp threshold
behavior in the context of processor scheduling using static task priorities,
not only for periodic real-time tasks but for aperiodic real-time tasks as
well. The notion of a utilization threshold provides a simple schedulability
test for most real-time applications. These results improve our understanding
of scheduling policies and provide an interesting characterization of the
typical behavior of policies. The threshold is sharp (small deviations around
the threshold cause schedulability, as a property, to appear or disappear) for
most policies; this is a happy consequence that can be used to address the
limitations of existing utilization-based tests for schedulability. We
demonstrate the use of such an approach for balancing power consumption with
the need to meet deadlines in web servers."
"Typical protocols for peer-to-peer file sharing over the Internet divide
files to be shared into pieces. New peers strive to obtain a complete
collection of pieces from other peers and from a seed. In this paper we
investigate a problem that can occur if the seeding rate is not large enough.
The problem is that, even if the statistics of the system are symmetric in the
pieces, there can be symmetry breaking, with one piece becoming very rare. If
peers depart after obtaining a complete collection, they can tend to leave
before helping other peers receive the rare piece. Assuming that peers arrive
with no pieces, there is a single seed, random peer contacts are made, random
useful pieces are downloaded, and peers depart upon receiving the complete
file, the system is stable if the seeding rate (in pieces per time unit) is
greater than the arrival rate, and is unstable if the seeding rate is less than
the arrival rate. The result persists for any piece selection policy that
selects from among useful pieces, such as rarest first, and it persists with
the use of network coding."
"We consider a wireless network where each flow (instead of each link) runs
its own CSMA (Carrier Sense Multiple Access) algorithm. Specifically, each flow
attempts to access the radio channel after some random time and transmits a
packet if the channel is sensed idle. We prove that, unlike the standard CSMA
algorithm, this simple distributed access scheme is optimal in the sense that
the network is stable for all traffic intensities in the capacity region of the
network."
"CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is
an open standard that can be used to program CPUs, GPUs, and other devices from
different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL
promises a portable language for GPU programming, its generality may entail a
performance penalty. In this paper, we use complex, near-identical kernels from
a Quantum Monte Carlo application to compare the performance of CUDA and
OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel
to an OpenCL kernel involves minimal modifications. Making such a kernel
compile with ATI's build tools involves more modifications. Our performance
tests measure and compare data transfer times to and from the GPU, kernel
execution times, and end-to-end application execution times for both CUDA and
OpenCL."
"We present a new tool, GPA, that can generate key performance measures for
very large systems. Based on solving systems of ordinary differential equations
(ODEs), this method of performance analysis is far more scalable than
stochastic simulation. The GPA tool is the first to produce higher moment
analysis from differential equation approximation, which is essential, in many
cases, to obtain an accurate performance prediction. We identify so-called
switch points as the source of error in the ODE approximation. We investigate
the switch point behaviour in several large models and observe that as the
scale of the model is increased, in general the ODE performance prediction
improves in accuracy. In the case of the variance measure, we are able to
justify theoretically that in the limit of model scale, the ODE approximation
can be expected to tend to the actual variance of the model."
"We present Mantis, a new framework that automatically predicts program
performance with high accuracy. Mantis integrates techniques from programming
language and machine learning for performance modeling, and is a radical
departure from traditional approaches. Mantis extracts program features, which
are information about program execution runs, through program instrumentation.
It uses machine learning techniques to select features relevant to performance
and creates prediction models as a function of the selected features. Through
program analysis, it then generates compact code slices that compute these
feature values for prediction. Our evaluation shows that Mantis can achieve
more than 93% accuracy with less than 10% training data set, which is a
significant improvement over models that are oblivious to program features. The
system generates code slices that are cheap to compute feature values."
"Multi-stage sensing is a novel concept that refers to a general class of
spectrum sensing algorithms that divide the sensing process into a number of
sequential stages. The number of sensing stages and the sensing technique per
stage can be used to optimize performance with respect to secondary user
throughput and the collision probability between primary and secondary users.
So far, the impact of multi-stage sensing on network throughput and collision
probability for a realistic network model is relatively unexplored. Therefore,
we present the first analytical framework which enables performance evaluation
of different multi-channel multi-stage spectrum sensing algorithms for
Opportunistic Spectrum Access networks. The contribution of our work lies in
studying the effect of the following parameters on performance: number of
sensing stages, physical layer sensing techniques and durations per each stage,
single and parallel channel sensing and access, number of available channels,
primary and secondary user traffic, buffering of incoming secondary user
traffic, as well as MAC layer sensing algorithms. Analyzed performance metrics
include the average secondary user throughput and the average collision
probability between primary and secondary users. Our results show that when the
probability of primary user mis-detection is constrained, the performance of
multi-stage sensing is, in most cases, superior to the single stage sensing
counterpart. Besides, prolonged channel observation at the first stage of
sensing decreases the collision probability considerably, while keeping the
throughput at an acceptable level. Finally, in realistic primary user traffic
scenarios, using two stages of sensing provides a good balance between
secondary users throughput and collision probability while meeting successful
detection constraints subjected by Opportunistic Spectrum Access communication."
"In multi-class communication networks, traffic surges due to one class of
users can significantly degrade the performance for other classes. During these
transient periods, it is thus of crucial importance to implement priority
mechanisms that conserve the quality of service experienced by the affected
classes, while ensuring that the temporarily unstable class is not entirely
neglected. In this paper, we examine the complex interaction occurring between
several classes of traffic when classes obtain bandwidth proportionally to
their incoming traffic.
  We characterize the evolution of the network from the moment the initial
surge takes place until the system reaches its equilibrium. Using an
appropriate scaling, we show that the trajectories of the temporarily unstable
class can be described by a differential equation, while those of the stable
classes retain their stochastic nature. A stochastic averaging phenomenon
occurs and the dynamics of the temporarily unstable and the stable classes
continue to influence one another. We further proceed to characterize the
obtained differential equations and the stability region under this scaling for
monotone networks. We illustrate these result on several toy examples and we
finally build a penalization rule using these results for a network integrating
streaming and elastic traffic."
"This technical report covers a set of experiments on the 64-core SPARC T3-4
system, comparing it to two similar AMD and Intel systems. Key characteristics
as maximum integer and floating point arithmetic throughput are measured as
well as memory throughput, showing the scalability of the SPARC T3-4 system.
The performance of POSIX threads primitives is characterized and compared in
detail, such as thread creation and mutex synchronization. Scalability tests
with a fine grained multithreaded runtime are performed, showing problems with
atomic CAS operations on such physically highly parallel systems."
"Our purpose in this paper is to characterize buffer starvations for streaming
services. The buffer is modeled as an M/M/1 queue, plus the consideration of
bursty arrivals. When the buffer is empty, the service restarts after a certain
amount of packets are \emph{prefetched}. With this goal, we propose two
approaches to obtain the \emph{exact distribution} of the number of buffer
starvations, one of which is based on \emph{Ballot theorem}, and the other uses
recursive equations. The Ballot theorem approach gives an explicit result. We
extend this approach to the scenario with a constant playback rate using
T\`{a}kacs Ballot theorem. The recursive approach, though not offering an
explicit result, can obtain the distribution of starvations with
non-independent and identically distributed (i.i.d.) arrival process in which
an ON/OFF bursty arrival process is considered in this work. We further compute
the starvation probability as a function of the amount of prefetched packets
for a large number of files via a fluid analysis. Among many potential
applications of starvation analysis, we show how to apply it to optimize the
objective quality of experience (QoE) of media streaming, by exploiting the
tradeoff between startup/rebuffering delay and starvations."
"Statistical model checking avoids the exponential growth of states associated
with probabilistic model checking by estimating properties from multiple
executions of a system and by giving results within confidence bounds. Rare
properties are often very important but pose a particular challenge for
simulation-based approaches, hence a key objective under these circumstances is
to reduce the number and length of simulations necessary to produce a given
level of confidence. Importance sampling is a well-established technique that
achieves this, however to maintain the advantages of statistical model checking
it is necessary to find good importance sampling distributions without
considering the entire state space.
  Motivated by the above, we present a simple algorithm that uses the notion of
cross-entropy to find the optimal parameters for an importance sampling
distribution. In contrast to previous work, our algorithm uses a low
dimensional vector of parameters to define this distribution and thus avoids
the often intractable explicit representation of a transition matrix. We show
that our parametrisation leads to a unique optimum and can produce many orders
of magnitude improvement in simulation efficiency. We demonstrate the efficacy
of our methodology by applying it to models from reliability engineering and
biochemistry."
"The electrical power consumed by typical magnetic hard disk drives (HDD) not
only increases linearly with the number of spindles but, more significantly, it
increases as very fast power-laws of speed (RPM) and diameter. Since the
theoretical basis for this relationship is neither well-known nor readily
accessible in the literature, we show how these exponents arise from
aerodynamic disk drag and discuss their import for green storage capacity
planning."
"In this paper we discuss ways to reduce the execution time of a software
Global Navigation Satellite System (GNSS) receiver that is meant for offline
operation in a cloud environment. Client devices record satellite signals they
receive, and send them to the cloud, to be processed by this software. The goal
of this project is for each client request to be processed as fast as possible,
but also to increase total system throughput by making sure as many requests as
possible are processed within a unit of time. The characteristics of our
application provided both opportunities and challenges for increasing
performance. We describe the speedups we obtained by enabling the software to
exploit multi-core CPUs and GPGPUs. We mention which techniques worked for us
and which did not. To increase throughput, we describe how we control the
resources allocated to each invocation of the software to process a client
request, such that multiple copies of the application can run at the same time.
We use the notion of effective running time to measure the system's throughput
when running multiple instances at the same time, and show how we can determine
when the system's computing resources have been saturated."
"Data centers have been evolved from a passive element of compute
infrastructure to become an active, core part of any ICT solution. In
particular, modular data centers (MDCs), which are a promising design approach
to improve resiliency of data centers, can play a key role in deploying ICT
infrastructure in remote and inhospitable environments in order to take
advantage of low temperatures and hydro- and wind-electric capabilities. This
is because of capability of the modular data centers to survive even in lack of
continuous on-site maintenance and support. The most critical part of a data
center is its network fabric that could impede the whole system even if all
other components are fully functional, assuming that other analyses has been
already performed to ensure the reliability of the underlying infrastructure
and support systems. In this work, a complete failure analysis of modular data
centers using failure models of various components including servers, switches,
and links is performed using a proposed Monte-Carlo approach. The proposed
Monte-Carlo approach, which is based on the concept of snapshots, allows us to
effectively calculate the performance of a design along its lifespan even up to
the terminal stages. To show the capabilities of the proposed approach, various
network topologies, such as FatTree, BCube, MDCube, and their modifications are
considered. The performance and also the lifespan of each topology design in
presence of failures of their components are studied against the topology
parameters."
"The ATLAS Virtual Organization is grid's largest Virtual Organization which
is currently in full production stage. Hereby a case is being made that a user
working within that VO is going to face a wide spectrum of different systems,
whose heterogeneity is enough to count as ""orders of magnitude"" according to a
number of metrics; including integer/float operations, memory throughput
(STREAM) and communication latencies. Furthermore, the spread of performance
does not appear to follow any known distribution pattern, which is demonstrated
in graphs produced during May 2007 measurements. It is implied that the current
practice where either ""all-WNs-are-equal"" or, the alternative of SPEC-based
rating used by LCG/EGEE is an oversimplification which is inappropriate and
expensive from an operational point of view, therefore new techniques are
needed for optimal grid resources allocation."
"We present a model of performance bound calculus on feedforward networks
where data packets are routed under wormhole routing discipline. We are
interested in determining maximum end-to-end delays and backlogs of messages or
packets going from a source node to a destination node, through a given virtual
path in the network. Our objective here is to give a network calculus approach
for calculating the performance bounds. First we propose a new concept of
curves that we call packet curves. The curves permit to model constraints on
packet lengths of a given data flow, when the lengths are allowed to be
different. Second, we use this new concept to propose an approach for
calculating residual services for data flows served under non preemptive
service disciplines. Third, we model a binary switch (with two input ports and
two output ports), where data is served under wormhole discipline. We present
our approach for computing the residual services and deduce the worst case
bounds for flows passing through a wormhole binary switch. Finally, we
illustrate this approach in numerical examples, and show how to extend it to
feedforward networks."
"Information about primary transmitter location is crucial in enabling several
key capabilities in cognitive radio networks, including improved
spatio-temporal sensing, intelligent location-aware routing, as well as aiding
spectrum policy enforcement. Compared to other proposed non-interactive
localization algorithms, the weighted centroid localization (WCL) scheme uses
only the received signal strength information, which makes it simple to
implement and robust to variations in the propagation environment. In this
paper we present the first theoretical framework for WCL performance analysis
in terms of its localization error distribution parameterized by node density,
node placement, shadowing variance, correlation distance and inaccuracy of
sensor node positioning. Using this analysis, we quantify the robustness of WCL
to various physical conditions and provide design guidelines, such as node
placement and spacing, for the practical deployment of WCL. We also propose a
power-efficient method for implementing WCL through a distributed cluster-based
algorithm, that achieves comparable accuracy with its centralized counterpart."
"The performance of cluster computing depends on how concurrent jobs share
multiple data center resource types like CPU, RAM and disk storage. Recent
research has discussed efficiency and fairness requirements and identified a
number of desirable scheduling objectives including so-called dominant resource
fairness (DRF). We argue here that proportional fairness (PF), long recognized
as a desirable objective in sharing network bandwidth between ongoing flows, is
preferable to DRF. The superiority of PF is manifest under the realistic
modelling assumption that the population of jobs in progress is a stochastic
process. In random traffic the strategy-proof property of DRF proves
unimportant while PF is shown by analysis and simulation to offer a
significantly better efficiency-fairness tradeoff."
"We consider a queueing system composed of a dispatcher that routes
deterministically jobs to a set of non-observable queues working in parallel.
In this setting, the fundamental problem is which policy should the dispatcher
implement to minimize the stationary mean waiting time of the incoming jobs. We
present a structural property that holds in the classic scaling of the system
where the network demand (arrival rate of jobs) grows proportionally with the
number of queues. Assuming that each queue of type $r$ is replicated $k$ times,
we consider a set of policies that are periodic with period $k \sum_r p_r$ and
such that exactly $p_r$ jobs are sent in a period to each queue of type $r$.
When $k\to\infty$, our main result shows that all the policies in this set are
equivalent, in the sense that they yield the same mean stationary waiting time,
and optimal, in the sense that no other policy having the same aggregate
arrival rate to \emph{all} queues of a given type can do better in minimizing
the stationary mean waiting time. This property holds in a strong probabilistic
sense. Furthermore, the limiting mean waiting time achieved by our policies is
a convex function of the arrival rate in each queue, which facilitates the
development of a further optimization aimed at solving the fundamental problem
above for large systems."
"Degradation analysis is used to analyze the useful lifetimes of systems,
their failure rates, and various other system parameters like mean time to
failure (MTTF), mean time between failures (MTBF), and the system failure rate
(SFR). In many systems, certain possible parallel paths of execution that have
greater chances of success are preferred over others. Thus we introduce here
the concept of probabilistic parallel choice. We use binary and $n$-ary
probabilistic choice operators in describing the selections of parallel paths.
These binary and $n$-ary probabilistic choice operators are considered so as to
represent the complete system (described as a series-parallel system) in terms
of the probabilities of selection of parallel paths and their relevant
parameters. Our approach allows us to derive new and generalized formulae for
system parameters like MTTF, MTBF, and SFR. We use a generalized exponential
distribution, allowing distinct installation times for individual components,
and use this model to derive expressions for such system parameters."
"We consider a GI/H/n queueing system. In this system, there are multiple
servers in the queue. The inter-arrival time is general and independent, and
the service time follows hyper-exponential distribution. Instead of stochastic
differential equations, we propose two heavy traffic limits for this system,
which can be easily applied in practical systems. In applications, we show how
to use these heavy traffic limits to design a power efficient cloud computing
environment based on different QoS requirements."
"Modern DRAM architectures allow a number of low-power states on individual
memory ranks for advanced power management. Many previous studies have taken
advantage of demotions on low-power states for energy saving. However, most of
the demotion schemes are statically performed on a limited number of
pre-selected low-power states, and are suboptimal for different workloads and
memory architectures. Even worse, the idle periods are often too short for
effective power state transitions, especially for memory intensive
applications. Wrong decisions on power state transition incur significant
energy and delay penalties. In this paper, we propose a novel memory system
design named RAMZzz with rank-aware energy saving optimizations including
dynamic page migrations and adaptive demotions. Specifically, we group the
pages with similar access locality into the same rank with dynamic page
migrations. Ranks have their hotness: hot ranks are kept busy for high
utilization and cold ranks can have more lengthy idle periods for power state
transitions. We further develop adaptive state demotions by considering all
low-power states for each rank and a prediction model to estimate the
power-down timeout among states. We experimentally compare our algorithm with
other energy saving policies with cycle-accurate simulation. Experiments with
benchmark workloads show that RAMZzz achieves significant improvement on
energy-delay2 and energy consumption over other energy saving techniques."